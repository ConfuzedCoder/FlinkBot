Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Outward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Flink FileSystem SQL Connector Generating SUCESS File Multiple Times,FLINK-35521,13581550,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ewangviasat,ewangviasat,04/Jun/24 16:27,04/Jun/24 16:30,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Connectors / FileSystem,,,,0,,,"Our Flink table SQL job received data from the Kafka streams and then sinked all partitioned data into the associated parquet files under the same S3 folder through the filesystem SQL connector.

For the S3 filesystem SQL connector, sink.partition-commit.policy.kind was set to 'success-file' and sink.partition-commit.trigger was set to 'partition-time'. We found that _SUCCESS file in the S3 folder was generated multiple times after multiple partitions are committed.

Because all partitioned parquet files and _SUCCESS file are in the same S3 folder and _SUCCESS file is used to trigger the downstream application, we really like the _SUCCESS file to be generated only once instead of multiple times after all partitions are committed and all parquet files are ready to be processed. Thus, one _SUCCESS file can be used to trigger the downstream application only once instead of multiple times.

We knew we could set sink.partition-commit.trigger to 'process-time' to generate _SUCCESS file only once in the S3 folder; however, 'process-time' would not meet our business requirements.

We'd request the FileSystem SQL connector should support to the following new user case:
Even if sink.partition-commit.trigger is set to 'partition-time', _SUCCESS file will be generated only once after all partitions are committed and all output files are ready to be processed, and will be used to trigger the downstream application only once instead of multiple times.",Our PyFlink SQL jobs are running in AWS EKS environment.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,2024-06-04 16:27:00.0,,,,,,,,,,"0|z1pm14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
master can't compile as license check failed,FLINK-35520,13581503,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,Weijie Guo,Weijie Guo,04/Jun/24 09:44,04/Jun/24 10:59,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=60024&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=45808,,,,,,,,,,,,,,,,,,,,,FLINK-35282,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 10:59:28 UTC 2024,,,,,,,,,,"0|z1plqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/24 09:53;Weijie Guo;Hi [~antoniovespoli], would you mind taking a look? I found this issue after FLINK-35282 merged.;;;","04/Jun/24 10:59;Sergey Nuyanzin;so far it was reverted as mentioned in comments FLINK-35282
 PS:  I have a fix for this at https://github.com/snuyanzin/flink/commit/5a4f4d0eb785050552c73fbfc74214f85ee278b0
which could be tried when build will be more stable;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Job fails with SingleValueAggFunction received more than one element,FLINK-35519,13581499,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dwysakowicz,dwysakowicz,04/Jun/24 09:32,04/Jun/24 09:32,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Table SQL / Planner,,,,0,,,"When running a query:
{code}
select 
   (SELECT 
       t.id 
    FROM raw_pagerduty_users, UNNEST(teams) AS t(id, type, summary, self, html_url))
from raw_pagerduty_users;
{code}
it is translated to:

{code}
Sink(table=[default_catalog.default_database.sink], fields=[EXPR$0])
+- Calc(select=[$f0 AS EXPR$0])
   +- Join(joinType=[LeftOuterJoin], where=[true], select=[c, $f0], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
      :- Exchange(distribution=[single])
      :  +- Calc(select=[c])
      :     +- TableSourceScan(table=[[default_catalog, default_database, raw_pagerduty_users, project=[c, teams], metadata=[]]], fields=[c, teams])(reuse_id=[1])
      +- Exchange(distribution=[single])
         +- GroupAggregate(select=[SINGLE_VALUE(id) AS $f0])
            +- Exchange(distribution=[single])
               +- Calc(select=[id])
                  +- Correlate(invocation=[$UNNEST_ROWS$1($cor0.teams)], correlate=[table($UNNEST_ROWS$1($cor0.teams))], select=[c,teams,id,type,summary,self,html_url], rowType=[RecordType(BIGINT c, RecordType:peek_no_expand(VARCHAR(2147483647) id, VARCHAR(2147483647) type, VARCHAR(2147483647) summary, VARCHAR(2147483647) self, VARCHAR(2147483647) html_url) ARRAY teams, VARCHAR(2147483647) id, VARCHAR(2147483647) type, VARCHAR(2147483647) summary, VARCHAR(2147483647) self, VARCHAR(2147483647) html_url)], joinType=[INNER])
                     +- Reused(reference_id=[1])
{code}

and it fails with:

{code}
java.lang.RuntimeException: SingleValueAggFunction received more than one element.
	at GroupAggsHandler$150.accumulate(Unknown Source)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:151)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
	at org.apache.flink.streaming.runtime.io.RecordProcessorUtils.lambda$getRecordProcessor$0(RecordProcessorUtils.java:60)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:237)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:146)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:571)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:900)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:849)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.base/java.lang.Thread.run(Thread.java:829)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-04 09:32:08.0,,,,,,,,,,"0|z1plps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI Bot doesn't run on PRs - status UNKNOWN,FLINK-35518,13581493,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,jingge,pnowojski,pnowojski,04/Jun/24 09:08,04/Jun/24 10:59,04/Jun/24 20:40,04/Jun/24 10:59,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"Doesn't want to run on my PR/branch. I was doing force-pushes, rebases, asking flink bot to run, closed and opened new PR, but nothing helped
https://github.com/apache/flink/pull/24868
https://github.com/apache/flink/pull/24883

I've heard others were having similar problems recently.",,,,,,,,,,,,,,,,,,,,FLINK-35517,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 09:11:03 UTC 2024,,,,,,,,,,"0|z1plog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/24 09:11;Weijie Guo;Yes, I have created a ticket(FLINK-35517) also intend to track this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI pipeline triggered by pull request seems unstable,FLINK-35517,13581488,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,Weijie Guo,Weijie Guo,04/Jun/24 08:32,04/Jun/24 11:00,04/Jun/24 20:40,04/Jun/24 11:00,1.20.0,,,,,,,,1.20.0,,,Build System / CI,,,,0,,,"Flink CI pipeline triggered by pull request seems sort of unstable. 

For example, https://github.com/apache/flink/pull/24883 was filed 15 hours ago, but CI report is UNKNOWN.",,,,,,,,,,,,,,,,,,,FLINK-35518,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 11:00:54 UTC 2024,,,,,,,,,,"0|z1plnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/24 08:56;Weijie Guo;I have reached out [~lorenzo.affetti] to see if this is related to the Ververica machine?

cc [~jingge] ;;;","04/Jun/24 10:48;jingge;It should work again. [~Weijie Guo] please let me know if you still have any issues. Thanks!;;;","04/Jun/24 10:51;fanrui;Hi [~jingge] , thanks for the quick fix. :)

May I know whether all old PRs can be recovered as well? Or only new PR works now?;;;","04/Jun/24 11:00;jingge;[~fanrui] all PRs will be recovered.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Update the Experimental annotation to PublicEvolving for files connector ,FLINK-35516,13581471,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,RocMarshal,RocMarshal,RocMarshal,04/Jun/24 05:17,04/Jun/24 11:09,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / FileSystem,,,,0,pull-request-available,,"as described in https://issues.apache.org/jira/browse/FLINK-35496
We should update the annotations for the stable APIs in files connector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 05:18:15 UTC 2024,,,,,,,,,,"0|z1pljk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/24 05:18;RocMarshal;I'd like to contribute for the Jira.
Could someone assign it to me ?
Thanks a lot.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade hive version to 4.0.0,FLINK-35515,13581468,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vikasap,vikasap,04/Jun/24 03:19,04/Jun/24 16:31,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Connectors / Hive,,,,0,,,Hive version 4.0.0 was released recently. However none of the major flink versions will work with this. Filing this so that major flink version's flink-sql and table api will be able to work with the new version of hive metastore.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-04 03:19:52.0,,,,,,,,,,"0|z1pliw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Flink CDC Channel to Apache Flink Slack Workspace,FLINK-35514,13581463,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,leonard,gongzhongqiang,gongzhongqiang,04/Jun/24 01:53,04/Jun/24 03:43,04/Jun/24 20:40,04/Jun/24 03:43,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Build System,,,,0,,,DISCUSS thread : https://lists.apache.org/thread/gqzrs3c0j9k7c5m9m5k2slomgorrqrwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 03:43:10 UTC 2024,,,,,,,,,,"0|z1plhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/24 02:27;gongzhongqiang;Hi [~martijnvisser]  , Can you help create slack channel for flink-cdc ? cc [~Leonard];;;","04/Jun/24 02:38;leonard;Let me have a try;;;","04/Jun/24 03:43;leonard;done;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ArtifactFetchManager unit tests are failing,FLINK-35513,13581411,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,elphastori,elphastori,03/Jun/24 15:10,03/Jun/24 15:24,04/Jun/24 20:40,03/Jun/24 15:24,1.19.0,,,,,,,,,,,Client / Job Submission,,,,0,,,"The following unit tests in [ArtifactFetchManagerTest.java|https://github.com/apache/flink/blob/e4fa72d9e480664656818395741c37a9995f9334/flink-clients/src/test/java/org/apache/flink/client/program/artifact/ArtifactFetchManagerTest.java] are failing
 * testFileSystemFetchWithAdditionalUri()
 * testHttpFetch()
 * testMixedArtifactFetch()

 
{code:java}
Test org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.testFileSystemFetchWithAdditionalUri[testFileSystemFetchWithAdditionalUri()] failed with:
java.lang.AssertionError: 
Expecting actual not to be empty
 at org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.getFlinkClientsJar(ArtifactFetchManagerTest.java:251)
 at org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.testFileSystemFetchWithAdditionalUri(ArtifactFetchManagerTest.java:104)
 ...

--------------------------------------------------------------------------------
Test org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.testMixedArtifactFetch[testMixedArtifactFetch()] failed with:
java.lang.AssertionError: 
Expecting actual not to be empty
 at org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.getFlinkClientsJar(ArtifactFetchManagerTest.java:251)
 at org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.testMixedArtifactFetch(ArtifactFetchManagerTest.java:149)
 ...

--------------------------------------------------------------------------------
Test org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.testHttpFetch[testHttpFetch()] failed with:
java.lang.AssertionError: 
Expecting actual not to be empty
 at org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.getFlinkClientsJar(ArtifactFetchManagerTest.java:251)
 at org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.testHttpFetch(ArtifactFetchManagerTest.java:124)
 ... 
{code}
 ",,,,,,,,,,,,,,,,,,,,FLINK-35512,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-03 15:10:10.0,,,,,,,,,,"0|z1pl68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArtifactFetchManagerTest unit tests fail,FLINK-35512,13581405,,Technical Debt,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hong,hong,03/Jun/24 14:54,04/Jun/24 11:04,04/Jun/24 20:40,,1.19.1,,,,,,,,1.19.1,,,,,,,0,,,"The below three tests from *ArtifactFetchManagerTest* seem to fail consistently:
 * ArtifactFetchManagerTest.testFileSystemFetchWithAdditionalUri
 * ArtifactFetchManagerTest.testMixedArtifactFetch
 * ArtifactFetchManagerTest.testHttpFetch

The error printed is
{code:java}
java.lang.AssertionError:
Expecting actual not to be empty
    at org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.getFlinkClientsJar(ArtifactFetchManagerTest.java:248)
    at org.apache.flink.client.program.artifact.ArtifactFetchManagerTest.testMixedArtifactFetch(ArtifactFetchManagerTest.java:146)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
 ",,,,,,,,,,,,,,,,,,,FLINK-35513,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 11:04:22 UTC 2024,,,,,,,,,,"0|z1pl4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/24 16:15;hong;Turns out we do run the build test and it passes on nightlies

 

https://github.com/apache/flink/actions/runs/9343221840/job/25712717860#step:10:10364;;;","03/Jun/24 16:25;hong;However, it seems that the tests pass because we do the following:
 # Compile the build artifact for *flink-client.*
 # Run unit tests for *flink-client* module.

 ;;;","04/Jun/24 11:04;elphastori;[~hong] +1, I reproduced the error by running
{noformat}
./mvnw clean package{noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Enhance metrics. The incremental and full phases count the number of records separately,FLINK-35511,13581400,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,03/Jun/24 14:06,03/Jun/24 14:06,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,,,,Flink CDC,,,,0,,,"dataworks 实时同步的metric 信息

!image-2024-06-03-22-06-38-591.png!",,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/24 14:06;melin;image-2024-06-03-22-06-38-591.png;https://issues.apache.org/jira/secure/attachment/13069269/image-2024-06-03-22-06-38-591.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-03 14:06:48.0,,,,,,,,,,"0|z1pl3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement basic incremental checkpoint for ForStStateBackend,FLINK-35510,13581390,13574114,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Feifan Wang,Feifan Wang,03/Jun/24 13:01,04/Jun/24 03:30,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,"Use low DB api implement a basic incremental checkpoint for ForStStatebackend, follow steps:
 # db.disableFileDeletions()
 # db.getLiveFiles(true)
 # db.entableFileDeletes(false)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-03 13:01:11.0,,,,,,,,,,"0|z1pl1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slack community invite link has expired,FLINK-35509,13581372,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,sverma,uce,uce,03/Jun/24 11:09,03/Jun/24 14:41,04/Jun/24 20:40,,,,,,,,,,,,,Project Website,,,,0,pull-request-available,,"The Slack invite link on the website has expired.

I've generated a new invite link without expiration here: [https://join.slack.com/t/apache-flink/shared_invite/zt-2k0fdioxx-D0kTYYLh3pPjMu5IItqx3Q]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-03 11:09:12.0,,,,,,,,,,"0|z1pkxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use OceanBase LTS version Docker image in testing,FLINK-35508,13581343,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,wanghe,wanghe,03/Jun/24 07:00,03/Jun/24 07:00,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-03 07:00:57.0,,,,,,,,,,"0|z1pkr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support For Individual Job Level Resource Allocation in Session Cluster in k8s,FLINK-35507,13581331,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,amar1404,amar1404,03/Jun/24 04:48,03/Jun/24 04:48,04/Jun/24 20:40,,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,We can have a setup like Spark where in Spark Cluster we can set individual job level setting in a spark cluster to access the resouces from memory to core. Also Support Dynamic memory allocation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-03 04:48:39.0,,,,,,,,,,"0|z1pkog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disable kafka auto-commit and rely on flink’s checkpointing if both are enabled,FLINK-35506,13581329,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,elon,elon,03/Jun/24 04:11,04/Jun/24 11:53,04/Jun/24 20:40,04/Jun/24 11:53,1.16.1,,,,,,,,,,,Connectors / Kafka,,,,0,,,"When I use KafkaSource for consuming topics and set the Kafka parameter {{{}enable.auto.commit=true{}}}, while also enabling checkpointing for the task, I notice that both will commit offsets. Should Kafka's auto-commit be disabled when enabling Flink checkpointing, similar to how it's done with FlinkKafkaConsumer?

 

*How to reproduce*

 
{code:java}
// code placeholder
Properties kafkaParams = new Properties();
kafkaParams.put(""enable.auto.commit"", ""true"");
kafkaParams.put(""auto.offset.reset"", ""latest"");
kafkaParams.put(""fetch.min.bytes"", ""4096"");
kafkaParams.put(""sasl.mechanism"", ""PLAIN"");
kafkaParams.put(""security.protocol"", ""SASL_PLAINTEXT"");
kafkaParams.put(""bootstrap.servers"", bootStrap);
kafkaParams.put(""group.id"", expoGroupId);
kafkaParams.put(""sasl.jaas.config"",""org.apache.kafka.common.security.plain.PlainLoginModule required username=\"""" + username + ""\"" password=\"""" + password + ""\"";"");

KafkaSource<String> source = KafkaSource
        .<String>builder()
        .setBootstrapServers(bootStrap)
        .setProperties(kafkaParams)
        .setGroupId(expoGroupId)
        .setTopics(Arrays.asList(expoTopic))
        .setValueOnlyDeserializer(new SimpleStringSchema())
        .setStartingOffsets(OffsetsInitializer.latest())
        .build();

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.fromSource(source, WatermarkStrategy.noWatermarks(), ""kafka-source"")
        .filter(r ->  true);

env.enableCheckpointing(3000 * 1000);
env.getCheckpointConfig().setMinPauseBetweenCheckpoints(1000 * 1000);
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.AT_LEAST_ONCE);
env.getCheckpointConfig().setCheckpointTimeout(1000 * 300);
env.execute(""kafka-consumer""); {code}
 

 

the kafka client's org.apache.kafka.clients.consumer.internals.ConsumerCoordinator continuously committing offsets.

!image-2024-06-03-23-39-28-270.png!",,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/24 15:39;elon;image-2024-06-03-23-39-28-270.png;https://issues.apache.org/jira/secure/attachment/13069270/image-2024-06-03-23-39-28-270.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-03 04:11:29.0,,,,,,,,,,"0|z1pko0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RegionFailoverITCase.testMultiRegionFailover has never ever restored state,FLINK-35505,13581317,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,03/Jun/24 01:36,03/Jun/24 01:37,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"{code:java}
May 31 17:17:59 17:17:59.555 [ERROR] org.apache.flink.test.checkpointing.RegionFailoverITCase.testMultiRegionFailover -- Time elapsed: 27.92 s <<< FAILURE!
May 31 17:17:59 java.lang.AssertionError: The test multi-region job has never ever restored state.
May 31 17:17:59 	at org.junit.Assert.fail(Assert.java:89)
May 31 17:17:59 	at org.junit.Assert.assertTrue(Assert.java:42)
May 31 17:17:59 	at org.apache.flink.test.checkpointing.RegionFailoverITCase.verifyAfterJobExecuted(RegionFailoverITCase.java:157)
May 31 17:17:59 	at org.apache.flink.test.checkpointing.RegionFailoverITCase.testMultiRegionFailover(RegionFailoverITCase.java:149)
May 31 17:17:59 	at java.lang.reflect.Method.invoke(Method.java:498)
May 31 17:17:59 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
May 31 17:17:59 	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=60001&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9222
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-03 01:36:44.0,,,,,,,,,,"0|z1pklc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Elasticsearch 8 connector observability,FLINK-35504,13581272,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liuml07,liuml07,01/Jun/24 23:22,02/Jun/24 01:30,04/Jun/24 20:40,,elasticsearch-3.1.0,,,,,,,,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,"Currently all logs are in DEBUG level. Some of those messages are very helpful to get the progress and errors, which can be changed to INFO or WARN level. We can also include error details into DEBUG level messages so it's easier to debug with more context.

Meanwhile, we can add a new metric to track {{{}numRecordsSendPartialFailureCounter{}}}, which is number of records that were returned by Elasticsearch as failed and were retried. FWIW, the base class tracks following metrics already so we don't need to implement them: {{CurrentSendTime}} Gauge, {{NumBytesOut}} and {{NumRecordsOut}} Counters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-06-01 23:22:29.0,,,,,,,,,,"0|z1pkbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OracleE2eITCase fails with error ORA-12528 on Mac M2,FLINK-35503,13581265,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,sakkurn,sakkurn,01/Jun/24 16:59,03/Jun/24 17:03,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,,,"Hello Flink CDC community,

I am attempting to run `OracleE2eITCase` (in the cdc source connector e2e tests), and I am running into the following runtime exception: 
{code:java}
java.sql.SQLException: 
Listener refused the connection with the following error:
ORA-12528, TNS:listener: all appropriate instances are blocking new connections
 
    at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:854)
    at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:793)
    at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:57)
    at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:747)
    at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:562)
    at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
    at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:228)
    at com.ververica.cdc.connectors.tests.OracleE2eITCase.getOracleJdbcConnection(OracleE2eITCase.java:197)
    at com.ververica.cdc.connectors.tests.OracleE2eITCase.testOracleCDC(OracleE2eITCase.java:149)
    at java.base/java.lang.reflect.Method.invoke(Method.java:567)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
Caused by: oracle.net.ns.NetException: Listener refused the connection with the following error:
ORA-12528, TNS:listener: all appropriate instances are blocking new connections
 
    at oracle.net.ns.NSProtocolNIO.negotiateConnection(NSProtocolNIO.java:284)
    at oracle.net.ns.NSProtocol.connect(NSProtocol.java:340)
    at oracle.jdbc.driver.T4CConnection.connect(T4CConnection.java:1596)
    at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:588)
    ... 11 more{code}
I have attached the test results to this issue.

`OracleE2eITCase` runs the `goodboy008/oracle-19.3.0-ee:non-cdb` docker image. I am able to reproduce the same issue when I run this docker image locally - my observation is that dockerized Oracle DB instance is not being set up properly, as I notice another ORA in the setup logs (`ORA-03113: end-of-file on communication channel`). I have also attached the logs from the docker image setup to this issue. To reproduce the ORA-12528 issue locally, I:
 * ran: `docker run goodboy008/oracle-19.3.0-ee:non-cdb`
 * ssh'ed into the db pod
 * ran: `sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba`

Any insight/workaround on getting this e2e test and the docker image running on my machine would be much appreciated. I'm also happy to provide any other information regarding my setup in the comments. Thank you!

 "," 
 * Mac M2 (Apple Silicon)
 * using docker desktop with Rosetta enabled for amd64 emulation

 ",,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/24 16:55;sakkurn;com.ververica.cdc.connectors.tests.OracleE2eITCase.txt;https://issues.apache.org/jira/secure/attachment/13069243/com.ververica.cdc.connectors.tests.OracleE2eITCase.txt","01/Jun/24 16:55;sakkurn;oracle-docker-setup-logs.txt;https://issues.apache.org/jira/secure/attachment/13069242/oracle-docker-setup-logs.txt",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Mon Jun 03 17:03:10 UTC 2024,,,,,,,,,,"0|z1pka0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/24 02:02;xiqian_yu;Hi [~sakkurn],
Currently [~gongzhongqiang] tweaks Oracle EE docker image for running E2e tests and repackaged it as goodboy008/oracle-19.3.0-ee:non-cdb. However it supports amd64 only, and requires Rosetta (or something similar) to run.

Oracle community discussions[2] revealed that ORA-03113 also appears when running cross-architecture emulation, perhaps caused by a bug in Rosetta 2, but is hard to investigate and debug since they're not open sourced.

As this PR [3] has brought official arm64 architecture support to Oracle docker image, I think we can repackage Oracle E2e docker image to support both amd64 and arm64 architecture, which could help Mac developers run E2e tests easier. I'd like to help implementing this if needed.

[1] https://hub.docker.com/r/gvenzl/oracle-xe/tags
[2] https://github.com/oracle/docker-images/discussions/1951
[3] https://github.com/oracle/docker-images/pull/2659 ;;;","03/Jun/24 03:08;gongzhongqiang;[~xiqian_yu] Let me take this issue. Because It have some change to orginal.;;;","03/Jun/24 17:03;sakkurn;Thank you [~gongzhongqiang], [~xiqian_yu]! Is there a workaround I can use to get the E2E test/docker image running while you repackage the image?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
compress the checkpoint metadata generated by ZK/ETCD HA Services,FLINK-35502,13581260,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yingz,yingz,01/Jun/24 13:11,04/Jun/24 17:33,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,"In the implementation of Flink HA, the metadata of checkpoints is stored in either Zookeeper (ZK HA) or ETCD (K8S HA), such as:
{code:java}
checkpointID-0000000000000036044: xxxx
checkpointID-0000000000000036045: xxxx
...
... {code}
However, neither of these are designed to store excessive amounts of data. If the [state.checkpoints.num-retained]([https://nightlies.apache.org/flink/flink-docs-release-1.19/zh/docs/deployment/config/#state-checkpoints-num-retained]) setting is set too large, it can easily cause abnormalities in ZK/ETCD. 

The error log when set state.checkpoints.num-retained to 1500:
{code:java}
Caused by: org.apache.flink.util.SerializedThrowable: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://xxx/api/v1/namespaces/default/configmaps/xxx-jobmanager-leader. Message: ConfigMap ""xxx-jobmanager-leader"" is invalid: 0J:
Too long: must have at most 1048576 bytes. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=(StatusCause(field=[J, message=Too long: must have at most 1048576 bytes, reason=FieldValueTooLong, additionalProperties={})l, group=null, kind=ConfigMap, name=xxx-jobmanager-leader, retryAfterSeconds=null, uid=null, additionalProperties=(}), kind=Status, message=ConfigMap ""xxx-jobmanager-leader"" is invalid: [): Too long: must have at most 1048576 bytes, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties=(}). {code}
In Flink's code, all checkpoint metadata are updated at the same time, and The checkpoint metadata contains many repeated bytes, therefore it can achieve a very good compression ratio.

Therefore, I suggest compressing the data when writing checkpoints and decompressing it when reading, to reduce storage pressure and improve IO efficiency.

Here is the sample code, and reduce the metadata size from 1M bytes to 30K.
{code:java}
// Map -> Json
ObjectMapper objectMapper = new ObjectMapper();
String checkpointJson = objectMapper.writeValueAsString(checkpointMap); // // copress and base64  
String compressedBase64 = compressAndEncode(checkpointJson); compressedData.put(""checkpoint-all"", compressedBase64);{code}
{code:java}
    private static String compressAndEncode(String data) throws IOException {
        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
        try (GZIPOutputStream gzipOutputStream = new GZIPOutputStream(outputStream))
{             gzipOutputStream.write(data.getBytes(StandardCharsets.UTF_8));         }
        byte[] compressedData = outputStream.toByteArray();
        return Base64.getEncoder().encodeToString(compressedData);
    } {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 17:33:28 UTC 2024,,,,,,,,,,"0|z1pk8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/24 14:15;roman;[~yingz] , could you clarify the use case requiring setting state.checkpoints.num-retained to 1500?

Most of the checkpoint data should already be stored on the (distributed) file system and etcd/zk is only used to store the pointer; so the load on etcd/zk shouldn't be high in normal cases.;;;","02/Jun/24 02:12;yingz;[~roman] 
My usage scenario is as follows:
The checkpoints of the task are stored in $state.checkpoints.dir, and the number of them is determined by $state.checkpoints.num-retained.
For example, if checkpoint interval is 5 minutes, and you want to recover from a checkpoint that was made 3 days ago when restarting the task, you would need to save 60/5*24*3 = 864 checkpoints.
If the checkpoint frequency is higher, or if you need to recover from an earlier time, state.checkpoints.num-retained will need to be set larger.;;;","03/Jun/24 17:35;liuml07;When would you need to restore from checkpoints 3 days old? If that is for planed manual operation, have you considered savepoints? In my day job, we enable daily savepoints automatically as a cron job in the control plane.;;;","04/Jun/24 01:03;yingz;Yes, I understand that using cron + savepoint can partially solve the problem. 
However, we need to recover the tasks from any point in the past three days, and maybe increasing the frequency of cron is also feasible. 
But I don't understand why we can't just use checkpointing to automatically achieve this?;;;","04/Jun/24 17:33;liuml07;I guess in my day job I don't see user requests that need to recover from any point in the past days. I think it works just fine to recover from recent checkpoints in the past days. And compressing is a good improvement as data is getting large.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Use common thread pools when transferring RocksDB state files,FLINK-35501,13581200,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,roman,roman,roman,31/May/24 18:23,04/Jun/24 15:11,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,"Currently, each RocksDB state backend creates an executor backed by a thread pool.

This makes it difficult to control the total number of threads per TM because it might have at least one task per slot and theoretically, many state backends per task (because of chaining).

Additionally, using a common thread pool allows to indirectly control the load on the underlying DFS (e.g. the total number of requests to S3 from a TM).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-31 18:23:34.0,,,,,,,,,,"0|z1pjvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamoDb Table API Sink fails to delete elements due to key not found,FLINK-35500,13581188,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chalixar,chalixar,31/May/24 15:01,03/Jun/24 08:22,04/Jun/24 20:40,,aws-connector-4.0.0,aws-connector-4.1.0,aws-connector-4.2.0,,,,,,aws-connector-4.4.0,,,Connectors / DynamoDB,,,,0,,,"h2. Description
When DynamoDbSink is used with CDC sources, it fails to process {{DELETE}} records and throws 
{quote}org.apache.flink.connector.dynamodb.shaded.software.amazon.awssdk.services.dynamodb.model.DynamoDbException: The provided key element does not match the schema{quote}

This is due to {{DynamoDbSinkWriter}} passing the whole DynamoDb Item as key instead of the constructed primary Key[1].

Note: The issue is reported in user mailing list[2]

h2. Steps to Reproduce

(1) Create a new DynamoDB table in AWS.  Command line:
aws dynamodb create-table \
  --table-name orders \
  --attribute-definitions AttributeName=userId,AttributeType=S \
  --key-schema AttributeName=userId,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST

(2) Create an input file in Debezium-JSON format with the following rows to start:
{""op"": ""c"", ""after"": {""orderId"": 1, ""userId"": ""a"", ""price"": 5}}
{""op"": ""c"", ""after"": {""orderId"": 2, ""userId"": ""b"", ""price"": 7}}
{""op"": ""c"", ""after"": {""orderId"": 3, ""userId"": ""c"", ""price"": 9}}
{""op"": ""c"", ""after"": {""orderId"": 4, ""userId"": ""a"", ""price"": 11}}

(3) Start the Flink SQL Client, and run the following, substituting in the proper local paths for the Dynamo Connector JAR file and for this local sample input file:

ADD JAR '/Users/robg/Downloads/flink-sql-connector-dynamodb-4.2.0-1.18.jar';
SET 'execution.runtime-mode' = 'streaming';
SET 'sql-client.execution.result-mode' = 'changelog';

CREATE TABLE Orders_CDC(
  orderId BIGINT,
  price float,
  userId STRING
 ) WITH (
   'connector' = 'filesystem',
   'path' = '/path/to/input_file.jsonl',
   'format' = 'debezium-json'
 );

CREATE TABLE Orders_Dynamo (
  orderId BIGINT,
  price float,
  userId STRING,
  PRIMARY KEY (userId) NOT ENFORCED
) PARTITIONED BY ( userId )
WITH (
  'connector' = 'dynamodb',
  'table-name' = 'orders',
  'aws.region' = 'us-east-1'
);

INSERT INTO Orders_Dynamo SELECT * FROM Orders_CDC ;

(4) At this point, we will see that things currently all work properly, and these 4 rows are inserted properly to Dynamo, because they are ""Insert"" operations.   So far, so good!

(5) Now, add the following row to the input file.  This represents a deletion in Debezium format, which should then cause a Deletion on the corresponding DynamoDB table:
{""op"": ""d"", ""before"": {""orderId"": 3, ""userId"": ""c"", ""price"": 9}}

(6) Re-Run the SQL statement:
INSERT INTO Orders_Dynamo SELECT * FROM Orders_CDC ;

h3. References
1-https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/sink/DynamoDbSinkWriter.java#L267
2- https://lists.apache.org/thread/ysvctpvn6n9kn0qlf5b24gxchfg64ylf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 08:22:18 UTC 2024,,,,,,,,,,"0|z1pjsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/24 15:17;a.pilipenko;Looking at this issue, it appears that issue is in TableAPI element converter implementation.

RowDataElementConverter[1] does not have information about DDB table partition and sort keys or primary keys of table on Flink side.

This is less of an issue on DataStream API part, since default element converter only support insert (PUT) operations[2] and user can use correct mapping in custom element converter implementation.

 

One option to resolve this - to get information on table primary keys in DynamicSinkFactory and provide it to element converter used by Table API sink implementation. Assumption here is that partition and sort keys of DDB table would be mirrored as primary keys on Flink SQL side.

 

[1] - [https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/table/RowDataElementConverter.java]

[2] - [https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/sink/DynamoDbBeanElementConverter.java#L58-L64];;;","31/May/24 16:23;chalixar;[~a.pilipenko] Thanks for chiming in,
{quote}Looking at this issue, it appears that issue is in TableAPI element converter implementation.

RowDataElementConverter[1] does not have information about DDB table partition and sort keys or primary keys of table on Flink side.
{quote}
yes, I agree, however It seems that by design the {{DynamoDbWriter}} is responsible for overriding partition keys propagated from the Table API DynamicSinkFactory, I believe it would be confusing to split the responsibility for both the  element converter and the writer.

 

{quote}This is less of an issue on DataStream API part, since default element converter only support insert (PUT) operations[2] and user can use correct mapping in custom element converter implementation.\{quote}

I am not sure I follow, even if the element converter did set the primary key tag in the element, we still use the complete item as a key in delete requests from writer[2], is this behaviour normal?

1-[https://github.com/apache/flink-connector-aws/blob/c688a8545ac1001c8450e8c9c5fe8bbafa13aeba/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/sink/DynamoDbSinkWriter.java#L174]

2-[https://github.com/apache/flink-connector-aws/blob/c688a8545ac1001c8450e8c9c5fe8bbafa13aeba/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/sink/DynamoDbSinkWriter.java#L267];;;","31/May/24 16:50;a.pilipenko;{quote}
yes, I agree, however It seems that by design the DynamoDbWriter is responsible for overriding partition keys propagated from the Table API DynamicSinkFactory, I believe it would be confusing to split the responsibility for both the  element converter and the writer.
{quote}
True, however overwriteByPartitionKeys is used for request deduplication and not strictly required to only have table keys.
Regardless, I believe that ElementConverter[1] should distinguish between PUT and DELETE requests while populating items field of DynamoDbWriteRequest object, currently only type is different between these 2 cases.
To do that, converter will need to have information about table keys in order to only include required fields.

[1] - https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/table/RowDataElementConverter.java#L48-L72;;;","31/May/24 18:41;chalixar;Thanks for clarifying;;;","31/May/24 20:04;robg;With regard to determining the proper Primary Key fields to pass in the DynamoDB Delete request, I had initially considered that we could reuse the existing ""overwriteByPartitionKeys"" value from the Table configuration.  That said, I could see how using that ""overwriteByPartitionKeys"", which has a very specific documented meaning as ""Used to deduplicate write requests within each batch pushed to DynamoDB"", would be confusing if we were using it for a different purpose now in determining the PK for DynamoDB Delete requests.

[~a.pilipenko]'s proposal that we'd instead:
{quote}get information on table primary keys in DynamicSinkFactory and provide it to element converter used by Table API sink implementation. Assumption here is that partition and sort keys of DDB table would be mirrored as primary keys on Flink SQL side.
{quote}
does make sense in that regard - We could document that by specifying PRIMARY KEY(s) on the Flink SQL Table, they will be used when making DELETE requests to DynamoDB.    Although now we'd have two places you'd need to specify the Primary Key fields - potentially in a PRIMARY KEY clause and also in the PARTITIONED BY clause.    Though they'd have different uses and you wouldn't always need to specify both, depending on your use case.   What are your thoughts on that?

 

On how we could make DELETEs work properly once we do know the Primary Key fields:

[~chalixar] - You mentioned:
{quote}I am not sure I follow, even if the element converter did set the primary key tag in the element, we still use the complete item as a key in delete requests from writer[2], is this behaviour normal?
{quote}
I think there are two possible places we could make the fix, and I list this below as Option 1 -

*Option 1* - DynamoDBWriteRequest [1] is a public interface exposed by this DynamoDB connector, and is the class that users must map their DataStream records into when using a Custom Element Converter.  DynamoDBWriteRequest currently only has the concepts of an ""Item"" and a ""Type"" ,and no explicit Key setters/getters.   So we'd adjust the DynamoDBWriteRequest to have the concept of the Key.   And then In the DynamoDbSinkWriter, we could adjust how DELETE requests are created, so that instead of doing:

{{DeleteRequest.builder().key(dynamoDbWriteRequest.getItem()).build())}}

It would instead do:

{{DeleteRequest.builder().key(dynamoDbWriteRequest.getKey()).build())}}

This would create a new responsibility for anyone implementing a Custom Element Converter to be sure to set this Key field (using a new setKey method we'd add to DynamoDBWriteRequest), which could also be a breaking change for anyone not setting it and expecting DELETE operations to work properly based on the Item itself.

 

*Option 2* - We could adjust the DELETE logic in RowDataElementConverter [3] to only include the Primary Key fields in the DynamoDbWriteRequest when calling setItem(), rather than including all fields with GetItem as is currently done here [4].   The good thing with this approach is that we do not introduce any breaking change or new requirement to call setKey() when constructing a DynamoDBWriteRequest with a Custom Element Converter.

I think Option 2 seems cleaner and simpler, and keeps the surface of this change limited to just the RowDataElementConverter, but am open to thoughts or other options here too!

 

1 - [https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/sink/DynamoDbWriteRequest.java]

2 - [https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/sink/DynamoDbSinkWriter.java#L267]

3 - [https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/table/RowDataElementConverter.java#L63-L65]

4 - [https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/table/RowDataElementConverter.java#L54-L56]

 

 ;;;","31/May/24 22:37;chalixar;[~robg] yes , given the DELETE requests is mostly served by Table API case, it seems more suitable to go with option 2 by implementing the change in {{RowDataElementConverter}}. Are you planning to work on this?;;;","01/Jun/24 02:10;robg;As a first time contributor to this project, I might need a little guidance, but I'd be happy to give it a shot!  I do need a decision around where to pull the Primary Key from first though as well.

Thoughts on just reusing the `overwritebyPrimaryKey` field that is specified in the SQL DDL ""PARTITIONED BY"" statement for this purpose? ;;;","01/Jun/24 02:24;robg;I'll also admit that I haven't written Java (professionally) in about 20 years (Yes, back when it was called J2SE!)   That said, most of this code seems pretty intuitive, I'd just probably need some pointers around the style guidelines and perhaps in test configuration.;;;","03/Jun/24 08:22;a.pilipenko;{quote}
 I do need a decision around where to pull the Primary Key from first though as well.
{quote}
This information is available in DynamoTableFactory.Context - you can get it in DynamoDbDynamicSinkFactory:
{code:java}
    catalogTable.getResolvedSchema().getPrimaryKey()
{code}

{quote}
As a first time contributor to this project, I might need a little guidance, but I'd be happy to give it a shot!
{quote}
Feel free to reach out to me on Slack, will be happy to help.
;;;",,,,,,,,,,,,,,,,,,,,
EventTimeWindowCheckpointingITCase times out due to Checkpoint expired before completing,FLINK-35499,13581174,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,31/May/24 13:19,31/May/24 13:19,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,test-stability,,"* 1.20 AdaptiveScheduler / Test (module: tests) https://github.com/apache/flink/actions/runs/9311892945/job/25632037990#step:10:8702
* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9275522134/job/25520829730#step:10:8264

Going into the logs, we see the following error occurs:
{code:java}
================================================================================
Test testTumblingTimeWindow[statebackend type =ROCKSDB_INCREMENTAL, buffersPerChannel = 2](org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase) is running.
--------------------------------------------------------------------------------
<...>
20:24:23,562 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 22 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1716927863562 for job 15d0a663cb415b09b9a68ccc40640c6d.
20:24:23,609 [jobmanager-io-thread-2] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 22 for job 15d0a663cb415b09b9a68ccc40640c6d (2349132 bytes, checkpointDuration=43 ms, finalizationTime=4 ms).
20:24:23,610 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 23 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1716927863610 for job 15d0a663cb415b09b9a68ccc40640c6d.
20:24:23,620 [jobmanager-io-thread-2] WARN  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Error while processing AcknowledgeCheckpoint message
java.lang.IllegalStateException: Attempt to reference unknown state: a9a90973-4ee5-384f-acef-58a7c7560920
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-core-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.state.SharedStateRegistryImpl.registerReference(SharedStateRegistryImpl.java:97) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.state.SharedStateRegistry.registerReference(SharedStateRegistry.java:53) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.registerSharedStates(IncrementalRemoteKeyedStateHandle.java:289) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.registerSharedState(OperatorSubtaskState.java:243) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.OperatorSubtaskState.registerSharedStates(OperatorSubtaskState.java:226) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.TaskStateSnapshot.registerSharedStates(TaskStateSnapshot.java:193) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1245) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$2(ExecutionGraphHandler.java:109) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$4(ExecutionGraphHandler.java:139) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.util.MdcUtils.lambda$wrapRunnable$1(MdcUtils.java:64) ~[flink-core-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_392]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_392]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_392]
20:24:23,663 [Source: Custom Source (1/1)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source (1/1)#1 (bc4de0d149fba0ca825771ff7eeae08d_bc764cd8ddf7a0cff126f51c16239658_0_1) switched from RUNNING to FINISHED.
20:24:23,663 [Source: Custom Source (1/1)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Custom Source (1/1)#1 (bc4de0d149fba0ca825771ff7eeae08d_bc764cd8ddf7a0cff126f51c16239658_0_1).
20:24:23,663 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Custom Source (1/1)#1 bc4de0d149fba0ca825771ff7eeae08d_bc764cd8ddf7a0cff126f51c16239658_0_1.
20:24:23,663 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (1/1) (bc4de0d149fba0ca825771ff7eeae08d_bc764cd8ddf7a0cff126f51c16239658_0_1) switched from RUNNING to FINISHED.
20:24:23,663 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Triggering a manual checkpoint for job 15d0a663cb415b09b9a68ccc40640c6d.
20:24:23,664 [TumblingEventTimeWindows (3/4)#1] INFO  org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend [] - Closed RocksDB State Backend. Cleaning up RocksDB working directory /tmp/junit4600599408056781950/junit4118530363439384731/job_15d0a663cb415b09b9a68ccc40640c6d_op_WindowOperator_0a448493b4782967b150582570326227__3_4__uuid_f9b935a1-38cc-4cb3-92f0-97bf257d499d.
20:24:23,665 [TumblingEventTimeWindows (3/4)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - TumblingEventTimeWindows (3/4)#1 (bc4de0d149fba0ca825771ff7eeae08d_0a448493b4782967b150582570326227_2_1) switched from RUNNING to FINISHED.
20:24:23,665 [TumblingEventTimeWindows (3/4)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for TumblingEventTimeWindows (3/4)#1 (bc4de0d149fba0ca825771ff7eeae08d_0a448493b4782967b150582570326227_2_1).
20:24:23,665 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task TumblingEventTimeWindows (3/4)#1 bc4de0d149fba0ca825771ff7eeae08d_0a448493b4782967b150582570326227_2_1.
20:24:23,665 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - TumblingEventTimeWindows (3/4) (bc4de0d149fba0ca825771ff7eeae08d_0a448493b4782967b150582570326227_2_1) switched from RUNNING to FINISHED.
20:24:23,665 [flink-pekko.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager [] - Received resource requirements from job 15d0a663cb415b09b9a68ccc40640c6d: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
20:26:01,189 [flink-pekko.actor.default-dispatcher-16] INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [27b0503b2930b17041ffd3635dfb819f].
20:26:01,190 [flink-pekko.actor.default-dispatcher-16] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:3, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=512.000gb (549755813888 bytes), taskOffHeapMemory=512.000gb (549755813888 bytes), managedMemory=64.000mb (67108864 bytes), networkMemory=32.000mb (33554432 bytes)}, allocationId: 27b0503b2930b17041ffd3635dfb819f, jobId: 15d0a663cb415b09b9a68ccc40640c6d).
20:26:01,205 [flink-pekko.actor.default-dispatcher-16] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer [] - Freeing slot 27b0503b2930b17041ffd3635dfb819f.
20:34:23,610 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint 23 of job 15d0a663cb415b09b9a68ccc40640c6d expired before completing.
20:34:23,611 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 23 for job 15d0a663cb415b09b9a68ccc40640c6d. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2346) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.util.MdcUtils.lambda$wrapRunnable$1(MdcUtils.java:64) ~[flink-core-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_392]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_392]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_392]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_392]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_392]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_392]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_392]
20:34:23,611 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointRequestDecider [] - checkpoint request time in queue: 599948
20:34:23,612 [flink-pekko.actor.default-dispatcher-130] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Trying to recover from a global failure.
org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold. The latest checkpoint failed due to Checkpoint expired before completing., view the Checkpoint History tab or the Job Manager log to find out why continuous checkpoints failed.
	at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(CheckpointFailureManager.java:212) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleJobLevelCheckpointException(CheckpointFailureManager.java:169) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:122) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2281) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2260) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.access$1200(CheckpointCoordinator.java:102) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2346) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.util.MdcUtils.lambda$wrapRunnable$1(MdcUtils.java:64) ~[flink-core-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_392]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_392]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_392]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_392]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_392]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_392]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_392]
20:34:23,613 [flink-pekko.actor.default-dispatcher-130] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Tumbling Window Test (15d0a663cb415b09b9a68ccc40640c6d) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:219) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailureAndReport(ExecutionFailureHandler.java:166) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.getGlobalFailureHandlingResult(ExecutionFailureHandler.java:140) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleGlobalFailure(DefaultScheduler.java:324) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyGlobalFailure(UpdateSchedulerNgOnInternalFailuresListener.java:57) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.failGlobal(DefaultExecutionGraph.java:1092) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph$1.lambda$failJob$0(DefaultExecutionGraph.java:477) ~[flink-runtime-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:460) ~[flink-rpc-akka44fe192f-1cb4-4526-9c09-a5d6b8d70d87.jar:1.20-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-core-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
<...> 
{code}
Notably: {{org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold. The latest checkpoint failed due to Checkpoint expired before completing., view the Checkpoint History tab or the Job Manager log to find out why continuous checkpoints failed.}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-31 13:19:17.0,,,,,,,,,,"0|z1pjps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected argument name conflict error when do extract method params from udf,FLINK-35498,13581154,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,lincoln.86xy,lincoln.86xy,31/May/24 11:00,02/Jun/24 15:17,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Table SQL / Planner,,,,0,,,"Follow the steps to reproduce the error:

test case:
{code:java}
util.addTemporarySystemFunction(""myudf"", new TestXyz)
util.tableEnv.explainSql(""select myudf(f1, f2) from t"")
{code}
 

udf: TestXyz 
{code:java}
public class TestXyz extends ScalarFunction {
public String eval(String s1, String s2) {

// will not fail if add initialization
String localV1;

if (s1 == null) {
if (s2 != null) {
localV1 = s2;
} else {
localV1 = s2 + s1;
}
} else {
if (""xx"".equals(s2)) {
localV1 = s1.length() >= s2.length() ? s1 : s2;
} else {
localV1 = s1;
}
}
if (s1 == null) {
return s2 + localV1;
}
if (s2 == null) {
return s1;
}
return s1.length() >= s2.length() ? s1 + localV1 : s2;
}
}
{code}
 

error stack:
{code:java}
Caused by: org.apache.flink.table.api.ValidationException: Unable to extract a type inference from method:
public java.lang.String org.apache.flink.table.planner.runtime.utils.TestXyz.eval(java.lang.String,java.lang.String)
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:362)
    at org.apache.flink.table.types.extraction.BaseMappingExtractor.extractResultMappings(BaseMappingExtractor.java:154)
    at org.apache.flink.table.types.extraction.BaseMappingExtractor.extractOutputMapping(BaseMappingExtractor.java:100)
    ... 53 more
Caused by: org.apache.flink.table.api.ValidationException: Argument name conflict, there are at least two argument names that are the same.
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:362)
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:357)
    at org.apache.flink.table.types.extraction.FunctionSignatureTemplate.of(FunctionSignatureTemplate.java:73)
    at org.apache.flink.table.types.extraction.BaseMappingExtractor.lambda$createParameterSignatureExtraction$9(BaseMappingExtractor.java:381)
    at org.apache.flink.table.types.extraction.BaseMappingExtractor.putExtractedResultMappings(BaseMappingExtractor.java:298)
    at org.apache.flink.table.types.extraction.BaseMappingExtractor.collectMethodMappings(BaseMappingExtractor.java:244)
    at org.apache.flink.table.types.extraction.BaseMappingExtractor.extractResultMappings(BaseMappingExtractor.java:137)
    ... 54 more

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/24 15:09;lincoln.86xy;image-2024-06-02-23-09-17-768.png;https://issues.apache.org/jira/secure/attachment/13069253/image-2024-06-02-23-09-17-768.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 02 15:17:45 UTC 2024,,,,,,,,,,"0|z1pjlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/24 15:17;lincoln.86xy;Add some inputs: 
For the test class TestXyz, if the local variable `localV1` without an initialization when declare, the `LocalVariableTable` of the method may include several lines for it, as the following code example and class files' snippet comparison 

{code}
// will not fail if add initializationString localV1;

// with initialization
String localV1 = """";
{code}

 

!image-2024-06-02-23-09-17-768.png!

this will cause different analysis results via asm tool, and we should consider these duplicate vars when extract the method params.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The wrong enum value was used to get month in timestampDiff,FLINK-35497,13581142,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,haishui,haishui,31/May/24 09:10,31/May/24 09:10,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,,,"In [SystemFunctionUtils#timestampDiff](https://github.com/apache/flink-cdc/blob/master/flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/functions/SystemFunctionUtils.java#L125): 

{code:java}
case ""MONTH"":
    return to.get(Calendar.YEAR) * 12
            + to.get(Calendar.MONDAY)
            - (from.get(Calendar.YEAR) * 12 + from.get(Calendar.MONDAY)); {code}
The Calendar.MONDAY can be replaced with Calendar.MONTH.

This does not affect the calculation results, because Calendar.MONDAY = Calendar.MONTH = 2.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-31 09:10:05.0,,,,,,,,,,"0|z1pjio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The annotations of the new JDBC connector should be changed to non-Public/non-PublicEvolving,FLINK-35496,13581140,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,eskabetxe,RocMarshal,RocMarshal,31/May/24 08:48,04/Jun/24 05:27,04/Jun/24 20:40,03/Jun/24 02:00,,,,,,,,,jdbc-3.2.0,,,Connectors / JDBC,,,,0,pull-request-available,,"In general, we use the Experimental annotation instead of {{PublicEvolving}}  or {{Public}}  for new features or new apis. But  {{JdbcSink}} and JdbcSource(merged ) was marked as {{PublicEvolving}}  in the first version. [~fanrui]  commented it to the original PR[1].[1] [https://github.com/apache/flink-connector-jdbc/pull/2#discussion_r1621857589]

CC [~eskabetxe] [~Sergey Nuyanzin] [~fanrui] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 05:27:22 UTC 2024,,,,,,,,,,"0|z1pji8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/24 01:59;fanrui;Merged to main(jdbc-3.2.0) via: b955e8b5479c65512f94ee7270eb2868a4a14ebe;;;","03/Jun/24 14:43;leonard;I just want to clarify two points here: 
（1）We can mark class as PublicEvolving  in the first version of Java Class/Interface (usually we called public interfaces) if we discussed it in dev mailing list and be approved via FLIP vote. 
  (2)  Experimental annotation is used for experimental feature, Using PublicEvolving is also okay when a connector functionality migrate from legacy API implementation to new API implementation,  because the unified Sink API is Public instead of Experimental. ;;;","04/Jun/24 02:01;fanrui;Thanks [~leonard] for the clarification, I misunderstood it before! It's clear for now.

I commented it at [https://github.com/apache/flink-connector-jdbc/pull/2#discussion_r1621857589]  due to I checked the FileSink[1] in flink repo, FileSink is marked as @Experimental since the first version.

And sorry for merging this Jira quickly. I discussed with [~eskabetxe] (Contributor of new jdbc sink), [~Sergey Nuyanzin] (Reviewer of new jdbc sink) and [~RocMarshal] (Contributor of new jdbc source) in Slack. All of them agree that marking new jdbc sink and source to Experimental. And in order not to block the jdbc connector 3.2.0 release, I merged it yesterday.

Next time I will merge the PR after hearing more confirmation. Sorry for that again.

 

IIUC, based on the current conclusions, we need to do the following steps:
 * [~RocMarshal] needs to cancel the -1 vote in the release mail [2]
 * We need to revert the b955e8b5479c65512f94ee7270eb2868a4a14ebe (The commit of current JIRA)
 * Also, we need to update the FIx version of current Jira from jdbc-3.2.0 to jdbc-3.3.0 due to all commits of current Jira won't be released in  jdbc-3.2.0

Please correct me if anything is wrong, and we can go ahead after [~leonard] 's double check, thank you again.:)

 

BTW, should we update the Experimental to PublicEvolving for FileSink as well[2]?

 

[1] [https://github.com/apache/flink/blob/e4fa72d9e480664656818395741c37a9995f9334/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/sink/FileSink.java#L132]

[2] [https://lists.apache.org/thread/b7xbjo4crt1527ldksw4nkwo8vs56csy];;;","04/Jun/24 03:40;leonard;> I commented it at https://github.com/apache/flink-connector-jdbc/pull/2#discussion_r1621857589  due to I checked the FileSink[1] in flink repo, FileSink is marked as @Experimental since the first version.

The Experimental annotation for FileSink was added when UnifiedSink was also annotated as Experimental
https://github.com/apache/flink/pull/13576/files#diff-8ba42ec08c6cd57a4aa2dd4da5660919896fd06c9c421ca1b2b32b2ebb8b60b3

>IIUC, based on the current conclusions, we need to do the following steps:
> RocMarshal needs to cancel the -1 vote in the release mail [2]
> We need to revert the b955e8b5479c65512f94ee7270eb2868a4a14ebe (The commit of current JIRA)
>Also, we need to update the FIx version of current Jira from jdbc-3.2.0 to jdbc-3.3.0 due to all commits of current Jira won't be released in  jdbc-3.2.0

Yes, it makes sense to me.

> BTW, should we update the Experimental to PublicEvolving for FileSink as well[2]?

+1 for this as Unified Sink has been changed to PublicEvolving
;;;","04/Jun/24 03:51;RocMarshal;Thanks [~fanrui] for the checking and [~leonard] for the clarify.

> We need to revert the b955e8b5479c65512f94ee7270eb2868a4a14ebe (The commit of current JIRA)

The revert of the PR is driving at https://github.com/apache/flink-connector-jdbc/pull/129;;;","04/Jun/24 05:19;RocMarshal;>> BTW, should we update the Experimental to PublicEvolving for FileSink as well[2]?

>+1 for this as Unified Sink has been changed to PublicEvolving

I created a Jira https://issues.apache.org/jira/browse/FLINK-35516 to track it.;;;","04/Jun/24 05:27;RocMarshal;- I carefully read the discussion and found that our Rules do not strictly require the use of Experimental annotations on the connector API. However, during the review, it was recommended not to use such annotations on the connector API.
- I checked and found that only the generate connector and file connector used Experimental annotations in the connectors module of the Flink main repository. And it was developed a long time ago.


*So, can we assume that there is ambiguity in the use of annotations for connectors?* 

*Should we change the Rules for connectors so that developers and reviewers could strictly adhere to this unified rules?*
[~leonard] I'd be appreciated with your confirmation.;;;",,,,,,,,,,,,,,,,,,,,,,
The native metrics for column family are not reported,FLINK-35495,13581089,13574085,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Yanfei Lei,Yanfei Lei,31/May/24 03:02,31/May/24 03:02,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-31 03:02:00.0,,,,,,,,,,"0|z1pj6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reorganize sources,FLINK-35494,13581043,13579395,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,eskabetxe,eskabetxe,eskabetxe,30/May/24 16:08,31/May/24 08:58,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-30 16:08:39.0,,,,,,,,,,"0|z1piww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make max history age and count configurable for FlinkStateSnapshot resources,FLINK-35493,13581010,13577544,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mateczagany,mateczagany,30/May/24 12:25,30/May/24 12:25,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-30 12:25:44.0,,,,,,,,,,"0|z1pipk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics for FlinkStateSnapshot resources,FLINK-35492,13581009,13577544,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mateczagany,mateczagany,30/May/24 12:24,30/May/24 12:24,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-30 12:24:47.0,,,,,,,,,,"0|z1pipc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: Flink CDC modules,FLINK-35491,13580993,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,m.orazow,m.orazow,30/May/24 09:45,30/May/24 09:45,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"Migrate Junit4 tests to Junit5 for the following modules:
 * flink-cdc-common
 * flink-cdc-composer
 * flink-cdc-runtime
 * flink-cdc-connect/flink-cdc-pipeline-connectors
 * flink-cdc-e2e-tests",,,,,,,,,,,,,,FLINK-34585,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-30 09:45:20.0,,,,,,,,,,"0|z1pils:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: Flink CDC flink-cdc-connect/flink-cdc-source-connectors,FLINK-35490,13580992,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,m.orazow,m.orazow,30/May/24 09:41,30/May/24 09:43,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"Migrate Junit4 tests to Junit5 in the Flink CDC following modules:

 

- flink-cdc-connect/flink-cdc-source-connectors",,,,,,,,,,,,,,FLINK-34585,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 09:43:33 UTC 2024,,,,,,,,,,"0|z1pilk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 09:43;m.orazow;I would be happy to work on this, please assign it to me, thanks (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metaspace size can be too little after autotuning change memory setting,FLINK-35489,13580978,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nfraison.datadog,nfraison.datadog,30/May/24 07:53,03/Jun/24 14:37,04/Jun/24 20:40,,1.8.0,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,"We have enable the autotuning feature on one of our flink job with below config
{code:java}
# Autoscaler configuration
job.autoscaler.enabled: ""true""
job.autoscaler.stabilization.interval: 1m
job.autoscaler.metrics.window: 10m
job.autoscaler.target.utilization: ""0.8""
job.autoscaler.target.utilization.boundary: ""0.1""
job.autoscaler.restart.time: 2m
job.autoscaler.catch-up.duration: 10m
job.autoscaler.memory.tuning.enabled: true
job.autoscaler.memory.tuning.overhead: 0.5
job.autoscaler.memory.tuning.maximize-managed-memory: true{code}
During a scale down the autotuning decided to give all the memory to to JVM (having heap being scale by 2) settting taskmanager.memory.managed.size to 0b.
Here is the config that was compute by the autotuning for a TM running on a 4GB pod:
{code:java}
    taskmanager.memory.network.max: 4063232b
    taskmanager.memory.network.min: 4063232b
    taskmanager.memory.jvm-overhead.max: 433791712b
    taskmanager.memory.task.heap.size: 3699934605b
    taskmanager.memory.framework.off-heap.size: 134217728b
    taskmanager.memory.jvm-metaspace.size: 22960020b
    taskmanager.memory.framework.heap.size: ""0 bytes""
    taskmanager.memory.flink.size: 3838215565b
    taskmanager.memory.managed.size: 0b {code}
This has lead to some issue starting the TM because we are relying on some javaagent performing some memory allocation outside of the JVM (rely on some C bindings).

Tuning the overhead or disabling the scale-down-compensation.enabled could have helped for that particular event but this can leads to other issue as it could leads to too little HEAP size being computed.

It would be interesting to be able to set a min memory.managed.size to be taken in account by the autotuning.
What do you think about this? Do you think that some other specific config should have been applied to avoid this issue?

 

Edit see this comment that leads to the metaspace issue: https://issues.apache.org/jira/browse/FLINK-35489?focusedCommentId=17850694&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17850694",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 07:52:59 UTC 2024,,,,,,,,,,"0|z1piig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 08:37;fanrui;Hi [~nfraison.datadog]
{quote}because we are relying on some javaagent performing some memory allocation outside of the JVM (rely on some C bindings).
{quote}
Do you mean your job need some native memory and you want to increase the taskmanager.memory.managed.size to reserve some native memory to avoid the TM total memory usage excesses the memory total limitation?

If yes (IIUC), the flink managed memory[1] is mainly used by RocksDB State backend in streaming jobs. You can increase the Flink JVM overhead memory if the native memory is not enough. You can check the Flink TM memory model here[2].

It means you can try to increase these options:
 * taskmanager.memory.jvm-overhead.min [3]
 * taskmanager.memory.jvm-overhead.max [4]
 * taskmanager.memory.jvm-overhead.fraction [5]

Feel free to correct me if anything is wrong, thanks.:)

 

[1][https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#managed-memory]

[2][https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#detailed-memory-model]

[3]https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#taskmanager-memory-jvm-overhead-min

[4] https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#taskmanager-memory-jvm-overhead-max

[5]https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#taskmanager-memory-jvm-overhead-fraction;;;","30/May/24 12:02;nfraison.datadog;Thks [~fanrui] for the feedback it help me realise that my analysis was wrong.

The issue we are facing is the JVM crashing after the [autotuning|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/autotuning/] change some memory config:
{code:java}
Starting kubernetes-taskmanager as a console application on host flink-kafka-job-apache-right-taskmanager-1-1.
Exception in thread ""main"" *** java.lang.instrument ASSERTION FAILED ***: ""result"" with message agent load/premain call failed at src/java.instrument/share/native/libinstrument/JPLISAgent.c line: 422
FATAL ERROR in native method: processing of -javaagent failed, processJavaStart failed
Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.so+0x78dee4]  jni_FatalError+0x70
V  [libjvm.so+0x88df00]  JvmtiExport::post_vm_initialized()+0x240
V  [libjvm.so+0xc353fc]  Threads::create_vm(JavaVMInitArgs*, bool*)+0x7ac
V  [libjvm.so+0x79c05c]  JNI_CreateJavaVM+0x7c
C  [libjli.so+0x3b2c]  JavaMain+0x7c
C  [libjli.so+0x7fdc]  ThreadJavaMain+0xc
C  [libpthread.so.0+0x7624]  start_thread+0x184 {code}
Seeing this big increase of HEAP (from 1.5 to more than 3GB and the fact that the memory.managed.size was shrink to 0b make me thing that it was linked to missing off heap.

But you are right that jvm-overhead already reserved some memory for the off heap (and we indeed have around 400 MB with that config)

So looking back to the new config I've identified the issue which is on the jvm-metaspace having been shrink to 22MB while it was set at 256MB.
I've done a test increasing this parameter and the TM is now able to start.

For the meta space computation size I can see the autotuning computing METASPACE_MEMORY_USED=1.41521584E8 which seems to be appropriate metaspace sizing.

But due to the the memBudget management it ends up setting only 22MB to the metaspace ([first allocate remaining memory to the heap and then this new remaining to metaspace and finally to managed memory|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-autoscaler/src/main/java/org/apache/flink/autoscaler/tuning/MemoryTuning.java#L130])

 ;;;","31/May/24 07:52;nfraison.datadog;[~mxm] what do you think about providing memory to the METASPACE_MEMORY before the HEAP_MEMORY (switching those lines [https://github.com/apache/flink-kubernetes-operator/blob/main/flink-autoscaler/src/main/java/org/apache/flink/autoscaler/tuning/MemoryTuning.java#L128-L131)]
And also ensuring that the METASPACE_MEMORY computed will never be bigger than the one assigned by default (from config taskmanager.memory.jvm-metaspace.size)

Looks to me that this space should not grow with some load change and the default one must be sufficiently big to have the TaskManager running fine. The autotuning should only scale down this memory space depending on the usage, no?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
DataType Support Geometry Type,FLINK-35488,13580967,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leopold_xlp,leopold_xlp,30/May/24 05:40,30/May/24 05:40,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,,,"     I want sync data from mysql to postgresql,but  in  Geometry Datatype filed i couldn't do it,mysql geometry datatype data can be transformed to string by spatialfuntion ,for example.st_astext(geom) .In other way,postgresql geometry datatype data also transformed to string .

     So,i hope Flink suport mysql  and postgrdql databse geometry datatype can be transform.

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-30 05:40:45.0,,,,,,,,,,"0|z1pig0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContinuousFileProcessingCheckpointITCase crashed as process exit with code 127,FLINK-35487,13580965,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,30/May/24 04:39,30/May/24 04:41,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"{code:java}
Process Exit Code: 127
May 29 16:46:04 16:46:04.495 [ERROR] Crashed tests:
May 29 16:46:04 16:46:04.495 [ERROR] org.apache.flink.test.checkpointing.ContinuousFileProcessingCheckpointITCase
May 29 16:46:04 16:46:04.495 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:643)
May 29 16:46:04 16:46:04.495 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.lambda$runSuitesForkPerTestSet$7(ForkStarter.java:401)
May 29 16:46:04 16:46:04.495 [ERROR] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
May 29 16:46:04 16:46:04.495 [ERROR] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
May 29 16:46:04 16:46:04.495 [ERROR] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
May 29 16:46:04 16:46:04.495 [ERROR] 	at java.lang.Thread.run(Thread.java:748)
May 29 16:46:04 16:46:04.495 [ERROR] -> [Help 1]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59952&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9477",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-30 04:39:52.0,,,,,,,,,,"0|z1pifk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential sql expression generation issues on SQL gateway,FLINK-35486,13580957,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xccui,xccui,30/May/24 02:28,30/May/24 02:28,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / Gateway,Table SQL / Planner,,,0,,,"We hit the following exceptions a few times when submitting queries to a session cluster with the Flink SQL gateway. When the same queries were submitted again, everything was good. There might be a concurrency problem for the expression generator.
{code:java}
""process.thread.name"":""sql-gateway-operation-pool-thread-111"",""log.logger"":""org.apache.flink.table.gateway.service.operation.OperationManager"",""error.type"":""org.apache.flink.table.planner.codegen.CodeGenException"",""error.message"":""Mismatch of expected output data type 'ARRAY<ROW<`key` BIGINT, `value` BIGINT> NOT NULL>' and function's output type 'ARRAY<ROW<`key` BIGINT, `value` FLOAT> NOT NULL>'."",""error.stack_trace"":""org.apache.flink.table.planner.codegen.CodeGenException: Mismatch of expected output data type 'ARRAY<ROW<`key` BIGINT, `value` BIGINT> NOT NULL>' and function's output type 'ARRAY<ROW<`key` BIGINT, `value` FLOAT> NOT NULL>'.
  at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.verifyOutputType(BridgingFunctionGenUtil.scala:369)
  at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.verifyFunctionAwareOutputType(BridgingFunctionGenUtil.scala:359)
  at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCallWithDataType(BridgingFunctionGenUtil.scala:107)
  at org.apache.flink.table.planner.codegen.calls.BridgingFunctionGenUtil$.generateFunctionAwareCall(BridgingFunctionGenUtil.scala:84)
  at org.apache.flink.table.planner.codegen.calls.BridgingSqlFunctionCallGen.generate(BridgingSqlFunctionCallGen.scala:79)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateCallExpression(ExprCodeGenerator.scala:820)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:481)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:56)
  at org.apache.calcite.rex.RexCall.accept(RexCall.java:189)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.$anonfun$visitCall$1(ExprCodeGenerator.scala:478)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at scala.collection.TraversableLike.map(TraversableLike.scala:233)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:469)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:56)
  at org.apache.calcite.rex.RexCall.accept(RexCall.java:189)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.$anonfun$visitCall$1(ExprCodeGenerator.scala:478)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at scala.collection.TraversableLike.map(TraversableLike.scala:233)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:469)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.visitCall(ExprCodeGenerator.scala:56)
  at org.apache.calcite.rex.RexCall.accept(RexCall.java:189)
  at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateExpression(ExprCodeGenerator.scala:134)
  at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.$anonfun$generateProcessCode$4(CalcCodeGenerator.scala:140)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at scala.collection.TraversableLike.map(TraversableLike.scala:233)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.produceProjectionCode$1(CalcCodeGenerator.scala:140)
  at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateProcessCode(CalcCodeGenerator.scala:185)
  at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:49)
  at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala)
  at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:100)
  at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:167)
  at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:258)
  at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:73)
  at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:167)
  at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:92)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
  at scala.collection.Iterator.foreach(Iterator.scala:937)
  at scala.collection.Iterator.foreach$(Iterator.scala:937)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
  at scala.collection.IterableLike.foreach(IterableLike.scala:70)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
  at scala.collection.TraversableLike.map(TraversableLike.scala:233)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:91)
  at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:184)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862)
  at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513)
  at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426)
  at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207)
  at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
  at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
  at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)
  at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
  at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-30 02:28:13.0,,,,,,,,,,"0|z1pids:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JobMaster failed with ""the job xx has not been finished""",FLINK-35485,13580956,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xccui,xccui,30/May/24 02:18,30/May/24 06:54,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Runtime / Coordination,,,,0,,,"We ran a session cluster on K8s and used Flink SQL gateway to submit queries. Hit the following rare exception once which caused the job manager to restart.
{code:java}
org.apache.flink.util.FlinkException: JobMaster for job 50d681ae1e8170f77b4341dda6aba9bc failed.
  at org.apache.flink.runtime.dispatcher.Dispatcher.jobMasterFailed(Dispatcher.java:1454)
  at org.apache.flink.runtime.dispatcher.Dispatcher.jobManagerRunnerFailed(Dispatcher.java:776)
  at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$runJob$6(Dispatcher.java:698)
  at java.base/java.util.concurrent.CompletableFuture.uniHandle(Unknown Source)
  at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(Unknown Source)
  at java.base/java.util.concurrent.CompletableFuture$Completion.run(Unknown Source)
  at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
  at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
  at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
  at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
  at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
  at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
  at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
  at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
  at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
  at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
  at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
  at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
  at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
  at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
  at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
  at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
  at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
  at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
  at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
  at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
  at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
  at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
  at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
  at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
Caused by: org.apache.flink.runtime.jobmaster.JobNotFinishedException: The job (50d681ae1e8170f77b4341dda6aba9bc) has not been finished.
  at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.closeAsync(DefaultJobMasterServiceProcess.java:157)
  at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.stopJobMasterServiceProcess(JobMasterServiceLeadershipRunner.java:431)
  at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.callIfRunning(JobMasterServiceLeadershipRunner.java:476)
  at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$stopJobMasterServiceProcessAsync$12(JobMasterServiceLeadershipRunner.java:407)
  at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(Unknown Source)
  at java.base/java.util.concurrent.CompletableFuture.thenCompose(Unknown Source)
  at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.stopJobMasterServiceProcessAsync(JobMasterServiceLeadershipRunner.java:405)
  at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.runIfStateRunning(JobMasterServiceLeadershipRunner.java:463)
  at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.revokeLeadership(JobMasterServiceLeadershipRunner.java:397)
  at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.notifyLeaderContenderOfLeadershipLoss(DefaultLeaderElectionService.java:484)
  at java.base/java.util.HashMap.forEach(Unknown Source)
  at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onRevokeLeadershipInternal(DefaultLeaderElectionService.java:452)
  at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.lambda$runInLeaderEventThread$3(DefaultLeaderElectionService.java:549)
  at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source)
  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  at java.base/java.lang.Thread.run(Unknown Source)"" {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 06:54:54 UTC 2024,,,,,,,,,,"0|z1pidk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 06:54;mapohl;Hi [~xccui], thanks for reporting the issue. Can you provide the JobManager logs for this case? That would help getting a better understanding of what was going on.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink document file had removed but website can access,FLINK-35484,13580955,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gongzhongqiang,gongzhongqiang,gongzhongqiang,30/May/24 02:17,31/May/24 02:36,04/Jun/24 20:40,30/May/24 10:32,1.19.0,,,,,,,,1.20.0,,,Documentation,,,,0,pull-request-available,,"Flink 1.18 document had remove document about DataSet : issue link https://issues.apache.org/jira/browse/FLINK-32741.

But I can still access the link : https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/dataset/formats/avro/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 02:36:07 UTC 2024,,,,,,,,,,"0|z1pidc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 02:18;gongzhongqiang;[~Leonard] This issue I had fixed in flink cdc https://issues.apache.org/jira/browse/FLINK-35447. Please assign to me.;;;","30/May/24 03:51;leonard;Thanks [~gongzhongqiang] for taking it, assigned to you;;;","30/May/24 10:32;leonard;Fixed via master: b1841c0ff34b6e8d118139f6f87b7fdf06afacd3;;;","31/May/24 01:30;gongzhongqiang;[~leonard] After build document ci scheduled，I checked:
* The link: https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/dataset/formats/avro/ is 404 not foud 
* Document website is functioning properly.

All result are excepted.;;;","31/May/24 02:36;leonard;thanks [~gongzhongqiang] for the confirm, cool!;;;",,,,,,,,,,,,,,,,,,,,,,,,
BatchJobRecoveryTest related to JM failover produced no output for 900 second,FLINK-35483,13580954,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,Weijie Guo,Weijie Guo,30/May/24 01:52,31/May/24 15:49,04/Jun/24 20:40,31/May/24 15:49,1.20.0,,,,,,,,1.20.0,,,Build System / CI,,,,0,pull-request-available,,"testRecoverFromJMFailover

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59919&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9476",,,,,,,,,,,,,,,,,,,,,FLINK-35465,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 15:49:21 UTC 2024,,,,,,,,,,"0|z1pid4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 01:55;Weijie Guo;testRecoverFromJMFailoverAndPartitionsUnavailable

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59920&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=10949;;;","30/May/24 01:56;Weijie Guo;testRecoverDecidedParallelismFromTheSameJobGraphInstance

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59920&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9678;;;","30/May/24 01:56;Weijie Guo;testRecoverDecidedParallelismFromTheSameJobGraphInstance

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59920&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=22568;;;","30/May/24 01:59;Weijie Guo;testRecoverFromJMFailover

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59920&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=27725;;;","30/May/24 04:39;JunRuiLi;Thanks [~Weijie Guo] , I will take a look.;;;","30/May/24 05:17;Weijie Guo;[~JunRuiLi] Thanks for the quick reply, I have assigned this ticket to you.;;;","30/May/24 06:24;JunRuiLi;The issue arises from cases in BatchJobRecoveryTest where we need to wait for a batch job to finish recovering. While the current implementation cannot accurately detect this state, I will prepare a fix pr.;;;","31/May/24 01:46;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59980&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10345;;;","31/May/24 13:26;rskraba;There were a bunch of these happening in GitHub actions over the last few days:
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9315015519/job/25642330745#step:10:9437
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9314750194/job/25642307976#step:10:10001
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9314653645/job/25640759437#step:10:9994
* 1.20 Java 8 / Test (module: core) https://github.com/apache/flink/actions/runs/9311892945/job/25632036362#step:10:9445
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/9311892945/job/25632060016#step:10:21861
* 1.20 Java 17 / Test (module: core) https://github.com/apache/flink/actions/runs/9311892945/job/25632066577#step:10:17919
* 1.20 Java 21 / Test (module: core) https://github.com/apache/flink/actions/runs/9311892945/job/25632044018#step:10:26883
* 1.20 Hadoop 3.1.3 / Test (module: core) https://github.com/apache/flink/actions/runs/9311892945/job/25632095313#step:10:10097
* 1.20 AdaptiveScheduler / Test (module: core) https://github.com/apache/flink/actions/runs/9311892945/job/25632037248#step:10:9465
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9304086727/job/25608398592#step:10:10041
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9301074793/job/25598821104#step:10:9997
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9297918541/job/25589186429#step:10:10047
* 1.20 Java 8 / Test (module: core) https://github.com/apache/flink/actions/runs/9295906525/job/25583826518#step:10:9461
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/9295906525/job/25583843903#step:10:10266
* 1.20 Java 17 / Test (module: core) https://github.com/apache/flink/actions/runs/9295906525/job/25583846258#step:10:9639
* 1.20 Java 21 / Test (module: core) https://github.com/apache/flink/commit/89c89d8522f779986f3f6f163d803e5d5f11ec62/checks/25583832153/logs
* 1.20 Hadoop 3.1.3 / Test (module: core) https://github.com/apache/flink/actions/runs/9295906525/job/25583978541#step:10:9502
* 1.20 AdaptiveScheduler / Test (module: core) https://github.com/apache/flink/actions/runs/9295906525/job/25583820574#step:10:9445
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9288555905/job/25560930733#step:10:10000
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9286437552/job/25553690409#step:10:10182
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9283663961/job/25545853323#step:10:9451
* 1.20 Java 8 / Test (module: core) https://github.com/apache/flink/actions/runs/9279196848/job/25531630521#step:10:9445
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/9279196848/job/25531656817#step:10:11015
* 1.20 Java 17 / Test (module: core) https://github.com/apache/flink/actions/runs/9279196848/job/25531646405#step:10:10394
* 1.20 Java 21 / Test (module: core) https://github.com/apache/flink/actions/runs/9279196848/job/25531643205#step:10:25928
* 1.20 Hadoop 3.1.3 / Test (module: core) https://github.com/apache/flink/actions/runs/9279196848/job/25531688535#step:10:10073
* 1.20 Adaptive Scheduler / Test (module: core) https://github.com/apache/flink/actions/runs/9279196848/job/25531631759#step:10:10013
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9277267623/job/25526325719#step:10:10023
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9275522134/job/25520828361#step:10:10012
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9265488039/job/25489065093#step:10:10041
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/9262951119/job/25480817370#step:10:10032;;;","31/May/24 13:58;rskraba;* 1.20 test_cron_azure core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59984&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=10736
* 1.20 test_cron_hadoop313 core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59984&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=10358
* 1.20 test_cron_jdk11 core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59984&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=16309
* 1.20 test_cron_jdk17 core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59984&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=35600
* 1.20 test_cron_jdk21 core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59984&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=27143;;;","31/May/24 15:49;zhuzh;f3a3f926c6c6c931bb7ccc52e823d70cfd8aadf5;;;",,,,,,,,,,,,,,,,,,
ParquetInt64TimestampReaderTest unit tests fail due to system timezone,FLINK-35482,13580951,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,robyoung,robyoung,30/May/24 00:56,30/May/24 01:03,04/Jun/24 20:40,,,,,,,,,,,,,Tests,,,,0,,,"To reproduce:
{code:java}
export TZ=Pacific/Auckland
./mvnw -Dfast -DskipTests -Dskip.npm=true install -pl :flink-parquet -am
./mvnw test -pl :flink-parquet{code}
The parquet tests fail with:


{code:java}
[ERROR] Failures: 
[ERROR]   ParquetInt64TimestampReaderTest.testReadInt64TimestampMicros:46 expected:<2021-11-22T1[7]:50:20.000112> but was:<2021-11-22T1[8]:50:20.000112>
[ERROR]   ParquetInt64TimestampReaderTest.testReadInt64TimestampMillis:66 expected:<2021-11-22T1[7]:50:20> but was:<2021-11-22T1[8]:50:20>
[ERROR]   ParquetInt64TimestampReaderTest.testReadInt64TimestampNanos:78 expected:<2021-11-22T1[7]:50:20.000112233> but was:<2021-11-22T1[8]:50:20.000112233>{code}
I think this is because the tests convert a LocalDateTime to epoch seconds using `OffsetDateTime.now().getOffset()` as the offset, but now's offset is different to what it would be at 2021-11-22T17:50:20.000112 NZST due to daylight savings.

Instead of using now's offset we could convert the localDateTime to a zonedDateTime using `localDateTime.atZone(ZoneId.systemDefault())`. If you're happy with that idea please assign me and I'll make a PR.

Another possible idea would be to set the user.timezone to GMT in the base argLine to use a consistent timezone for tests, which would be picked up by surefire and IDEA. But improving the tests feels like a better solution.

Thanks

 ","on master commit 89c89d8522f779986f3f6f163d803e5d5f11ec62 (HEAD -> master, origin/master, origin/HEAD)

linux version: uname -r
6.9.1-arch1-1

checked with:

openjdk version ""21"" 2023-09-19 LTS
OpenJDK Runtime Environment Temurin-21+35 (build 21+35-LTS)

openjdk 17.0.5 2022-10-18
OpenJDK Runtime Environment Temurin-17.0.5+8 (build 17.0.5+8)

openjdk version ""11.0.21"" 2023-10-17 LTS
OpenJDK Runtime Environment Zulu11.68+17-CA (build 11.0.21+9-LTS)

openjdk version ""1.8.0_382""
OpenJDK Runtime Environment (Temurin)(build 1.8.0_382-b05)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-30 00:56:00.0,,,,,,,,,,"0|z1picg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add HISTOGRAM function in SQL & Table API,FLINK-35481,13580879,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,uce,uce,29/May/24 13:43,29/May/24 13:43,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,Table SQL / API,,,,0,,,Consider adding a HISTOGRAM aggregate function similar to ksqlDB (https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/aggregate-functions/#histogram).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-29 13:43:35.0,,,,,,,,,,"0|z1phwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add FIELD function in SQL & Table API,FLINK-35480,13580872,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,uce,uce,29/May/24 13:19,29/May/24 13:19,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,Table SQL / API,,,,0,,,"Add support for the {{FIELD}} function to return the position of {{str}} in {{{}args{}}}, or 0 if not found.

*References*
 * ksqlDB: [https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/scalar-functions/#field]
 * MySQL: https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_elt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-29 13:19:15.0,,,,,,,,,,"0|z1phuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add end-to-end test for materialized table,FLINK-35479,13580867,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hackergin,hackergin,29/May/24 12:33,29/May/24 12:33,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Gateway,Tests,,0,,,"Add end-to-end test cases related to materialized tables, including the processes of dropping, refreshing, and dropping materialized tables.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-29 12:33:55.0,,,,,,,,,,"0|z1phts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pipeline sink Unified primary.key parameter,FLINK-35478,13580781,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,29/May/24 02:43,29/May/24 02:43,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,"Customize the primary key of the sink table. For example, if multiple different area tables write to the same paimon table, the primary key must be combined to avoid duplicate primary keys",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-29 02:43:06.0,,,,,,,,,,"0|z1phao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector resets existing subscription,FLINK-35477,13580719,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mrbazzik,mrbazzik,28/May/24 16:01,28/May/24 16:01,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,Connectors / Pulsar,,,,0,,," 

The issue occurs in the following circumstances:
 * There is an existing subscription in a Pulsar topic, and it has some accumulated backlog;
 * Flink job is deployed from a clear state (no checkpoints)
 * Flink job uses the same subscription name as the existing one; the start cursor is the default one (earliest)
 * {{pulsar.source.resetSubscriptionCursor}} is not set (default: false)

 

*Expected behaviour*

Based on the docs [here|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/pulsar/#starting-position], the priority for setting up the cursor position should be: {{{}checkpoint > existed subscription position > StartCursor{}}}. So, since there are no checkpoints, the job should get the existing position from Pulsar and start reading from there.

 

*Observed behaviour*

As soon as the job is connected to the topic, the number of messages in the subscription backlog jumps to a new high, and JM logs show messages:
{code:java}
Seeking subscription to the message -1:-1:-1
Successfully reset subscription to the message -1:-1:-1 {code}
Apparently, Flink ignored the existing subscription position and reset its cursor position to the earliest in the topic.

 

*Further observations*

The related code seems to be [here|https://github.com/apache/flink-connector-pulsar/blob/b37a8b32f30683664ff25888d403c4de414043e1/flink-connector-pulsar/src/main/java/org/apache/flink/connector/pulsar/source/enumerator/PulsarSourceEnumerator.java#L223].

I believe the breaking changes were introduced in this [commit:|https://github.com/apache/flink-connector-pulsar/commit/78d00ea9e3e278d4ce2fbb0c8a8d380abef7b858#]
 * The check if the subscription already exists is removed [here|https://github.com/apache/flink-connector-pulsar/commit/78d00ea9e3e278d4ce2fbb0c8a8d380abef7b858#diff-ce7a6c1d29387077c2b19992312c0120bd16580ba5cf9bf222c718dd18a0db2aL86]
 * The check for {{isResetSubscriptionCursor()}} is removed [here|https://github.com/apache/flink-connector-pulsar/commit/78d00ea9e3e278d4ce2fbb0c8a8d380abef7b858#diff-4db00b10562cef1def73b06f0e2765a650c51954b4cf13487984204495d8a776L231].

I was able to confirm that it works as expected if I downgrade connector to {{4.0.0-1.17.}}

This issue will be blocking us from upgrading to Flink 1.18 and later versions.

 ","Flink version: 1.18.1

Pulsar connector version: 4.1.0-1.18

Pulsar version: 3.1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-28 16:01:26.0,,,,,,,,,,"0|z1pgww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The IF built-in function returns an exception when returning a string,FLINK-35476,13580664,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,lyndon.lee,lyndon.lee,28/May/24 09:06,28/May/24 13:56,04/Jun/24 20:40,,1.15.3,,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,0,,,"The IF built-in function returns an exception when returning a string

There is no explanation in the official documentation！

offlical link：

[https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/functions/systemfunctions/]

Flink version：1.15.1/1.15.3

 

CASE SQL:
{code:java}
select 
        if('asd'='asd', '1234', '12345' )  as expect_1234
        , if('asd'='123', '1234', '12345') as expect_12345
;  {code}
Expected Result：

 
{code:java}
the column expect_1234  value should be '1234'
the column expect_12345 value should be '12345'{code}
 

 

Real Returend Result( test in sql-client):

!image-2024-05-28-17-04-59-069.png|width=495,height=337!",,,,,,,,,,,,,,,,,,,,FLINK-30559,,,,,,"28/May/24 09:05;lyndon.lee;image-2024-05-28-17-04-59-069.png;https://issues.apache.org/jira/secure/attachment/13069141/image-2024-05-28-17-04-59-069.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 13:55:35 UTC 2024,,,,,,,,,,"0|z1pgko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 13:55;lincoln.86xy;[~lyndon.lee] Thanks for reporting this! 
It's a known issue(FLINK-30559) and fixed in 1.16+, due to Flink version policy( [update-policy-for-old-releases|https://flink.apache.org/downloads/#update-policy-for-old-releases], [[DISCUSS] Flink minor version support policy for old releases|https://lists.apache.org/thread/szq23kr3rlkm80rw7k9n95js5vqpsnbv], [[DISCUSS]Use 1.15.4 as the final patch version of Flink 1.15 and drop 1.15 support|https://lists.apache.org/thread/csm81mlcktobl00p37x9t2jkpwmj9m58] ), the fix hasn't been backported to 1.15.

So it is recommended to use a higher version, such as 1.19.0 or 1.18.1(because there're also several other fixes related to 'if' function since 1.18.0, ref to FLINK-30966).

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce isInternalSorterSupport to OperatorAttributes,FLINK-35475,13580659,13553221,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,28/May/24 08:29,04/Jun/24 13:31,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / Task,,,,0,pull-request-available,,Introduce isInternalSorterSupport to OperatorAttributes to notify Flink whether the operator will sort the data internally in batch mode or during backlog.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-28 08:29:16.0,,,,,,,,,,"0|z1pgjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"During the process of catching up after a restart, Flink CDC uses the latest table information to parse old Mysql binlogs.",FLINK-35474,13580656,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stevedengzhi,stevedengzhi,28/May/24 08:20,28/May/24 08:33,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,Flink CDC,,,,0,,,"!image-2024-05-28-16-13-58-566.png!

IN FLINK CDC 2.3.0 VERSION:

This DDL issue can prevent recovery from checkpoints, leading to infinite restarts.
How to reproduce this issue?（翻译by chatGPT，English not well ）
Step 1: Turn off Flink CDC.
Step 2: Perform DDL operations on multiple tables, including deleting columns, reducing columns, and adding columns.
Step 3: Restore Flink CDC from the last checkpoint before it was turned off.
 
 
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/24 08:14;stevedengzhi;image-2024-05-28-16-13-58-566.png;https://issues.apache.org/jira/secure/attachment/13069140/image-2024-05-28-16-13-58-566.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 08:33:48 UTC 2024,,,,,,,,,,"0|z1pgiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 08:33;stevedengzhi;First Question: Does the information in the checkpoint record the old table schema along with the binlog position?:);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-457: Improve Table/SQL Configuration for Flink 2.0,FLINK-35473,13580642,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,qingyue,qingyue,qingyue,28/May/24 07:38,04/Jun/24 07:54,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,,,"This is the parent task for [FLIP-457|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=307136992].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 07:54:37 UTC 2024,,,,,,,,,,"0|z1pgfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/24 03:23;lincoln.86xy;Hi [~qingyue], any updates on this?

As the FLIP says 'In Flink 2.0, the deprecated options will be removed', we should get this done in 1.20 (which plans a codefreeze on 15th June[1]). Let me know if I can help with a review.

[1] https://cwiki.apache.org/confluence/display/FLINK/1.20+Release

 

 

 ;;;","04/Jun/24 07:54;qingyue;[~lincoln.86xy] Thank you for the reminder. I'm planning to open a PR in the coming days, and it would be great if you could help review it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve tests for Elasticsearch 8 connector,FLINK-35472,13580640,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liuml07,liuml07,28/May/24 07:07,30/May/24 16:38,04/Jun/24 20:40,,elasticsearch-3.2.0,,,,,,,,,,,Connectors / ElasticSearch,Tests,,,0,pull-request-available,,"Per discussion in [this PR|https://github.com/apache/flink-connector-elasticsearch/pull/104],  it makes the tests more reusable if we use parameterized tests. It requires some changes of the existing tests, which includes:
# Make base test class parameterized with secure parameter. As JUnit 5 has limited support for parameterized tests with inheritance, we can use the {{ParameterizedTestExtension}} introduced in Flink, see this doc
# Manage the test container lifecycle instead of using the managed annotation {{@Testcontainers}} and {{@Container}} so that the test containers can be used as a singleton for all tests in the suite
# Create and use common methods in the base class that concrete test classes can be mostly parameter-agnostic

This JIRA intends to not change any logic or functionality. Instead it focuses on tests refactoring for more reusable tests and future proof.
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-35424,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-28 07:07:29.0,,,,,,,,,,"0|z1pgfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator bumps the flink version to 1.19,FLINK-35471,13580635,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,28/May/24 06:35,28/May/24 06:35,04/Jun/24 20:40,,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,,,Kubernetes operator bumps the flink version to 1.19 after 1.19.1 is released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-28 06:35:34.0,,,,,,,,,,"0|z1pge8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.18.1 and 1.19.0 having build jdk 1.8 and causing incompatibilities with Java 17,FLINK-35470,13580633,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Invalid,,rajat_singh,rajat_singh,28/May/24 06:25,28/May/24 15:27,04/Jun/24 20:40,28/May/24 06:45,1.18.1,,,,,,,,,,,Flink CDC,,,,0,,,"I am writing flink jobs with latest release version for flink (1.18.1). The jobmanager is also deployed with the same version build. But we faced issues when we deployed the jobs. On further investigation, I noticed all libraries from flink have build jdk 1.8. is this the correct version? How to write flink jobs with Java 17 code and which library versions to use? Is Java 17 even supported on the client side or is it just on the server side support for java 17?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,JAVA,Tue May 28 15:27:43 UTC 2024,,,,,,,,,,"0|z1pgds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 06:45;martijnvisser;Please post user questions to the User mailing list, Stackoverflow or Slack, as is explained in our docs https://flink.apache.org/how-to-contribute/getting-help/#having-a-question;;;","28/May/24 15:27;rajat_singh;Have sent an email and posted on stack overflow as well. Having not received any reply I posted here. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test case 'testProject' of JdbcDynamicTableSourceITCase,FLINK-35469,13580626,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,RocMarshal,RocMarshal,28/May/24 05:18,28/May/24 06:09,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,JDBC,,https://github.com/apache/flink-connector-jdbc/actions/runs/9263628064/job/25482376215?pr=119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-28 05:18:32.0,,,,,,,,,,"0|z1pgc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Connectors/Pulsar] update isEnableAutoAcknowledgeMessage config comment,FLINK-35468,13580620,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,geniusjoe,geniusjoe,28/May/24 04:01,28/May/24 04:06,04/Jun/24 20:40,,pulsar-4.1.0,,,,,,,,pulsar-4.1.0,,,Connectors / Pulsar,,,,0,pull-request-available,,"Since we [deleted {{shared}} and {{key-shared}} subscription type|https://issues.apache.org/jira/browse/FLINK-30413] in pulsar source connector, I think it is better to remove these subscription type in {{isEnableAutoAcknowledgeMessage}} option comment to prevent misunderstanding.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-28 04:01:39.0,,,,,,,,,,"0|z1pgaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The flink-cdc.sh script of flink-cdc-3.1.0 cannot read the correct Flink configuration information specified by the flink-conf.yaml through the FLINK_CONF_DIR environment variable.,FLINK-35467,13580616,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xjtulg,xjtulg,28/May/24 00:46,28/May/24 00:46,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,Flink-CDC,,"Problem Description:
When starting our Flink service, we use the FLINK_CONF_DIR environment variable to specify the flink-conf.yaml to a directory other than $FLINK_HOME/conf. The flink-conf.yaml directory under $FLINK_HOME/conf is incorrect.

When submitting CDC tasks using the flink-cdc.sh script of flink-cdc-3.1.0, we attempted to let flink-cdc read information from the correct flink-conf.yaml configuration file through the FLINK_CONF_DIR environment variable. However, it failed, and it still reads the configuration information from the flink-conf.yaml under $FLINK_HOME/conf, causing task submission failures.","Environment Information:
flink-1.17.1
flink-cdc-3.1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2024-05-28 00:46:01.0,,,,,,,,,,"0|z1pga0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot pass all columns to SQL UDFs using `*`,FLINK-35466,13580549,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gyfora,gyfora,27/May/24 12:01,27/May/24 12:01,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,"The SQL API does not allow calling UDFs using all the columns of a given table using * notation such as:
{noformat}
tableEnv.executeSql(""SELECT MyFun(Orders.*) FROM Orders"").print();{noformat}
The above call fails with the following error:
{noformat}
org.apache.flink.table.api.ValidationException: SQL validation failed. At line 1, column 21: Unknown field '*'
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:200)	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:117)	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:261)	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)	at com.apple.pie.flink.utils.testing.Test2.test(Test2.java:29)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.base/java.lang.reflect.Method.invoke(Method.java:566)	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)	at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)Caused by: org.apache.calcite.runtime.CalciteContextException: At line 1, column 21: Unknown field '*'	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490){noformat}
However when using the Table API directly, similar expression is possible:
{noformat}
tableEnv.from(""Orders"").select(call(MyFun.class, $(""*""))).printSchema();{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-27 12:01:38.0,,,,,,,,,,"0|z1pfv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce BatchJobRecoveryHandler for recovery of batch jobs from JobMaster failures.,FLINK-35465,13580545,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,27/May/24 11:46,30/May/24 06:49,04/Jun/24 20:40,28/May/24 01:44,,,,,,,,,1.20.0,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35483,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 01:44:42 UTC 2024,,,,,,,,,,"0|z1pfu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 01:44;zhuzh;master:
b8173eb662ee5823de40de356869d0064de2c22a
3206659db5b7c4ce645072f11f091e0e9e92b0ce
e964af392476e011147be73ae4dab8ff89512994;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC 3.1 breaks operator state compatiblity,FLINK-35464,13580540,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,xiqian_yu,xiqian_yu,xiqian_yu,27/May/24 11:31,04/Jun/24 09:26,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.1.1,,,Flink CDC,,,,0,pull-request-available,,"Flink CDC 3.1 changed how SchemaRegistry [de]serializes state data, which causes any checkpoint states saved with earlier version could not be restored in version 3.1.0.

This could be resolved by adding serialization versioning control logic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 11:34:04 UTC 2024,,,,,,,,,,"0|z1pft4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 11:32;xiqian_yu;[~Leonard] I'm willing to investigate this.;;;","27/May/24 11:34;leonard;Thanks [~xiqian_yu] for the investigation, and I improve the issue priority to blocker;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC job failed when restored from checkpoint when route rule changed.,FLINK-35463,13580515,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hk__lrzy,hk__lrzy,27/May/24 08:05,30/May/24 10:00,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"*Exception:* 

 
{code:java}
java.lang.IllegalStateException: Unable to get latest schema for table ""demo.partition_all"" {code}
*Reason:* 

Now *ScheamRegister* will stored the all original tables which from source and all derivered table which from route rules.

Assume we have a table in mysql which named *demo.partition_all* and have route rule *demo1.partition_all* for it.

1. Before first checkpoint be triggered,  *ScheamRegister* will store both *demo.partition_all* and {*}demo1.partition_all{*}'s schema in the {*}SchemaManager{*}.

2. Stop the job, change route rule as {*}demo1.partition_all_1{*}, and restart it with checkpoint.

3. According the logical follow:
{code:java}
if (request.getSchemaChangeEvent() instanceof CreateTableEvent
    && schemaManager.schemaExists(request.getTableId())) {
  return CompletableFuture.completedFuture(
      wrap(new SchemaChangeResponse(Collections.emptyList())));
} {code}
We will not create a schema for the table *demo1.partition_all_1* and job will be failed when we request schema from {*}SchemaOperator{*}.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-27 08:05:50.0,,,,,,,,,,"0|z1pfnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a parameter to control the chaining mode between a Flink SQL connector sink or source and its adjacent operator.,FLINK-35462,13580513,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,heigebupahei,heigebupahei,27/May/24 07:48,27/May/24 07:50,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,"When using the flinksql connector, we can set the parallelism of the sink. However, when we modify the parallelism of the sink to be the same as the global one, the global chaining strategy may cause the two operators to be linked together, When use checkpoint or savepoint recovery, which will make the task unable to proceed.  we should allow explicit specification of the chaining mode of the connector.
 
Related issues:

[https://github.com/apache/iceberg/issues/10371]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-27 07:48:58.0,,,,,,,,,,"0|z1pfn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Runtime Configuration for Flink 2.0,FLINK-35461,13580504,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xuannan,xuannan,xuannan,27/May/24 03:48,30/May/24 07:01,04/Jun/24 20:40,30/May/24 07:01,,,,,,,,,1.20.0,,,Runtime / Configuration,,,,0,pull-request-available,,"As Flink moves toward 2.0, we have revisited all runtime configurations and identified several improvements to enhance user-friendliness and maintainability. We want to refine the runtime configuration.

 

This ticket implements all the changes discussed in [FLIP-450|https://cwiki.apache.org/confluence/display/FLINK/FLIP-450%3A+Improve+Runtime+Configuration+for+Flink+2.0].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 07:01:21 UTC 2024,,,,,,,,,,"0|z1pfl4:",9223372036854775807,"The following configurations have been deprecated  as we are phasing out the hash-based blocking shuffle:
- `taskmanager.network.sort-shuffle.min-parallelism`
- `taskmanager.network.blocking-shuffle.type`

The following configurations have been deprecated  as we are phasing out the legacy hybrid shuffle:
- `taskmanager.network.hybrid-shuffle.spill-index-region-group-size`
- `taskmanager.network.hybrid-shuffle.num-retained-in-memory-regions-max`
- `taskmanager.network.hybrid-shuffle.enable-new-mode`

The following configurations have been deprecated  to simply the configuration of network buffers:
- `taskmanager.network.memory.buffers-per-channel`
- `taskmanager.network.memory.floating-buffers-per-gate`
- `taskmanager.network.memory.max-buffers-per-channel`
- `taskmanager.network.memory.max-overdraft-buffers-per-gate`
- `taskmanager.network.memory.exclusive-buffers-request-timeout-ms` (Please use `taskmanager.network.memory.buffers-request-timeout` instead.)

The configuration `taskmanager.network.batch-shuffle.compression.enabled` has been deprecated. Please set `taskmanager.network.compression.codec` to ""NONE"" to disable compression.

The following Netty-related configurations are no longer recommended for use and have been deprecated:
- `taskmanager.network.netty.num-arenas`
- `taskmanager.network.netty.server.numThreads`
- `taskmanager.network.netty.client.numThreads`
- `taskmanager.network.netty.server.backlog`
- `taskmanager.network.netty.sendReceiveBufferSize`
- `taskmanager.network.netty.transport`

The following configurations are unnecessary and have been deprecated:
- `taskmanager.network.max-num-tcp-connections`
- `fine-grained.shuffle-mode.all-blocking`",,,,,,,,,,,,,,,,,,,"30/May/24 07:01;xtsong;master (1.20): 132722e1b43607fe2236ecbbc3ba1faad1ad147f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check file size when position read for ForSt,FLINK-35460,13580501,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,27/May/24 03:13,28/May/24 01:57,04/Jun/24 20:40,28/May/24 01:57,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 01:57:02 UTC 2024,,,,,,,,,,"0|z1pfkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 01:57;masteryhx;Merged 9fbe9f30 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Incremental Source Framework in Flink CDC TiKV Source Connector,FLINK-35459,13580500,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ouyangwuli,ouyangwuli,27/May/24 02:51,27/May/24 02:51,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,Use Incremental Source Framework in Flink CDC TiKV Source Connector,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-27 02:51:48.0,,,,,,,,,,"0|z1pfk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add serializer upgrade test for set serializer,FLINK-35458,13580498,13564005,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,27/May/24 02:43,27/May/24 02:43,04/Jun/24 20:40,,1.20.0,,,,,,,,2.0.0,,,API / Type Serialization System,,,,0,,,"New dedicated serializer for Sets is introduced in [FLINK-35068|https://issues.apache.org/jira/browse/FLINK-35068]. Since serializer upgrade test requires at least one previous release to test the upgrade of set serializers (which does not exist yet), we'll add the upgrade test for set serializer after the release of v1.20.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-27 02:43:04.0,,,,,,,,,,"0|z1pfjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EventTimeWindowCheckpointingITCase fails on AZP as NPE,FLINK-35457,13580497,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Weijie Guo,Weijie Guo,27/May/24 02:29,31/May/24 10:20,04/Jun/24 20:40,28/May/24 03:11,1.20.0,,,,,,,,,,,Build System / CI,,,,0,pull-request-available,,"
{code:java}
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.checkpoint.filemerging.PhysicalFile.deleteIfNecessary(PhysicalFile.java:155)
	at org.apache.flink.runtime.checkpoint.filemerging.PhysicalFile.decRefCount(PhysicalFile.java:141)
	at org.apache.flink.runtime.checkpoint.filemerging.LogicalFile.discardWithCheckpointId(LogicalFile.java:118)
	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.discardSingleLogicalFile(FileMergingSnapshotManagerBase.java:574)
	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.discardLogicalFiles(FileMergingSnapshotManagerBase.java:588)
	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.notifyCheckpointAborted(FileMergingSnapshotManagerBase.java:490)
	at org.apache.flink.runtime.checkpoint.filemerging.WithinCheckpointFileMergingSnapshotManager.notifyCheckpointAborted(WithinCheckpointFileMergingSnapshotManager.java:61)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyFileMergingSnapshotManagerCheckpoint(SubtaskCheckpointCoordinatorImpl.java:505)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:490)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:414)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$21(StreamTask.java:1513)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$23(StreamTask.java:1536)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:998)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:923)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:949)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59821&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8538
",,,,,,,,,,,,,,,,,,,,FLINK-35418,,,FLINK-35446,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 03:10:42 UTC 2024,,,,,,,,,,"0|z1pfjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 03:10;Yanfei Lei;Merged into master via 1e996b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Many tests fails on AZP as NPE related to FileMerging,FLINK-35456,13580496,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,Weijie Guo,Weijie Guo,27/May/24 02:20,30/May/24 05:16,04/Jun/24 20:40,30/May/24 05:16,1.20.0,,,,,,,,1.20.0,,,Build System / CI,,,,0,,,"{code:java}
May 25 02:12:16 Caused by: java.lang.NullPointerException
May 25 02:12:16 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.isManagedByFileMergingManager(FileMergingSnapshotManagerBase.java:733)
May 25 02:12:16 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.lambda$null$4(FileMergingSnapshotManagerBase.java:687)
May 25 02:12:16 	at java.util.HashMap.computeIfAbsent(HashMap.java:1127)
May 25 02:12:16 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.lambda$restoreStateHandles$5(FileMergingSnapshotManagerBase.java:683)
May 25 02:12:16 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
May 25 02:12:16 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
May 25 02:12:16 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
May 25 02:12:16 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
May 25 02:12:16 	at java.util.stream.Streams$StreamBuilderImpl.forEachRemaining(Streams.java:419)
May 25 02:12:16 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
May 25 02:12:16 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
May 25 02:12:16 	at java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:901)
May 25 02:12:16 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
May 25 02:12:16 	at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)
May 25 02:12:16 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
May 25 02:12:16 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
May 25 02:12:16 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
May 25 02:12:16 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
May 25 02:12:16 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
May 25 02:12:16 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
May 25 02:12:16 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
May 25 02:12:16 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
May 25 02:12:16 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
May 25 02:12:16 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
May 25 02:12:16 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
May 25 02:12:16 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
May 25 02:12:16 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
May 25 02:12:16 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
May 25 02:12:16 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.restoreStateHandles(FileMergingSnapshotManagerBase.java:680)
May 25 02:12:16 	at org.apache.flink.runtime.checkpoint.filemerging.SubtaskFileMergingManagerRestoreOperation.restore(SubtaskFileMergingManagerRestoreOperation.java:102)
May 25 02:12:16 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.registerRestoredStateToFileMergingManager(StreamTaskStateInitializerImpl.java:353)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59821&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12158
",,,,,,,,,,,,,,,,,,,,,,,FLINK-35446,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 05:16:38 UTC 2024,,,,,,,,,,"0|z1pfjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 02:40;Weijie Guo;WindowJoinITCase.testInnerJoinOnWTFWithOffset also has same issue:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59821&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11935;;;","27/May/24 02:43;Weijie Guo;testTop1WithGroupByCount:535->testTopNthWithGroupByCountBase

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59828&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12064;;;","27/May/24 02:44;Weijie Guo;SplitAggregateITCase.testMultipleDistinctAggOnSameColumn

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59828&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11930
;;;","27/May/24 04:00;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59839&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11943

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59839&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12577

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59839&view=logs&j=26b84117-e436-5720-913e-3e280ce55cae&t=77cc7e77-39a0-5007-6d65-4137ac13a471&l=12538;;;","30/May/24 05:16;Weijie Guo;This should be resolved by FLINK-35446.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Non-idempotent unit tests,FLINK-35455,13580486,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kaiyaok2,kaiyaok2,26/May/24 16:33,26/May/24 16:33,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,"We found that several unit tests are non-idempotent, as they pass in the first run but fail in the second run. These will not be triggered by the CI since Surefire only executes a test once by default. A fix is necessary since unit tests shall be self-contained, ensuring that the state of the system under test is consistent at the beginning of each test. In practice, fixing non-idempotent tests can help proactively avoid state pollution that results in test order dependency (which could cause problems under test selection , prioritization or parallelization).

An example of a non-idempotent test:
`SerializerConfigImplTest#testLoadingTypeInfoFactoriesFromSerializationConfig`
This test fails in repeated runs because a typeInfoFactory for type `class org.apache.flink.api.common.serialization.SerializerConfigImplTest` is already registered in the first test execution.

Reproduce (using the `NIOInspector` plugin because Surefire does not support re-running a specific test twice without changing the source code):
```
cd flink-core
mvn edu.illinois:NIOInspector:rerun -Dtest=org.apache.flink.api.common.serialization.SerializerConfigImplTest#testLoadingTypeInfoFactoriesFromSerializationConfig
```

Please kindly let us know if you find it necessary to resolve test non-idempotency. We would open PRs to fix tests like this if so.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-26 16:33:31.0,,,,,,,,,,"0|z1pfh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector ArchTests fails due to dependency on fink.util.Preconditions,FLINK-35454,13580477,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,chalixar,chalixar,26/May/24 08:37,28/May/24 22:38,04/Jun/24 20:40,28/May/24 22:38,1.20.0,,,,,,,,1.20.0,,,Test Infrastructure,,,,0,pull-request-available,,"h2. Description
 - Arch Unit Rules for connectors limits dependencies of any classes in connectors on @Public or @PublicEvolving with exceptions of connector package classes, this is not true since we should be able to depend on internal util classes like {{Preconditions}} and {{ExceptionsUtils}}

{code:java}
freeze(
                    javaClassesThat(resideInAnyPackage(CONNECTOR_PACKAGES))
                            .and()
                            .areNotAnnotatedWith(Deprecated.class)
                            .should()
                            .onlyDependOnClassesThat(
                                  areFlinkClassesThatResideOutsideOfConnectorPackagesAndArePublic()
                                            .or(
                                                    JavaClass.Predicates.resideOutsideOfPackages(
                                                            ""org.apache.flink..""))
                                            .or(
                                                    JavaClass.Predicates.resideInAnyPackage(
                                                            CONNECTOR_PACKAGES)) )
                            .as(
                                    ""Connector production code must depend only on public API when outside of connector packages""));

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 22:37:59 UTC 2024,,,,,,,,,,"0|z1pff4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/24 08:39;chalixar;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59823&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=fc5181b0-e452-5c8f-68de-1097947f6483;;;","28/May/24 22:37;jingge;master: b1025e66e7b13bce6a6407544420dc494cd5e1b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamReader Charset fix with UTF8 in core files,FLINK-35453,13580472,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuzifu,xuzifu,26/May/24 03:36,03/Jun/24 10:52,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,1.19.2,,,,,,,0,pull-request-available,,StreamReader Charset fix with UTF8 in core files,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 10:52:08 UTC 2024,,,,,,,,,,"0|z1pfe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/24 10:52;xuzifu;[~hlteoh37] , could you give a review this pr? https://github.com/apache/flink/pull/24842;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
MySQL CDC is not tracking newly added tables when a failure happens after the snapshot phase,FLINK-35452,13580471,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sharonxr55,sharonxr55,26/May/24 03:25,26/May/24 03:31,04/Jun/24 20:40,,2.0.0,,,,,,,,,,,Flink CDC,,,,0,,,"We've seen some cases where a MySQL CDC job is restarted with some newly added tables, and the newly added tables are not getting change events after the snapshot. 

From the log, it seems that a failure happened right after the snapshot phase, and when the job is restored from the last checkpoint, the newly added tables are not in the state. It looks like whenever a failure happens in the snapshot phase could also cause this to happen in general.

Using Flink 1.15.3 with MySQL CDC 2.4.2

Some logs and analysis

!image-2024-05-25-20-24-40-460.png|width=1360,height=837!",,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/24 03:24;sharonxr55;image-2024-05-25-20-24-40-460.png;https://issues.apache.org/jira/secure/attachment/13069111/image-2024-05-25-20-24-40-460.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-26 03:25:17.0,,,,,,,,,,"0|z1pfds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support `ALTER CATALOG COMMENT` syntax,FLINK-35451,13580465,13572934,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liyubin117,liyubin117,25/May/24 18:11,25/May/24 18:11,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,Table SQL / API,,,,0,,,"Set comment in the specified catalog. If the comment is already set in the catalog, override the old value with the new one.

!image-2024-05-26-02-11-30-070.png|width=575,height=415!",,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/24 18:11;liyubin117;image-2024-05-26-02-11-30-070.png;https://issues.apache.org/jira/secure/attachment/13069110/image-2024-05-26-02-11-30-070.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-25 18:11:58.0,,,,,,,,,,"0|z1pfcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Tikv pipeline DataSource,FLINK-35450,13580445,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ouyangwuli,ouyangwuli,25/May/24 08:56,25/May/24 08:56,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,After we add hostmapping(https://issues.apache.org/jira/browse/FLINK-35354) to tikv，we can use flink cdc sync data from tikv. We need more convenient and high-performance data synchronization capabilities from tikv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-25 08:56:51.0,,,,,,,,,,"0|z1pf80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MySQL CDC Flink SQL column names are case-sensitive,FLINK-35449,13580433,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leen,leen,25/May/24 03:06,25/May/24 03:06,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,,,"Using Flink SQL with MySQL CDC, I noticed that the column names in Flink SQL are case-sensitive with respect to the column names in the MySQL tables. I couldn't find any configuration options to change this behavior.

Do we have support for case-insensitive configurations to address this issue? Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-25 03:06:00.0,,,,,,,,,,"0|z1pf5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate pod templates documentation into Chinese,FLINK-35448,13580367,13573144,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,caicancai,caicancai,24/May/24 13:14,30/May/24 13:33,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,,"Translate pod templates documentation into Chinese
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-24 13:14:16.0,,,,,,,,,,"0|z1peqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC Document document file had removed but website can access,FLINK-35447,13580359,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,gongzhongqiang,gongzhongqiang,gongzhongqiang,24/May/24 12:04,28/May/24 02:58,04/Jun/24 20:40,28/May/24 02:58,cdc-3.1.0,,,,,,,,cdc-3.1.1,cdc-3.2.0,,Flink CDC,,,,0,pull-request-available,,https://nightlies.apache.org/flink/flink-cdc-docs-master/docs/connectors/overview/ the link should not appeared.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 02:56:31 UTC 2024,,,,,,,,,,"0|z1peow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 02:56;leonard;master: d97124fb9f531193d8b232862af44e6bcca03277
release-3.1: f47a95629e077f4e4f14d81c429788042ff39bf8
release-3.0: 93d82c327d2ac86b07d295ed64b74c22967435ba
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileMergingSnapshotManagerBase throws a NullPointerException,FLINK-35446,13580358,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zakelly,rskraba,rskraba,24/May/24 11:48,28/May/24 06:18,04/Jun/24 20:40,26/May/24 16:41,,,,,,,,,1.20.0,,,,,,,0,pull-request-available,test-stability,"* 1.20 Java 11 / Test (module: tests) https://github.com/apache/flink/actions/runs/9217608897/job/25360103124#step:10:8641

{{ResumeCheckpointManuallyITCase.testExternalizedIncrementalRocksDBCheckpointsWithLocalRecoveryZookeeper}} throws a NullPointerException when it tries to restore state handles: 

{code}
Error: 02:57:52 02:57:52.551 [ERROR] Tests run: 48, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 268.6 s <<< FAILURE! -- in org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase
Error: 02:57:52 02:57:52.551 [ERROR] org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedIncrementalRocksDBCheckpointsWithLocalRecoveryZookeeper[RestoreMode = CLAIM] -- Time elapsed: 3.145 s <<< ERROR!
May 24 02:57:52 org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
May 24 02:57:52 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:219)
May 24 02:57:52 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailureAndReport(ExecutionFailureHandler.java:166)
May 24 02:57:52 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:121)
May 24 02:57:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:279)
May 24 02:57:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:270)
May 24 02:57:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:263)
May 24 02:57:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:788)
May 24 02:57:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:765)
May 24 02:57:52 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
May 24 02:57:52 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:496)
May 24 02:57:52 	at jdk.internal.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
May 24 02:57:52 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 24 02:57:52 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
May 24 02:57:52 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:318)
May 24 02:57:52 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
May 24 02:57:52 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:316)
May 24 02:57:52 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:229)
May 24 02:57:52 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:88)
May 24 02:57:52 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:174)
May 24 02:57:52 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
May 24 02:57:52 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
May 24 02:57:52 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
May 24 02:57:52 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
May 24 02:57:52 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
May 24 02:57:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
May 24 02:57:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
May 24 02:57:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
May 24 02:57:52 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
May 24 02:57:52 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
May 24 02:57:52 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
May 24 02:57:52 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
May 24 02:57:52 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
May 24 02:57:52 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
May 24 02:57:52 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
May 24 02:57:52 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
May 24 02:57:52 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
May 24 02:57:52 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
May 24 02:57:52 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
May 24 02:57:52 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
May 24 02:57:52 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
May 24 02:57:52 Caused by: java.lang.NullPointerException
May 24 02:57:52 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.isManagedByFileMergingManager(FileMergingSnapshotManagerBase.java:733)
May 24 02:57:52 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.lambda$restoreStateHandles$4(FileMergingSnapshotManagerBase.java:687)
May 24 02:57:52 	at java.base/java.util.HashMap.computeIfAbsent(HashMap.java:1134)
May 24 02:57:52 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.lambda$restoreStateHandles$5(FileMergingSnapshotManagerBase.java:683)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
May 24 02:57:52 	at java.base/java.util.stream.Streams$StreamBuilderImpl.forEachRemaining(Streams.java:411)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:274)
May 24 02:57:52 	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:274)
May 24 02:57:52 	at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033)
May 24 02:57:52 	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
May 24 02:57:52 	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:274)
May 24 02:57:52 	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
May 24 02:57:52 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
May 24 02:57:52 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
May 24 02:57:52 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
May 24 02:57:52 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.restoreStateHandles(FileMergingSnapshotManagerBase.java:680)
May 24 02:57:52 	at org.apache.flink.runtime.checkpoint.filemerging.SubtaskFileMergingManagerRestoreOperation.restore(SubtaskFileMergingManagerRestoreOperation.java:102)
May 24 02:57:52 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.registerRestoredStateToFileMergingManager(StreamTaskStateInitializerImpl.java:353)
May 24 02:57:52 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:163)
May 24 02:57:52 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:267)
May 24 02:57:52 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
May 24 02:57:52 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreStateAndGates(StreamTask.java:858)
May 24 02:57:52 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$restoreInternal$5(StreamTask.java:812)
May 24 02:57:52 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
May 24 02:57:52 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:812)
May 24 02:57:52 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:771)
May 24 02:57:52 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:970)
May 24 02:57:52 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:939)
May 24 02:57:52 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:763)
May 24 02:57:52 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
May 24 02:57:52 	at java.base/java.lang.Thread.run(Thread.java:829)
{code}

This looks like a similar error that happened in FLINK-35382, but for a different reason.

",,,,,,,,,,,,,,,,,,,,,,,,FLINK-35456,FLINK-35457,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 06:18:56 UTC 2024,,,,,,,,,,"0|z1peoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 12:38;rskraba;* 1.20 Java 11 / Test (module: tests) https://github.com/apache/flink/actions/runs/9217608897/job/25360103124#step:10:8641
* 1.20 Default (Java 8) / Test (module: table) https://github.com/apache/flink/actions/runs/9219075449/job/25363874486#step:10:11847 {{PruneAggregateCallITCase.testNoneEmptyGroupKey}}
* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9219075449/job/25363874825#step:10:8005

The last one is different than the others: 
{code}
Error: 05:48:38 05:48:38.790 [ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.78 s <<< FAILURE! -- in org.apache.flink.test.classloading.ClassLoaderITCase
Error: 05:48:38 05:48:38.790 [ERROR] org.apache.flink.test.classloading.ClassLoaderITCase.testCheckpointedStreamingClassloaderJobWithCustomClassLoader -- Time elapsed: 2.492 s <<< FAILURE!
May 24 05:48:38 org.assertj.core.error.AssertJMultipleFailuresError: 
May 24 05:48:38 
May 24 05:48:38 Multiple Failures (1 failure)
May 24 05:48:38 -- failure 1 --
May 24 05:48:38 [Any cause is instance of class 'class org.apache.flink.util.SerializedThrowable' and contains message 'org.apache.flink.test.classloading.jar.CheckpointedStreamingProgram$SuccessException'] 
May 24 05:48:38 Expecting any element of:
May 24 05:48:38   [org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Job execution failed.
May 24 05:48:38 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:373)
May 24 05:48:38 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:223)
May 24 05:48:38 	at org.apache.flink.test.classloading.ClassLoaderITCase.lambda$testCheckpointedStreamingClassloaderJobWithCustomClassLoader$1(ClassLoaderITCase.java:260)
May 24 05:48:38 	...(54 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
May 24 05:48:38     org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 24 05:48:38 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
May 24 05:48:38 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
May 24 05:48:38 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
May 24 05:48:38 	...(45 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
May 24 05:48:38     org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=100)
May 24 05:48:38 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:219)
May 24 05:48:38 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailureAndReport(ExecutionFailureHandler.java:166)
May 24 05:48:38 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:121)
May 24 05:48:38 	...(36 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
May 24 05:48:38     java.lang.NullPointerException
May 24 05:48:38 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.isManagedByFileMergingManager(FileMergingSnapshotManagerBase.java:733)
May 24 05:48:38 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.lambda$null$4(FileMergingSnapshotManagerBase.java:687)
May 24 05:48:38 	at java.util.HashMap.computeIfAbsent(HashMap.java:1128)
May 24 05:48:38 	...(41 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)]
May 24 05:48:38 to satisfy the given assertions requirements but none did:
May 24 05:48:38 
May 24 05:48:38 org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Job execution failed.
May 24 05:48:38 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:373)
May 24 05:48:38 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:223)
May 24 05:48:38 	at org.apache.flink.test.classloading.ClassLoaderITCase.lambda$testCheckpointedStreamingClassloaderJobWithCustomClassLoader$1(ClassLoaderITCase.java:260)
May 24 05:48:38 	...(54 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
{code}


;;;","24/May/24 12:42;rskraba;[~lijinzhong] or [~zakelly] Do you think this needs a similar fix as FLINK-35382 ? ;;;","24/May/24 15:00;zakelly;[~rskraba] Thanks for letting me know! I'll fix this.;;;","26/May/24 16:41;zakelly;Merged into master via 096b4a6bef98ccff918ef3994c16b7361bca21b8;;;","27/May/24 15:19;rskraba;Thanks for the fix!  There were a bunch of failures over the weekend before the merge to master:

* 1.20 Default (Java 8) / Test (module: table) https://github.com/apache/flink/actions/runs/9249920179/job/25442781056#step:10:12157
* 1.20 Java 8 / Test (module: tests) https://github.com/apache/flink/actions/runs/9248172120/job/25438340923#step:10:8501
* 1.20 Java 17 / Test (module: table) https://github.com/apache/flink/actions/runs/9248172120/job/25438314807#step:10:11974
* 1.20 Java 17 / Test (module: tests) https://github.com/apache/flink/actions/runs/9248172120/job/25438315031#step:10:8441
* 1.20 Java 21 / Test (module: table) https://github.com/apache/flink/actions/runs/9248172120/job/25438306000#step:10:12064
* 1.20 Java 21 / Test (module: tests) https://github.com/apache/flink/actions/runs/9248172120/job/25438306359#step:10:9072
* 1.20 Hadoop 3.1.3 / Test (module: table) https://github.com/apache/flink/actions/runs/9248172120/job/25438381891#step:10:12151
* 1.20 Hadoop 3.1.3 / Test (module: tests) https://github.com/apache/flink/actions/runs/9248172120/job/25438382250#step:10:8131
* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9248172120/job/25438295648#step:10:12081
* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9248089774/job/25438060032#step:10:8040
* 1.20 Default (Java 8) / Test (module: table) https://github.com/apache/flink/actions/runs/9244756333/job/25430934260#step:10:11992
* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9244756333/job/25430934479#step:10:8471
* 1.20 Java 8 / Test (module: table) https://github.com/apache/flink/actions/runs/9239908683/job/25419730553#step:10:11972
* 1.20 Java 11 / Test (module: table) https://github.com/apache/flink/actions/runs/9239908683/job/25419746284#step:10:11933
* 1.20 Java 17 / Test (module: tests) https://github.com/apache/flink/actions/runs/9239908683/job/25419747284#step:10:8437
* 1.20 Default (Java 8) / Test (module: table) https://github.com/apache/flink/actions/runs/9236391640/job/25412610305#step:10:12028
* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9236391640/job/25412610424#step:10:8615
* 1.20 Java 8 / Test (module: table) https://github.com/apache/flink/actions/runs/9232146809/job/25403130654#step:10:11954
* 1.20 Java 17 / Test (module: table) https://github.com/apache/flink/actions/runs/9232146809/job/25403143495#step:10:12425
* 1.20 Java 17 / Test (module: tests) https://github.com/apache/flink/actions/runs/9232146809/job/25403143840#step:10:8431
* 1.20 Java 21 / Test (module: table) https://github.com/apache/flink/actions/runs/9232146809/job/25403134721#step:10:11960
* 1.20 Java 21 / Test (module: tests) https://github.com/apache/flink/actions/runs/9232146809/job/25403134721#step:10:11960
* 1.20 Hadoop 3.1.3 / Test (module: table) https://github.com/apache/flink/actions/runs/9232146809/job/25403165764#step:10:12305
* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9232146809/job/25403133340#step:10:12266
* 1.20 AdaptiveScheduler / Test (module: tests) https://github.com/apache/flink/actions/runs/9232146809/job/25403133470#step:10:8553

Unfortunately, I think these two failures happened on master **after** the fix was merged -- do you think something was missed?  This can definitely be verified with the next nightly build!

* 1.20 Default (Java 8) / Test (module: table) https://github.com/apache/flink/actions/runs/9250759677/job/25445310702#step:10:12049
* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9250759677/job/25445311108#step:10:8510;;;","28/May/24 06:18;zakelly;Thanks for the reminder [~rskraba] , actually I do miss something, which is fixed by FLINK-35457 just now.
I'll keep eye on the following builds.;;;",,,,,,,,,,,,,,,,,,,,,,,
Update Async Sink documentation for Timeout configuration ,FLINK-35445,13580357,13580242,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chalixar,chalixar,24/May/24 11:44,24/May/24 11:45,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,Connectors / Common,Documentation,,,0,,,"Update Documentation for AsyncSink Changes introduced by [FLIP-451|https://cwiki.apache.org/confluence/display/FLINK/FLIP-451%3A+Introduce+timeout+configuration+to+AsyncSink+API]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-24 11:44:53.0,,,,,,,,,,"0|z1peog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paimon Pipeline Connector support changing column names to lowercase for Hive metastore,FLINK-35444,13580344,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,kunni,kunni,24/May/24 08:01,24/May/24 08:01,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,"Hive metastore Require column names to be lowercase, but field names from upstream tables may not meet the requirements. 
We can add a parameter configuration in sink to convert all column names to lowercase.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-24 08:01:40.0,,,,,,,,,,"0|z1pelk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogLocalRecoveryITCase failed fatally with 239 exit code,FLINK-35443,13580327,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,24/May/24 05:35,24/May/24 05:36,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"{code:java}
May 24 01:55:13 01:55:13.114 [ERROR] Error occurred in starting fork, check output in log
May 24 01:55:13 01:55:13.114 [ERROR] Process Exit Code: 239
May 24 01:55:13 01:55:13.114 [ERROR] Crashed tests:
May 24 01:55:13 01:55:13.114 [ERROR] org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase
May 24 01:55:13 01:55:13.114 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
May 24 01:55:13 01:55:13.114 [ERROR] Command was /bin/sh -c cd '/__w/3/s/flink-tests' && '/usr/lib/jvm/jdk-21.0.1+12/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/3/s/flink-tests/target/surefire/surefirebooter-20240524010735067_567.jar' '/__w/3/s/flink-tests/target/surefire' '2024-05-24T01-07-32_135-jvmRun2' 'surefire-20240524010735067_565tmp' 'surefire_187-20240524010735067_566tmp'
May 24 01:55:13 01:55:13.114 [ERROR] Error occurred in starting fork, check output in log
May 24 01:55:13 01:55:13.114 [ERROR] Process Exit Code: 239
May 24 01:55:13 01:55:13.114 [ERROR] Crashed tests:
May 24 01:55:13 01:55:13.114 [ERROR] org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase
May 24 01:55:13 01:55:13.114 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456)
May 24 01:55:13 01:55:13.114 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:418)
May 24 01:55:13 01:55:13.114 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
May 24 01:55:13 01:55:13.114 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:250)
May 24 01:55:13 01:55:13.114 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1240)
May 24 01:55:13 01:55:13.114 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1089)
May 24 01:55:13 01:55:13.114 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:905)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2(MojoExecutor.java:370)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute(MojoExecutor.java:351)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:215)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:171)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:163)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:294)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:960)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:293)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:196)
May 24 01:55:13 01:55:13.115 [ERROR] 	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
May 24 01:55:13 01:55:13.115 [ERROR] 	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
May 24 01:55:13 01:55:13.115 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:282)

{code}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59793&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=9615",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-24 05:35:14.0,,,,,,,,,,"0|z1pehs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Pipeline Sink may distribute the same key to different partitions after modify parallelism,FLINK-35442,13580323,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,kunni,kunni,24/May/24 03:57,24/May/24 03:57,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,," Currently, Kafka Pipeline Sink partition events by 
parallelInstanceId % partitions.length，this is because the previous partition operator has already shuffled events by key.

However, If the parallelism of the task is modified and restarted, data with the same key before and after the task may be in different partitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-24 03:57:17.0,,,,,,,,,,"0|z1pegw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add CDC upgrade compatibility tests,FLINK-35441,13580319,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xiqian_yu,xiqian_yu,xiqian_yu,24/May/24 02:15,27/May/24 09:07,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"Currently, there's no test cases to guarantee checkpoint state compatibility between different CDC versions like Flink's SerializerUpgradeTestBase.

Adding it should help CDC users upgrading version with more confidence.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 02:15:25 UTC 2024,,,,,,,,,,"0|z1peg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 02:15;xiqian_yu;[~renqs] I'd love to take this ticket if needed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to connect tableau to jdbc flink url using flink sql driver,FLINK-35440,13580316,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,blaskovicz,blaskovicz,24/May/24 00:44,31/May/24 15:42,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Table SQL / JDBC,,,,0,pull-request-available,,"Tableau 2023.1 using [https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-jdbc-driver-bundle/] version 1.19.0 yields the following error when a connection is established to a local flink sql cluster using the uri {{{}jdbc:{}}}{{{}[flink://localhost:8083]{}}}

{{{""ts"":""2024-05-23T14:21:05.858"",""pid"":12172,""tid"":""6a70"",""sev"":""error"",""req"":""-"",""sess"":""-"",""site"":""-"",""user"":""-"",""k"":""jdbc-error"",""e"":\{""excp-error-code"":""0xFAB9A2C5"",""excp-source"":""NeedsClassification"",""excp-status-code"":""UNKNOWN""},""v"":\{""context"":""GrpcProtocolProxy::IsConnected (D:\\tc\\work\\t231\\g_pc\\modules\\connectors\\tabmixins\\main\\db\\GrpcProtocolProxy.cpp:456)"",""driver-name"":""org.apache.flink.table.jdbc.FlinkDriver"",""driver-version"":""1.19.0"",""error-code"":""0"",""error-messages"":[""FlinkConnection#isValid is not supported yet.""],""grpc-status-code"":""2"",""protocol-id"":3,""sql-state"":""0""}}}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 01:06:34 UTC 2024,,,,,,,,,,"0|z1pefc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 01:06;blaskovicz;After the above pull request, a new error is seen
```\{""ts"":""2024-05-23T20:59:19.463"",""pid"":97497,""tid"":""7120f"",""sev"":""info"",""req"":""-"",""sess"":""-"",""site"":""-"",""user"":""-"",""k"":""end-protocol.query"",""l"":{},""a"":\{""depth"":4,""elapsed"":0.995,""exclusive"":0.995,""id"":""P8RI+WqHEfuLJSiVnHo5Mu"",""name"":""protocol.query"",""rk"":""exception"",""root"":""DhXdXdGsUU/JYsr0MZE8iE"",""rv"":{""e-code"":""0xFAB9A2C5"",""e-source"":""NeedsClassification"",""e-status-code"":""2"",""msg"":""\""java.lang.ClassCastException: class java.lang.Integer cannot be cast to class java.lang.Long (java.lang.Integer and java.lang.Long are in module java.base of loader 'bootstrap')\n\"""",""type"":""ConnectivityException""},""sponsor"":""JhI/Z+7h0BrIJHSP0v9JKb"",""type"":""end""},""v"":\{""cols"":0,""is-command"":false,""protocol-class"":""genericjdbc"",""protocol-id"":2,""query-category"":""Metadata"",""query-hash"":2970729448,""query-tags"":"""",""query-trunc"":""SELECT 1"",""rows"":0},""ctx"":{}}```;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Support for AvroSchemaConverter,FLINK-35439,13580287,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gregwills1,gregwills1,23/May/24 14:53,23/May/24 14:53,04/Jun/24 20:40,,1.17.2,1.18.1,,,,,,,,,,API / Python,,,,0,,,"The java class org.apache.flink.formats.avro.typeutils.AvroSchemaConverter is not integrated into the apache-flink Python library (pyflink). My goal is to dynamically create a Pyflink table schema from an avro schema, but without a Python interface via py4j do so, that task is difficult. 

Especially for the Table API, this would improve schema management and evolution practices. Currently, the responsibility is on the user to statically create the table schema (used as the Avro reader schema). Schema updates can cause this static reader schema that is generated from the table schema to be incompatible with the writer schema (when consuming/deserializing). 

This is the case specifically for the format avro-confluent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,2024-05-23 14:53:01.0,,,,,,,,,,"0|z1pe8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinatorTest.testErrorThrownFromSplitEnumerator fails on wrong error,FLINK-35438,13580284,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,23/May/24 14:43,28/May/24 03:45,04/Jun/24 20:40,,1.18.2,,,,,,,,,,,,,,,0,test-stability,,"* 1.18 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/9201159842/job/25309197630#step:10:7375

We expect to see an artificial {{Error(""Test Error"")}} being reported in the test as the cause of a job failure, but the reported job failure is null:

{code}
Error: 02:32:31 02:32:31.950 [ERROR] Tests run: 18, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.187 s <<< FAILURE! - in org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest
Error: 02:32:31 02:32:31.950 [ERROR] org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest.testErrorThrownFromSplitEnumerator  Time elapsed: 0.01 s  <<< FAILURE!
May 23 02:32:31 org.opentest4j.AssertionFailedError: 
May 23 02:32:31 
May 23 02:32:31 expected: 
May 23 02:32:31   java.lang.Error: Test Error
May 23 02:32:31   	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest.testErrorThrownFromSplitEnumerator(SourceCoordinatorTest.java:296)
May 23 02:32:31   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 23 02:32:31   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 23 02:32:31   	...(57 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
May 23 02:32:31  but was: 
May 23 02:32:31   null
May 23 02:32:31 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
May 23 02:32:31 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
May 23 02:32:31 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
May 23 02:32:31 	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorTest.testErrorThrownFromSplitEnumerator(SourceCoordinatorTest.java:322)
May 23 02:32:31 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 23 02:32:31 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 23 02:32:31 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 23 02:32:31 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
May 23 02:32:31 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
May 23 02:32:31 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
May 23 02:32:31 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
May 23 02:32:31 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
May 23 02:32:31 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
{code}

This looks like it's a multithreading error with the test {{MockOperatorCoordinatorContext}}, perhaps where {{isJobFailure}} can return true before the reason has been populated. I couldn't reproduce it after running it 1M times.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 01:26:35 UTC 2024,,,,,,,,,,"0|z1pe88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 01:26;robyoung;I agree there's a race that's hard to reproduce, I can only provoke it by adding in a thread sleep in the spot where it can occur in `MockOperatorCoordinatorContext`
{code:java}
@Override
public void failJob(Throwable cause) {
    jobFailed = true;
    try {
        Thread.sleep(50);
    } catch (InterruptedException e) {
        throw new RuntimeException(e);
    }
    jobFailureReason = cause;
    jobFailedFuture.complete(null);
} 

public boolean isJobFailed() {
    return jobFailed;
}

public Throwable getJobFailureReason() {
    return jobFailureReason;
}{code}
If getJobFailureReason() is called between jobFailed and jobFailureReason being assigned then the test thread can unexpectedly observe a null jobFailureReason while jobFailed is true.

The same race is present in master.

Happy to contribute a fix if someone could please assign me to the ticket. Synchronizing access to the two fields would be one way. Perhaps you could replace the two fields with one that contains the failed bool and reason so they are assigned together.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlockStatementGrouper uses lots of memory,FLINK-35437,13580266,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,23/May/24 12:48,24/May/24 08:26,04/Jun/24 20:40,24/May/24 08:26,1.19.0,,,,,,,,1.20.0,,,Table SQL / Runtime,,,,0,pull-request-available,,"For deeply nested {{if else}} statements {{BlockStatementGrouper}} uses loads of memory and fails with OOM quickly.

When running JMs with around 400mb a query like:
{code}
select case when orderid = 0 then 1 when orderid = 1 then 2 when orderid
    = 2 then 3 when orderid = 3 then 4 when orderid = 4 then 5 when orderid = 5 then
    6 when orderid = 6 then 7 when orderid = 7 then 8 when orderid = 8 then 9 when
    orderid = 9 then 10 when orderid = 10 then 11 when orderid = 11 then 12 when orderid
    = 12 then 13 when orderid = 13 then 14 when orderid = 14 then 15 when orderid
    = 15 then 16 when orderid = 16 then 17 when orderid = 17 then 18 when orderid
    = 18 then 19 when orderid = 19 then 20 when orderid = 20 then 21 when orderid
    = 21 then 22 when orderid = 22 then 23 when orderid = 23 then 24 when orderid
    = 24 then 25 when orderid = 25 then 26 when orderid = 26 then 27 when orderid
    = 27 then 28 when orderid = 28 then 29 when orderid = 29 then 30 when orderid
    = 30 then 31 when orderid = 31 then 32 when orderid = 32 then 33 when orderid
    = 33 then 34 when orderid = 34 then 35 when orderid = 35 then 36 when orderid
    = 36 then 37 when orderid = 37 then 38 when orderid = 38 then 39 when orderid
    = 39 then 40 when orderid = 40 then 41 when orderid = 41 then 42 when orderid
    = 42 then 43 when orderid = 43 then 44 when orderid = 44 then 45 when orderid
    = 45 then 46 when orderid = 46 then 47 when orderid = 47 then 48 when orderid
    = 48 then 49 when orderid = 49 then 50 when orderid = 50 then 51 when orderid
    = 51 then 52 when orderid = 52 then 53 when orderid = 53 then 54 when orderid
    = 54 then 55 when orderid = 55 then 56 when orderid = 56 then 57 when orderid
    = 57 then 58 when orderid = 58 then 59 when orderid = 59 then 60 when orderid
    = 60 then 61 when orderid = 61 then 62 when orderid = 62 then 63 when orderid
    = 63 then 64 when orderid = 64 then 65 when orderid = 65 then 66 when orderid
    = 66 then 67 when orderid = 67 then 68 when orderid = 68 then 69 when orderid
    = 69 then 70 when orderid = 70 then 71 when orderid = 71 then 72 when orderid
    = 72 then 73 when orderid = 73 then 74 when orderid = 74 then 75 when orderid
    = 75 then 76 when orderid = 76 then 77 when orderid = 77 then 78 when orderid
    = 78 then 79 when orderid = 79 then 80 when orderid = 80 then 81 when orderid
    = 81 then 82 when orderid = 82 then 83 when orderid = 83 then 84 when orderid
    = 84 then 85 when orderid = 85 then 86 when orderid = 86 then 87 when orderid
    = 87 then 88 when orderid = 88 then 89 when orderid = 89 then 90 when orderid
    = 90 then 91 when orderid = 91 then 92 when orderid = 92 then 93 when orderid
    = 93 then 94 when orderid = 94 then 95 when orderid = 95 then 96 when orderid
    = 96 then 97 when orderid = 97 then 98 when orderid = 98 then 99 when orderid
    = 99 then 100 when orderid = 100 then 101 when orderid = 101 then 102 when orderid
    = 102 then 103 when orderid = 103 then 104 when orderid = 104 then 105 when orderid
    = 105 then 106 when orderid = 106 then 107 when orderid = 107 then 108 when orderid
    = 108 then 109 when orderid = 109 then 110 when orderid = 110 then 111 when orderid
    = 111 then 112 when orderid = 112 then 113 when orderid = 113 then 114 when orderid
    = 114 then 115 when orderid = 115 then 116 when orderid = 116 then 117 when orderid
    = 117 then 118 when orderid = 118 then 119 when orderid = 119 then 120 when orderid
    = 120 then 121 when orderid = 121 then 122 when orderid = 122 then 123 when orderid
    = 123 then 124 when orderid = 124 then 125 when orderid = 125 then 126 when orderid
    = 126 then 127 when orderid = 127 then 128 when orderid = 128 then 129 when orderid
    = 129 then 130 when orderid = 130 then 131 when orderid = 131 then 132 when orderid
    = 132 then 133 when orderid = 133 then 134 when orderid = 134 then 135 when orderid
    = 135 then 136 when orderid = 136 then 137 when orderid = 137 then 138 when orderid
    = 138 then 139 when orderid = 139 then 140 when orderid = 140 then 141 when orderid
    = 141 then 142 when orderid = 142 then 143 when orderid = 143 then 144 when orderid
    = 144 then 145 when orderid = 145 then 146 when orderid = 146 then 147 when orderid
    = 147 then 148 when orderid = 148 then 149 when orderid = 149 then 150 when orderid
    = 150 then 151 when orderid = 151 then 152 when orderid = 152 then 153 when orderid
    = 153 then 154 when orderid = 154 then 155 when orderid = 155 then 156 when orderid
    = 156 then 157 when orderid = 157 then 158 when orderid = 158 then 159 when orderid
    = 159 then 160 when orderid = 160 then 161 when orderid = 161 then 162 when orderid
    = 162 then 163 when orderid = 163 then 164 when orderid = 164 then 165 when orderid
    = 165 then 166 when orderid = 166 then 167 when orderid = 167 then 168 when orderid
    = 168 then 169 when orderid = 169 then 170 when orderid = 170 then 171 when orderid
    = 171 then 172 when orderid = 172 then 173 when orderid = 173 then 174 when orderid
    = 174 then 175 when orderid = 175 then 176 when orderid = 176 then 177 when orderid
    = 177 then 178 when orderid = 178 then 179 when orderid = 179 then 180 when orderid
    = 180 then 181 when orderid = 181 then 182 when orderid = 182 then 183 when orderid
    = 183 then 184 when orderid = 184 then 185 when orderid = 185 then 186 when orderid
    = 186 then 187 when orderid = 187 then 188 when orderid = 188 then 189 when orderid
    = 189 then 190 when orderid = 190 then 191 when orderid = 191 then 192 when orderid
    = 192 then 193 when orderid = 193 then 194 when orderid = 194 then 195 when orderid
    = 195 then 196 when orderid = 196 then 197 when orderid = 197 then 198 when orderid
    = 198 then 199 when orderid = 199 then 200 when orderid = 200 then 201 when orderid
    = 201 then 202 when orderid = 202 then 203 when orderid = 203 then 204 when orderid
    = 204 then 205 when orderid = 205 then 206 when orderid = 206 then 207 when orderid
    = 207 then 208 when orderid = 208 then 209 when orderid = 209 then 210 when orderid
    = 210 then 211 when orderid = 211 then 212 when orderid = 212 then 213 when orderid
    = 213 then 214 when orderid = 214 then 215 when orderid = 215 then 216 when orderid
    = 216 then 217 when orderid = 217 then 218 when orderid = 218 then 219 when orderid
    = 219 then 220 when orderid = 220 then 221 when orderid = 221 then 222 when orderid
    = 222 then 223 when orderid = 223 then 224 when orderid = 224 then 225 when orderid
    = 225 then 226 when orderid = 226 then 227 when orderid = 227 then 228 when orderid
    = 228 then 229 when orderid = 229 then 230 when orderid = 230 then 231 when orderid
    = 231 then 232 when orderid = 232 then 233 when orderid = 233 then 234 when orderid
    = 234 then 235 when orderid = 235 then 236 when orderid = 236 then 237 when orderid
    = 237 then 238 when orderid = 238 then 239 when orderid = 239 then 240 when orderid
    = 240 then 241 when orderid = 241 then 242 when orderid = 242 then 243 when orderid
    = 243 then 244 when orderid = 244 then 245 when orderid = 245 then 246 when orderid
    = 246 then 247 when orderid = 247 then 248 when orderid = 248 then 249 when orderid
    = 249 then 250 else 9999 end case_when_col from sample_data_1;
{code}

fails with an OOM. (Yes, I know the query can be simplified, but it shows the case).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 08:26:03 UTC 2024,,,,,,,,,,"0|z1pe48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 08:26;dwysakowicz;Fixed in 3d40bd7dd197b12b7b156bd758b4129148e885d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job can't launch when setting the option schema.change.behavior to IGNORE,FLINK-35436,13580256,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hk__lrzy,hk__lrzy,23/May/24 11:38,23/May/24 12:34,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"Now in the 3.0 pipeline,  *SchemaOperator* already was necessary operator in the Flink DAG, both *PrePartitionOperator* and *DataSinkWriterOperator* have connection with the *SchemaRegister* according the *schemaEvolutionClient,* but when we set the option schema.change.behavior to ignore or exception, the pipeline will add a filter operator instead of the *Schema Operator,* final cause the job fail.

I think we still need keep the option for the schema.change.behavior to meet the difference cases, so i advice to move schema.change.behavior to the *SchemaRegister* to let *SchemaOperator* will be always in the DAG, and let to *SchemaRegister* decided to apply the schema change or not.{*}{*}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-23 11:38:02.0,,,,,,,,,,"0|z1pe20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-451] Introduce timeout configuration to AsyncSink,FLINK-35435,13580242,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chalixar,chalixar,23/May/24 10:00,04/Jun/24 10:15,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,Connectors / Common,,,,0,pull-request-available,,"Implementation Ticket for:
https://cwiki.apache.org/confluence/display/FLINK/FLIP-451%3A+Introduce+timeout+configuration+to+AsyncSink+API ",,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/24 11:21;chalixar;Screenshot 2024-05-24 at 11.06.30.png;https://issues.apache.org/jira/secure/attachment/13069083/Screenshot+2024-05-24+at+11.06.30.png","24/May/24 11:21;chalixar;Screenshot 2024-05-24 at 12.06.20.png;https://issues.apache.org/jira/secure/attachment/13069084/Screenshot+2024-05-24+at+12.06.20.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 11:58:33 UTC 2024,,,,,,,,,,"0|z1pdyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 11:40;chalixar;h1. Non-Functional Backward compatibility 
To assure that we haven't imposed any regressions to existing implementers we tested {{ KinesisStreamsSink }} with default request timeout vs no timeout on 2 levels

h2. Sanity testing

We have run the [example job|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-aws-kinesis-streams/src/test/java/org/apache/flink/connector/kinesis/sink/examples/SinkIntoKinesis.java] with checkpoint interval of 10 seconds, we set the request timeout for 3 minutes and verified no requests were retried due to timeout during a period of 30 minutes of job execution.

h2. Performance Benchmark

I have benchmarked the kinesis sink with the default timeout (10 minutes) with batch size = 20, and default values of inflight requests.

The result show no difference (except for a small network blip)

h3. Sink With Timeout
 !Screenshot 2024-05-24 at 11.06.30.png! 

h3. Sink With No Timeout
 !Screenshot 2024-05-24 at 12.06.20.png! ;;;","24/May/24 11:58;chalixar;[~danny.cranmer] Could you please take a look when you have time?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support pass exception in StateExecutor to runtime,FLINK-35434,13580240,13574094,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,23/May/24 09:52,27/May/24 11:46,04/Jun/24 20:40,27/May/24 10:23,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,"Exception may thrown when _StateExecutor_ execute the state request , such as a IOException. We should pass the exception to runtime then failed the job in this situation.

 
_InternalStateFuture#completeExceptionally()_ will be added as [discussion here|https://github.com/apache/flink/pull/24739#discussion_r1590633134].
And then,  _ForStWriteBatchOperation_ and _ForStGeneralMultiGetOperation_ will call this method when exception occurred.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 10:23:55 UTC 2024,,,,,,,,,,"0|z1pdyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 10:23;masteryhx;Merged f926d2a5 and 2f7099e7 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a config parameter to set {{publishNotReadyAddresses}} option for the jobmanager's RPC Kubernetes service,FLINK-35433,13580237,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dantalian_pv,dantalian_pv,23/May/24 09:33,23/May/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,"*Context:*
In native Kubernetes deployment Flink creates a headless service for JobManager's RPC calls. The description down below is only relevant for Flink deployment in Application mode.

When there are {{livenessProbe}} and/or {{readinessProbe}} are defined with {{{}initialDelaySeconds{}}}, created instances of TaskManager have to wait until JobManager's probes are green, before they are able to connect to the JobManager.

Probes configuration:
{code:yaml}
- name: flink-main-container
  livenessProbe:               
    httpGet:                 
      path: /jobs/overview
      port: rest
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 10
    failureThreshold: 6
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    httpGet:
      path: /jobs/overview
      port: rest
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 10
    failureThreshold: 6
    successThreshold: 1
    timeoutSeconds: 5
{code}
During this period there are log messages in the TaskManager like:
{code:java}
Failed to connect to [dev-pipeline.dev-namespace:6123] from local address [dev-pipeline-taskmanager-1-1/11.41.6.81] with timeout [200] due to: dev-pipeline.dev-namespace
{code}
 

*Issue:*
Because initialization time of different Flink jobs (read: Flink deployments) can vary in a wide range, it would be convenient to have a common configuration for {{livenessProbe}} and/or {{readinessProbe}} for all deployments, which will then cover the worst case, instead of tuning it for every deployment. On the other hand, it would be nice to reduce the job's bootstrap time as a whole, because the jobs' re-deployment in our case happens often and it affects response time of incoming requests from clients.

 

*Solution:*
To reduce the job's bootstrap time as a whole one solution could be to set {{publishNotReadyAddresses}} flag via config parameter in jobmanager's RPC Kubernetes service, so that created instance of a taskmanager can connect to the jobmanager immediately. 
Publishing ""not ready"" JobManager's RPC should not cause any issue, because the TaskManager instances in Kubernetes native deployment are created by a ResourceManager, which is part of the JobManager, which in turn guarantees, that JobManager is ready and ExecutionGraph was built successfully when a TaskManager is starting.
Making this flag optional guarantees, that such approach will work correctly, when the flag is disabled and JobManager High Availability is defined, which in turn involves the leader election.

 

_Affected Classes:_
 - {{org.apache.flink.kubernetes.kubeclient.services.HeadlessClusterIPService}} - by adding one line {{.withPublishNotReadyAddresses(kubernetesJobManagerParameters.isPublishNotReadyAddresses())}} in {{Service buildUpInternalService(
KubernetesJobManagerParameters kubernetesJobManagerParameters)}}
 - {{org.apache.flink.kubernetes.configuration.KubernetesConfigOptions}} - by adding something like {{kubernetes.jobmanager.rpc.service.publish-not-ready-addresses}} option
 - {{org.apache.flink.kubernetes.kubeclient.parameters.KubernetesJobManagerParameters}} - by adding the get method for the parameter: {{public boolean isPublishNotReadyAddresses() \{ return flinkConfig.getBoolean(KubernetesConfigOptions.KUBERNETES_JOBMANAGER_RPC_SERVICE_PUBLISH_NOT_READY_ADDRESSES); }}}
 - Tests to cover the new parameter

If there is a decision, that such improvement worth to be part of Flink, I am ready to provide a PR for it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-23 09:33:47.0,,,,,,,,,,"0|z1pdxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support catch modify-event for mysql-cdc,FLINK-35432,13580228,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hk__lrzy,hk__lrzy,23/May/24 08:31,28/May/24 03:42,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"Now users common use sql like to modify the column type in MySQL.
{code:java}
Alter table MODIFY COLUMN `new_name` new_type{code}
 

Flink-CDC use *CustomAlterTableParserListener* to parse the ddl and wrap it as ChangeEevent now. But i noticed that *CustomAlterTableParserListener* not implement the method *enterAlterByModifyColumn* and {*}exitAlterByModifyColumn{*}, and it will cause we can't received the  *AlterColumnTypeEvent* now

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-23 08:31:09.0,,,,,,,,,,"0|z1pdvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate references in Flink CDC documentation from Debezium 1.9 to 2.0,FLINK-35431,13580226,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gongzhongqiang,gongzhongqiang,gongzhongqiang,23/May/24 08:26,24/May/24 05:56,04/Jun/24 20:40,24/May/24 05:56,cdc-3.2.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Backgroud:
The website has taken down the documentation for versions before 2.0. And flink cdc document ci failed.

Discuss link: https://lists.apache.org/thread/twdpd4bvoxtmjd0tykk7hs73hx4rm6yv",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 05:56:20 UTC 2024,,,,,,,,,,"0|z1pdvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 05:56;leonard;Fixed via 
master:644b5c2ce1dea17df93bac6ddf5afd816c33fffb
release-3.1:8e533487dd4c77f75f8f7d2d89130ec0be1fdebb
release-3.0: 0577cd0a0dc257848525bd6473a0d470e0e662c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZoneId is not passed to DebeziumJsonSerializationSchema,FLINK-35430,13580221,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,tianzhu.wen_1998,kunni,kunni,23/May/24 07:54,03/Jun/24 08:43,04/Jun/24 20:40,03/Jun/24 08:40,cdc-3.1.1,,,,,,,,cdc-3.1.1,cdc-3.2.0,,Flink CDC,,,,0,pull-request-available,,"ZoneId is used to convert TIMESTAMP_WITH_LOCAL_TIME_ZONE type to specific time zone, but DebeziumJsonSerializationSchema does not used the user-defined zoneid.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 08:40:57 UTC 2024,,,,,,,,,,"0|z1pdu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 06:49;tianzhu.wen_1998;Please assgined this to me, thanks.;;;","03/Jun/24 08:40;leonard;Fixed via 
master: 0e8b2c7cc59db227dc0e15f219bd3d1b4c28ad42 
3.1: 19f7afe6af8e7fb0f1f5e090da4c701dfef52633;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
We don't need introduce getFlinkConfigurationOptions for SqlGatewayRestEndpointFactory#Context,FLINK-35429,13580219,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,23/May/24 07:47,27/May/24 01:42,04/Jun/24 20:40,23/May/24 12:11,1.19.1,,,,,,,,1.19.1,1.20.0,,,,,,0,pull-request-available,,"We don't need this method, as ReadableConfig has a toMap method now.

This will fix the compile error in 1.19

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59754&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=13638.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 12:11:53 UTC 2024,,,,,,,,,,"0|z1pdts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/24 12:11;Weijie Guo;master(1.20) via c3221a649b9daa8374cc24ab039f6352fb2c6edf.
release-1.19 via a450980de65eaead734349ed44452f572e5e329d.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowJoinITCase#testInnerJoin failed on AZP as NPE,FLINK-35428,13580214,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,23/May/24 07:14,23/May/24 07:28,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"{code:java}
Caused by: java.lang.NullPointerException
May 23 02:00:33 	at org.apache.flink.runtime.checkpoint.filemerging.PhysicalFile.deleteIfNecessary(PhysicalFile.java:155)
May 23 02:00:33 	at org.apache.flink.runtime.checkpoint.filemerging.PhysicalFile.decRefCount(PhysicalFile.java:141)
May 23 02:00:33 	at org.apache.flink.runtime.checkpoint.filemerging.LogicalFile.discardWithCheckpointId(LogicalFile.java:118)
May 23 02:00:33 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.discardSingleLogicalFile(FileMergingSnapshotManagerBase.java:574)
May 23 02:00:33 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.discardLogicalFiles(FileMergingSnapshotManagerBase.java:588)
May 23 02:00:33 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.notifyCheckpointAborted(FileMergingSnapshotManagerBase.java:490)
May 23 02:00:33 	at org.apache.flink.runtime.checkpoint.filemerging.WithinCheckpointFileMergingSnapshotManager.notifyCheckpointAborted(WithinCheckpointFileMergingSnapshotManager.java:61)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyFileMergingSnapshotManagerCheckpoint(SubtaskCheckpointCoordinatorImpl.java:505)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:490)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:414)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$21(StreamTask.java:1513)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$23(StreamTask.java:1536)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:968)
May 23 02:00:33 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:917)
May 23 02:00:33 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:966)
May 23 02:00:33 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:945)
May 23 02:00:33 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759)
May 23 02:00:33 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:573)
May 23 02:00:33 	at java.lang.Thread.run(Thread.java:748)
{code}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59751&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11944",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 07:28:16 UTC 2024,,,,,,,,,,"0|z1pdso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/24 07:28;rskraba;This could be a duplicate of FLINK-35418 (which occurred on EventTimeWindowCheckpointingITCase).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support fail-on-unknown-field config in json format,FLINK-35427,13580194,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,huhuan1898,huhuan1898,23/May/24 03:47,27/May/24 10:54,04/Jun/24 20:40,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,"In many cases, the consumer and producer of message queues come from different teams, or even different companies.
As message consumer, sometimes it is difficult to subscribe updates on message format, which may result in data loss.
We want to ensure the message format strictly matches the schema, if some field is missing or new field is added, it is better to fail the application so that we can notice and fix it quickly.
In this case, a fail-on-unknown-field config for JSON format could be very helpful, especially when works with fail-on-missing-field.",,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-23 03:47:48.0,,,,,,,,,,"0|z1pdo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the distribution of DynamicFilteringDataCollector to Broadcast,FLINK-35426,13580187,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,xiasun,xiasun,23/May/24 02:57,27/May/24 04:08,04/Jun/24 20:40,27/May/24 04:08,1.20.0,,,,,,,,1.20.0,,,Table SQL / Planner,,,,0,pull-request-available,,"Currently, the DynamicFilteringDataCollector is utilized in the dynamic partition pruning feature of batch jobs to collect the partition information dynamically filtered by the source. Its current data distribution method is rebalance, and it also acts as an upstream vertex to the probe side Source.

Presently, when the Scheduler dynamically infers the parallelism for vertices that are both downstream and Source, it considers factors from both sides, which can lead to an overestimation of parallelism due to DynamicFilteringDataCollector being an upstream of the Source. We aim to change the distribution method of the DynamicFilteringDataCollector to broadcast to prevent the dynamic overestimation of Source parallelism.

Furthermore, given that the DynamicFilteringDataCollector transmits data through the OperatorCoordinator rather than through normal data distribution, this change will not affect the DPP (Dynamic Partition Pruning) functionality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 04:08:38 UTC 2024,,,,,,,,,,"0|z1pdmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/24 03:01;xiasun;[~zhuzh] Could you please assign this ticket to me? Thanks.;;;","23/May/24 03:12;zhuzh;Good point! [~xiasun]
The task is assigned to you. Feel free to open a pr for it.;;;","27/May/24 04:08;zhuzh;master: 4b342da6d149113dde821b370a136beef3430fff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support convert Freshness to cron expression for full refresh mode ,FLINK-35425,13580184,13579167,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lsy,lsy,23/May/24 02:39,28/May/24 07:49,04/Jun/24 20:40,28/May/24 07:49,1.20.0,,,,,,,,1.20.0,,,Table SQL / Gateway,Table SQL / Planner,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 07:48:56 UTC 2024,,,,,,,,,,"0|z1pdm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 07:48;lsy;Merged in master: 49f22254a78d554ac49810058c209297331129cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch connector 8 supports SSL context,FLINK-35424,13580174,13567477,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liuml07,liuml07,liuml07,22/May/24 23:47,28/May/24 07:18,04/Jun/24 20:40,26/May/24 15:13,1.17.1,,,,,,,,elasticsearch-3.2.0,,,Connectors / ElasticSearch,,,,0,pull-request-available,,"In  FLINK-34369, we added SSL support for the base Elasticsearch sink class that is used by both Elasticsearch 6 and 7. The Elasticsearch 8 connector is using the AsyncSink API and it does not use the aforementioned base sink class. It needs separate change to support this feature.

This is specially important to Elasticsearch 8 which enables secure by default. Meanwhile, it merits if we add integration tests for this SSL context support.",,,,,,,,,,,,,,,,,,,,,,,FLINK-35472,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 26 15:13:20 UTC 2024,,,,,,,,,,"0|z1pdjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/24 15:13;Sergey Nuyanzin;Merged as [50327f84d46ac2af10d352730ab0dfd0432249bd|https://github.com/apache/flink-connector-elasticsearch/commit/50327f84d46ac2af10d352730ab0dfd0432249bd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ARRAY_EXCEPT should support set semantics,FLINK-35423,13580132,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/May/24 14:58,04/Jun/24 06:43,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,"After a number of discussions e.g. here [1]
It was decided to follow set semantics for {{ARRAY_EXCEPT}} and {{ARRAY_INTERSECT}}.

It is marked as a blocker since {{ARRAY_EXCEPT}} was added in 1.20 only and has not been released yet, so the change should be done before 1.20.0 release to avoid inconsistencies.

[1] https://github.com/apache/flink/pull/24526",,,,,,,,,,,,,,,,,,,,,,,FLINK-31663,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-22 14:58:06.0,,,,,,,,,,"0|z1pdag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Flink dynamically adjusts operator-level resource usage without stopping the job,FLINK-35422,13580127,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rockyyin,rockyyin,22/May/24 14:30,22/May/24 14:30,04/Jun/24 20:40,,1.16.3,,,,,,,,,,,API / DataStream,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,2024-05-22 14:30:48.0,,,,,,,,,,"0|z1pd9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema Operator blocking forever when Akka Rpc timeout,FLINK-35421,13580118,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hk__lrzy,hk__lrzy,22/May/24 13:46,24/May/24 06:02,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"When SchemaOperator restart and there have no checkpoint before, SchemaOperator will not send *RefreshPendingListsRequest* to coordinator, and if coordinator have pending schema events and SchemaOperator will block forever.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 02:35:36 UTC 2024,,,,,,,,,,"0|z1pd7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/24 01:31;kunni;Thanks for pointing out this issue. I am willing to fix it.;;;","23/May/24 02:35;hk__lrzy;[~kunni]  Thank your for relay,  i already have a  pull request to fix the issue, maybe you can assign the Jira to me and review the PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
WordCountMapredITCase fails to compile in IntelliJ,FLINK-35420,13580115,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,22/May/24 13:20,23/May/24 07:14,04/Jun/24 20:40,23/May/24 07:13,1.20.0,,,,,,,,1.20.0,,,Connectors / Hadoop Compatibility,,,,0,pull-request-available,,"{noformat}
flink-connectors/flink-hadoop-compatibility/src/test/scala/org/apache/flink/api/hadoopcompatibility/scala/WordCountMapredITCase.scala:42:8
value isFalse is not a member of ?0
possible cause: maybe a semicolon is missing before `value isFalse'?
      .isFalse()
{noformat}

Might be caused by:
https://youtrack.jetbrains.com/issue/SCL-20679
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 07:13:38 UTC 2024,,,,,,,,,,"0|z1pd6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/24 07:13;pnowojski; 9ccfb65 into apache:master now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
scan.bounded.latest-offset makes queries never finish if the latest message is a EndTxn Kafka marker,FLINK-35419,13580106,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fpaul,fpaul,22/May/24 12:42,22/May/24 12:43,04/Jun/24 20:40,,1.16.0,1.17.0,1.19.0,1.8.0,,,,,,,,Connectors / Kafka,,,,0,,,"When running the kafka connector in bounded mode, the stop condition can be defined as the latest offset when the job starts. Unfortunately, Kafka's latest offset calculation also includes special marker records, such as transaction markers, in the overall count.

 

When Flink waits for a job to finish, it compares the number of records read until the point with the original latest offset [1]. Since the consumer will never see the special marker records, the latest offset is never reached, and the job gets stuck. 

 

To reproduce the issue, you can write into a Kafka topic and make sure that the latest record is a transaction end event. Afterwards you can start a Flink job configured with `scan.bounded.latest-offset` pointing to that topic.

 

[1]https://github.com/confluentinc/flink/blob/59c5446c4aac0d332a21b456f4a3f82576104b80/flink-connectors/confluent-connector-kafka/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java#L128",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-22 12:42:11.0,,,,,,,,,,"0|z1pd4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EventTimeWindowCheckpointingITCase fails with an NPE,FLINK-35418,13580105,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,rskraba,rskraba,22/May/24 12:41,31/May/24 10:20,04/Jun/24 20:40,31/May/24 10:20,1.20.0,,,,,,,,,,,,,,,0,test-stability,,"* 1.20 Default (Java 8) / Test (module: tests) [https://github.com/apache/flink/actions/runs/9185169193/job/25258948607#step:10:8106]

It looks like it's possible for PhysicalFile to generate a NullPointerException while a checkpoint is being aborted:

{code}
May 22 04:35:18 Starting org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase#testTumblingTimeWindow[statebackend type =ROCKSDB_INCREMENTAL_ZK, buffersPerChannel = 2].
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1287)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)
	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)
	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)
	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)
	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
	at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)
	at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)
	at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)
	at org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)
	at org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:219)
	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailureAndReport(ExecutionFailureHandler.java:166)
	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:121)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:279)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:270)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:263)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:788)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:765)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:496)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:318)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:316)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:229)
	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:88)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:174)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.checkpoint.filemerging.PhysicalFile.deleteIfNecessary(PhysicalFile.java:155)
	at org.apache.flink.runtime.checkpoint.filemerging.PhysicalFile.decRefCount(PhysicalFile.java:141)
	at org.apache.flink.runtime.checkpoint.filemerging.LogicalFile.discardWithCheckpointId(LogicalFile.java:118)
	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.discardSingleLogicalFile(FileMergingSnapshotManagerBase.java:574)
	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.discardLogicalFiles(FileMergingSnapshotManagerBase.java:588)
	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.notifyCheckpointAborted(FileMergingSnapshotManagerBase.java:490)
	at org.apache.flink.runtime.checkpoint.filemerging.WithinCheckpointFileMergingSnapshotManager.notifyCheckpointAborted(WithinCheckpointFileMergingSnapshotManager.java:61)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyFileMergingSnapshotManagerCheckpoint(SubtaskCheckpointCoordinatorImpl.java:505)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:490)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointAborted(SubtaskCheckpointCoordinatorImpl.java:414)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$21(StreamTask.java:1513)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$23(StreamTask.java:1536)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:998)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:923)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:966)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:945)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:573)
	at java.lang.Thread.run(Thread.java:750)
{code}",,,,,,,,,,,,,,,,,,,FLINK-35457,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 23:56:56 UTC 2024,,,,,,,,,,"0|z1pd4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 23:56;robyoung;looks like the same issue as https://issues.apache.org/jira/browse/FLINK-35457 being addressed in https://github.com/apache/flink/pull/24846;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager and TaskManager support merging and run in a single process,FLINK-35417,13580053,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jeyhunkarimov,melin,melin,22/May/24 05:52,24/May/24 01:29,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,"flink is widely used in data integration scenarios, where a single concurrency is not high, and in many cases a single concurrency can run a task. Consider the high availability, application mode, and large number of JobManger nodes that cost a lot of resources. If the Session mode is used, the stability is not high.
In application mode, JobManager and TaskManager can be run together to achieve reliability and save resources.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 18:29:38 UTC 2024,,,,,,,,,,"0|z1pcsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/24 07:20;martijnvisser;Per https://flink.apache.org/how-to-contribute/contribute-code/#consensus this should first be a discussion on the Dev mailing list before opening a Jira ticket. ;;;","22/May/24 09:00;xtsong;Hi [~melin],

Please help me understand. Are you willing to make a contribution on the proposed feature? Or are you only expressing a demand but has no intention to work on it?

If you'd like to contribute, I agree with [~martijnvisser] that this is a bit too big for a Jira ticket. Please propose this on the mailing list. My gut feeling this would introduce a new deployment mode, as well as public interface changes, which means it needs to go through a FLIP process.

If you're only making a demand, then I think this is a valid demand. We have similar observations in our production, and I have even thought about similar ideas. Unfortunately, I haven't yet find enough time and resources to work on it.;;;","22/May/24 13:37;jeyhunkarimov;Hi [~melin], I echo the comments above. In case, you are not planning to work on this, I can spend some cycles to drive this issue.;;;","22/May/24 15:56;melin;The current deployment model has encountered challenges in many projects. I just want to send an issue to discuss, perhaps a mailing list is more appropriate.
There is currently no ability to implement new deployment patterns. [~jeyhunkarimov] You are welcome to drive this issue. Thank you very much!;;;","22/May/24 18:29;jingge;[~jeyhunkarimov] assigned you, please pay attention to comments above, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,
Weekly CI for ElasticSearch connector failed to compile,FLINK-35416,13580049,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,22/May/24 04:26,22/May/24 07:06,04/Jun/24 20:40,22/May/24 07:06,,,,,,,,,elasticsearch-3.0.0,,,Connectors / ElasticSearch,,,,0,pull-request-available,,"ElasticsearchSinkBaseITCase.java:[31,65] package org.apache.flink.shaded.guava30.com.google.common.collect does not exist",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 07:06:22 UTC 2024,,,,,,,,,,"0|z1pcs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/24 07:06;Weijie Guo;v3.0 via 83cc4294c79abc18f1be2ce36e72bc87e26c026e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC Fails to create sink with Flink 1.19,FLINK-35415,13580044,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xiqian_yu,xiqian_yu,xiqian_yu,22/May/24 01:28,29/May/24 06:48,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Currently, Flink CDC doesn't work with Flink 1.19 with the following exception:

Exception in thread ""main"" java.lang.NoSuchMethodError: 'void org.apache.flink.streaming.runtime.operators.sink.CommitterOperatorFactory.<init>(org.apache.flink.api.connector.sink2.TwoPhaseCommittingSink, boolean, boolean)'

The reason is Flink CDC uses Flink @Internal API and it was changed in 1.19 update.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-22 01:28:37.0,,,,,,,,,,"0|z1pcqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancel jobs through rest api for last-state upgrades,FLINK-35414,13579995,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gyfora,gyfora,gyfora,21/May/24 14:51,21/May/24 14:51,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,"The kubernetes operator currently always deletes the JM deployment directly during last-state upgrades instead of attempting any type of graceful shutdown.

We could improve the last-state upgrade logic to cancel the job in cases where the JM is healthy and then simply extract the last checkpoint info through the rest api like we already do for terminal job states.

This would allow the last-state upgrade mode to work even for session jobs and this may even eliminate a few corner cases that can result from the current forceful upgrade mechanism. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-21 14:51:56.0,,,,,,,,,,"0|z1pcg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VertexFinishedStateCheckerTest causes exit 239,FLINK-35413,13579976,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,21/May/24 13:38,03/Jun/24 01:41,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,test-stability,,"1.20 test_cron_azure core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59676&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9429

{code}
May 21 01:31:42 01:31:42.160 [ERROR] org.apache.flink.runtime.checkpoint.VertexFinishedStateCheckerTest
May 21 01:31:42 01:31:42.160 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
May 21 01:31:42 01:31:42.160 [ERROR] Command was /bin/sh -c cd '/__w/1/s/flink-runtime' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.lang=ALL-UNNAMED' '--add-opens=java.base/java.net=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' '-Xmx768m' '-jar' '/__w/1/s/flink-runtime/target/surefire/surefirebooter-20240521011847857_99.jar' '/__w/1/s/flink-runtime/target/surefire' '2024-05-21T01-15-09_325-jvmRun1' 'surefire-20240521011847857_97tmp' 'surefire_29-20240521011847857_98tmp'
May 21 01:31:42 01:31:42.160 [ERROR] Error occurred in starting fork, check output in log
May 21 01:31:42 01:31:42.160 [ERROR] Process Exit Code: 239
May 21 01:31:42 01:31:42.160 [ERROR] Crashed tests:
May 21 01:31:42 01:31:42.160 [ERROR] org.apache.flink.runtime.checkpoint.VertexFinishedStateCheckerTest
May 21 01:31:42 01:31:42.160 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456)
May 21 01:31:42 01:31:42.160 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:358)
May 21 01:31:42 01:31:42.160 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:296)
May 21 01:31:42 01:31:42.160 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:250)
May 21 01:31:42 01:31:42.160 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1240)
May 21 01:31:42 01:31:42.160 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1089)
May 21 01:31:42 01:31:42.160 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:905)
May 21 01:31:42 01:31:42.160 [ERROR] 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
{code}

In the build artifact {{mvn-1.log}} the following FATAL error is found:

{code}
01:19:08,584 [     pool-9-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'pool-9-thread-1' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@5ead9062 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4d0e55ac[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:851) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.handleAsync(CompletableFuture.java:2178) ~[?:1.8.0_292]
	at org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer.allocateSlot(DefaultSlotStatusSyncer.java:138) ~[classes/:?]
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.allocateSlotsAccordingTo(FineGrainedSlotManager.java:722) ~[classes/:?]
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.checkResourceRequirements(FineGrainedSlotManager.java:645) ~[classes/:?]
	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.lambda$null$12(FineGrainedSlotManager.java:603) ~[classes/:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@5ead9062 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4d0e55ac[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_292]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622) ~[?:1.8.0_292]
	at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:826) ~[?:1.8.0_292]
	... 14 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 01:41:44 UTC 2024,,,,,,,,,,"0|z1pcbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/24 01:41;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=60006&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=11070;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch execution of async state request callback,FLINK-35412,13579944,13574083,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,21/May/24 10:22,28/May/24 10:28,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,Runtime / Task,,,0,pull-request-available,,There is one mail for each callback when async state result returns. One possible optimization is to encapsulate multiple callbacks into one mail.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-21 10:22:43.0,,,,,,,,,,"0|z1pc4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize wait logic in draining of async state requests,FLINK-35411,13579942,13574083,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yanfei Lei,zakelly,zakelly,21/May/24 10:20,24/May/24 15:48,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,Runtime / Task,,,0,,,"Currently during draining of async state requests, the task thread performs {{Thread.sleep}} to avoid cpu overhead when polling mails. This can be optimized by wait & notify.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 15:48:25 UTC 2024,,,,,,,,,,"0|z1pc48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/24 06:04;spoon-lz;[~zakelly] I saw that this issue has not been assigned yet. The whole FLIP-425 seems to be a very complicated and difficult work. I also want to try to participate in it.;;;","23/May/24 04:49;zakelly;[~spoon-lz] Actually this is found by some our internal benchmarks and [~Yanfei Lei]  have made a draft resolving this, still validating.... I forgot to assign this, sorry about that. And we have more work to do which I think maybe you are interested in getting involved. ;;;","23/May/24 06:50;spoon-lz;[~zakelly] I am very interested in changes related to FLIP-423. If you have a suitable issue, please call me.;;;","24/May/24 15:48;zakelly;[~spoon-lz] sure, will do. Thanks for your volunteering.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Avoid sync waiting in coordinator thread of ForSt executor,FLINK-35410,13579941,13574085,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,21/May/24 10:15,23/May/24 07:07,04/Jun/24 20:40,23/May/24 07:07,,,,,,,,,2.0.0,,,,,,,0,pull-request-available,,"Currently, the coordinator thread of ForSt executor will sync wait the state access result, which can be optimized.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 07:06:51 UTC 2024,,,,,,,,,,"0|z1pc40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/24 07:06;zakelly;Merged into master via 77111f7d33d64005a583dce4fbb72ab8e19f7a99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Request more splits if all splits are filtered from addSplits method,FLINK-35409,13579940,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,ShawnHx,ShawnHx,ShawnHx,21/May/24 10:14,30/May/24 03:57,04/Jun/24 20:40,30/May/24 03:57,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Suppose this scenario: A job is still in the snapshot phase, and the remaining uncompleted snapshot splits all belong to a few tables that have been deleted by the user.

In such case, when restarting from a savepoint, these uncompleted snapshot splits will not trigger a call to the addSplits method. Moreover, since the BinlogSplit has not been sent yet, the job will not start the SplitReader to read data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 03:57:53 UTC 2024,,,,,,,,,,"0|z1pc3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 03:57;leonard;master via: 19c382c0f2f2736baeb1c652130cb30dd98ca90c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add 30 min tolerance value when validating the time-zone setting,FLINK-35408,13579934,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,ShawnHx,ShawnHx,ShawnHx,21/May/24 09:54,03/Jun/24 08:42,04/Jun/24 20:40,27/May/24 11:06,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Now, MySQL CDC connector will retrieve the offset seconds between the configured timezone and UTC by executing the SQL statement below, and then compare it with the configured timezone.
{code:java}
SELECT TIME_TO_SEC(TIMEDIFF(NOW(), UTC_TIMESTAMP())) {code}
For some MySQL instances, the validating for time-zone is too strict. We can add 30min tolerance value.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 09:17:47 UTC 2024,,,,,,,,,,"0|z1pc2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/24 11:07;davidradl;some feedback:
 * I wonder why you need a 30 minute tolerance for mySQL time zone setting for CDC?
 * you have no unit tests in your pr.
 * I notice [Australia|https://en.wikipedia.org/wiki/Australia]: [Lord Howe Island|https://en.wikipedia.org/wiki/Lord_Howe_Island] has  30 minute DSL - will this work? 
 * as this is an improvement - a Flip is the usual process - is there a reason you are not raising a Flip for this.

 

 ;;;","27/May/24 11:03;leonard;[~davidradl] I think a FLIP is not necessary as this minor improvement doesn’t influence users' API. The improvement is pretty useful when validate timezone, 30 minutes is the best value for the timezone tolerance as the granularity of the time zone is hours. ;;;","27/May/24 11:05;leonard;I merged the PR after saw your comments [~davidradl]，and I agree that tests should be encouraged in this PR， CC [~ShawnHx].

;;;","27/May/24 11:06;leonard;Implemented in 
master: b2cb30f2406502b5672f8ed8a3a8eaef1d6d6a07
3.1: 536f707eb783f9f1429bf51fd3ce09addffaefe9;;;","27/May/24 11:14;ShawnHx;[~davidradl] I have provided the background for this pr in description. I'll try to add some tests, thanks for your remind.;;;","28/May/24 09:17;davidradl;[~leonard] [~ShawnHx] thanks 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,
 ensure orderly transactions during the full consumption stage of MySQL CDC,FLINK-35407,13579917,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,xiaotouming,xiaotouming,21/May/24 07:22,21/May/24 07:22,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,,,"Is there a way to ensure orderly transactions during the full consumption stage of MySQL CDC? During the snapshot consumption stage, new additions, deletions, and modifications do not cause any changes to the original snapshot data",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-21 07:22:52.0,,,,,,,,,,"0|z1pbyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use inner serializer when casting RAW type to BINARY or STRING in cast rules,FLINK-35406,13579909,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,docete,docete,21/May/24 06:16,31/May/24 04:15,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,"The generated code in RawToStringCastRule and RawToBinaryCastRule use 
BinaryRawValueData::toBytes and BinaryRawValueData::toObject to convert RawValueData(to java object or byte array), which should use inner serializer instead of RawValueDataSerializer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-21 06:16:19.0,,,,,,,,,,"0|z1pbx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add buffer size/in-flight records metrics for AsyncExecutionController,FLINK-35405,13579901,13574083,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Yanfei Lei,Yanfei Lei,21/May/24 04:27,21/May/24 04:27,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-21 04:27:15.0,,,,,,,,,,"0|z1pbvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Report metrics of KafkaConsumer in new Kafka source,FLINK-35404,13579880,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,renqs,hmittal83,hmittal83,20/May/24 20:51,20/May/24 21:36,04/Jun/24 20:40,,1.13.0,,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,"Currently new Kafka source only registers metrics that are available on KafkaPartitionSplitReader initialization. However, there are metrics added later in the lifecycle like consumer lag metrics that are missing from the KafkaConsumer metric group.

 ",,,,,,,,,,,,,,,,,FLINK-22766,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 20 20:54:33 UTC 2024,,,,,,,,,,"0|z1pbqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/24 20:54;hmittal83;A better way to solve this problem is to implement interface `MetricsReporter` from kafka metrics using a MetricGroup from apache-flink.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-452: Allow Skipping Invocation of Function Calls While Constant-folding,FLINK-35403,13579879,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,asheinberg,asheinberg,asheinberg,20/May/24 20:43,27/May/24 07:36,04/Jun/24 20:40,27/May/24 07:36,,,,,,,,,1.20.0,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 07:36:21 UTC 2024,,,,,,,,,,"0|z1pbqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 07:36;twalthr;Fixed in master: efcd146e0119cc7e67fea74cc46fbde08385de75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[flink-operator][Deployment] add labels to metadata,FLINK-35402,13579869,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,luismacosta,luismacosta,20/May/24 16:50,03/Jun/24 17:29,04/Jun/24 20:40,,,,,,,,,,,,,flink-contrib,,,,0,pull-request-available,,"Greetings dear team,

I would like to add labels to flink-operator Deployment metadata - https://github.com/apache/flink-kubernetes-operator/pull/829

Best regards,
Luís Costa",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-20 16:50:21.0,,,,,,,,,,"0|z1pbo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add SQS Table API support,FLINK-35401,13579861,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chalixar,chalixar,20/May/24 14:15,20/May/24 14:18,04/Jun/24 20:40,,,,,,,,,,aws-connector-4.4.0,,,Connectors / AWS,,,,0,,,"# Add Table API support for Amazon SQS sink as per [FLIP-481|https://cwiki.apache.org/confluence/display/FLINK/FLIP-438%3A+Amazon+SQS+Sink+Connector]",,,,,,,,,,,,FLINK-35305,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-20 14:15:45.0,,,,,,,,,,"0|z1pbmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebuild FileMergingSnapshotManager in failover,FLINK-35400,13579808,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,20/May/24 06:32,30/May/24 06:18,04/Jun/24 20:40,30/May/24 06:18,,,,,,,,,1.20.0,,,,,,,0,pull-request-available,,"Currently, the {{FileMergingSnapshotManager}} is released within {{{}releaseJobResources{}}}, which will not be invoked during failover and restore. However, the manager should be created again to clear all internal states in a new job attempt.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 06:18:36 UTC 2024,,,,,,,,,,"0|z1pbao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 06:18;zakelly;Merged into master via 54f037f9b2f08bd958d02eac2ffd018919b3f43e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documents for batch job master failure recovery,FLINK-35399,13579801,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,zhuzh,zhuzh,20/May/24 02:07,30/May/24 14:34,04/Jun/24 20:40,30/May/24 05:45,,,,,,,,,1.20.0,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 14:34:53 UTC 2024,,,,,,,,,,"0|z1pb94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 05:45;zhuzh;aabfd6e2eaa37d554632129c959a309f85d528c5;;;","30/May/24 14:34;zhuzh;56cd9607713d0da874dcc54c4cf6d5b3b52b1050 refined the doc a bit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayE2ECase failed to pull image,FLINK-35398,13579799,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,20/May/24 01:48,20/May/24 01:48,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Build System / CI,,,,0,,,"
{code:java}
Caused by: org.testcontainers.containers.ContainerFetchException: Failed to pull image: prestodb/hdp2.6-hive:10
May 17 14:02:23 	at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:119)
May 17 14:02:23 	at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:28)
May 17 14:02:23 	at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:19)
May 17 14:02:23 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:41)
May 17 14:02:23 	at org.testcontainers.containers.GenericContainer.getDockerImageName(GenericContainer.java:1406)
May 17 14:02:23 	... 30 more
May 17 14:02:23 Caused by: java.lang.InterruptedException
May 17 14:02:23 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
May 17 14:02:23 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
May 17 14:02:23 	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
May 17 14:02:23 	at com.github.dockerjava.api.async.ResultCallbackTemplate.awaitCompletion(ResultCallbackTemplate.java:91)
May 17 14:02:23 	at org.testcontainers.images.TimeLimitedLoggedPullImageResultCallback.awaitCompletion(TimeLimitedLoggedPullImageResultCallback.java:58)
May 17 14:02:23 	at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:90)
May 17 14:02:23 	... 34 more

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59631&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=16131
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-20 01:48:10.0,,,,,,,,,,"0|z1pb8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate Controller Flow documentation into Chinese,FLINK-35397,13579784,13573144,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,caicancai,caicancai,19/May/24 13:19,19/May/24 13:19,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,Translate Controller Flow documentation into Chinese,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-19 13:19:33.0,,,,,,,,,,"0|z1pb5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamoDB Streams Consumer consumption data is out of order,FLINK-35396,13579766,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Suxing Lee,Suxing Lee,19/May/24 05:51,19/May/24 17:07,04/Jun/24 20:40,19/May/24 17:07,1.16.2,aws-connector-4.2.0,,,,,,,,,,Connectors / AWS,Connectors / Kinesis,,19/May/24 00:00,0,AWS,,"When we use `FlinkDynamoDBStreamsConsumer` in `flink-connector-aws/flink-connector-kinesis` to consume dynamodb stream data, there is an out-of-order problem.
The service exception log is as follows:
{noformat}
2024-05-06 00:00:40,639 INFO  org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 has discovered a new shard StreamShardHandle{streamName='arn:aws:dynamodb:ap-southeast-1:***', shard='{ShardId: shardId-00000001714924828427-d73b6b68,
  ParentShardId: shardId-00000001714910797443-fb1d3b22,HashKeyRange: {StartingHashKey: 0,EndingHashKey: 1},SequenceNumberRange: {StartingSequenceNumber: 2958376400000000058201168012,}}'} due to resharding, and will start consuming the shard from sequence number EARLIEST_SEQUENCE_NUM with ShardConsumer 2807
......
......
......
2024-05-06 00:00:46,729 INFO  org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 has reached the end of subscribed shard: StreamShardHandle{streamName='arn:aws:dynamodb:ap-southeast-1:***', shard='{ShardId: shardId-00000001714910797443-fb1d3b22,ParentShardId: shardId-00000001714897099372-17932b9a,HashKeyRange: {StartingHashKey: 0,EndingHashKey: 1},SequenceNumberRange: {StartingSequenceNumber: 2955440900000000051102788386,}}'}
{noformat}

It looks like the failure process is:
`2024-05-06 00:00:40,639` A new shard is discovered and new sub-shards are consumed immediately.（ShardId: shardId-00000001714924828427-d73b6b68）.
`2024-05-06 00:00:46,729` Consume the old parent shard:（ShardId: shardId-00000001714910797443-fb1d3b22）end.

There was a gap of 6 seconds. In other words, before the data consumption of the parent shard has finished, the child shard has already started consuming data. This causes the data we read to be sent downstream out of order.

https://github.com/apache/flink-connector-aws/blob/c688a8545ac1001c8450e8c9c5fe8bbafa13aeba/flink-connector-aws/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java#L689-L740 

This is because the code immediately submits `ShardConsumer` to `shardConsumersExecutor` when `discoverNewShards` is created, and `shardConsumersExecutor` is created through Executors.newCachedThreadPool(), which does not limit the number of threads, causing new and old shards to be consumed at the same time , so data consumption is out of order?
`flink-connector-kinesis` relies on `dynamodb-streams-kinesis-adapter` to subscribe to messages from dynamodb stream. But why does `dynamodb-streams-kinesis-adapter` directly consume data without similar problems?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 19 17:07:21 UTC 2024,,,,,,,,,,"0|z1pb1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/24 10:22;chalixar;[~Suxing Lee] Thanks for reporting and the deep dive, I will try to take a look at it. ;;;","19/May/24 14:48;Suxing Lee;[~chalixar] Thanks for your reply.
`dynamodb-streams-kinesis-adapter` SDK use [ShardSyncer/ Leases|https://github.com/awslabs/dynamodb-streams-kinesis-adapter/blob/4fc3d79a0a0ea51b05175510b4cbd7daaa286587/src/main/java/com/amazonaws/services/dynamodbv2/streamsadapter/DynamoDBStreamsShardSyncer.java] to ensure that 
child shard does not start processing data until data from all its parents has been processed.
But this logic does not seem to be seen in `FlinkDynamoDBStreamsConsumer`. This may be the cause of the problem.;;;","19/May/24 16:19;hong;Hi [~Suxing Lee], thanks for reporting this problem. This issue is not the fault of the DDBStreams adapter, but rather a limitation of the FlinkKinesisConsumer itself, where parent-child shard ordering is not respected. See associated Jira FLINK-6349.

In flink-connector-aws/flink-connector-kinesis, the FlinkDynamoDBStreamsConsumer uses the same framework as the FlinkKinesisConsumer, but the Kinesis client is swapped out to DynamoDB streams client using the dynamodb-streams-kinesis-adapter. Since the base logic of FlinkKinesisConsumer doesn't respect parent-child shard ordering, it doesn't matter what the adapter does. The moment a shard is created, the Flink job will discover it and read from it.

We are working on a new DynamoDB Streams connector based on FLIP-27, which should resolve this issue.;;;","19/May/24 16:21;hong;Here is the Parent-Jira that will introduce the new DynamoDBStreams connector based on FLIP-27 Source APIs, https://issues.apache.org/jira/browse/FLINK-24438

 

This is the Jira that will address parent-child shard ordering. https://issues.apache.org/jira/browse/FLINK-32218

 

If you are ok, I will resolve this as a duplicate to this JIRA! https://issues.apache.org/jira/browse/FLINK-32218;;;","19/May/24 17:07;Suxing Lee;Thanks for your reply. ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix KeyedStateStore class annotation error,FLINK-35395,13579740,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bradley,bradley,bradley,18/May/24 09:32,21/May/24 05:19,04/Jun/24 20:40,21/May/24 05:19,,,,,,,,,1.20.0,,,API / Core,,,,0,pull-request-available,,"The KeyedStateStore class is annotated incorrectly, and the examples there are obviously wrong and will mislead users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 05:19:38 UTC 2024,,,,,,,,,,"0|z1pavk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/24 05:19;Weijie Guo;master(1.20) via 5f5722fe916bafc00ec05539d6c19ac28ac1c5c4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
paimon pipeline support dynamic bucket,FLINK-35394,13579738,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,18/May/24 08:33,20/May/24 08:13,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,paimon pipeline support dynamic bucket,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 20 08:13:10 UTC 2024,,,,,,,,,,"0|z1pav4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/24 08:13;kwafor;Interested in this, please assign it to me, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support flink kafka catalog,FLINK-35393,13579737,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,melin,melin,18/May/24 08:15,20/May/24 12:22,04/Jun/24 20:40,20/May/24 12:22,,,,,,,,,,,,Connectors / Kafka,,,,0,,,Support for flink kafka catalog,,,,,,,,,,,,,,,,,,,FLINK-32766,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 20 12:04:21 UTC 2024,,,,,,,,,,"0|z1pauw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/24 12:04;chalixar;This is a great addition, However I would prefer to have a FLIP for this one! happy to coordinate about it or if you want to drive the FLIP;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Lookup Join on external database failing when using 'FOR SYSTEM_TIME AS OF' statement,FLINK-35392,13579672,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,glaubermd,glaubermd,17/May/24 13:47,17/May/24 13:56,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / Planner,,,,0,,,"When transforming a DataStream to a Table and then using that table in a Join Lookup, an exception is raised: {{{}""Temporal table join currently only supports 'FOR SYSTEM_TIME AS OF' left table's time attribute field""{}}}.

From what I understood from the issue, it happens as the planner expects that the Tables part of the Join are registered previously on the Catalog.

The solution was to avoid using the DataStream to Table conversion:

 

{{Table kinesisStreamTbl = tblEnv.fromDataStream(}}
{{                kinesisDataStream,}}
{{                Schema.newBuilder()}}
{{                        // ""proc_time"" would be needed to make the lookup using 'FOR SYSTEM_TIME AS OF'; see: [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/joins/#processing-time-temporal-join]}}
{{                        .columnByExpression(""proc_time"", ""PROCTIME()"")}}
{{                        .build()}}
{{        );}}

And instead create and use a Table registered on the Catalog:

 

{{tblEnv.createTable(""KinesisInput"", TableDescriptor.forConnector(""kinesis"")...);}}

 

If you need more details, feel free to contact me or you may access the discussion at: [https://stackoverflow.com/questions/78485099/sedona-flink-sql-lookup-on-external-database-failing-when-using-for-system-time]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 17 13:51:34 UTC 2024,,,,,,,,,,"0|z1pagw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/24 13:51;glaubermd;I see there is an old bug that might relate to this one: https://issues.apache.org/jira/browse/FLINK-23307;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump dependency of Paimon Pipeline connector to 0.8.0,FLINK-35391,13579653,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kunni,kunni,17/May/24 12:02,22/May/24 01:26,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Apache Paimon has [released 0.8|https://paimon.apache.org/releases/release-0.8/] recently, We can update dependencies to use new features. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 20 01:44:57 UTC 2024,,,,,,,,,,"0|z1paco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/24 01:27;kwafor;Interested in this, please assign it to me, thanks.;;;","20/May/24 01:44;kunni;Thanks for taking this, please go ahead, I am willing to help review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Error reporting in SQL Server connection table structure table in Flink CDC-3.0 version,FLINK-35390,13579625,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,灰度。。。,灰度。。。,17/May/24 08:59,24/May/24 09:17,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"I am currently using SQL Server connector. I have upgraded from CDC2.2 version to 3.0 version. If there is a change in the table structure of SQL Server, the online task will report an error, while the old version will not",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-17 08:59:21.0,,,,,,,,,,"0|z1pa6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement List Async State API for ForStStateBackend,FLINK-35389,13579624,13574085,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,masteryhx,masteryhx,17/May/24 08:57,17/May/24 08:57,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-17 08:57:51.0,,,,,,,,,,"0|z1pa68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.18.1 get stuck after creating highly available job result storage directory,FLINK-35388,13579613,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pedromazala,pedromazala,17/May/24 07:42,21/May/24 15:45,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Runtime / Checkpointing,,,,0,,,"After upgrading to Flink 1.18.1 (from 1.17.2) some pipelines get stuck after creating highly available job result storage directory.

I use GCS for storage and I did not find anything unusual in the logs. 


The pod on k8s stays unhealthy until a manual restart (kill JM pod)",,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/24 07:41;pedromazala;output2.json;https://issues.apache.org/jira/secure/attachment/13068923/output2.json",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 15:45:43 UTC 2024,,,,,,,,,,"0|z1pa3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/24 15:45;pedromazala;I found this https://stackoverflow.com/questions/78329856/flink-checkpoints-stalling-and-timing-out-with-latency-related-errors;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PG CDC source support heart beat,FLINK-35387,13579597,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,loserwang1024,loserwang1024,17/May/24 05:47,17/May/24 05:47,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,"Though, document of PG CDC [1] has heartbeat.interval.ms, but it's not valid. The reason is bellow.

In debezium dos says: For the connector to detect and process events from a heartbeat table, you must add the table to the PostgreSQL publication specified by the [publication.name|https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-property-publication-name] property. If this publication predates your Debezium deployment, the connector uses the publications as defined. If the publication is not already configured to automatically replicate changes {{FOR ALL TABLES}} in the database, you must explicitly add the heartbeat table to the publication[2].

Thus, if you want use heart beat in cdc:

1. add a heartbeat table to publication: ALTER PUBLICATION _<publicationName>_ ADD TABLE {_}<heartbeatTableName>{_};

2. set heartbeatInterval

3. add debezium.[{{heartbeat.action.query}}|https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-property-heartbeat-action-query] [3]

 

However, when I use it it CDC, some exception occurs:
{code:java}
Caused by: java.lang.NullPointerException
at io.debezium.heartbeat.HeartbeatFactory.createHeartbeat(HeartbeatFactory.java:55)
at io.debezium.pipeline.EventDispatcher.<init>(EventDispatcher.java:127)
at io.debezium.pipeline.EventDispatcher.<init>(EventDispatcher.java:94){code}
!https://alidocs.dingtalk.com/core/api/resources/img/5eecdaf48460cde5292b7c63c883d1620bbf7d3875a3a5b158e70b814913bc360a414d3de9277d871abf3af1cbd75249eddaaa1b37c2b2f5421a918fb1a2f0f3853c0ce41721e620699d98626fa2281948c58faa63edf8ebfc653b69905bac42?tmpCode=9193555a-7bf3-4335-9427-b59c1dfe1931!

 

It seems CDC don't add  a HeartbeatConnectionProvider  when configure PostgresEventDispatcher:
{code:java}
//org.apache.flink.cdc.connectors.postgres.source.fetch.PostgresSourceFetchTaskContext#configure
this.postgresDispatcher =
                new PostgresEventDispatcher<>(
                        dbzConfig,
                        topicSelector,
                        schema,
                        queue,
                        dbzConfig.getTableFilters().dataCollectionFilter(),
                        DataChangeEvent::new,
                        metadataProvider,
                        schemaNameAdjuster); {code}
in debezium, when PostgresConnectorTask start, it will  do it
{code:java}
//io.debezium.connector.postgresql.PostgresConnectorTask#start
  final PostgresEventDispatcher<TableId> dispatcher = new PostgresEventDispatcher<>(
                    connectorConfig,
                    topicNamingStrategy,
                    schema,
                    queue,
                    connectorConfig.getTableFilters().dataCollectionFilter(),
                    DataChangeEvent::new,
                    PostgresChangeRecordEmitter::updateSchema,
                    metadataProvider,
                    connectorConfig.createHeartbeat(
                            topicNamingStrategy,
                            schemaNameAdjuster,
                            () -> new PostgresConnection(connectorConfig.getJdbcConfig(), PostgresConnection.CONNECTION_GENERAL),
                            exception -> {
                                String sqlErrorId = exception.getSQLState();
                                switch (sqlErrorId) {
                                    case ""57P01"":
                                        // Postgres error admin_shutdown, see https://www.postgresql.org/docs/12/errcodes-appendix.html
                                        throw new DebeziumException(""Could not execute heartbeat action query (Error: "" + sqlErrorId + "")"", exception);
                                    case ""57P03"":
                                        // Postgres error cannot_connect_now, see https://www.postgresql.org/docs/12/errcodes-appendix.html
                                        throw new RetriableException(""Could not execute heartbeat action query (Error: "" + sqlErrorId + "")"", exception);
                                    default:
                                        break;
                                }
                            }),
                    schemaNameAdjuster,
                    signalProcessor); {code}
Thus, this jira will add this.

 

 [1] https://nightlies.apache.org/flink/flink-cdc-docs-master/docs/connectors/legacy-flink-cdc-sources/postgres-cdc/

[2] https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-property-heartbeat-interval-ms

[3] https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-property-heartbeat-action-query",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-17 05:47:44.0,,,,,,,,,,"0|z1pa08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Flink CDC 3.1 Documentation and mark it as stable,FLINK-35386,13579594,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gongzhongqiang,renqs,renqs,17/May/24 04:17,17/May/24 06:23,04/Jun/24 20:40,17/May/24 05:36,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,This is part of Flink CDC 3.1.0 release. We need to update the Github Action configuration to build release-3.1 doc and mark it as stable.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 17 05:35:55 UTC 2024,,,,,,,,,,"0|z1p9zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/24 05:35;renqs;flink-cdc master: 999027fbb1bc02109e8a79ac25dd2a43c0483f84

release-3.1: 4c0cafe9f4171f304ca9ed87d46e92c274ea5d88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrader flink dependency version to 1.19,FLINK-35385,13579588,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,loserwang1024,loserwang1024,17/May/24 02:01,22/May/24 04:14,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,"Flink 1.19 was released on 2024-03-18  and the connectors have not yet
caught up. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 04:14:10 UTC 2024,,,,,,,,,,"0|z1p9y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/24 01:15;xiqian_yu;Seems https://github.com/apache/flink-cdc/issues/3327 was caused by this since CommitterOperatorFactory API has changed in Flink 1.19.

[~renqs] I'd like to take this if needed.;;;","22/May/24 04:14;loserwang1024;We can ensure multi-version compatibility for the connector( which only rely on  Public class in flink), but the pipeline runtime cannot be compatible with multiple Flink versions simultaneously.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose TaskIOMetricGroup to custom Partitioner via init Context,FLINK-35384,13579564,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stevenz3wu,stevenz3wu,16/May/24 17:10,21/May/24 09:31,04/Jun/24 20:40,,1.9.4,,,,,,,,,,,Runtime / Metrics,,,,0,,,"I am trying to implement a custom range partitioner in the Flink Iceberg sink. Want to publish some counter metrics for certain scenarios. This is like the network metrics exposed in `TaskIOMetricGroup`.

We can implement the range partitioner using the public interface from `DataStream`. 
{code}
    public <K> DataStream<T> partitionCustom(
            Partitioner<K> partitioner, KeySelector<T, K> keySelector)
{code}

We can pass the `TaskIOMetricGroup` to the `StreamPartitioner` that `CustomPartitionerWrapper` extends from. `CustomPartitionerWrapper` wraps the pubic `Partitioner` interface, where we can implement the custom range partitioner.

`Partitioner` interface is a functional interface today. we can add a new default `setup` method without breaking the backward compatibility.
{code}
@Public
@FunctionalInterface
public interface Partitioner<K> extends java.io.Serializable, Function {
    *default void setup(TaskIOMetricGroup metrics) {}*
    int partition(K key, int numPartitions);
}
{code}

I know public interface requires a FLIP process. will do that if the community agree with this feature request.

Personally, `numPartitions` should be passed in the `setup` method too. But it is a breaking change that is NOT worth the benefit right now.
{code}
@Public
@FunctionalInterface
public interface Partitioner<K> extends java.io.Serializable, Function {
    public void setup(int numPartitions, TaskIOMetricGroup metrics) {}
    int partition(K key);
}
{code}

That would be similar to `StreamPartitioner#setup()` method that we would need to modify for passing the metrics group.
{code}
@Internal
public abstract class StreamPartitioner<T>
        implements ChannelSelector<SerializationDelegate<StreamRecord<T>>>, Serializable {
    @Override
    public void setup(int numberOfChannels, TaskIOMetricGroup metrics) {
        this.numberOfChannels = numberOfChannels;
    }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 09:31:07 UTC 2024,,,,,,,,,,"0|z1p9sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/24 00:00;stevenz3wu;one potential risk of this type of API is that it is not extensible. if we want to pass in another arg to partitioner, we need to break the compatibility or add a new method.
{code}
default void setup(TaskIOMetricGroup metrics) {}
{code}

Maybe we can move to the context model that is widely used in Flink
{code}
@Public
@FunctionalInterface
public interface Partitioner<K> extends java.io.Serializable, Function {
    int partition(K key, int numPartitions);

    void init(Context context) {}

    interface Context {
        int numberOfChannels();
        TaskIOMetricGroup metrics();
    }
}
{code}

;;;","21/May/24 09:31;zhuzh;Enabling metrics for partitioners makes sense and the proposed approach of introducing a context sounds good.

How about introducing a sub-metric group specifically for partitioner metrics? A single task might contain multiple partitioners for which the metrics should not get mixed. It also avoids exposing the internal TaskIOMetricGroup class to users.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Update compatibility matrix to include 1.19 release,FLINK-35383,13579562,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,16/May/24 16:47,03/Jun/24 07:44,04/Jun/24 20:40,21/May/24 15:11,1.19.0,,,,,,,,1.19.1,1.20.0,,Documentation,,,,0,pull-request-available,,"Update compatibility matrix in documentation to include Flink 1.19 release:

https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/ops/upgrading/#compatibility-table

https://nightlies.apache.org/flink/flink-docs-release-1.19/zh/docs/ops/upgrading/#%E5%85%BC%E5%AE%B9%E6%80%A7%E9%80%9F%E6%9F%A5%E8%A1%A8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 15:10:55 UTC 2024,,,,,,,,,,"0|z1p9sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/24 15:10;hong;merged commit [{{8b73ca9}}|https://github.com/apache/flink/commit/8b73ca955bcd796916597046dd8aa80407d0aa07] into   apache:master;;;","21/May/24 15:10;hong;merged commit [{{c4af0c3}}|https://github.com/apache/flink/commit/c4af0c388d07316282819dc2741fa7bc758fa767] into   apache:release-1.19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogCompatibilityITCase.testRestore fails with an NPE,FLINK-35382,13579559,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lijinzhong,rskraba,rskraba,16/May/24 16:11,21/May/24 14:05,04/Jun/24 20:40,21/May/24 02:34,1.20.0,,,,,,,,,,,,,,,0,pull-request-available,test-stability,"* 1.20 Java 8 / Test (module: tests) https://github.com/apache/flink/actions/runs/9110398985/job/25045798401#step:10:8192

It looks like there can be a [NullPointerException at this line|https://github.com/apache/flink/blob/9a5a99b1a30054268bbde36d565cbb1b81018890/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/filemerging/FileMergingSnapshotManagerBase.java#L666] causing a test failure:

{code}
Error: 10:36:23 10:36:23.312 [ERROR] Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.31 s <<< FAILURE! -- in org.apache.flink.test.state.ChangelogCompatibilityITCase
Error: 10:36:23 10:36:23.313 [ERROR] org.apache.flink.test.state.ChangelogCompatibilityITCase.testRestore[startWithChangelog=false, restoreWithChangelog=true, restoreFrom=CHECKPOINT, allowStore=true, allowRestore=true] -- Time elapsed: 1.492 s <<< ERROR!
May 16 10:36:23 java.lang.RuntimeException: org.opentest4j.AssertionFailedError: Graph is in globally terminal state (FAILED)
May 16 10:36:23 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.tryRun(ChangelogCompatibilityITCase.java:204)
May 16 10:36:23 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.restoreAndValidate(ChangelogCompatibilityITCase.java:190)
May 16 10:36:23 	at java.util.Optional.ifPresent(Optional.java:159)
May 16 10:36:23 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.testRestore(ChangelogCompatibilityITCase.java:118)
May 16 10:36:23 	at java.lang.reflect.Method.invoke(Method.java:498)
May 16 10:36:23 Caused by: org.opentest4j.AssertionFailedError: Graph is in globally terminal state (FAILED)
May 16 10:36:23 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:42)
May 16 10:36:23 	at org.junit.jupiter.api.Assertions.fail(Assertions.java:150)
May 16 10:36:23 	at org.apache.flink.runtime.testutils.CommonTestUtils.lambda$waitForAllTaskRunning$3(CommonTestUtils.java:214)
May 16 10:36:23 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
May 16 10:36:23 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
May 16 10:36:23 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:209)
May 16 10:36:23 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:182)
May 16 10:36:23 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.submit(ChangelogCompatibilityITCase.java:284)
May 16 10:36:23 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.tryRun(ChangelogCompatibilityITCase.java:197)
May 16 10:36:23 	... 4 more
May 16 10:36:23 Caused by: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
May 16 10:36:23 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:219)
May 16 10:36:23 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailureAndReport(ExecutionFailureHandler.java:166)
May 16 10:36:23 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:121)
May 16 10:36:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:279)
May 16 10:36:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:270)
May 16 10:36:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:263)
May 16 10:36:23 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:788)
May 16 10:36:23 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:765)
May 16 10:36:23 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
May 16 10:36:23 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:496)
May 16 10:36:23 	at java.lang.reflect.Method.invoke(Method.java:498)
May 16 10:36:23 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:318)
May 16 10:36:23 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
May 16 10:36:23 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:316)
May 16 10:36:23 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:229)
May 16 10:36:23 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:88)
May 16 10:36:23 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:174)
May 16 10:36:23 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
May 16 10:36:23 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
May 16 10:36:23 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
May 16 10:36:23 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
May 16 10:36:23 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
May 16 10:36:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
May 16 10:36:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
May 16 10:36:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
May 16 10:36:23 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
May 16 10:36:23 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
May 16 10:36:23 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
May 16 10:36:23 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
May 16 10:36:23 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
May 16 10:36:23 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
May 16 10:36:23 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
May 16 10:36:23 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
May 16 10:36:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
May 16 10:36:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
May 16 10:36:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
May 16 10:36:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
May 16 10:36:23 Caused by: java.lang.NullPointerException: java.lang.NullPointerException
May 16 10:36:23 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.isManagedByFileMergingManager(FileMergingSnapshotManagerBase.java:666)
May 16 10:36:23 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.lambda$null$3(FileMergingSnapshotManagerBase.java:620)
May 16 10:36:23 	at java.util.HashMap.computeIfAbsent(HashMap.java:1128)
May 16 10:36:23 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.lambda$restoreStateHandles$4(FileMergingSnapshotManagerBase.java:616)
May 16 10:36:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
May 16 10:36:23 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
May 16 10:36:23 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
May 16 10:36:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
May 16 10:36:23 	at java.util.stream.Streams$StreamBuilderImpl.forEachRemaining(Streams.java:419)
May 16 10:36:23 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
May 16 10:36:23 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
May 16 10:36:23 	at java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:901)
May 16 10:36:23 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
May 16 10:36:23 	at java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)
May 16 10:36:23 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
May 16 10:36:23 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
May 16 10:36:23 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
May 16 10:36:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
May 16 10:36:23 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
May 16 10:36:23 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
May 16 10:36:23 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
May 16 10:36:23 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
May 16 10:36:23 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
May 16 10:36:23 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
May 16 10:36:23 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
May 16 10:36:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
May 16 10:36:23 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
May 16 10:36:23 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
May 16 10:36:23 	at org.apache.flink.runtime.checkpoint.filemerging.FileMergingSnapshotManagerBase.restoreStateHandles(FileMergingSnapshotManagerBase.java:613)
May 16 10:36:23 	at org.apache.flink.runtime.checkpoint.filemerging.SubtaskFileMergingManagerRestoreOperation.restore(SubtaskFileMergingManagerRestoreOperation.java:98)
May 16 10:36:23 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.registerRestoredStateToFileMergingManager(StreamTaskStateInitializerImpl.java:353)
May 16 10:36:23 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:163)
May 16 10:36:23 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:267)
May 16 10:36:23 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
May 16 10:36:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreStateAndGates(StreamTask.java:851)
May 16 10:36:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$restoreInternal$4(StreamTask.java:805)
May 16 10:36:23 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
May 16 10:36:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:805)
May 16 10:36:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:764)
May 16 10:36:23 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:960)
May 16 10:36:23 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:929)
May 16 10:36:23 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:753)
May 16 10:36:23 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568)
May 16 10:36:23 	at java.lang.Thread.run(Thread.java:750)
May 16 10:36:23 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 14:05:42 UTC 2024,,,,,,,,,,"0|z1p9rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 16:13;rskraba;[~lijinzhong] Do you think this is related to the changes made in FLINK-32080?;;;","17/May/24 12:26;rskraba;* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9123384110/job/25085945877#step:10:8789;;;","20/May/24 01:54;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59635&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8656;;;","20/May/24 05:30;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59655&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9500;;;","20/May/24 07:28;lijinzhong;1. Because FLINK-32085 has not been resolved, the changelog StateBackend still do not support snapshot-file-merging(FLIP_306) at this time.

So the FILE_MERGING_ENABLED config should not be enabled for the ChangelogCompatibilityITCase yet.


2.In FLINK-32092, the FILE_MERGING_ENABLED config is randomly enabled in TestStreamEnvironment, and try to disable it in ChangelogCompatibilityITCase, but it does not actually work. 

 

I will fix it by disable FILE_MERGING_ENABLED explicitly in ChangelogCompatibilityITCase.;;;","21/May/24 02:33;Yanfei Lei;Merged into master via 26b149a.

 

Let's observe for a while and then close it.;;;","21/May/24 14:05;rskraba;Thanks for the fix!  I'm just noting this build failure from 3 days ago (doesn't include the fix yet):

* 1.20 Java 11 / Test (module: tests) https://github.com/apache/flink/actions/runs/9136523142/job/25125588800#step:10:8741;;;",,,,,,,,,,,,,,,,,,,,,,
LocalRecoveryITCase failure on deleting directory,FLINK-35381,13579545,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,16/May/24 14:58,16/May/24 14:58,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,"* 1.20 Java 11 / Test (module: tests) https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54856&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11288
F

It looks like some resources in a subdirectory of a JUnit4 {{ClassRule}} temp directory prevent it from being cleaned up.  This was fixed in a different test in FLINK-33641.

{code}
SEVERE: Caught exception while closing extension context: org.junit.jupiter.engine.descriptor.MethodExtensionContext@2fc91366
java.io.IOException: Failed to delete temp directory /tmp/junit7935976901063386613. The following paths could not be deleted (see suppressed exceptions for details): tm_taskManager_0/localState/aid_1501e77149be2f931eab0a6c2e818f81/jid_fe61a39afa9873389353abb8bfbfba66/vtx_0a448493b4782967b150582570326227_sti_0, tm_taskManager_0/localState/aid_1501e77149be2f931eab0a6c2e818f81/jid_fe61a39afa9873389353abb8bfbfba66/vtx_bc764cd8ddf7a0cff126f51c16239658_sti_0/chk_51
	at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.createIOExceptionWithAttachedFailures(TempDirectory.java:431)
	at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:312)
	at org.junit.jupiter.engine.descriptor.AbstractExtensionContext.lambda$static$0(AbstractExtensionContext.java:45)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore$EvaluatedValue.close(NamespacedHierarchicalStore.java:333)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore$EvaluatedValue.access$800(NamespacedHierarchicalStore.java:317)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore.lambda$close$3(NamespacedHierarchicalStore.java:98)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore.lambda$close$4(NamespacedHierarchicalStore.java:98)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at java.base/java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:395)
	at java.base/java.util.stream.Sink$ChainedReference.end(Sink.java:258)
	at java.base/java.util.stream.Sink$ChainedReference.end(Sink.java:258)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
	at org.junit.platform.engine.support.store.NamespacedHierarchicalStore.close(NamespacedHierarchicalStore.java:98)
	at org.junit.jupiter.engine.descriptor.AbstractExtensionContext.close(AbstractExtensionContext.java:87)
	at org.junit.jupiter.engine.execution.JupiterEngineExecutionContext.close(JupiterEngineExecutionContext.java:53)
	at org.junit.jupiter.engine.descriptor.JupiterTestDescriptor.cleanUp(JupiterTestDescriptor.java:224)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$cleanUp$1(TestMethodTestDescriptor.java:156)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.cleanUp(TestMethodTestDescriptor.java:156)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.cleanUp(TestMethodTestDescriptor.java:69)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$cleanUp$10(NodeTestTask.java:167)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.cleanUp(NodeTestTask.java:167)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:98)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:202)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:146)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:202)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:146)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:202)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
	Suppressed: java.nio.file.NoSuchFileException: /tmp/junit7935976901063386613/tm_taskManager_0/localState/aid_1501e77149be2f931eab0a6c2e818f81/jid_fe61a39afa9873389353abb8bfbfba66/vtx_0a448493b4782967b150582570326227_sti_0
		at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
		at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
		at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
		at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
		at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
		at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
		at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
		at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
		at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
		at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:373)
		at java.base/java.nio.file.Files.walkFileTree(Files.java:2761)
		at java.base/java.nio.file.Files.walkFileTree(Files.java:2797)
		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.deleteAllFilesAndDirectories(TempDirectory.java:329)
		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:310)
		... 56 more
		Suppressed: java.nio.file.NoSuchFileException: /tmp/junit7935976901063386613/tm_taskManager_0/localState/aid_1501e77149be2f931eab0a6c2e818f81/jid_fe61a39afa9873389353abb8bfbfba66/vtx_0a448493b4782967b150582570326227_sti_0
			at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
			at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
			at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
			at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:249)
			at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:105)
			at java.base/java.nio.file.Files.delete(Files.java:1142)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.resetPermissionsAndTryToDeleteAgain(TempDirectory.java:382)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:342)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:329)
			at java.base/java.nio.file.Files.walkFileTree(Files.java:2727)
			... 59 more
	Suppressed: java.nio.file.NoSuchFileException: /tmp/junit7935976901063386613/tm_taskManager_0/localState/aid_1501e77149be2f931eab0a6c2e818f81/jid_fe61a39afa9873389353abb8bfbfba66/vtx_bc764cd8ddf7a0cff126f51c16239658_sti_0/chk_51
		at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
		at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
		at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
		at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
		at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
		at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
		at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
		at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
		at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
		at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:373)
		at java.base/java.nio.file.Files.walkFileTree(Files.java:2761)
		at java.base/java.nio.file.Files.walkFileTree(Files.java:2797)
		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.deleteAllFilesAndDirectories(TempDirectory.java:329)
		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:310)
		... 56 more
		Suppressed: java.nio.file.NoSuchFileException: /tmp/junit7935976901063386613/tm_taskManager_0/localState/aid_1501e77149be2f931eab0a6c2e818f81/jid_fe61a39afa9873389353abb8bfbfba66/vtx_bc764cd8ddf7a0cff126f51c16239658_sti_0/chk_51
			at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
			at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
			at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
			at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:249)
			at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:105)
			at java.base/java.nio.file.Files.delete(Files.java:1142)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.resetPermissionsAndTryToDeleteAgain(TempDirectory.java:382)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:342)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:329)
			at java.base/java.nio.file.Files.walkFileTree(Files.java:2727)
			... 59 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-16 14:58:03.0,,,,,,,,,,"0|z1p9oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResumeCheckpointManuallyITCase hanging on tests ,FLINK-35380,13579543,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,16/May/24 14:51,27/May/24 15:16,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,test-stability,,"* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9105407291/job/25031170942#step:10:11841 

(This is a slightly different error, waiting in a different place than FLINK-28319)

{code}
May 16 03:23:58 ==============================================================================
May 16 03:23:58 Process produced no output for 900 seconds.
May 16 03:23:58 ==============================================================================

... snip until stack trace ...

ay 16 03:23:58 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
May 16 03:23:58 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
May 16 03:23:58 	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
May 16 03:23:58 	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.runJobAndGetExternalizedCheckpoint(ResumeCheckpointManuallyITCase.java:410)
May 16 03:23:58 	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedCheckpoints(ResumeCheckpointManuallyITCase.java:378)
May 16 03:23:58 	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedCheckpoints(ResumeCheckpointManuallyITCase.java:318)
May 16 03:23:58 	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedFullRocksDBCheckpointsWithLocalRecoveryStandalone(ResumeCheckpointManuallyITCase.java:133)
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28319,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 15:16:39 UTC 2024,,,,,,,,,,"0|z1p9o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 16:15;rskraba;* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9108427134/job/25040554694#step:10:12099;;;","17/May/24 12:26;rskraba;* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9121965925/job/25082216099#step:10:9953
* 1.20 test_cron_jdk21 tests https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59617&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=12051
* 1.20 test_cron_adaptive_scheduler tests https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59617&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=13040;;;","21/May/24 14:03;rskraba;* 1.20 Java 11 / Test (module: tests) https://github.com/apache/flink/actions/runs/9167756989/job/25205690872#step:10:11589
* 1.20 Java 21 / Test (module: tests) https://github.com/apache/flink/actions/runs/9152485864/job/25160249723#step:10:9091
* 1.20 Java 17 / Test (module: tests) https://github.com/apache/flink/actions/runs/9144334458/job/25142210330#step:10:9094
* 1.20 AdaptiveScheduler / Test (module: tests) https://github.com/apache/flink/actions/runs/9144334458/job/25142199866#step:10:11729
* 1.20 AdaptiveScheduler / Test (module: tests) https://github.com/apache/flink/actions/runs/9136523142/job/25125573321#step:10:11731;;;","22/May/24 12:45;rskraba;* 1.20 Java 21 / Test (module: tests) https://github.com/apache/flink/actions/runs/9184288079/job/25256625597#step:10:9284;;;","23/May/24 12:59;rskraba;* 1.20 Hadoop 3.1.3 / Test (module: tests) https://github.com/apache/flink/actions/runs/9201159914/job/25309205615#step:10:12158;;;","27/May/24 15:16;rskraba;* 1.20 Java 21 / Test (module: tests) https://github.com/apache/flink/actions/runs/9239908683/job/25419736576#step:10:11668
* 1.20 Hadoop 3.1.3 / Test (module: tests) https://github.com/apache/flink/actions/runs/9239908683/job/25419763729#step:10:12152;;;",,,,,,,,,,,,,,,,,,,,,,,
File merging manager is not properly notified about checkpoint,FLINK-35379,13579522,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,16/May/24 13:01,30/May/24 06:19,04/Jun/24 20:40,30/May/24 06:19,,,,,,,,,1.20.0,,,Runtime / Checkpointing,,,,0,pull-request-available,,"Currently, the \{{FileMergingSnapshotManager}} from checkpoint file merging mechanism is not properly notified about checkpoint, and it does not handle the notifications properly as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 06:19:27 UTC 2024,,,,,,,,,,"0|z1p9jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 06:19;zakelly;Merged into master via 2bcc308d3600b429416b01e95159c360dafcb0ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-453] Promote Unified Sink API V2 to Public and Deprecate SinkFunc,FLINK-35378,13579513,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,martijnvisser,martijnvisser,martijnvisser,16/May/24 12:10,23/May/24 04:12,04/Jun/24 20:40,,,,,,,,,,,,,API / Core,,,,0,pull-request-available,,https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=303794871&draftShareId=af4ace88-98b7-4a53-aece-cd67d2f91a15&,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-16 12:10:47.0,,,,,,,,,,"0|z1p9hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debezium-json support soft deletion.,FLINK-35377,13579494,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yintongkai,yintongkai,16/May/24 09:37,16/May/24 09:58,04/Jun/24 20:40,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,"When we use kafka to do some real-time synchronization tasks, sometimes our business scenarios need to use soft delete instead of hard delete. Our data is more connected to kafka, so we want debezium-json to support soft deletion.Based on this, we modified the flink-json module.",,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/24 09:56;yintongkai;20240516175523.jpg;https://issues.apache.org/jira/secure/attachment/13068900/20240516175523.jpg","16/May/24 09:57;yintongkai;20240516175543.jpg;https://issues.apache.org/jira/secure/attachment/13068901/20240516175543.jpg",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 09:58:38 UTC 2024,,,,,,,,,,"0|z1p9dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 09:58;yintongkai;{code:java}
//代码占位符
set 'execution.checkpointing.interval'='10s';
SET sql-client.execution.result-mode=tableau; 
CREATE TABLE user_scores1
(
    `id`         INT,
    `uid`        INT,
    `score`      INT
) WITH (
'connector' = 'kafka',
'topic' = 'topic_user_scores',
'properties.bootstrap.servers' = 'hdp04:6667',
'properties.group.id' = 'user_scores',
'scan.startup.mode' = 'earliest-offset',
'format' = 'debezium-json'
);
select * from user_scores1;{code}
!20240516175523.jpg|width=693,height=296!
{code:java}
//代码占位符
set 'execution.checkpointing.interval'='10s';
SET sql-client.execution.result-mode=tableau;
CREATE TABLE user_scores
(
    `id`         INT,
    `uid`        INT,
    `score`      INT,
    `is_delete`  INT
) WITH (
'connector' = 'kafka',
'topic' = 'topic_user_scores',
'properties.bootstrap.servers' = 'hdp04:6667',
'properties.group.id' = 'user_scores',
'scan.startup.mode' = 'earliest-offset',
'format' = 'debezium-json',
'debezium-json.enable-soft-delete' = 'true',
'debezium-json.delete-field-name' = 'is_delete'
);
select * from user_scores;{code}
!20240516175543.jpg|width=704,height=281!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When flink submits the job by calling the rest api, the dependent jar package generated to the tmp is not removed",FLINK-35376,13579490,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,18380428197@163.com,18380428197@163.com,16/May/24 09:27,17/May/24 06:48,04/Jun/24 20:40,,1.14.4,,,,,,,,,,,Runtime / REST,,,,0,,,"When org. Apache. Flink. Runtime. Webmonitor. Handlers. JarRunHandler# handleRequest receives job submission request,
{code:java}
final JarHandlerContext context = JarHandlerContext.fromRequest(request, jarDir, log);
context.applyToConfiguration(effectiveConfiguration); {code}
The toPackagedProgram(configuration) method generates a dependency jar to the tmp directory;

Then,

final PackagedProgram program = context.toPackagedProgram(effectiveConfiguration);

Will generate a dependent jars, and org. Apache. The flink. Client. The program. The PackagedProgram# PackagedProgram method inside
{code:java}
this.extractedTempLibraries =
                this.jarFile == null
                        ? Collections.emptyList()
                        : extractContainedLibraries(this.jarFile); {code}
Will be overwritten by the second generation;

 

As a result, the dependent jar package generated for the first time cannot be deleted when close. If the job status detection is done and the rest api is automatically invoked to pull up, the jar file will always be generated in tmp.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 17 06:48:33 UTC 2024,,,,,,,,,,"0|z1p9cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/24 06:48;bgeng777;hi [~18380428197@163.com], in the implementation of the `extractContainerdLibraries()`, it would call `tempFile.deleteOnExit();`. So when the job exits, these temp files should be cleared. (See https://github.com/apache/flink/blob/f1ecb9e4701d612050da54589a8f561857debf34/flink-clients/src/main/java/org/apache/flink/client/program/PackagedProgram.java#L585 for details)
Have you encountered any residual jars when the job exits?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"About a question report ""Failed to deserialize data of EventHeaderV4"" of flink cdc2.4.2",FLINK-35375,13579477,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,camellia-lili,camellia-lili,16/May/24 08:20,16/May/24 08:24,04/Jun/24 20:40,,,,,,,,,,,,,,,,,1,,,"This is an error reported by Flink CDC data synchronization because a lot of data was missed during the process of synchronizing from polardb to selectdb. We restarted the task from the binlog position and reported the same error later. We saw from the Alibaba community that someone encountered this problem and provided a debezium parameter setting that can solve it. 
{code:java}
connect.keep.live.interval.ms=3000{code}
.We set this parameter but did not change any results, and as soon as the task was started, an error was immediately reported, as shown below:

 

java.lang.RuntimeException: One or more fetchers have encountered exception at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:156) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_341]

{color:#FF0000}Caused by:{color} {color:#ff0000}java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records at{color} org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_341] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_341] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_341] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_341] ... 1 more

{color:#ff0000}Caused by: org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped. at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:50){color} ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.ververica.cdc.connectors.mysql.debezium.task.context.MySqlErrorHandler.setProducerThrowable(MySqlErrorHandler.java:85) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$ReaderThreadLifecycleListener.onEventDeserializationFailure(MySqlStreamingChangeEventSource.java:1553) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1064) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] ... 1 more

{color:#ff0000}Caused by: io.debezium.DebeziumException: Failed to deserialize data of EventHeaderV4\{timestamp=1715762712000, eventType=WRITE_ROWS, serverId=27545817, headerLength=19, dataLength=8069, nextPosition=394608452, flags=0} at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.wrap(MySqlStreamingChangeEventSource.java:1488) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] {color}at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$ReaderThreadLifecycleListener.onEventDeserializationFailure(MySqlStreamingChangeEventSource.java:1553) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1064) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] ... 1 more

{color:#ff0000}Caused by: com.github.shyiko.mysql.binlog.event.deserialization.EventDataDeserializationException: Failed to deserialize data of EventHeaderV4\{timestamp=1715762712000, eventType=WRITE_ROWS, serverId=27545817, headerLength=19, dataLength=8069, nextPosition=394608452, flags=0}{color} at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:341) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.nextEvent(EventDeserializer.java:244) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$1.nextEvent(MySqlStreamingChangeEventSource.java:259) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1051) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] ... 1 more

{color:#ff0000}Caused by: com.github.shyiko.mysql.binlog.event.deserialization.MissingTableMapEventException: No TableMapEventData has been found for table id:73858. Usually that means that you have started reading binary log 'within the logical event group' (e.g. from WRITE_ROWS and not proceeding TABLE_MAP at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeRow(AbstractRowsEventDataDeserializer.java:109){color} ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.event.deserialization.WriteRowsEventDataDeserializer.deserializeRows(WriteRowsEventDataDeserializer.java:64) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.event.deserialization.WriteRowsEventDataDeserializer.deserialize(WriteRowsEventDataDeserializer.java:56) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.event.deserialization.WriteRowsEventDataDeserializer.deserialize(WriteRowsEventDataDeserializer.java:32) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:335) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.nextEvent(EventDeserializer.java:244) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$1.nextEvent(MySqlStreamingChangeEventSource.java:259) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1051) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932) ~[hlkj-bigdata-1.0-SNAPSHOT.jar:?] ... 1 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-16 08:20:33.0,,,,,,,,,,"0|z1p99s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.14 kafka connector Demo Error,FLINK-35374,13579475,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,maomao,maomao,16/May/24 08:16,16/May/24 08:23,04/Jun/24 20:40,16/May/24 08:23,1.14.4,,,,,,,,,,,Documentation,,,,0,,,"!image-2024-05-16-16-14-52-414.png|width=249,height=139!

It should be

!image-2024-05-16-16-16-01-621.png|width=486,height=100!",,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/24 08:14;maomao;image-2024-05-16-16-14-52-414.png;https://issues.apache.org/jira/secure/attachment/13068899/image-2024-05-16-16-14-52-414.png","16/May/24 08:16;maomao;image-2024-05-16-16-16-01-621.png;https://issues.apache.org/jira/secure/attachment/13068898/image-2024-05-16-16-16-01-621.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 08:23:31 UTC 2024,,,,,,,,,,"0|z1p99c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 08:16;maomao;if this need fix,can assign me.;;;","16/May/24 08:23;martijnvisser;This has already been fixed for supported versions; Flink 1.14. isn't supported anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify Flink CDC Paimon Sink web ui ,FLINK-35373,13579474,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,beryllwang,beryllwang,16/May/24 07:47,16/May/24 07:50,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"!image-2024-05-16-15-43-26-979.png!

Modify ""org.apache.flink.cdc.connectors.paimon.sink.v2.MultiTableCommittableChannelComputer"" to ""shuffle by database & table"", simplify web UI.",,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/24 07:43;beryllwang;image-2024-05-16-15-43-26-979.png;https://issues.apache.org/jira/secure/attachment/13068897/image-2024-05-16-15-43-26-979.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-16 07:47:57.0,,,,,,,,,,"0|z1p994:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-cdc from mysql to doris, gives an error Unsupported type:TIME(0) when mysql data type is TIME and synchronise table structure or data.",FLINK-35372,13579471,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,865524591@qq.com,865524591@qq.com,16/May/24 07:31,16/May/24 07:32,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"When flink-cdc synchronizes mysql data to doris, the synchronization fails if the mysql data type is TIME. flink-cdc-pipeline-connector-doris does not process the TIME type, resulting in an error: 
Unsupported type:TIME(0)","mysql:8.0.27

doris:2.1.1

flink:1.18.0

flink-cdc:3.0.1",,,345600,345600,,0%,345600,345600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2024-05-16 07:31:27.0,,,,,,,,,,"0|z1p98g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow the keystore and truststore type to configured for SSL,FLINK-35371,13579455,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,ammarm,ammarm,ammarm,16/May/24 05:25,16/May/24 13:08,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Runtime / Network,,,,0,SSL,,"Flink always creates a keystore and trustore using the [default type|https://github.com/apache/flink/blob/b87ead743dca161cdae8a1fef761954d206b81fb/flink-runtime/src/main/java/org/apache/flink/runtime/net/SSLUtils.java#L236] defined in the JDK, which in most cases is JKS.

{code}

KeyStore trustStore = KeyStore.getInstance(KeyStore.getDefaultType());

{code}

We should add other configuration options to set the type explicitly to support other custom formats, and match the options provided by other applications by [Spark|https://spark.apache.org/docs/latest/security.html#:~:text=the%20key%20store.-,%24%7Bns%7D.keyStoreType,-JKS] and [Kafka|https://kafka.apache.org/documentation/#:~:text=per%2Dbroker-,ssl.keystore.type,-The%20file%20format] already. The default would continue to be specified by the JDK.

 

The SSLContext for the REST API can read the configuration option directly, and we need to add extra logic to the [CustomSSLEngineProvider|https://github.com/apache/flink/blob/master/flink-rpc/flink-rpc-akka/src/main/java/org/apache/flink/runtime/rpc/pekko/CustomSSLEngineProvider.java] for Pekko.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 12:37:05 UTC 2024,,,,,,,,,,"0|z1p94w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 12:27;gaborgsomogyi;That makes sense. Started to have a look...;;;","16/May/24 12:37;ammarm;Thanks, can you assign it to me if it makes sense? I have a patch almost ready.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a temp module to test backward compatibility,FLINK-35370,13579449,13579395,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,eskabetxe,eskabetxe,eskabetxe,16/May/24 04:06,31/May/24 08:58,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-16 04:06:58.0,,,,,,,,,,"0|z1p93k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve `Table API and SQL` overview page or add new page to guide new users to right Flink SQL option,FLINK-35369,13579419,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leekeiabstraction,leekeiabstraction,15/May/24 20:44,15/May/24 21:12,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Project Website,,,,0,,,"Flink has rich and varied SQL offerings/deployment mode, it can take some time for new users to investigate and arrive at the right offering for them. Consider the available options:

1. Flink SQL Client (through SQL gateway, embedded or remote)
2. REST through SQL Gateway
3. A SQL client with Flink JDBC driver (through SQL gateway's REST interface)
4. A SQL client with Hive JDBC driver (through SQL gateway's HiveServer2 interface)
5. Compile and submit code that uses Table API through Flink Client (Java/Scala/Python)
6. Submitting packaged archive with code that uses Table API to JobManager REST endpoint 

(Additionally, Apache Zeppelin also provide notebook experience with its Flink SQL interpreter which builds upon Flink Client.)

The improvement being suggested here is to either enrich existing [Table API and SQL overview page|https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/table/overview/] or create new page that contains the following information:

1. Diagram on the various options available (see diagram below)
2. Table explaining pros of each approach e.g. Flink SQL Client for initial experimentation, development, OLAP. Implementing on top of Flink SQL JDBC client or SQL Gateway REST for automation, HiveServer2 for inter-operabilty with Hive etc. The table will guide users to the corresponding page for each option.  !LandscapeOfFlinkSQL.drawio(6).png!",,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/24 20:38;leekeiabstraction;LandscapeOfFlinkSQL.drawio(6).png;https://issues.apache.org/jira/secure/attachment/13068872/LandscapeOfFlinkSQL.drawio%286%29.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-15 20:44:45.0,,,,,,,,,,"0|z1p8ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reorganize table code,FLINK-35368,13579405,13579395,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,eskabetxe,eskabetxe,eskabetxe,15/May/24 17:50,31/May/24 08:57,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-15 17:50:28.0,,,,,,,,,,"0|z1p8ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reorganize sinks,FLINK-35367,13579404,13579395,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,eskabetxe,eskabetxe,eskabetxe,15/May/24 17:48,31/May/24 08:57,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,Reorganize datastream sink and source,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-15 17:48:23.0,,,,,,,,,,"0|z1p8tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create all database modules,FLINK-35366,13579403,13579395,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,eskabetxe,eskabetxe,eskabetxe,15/May/24 17:44,31/May/24 08:57,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,Create all database modules and move related code there,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-15 17:44:22.0,,,,,,,,,,"0|z1p8tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reorganize catalog and dialect code,FLINK-35365,13579402,13579395,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,eskabetxe,eskabetxe,eskabetxe,15/May/24 17:43,31/May/24 08:57,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,Reorganize code for catalog and dialect,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-15 17:43:16.0,,,,,,,,,,"0|z1p8t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create core module and move code,FLINK-35364,13579401,13579395,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,eskabetxe,eskabetxe,eskabetxe,15/May/24 17:35,31/May/24 08:57,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,"* create core module
* move all code to this new module as is
* transforme flink-connector-jdbc in shaded module",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-15 17:35:40.0,,,,,,,,,,"0|z1p8sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-449: Reorganization of flink-connector-jdbc,FLINK-35363,13579395,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,eskabetxe,eskabetxe,eskabetxe,15/May/24 16:13,31/May/24 08:57,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,,"Described in: [FLIP-449|https://cwiki.apache.org/confluence/display/FLINK/FLIP-449%3A+Reorganization+of+flink-connector-jdbc]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 08:57:02 UTC 2024,,,,,,,,,,"0|z1p8rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/24 18:27;eskabetxe;can anyone assign this to me please;;;","31/May/24 02:38;leonard;Hey [~eskabetxe]， assigned to you.;;;","31/May/24 08:12;eskabetxe;Thank [~leonard]..
could you please assign also the sub-tasks please..;;;","31/May/24 08:57;leonard;Sure!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Reorganization of flink-connector-jdbc,FLINK-35362,13579394,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,eskabetxe,eskabetxe,15/May/24 16:13,15/May/24 18:02,04/Jun/24 20:40,15/May/24 18:02,,,,,,,,,,,,Connectors / JDBC,,,,0,,,"Described in: [FLIP-449|https://cwiki.apache.org/confluence/display/FLINK/FLIP-449%3A+Reorganization+of+flink-connector-jdbc]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 15 18:01:57 UTC 2024,,,,,,,,,,"0|z1p8rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/24 18:01;eskabetxe;closing as duplicated..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete Flinkhistory files that failed to write to the local directory,FLINK-35361,13579372,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dengxiang,dengxiang,15/May/24 13:24,15/May/24 13:58,04/Jun/24 20:40,,1.17.0,1.18.0,1.19.0,1.20.0,,,,,,,,,,,,0,pull-request-available,,"I found a bug in org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandler

!image-2024-05-15-21-15-54-973.png!

 

When the local directory is full, the above code will create an empty or incomplete file in local. At this point, the Flink History webui page cannot be open or display abnormally.

However, when the local directory is expanded, the Flink History webui page will not return to normal because new files will not be regenerated.",,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/24 13:15;dengxiang;image-2024-05-15-21-15-54-973.png;https://issues.apache.org/jira/secure/attachment/13068865/image-2024-05-15-21-15-54-973.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-15 13:24:18.0,,,,,,,,,,"0|z1p8mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature] Submit Flink CDC pipeline job yarn Application mode,FLINK-35360,13579351,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kwafor,kwafor,15/May/24 09:55,28/May/24 02:03,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,For now flink-cdc pipeline support cli yarn session mode submit.I'm willing to support yarn application mode submit.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 02:00:09 UTC 2024,,,,,,,,,,"0|z1p8hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/24 03:00;ouyangwuli;With many enterprises still running on yarn, this functionality is desperately needed.

[~kwafor]  Let's push it together. ;;;","28/May/24 02:00;kwafor;discuss in FLINK-34853 and FLINK-34904;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
General Improvement to Configuration for Flink 2.0,FLINK-35359,13579350,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xuannan,xuannan,xuannan,15/May/24 09:53,23/May/24 08:11,04/Jun/24 20:40,23/May/24 08:11,,,,,,,,,1.20.0,,,Runtime / Configuration,,,,0,pull-request-available,,"As Flink moves toward version 2.0, we want to provide users with a better experience with the existing configuration. In this FLIP, we outline several general improvements to the current configuration:
 * Ensure all the ConfigOptions are properly annotated

 * Ensure all user-facing configurations are included in the documentation generation process

 * Make the existing ConfigOptions use the proper type

 * Mark all internally used ConfigOptions with the @Internal annotation

 

https://cwiki.apache.org/confluence/display/FLINK/FLIP-442%3A+General+Improvement+to+Configuration+for+Flink+2.0

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 08:11:03 UTC 2024,,,,,,,,,,"0|z1p8hk:",9223372036854775807,"The following configurations have been updated to the Duration type in a backward-compatible manner:
- `client.heartbeat.interval`
- `client.heartbeat.timeout`
- `cluster.registration.error-delay`
- `cluster.registration.initial-timeout`
- `cluster.registration.max-timeout`
- `cluster.registration.refused-registration-delay`
- `cluster.services.shutdown-timeout`
- `heartbeat.interval`
- `heartbeat.timeout`
- `high-availability.zookeeper.client.connection-timeout`
- `high-availability.zookeeper.client.retry-wait`
- `high-availability.zookeeper.client.session-timeout`
- `historyserver.archive.fs.refresh-interval`
- `historyserver.web.refresh-interval`
- `metrics.fetcher.update-interval`
- `metrics.latency.interval`
- `metrics.reporter.influxdb.connectTimeout`
- `metrics.reporter.influxdb.writeTimeout`
- `metrics.system-resource-probing-interval`
- `pekko.startup-timeout`
- `pekko.tcp.timeout`
- `resourcemanager.job.timeout`
- `resourcemanager.standalone.start-up-time`
- `resourcemanager.taskmanager-timeout`
- `rest.await-leader-timeout`
- `rest.connection-timeout`
- `rest.idleness-timeout`
- `rest.retry.delay`
- `slot.idle.timeout`
- `slot.request.timeout`
- `task.cancellation.interval`
- `task.cancellation.timeout`
- `task.cancellation.timers.timeout`
- `taskmanager.debug.memory.log-interval`
- `web.refresh-interval`
- `web.timeout`
- `yarn.heartbeat.container-request-interval`

The following configurations have been updated to the Enum type in a backward-compatible manner:
- `taskmanager.network.compression.codec`
- `table.optimizer.agg-phase-strategy`

The following configurations have been updated to the Int type in a backward-compatible manner:
- `yarn.application-attempts` ",,,,,,,,,,,,,,,,,,,"23/May/24 08:11;xtsong;master (1.20): 4f1427cabffe0a2609b54b3349db300636977811;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Breaking change when loading artifacts,FLINK-35358,13579335,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,ferenc-csaky,rkth,rkth,15/May/24 07:55,03/Jun/24 07:46,04/Jun/24 20:40,30/May/24 14:15,1.19.0,,,,,,,,1.19.1,1.20.0,,Client / Job Submission,flink-docker,,,0,pull-request-available,,"We have been using the following code snippet in our Dockerfiles for running a Flink job in application mode

 
{code:java}
FROM flink:1.18.1-scala_2.12-java17

COPY --from=build /app/target/my-job*.jar /opt/flink/usrlib/artifacts/my-job.jar

USER flink {code}
 

Which has been working since at least around Flink 1.14, but the 1.19 update has broken our Dockerfiles. The fix is to put the jar file a step further out so the code snippet becomes

 
{code:java}
FROM flink:1.18.1-scala_2.12-java17

COPY --from=build /app/target/my-job*.jar /opt/flink/usrlib/my-job.jar

USER flink  {code}
 

We have not spent too much time looking into what the cause is, but we get the stack trace

 
{code:java}
myjob-jobmanager-1   | org.apache.flink.util.FlinkException: Could not load the provided entrypoint class.
myjob-jobmanager-1   |     at org.apache.flink.client.program.DefaultPackagedProgramRetriever.getPackagedProgram(DefaultPackagedProgramRetriever.java:230) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.getPackagedProgram(StandaloneApplicationClusterEntryPoint.java:149) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.lambda$main$0(StandaloneApplicationClusterEntryPoint.java:90) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.main(StandaloneApplicationClusterEntryPoint.java:89) [flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   | Caused by: org.apache.flink.client.program.ProgramInvocationException: The program's entry point class 'my.company.job.MyJob' was not found in the jar file.
myjob-jobmanager-1   |     at org.apache.flink.client.program.PackagedProgram.loadMainClass(PackagedProgram.java:481) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:153) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:65) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.client.program.PackagedProgram$Builder.build(PackagedProgram.java:691) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.client.program.DefaultPackagedProgramRetriever.getPackagedProgram(DefaultPackagedProgramRetriever.java:228) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     ... 4 more
myjob-jobmanager-1   | Caused by: java.lang.ClassNotFoundException: my.company.job.MyJob
myjob-jobmanager-1   |     at java.net.URLClassLoader.findClass(Unknown Source) ~[?:?]
myjob-jobmanager-1   |     at java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]
myjob-jobmanager-1   |     at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:67) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:74) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:51) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]
myjob-jobmanager-1   |     at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:197) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at java.lang.Class.forName0(Native Method) ~[?:?]
myjob-jobmanager-1   |     at java.lang.Class.forName(Unknown Source) ~[?:?]
myjob-jobmanager-1   |     at org.apache.flink.client.program.PackagedProgram.loadMainClass(PackagedProgram.java:479) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:153) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.client.program.PackagedProgram.<init>(PackagedProgram.java:65) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.client.program.PackagedProgram$Builder.build(PackagedProgram.java:691) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     at org.apache.flink.client.program.DefaultPackagedProgramRetriever.getPackagedProgram(DefaultPackagedProgramRetriever.java:228) ~[flink-dist-1.19.0.jar:1.19.0]
myjob-jobmanager-1   |     ... 4 more{code}
 

I have changed some text in the stack trace to keep it anonymous so it is possible there is a typo but that is not the issue. As you can see, the stack trace leads to PackagedProgram and DefaultPackagedProgramRetriever to which the only commits after Flink 1.18 are [PackagedProgram commit|https://github.com/apache/flink/commit/d0ce5349fdf1a611518eba20a169c475ee0b46c5] and [DefaultPackagedProgramRetriever commit|https://github.com/apache/flink/commit/e63aa12252843d0098a56f3091b28d48aff5b5af] and we suspect the culprit is the latter, specifically [this line|https://github.com/apache/flink/commit/e63aa12252843d0098a56f3091b28d48aff5b5af#diff-11b5162d6745014c68e96303d26c71bdb88bac068c27834dbdbb7c9089ffbe9fL227] which we think has made the artifact check non-recursive. We assume it is intended to have your artifacts directly in /opt/flink/usrlib without the artifacts directory so we are planning on changing that for our Dockerfiles anyway, but it is still a breaking change so we wanted to make an issue on it first.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 14:15:10 UTC 2024,,,,,,,,,,"0|z1p8e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/24 08:04;martijnvisser;[~ferenc-csaky] [~mbalassi] Any thoughts on this one?;;;","15/May/24 08:46;rkth;I would like to add that we have a project where upgrading from Flink 1.18.1 to 1.19.0 right now would mean we have to make a new major release, however, if this gets fixed, it will only be a minor. We will wait a little bit to see what the Flink community agrees on :);;;","15/May/24 14:07;ferenc-csaky;This change was unintentional. Restricting JARs to only be put the root level of {{usrlib}} seems an unnecessary boundary. I think your analysis is on point, but AFAIK whatever we do this only can be fixed in a Flink patch version, but [~martijnvisser] correct me if I am wrong.

The fix is easy, I can open a PR with the fix and added unit tests to cover this case by EOD today. So feel free to assign this to me.;;;","16/May/24 11:07;ferenc-csaky;PR is opened against master and 1.19, CI run were successful for both.;;;","16/May/24 11:20;rkth;Yep I saw it, code looks good to me. In your test, is it correct that you are setting the usrlib to the parent directory so it looks from /opt/flink instead of /opt/flink/usrlib and therefore it needs to search recursively to also find the artifacts inside /opt/flink/usrlib?;;;","16/May/24 14:53;ferenc-csaky;In that test class all directories are generated before every test case, so it looks like this:
{code}
/var/folders/wp/ccy48gw1255bswh9bx9svxjc0000gn/T/5ea4975f-13a5-47eb-a61f-4b1166e7f885/_user_dir_with_single_entry_class
{code}
The JAR(s) are under the {{_user_dir_with_single_entry_class}} dir, and even its parent is generated specifically for the test case, so it is safe to simply get the parent for testing the recursive file listing. These tests do not use the actual {{/opt/flink/usrlib}} path at all.;;;","17/May/24 07:30;rkth;Right, but the same idea with just searching the parent directory since that requires a recursive search. Cool way to test it and thanks a lot!;;;","30/May/24 14:14;hong; merged commit [{{853989b}}|https://github.com/apache/flink/commit/853989bd862c31e0c74cd5a584177dc401c5a3d4] into   apache:master;;;","30/May/24 14:15;hong;merged commit [{{90a71a1}}|https://github.com/apache/flink/commit/90a71a124771697a0b8b2c2bbc520856d6ae9e25] into   apache:release-1.19;;;",,,,,,,,,,,,,,,,,,,,
"Add ""kubernetes.operator.plugins.listeners"" parameter description to the Operator configuration document",FLINK-35357,13579319,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xinzhuxianshenger,xinzhuxianshenger,xinzhuxianshenger,15/May/24 05:37,15/May/24 09:44,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,"In Flink Operator ""Custom Flink Resource Listeners"" in practice (doc: [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.8/docs/operations/plugins/#custom-flink-resource] -listeners)

It was found that the ""Operator Configuration Reference"" document did not explain the ""Custom Flink Resource Listeners"" configuration parameters.

So I wanted to come up with adding:

kubernetes.operator.plugins.listeners.<listener-name>.class: <fully-qualified-class-name>

, after all it is useful.

I want to submit a PR to optimize the document.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 15 09:44:58 UTC 2024,,,,,,,,,,"0|z1p8ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/24 09:39;fanrui;Thanks [~xinzhuxianshenger] for reporting this JIRA. I checked the operator code just now, I think you are right. You are assigned, please go ahead.

cc [~gyfora] ;;;","15/May/24 09:44;fanrui;IIUC, kubernetes.operator.plugins.listeners.<listener-name>.class is similar to metrics.reporter.<name>.factory.class (org.apache.flink.configuration.MetricOptions#REPORTER_FACTORY_CLASS).

We can export it in flink kubernetes operator configuration page[1] as well.

 

[1]https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Async reducing state,FLINK-35356,13579228,13574084,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,14/May/24 13:13,17/May/24 02:24,04/Jun/24 20:40,17/May/24 02:24,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 17 02:24:51 UTC 2024,,,,,,,,,,"0|z1p7qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/24 02:24;zakelly;Merged into master via cc21eec066b3426a7eb0af5303c4590f08d65b2d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Async aggregating state,FLINK-35355,13579227,13574084,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jectpro,zakelly,zakelly,14/May/24 13:12,24/May/24 15:46,04/Jun/24 20:40,24/May/24 15:46,,,,,,,,,2.0.0,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 15:46:21 UTC 2024,,,,,,,,,,"0|z1p7q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 15:46;zakelly;Merged into master via 467f94f9ecef91b671ebbdc4774f2b690f4fa713;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support host mapping in Flink tikv cdc,FLINK-35354,13579225,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ouyangwuli,ouyangwuli,14/May/24 13:07,26/May/24 12:54,04/Jun/24 20:40,,cdc-3.1.0,cdc-3.2.0,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"In tidb production environment deployment, there are usually two kinds of network: internal network and public network. When we use pd mode in tikv, we need to do network mapping, such as `spark.tispark.host_mapping` in [https://github.com/pingcap/tispark/blob/master/docs/userguide_3.0.md]. So I think we need support `host_mapping` in our Flink tikv cdc connector.

 

Add param:

 tikv.host_mapping:192.168.0.2:8.8.8.8;192.168.0.3:9.9.9.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-14 13:07:42.0,,,,,,,,,,"0|z1p7ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate  ""Profiler"" page into Chinese",FLINK-35353,13579223,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,drymartini,drymartini,drymartini,14/May/24 12:51,04/Jun/24 13:12,04/Jun/24 20:40,,1.19.0,,,,,,,,1.19.0,,,chinese-translation,Documentation,,,0,pull-request-available,,The links are https://nightlies.apache.org/flink/flink-docs-release-1.19/zh/docs/ops/debugging/profiler/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 08:18:03 UTC 2024,,,,,,,,,,"0|z1p7pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 12:52;drymartini;Hi [~jark], could you assign this to me?;;;","23/May/24 08:18;drymartini;Hi [~jark]  ,I have finished the translation of the page.Could you please verify my pull request?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add -sae submit task to run for a period of time and automatically cancel,FLINK-35352,13579207,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gujc,gujc,14/May/24 09:07,14/May/24 09:07,04/Jun/24 20:40,,1.17.2,1.18.1,1.19.0,,,,,,,,,API / Core,,,,0,,,"Add -sae submit task to run for a period of time and automatically cancel

I know Flink has added a heartbeat mechanism in recent versions, and it can be adjusted through client.startbeat. interval and client.startbeat. timeout. However, I found that although the client has heartbeat logs, the task will still be automatically canceled when the timeout is reached
I found through debugging the source code that there is a section of code that should have modified expiredTimestamp after receiving the heartbeat, but it was not successfully modified

I have fixed this bug locally and would like to submit it to the community",centos7.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-14 09:07:17.0,,,,,,,,,,"0|z1p7ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore from unaligned checkpoints with a custom partitioner fails.,FLINK-35351,13579198,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lda-dima,lda-dima,lda-dima,14/May/24 08:06,31/May/24 08:02,04/Jun/24 20:40,31/May/24 08:02,,,,,,,,,1.18.2,1.19.1,1.20.0,Runtime / Checkpointing,,,,0,pull-request-available,,"We encountered a problem when using a custom partitioner with unaligned checkpoints. The bug reproduces under the following steps:
 # Run a job with graph: Source[2]->Sink[3], the custom partitioner applied after the Source task.
 # Make a checkpoint.
 # Restore from the checkpoint with a different source parallelism: Source[1]->Sink[3].
 # An exception is thrown.

This issue does not occur when restoring with the same parallelism or when changing the Sink parallelism. The exception only occurs when the parallelism of the Source is changed while the Sink parallelism remains the same.

See the exception below and the test code at the end. 
{code:java}
[db13789c52b80aad852c53a0afa26247] Task [Sink: sink (3/3)#0] WARN  Sink: sink (3/3)#0 (be1d158c2e77fc9ed9e3e5d9a8431dc2_0a448493b4782967b150582570326227_2_0) switched from RUNNING to FAILED with failure cause:
java.io.IOException: Can't get next record for channel InputChannelInfo{gateIdx=0, inputChannelIdx=0}
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:106) ~[classes/:?]
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:600) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:930) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:879) ~[classes/:?]
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:960) ~[classes/:?]
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:939) [classes/:?]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:753) [classes/:?]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568) [classes/:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.io.IOException: Corrupt stream, found tag: -1
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:222) ~[classes/:?]
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:44) ~[classes/:?]
    at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:53) ~[classes/:?]
    at org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.readInto(NonSpanningWrapper.java:337) ~[classes/:?]
    at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.readNonSpanningRecord(SpillingAdaptiveSpanningRecordDeserializer.java:128) ~[classes/:?]
    at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.readNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:103) ~[classes/:?]
    at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:93) ~[classes/:?]
    at org.apache.flink.streaming.runtime.io.recovery.DemultiplexingRecordDeserializer$VirtualChannel.getNextRecord(DemultiplexingRecordDeserializer.java:79) ~[classes/:?]
    at org.apache.flink.streaming.runtime.io.recovery.DemultiplexingRecordDeserializer.getNextRecord(DemultiplexingRecordDeserializer.java:154) ~[classes/:?]
    at org.apache.flink.streaming.runtime.io.recovery.DemultiplexingRecordDeserializer.getNextRecord(DemultiplexingRecordDeserializer.java:54) ~[classes/:?]
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:103) ~[classes/:?]
    ... 10 more {code}
We discovered that this issue occurs due to an optimization in the [StateAssignmentOperation::reDistributeInputChannelStates|https://github.com/apache/flink/blame/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L424]
{code:java}
        if (inputState.getParallelism() == executionJobVertex.getParallelism()) {
            stateAssignment.inputChannelStates.putAll(
                    toInstanceMap(stateAssignment.inputOperatorID, inputOperatorState));
            return;
        }
 {code}
At the moment of checkpointing, some in-flight records could be partially sent. The Sink sub-task's input has one half of a record, and the Source sub-task's output has the second part of the record. When restoring in-flight data, all the records are sent from all Source sub-tasks to all Sink sub-tasks. However, due to the optimization mentioned above, some Sink sub-tasks might not have information about the beginnings of in-flight records.

The proposed fix involves checking the situations when the optimization should not be applied:
{code:java}
boolean noNeedRescale = stateAssignment.executionJobVertex
        .getJobVertex()
        .getInputs()
        .stream()
        .map(JobEdge::getDownstreamSubtaskStateMapper)
        .anyMatch(m -> !m.equals(SubtaskStateMapper.FULL))
        && stateAssignment.executionJobVertex
        .getInputs()
        .stream()
        .map(IntermediateResult::getProducer)
        .map(vertexAssignments::get)
        .anyMatch(taskStateAssignment -> {
            final int oldParallelism =
                    stateAssignment.oldState.get(stateAssignment.inputOperatorID).getParallelism();
            return oldParallelism == taskStateAssignment.executionJobVertex.getParallelism();
        });
if (inputState.getParallelism() == executionJobVertex.getParallelism() && noNeedRescale) {code}
 

Test for reproduce:
{code:java}
package org.apache.flink.test.checkpointing;

import org.apache.flink.api.common.JobID;
import org.apache.flink.api.common.JobStatus;
import org.apache.flink.api.common.functions.Partitioner;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.core.fs.Path;
import org.apache.flink.runtime.jobgraph.JobGraph;
import org.apache.flink.runtime.jobgraph.SavepointRestoreSettings;
import org.apache.flink.runtime.minicluster.MiniCluster;
import org.apache.flink.runtime.minicluster.MiniClusterConfiguration;
import org.apache.flink.streaming.api.environment.CheckpointConfig;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.SinkFunction;
import org.apache.flink.streaming.api.functions.source.ParallelSourceFunction;
import org.apache.flink.util.FileUtils;

import org.junit.After;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TemporaryFolder;

import java.io.File;

import static org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createLocalEnvironmentWithWebUI;
import static org.junit.Assert.fail;

/** Integration test for performing rescale of unaligned checkpoint with custom partitioner. */
public class UnalignedCheckpointCustomRescaleITCase {

    @Rule
    public TemporaryFolder tempFolder = new TemporaryFolder();

    private final static File CHECKPOINT_FILE = new File(""src/test/resources/custom-checkpoint"");

    @Test
    public void createCheckpoint() {
        runJob(2, 3, true);
    }

    @Test
    public void restoreFromCheckpoint() {
        runJob(1, 3, false);
    }

    @After
    public void after() {
        tempFolder.delete();
    }

    private void runJob(int sourceParallelism, int sinkParallelism, boolean createCheckpoint) {
        try (MiniCluster miniCluster = new MiniCluster(buildMiniClusterConfig())) {
            miniCluster.start();
            Configuration configuration = new Configuration();
            StreamExecutionEnvironment env = createLocalEnvironmentWithWebUI(configuration);
            CheckpointConfig checkpointConfig = env.getCheckpointConfig();
            env.enableCheckpointing(Integer.MAX_VALUE);
            checkpointConfig.setForceUnalignedCheckpoints(true);
            checkpointConfig.enableUnalignedCheckpoints();
            checkpointConfig.setMaxConcurrentCheckpoints(1);
            checkpointConfig.setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
            checkpointConfig.setCheckpointStorage(""file://"" + tempFolder.newFolder() + ""/checkPoints"");
            env
                    .addSource(new StringsSource(createCheckpoint ? 10 : 0, sinkParallelism))
                    .name(""source"")
                    .setParallelism(sourceParallelism)
                    .partitionCustom(new StringPartitioner(), str -> str.split("" "")[0])
                    .addSink(new StringSink(createCheckpoint ? 16 : 100000))
                    .name(""sink"")
                    .setParallelism(sinkParallelism);
            JobGraph job = env.getStreamGraph().getJobGraph();
            if (!createCheckpoint) {
                job.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(""file://"" + CHECKPOINT_FILE.getAbsolutePath(), false));
            }
            JobID jobId = miniCluster.submitJob(job).get().getJobID();
            if (createCheckpoint) {
                while (!miniCluster.getJobStatus(jobId).get().equals(JobStatus.RUNNING)) {
                    Thread.sleep(1000);
                }
                String savepointPath = miniCluster.triggerCheckpoint(jobId).get();
                System.out.println(""SAVE PATH "" + savepointPath);
                Thread.sleep(1000);
                miniCluster.cancelJob(jobId);
                FileUtils.copy(new Path(savepointPath), Path.fromLocalFile(CHECKPOINT_FILE), false);
            } else {
                int count = 0;
                while (!miniCluster.getJobStatus(jobId).get().equals(JobStatus.RUNNING)) {
                    Thread.sleep(1000);
                    count++;
                    if (count > 10) {
                        break;
                    }
                }
                Thread.sleep(10000);
                boolean fail = !miniCluster.getJobStatus(jobId).get().equals(JobStatus.RUNNING);
                miniCluster.cancelJob(jobId);
                if (fail) {
                    fail(""Job fails"");
                }
            }
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    private static MiniClusterConfiguration buildMiniClusterConfig() {
        return new MiniClusterConfiguration.Builder()
                .setNumTaskManagers(2)
                .setNumSlotsPerTaskManager(4)
                .build();
    }

    private static class StringsSource implements ParallelSourceFunction<String> {
        volatile boolean isCanceled;
        final int producePerPartition;
        final int partitionCount;

        public StringsSource(int producePerPartition, int partitionCount) {
            this.producePerPartition = producePerPartition;
            this.partitionCount = partitionCount;
        }

        private String buildString(int partition, int index) {
            String longStr = new String(new char[3713]).replace('\0', '\uFFFF');
            return partition + "" "" + index + "" "" + longStr;
        }

        @Override
        public void run(SourceContext<String> ctx) throws Exception {
            for (int i = 0; i < producePerPartition; i++) {
                for (int partition = 0; partition < partitionCount; partition++) {
                    ctx.collect(buildString(partition, i));
                }
            }
            while (!isCanceled) { Thread.sleep(1000); }
        }

        @Override
        public void cancel() { isCanceled = true; }
    }

    private static class StringSink implements SinkFunction<String> {
        final int consumeBeforeCheckpoint;
        int consumed = 0;

        public StringSink(int consumeBeforeCheckpoint) {
            this.consumeBeforeCheckpoint = consumeBeforeCheckpoint;
        }

        @Override
        public void invoke(String value, Context ctx) throws InterruptedException {
            consumed++;
            System.out.println(""--- CONSUMED --- "" + value.substring(0, 10));
            if (consumed == consumeBeforeCheckpoint) {
                System.out.println(""--- WAITING FOR CHECKPOINT START ---"");
                Thread.sleep(4000);
            }
        }
    }

    public static class StringPartitioner implements Partitioner<String> {
        @Override
        public int partition(String key, int numPartitions) {
            return Integer.parseInt(key) % numPartitions;
        }
    }
}
 {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 08:02:01 UTC 2024,,,,,,,,,,"0|z1p7js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 08:47;lda-dima;[~MartijnVisser] [~trohrmann] 

Hi,

Please, assign me to this task;;;","14/May/24 09:07;pnowojski;Hi [~lda-dima], thanks for reporting this issue and what looks like correct analysis. However I think the proposed solution goes in the wrong direction - we can not disallow rescaling, as that might prevent jobs from recovering in case of for example a loss of a TaskManager. Note that the problem probably happens due to the use of "" custom partitioner"" here. AFAIR unaligned checkpoints are (for most likely the exactly same reason that you discovered in this bug) enabled only for keyed exchanges. [Pointwise/broadcast connections with unaligned checkpoints are unsupported|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpointing_under_backpressure/#certain-data-distribution-patterns-are-not-checkpointed]. There is somewhere in the codebase code, that checks whether for given partitioning unaligned checkpoints should be enabled or not. I would guess that custom partitioners are mishandled there?  

Relevant code pointers:
* {{org.apache.flink.runtime.checkpoint.CheckpointOptions.AlignmentType#FORCED_ALIGNED}}
* {{org.apache.flink.streaming.runtime.io.RecordWriterOutput#supportsUnalignedCheckpoints}}
* {{org.apache.flink.streaming.api.graph.StreamGraphGenerator#shouldDisableUnalignedCheckpointing}}
;;;","14/May/24 09:21;lda-dima;Hi [~pnowojski] ,

I am not suggesting to disallow rescaling, just that in our particular example we change parallelism only for source and do not change sink. If we change parallelism for both, there will be no problems with recovery.
I think the problem is not in the custom partitioner, but in SubtaskStateMapper.FULL.

In the nearest future I will try to create a Pull Request, which will make the proposed solution clearer.;;;","16/May/24 08:31;pnowojski;Thanks, after reading the PR indeed I understand this more :);;;","31/May/24 08:02;pnowojski;merged to master as ce0b61f376b and ce0b61f376b^
merged to release-1.19 as 551f4ae7dde and 551f4ae7dde^
merged to release-1.18 as 70f775e7ba1 and 70f775e7ba1^;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add documentation for Kudu,FLINK-35350,13579180,13573180,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,martijnvisser,martijnvisser,14/May/24 07:07,14/May/24 07:08,04/Jun/24 20:40,,,,,,,,,,kudu-2.0.0,,,Connectors / Kudu,,,,0,,,"There's currently no documentation for Kudu; this should be added",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-14 07:07:38.0,,,,,,,,,,"0|z1p7fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use connection in openJdbcConnection of SqlServerDialect/Db2Dialect/OracleDialect,FLINK-35349,13579175,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,loserwang1024,loserwang1024,14/May/24 06:11,14/May/24 06:12,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,"Current, some dialect's `openJdbcConnection`  create connection without connection pool. It means that will create a new connection each time.

Howver , openJdbcConnection is used in generateSplits now, which means that enumerator will create a new connection for once split. A big table will create connection again and again.
{code:java}
public Collection<SnapshotSplit> generateSplits(TableId tableId) {
    try (JdbcConnection jdbc = dialect.openJdbcConnection(sourceConfig)) {

        LOG.info(""Start splitting table {} into chunks..."", tableId);
        long start = System.currentTimeMillis();

        Table table =
                Objects.requireNonNull(dialect.queryTableSchema(jdbc, tableId)).getTable();
        Column splitColumn = getSplitColumn(table, sourceConfig.getChunkKeyColumn());
        final List<ChunkRange> chunks; {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 06:12:07 UTC 2024,,,,,,,,,,"0|z1p7eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 06:12;loserwang1024;I'd like to do it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement materialized table refresh rest api ,FLINK-35348,13579171,13579167,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,lsy,lsy,14/May/24 05:36,29/May/24 09:30,04/Jun/24 20:40,29/May/24 09:30,1.20.0,,,,,,,,1.20.0,,,Table SQL / Gateway,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 29 09:30:04 UTC 2024,,,,,,,,,,"0|z1p7ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/24 09:30;lsy;Merged in master: 2c35e48addf3e0bb4a3dbea45e578d22859c5a36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement InMemory workflow scheduler service and plugin to support materialized table,FLINK-35347,13579170,13579167,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,14/May/24 05:35,24/May/24 03:26,04/Jun/24 20:40,24/May/24 03:26,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Gateway,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 03:26:45 UTC 2024,,,,,,,,,,"0|z1p7dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 03:26;lsy;Merged in master: 71e6746727aaa57a6082f91576365807fa51b478;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce pluggable workflow scheduler interface for materialized table,FLINK-35346,13579169,13579167,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,14/May/24 05:33,16/May/24 11:42,04/Jun/24 20:40,16/May/24 11:42,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 11:42:28 UTC 2024,,,,,,,,,,"0|z1p7dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 11:42;lsy;Merged in master: 1378979f02eed55bbf3f91b08ec166d55b2c42a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-448: Introduce Pluggable Workflow Scheduler Interface for Materialized Table,FLINK-35345,13579167,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,14/May/24 04:19,29/May/24 09:30,04/Jun/24 20:40,29/May/24 09:30,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Ecosystem,,,0,,,"This is an umbrella issue for FLIP-448: Introduce Pluggable Workflow Scheduler Interface for Materialized Table, for more detail, please see https://cwiki.apache.org/confluence/display/FLINK/FLIP-448%3A+Introduce+Pluggable+Workflow+Scheduler+Interface+for+Materialized+Table.",,,,,,,,,,,,,,FLINK-35187,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-14 04:19:27.0,,,,,,,,,,"0|z1p7cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move same code from multiple subclasses to JdbcSourceChunkSplitter,FLINK-35344,13579165,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,loserwang1024,loserwang1024,14/May/24 03:44,15/May/24 05:38,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Current, subclasses of JdbcSourceChunkSplitter almost share same code, but each have one copy. It's hard for later maintenance. 

Thus, this Jira aim to move same code from multiple subclasses to JdbcSourceChunkSplitter, just like what have done in 
AbstractScanFetchTask(https://github.com/apache/flink-cdc/issues/2690)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 03:46:19 UTC 2024,,,,,,,,,,"0|z1p7cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 03:46;loserwang1024;I'd like to do it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in SourceReaderBase,FLINK-35343,13579161,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,zyhh,zyhh,14/May/24 02:55,17/May/24 02:41,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,API / Core,,,,0,,,"h2. operation

I used flink batch to read data from Doris and write to Doris. 

The flink job include two source task, one table join task and one sink task, which like:

source: Table A

source: Table B

hashjoin: c= a join b

sink: c
h2.  
h2. table properties
h3. source
properties.put(""connector"", ""doris"");
properties.put(""fenodes"", inputDataSource.getHttpUrl());
properties.put(""table.identifier"", inputDataSource.getDatabase() + ""."" + sourceTable.getName());
properties.put(""username"", inputDataSource.getUsername());
properties.put(""password"", inputDataSource.getPassword());
h3. sink
properties.put(""connector"", ""doris"");
properties.put(""fenodes"", dataExplore.getOutputDataSource().getHttpUrl());
properties.put(""table.identifier"", dataExplore.getOutputDataSource().getDatabase() + ""."" + tableName);
properties.put(""username"", dataExplore.getOutputDataSource().getUsername());
properties.put(""password"", dataExplore.getOutputDataSource().getPassword());
properties.put(""sink.properties.format"", ""csv"");
//列分隔符
properties.put(""sink.properties.column_separator"", ""#cs_"");
//行分隔符
properties.put(""sink.properties.line_delimiter"", ""#ld_"");
properties.put(""sink.label-prefix"", ""doris_label"" + UUID.randomUUID());
properties.put(""sink.parallelism"", ""2"");
 
 
 
h2. exception stack
{code:java}
Caused by: java.lang.NullPointerException    at org.apache.flink.connector.base.source.reader.SourceReaderBase.finishCurrentFetch(SourceReaderBase.java:194)    at org.apache.flink.connector.base.source.reader.SourceReaderBase.moveToNextSplit(SourceReaderBase.java:208)    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:173)    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:131)    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:419)    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)    at java.lang.Thread.run(Thread.java:748) {code}
 
h2. other

The problem only occur in flink local mode and deploy on k8s.

 ","* flink(1.17.2), local mode and deploy on k8s
 * doris-flink-connector-1.17(1.6.0)
 * Doris(2.1)

h2.  ",,,,,,,,,,,,,,,,,,,,,,,,,"14/May/24 02:55;zyhh;flinktask.png;https://issues.apache.org/jira/secure/attachment/13068844/flinktask.png","14/May/24 02:55;zyhh;servicelog.png;https://issues.apache.org/jira/secure/attachment/13068845/servicelog.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-14 02:55:41.0,,,,,,,,,,"0|z1p7bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MaterializedTableStatementITCase test can check for wrong status,FLINK-35342,13579112,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,hackergin,rskraba,rskraba,13/May/24 16:27,27/May/24 06:01,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,pull-request-available,test-stability,"* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9056197319/job/24879135605#step:10:12490
 
It looks like {{MaterializedTableStatementITCase.testAlterMaterializedTableSuspendAndResume}} can be flaky, where the expected status is not yet RUNNING:

{code}
Error: 03:24:03 03:24:03.902 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 26.78 s <<< FAILURE! -- in org.apache.flink.table.gateway.service.MaterializedTableStatementITCase
Error: 03:24:03 03:24:03.902 [ERROR] org.apache.flink.table.gateway.service.MaterializedTableStatementITCase.testAlterMaterializedTableSuspendAndResume(Path, RestClusterClient) -- Time elapsed: 3.850 s <<< FAILURE!
May 13 03:24:03 org.opentest4j.AssertionFailedError: 
May 13 03:24:03 
May 13 03:24:03 expected: ""RUNNING""
May 13 03:24:03  but was: ""CREATED""
May 13 03:24:03 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
May 13 03:24:03 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
May 13 03:24:03 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
May 13 03:24:03 	at org.apache.flink.table.gateway.service.MaterializedTableStatementITCase.testAlterMaterializedTableSuspendAndResume(MaterializedTableStatementITCase.java:650)
May 13 03:24:03 	at java.lang.reflect.Method.invoke(Method.java:498)
May 13 03:24:03 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
May 13 03:24:03 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
May 13 03:24:03 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
May 13 03:24:03 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
May 13 03:24:03 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
May 13 03:24:03 
May 13 03:24:04 03:24:04.270 [INFO] 
May 13 03:24:04 03:24:04.270 [INFO] Results:
May 13 03:24:04 03:24:04.270 [INFO] 
Error: 03:24:04 03:24:04.270 [ERROR] Failures: 
Error: 03:24:04 03:24:04.271 [ERROR]   MaterializedTableStatementITCase.testAlterMaterializedTableSuspendAndResume:650 
May 13 03:24:04 expected: ""RUNNING""
May 13 03:24:04  but was: ""CREATED""
May 13 03:24:04 03:24:04.271 [INFO] 
Error: 03:24:04 03:24:04.271 [ERROR] Tests run: 82, Failures: 1, Errors: 0, Skipped: 0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 06:01:03 UTC 2024,,,,,,,,,,"0|z1p70o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 16:33;rskraba;[~hackergin] I found this failure in this new test, from a few days ago.  Do you think it's likely to happen again?;;;","14/May/24 02:27;hackergin;[~rskraba] {color:#000000}Thank you for reporting this issue, I will follow up to resolve it.{color};;;","14/May/24 03:26;hackergin;{color:#000000}This issue will be fixed in this PR.{color}  https://github.com/apache/flink/pull/24777;;;","14/May/24 12:24;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59528&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12763;;;","14/May/24 13:07;hackergin;fixed by:   94d861b08fef1e350d80a3f5f0f63168d327bc64;;;","14/May/24 13:42;rskraba;Thank you for the fix!

Just to be complete, this failure occurred before the fix was merged:

* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9072668322/job/24928769693#step:10:12490;;;","16/May/24 08:40;rskraba;It looks like a related problem is occurring with {{testDropMaterializedTable}}

* * 1.20 test_cron_adaptive_scheduler table https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59583&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12764
* 1.20 test_cron_adaptive_scheduler table https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59558&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12764;;;","16/May/24 09:03;hackergin;{color:#000000}{color:#000000}I created a new pull request to fix this issue. https://github.com/apache/flink/pull/24799{color}{color};;;","16/May/24 09:31;hackergin;[~rskraba]  {color:#000000}Is there a way to manually trigger test_cron_adaptive_scheduler to verify if this issue has been fixed? {color};;;","16/May/24 16:26;rskraba;I'm not entirely sure how this can be reproduced locally, but it can also be observed in GitHub actions if that helps!

* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9108427134/job/25040554694#step:10:12099
* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9105407291/job/25031170942#step:10:11841;;;","16/May/24 16:36;hackergin;[~rskraba] Thank you for the reminder. I will run repeated tests on GitHub Actions to confirm that this issue has been fixed correctly.;;;","17/May/24 12:25;rskraba;I didn't succeed in reproducing the error locally... unfortunately, sometimes CI is the only way for me too!  If anyone has some expertise in this area or a pointer, I would love to learn!

In the meantime: 
* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9121895520/job/25082050482#step:10:12475;;;","20/May/24 02:37;Weijie Guo;1.20 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59635&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12483;;;","20/May/24 02:38;Weijie Guo;1.20 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59647&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12444;;;","21/May/24 14:03;rskraba;* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9167756989/job/25205665075#step:10:12475
* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9152485864/job/25160229892#step:10:12475
* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9144334458/job/25142199658#step:10:12490
* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9136523142/job/25125573106#step:10:12493;;;","22/May/24 06:40;hackergin;I can trigger AdaptiveScheduler/Test (module: table) in my personal repository by modifying

https://github.com/apache/flink/commit/ac5c0c7d74ac8e1e446a3aa2122fdad47461811e

Currently, the test seems to be working fine.  [https://github.com/hackergin/flink/actions/runs/9184440112/job/25257010236]

I will run more tests to verify the fix.;;;","22/May/24 07:11;Weijie Guo;1.20 test_cron_adaptive_scheduler table

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59713&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12444;;;","22/May/24 12:45;rskraba;* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9184288079/job/25256599953#step:10:12493;;;","23/May/24 07:19;Weijie Guo;1.20 test_cron_adaptive_scheduler table

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59751&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12806;;;","23/May/24 12:59;rskraba;* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9201159914/job/25309139160#step:10:12492;;;","24/May/24 05:38;Weijie Guo;1.20 test_cron_adaptive_scheduler:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59793&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12812;;;","24/May/24 12:36;rskraba;* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9217608897/job/25360076574#step:10:12483;;;","27/May/24 02:41;Weijie Guo;1.20 test_cron_adaptive_scheduler

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59821&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12792;;;","27/May/24 02:46;Weijie Guo;1.20 test_cron_adaptive_scheduler table

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59828&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12471;;;","27/May/24 06:01;lsy;Merged in master: 90e2d6cfeea44f6302b9d1f1ee4396c5a5c69b8f;;;",,,,
Retraction stop working with Clock dependent function in Filter,FLINK-35341,13579109,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,qingwei91,qingwei91,13/May/24 15:58,20/May/24 05:13,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,Table SQL / Runtime,,,,0,,," 

Say we have a Flink SQL view where
 # we use clock dependent function like `UNIX_TIMESTAMP()` in query filter, eg. WHERE clause, eg. table.timestamp < UNIX_TIMESTAMP()
 # source record is retracted at a time where the filter is evaluated as false

we expect a retraction is produced from the view, but in practice nothing happen.

 

We are using kafka as a source, here's a small snippet that shows the problem.

 
{code:java}
CREATE TEMPORARY VIEW my_view AS
    SELECT key,
            someData,
            expiry
    FROM upstream 
    WHERE expiry > UNIX_TIMESTAMP();


select * from my_view where key = 5574332;{code}
 

 

The actual query is a bit more complicated but this simplified one should illustrate the issue. Below is the event happen in chronological order:

 
 # Run this query as a stream
 # Create a record in upstream where key = 5574332, and expiry to be in 3 minutes into the future.
 # Observe insertion of the record, as expected
 # Wait for 3 minutes
 # Now the record should expired, but given there's no update, there's no change to the stream output just yet
 # Delete the upstream record (using tombstone in kafka)
 # Observe no change in the stream output, but we are expecting retraction(aka deletion)

 

Is this a known issue? I've search Jira but could find any, I observed this in 1.15 until 1.17, havent tested with 1.18 and above though",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 20 05:13:06 UTC 2024,,,,,,,,,,"0|z1p700:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/24 05:13;libenchao;I think it's a known issue, and has been resolved with the ""table.optimizer.non-deterministic-update.strategy""[1] feature, you can try that.

https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/table/config/#table-optimizer-non-deterministic-update-strategy;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kubernetes Operator CRDs not applying correctly,FLINK-35340,13579098,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,neilprasad,neilprasad,13/May/24 14:01,13/May/24 14:01,04/Jun/24 20:40,,kubernetes-operator-1.7.0,kubernetes-operator-1.8.0,,,,,,,,,,Kubernetes Operator,,,,0,,,"From version 1.7.0 onwards, CRDs include an additional parameter for additionalPrinterColumns, ""priority"":


{code:java}
  - additionalPrinterColumns:
    - description: Last observed state of the job.
      jsonPath: .status.jobStatus.state
      name: Job Status
      priority: 0
      type: string
    - description: ""Lifecycle state of the Flink resource (including being rolled\
        \ back, failed etc.).""
      jsonPath: .status.lifecycleState
      name: Lifecycle State
      priority: 0
      type: string {code}
When applying the CRDs initially or as an upgrade the CRD doesn't apply correctly and omits the newly added ""priority"" field. This is both tested on a fresh install via Minikube as well as an existing GKE deployment.

For what it's worth, the versions test on is k8s v1.29.3. When applying these CRDs, there is no error message that comes up and nothing appears in the api server logs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-13 14:01:44.0,,,,,,,,,,"0|z1p6xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compilation timeout while building flink-dist,FLINK-35339,13579095,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,13/May/24 13:37,31/May/24 13:24,04/Jun/24 20:40,,1.19.1,,,,,,,,,,,,,,,0,test-stability,,"* 1.19 Java 17 / Test (module: python) https://github.com/apache/flink/actions/runs/9040330904/job/24844527283#step:10:14325

The CI pipeline fails with:

{code}
May 11 02:44:25 Process exited with EXIT CODE: 143.
May 11 02:44:25 Trying to KILL watchdog (49546).
May 11 02:44:25 ==============================================================================
May 11 02:44:25 Compilation failure detected, skipping test execution.
May 11 02:44:25 ==============================================================================
{code}

It looks like this is due to a failed network connection while building src/assemblies/bin.xml :

{code}
May 11 02:44:25    java.lang.Thread.State: RUNNABLE
May 11 02:44:25 	at sun.nio.ch.Net.connect0(java.base@17.0.7/Native Method)
May 11 02:44:25 	at sun.nio.ch.Net.connect(java.base@17.0.7/Net.java:579)
May 11 02:44:25 	at sun.nio.ch.Net.connect(java.base@17.0.7/Net.java:568)
May 11 02:44:25 	at sun.nio.ch.NioSocketImpl.connect(java.base@17.0.7/NioSocketImpl.java:588)
May 11 02:44:25 	at java.net.SocksSocketImpl.connect(java.base@17.0.7/SocksSocketImpl.java:327)
May 11 02:44:25 	at java.net.Socket.connect(java.base@17.0.7/Socket.java:633)
May 11 02:44:25 	at org.apache.maven.wagon.providers.http.httpclient.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:368)
May 11 02:44:25 	at org.apache.maven.wagon.providers.http.httpclient.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)
May 11 02:44:25 	at org.apache.maven.wagon.providers.http.httpclient.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)
May 11 02:44:25 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)
May 11 02:44:25 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
May 11 02:44:25 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
May 11 02:44:25 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.execute(RetryExec.java:89)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 13:24:46 UTC 2024,,,,,,,,,,"0|z1p6ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/24 13:24;rskraba;* 1.20 Default (Java 8) / Test (module: python) https://github.com/apache/flink/actions/runs/9315015519/job/25642331343#step:10:14876;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable FS Plugins as non-root,FLINK-35338,13579058,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,rkth,rkth,13/May/24 11:25,14/May/24 07:32,04/Jun/24 20:40,,1.8.0,,,,,,,,,,,Kubernetes Operator,,,,0,,,"[This pull request|https://github.com/apache/flink-kubernetes-operator/pull/609] was made to allow enabling FS plugins on the Flink Kubernetes Operator which allows reading a jar for a session job on various file systems. It normally works well, but we are running our cluster with *[Restricted Pod Security|https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted]* which among other things means the Flink Operator pod is configured to use *readOnlyRootFilesystem* and *runAsNonRoot* which means we are not allowed to write to our plugins directory.

We have tried using *operatorVolumes* and *operatorVolumeMounts* to mount */opt/flink/plugins* which would allow us to write to it, but that overrides all the pre-installed plugins. When all the pre-installed plugins are removed before startup, the operator sees the directory for the plugin we are trying to install, but does not find a jar file inside the directory and therefore complains. We think that when the pre-installed plugins are there, the operator takes a bit longer before it starts reading the new plugin and therefore there is enough time to download the new plugin with curl.

We are open to suggestions for how we can solve this issue while keeping *readOnlyRootFilesystem* and {*}runAsNonRoot{*}. We are considering a solution where we mount a volume and download all the pre-installed plugins as well as any extra plugins we need through an init container and we propose a new value to the Flink Operator Helm chart.

We have tested that it also works if we build our own image where we add the plugin, but we need to deploy the operator in different clusters with different requirements for filesystems so we would have to create a new image for each filesystem as well as updating all our own images every time there is an update to the official Flink Operator image",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-13 11:25:42.0,,,,,,,,,,"0|z1p6oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keep up with the latest version of tikv client,FLINK-35337,13579057,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,ouyangwuli,ouyangwuli,13/May/24 11:20,22/May/24 02:12,04/Jun/24 20:40,22/May/24 02:12,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 11:35:34 UTC 2024,,,,,,,,,,"0|z1p6og:",9223372036854775807,tikv version not supported,,,,,,,,,,,,,,,,,,,"13/May/24 11:35;ouyangwuli;[~Leonard]， Our production environment has been updated  tikv version 6.5.4,Can we update it to new version.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL failed to restore from savepoint after change in default-parallelism,FLINK-35336,13579052,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leekeiabstraction,leekeiabstraction,13/May/24 10:36,13/May/24 12:25,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / Planner,,,,0,,,"After bumping 'table.exec.resource.default-parallelism' from 1 to 4, I am observing the following exception on restoring job from savepoint with an unmodified statement set. 
 
{quote}[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint [file:/tmp/flink-savepoints/savepoint-4392e6-575fa6b692ff|file:///tmp/flink-savepoints/savepoint-4392e6-575fa6b692ff]. Cannot map checkpoint/savepoint state for operator 46ba9b22862c3bbe9373c6abee964b2a to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.
{quote}
When started without savepoints, the jobgraph differs for the jobs despite identical statements being ran.

There are 2 operators when default parallelism is 1.
{quote}A: Source: UserBehaviourKafkaSource[68] -> (Calc[69] -> StreamRecordTimestampInserter[70] -> StreamingFileWriter -> Sink: end, Calc[71] -> LocalWindowAggregate[72])
B: GlobalWindowAggregate[74] -> Calc[75] -> Sink: CampaignAggregationsJDBC[76]
{quote}
Three operators when default parallelism is 4.
{quote}A: Source: UserBehaviourKafkaSource[86] -> (Calc[87] -> StreamRecordTimestampInserter[88] -> StreamingFileWriter, Calc[89] -> LocalWindowAggregate[90]) 
B: Sink: end 
C: GlobalWindowAggregate[92] -> Calc[93] -> Sink: CampaignAggregationsJDBC[94]
{quote}
 
Notice that the operator 'Sink: end' is separated out when parallelism is set to 4, causing the incompatibility in job graph. EXPLAIN PLAN did not show any difference between syntax tree, physical plan or execution plan.

I have attempted various configurations in `table.optimizer.*`.

Steps to reproduce
{quote}SET 'table.exec.resource.default-parallelism' = '1';
EXECUTE STATEMENT SET BEGIN 
    INSERT INTO UserErrorExperienceS3Sink (user_id, user_session, interaction_type, interaction_target, interaction_tags, event_date, event_hour, event_time)
    SELECT
        user_id, 
        user_session,
        interaction_type,
        interaction_target,
        interaction_tags, 
        DATE_FORMAT(event_time , 'yyyy-MM-dd'),
        DATE_FORMAT(event_time , 'HH'),
        event_time 
    FROM UserBehaviourKafkaSource 
    WHERE 
        interaction_result Like '%ERROR%'; 

    INSERT INTO CampaignAggregationsJDBC 
    SELECT 
        CONCAT_WS('/', interaction_tags, interaction_result, DATE_FORMAT(window_start, 'YYYY-MM-DD HH:mm:ss.SSS'), DATE_FORMAT(window_end, 'YYYY-MM-DD HH:mm:ss.SSS')) AS id, 
        interaction_tags as campaign, 
        interaction_result, 
        COUNT(*) AS interaction_count, 
        window_start, 
        window_end 
    FROM 
        TABLE(TUMBLE(TABLE UserBehaviourKafkaSource, DESCRIPTOR(event_time), INTERVAL '10' SECONDS)) 
    GROUP BY window_start, window_end, interaction_tags, interaction_result; 
END;

STOP JOB '<JOB_ID>' WITH SAVEPOINT;
SET 'execution.savepoint.path' = '/<SAVEPOINT_PATH>/';
SET 'table.exec.resource.default-parallelism' = '4';

<Re-run DML at line 2>
{quote}
DDLs
{quote}– S3 Sink
CREATE TABLE UserErrorExperienceS3Sink (
  user_id BIGINT,
  user_session STRING,
  interaction_type STRING,
  interaction_target STRING,
  interaction_tags STRING,
  event_date STRING,
  event_hour STRING,
  event_time TIMESTAMP(3) WITHOUT TIME ZONE)
PARTITIONED BY (event_date, event_hour)
WITH (
  'connector' = 'filesystem',
  'path' = 's3://<S3BUCKET>/userErrorExperience/',
  'format' = 'json');

– Kafka Source
ADD JAR 'file:///Users/leekei/Downloads/flink-sql-connector-kafka-3.1.0-1.18.jar';
CREATE TABLE UserBehaviourKafkaSource (
  user_id BIGINT,
  user_session STRING,
  interaction_type STRING,
  interaction_target STRING,
  interaction_tags STRING,
  interaction_result STRING,
  event_time TIMESTAMP(3) WITHOUT TIME ZONE METADATA FROM 'timestamp',
  WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND)
WITH (
  'connector' = 'kafka',
  'topic' = 'user_behaviour',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'demoGroup',
  'scan.startup.mode' = 'latest-offset',
  'format' = 'csv');

– PostgreSQL Source/Sink 
ADD JAR 'file:///Users/leekei/Downloads/flink-connector-jdbc-3.1.2-1.18.jar';
ADD JAR 'file:///Users/leekei/Downloads/postgresql-42.7.3.jar';
CREATE TABLE CampaignAggregationsJDBC (
  id STRING,
  campaign STRING,
  interaction_result STRING,
  interaction_count BIGINT,
  window_start TIMESTAMP(3) WITHOUT TIME ZONE,
  window_end TIMESTAMP(3) WITHOUT TIME ZONE)
WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:postgresql://localhost:5432/postgres',
  'table-name' = 'campaign_aggregations');
{quote}","Flink SQL Client, Flink 1.18.1 on MacOS",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 12:25:44 UTC 2024,,,,,,,,,,"0|z1p6nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 12:25;martijnvisser;That's not a bug: changing the parallelism is generates a new jobgraph and that will lead to state incompatibility, as outlined in https://nightlies.apache.org/flink/flink-docs-master/docs/ops/upgrading/#table-api--sql

See https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489 for more details as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateCheckpointedITCase failed fatally with 127 exit code,FLINK-35335,13579049,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rskraba,rskraba,13/May/24 10:18,13/May/24 14:04,04/Jun/24 20:40,,1.19.1,,,,,,,,,,,,,,,0,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59499&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8379

{code}
May 13 01:50:22 01:50:22.272 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.03 s -- in org.apache.flink.test.streaming.runtime.CacheITCase
May 13 01:50:23 01:50:23.142 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.234 s -- in org.apache.flink.test.streaming.experimental.CollectITCase
May 13 01:50:23 01:50:23.611 [INFO] 
May 13 01:50:23 01:50:23.611 [INFO] Results:
May 13 01:50:23 01:50:23.611 [INFO] 
May 13 01:50:23 01:50:23.611 [WARNING] Tests run: 1960, Failures: 0, Errors: 0, Skipped: 25
May 13 01:50:23 01:50:23.611 [INFO] 
May 13 01:50:23 01:50:23.674 [INFO] ------------------------------------------------------------------------
May 13 01:50:23 01:50:23.674 [INFO] BUILD FAILURE
May 13 01:50:23 01:50:23.674 [INFO] ------------------------------------------------------------------------
May 13 01:50:23 01:50:23.676 [INFO] Total time:  41:24 min
May 13 01:50:23 01:50:23.677 [INFO] Finished at: 2024-05-13T01:50:23Z
May 13 01:50:23 01:50:23.677 [INFO] ------------------------------------------------------------------------
May 13 01:50:23 01:50:23.677 [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
May 13 01:50:23 01:50:23.678 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.2.2:test (integration-tests) on project flink-tests: 
May 13 01:50:23 01:50:23.678 [ERROR] 
May 13 01:50:23 01:50:23.678 [ERROR] Please refer to /__w/2/s/flink-tests/target/surefire-reports for the individual test results.
May 13 01:50:23 01:50:23.678 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
May 13 01:50:23 01:50:23.678 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
May 13 01:50:23 01:50:23.678 [ERROR] Command was /bin/sh -c cd '/__w/2/s/flink-tests' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/2/s/flink-tests/target/surefire/surefirebooter-20240513010926195_686.jar' '/__w/2/s/flink-tests/target/surefire' '2024-05-13T01-09-20_665-jvmRun1' 'surefire-20240513010926195_684tmp' 'surefire_206-20240513010926195_685tmp'
May 13 01:50:23 01:50:23.679 [ERROR] Error occurred in starting fork, check output in log
May 13 01:50:23 01:50:23.679 [ERROR] Process Exit Code: 127
May 13 01:50:23 01:50:23.679 [ERROR] Crashed tests:
May 13 01:50:23 01:50:23.679 [ERROR] org.apache.flink.test.checkpointing.StateCheckpointedITCase
May 13 01:50:23 01:50:23.679 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
May 13 01:50:23 01:50:23.679 [ERROR] Command was /bin/sh -c cd '/__w/2/s/flink-tests' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/2/s/flink-tests/target/surefire/surefirebooter-20240513010926195_686.jar' '/__w/2/s/flink-tests/target/surefire' '2024-05-13T01-09-20_665-jvmRun1' 'surefire-20240513010926195_684tmp' 'surefire_206-20240513010926195_685tmp'
May 13 01:50:23 01:50:23.679 [ERROR] Error occurred in starting fork, check output in log
May 13 01:50:23 01:50:23.679 [ERROR] Process Exit Code: 127
May 13 01:50:23 01:50:23.679 [ERROR] Crashed tests:
May 13 01:50:23 01:50:23.679 [ERROR] org.apache.flink.test.checkpointing.StateCheckpointedITCase
May 13 01:50:23 01:50:23.679 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456)
May 13 01:50:23 01:50:23.679 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:418)
May 13 01:50:23 01:50:23.679 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
May 13 01:50:23 01:50:23.679 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:250)
{code}

In the maven logs, {{runCheckpointedProgram[FailoverStrategy: RestartPipelinedRegionFailoverStrategy]}} is started but never completes.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 14:04:10 UTC 2024,,,,,,,,,,"0|z1p6mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 14:04;rskraba;* 1.19 test_cron_adaptive_scheduler tests https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59499&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code generation: init method exceeds 64 KB when there is a long array field with Table API,FLINK-35334,13579034,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,simonlee1206,simonlee1206,13/May/24 09:15,22/May/24 05:04,04/Jun/24 20:40,,1.17.1,1.18.1,1.19.0,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,0,,,"Hi team,

I encountered the following error when trying to execute SQL on a table that has rows with fields that are long arrays (e.g., array length > 200):
{code:java}
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'BatchExecCalc$4950'
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:84)
 ...
Caused by: org.codehaus.janino.InternalCompilerException: Code of method ""<init>([Ljava/lang/Object;Lorg/apache/flink/streaming/runtime/tasks/StreamTask;Lorg/apache/flink/streaming/api/graph/StreamConfig;Lorg/apache/flink/streaming/api/operators/Output;Lorg/apache/flink/streaming/runtime/tasks/ProcessingTimeService;)V"" of class ""BatchExecCalc$4950"" grows beyond 64 KB {code}
A minimal example that reproduces the situation described above:
[https://gist.github.com/nlpersimon/df71c0bec93c13667965ce1706099fdb]

After running the example with Intellij, I got this output:
[https://gist.github.com/nlpersimon/f741b79c37da7426aeefc7a157cdd124]

Please let me know if you need any other information. Thank you!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 20 04:00:34 UTC 2024,,,,,,,,,,"0|z1p6jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/24 04:00;simonlee1206;Hi [~TsReaper] , I noticed that you solved a similar issue before:
https://issues.apache.org/jira/browse/FLINK-25491

Could you please take a look at this issue if you have some bandwidth?

Thank you very much!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcXaSinkTestBase fails in weekly Flink JDBC Connector tests,FLINK-35333,13579032,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,eskabetxe,martijnvisser,martijnvisser,13/May/24 09:06,31/May/24 10:59,04/Jun/24 20:40,31/May/24 10:59,jdbc-3.2.0,,,,,,,,jdbc-3.2.0,,,Connectors / JDBC,,,,0,pull-request-available,test-stability,"https://github.com/apache/flink-connector-jdbc/actions/runs/9047366679/job/24859224407#step:15:147

{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-jdbc: Compilation failure
Error:  /home/runner/work/flink-connector-jdbc/flink-connector-jdbc/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/xa/JdbcXaSinkTestBase.java:[164,37] <anonymous org.apache.flink.connector.jdbc.xa.JdbcXaSinkTestBase$1> is not abstract and does not override abstract method getTaskInfo() in org.apache.flink.api.common.functions.RuntimeContext
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 10:59:44 UTC 2024,,,,,,,,,,"0|z1p6iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 16:32;eskabetxe;[~martijnvisser] could you assign this to me..;;;","14/May/24 16:41;eskabetxe;I think I resolve the problem, but on v3.1 cut the branch was not prepared for 1.19

I introduce this change on weekly workflow with the new Sink, I know I don't know if was ok..
{code:json}
{
    flink: 1.19.0,
    jdk: '8, 11, 17, 21',
    branch: v3.1
}
{code}


Should we avoid add new version checks on weekly that the cut was not prepared for??

The simple solution is to remove that from weekly file from master as v3.1 cut was not prepared for 1.19 versions
;;;","14/May/24 17:15;eskabetxe;on #120 I try to fix the error, but another error appears so I believe that remove the check for v3.1 is the best solution (done on #121)

I left the two PRs open so we can evaluate how to solve this.;;;","31/May/24 09:17;Sergey Nuyanzin;Thanks for working on it [~eskabetxe]
Merged as [c80f95c5461464b5cf7613602e1bbd097ff418d8|https://github.com/apache/flink-connector-jdbc/commit/c80f95c5461464b5cf7613602e1bbd097ff418d8];;;","31/May/24 09:20;Sergey Nuyanzin;I've started weekly job to see whether it passes or not
https://github.com/apache/flink-connector-jdbc/actions/runs/9315890964

in case it passes we can close the issue as fixed;;;","31/May/24 10:59;Sergey Nuyanzin;weekly passed
https://github.com/apache/flink-connector-jdbc/actions/runs/9316249855;;;",,,,,,,,,,,,,,,,,,,,,,,
Manually setting rest.bind-address does not work for Flink running on Hadoop Yarn dual network environment,FLINK-35332,13579018,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,paul8263,paul8263,13/May/24 07:47,13/May/24 07:58,04/Jun/24 20:40,,1.15.4,,,,,,,,,,,Deployment / YARN,,,,0,pull-request-available,,"Given the Hadoop Yarn cluster with dual networks:
* 192.168.x.x: For data transfer. Speed: 10Gbps.
* 10.x.x.x: For management only. Speed: 1Gbps.

 

A client outside the Hadoop Yarn cluster is configured, with management network only(10.x.x.x) and data transfer high speed network not accessible. To reproduce, we sumbit a Flink job from this client(Batch word count for example), the job can be successfully submitted but the result cannot be retrieved, with the exception: Connection refused: \{jobmanager_hostname}:\{jm_port}. The root cause is the job manager rest address is bind to its actual address (192.168.x.x) rather than 0.0.0.0. Manually setting rest.bind-address does not work.

 

One of the changes in Flink-24474 in YarnEntrypointUtils overwrites RestOptions.BIND_ADDRESS to the node's actual address. This change should be reverted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-13 07:47:33.0,,,,,,,,,,"0|z1p6fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Download links for binary releases are displayed as source releases on website,FLINK-35331,13578850,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hector.s,xtsong,xtsong,10/May/24 08:08,16/May/24 15:00,04/Jun/24 20:40,,,,,,,,,,,,,Project Website,,,,0,pull-request-available,,"Take Pre-bundled Hadoop as examples. The content for downloading are binary releases, while the link is displayed as ""Pre-bundled Hadoop 2.x.y Source Release (asc, sha512)"". The problem is caused by misusing `source_release_[url|asc_url|sha512_url]` for binary contents in the corresponding [yaml file.|https://github.com/apache/flink-web/blob/asf-site/docs/data/additional_components.yml]

There are many similar cases in the webpage.

And a relevant issues is that, some source releases are displayed as ""XXX Source Release Source Release"", due to including ""Source Release"" in the `name` field of the corresponding yaml file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 12:21:41 UTC 2024,,,,,,,,,,"0|z1p5eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 12:21;hector.s;Added [PR#742|https://github.com/apache/flink-web/pull/742] with fix.

The shortcode {{flink_download}} used to create the download links had the property {{source_release_url}} mandatory. I just added a conditional so no need to have that property anymore. This way we can have only the {{binary_release_url}} for those binary only downloads.

I have updated the `[additional_components.yml|https://github.com/apache/flink-web/pull/742/files#diff-45fd031d774cda3e995f0ebe71de328688f561f75000a4d0a234fccd72bd2151]` file update the hadoop links. I haven't found any other download links with binary only. Please report if there is any other missing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for Vertica Connector,FLINK-35330,13578842,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,otvgarg,otvgarg,10/May/24 06:45,10/May/24 06:45,04/Jun/24 20:40,,jdbc-3.1.2,,,,,,,,1.8.4,,,Connectors / JDBC,,,,0,Vertica,,"I am from Opentext and our product Vertica is one of the most widely used analytical database.

we have developed a Flink-Vertica connector which will be made opensource.

 

We will be managing the forward development, enhancements and improvements of the Flink connector.

 

Looking to get it approved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,2024-05-10 06:45:25.0,,,,,,,,,,"0|z1p5cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support modify table/column comments,FLINK-35329,13578836,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,10/May/24 05:32,10/May/24 05:34,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,,,,Flink CDC,,,,0,,,The database has ddl sql to change the comment of the table/column. It is recommended to add the change comment event. You can also set unsynchronized comments. Some scenarios cannot synchronize comments.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 05:34:55 UTC 2024,,,,,,,,,,"0|z1p5bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/24 05:34;xiqian_yu;Hi [~melin], I'm trying to add AlterTableCommentEvent / AlterColumnCommentEvent support in PR https://github.com/apache/flink-cdc/pull/3296, PTAL.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutoScale supports setting the maximum floating parallelism by the number of Pulsar partitions,FLINK-35328,13578830,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,shenwenbing,shenwenbing,10/May/24 03:55,27/May/24 06:03,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,AutoScale supports setting the maximum floating parallelism by the number of Pulsar partitions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-10 03:55:38.0,,,,,,,,,,"0|z1p5a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Explain show push down condition ,FLINK-35327,13578829,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xuyangzhong,loserwang1024,loserwang1024,10/May/24 03:52,10/May/24 15:14,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,Table SQL / Planner,,,,0,,,"Current, we can not determine whether filter/limit/partition condition is pushed down to source. For example, we can only know filter condition is pushed down if it is not included in Filter any more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 15:14:05 UTC 2024,,,,,,,,,,"0|z1p5a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/24 03:53;loserwang1024;[~lincoln] , CC;;;","10/May/24 04:02;xuyangzhong;Hi, I'd like to take this Jira and take a look at it.;;;","10/May/24 15:14;lincoln.86xy;Hi [~loserwang1024], thanks for reporting this! Assigned to you [~xuyangzhong].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement lineage interface for hive connector,FLINK-35326,13578824,13526635,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ZhenqiuHuang,ZhenqiuHuang,10/May/24 03:16,24/May/24 10:44,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-10 03:16:43.0,,,,,,,,,,"0|z1p58w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paimon connector miss the position of AddColumnEvent,FLINK-35325,13578823,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kunni,kunni,10/May/24 02:59,03/Jun/24 02:38,04/Jun/24 20:40,,cdc-3.1.1,,,,,,,,cdc-3.1.1,,,Flink CDC,,,,0,pull-request-available,,"Currently, new columns are always added in the last position, however some newly add columns had a specific before and after relationship with other column.

Source code:

[https://github.com/apache/flink-cdc/blob/fa6e7ea51258dcd90f06036196618224156df367/flink-cdc-connect/flink-cdc-pipeline-connectors/flink-cdc-pipeline-connector-paimon/src/main/java/org/apache/flink/cdc/connectors/paimon/sink/PaimonMetadataApplier.java#L137]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 02:04:48 UTC 2024,,,,,,,,,,"0|z1p58o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 02:04;tianzhu.wen_1998;I'd like to fix it, please assign it to me, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro format can not perform projection pushdown for specific fields,FLINK-35324,13578821,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,sudewei.sdw,sudewei.sdw,10/May/24 02:57,27/May/24 02:01,04/Jun/24 20:40,,1.17.0,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,"AvroFormatFactory.java#createDecodingFormat would return a ProjectableDecodingFormat，which means avro format deserializer could perform the projection pushdown. However, it is found in practice that the Avro format seems unable to perform projection pushdown for specific fields. 

For example, there are such schema and sample data in Kafka:
{code:java}
-- schema
CREATE TABLE kafka (
   `user_id` BIGINT,
   `name` STRING,
    `timestamp` TIMESTAMP(3) METADATA,
    `event_id` BIGINT,
    `payload` STRING not null
) WITH (
     'connector' = 'kafka',
     ...
)
 
 -- sample data like    
(3, 'name 3', TIMESTAMP '2020-03-10 13:12:11.123', 102, 'payload 3') {code}
The data can be successfully deserialized in this way:
{code:java}
Projection physicalProjections = Projection.of( new int[] {0,1,2} );

DataType physicalFormatDataType = physicalProjections.project(this.physicalDataType);

(DeserializationSchema<RowData>) ((ProjectableDecodingFormat) format)
    .createRuntimeDecoder(context, this.physicalDataType, physicalProjections.toNestedIndexes()); {code}
The data would be:
{code:java}
+I(3,name 3,102) {code}
However, when the projection index is replaced with values that do not start from 0, the data cannot be successfully deserialized, for example:
{code:java}
Projection physicalProjections = Projection.of( new int[] {1,2} );

DataType physicalFormatDataType = physicalProjections.project(this.physicalDataType);

(DeserializationSchema<RowData>) ((ProjectableDecodingFormat) format)
    .createRuntimeDecoder(context, this.physicalDataType, physicalProjections.toNestedIndexes()); {code}
The exception would be like:
{code:java}
Caused by: java.lang.ArrayIndexOutOfBoundsException: -49
        at org.apache.avro.io.parsing.Symbol$Alternative.getSymbol(Symbol.java:460)
        at org.apache.avro.io.ResolvingDecoder.readIndex(ResolvingDecoder.java:283)
        at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:188)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:161)
        at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:260)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:248)
        at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:180)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:161)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:154)
        at org.apache.flink.formats.avro.AvroDeserializationSchema.deserialize(AvroDeserializationSchema.java:142)
        at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.deserialize(AvroRowDataDeserializationSchema.java:103)
        ... 19 more {code}
It seems that Avro format does not support projection pushdown for arbitrary fields. Is my understanding correct?

If this is the case, then I think Avro format should not implement the ProjectableDecodingFormat interface , since it can only provide very limited pushdown capabilities.

This problem may block the connector implementing the projection pushdown capability since the connector would determine whether projection pushdown can be performed by judging whether the format has implemented the ProjectableDecodingFormat interface or not.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 02:01:06 UTC 2024,,,,,,,,,,"0|z1p588:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 02:01;Weijie Guo;Does 1.20 has the same problem?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only the schema of the first hit table is recorded when the source-table of the transformer hits multiple tables,FLINK-35323,13578820,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,wink,wink,10/May/24 02:23,03/Jun/24 09:39,04/Jun/24 20:40,03/Jun/24 04:06,cdc-3.1.0,,,,,,,,cdc-3.1.1,cdc-3.2.0,,Flink CDC,,,,0,pull-request-available,,"{code:java}
transform:
  - source-table: mydb.web_\.*
    projection: \*, localtimestamp as new_timestamp
   description: project fields from source table {code}
Table mydb.web_order: col1, col2, col3

Table mydb.web_info: col1, col4

If transform data operator processes `mydb.web_info` first and then `mydb.web_order`, its schema will always be `col1, col4`.

Cause by:  TransformDataOperator.java
{code:java}
private transient Map<TransformProjection, TransformProjectionProcessor>
        transformProjectionProcessorMap;
private transient Map<TransformFilter, TransformFilterProcessor> transformFilterProcessorMap; {code}
The relationship of `TableId` is missing here.",,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 04:05:18 UTC 2024,,,,,,,,,,"0|z1p580:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/24 04:05;leonard;master: 88afc5f18d051b52da86b5017dc896a4a66e6e10
3.1: f4045fb8e78d905c0c7494a0485b1aca40a12f62;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PubSub Connector Weekly build fails ,FLINK-35322,13578772,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chalixar,chalixar,chalixar,09/May/24 15:57,24/May/24 09:17,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Google Cloud PubSub,,,,0,pull-request-available,test-stability,"Weekly builds for GCP pubSub connector is failing for 1.19 due to compilation error in tests.

https://github.com/apache/flink-connector-gcp-pubsub/actions/runs/8768752932/job/24063472769
https://github.com/apache/flink-connector-gcp-pubsub/actions/runs/8863605354
https://github.com/apache/flink-connector-gcp-pubsub/actions/runs/8954270618
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 12 10:14:56 UTC 2024,,,,,,,,,,"0|z1p4xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 16:07;chalixar;[~snuyanzin] It seems that you have already provided the fix, we should bump the version under test once 3.1 is released. 

Would be nice to speed up the vote.;;;","12/May/24 09:20;Sergey Nuyanzin;Merged as [725f3d66e8065e457acab332216a63184db16e0b|https://github.com/apache/flink-connector-gcp-pubsub/commit/725f3d66e8065e457acab332216a63184db16e0b]

Thanks for the fix [~chalixar];;;","12/May/24 09:22;Sergey Nuyanzin;Currently not sure whether we can close this task, since 3.1.0 is under voting phase and in case it passes this issue will not be a part of 3.1.0...

let's keep it open so far...


or, WDYT, [~danny.cranmer] since you are a RM for this release;;;","12/May/24 10:14;chalixar;[~Sergey Nuyanzin]
yes, let's keep it till after the release as we are not sure about the fix version at the moment.
Also this is not a blocker for the ongoing release since the weekly tests are only triggered from main branch so hence there should be no issue from the current release in my opinion. We just need to follow up with a ticket to re-enable 1.19 tests for 3.1 after the release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointCommittableManagerImpl re-registers pendingCommittables gauge on every commit operation,FLINK-35321,13578747,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lassaegis,lassaegis,09/May/24 12:12,09/May/24 12:12,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,API / Core,Runtime / Checkpointing,Runtime / Metrics,,0,,,"Found while testing a home-made Sink implementation that implements SupportCommitter. We observed that starting from the *second* checkpoint, every committer commit will be accompanied by the warning log:
{quote}
Name collision: Group already contains a Metric with the name 'pendingCommittables'.
{quote}

Enabling the debugger and tracing the origin of this log took us to {{org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImpl}} at line 137 of the commit() method:

{code:java}
metricGroup.setCurrentPendingCommittablesGauge(() -> getPendingRequests(false).size());
{code}

It looks like that instead of modifying the value of the gauge, the manager class is *re-setting with a different guage* on every commit operation, which explains the appearance of the warning log shown above.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-09 12:12:44.0,,,,,,,,,,"0|z1p4rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RabbitMQ Source fails to consume from quorum queues when prefetch Count is set,FLINK-35320,13578741,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chalixar,chalixar,09/May/24 11:09,13/May/24 09:33,04/Jun/24 20:40,,1.16.0,1.16.2,1.16.3,,,,,,rabbitmq-3.1.0,,,Connectors/ RabbitMQ,,,,0,,,"h2. Description

{{RMQSource}} currently sets prefetch Count with [global QoS |https://github.com/apache/flink-connector-rabbitmq/blob/66e323a3e79befc08ae03f2789a8aa94b343d504/flink-connector-rabbitmq/src/main/java/org/apache/flink/streaming/connectors/rabbitmq/RMQSource.java#L223] which is incompatible with [Quorum queues|https://www.rabbitmq.com/docs/quorum-queues#global-qos].


h2. Consideration

Currently the {{RMQSource}} implements {{SourceFunction}} which is deprecated from 1.18, the current RabbitMQ connector is compatible with 1.16 which is out of support, another approach would be migrating the source to the new API for the next connector release.   
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-09 11:09:51.0,,,,,,,,,,"0|z1p4qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specific Operator getting stuck in Initialization for a long time,FLINK-35319,13578725,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kartikeypant,kartikeypant,09/May/24 08:50,09/May/24 08:50,04/Jun/24 20:40,,1.16.1,,,,,,,,,,,,,,,0,,,"Our team has been working on a Flink service. After completing the service development, we moved on to the Job Stabilisation exercises at the production load.
 
During high load, we see that if the job restarts (mostly due to the ""org.apache.flink.util.FlinkExpectedException: The TaskExecutor is shutting down""), one of the operators gets stuck in the INITIALISATION state.
 
This happens even when all the required capacity is present and all the TMs are up and running. Other operators that have even higher parallelism than this particular operator initialise fast whilst this particular operator sometimes takes more than 30 minutes.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/24 08:50;kartikeypant;Screenshot 2024-05-09 at 2.19.10 PM.png;https://issues.apache.org/jira/secure/attachment/13068766/Screenshot+2024-05-09+at+2.19.10%E2%80%AFPM.png","09/May/24 08:48;kartikeypant;image.png;https://issues.apache.org/jira/secure/attachment/13068767/image.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-09 08:50:35.0,,,,,,,,,,"0|z1p4mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect timezone handling for TIMESTAMP_WITH_LOCAL_TIME_ZONE type during predicate pushdown,FLINK-35318,13578701,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,linshangquan,linshangquan,linshangquan,09/May/24 06:18,21/May/24 06:45,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,"In our scenario, we have an Iceberg table that contains a column named 'time' of the {{timestamptz}} data type. This column has 10 rows of data where the 'time' value is {{'2024-04-30 07:00:00'}} expressed in the ""Asia/Shanghai"" timezone.

!image-2024-05-09-14-06-58-007.png!

 

We encountered a strange phenomenon when accessing the table using Iceberg-flink.

When the {{WHERE}} clause includes the {{time}} column, the results are incorrect.

ZoneId.{_}systemDefault{_}() = ""Asia/Shanghai"" 

!image-2024-05-09-18-52-03-741.png!

When there is no {{WHERE}} clause, the results are correct.

!image-2024-05-09-18-52-28-584.png!

During debugging, we found that when a {{WHERE}} clause is present, a {{FilterPushDownSpec}} is generated, and this {{FilterPushDownSpec}} utilizes {{RexNodeToExpressionConverter}} for translation.

!image-2024-05-09-14-11-38-476.png!

!image-2024-05-09-14-22-59-370.png!

When {{RexNodeToExpressionConverter#visitLiteral}} encounters a {{TIMESTAMP_WITH_LOCAL_TIME_ZONE}} type, it uses the specified timezone ""Asia/Shanghai"" to convert the {{TimestampString}} type to an {{Instant}} type. However, the upstream {{TimestampString}} data has already been processed in UTC timezone. By applying the local timezone processing here, an error occurs due to the mismatch in timezones.

Whether the handling of {{TIMESTAMP_WITH_LOCAL_TIME_ZONE}} type of data in {{RexNodeToExpressionConverter#visitLiteral}} is a bug, and whether it should process the data in UTC timezone.

 
Please help confirm if this is the issue, and if so, we can submit a patch to fix it.
 
 ","flink version 1.18.1

iceberg version 1.15.1",,,,,,,,,,,,,,,,,,,,,,,,,"09/May/24 06:06;linshangquan;image-2024-05-09-14-06-58-007.png;https://issues.apache.org/jira/secure/attachment/13068753/image-2024-05-09-14-06-58-007.png","09/May/24 06:09;linshangquan;image-2024-05-09-14-09-38-453.png;https://issues.apache.org/jira/secure/attachment/13068752/image-2024-05-09-14-09-38-453.png","09/May/24 06:11;linshangquan;image-2024-05-09-14-11-38-476.png;https://issues.apache.org/jira/secure/attachment/13068751/image-2024-05-09-14-11-38-476.png","09/May/24 06:22;linshangquan;image-2024-05-09-14-22-14-417.png;https://issues.apache.org/jira/secure/attachment/13068754/image-2024-05-09-14-22-14-417.png","09/May/24 06:23;linshangquan;image-2024-05-09-14-22-59-370.png;https://issues.apache.org/jira/secure/attachment/13068755/image-2024-05-09-14-22-59-370.png","09/May/24 10:52;linshangquan;image-2024-05-09-18-52-03-741.png;https://issues.apache.org/jira/secure/attachment/13068770/image-2024-05-09-18-52-03-741.png","09/May/24 10:52;linshangquan;image-2024-05-09-18-52-28-584.png;https://issues.apache.org/jira/secure/attachment/13068771/image-2024-05-09-18-52-28-584.png",,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 06:45:33 UTC 2024,,,,,,,,,,"0|z1p4hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 07:52;linshangquan;[~qingyue]  Please take a look at this issue when you have time. Thanks;;;","10/May/24 16:04;qingyue;Hi [~linshangquan], thanks for reporting this issue. Your understanding is correct. 
RexNodeToExpressionConverter#visitLiteral should not convert the literal to UTC again since this has been done before at the SQL to Rel phase.;;;","16/May/24 02:15;linshangquan;[https://github.com/apache/flink/pull/24787]

Hi [~qingyue] , Could you possibly review this PR. Thank you very much!;;;","21/May/24 06:45;qingyue;Thanks for your contribution [~linshangquan], I'll take a look as quickly as I can.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC CLI Supports submitting multiple YAML job at once,FLINK-35317,13578700,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xiqian_yu,xiqian_yu,09/May/24 06:16,10/May/24 02:22,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"Currently, Flink CDC CLI only allows submitting one YAML pipeline job each time. It would be convenient if users can submit multiple .yml files at once like this:

{{./bin/flink-cdc.sh job1.yml job2.yml --flink-home /opt/flink ...}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-09 06:16:15.0,,,,,,,,,,"0|z1p4hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add CDC e2e test case for on Flink 1.19,FLINK-35316,13578699,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xiqian_yu,xiqian_yu,09/May/24 06:10,24/May/24 15:53,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"Since Flink 1.19 has been generally available, Flink CDC is expected to be used with it. E2e test cases should cover this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-09 06:10:53.0,,,,,,,,,,"0|z1p4h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryManagerConcurrentModReleaseTest executes more than 15 minutes,FLINK-35315,13578688,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fanrui,fanrui,09/May/24 03:53,09/May/24 03:54,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Runtime / Network,Tests,,,0,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59395&view=results]

 

It seems 

MemoryManagerConcurrentModReleaseTest.testConcurrentModificationWhileReleasing executes more than 15 minutes.

The root cause may be {color:#e1dfdd}ConcurrentModificationException{color}

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59395&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10060]

 

!image-2024-05-09-11-53-10-037.png!

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/24 03:53;fanrui;image-2024-05-09-11-53-10-037.png;https://issues.apache.org/jira/secure/attachment/13068750/image-2024-05-09-11-53-10-037.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-09 03:53:22.0,,,,,,,,,,"0|z1p4eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Flink CDC pipeline transform user document,FLINK-35314,13578683,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wink,wink,09/May/24 03:33,15/May/24 08:00,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,documentation,pull-request-available,"The document outline is as follows:
 # Definition
 # Parameters
 # Metadata Fields
 # Functions
 # Example
 # Problem

 ",,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-09 03:33:27.0,,,,,,,,,,"0|z1p4dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add upsert changelog mode to avoid UPDATE_BEFORE records push down,FLINK-35313,13578682,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wczhu,wczhu,09/May/24 03:12,11/May/24 06:16,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"I try to use flink sql to write mysql cdc-data into redis as a dimension table for other business use. When executing {{UPDATE}} DML, the cdc-data will be converted into {{-D (UPDATE_BEFORE)}} and {{+I (UPDATE_AFTER)}} two records to sink redis. However, delete first will cause other data streams to be lost(NULL) when join data, which is unacceptable.
I think we can add support for [upser changelog mode|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables/#table-to-stream-conversion] by adding changelogMode option with mandatory primary key configuration.Basically, with {{changelogMode=upsert}} we will avoid {{UPDATE_BEFORE}} rows and we will require a primary key for the table.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 09 06:12:15 UTC 2024,,,,,,,,,,"0|z1p4dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 03:13;wczhu;migrate from [https://github.com/apache/flink-cdc/issues/1898|https://github.com/apache/flink-cdc/issues/1898];;;","09/May/24 06:12;loserwang1024;I don't understand whether we need to do it in sql? As far as I know, if your sink is upsert mode and no need retract operation in middle operator, the planner will add a StreamExecDropUpdateBefore node.

I think you can add {{changelogMode=upsert}}  in redis sink.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Insufficient number of arguments were supplied for the procedure or function cdc.fn_cdc_get_all_changes_,FLINK-35312,13578674,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,xiqian_yu,xiqian_yu,09/May/24 02:09,09/May/24 09:28,04/Jun/24 20:40,09/May/24 09:28,,,,,,,,,,,,Flink CDC,,,,0,,,"h3. Flink version

1.17.0
h3. Flink CDC version

2.4.1
h3. Database and its version

sql server 2014
h3. Minimal reproduce step

1
h3. What did you expect to see?

Caused by: java.lang.RuntimeException: SplitFetcher thread 22 received unexpected exception while polling the records
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
... 1 more
Caused by: org.apache.kafka.connect.errors.RetriableException: An exception occurred in the change event producer. This connector will be restarted.
at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:46)
at io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource.executeIteration(SqlServerStreamingChangeEventSource.java:458)
at io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource.execute(SqlServerStreamingChangeEventSource.java:138)
at com.ververica.cdc.connectors.sqlserver.source.reader.fetch.SqlServerStreamFetchTask$LsnSplitReadTask.execute(SqlServerStreamFetchTask.java:161)
at com.ververica.cdc.connectors.sqlserver.source.reader.fetch.SqlServerScanFetchTask.execute(SqlServerScanFetchTask.java:123)
at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.lambda$submitTask$0(IncrementalSourceScanFetcher.java:95)
... 5 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: An insufficient number of arguments were supplied for the procedure or function cdc.fn_cdc_get_all_changes_ ... .
at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
at com.microsoft.sqlserver.jdbc.SQLServerResultSet$FetchBuffer.nextRow(SQLServerResultSet.java:5471)
at com.microsoft.sqlserver.jdbc.SQLServerResultSet.fetchBufferNext(SQLServerResultSet.java:1794)
at com.microsoft.sqlserver.jdbc.SQLServerResultSet.next(SQLServerResultSet.java:1052)
at io.debezium.pipeline.source.spi.ChangeTableResultSet.next(ChangeTableResultSet.java:63)
at io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource.lambda$executeIteration$1(SqlServerStreamingChangeEventSource.java:269)
at io.debezium.jdbc.JdbcConnection.prepareQuery(JdbcConnection.java:606)
at io.debezium.connector.sqlserver.SqlServerConnection.getChangesForTables(SqlServerConnection.java:329)
at io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource.executeIteration(SqlServerStreamingChangeEventSource.java:251)
... 9 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 09 09:28:50 UTC 2024,,,,,,,,,,"0|z1p4bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 02:25;vmaster;> An exception occurred in the change event producer. This connector will be restarted
--------
After restart, this may lead to following error?

2024-05-07 15:11:46.919 [debezium-engine] ERROR com.ververica.cdc.debezium.internal.Handover - Reporting error:
java.lang.IllegalStateException: Retrieve schema history failed, the schema records for engine 040e0553-c4b1-41aa-a05b-ec14759e16b5 has been removed, this might because the debezium engine has been shutdown due to other errors.
	at com.ververica.cdc.debezium.utils.DatabaseHistoryUtil.retrieveHistory(DatabaseHistoryUtil.java:77) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at com.ververica.cdc.debezium.internal.FlinkDatabaseSchemaHistory.configure(FlinkDatabaseSchemaHistory.java:82) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.relational.HistorizedRelationalDatabaseConnectorConfig.getDatabaseHistory(HistorizedRelationalDatabaseConnectorConfig.java:105) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.relational.HistorizedRelationalDatabaseSchema.<init>(HistorizedRelationalDatabaseSchema.java:39) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.connector.sqlserver.SqlServerDatabaseSchema.<init>(SqlServerDatabaseSchema.java:34) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.connector.sqlserver.SqlServerConnectorTask.start(SqlServerConnectorTask.java:84) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.connector.common.BaseSourceTask.start(BaseSourceTask.java:130) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.connector.common.BaseSourceTask.startIfNeededAndPossible(BaseSourceTask.java:207) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.connector.common.BaseSourceTask.poll(BaseSourceTask.java:148) ~[flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.embedded.EmbeddedEngine.run(EmbeddedEngine.java:788) [flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at io.debezium.embedded.ConvertingEngineBuilder$2.run(ConvertingEngineBuilder.java:188) [flink-sql-connector-sqlserver-cdc.jar:2.2.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_382]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_382]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_382];;;","09/May/24 08:28;vmaster;This issue has been fixed, is this a duplicate issue ?


https://github.com/apache/flink-cdc/pull/2551;;;","09/May/24 09:28;xiqian_yu;This ticket has been resolved by https://github.com/apache/flink-cdc/pull/2551.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-454: New Apicurio Avro format,FLINK-35311,13578622,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,davidradl,davidradl,08/May/24 14:46,29/May/24 15:02,04/Jun/24 20:40,,1.17.2,1.18.1,1.19.0,,,,,,1.20.0,2.0.0,,Connectors / Kafka,,,,0,pull-request-available,,"This Jira is for the accepted [FLIP-454|https://cwiki.apache.org/confluence/display/FLINK/FLIP-454%3A+New+Apicurio+Avro+format]. It involves changes to 2 repositories, core Flink and the Flink Kafka connector  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-08 14:46:32.0,,,,,,,,,,"0|z1p40g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace RBAC verb wildcards with actual verbs,FLINK-35310,13578618,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,timsn,timsn,timsn,08/May/24 14:29,27/May/24 10:40,04/Jun/24 20:40,27/May/24 10:40,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,pull-request-available,,"We are deploying the flink operator on a managed Kubernetes cluster which utilizes [Kyverno Policy Management|https://kyverno.io/] and all it's default rules. Not complying to certain rules, leads to a restriction in deploying.

As we are using Helm to build the manifest files (which is super useful) I recognized that in the RBAC template ""wildcards"" are being used for all verbs (""*"").

This violates the following Kyverno ruleset: [https://kyverno.io/policies/other/restrict-wildcard-verbs/restrict-wildcard-verbs/]

Besides that I think that it would also be cleaner to explicitly list the needed verbs instead of just using the star symbol as a wildcard.

I have already attempted to change this in a fork as a demonstration how it could be changed to be conform. Please take a look and I would greatly appreciate a change in that direction.",Running on Kubernetes using the flink-operator version 1.8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 10:40:11 UTC 2024,,,,,,,,,,"0|z1p3zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 10:40;gyfora;merged to main f21ed8d28b1e50ffe8300fc7694641280ecb6628;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable Notice file ci check and fix Notice ,FLINK-35309,13578568,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gongzhongqiang,gongzhongqiang,gongzhongqiang,08/May/24 07:46,17/May/24 03:30,04/Jun/24 20:40,17/May/24 03:30,cdc-3.1.0,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"Changes:
* Add ci to check Notice file 
* Fix Notice file issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 09:12:35 UTC 2024,,,,,,,,,,"0|z1p3og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/24 09:12;renqs;CDC release-3.1: e452d66b3bfabbab9875d8d5be6f6262749ff30f..62cc62ef722fbe47277704e45166aefaee8d1ac4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
MySQL to StarRocks pipeline job maps TINYINT type to BOOLEAN incorrectly,FLINK-35308,13578553,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiqian_yu,xiqian_yu,08/May/24 06:45,08/May/24 06:51,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"In MySQL -> StarRocks pipeline job, the following MySQL source table schema:


CREATE TABLE fallen_angel(
     ID VARCHAR(177) NOT NULL,
     BOOLEAN_COL BOOLEAN, 
     TINYINT_COL TINYINT(1),
     PRIMARY KEY (ID)
);

will be mapped to StarRocks sink as follows:


Field           Type            Null    Key     Default Extra
ID              varchar(531)    NO      true    NULL       
BOOLEAN_COL     boolean         YES     false   NULL    
TINYINT_COL     boolean         YES     false   NULL 



where the TINYINT_COL's type is mapped to BOOLEAN, wrongly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-08 06:45:38.0,,,,,,,,,,"0|z1p3l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Compile CI check on jdk17,FLINK-35307,13578542,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zakelly,zakelly,08/May/24 04:28,08/May/24 04:29,04/Jun/24 20:40,,,,,,,,,,,,,Build System / CI,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35306,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-08 04:28:10.0,,,,,,,,,,"0|z1p3io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink cannot compile with jdk17,FLINK-35306,13578539,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,fanrui,fanrui,08/May/24 03:48,10/May/24 09:42,04/Jun/24 20:40,08/May/24 09:53,1.20.0,,,,,,,,1.20.0,,,Build System / CI,Tests,,,0,pull-request-available,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59380&view=results] fails and benchmark with 17 fails as well

 

Reason: TypeSerializerUpgradeTestBase.UpgradeVerifier update the schemaCompatibilityMatcher method name to schemaCompatibilityCondition, but some subclasses didn't change it, such as: PojoRecordSerializerUpgradeTestSpecifications.PojoToRecordVerifier.

 

It belongs to flink-tests-java17 module, and it doesn't compile by default.

 

it's caused by
 * https://issues.apache.org/jira/browse/FLINK-25537
 * [https://github.com/apache/flink/pull/24603]

 

!image-2024-05-08-11-48-04-161.png!",,,,,,,,,,,,,,,,,,,,,FLINK-25537,,FLINK-35307,,,"08/May/24 03:48;fanrui;image-2024-05-08-11-48-04-161.png;https://issues.apache.org/jira/secure/attachment/13068722/image-2024-05-08-11-48-04-161.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 09:42:56 UTC 2024,,,,,,,,,,"0|z1p3i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/24 04:25;zakelly;How about also adding compilation CI check on java17;;;","08/May/24 05:03;fanrui;Sounds make sense to me, let's follow FLINK-35307.;;;","08/May/24 09:53;fanrui;Merged to master(1.20) via 1c34ca011cacdbb3b0f48b485eac89dd913d29bf;;;","10/May/24 09:42;rskraba;Thanks for the quick fix!  Just to document this, we saw the compilation fail on GitHub Actions too:
* 1.20 Java 17 / Compile https://github.com/apache/flink/commit/29736b8c01924b7da03d4bcbfd9c812a8e5a08b4/checks/24709533133/logs;;;",,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-438: Amazon SQS Sink Connector,FLINK-35305,13578525,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dhipriya,dhipriya,08/May/24 00:11,04/Jun/24 09:49,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / AWS,,,,0,pull-request-available,,This is an umbrella task for FLIP-438. FLIP-438: https://cwiki.apache.org/confluence/display/FLINK/FLIP-438%3A+Amazon+SQS+Sink+Connector,,,,,,,,,,,,,FLINK-35401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-08 00:11:12.0,,,,,,,,,,"0|z1p3ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mongo ITCase fails due to duplicate records after resuming,FLINK-35304,13578404,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xiqian_yu,xiqian_yu,07/May/24 09:16,07/May/24 09:16,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"Test case testRemoveAndAddCollectionsOneByOne keeps failing since downstream receives duplicate data rows after MongoDB token resume.

2024-05-07T08:57:16.4720998Z [ERROR] Tests run: 20, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 498.203 s <<< FAILURE! - in org.apache.flink.cdc.connectors.mongodb.source.NewlyAddedTableITCase
2024-05-07T08:57:16.4723517Z [ERROR] org.apache.flink.cdc.connectors.mongodb.source.NewlyAddedTableITCase.testRemoveAndAddCollectionsOneByOne  Time elapsed: 38.114 s  <<< FAILURE!
2024-05-07T08:57:16.4725419Z java.lang.AssertionError: expected:<33> but was:<34>
2024-05-07T08:57:16.4726168Z     at org.junit.Assert.fail(Assert.java:89)
2024-05-07T08:57:16.4726828Z     at org.junit.Assert.failNotEquals(Assert.java:835)
2024-05-07T08:57:16.4727540Z     at org.junit.Assert.assertEquals(Assert.java:647)
2024-05-07T08:57:16.4728301Z     at org.junit.Assert.assertEquals(Assert.java:633)
2024-05-07T08:57:16.4729698Z     at org.apache.flink.cdc.connectors.mongodb.utils.MongoDBAssertUtils.assertEqualsInOrder(MongoDBAssertUtils.java:118)
2024-05-07T08:57:16.4731863Z     at org.apache.flink.cdc.connectors.mongodb.utils.MongoDBAssertUtils.assertEqualsInAnyOrder(MongoDBAssertUtils.java:111)
2024-05-07T08:57:16.4734296Z     at org.apache.flink.cdc.connectors.mongodb.source.NewlyAddedTableITCase.testRemoveAndAddCollectionsOneByOne(NewlyAddedTableITCase.java:501)
2024-05-07T08:57:16.4736882Z     at org.apache.flink.cdc.connectors.mongodb.source.NewlyAddedTableITCase.testRemoveAndAddCollectionsOneByOne(NewlyAddedTableITCase.java:330)
2024-05-07T08:57:16.4738847Z     at java.lang.reflect.Method.invoke(Method.java:498)
2024-05-07T08:57:16.4739923Z     at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2024-05-07T08:57:16.4740790Z     at java.lang.Thread.run(Thread.java:750)
2024-05-07T08:57:16.4741257Z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-07 09:16:50.0,,,,,,,,,,"0|z1p2o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Support logical deletion of data,FLINK-35303,13578397,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,07/May/24 08:40,10/May/24 07:05,04/Jun/24 20:40,,cdc-3.2.0,,,,,,,,,,,Flink CDC,,,,0,,,"delete event is logical deletion. Add a field to the table. For example: is_delete, the default is false, if it is a delete event, is_delete is set to true。",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 09:30:03 UTC 2024,,,,,,,,,,"0|z1p2mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 09:30;kunni;Currently, we can add a metadata column from row_kind [https://nightlies.apache.org/flink/flink-cdc-docs-release-3.0/docs/connectors/legacy-flink-cdc-sources/mysql-cdc/#available-metadata] to represent that. 
However, for a delete event, we will still delete this record in the downstream, we need to change the RowKind of those events to Insert to achieve the demand.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink REST server throws exception on unknown fields in RequestBody,FLINK-35302,13578363,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,07/May/24 02:36,14/May/24 01:58,04/Jun/24 20:40,13/May/24 07:23,1.19.0,,,,,,,,1.20.0,,,Runtime / REST,,,,0,pull-request-available,,"As [FLIP-401|https://cwiki.apache.org/confluence/display/FLINK/FLIP-401%3A+REST+API+JSON+response+deserialization+unknown+field+tolerance] and FLINK-33268 mentioned, when an old version REST client receives response from a new version REST server, with strict JSON mapper, the client will throw exceptions on newly added fields, which is not convenient for situations where a centralized client deals with REST servers of different versions (e.g. k8s operator).

But this incompatibility can also happens at server side, when a new version REST client sends requests to an old version REST server with additional fields. Making server flexible with unknown fields can save clients from backward compatibility code.",,,,,,,,,,,,,,,,,,,,,,,FLINK-33268,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 07:23:31 UTC 2024,,,,,,,,,,"0|z1p2ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 07:08;Juntao Hu;Hi [~gaborgsomogyi], do you have time to look at this PR?;;;","13/May/24 07:23;gaborgsomogyi;36b1d2a on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix deadlock when loading driver classes,FLINK-35301,13578361,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ShawnHx,ShawnHx,ShawnHx,07/May/24 01:44,03/Jun/24 08:31,04/Jun/24 20:40,27/May/24 10:54,cdc-3.1.0,,,,,,,,cdc-3.1.1,cdc-3.2.0,,Flink CDC,,,,0,pull-request-available,,"In JDK 8 and earlier version, if multiple threads invoke Class.forName(""com.xx.Driver"") simultaneously, it may cause jdbc driver deadlock.

FLINK-19435  analyzes this problem too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 10:54:54 UTC 2024,,,,,,,,,,"0|z1p2eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 09:13;leonard;Thanks [~ShawnHx] for report this issue, I saw you've opened a PR to fix it, thus I assigned this ticket to you;;;","27/May/24 10:54;leonard;master: 6350eec66c80a4c3815dce604305d417ee862e14
3.1: 84ae0dc45349f19175552a9dbcbfd2f296cfc56a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve MySqlStreamingChangeEventSource to skip null events in event deserializer,FLINK-35300,13578359,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,ShawnHx,ShawnHx,ShawnHx,07/May/24 01:24,27/May/24 11:28,04/Jun/24 20:40,27/May/24 11:28,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,As described in title.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-07 01:24:45.0,,,,,,,,,,"0|z1p2e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisConsumer does not respect StreamInitialPosition for new Kinesis Stream when restoring from snapshot,FLINK-35299,13578294,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hong,hong,06/May/24 14:36,07/May/24 15:55,04/Jun/24 20:40,,aws-connector-4.2.0,,,,,,,,aws-connector-4.4.0,,,Connectors / Kinesis,,,,0,pull-request-available,,"h3. What

The FlinkKinesisConsumer allows users to read from [multiple Kinesis Streams|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L224].

Users can also specify a STREAM_INITIAL_POSITION, which configures if the consumer starts reading the stream from TRIM_HORIZON / LATEST / AT_TIMESTAMP.

When restoring the Kinesis Consumer from an existing snapshot, users can configure the consumer to read from additional Kinesis Streams. The expected behavior would be for the FlinkKinesisConsumer to start reading from the additional Kinesis Streams respecting the STREAM_INITIAL_POSITION configuration. However, we find that it currently reads from TRIM_HORIZON.

This is surprising behavior and should be corrected.
h3. Why

Principle of Least Astonishment
h3. How

We recommend that we reconstruct the previously seen streams by iterating through the [sequenceNumsStateForCheckpoint in FlinkKinesisConsumer#initializeState()|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L454].
h3. Risks

This might increase the state restore time. We can consider adding a feature flag for users to turn this check off.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-06 14:36:48.0,,,,,,,,,,"0|z1p1zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve metric reporter logic,FLINK-35298,13578262,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,ShawnHx,ShawnHx,ShawnHx,06/May/24 11:48,24/May/24 09:59,04/Jun/24 20:40,24/May/24 09:59,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,,,,,0,pull-request-available,,"* In snapshot phase, set -1 as the value of currentFetchEventTimeLag metric.
 * Support currentEmitEventTimeLag metric.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 09:59:01 UTC 2024,,,,,,,,,,"0|z1p1sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 09:59;leonard;via master: 8e8fd304afdd9668247a8869698e0949806cad7b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add validation for option connect.timeout,FLINK-35297,13578256,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ShawnHx,ShawnHx,06/May/24 10:31,31/May/24 02:50,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,the value of option `connector.timeout` needs to be checked at compile time.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-06 10:31:12.0,,,,,,,,,,"0|z1p1r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink mysql-cdc connector stops reading data,FLINK-35296,13578251,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,清月,清月,06/May/24 09:47,21/May/24 03:18,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,"*Background:*
Consume sub-database and sub-table data through regular expressions, scan.startup.mode=initial

*Problems:*
1. The task occurs during the snapshot data synchronization phase;
2. After the task runs normally for a period of time, no more data will be read. In fact, there is still a lot of data in the upstream Mysql table;
3. When the task is restarted from the state, it will read normally for a period of time and then stop reading.",,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/24 09:42;清月;image-2024-05-06-17-42-19-059.png;https://issues.apache.org/jira/secure/attachment/13068662/image-2024-05-06-17-42-19-059.png","14/May/24 03:25;清月;image-2024-05-14-11-25-55-565.png;https://issues.apache.org/jira/secure/attachment/13068846/image-2024-05-14-11-25-55-565.png","21/May/24 02:51;清月;image-2024-05-21-10-51-08-452.png;https://issues.apache.org/jira/secure/attachment/13068991/image-2024-05-21-10-51-08-452.png","21/May/24 02:51;清月;image-2024-05-21-10-51-17-196.png;https://issues.apache.org/jira/secure/attachment/13068992/image-2024-05-21-10-51-17-196.png","21/May/24 02:52;清月;image-2024-05-21-10-52-02-530.png;https://issues.apache.org/jira/secure/attachment/13068993/image-2024-05-21-10-52-02-530.png","21/May/24 02:52;清月;image-2024-05-21-10-52-52-827.png;https://issues.apache.org/jira/secure/attachment/13068994/image-2024-05-21-10-52-52-827.png","21/May/24 02:53;清月;image-2024-05-21-10-53-05-634.png;https://issues.apache.org/jira/secure/attachment/13068995/image-2024-05-21-10-53-05-634.png","21/May/24 02:53;清月;image-2024-05-21-10-53-54-946.png;https://issues.apache.org/jira/secure/attachment/13068996/image-2024-05-21-10-53-54-946.png","21/May/24 03:16;清月;image-2024-05-21-11-16-01-719.png;https://issues.apache.org/jira/secure/attachment/13068997/image-2024-05-21-11-16-01-719.png","21/May/24 03:17;清月;image-2024-05-21-11-17-13-441.png;https://issues.apache.org/jira/secure/attachment/13068998/image-2024-05-21-11-17-13-441.png",,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 03:18:05 UTC 2024,,,,,,,,,,"0|z1p1q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 06:05;leonard;Thanks for you report, have you check is there a un-evenly distribution table ? [~清月];;;","14/May/24 03:21;清月;Yes, I have checked it. The maximum data volume of a sub-table is 1315883 and the minimum is 443151. The difference is no more than three times.  [~leonard] ;;;","14/May/24 03:27;清月;This abnormal situation occurs not only in sub-databases and sub-tables, but also in single tables,As shown below：

!image-2024-05-14-11-25-55-565.png!

diff=max(id)-min(id), which is about 3 times different from the count value. [~leonard] ;;;","17/May/24 08:07;chenyechao;our version is 3.0.1 

log:
 
{code:java}
2024-05-17 14:14:29.787 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - Handling split change SplitAddition:[[MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21075', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626055826], splitEnd=[626085527], highWatermark=null}]]
2024-05-17 14:14:29.787 INFO org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Finished reading from splits [ads_brand.data_convert_nodup_log:21074]
2024-05-17 14:14:29.787 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - Split read by poll split,binlogRead=false,snpshotRead=true
2024-05-17 14:14:29.787 WARN io.debezium.connector.mysql.MySqlConnection - Database configuration option 'serverTimezone' is set but is obsolete, please use 'connectionTimeZone' instead
2024-05-17 14:14:29.821 INFO com.ververica.cdc.connectors.mysql.debezium.task.context.StatefulTaskContext - Starting offset is initialized to {ts_sec=0, file=, pos=0, kind=EARLIEST, row=0, event=0}
2024-05-17 14:14:29.822 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - Snapshot currentReader submitSplit has finished,nextSplit=MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21075', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626055826], splitEnd=[626085527], highWatermark=null}
2024-05-17 14:14:29.825 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Snapshot step 1 - Determining low watermark {ts_sec=0, file=mysql-bin.146439, pos=332807150, kind=SPECIFIC, gtids=, row=0, event=0} for split MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21075', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626055826], splitEnd=[626085527], highWatermark=null}
2024-05-17 14:14:29.825 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Snapshot step 2 - Snapshotting data
2024-05-17 14:14:29.825 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Exporting data from split 'ads_brand.data_convert_nodup_log:21075' of table ads_brand.data_convert_nodup_log
2024-05-17 14:14:29.825 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - For split 'ads_brand.data_convert_nodup_log:21075' of table ads_brand.data_convert_nodup_log using select statement: 'SELECT * FROM `ads_brand`.`data_convert_nodup_log` WHERE `id` >= ? AND NOT (`id` = ?) AND `id` <= ?'
2024-05-17 14:14:29.967 INFO org.apache.flink.connector.base.source.reader.SourceReaderBase - Finished reading split(s) [ads_brand.data_convert_nodup_log:21074]
2024-05-17 14:14:29.967 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader - Split read has finished,check read next split,requestNextSplit=true
2024-05-17 14:14:29.970 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader - Source reader 1 adds split MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}
2024-05-17 14:14:29.970 INFO org.apache.flink.connector.base.source.reader.SourceReaderBase - Adding split(s) to reader: [MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}]
2024-05-17 14:14:30.214 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Finished exporting 15917 records for split 'ads_brand.data_convert_nodup_log:21075', total duration '00:00:00.389'
2024-05-17 14:14:30.217 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Snapshot step 3 - Determining high watermark {ts_sec=0, file=mysql-bin.146439, pos=333744355, kind=SPECIFIC, gtids=, row=0, event=0} for split MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21075', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626055826], splitEnd=[626085527], highWatermark=null}
2024-05-17 14:14:30.218 INFO com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader - Mysql Snapshot read has finished,currentSnapshotSplit=MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21075', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626055826], splitEnd=[626085527], highWatermark=null}
2024-05-17 14:14:30.218 INFO com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader - Back fill to starting execute binlog read task,currentSnapshotSplit = MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21075', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626055826], splitEnd=[626085527], highWatermark=null}
2024-05-17 14:14:30.218 INFO io.debezium.util.Threads - Requested thread factory for connector MySqlConnector, id = mysql_binlog_source named = binlog-client
2024-05-17 14:14:30.218 INFO com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader - Starting binlog read task,currentSnapshotSplit=MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21075', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626055826], splitEnd=[626085527], highWatermark=null}
2024-05-17 14:14:30.225 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Skip 0 events on streaming start
2024-05-17 14:14:30.225 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Skip 0 rows on streaming start
2024-05-17 14:14:30.225 INFO io.debezium.util.Threads - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-05-17 14:14:30.228 INFO io.debezium.util.Threads - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-05-17 14:14:30.243 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Connected to MySQL binlog at mysql-xima-slave-073.ximalaya.local:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=mysql-bin.146439, currentBinlogPosition=332807150, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=mysql-bin.146439, restartBinlogPosition=332807150, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]
2024-05-17 14:14:30.243 INFO io.debezium.util.Threads - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-05-17 14:14:30.243 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Waiting for keepalive thread to start
2024-05-17 14:14:30.296 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - CurrentReader has init finished,dataIt=false,currentReader.class=SnapshotSplitReader
2024-05-17 14:14:30.299 INFO io.debezium.jdbc.JdbcConnection - Connection gracefully closed
2024-05-17 14:14:30.300 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1715926470, file=mysql-bin.146439, pos=333963403, server_id=3089103, event=4}
2024-05-17 14:14:35.243 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Client disconnect is successfully,taskId=0
2024-05-17 14:14:35.244 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlBinlogSplitReadTask - Binlog split read task execute has finished,startOffset={ts_sec=0, file=mysql-bin.146439, pos=332807150, kind=SPECIFIC, gtids=, row=0, event=0},endOffset={ts_sec=0, file=mysql-bin.146439, pos=333744355, kind=SPECIFIC, gtids=, row=0, event=0}
2024-05-17 14:14:35.244 INFO com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader - Binlog read task has finished,currentSnapshotSplit=MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21075', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626055826], splitEnd=[626085527], highWatermark=null}
2024-05-17 14:14:35.244 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - Source record has fetch finished,subtaskId=1
2024-05-17 14:14:35.244 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - Handling split change SplitAddition:[[MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}]]
2024-05-17 14:14:35.244 INFO org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Finished reading from splits [ads_brand.data_convert_nodup_log:21075]
2024-05-17 14:14:35.244 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - Split read by poll split,binlogRead=false,snpshotRead=true
2024-05-17 14:14:35.244 WARN io.debezium.connector.mysql.MySqlConnection - Database configuration option 'serverTimezone' is set but is obsolete, please use 'connectionTimeZone' instead
2024-05-17 14:14:35.282 INFO com.ververica.cdc.connectors.mysql.debezium.task.context.StatefulTaskContext - Starting offset is initialized to {ts_sec=0, file=, pos=0, kind=EARLIEST, row=0, event=0}
2024-05-17 14:14:35.283 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - Snapshot currentReader submitSplit has finished,nextSplit=MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}
2024-05-17 14:14:35.286 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Snapshot step 1 - Determining low watermark {ts_sec=0, file=mysql-bin.146439, pos=346179811, kind=SPECIFIC, gtids=, row=0, event=0} for split MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}
2024-05-17 14:14:35.286 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Snapshot step 2 - Snapshotting data
2024-05-17 14:14:35.286 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Exporting data from split 'ads_brand.data_convert_nodup_log:21076' of table ads_brand.data_convert_nodup_log
2024-05-17 14:14:35.286 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - For split 'ads_brand.data_convert_nodup_log:21076' of table ads_brand.data_convert_nodup_log using select statement: 'SELECT * FROM `ads_brand`.`data_convert_nodup_log` WHERE `id` >= ?'
2024-05-17 14:14:35.422 INFO org.apache.flink.connector.base.source.reader.SourceReaderBase - Finished reading split(s) [ads_brand.data_convert_nodup_log:21075]
2024-05-17 14:14:35.422 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader - Split read has finished,check read next split,requestNextSplit=true
2024-05-17 14:14:39.726 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Finished exporting 175433 records for split 'ads_brand.data_convert_nodup_log:21076', total duration '00:00:04.44'
2024-05-17 14:14:39.729 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask - Snapshot step 3 - Determining high watermark {ts_sec=0, file=mysql-bin.146439, pos=355392617, kind=SPECIFIC, gtids=, row=0, event=0} for split MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}
2024-05-17 14:14:39.730 INFO com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader - Mysql Snapshot read has finished,currentSnapshotSplit=MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}
2024-05-17 14:14:39.730 INFO com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader - Back fill to starting execute binlog read task,currentSnapshotSplit = MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}
2024-05-17 14:14:39.730 INFO io.debezium.util.Threads - Requested thread factory for connector MySqlConnector, id = mysql_binlog_source named = binlog-client
2024-05-17 14:14:39.730 INFO com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader - Starting binlog read task,currentSnapshotSplit=MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}
2024-05-17 14:14:39.737 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Skip 0 events on streaming start
2024-05-17 14:14:39.738 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Skip 0 rows on streaming start
2024-05-17 14:14:39.738 INFO io.debezium.util.Threads - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-05-17 14:14:39.740 INFO io.debezium.util.Threads - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-05-17 14:14:39.756 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Connected to MySQL binlog at mysql-xima-slave-073.ximalaya.local:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=mysql-bin.146439, currentBinlogPosition=346179811, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=mysql-bin.146439, restartBinlogPosition=346179811, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]
2024-05-17 14:14:39.756 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Waiting for keepalive thread to start
2024-05-17 14:14:39.756 INFO io.debezium.util.Threads - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-05-17 14:14:39.872 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Keepalive thread is running
2024-05-17 14:14:39.956 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1715926478, file=mysql-bin.146439, pos=352990413, server_id=3089103, event=18}
2024-05-17 14:14:39.956 INFO io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Client disconnect is successfully,taskId=0
2024-05-17 14:14:39.956 INFO com.ververica.cdc.connectors.mysql.debezium.task.MySqlBinlogSplitReadTask - Binlog split read task execute has finished,startOffset={ts_sec=0, file=mysql-bin.146439, pos=346179811, kind=SPECIFIC, gtids=, row=0, event=0},endOffset={ts_sec=0, file=mysql-bin.146439, pos=355392617, kind=SPECIFIC, gtids=, row=0, event=0}
2024-05-17 14:14:39.956 INFO com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader - Binlog read task has finished,currentSnapshotSplit=MySqlSnapshotSplit{tableId=ads_brand.data_convert_nodup_log, splitId='ads_brand.data_convert_nodup_log:21076', splitKeyType=[`id` BIGINT NOT NULL], splitStart=[626085527], splitEnd=null, highWatermark=null}
2024-05-17 14:14:40.154 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - CurrentReader has init finished,dataIt=false,currentReader.class=SnapshotSplitReader
2024-05-17 14:14:40.157 INFO io.debezium.jdbc.JdbcConnection - Connection gracefully closed
2024-05-17 14:14:40.157 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader - Source record has fetch finished,subtaskId=1
2024-05-17 14:14:40.157 INFO org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Finished reading from splits [ads_brand.data_convert_nodup_log:21076]
2024-05-17 14:14:41.920 INFO org.apache.flink.connector.base.source.reader.SourceReaderBase - Finished reading split(s) [ads_brand.data_convert_nodup_log:21076]
2024-05-17 14:14:41.920 INFO com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader - Split read has finished,check read next split,requestNextSplit=true
2024-05-17 14:14:41.920 INFO org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager - Closing splitFetcher 0 because it is idle.
2024-05-17 14:14:41.920 INFO org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Shutting down split fetcher 0
2024-05-17 14:14:41.920 INFO org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher - Split fetcher 0 exited.

  {code}
Thread dump:
{code:java}
""AsyncOperations-thread-115"" Id=42816 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3621aba6
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3621aba6
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""AsyncOperations-thread-114"" Id=42815 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@437cadf8
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@437cadf8
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-taskexecutor-io-thread-8"" Id=42812 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-taskexecutor-io-thread-7"" Id=42810 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-taskexecutor-io-thread-6"" Id=42809 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-taskexecutor-io-thread-5"" Id=42808 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-metrics-22"" Id=42805 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@26025fa1
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@26025fa1
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-taskexecutor-io-thread-4"" Id=42760 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-taskexecutor-io-thread-3"" Id=42759 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-taskexecutor-io-thread-2"" Id=42758 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-taskexecutor-io-thread-1"" Id=42757 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3678563e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""blc-mysql-xima-slave-073.ximalaya.local:3306"" Id=37065 TIMED_WAITING on io.debezium.connector.base.ChangeEventQueue@2b0adb40
    at java.lang.Object.wait(Native Method)
    -  waiting on io.debezium.connector.base.ChangeEventQueue@2b0adb40
    at io.debezium.connector.base.ChangeEventQueue.doEnqueue(ChangeEventQueue.java:204)
    at io.debezium.connector.base.ChangeEventQueue.enqueue(ChangeEventQueue.java:169)
    at com.ververica.cdc.connectors.mysql.debezium.dispatcher.SignalEventDispatcher.dispatchWatermarkEvent(SignalEventDispatcher.java:91)
    at com.ververica.cdc.connectors.mysql.debezium.task.MySqlBinlogSplitReadTask.handleEvent(MySqlBinlogSplitReadTask.java:105)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.lambda$execute$25(MySqlStreamingChangeEventSource.java:1095)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$$Lambda$1436/136127176.onEvent(Unknown Source)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1246)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1072)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631)
    at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.locks.ReentrantLock$NonfairSync@10cd2e64""debezium-reader-0"" Id=37064 TIMED_WAITING on java.util.concurrent.locks.ReentrantLock$NonfairSync@10cd2e64 owned by ""blc-mysql-xima-slave-073.ximalaya.local:3306"" Id=37065
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.ReentrantLock$NonfairSync@10cd2e64
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireNanos(AbstractQueuedSynchronizer.java:934)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireNanos(AbstractQueuedSynchronizer.java:1247)
    at java.util.concurrent.locks.ReentrantLock.tryLock(ReentrantLock.java:442)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.tryLockInterruptibly(BinaryLogClient.java:1335)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.terminateConnect(BinaryLogClient.java:1329)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.disconnect(BinaryLogClient.java:1298)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.execute(MySqlStreamingChangeEventSource.java:1247)
    at com.ververica.cdc.connectors.mysql.debezium.task.MySqlBinlogSplitReadTask.execute(MySqlBinlogSplitReadTask.java:84)
    at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.backfill(SnapshotSplitReader.java:197)
    at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.lambda$submitSplit$1(SnapshotSplitReader.java:153)
    at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader$$Lambda$1405/15047152.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@4fe23624""flink-metrics-pekko.remote.default-remote-dispatcher-16"" Id=35313 WAITING on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@1f4176d7
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@1f4176d7
    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)""flink-pekko.remote.default-remote-dispatcher-19"" Id=35312 WAITING on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@6b7c0aa7
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@6b7c0aa7
    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)""I/O client dispatch - 46d5c4b9-a728-491a-9996-0a583ca6026e"" Id=472 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@123bb7a2
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@123bb7a2
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""I/O client dispatch - 4b2937c5-9326-4fdd-bb93-6679fbc30c33"" Id=447 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6f96ae7e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6f96ae7e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""I/O client dispatch - fbb42f49-a225-419f-845b-1a41d73a763f"" Id=203 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6f96ae7e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6f96ae7e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""I/O client dispatch - 84ea7fea-2803-460c-84c3-27496c47c789"" Id=202 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@123bb7a2
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@123bb7a2
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""mysql-cj-abandoned-connection-cleanup"" Id=101 TIMED_WAITING on java.lang.ref.ReferenceQueue$Lock@7bf58a95
    at java.lang.Object.wait(Native Method)
    -  waiting on java.lang.ref.ReferenceQueue$Lock@7bf58a95
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
    at com.mysql.cj.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:91)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@43f4dc77""System Time Trigger for Source: source_table[4] -> Calc[5] -> ConstraintEnforcer[6] -> Sink: sink_table[6] (1/2)#0"" Id=100 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@70579e7e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@70579e7e
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""System Time Trigger for Source: source_table[4] -> Calc[5] -> ConstraintEnforcer[6] -> Sink: sink_table[6] (2/2)#0"" Id=99 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3f9ea171
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3f9ea171
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""Time Trigger for Source: source_table[4] -> Calc[5] -> ConstraintEnforcer[6] -> Sink: sink_table[6] (1/2)#0"" Id=97 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@65210da2
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@65210da2
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""Time Trigger for Source: source_table[4] -> Calc[5] -> ConstraintEnforcer[6] -> Sink: sink_table[6] (2/2)#0"" Id=95 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@325792b1
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@325792b1
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""Source Data Fetcher for Source: source_table[4] -> Calc[5] -> ConstraintEnforcer[6] -> Sink: sink_table[6] (1/2)#0"" Id=94 TIMED_WAITING on java.util.concurrent.locks.ReentrantLock$NonfairSync@10cd2e64 owned by ""blc-mysql-xima-slave-073.ximalaya.local:3306"" Id=37065
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.ReentrantLock$NonfairSync@10cd2e64
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireNanos(AbstractQueuedSynchronizer.java:934)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireNanos(AbstractQueuedSynchronizer.java:1247)
    at java.util.concurrent.locks.ReentrantLock.tryLock(ReentrantLock.java:442)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.tryLockInterruptibly(BinaryLogClient.java:1335)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.terminateConnect(BinaryLogClient.java:1329)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.disconnect(BinaryLogClient.java:1298)
    at com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader.close(SnapshotSplitReader.java:392)
    at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.closeSnapshotReader(MySqlSplitReader.java:282)
    at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.forRecords(MySqlSplitReader.java:205)
    at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.pollSplitRecords(MySqlSplitReader.java:138)
    at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:87)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:117)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@b382b59""StarRocks-Sink-Manager"" Id=92 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7ebf5eeb
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7ebf5eeb
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
    at com.starrocks.data.load.stream.v2.StreamLoadManagerV2.lambda$init$0(StreamLoadManagerV2.java:161)
    at com.starrocks.data.load.stream.v2.StreamLoadManagerV2$$Lambda$1159/452173932.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:750)""StarRocks-Sink-Manager"" Id=91 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3460ed64
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3460ed64
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
    at com.starrocks.data.load.stream.v2.StreamLoadManagerV2.lambda$init$0(StreamLoadManagerV2.java:161)
    at com.starrocks.data.load.stream.v2.StreamLoadManagerV2$$Lambda$1159/452173932.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:750)""Thread-25"" Id=89 RUNNABLE (in native)""Thread-24"" Id=88 RUNNABLE (in native)""org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner"" Id=87 WAITING on java.lang.ref.ReferenceQueue$Lock@59c1ba57
    at java.lang.Object.wait(Native Method)
    -  waiting on java.lang.ref.ReferenceQueue$Lock@59c1ba57
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
    at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:4021)
    at java.lang.Thread.run(Thread.java:750)""Source: source_table[4] -> Calc[5] -> ConstraintEnforcer[6] -> Sink: sink_table[6] (2/2)#0"" Id=86 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@578d79c4
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@578d79c4
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
    at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:363)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
    at org.apache.flink.runtime.taskmanager.Task$$Lambda$1236/1566291491.run(Unknown Source)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:750)""CloseableReaperThread"" Id=85 WAITING on java.lang.ref.ReferenceQueue$Lock@53413dd8
    at java.lang.Object.wait(Native Method)
    -  waiting on java.lang.ref.ReferenceQueue$Lock@53413dd8
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
    at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:215)""Source: source_table[4] -> Calc[5] -> ConstraintEnforcer[6] -> Sink: sink_table[6] (1/2)#0"" Id=84 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6079e119
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6079e119
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
    at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:363)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
    at org.apache.flink.runtime.taskmanager.Task$$Lambda$1236/1566291491.run(Unknown Source)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:750)""flink-pekko.actor.default-dispatcher-18"" Id=83 WAITING on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@34beec43
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@34beec43
    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)""pool-6-thread-1"" Id=81 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@8c1f049
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@8c1f049
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""taskmanager_0-main-scheduler-thread-1"" Id=80 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@52b4a374
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@52b4a374
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""flink-pekko.actor.default-dispatcher-17"" Id=79 WAITING on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@34beec43
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@34beec43
    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)""flink-pekko.actor.default-dispatcher-16"" Id=78 WAITING on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@34beec43
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@34beec43
    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)""Hashed wheel timer #1"" Id=25 TIMED_WAITING
    at java.lang.Thread.sleep(Native Method)
    at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
    at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at java.lang.Thread.run(Thread.java:750)""Flink Netty Server (0) Thread 0"" Id=72 RUNNABLE (in native)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native Method)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native.java:209)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native.java:202)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:306)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:363)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at java.lang.Thread.run(Thread.java:750)""Flink-Metric-View-Updater-thread-1"" Id=71 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7deae112
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7deae112
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""IOManager reader thread #6"" Id=68 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3ef5d618
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3ef5d618
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:372)""IOManager reader thread #5"" Id=67 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@39b12a37
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@39b12a37
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:372)""IOManager reader thread #4"" Id=66 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@4e698176
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@4e698176
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:372)""IOManager reader thread #3"" Id=65 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64211bd8
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64211bd8
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:372)""IOManager reader thread #2"" Id=64 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@74f45452
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@74f45452
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:372)""IOManager reader thread #1"" Id=63 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@44a42869
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@44a42869
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:372)""IOManager writer thread #6"" Id=62 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@46c463ba
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@46c463ba
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:482)""IOManager writer thread #5"" Id=61 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2ad75f61
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2ad75f61
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:482)""IOManager writer thread #4"" Id=60 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7c57c8ad
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7c57c8ad
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:482)""IOManager writer thread #3"" Id=59 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1955502e
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1955502e
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:482)""IOManager writer thread #2"" Id=58 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7c69390d
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7c69390d
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:482)""IOManager writer thread #1"" Id=57 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6cd55db6
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6cd55db6
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:482)""Timer-1"" Id=55 TIMED_WAITING on java.util.TaskQueue@2e85e221
    at java.lang.Object.wait(Native Method)
    -  waiting on java.util.TaskQueue@2e85e221
    at java.util.TimerThread.mainLoop(Timer.java:552)
    at java.util.TimerThread.run(Timer.java:505)""Timer-0"" Id=53 TIMED_WAITING on java.util.TaskQueue@1de213ff
    at java.lang.Object.wait(Native Method)
    -  waiting on java.util.TaskQueue@1de213ff
    at java.util.TimerThread.mainLoop(Timer.java:552)
    at java.util.TimerThread.run(Timer.java:505)""New I/O server boss #12"" Id=49 RUNNABLE (in native)
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@28f6f01f
    -  locked java.util.Collections$UnmodifiableSet@6e08205a
    -  locked sun.nio.ch.EPollSelectorImpl@6f9d4350
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@19d14b65""New I/O worker #11"" Id=48 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@626dc8b7
    -  locked java.util.Collections$UnmodifiableSet@7d84baed
    -  locked sun.nio.ch.EPollSelectorImpl@52215ff7
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@529e2c3""New I/O worker #10"" Id=47 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@57a31120
    -  locked java.util.Collections$UnmodifiableSet@5b874313
    -  locked sun.nio.ch.EPollSelectorImpl@30cc95c
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@28b3a765""New I/O boss #9"" Id=46 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@5b8ae02d
    -  locked java.util.Collections$UnmodifiableSet@14efc06d
    -  locked sun.nio.ch.EPollSelectorImpl@75119638
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@6db124b7""New I/O worker #8"" Id=44 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@10b2e7ef
    -  locked java.util.Collections$UnmodifiableSet@1cb5e65c
    -  locked sun.nio.ch.EPollSelectorImpl@76b3b10f
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@698ead5e""New I/O worker #7"" Id=43 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@f6a49b7
    -  locked java.util.Collections$UnmodifiableSet@8605a2a
    -  locked sun.nio.ch.EPollSelectorImpl@3bd4ca7c
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@67494fc7""flink-metrics-pekko.remote.default-remote-dispatcher-6"" Id=41 TIMED_WAITING on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@1f4176d7
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@1f4176d7
    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)""flink-metrics-scheduler-1"" Id=36 TIMED_WAITING
    at java.lang.Thread.sleep(Native Method)
    at org.apache.pekko.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:99)
    at org.apache.pekko.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:310)
    at org.apache.pekko.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:280)
    at java.lang.Thread.run(Thread.java:750)""Flink-Metric-Reporter-thread-1"" Id=35 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@50042fc0
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@50042fc0
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""MetricManager$$anon$1"" Id=34 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@d7ef386
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@d7ef386
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""kafka-producer-network-thread | producer-1"" Id=33 RUNNABLE (in native)
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@7454909
    -  locked java.util.Collections$UnmodifiableSet@1adff4b7
    -  locked sun.nio.ch.EPollSelectorImpl@6bd4b203
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.apache.kafka.common.network.Selector.select(Selector.java:873)
    at org.apache.kafka.common.network.Selector.poll(Selector.java:465)
    at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
    at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:328)
    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:243)
    at java.lang.Thread.run(Thread.java:750)""Thread-4"" Id=32 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@11cc8e3c
    -  locked java.util.Collections$UnmodifiableSet@3c6c800d
    -  locked sun.nio.ch.EPollSelectorImpl@5973a17b
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:453)
    at java.lang.Thread.run(Thread.java:750)""idle-timeout-task"" Id=31 TIMED_WAITING on java.util.TaskQueue@177954fc
    at java.lang.Object.wait(Native Method)
    -  waiting on java.util.TaskQueue@177954fc
    at java.util.TimerThread.mainLoop(Timer.java:552)
    at java.util.TimerThread.run(Timer.java:505)""New I/O server boss #6"" Id=29 RUNNABLE (in native)
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@62b19541
    -  locked java.util.Collections$UnmodifiableSet@3a050df2
    -  locked sun.nio.ch.EPollSelectorImpl@7e390bb0
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@19d921eb""New I/O worker #5"" Id=28 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@23ba6e3f
    -  locked java.util.Collections$UnmodifiableSet@4dcfec94
    -  locked sun.nio.ch.EPollSelectorImpl@5fb71242
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@776f755d""New I/O worker #4"" Id=27 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@6965f68f
    -  locked java.util.Collections$UnmodifiableSet@195eb60a
    -  locked sun.nio.ch.EPollSelectorImpl@2bc752f9
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@4a61a06""New I/O boss #3"" Id=26 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@4c0570c1
    -  locked java.util.Collections$UnmodifiableSet@a123640
    -  locked sun.nio.ch.EPollSelectorImpl@66103fee
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@4cdb5b07""New I/O worker #2"" Id=24 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@7751e94f
    -  locked java.util.Collections$UnmodifiableSet@10350c1c
    -  locked sun.nio.ch.EPollSelectorImpl@b0e986c
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@7918f451""New I/O worker #1"" Id=23 RUNNABLE
    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
    -  locked sun.nio.ch.Util$3@6c8629ac
    -  locked java.util.Collections$UnmodifiableSet@14e5bee2
    -  locked sun.nio.ch.EPollSelectorImpl@2c08158e
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
    at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)    Number of locked synchronizers = 1
    - java.util.concurrent.ThreadPoolExecutor$Worker@293a14d3""flink-pekko.remote.default-remote-dispatcher-6"" Id=21 TIMED_WAITING on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@6b7c0aa7
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinPool@6b7c0aa7
    at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)""flink-pekko.actor.default-dispatcher-5"" Id=20 RUNNABLE
    at sun.management.ThreadImpl.dumpThreads0(Native Method)
    at sun.management.ThreadImpl.dumpAllThreads(ThreadImpl.java:496)
    at sun.management.ThreadImpl.dumpAllThreads(ThreadImpl.java:484)
    at org.apache.flink.runtime.util.JvmUtils.createThreadDump(JvmUtils.java:50)
    at org.apache.flink.runtime.rest.messages.ThreadDumpInfo.dumpAndCreate(ThreadDumpInfo.java:59)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.requestThreadDump(TaskExecutor.java:1350)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor$$Lambda$797/432379350.get(Unknown Source)
    at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor$$Lambda$523/886869080.apply(Unknown Source)
    at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
    at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
    at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
    at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
    at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
    at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
    at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
    at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
    at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
    at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
    at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)""flink-scheduler-1"" Id=16 TIMED_WAITING
    at java.lang.Thread.sleep(Native Method)
    at org.apache.pekko.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:99)
    at org.apache.pekko.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:310)
    at org.apache.pekko.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:280)
    at java.lang.Thread.run(Thread.java:750)""logback-1"" Id=9 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@533a259b
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@533a259b
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)""Signal Dispatcher"" Id=5 RUNNABLE""Finalizer"" Id=3 WAITING on java.lang.ref.ReferenceQueue$Lock@e2cbd19
    at java.lang.Object.wait(Native Method)
    -  waiting on java.lang.ref.ReferenceQueue$Lock@e2cbd19
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
    at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:188)""Reference Handler"" Id=2 WAITING on java.lang.ref.Reference$Lock@37870090
    at java.lang.Object.wait(Native Method)
    -  waiting on java.lang.ref.Reference$Lock@37870090
    at java.lang.Object.wait(Object.java:502)
    at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
    at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)""main"" Id=1 WAITING on java.util.concurrent.CompletableFuture$Signaller@5b4fd78b
    at sun.misc.Unsafe.park(Native Method)
    -  waiting on java.util.concurrent.CompletableFuture$Signaller@5b4fd78b
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
    at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
    at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
    at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:497)
    at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.lambda$runTaskManagerProcessSecurely$5(TaskManagerRunner.java:535)
    at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$$Lambda$55/1423983012.call(Unknown Source)
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext$$Lambda$56/127791068.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
    at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:535)
    at org.apache.flink.yarn.YarnTaskExecutorRunner.runTaskManagerSecurely(YarnTaskExecutorRunner.java:94)
    at org.apache.flink.yarn.YarnTaskExecutorRunner.main(YarnTaskExecutorRunner.java:68) {code};;;","17/May/24 09:18;ruanhang1993;Hi, [~清月] .

What is the configuration of this mysql table and could you please provide the full logs of TM and JM?

I could not find the problem from the information by now. Have you ever tried to reproduce this problem with a test case?

Thanks ~;;;","21/May/24 02:17;清月;Hi，[~ruanhang1993] 

At present, I found that I am stuck in：com.ververica.cdc.connectors.mysql.debezium.reader.SnapshotSplitReader#close

mainly here：
if (statefulTaskContext.getBinaryLogClient() != null)

{ statefulTaskContext.getBinaryLogClient().disconnect(); };;;","21/May/24 02:54;清月;Hi，[~ruanhang1993] ，The following is a screenshot of my customized log
com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader#pollSplitRecords

!image-2024-05-21-10-51-17-196.png!

!image-2024-05-21-10-53-54-946.png!

com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader#fetch
!image-2024-05-21-10-52-02-530.png!

!image-2024-05-21-10-53-05-634.png!;;;","21/May/24 03:18;清月;Hi，[~ruanhang1993] ，Below is a screenshot of the thread dump
!image-2024-05-21-11-16-01-719.png!

 

The parallelism of my job is 2：

!image-2024-05-21-11-17-13-441.png!;;;",,,,,,,,,,,,,,,,,,,,,
Improve jdbc connection pool initialization failure message,FLINK-35295,13578250,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ShawnHx,ShawnHx,ShawnHx,06/May/24 09:46,31/May/24 03:13,04/Jun/24 20:40,27/May/24 04:18,cdc-3.1.0,,,,,,,,cdc-3.1.1,cdc-3.2.0,,Flink CDC,,,,0,pull-request-available,,As described in ticket title.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 03:13:07 UTC 2024,,,,,,,,,,"0|z1p1ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 04:18;leonard;master: 18b0627e343ddd218adcbccef8fb5f5f65813479;;;","31/May/24 03:13;jiabaosun;release-3.1: e18e7a2523ac1ea59471e5714eb60f544e9f4a04;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Use source config to check if the filter should be applied in timestamp starting mode,FLINK-35294,13578248,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ShawnHx,ShawnHx,ShawnHx,06/May/24 09:28,03/Jun/24 09:59,04/Jun/24 20:40,03/Jun/24 09:59,cdc-3.1.0,,,,,,,,cdc-3.1.1,cdc-3.2.0,,Flink CDC,,,,0,pull-request-available,,"Since MySQL does not support the ability to quickly locate an binlog offset through a timestamp, the current logic for starting from a timestamp is to begin from the earliest binlog offset and then filter out the data before the user-specified position.

If the user restarts the job during the filtering process, this filter will become ineffective.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 04:15:49 UTC 2024,,,,,,,,,,"0|z1p1pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 04:15;leonard;Implemented via 
master: bddcaae7846671c0447b2aa6c7d8e96638988f99
3.1: 5650ee29c6afaaa9139f5cddab4d5391e100b7be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-445: Support dynamic parallelism inference for HiveSource,FLINK-35293,13578239,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiasun,xiasun,xiasun,06/May/24 08:14,22/May/24 03:24,04/Jun/24 20:40,14/May/24 11:22,1.20.0,,,,,,,,1.20.0,,,Connectors / Hive,,,,0,pull-request-available,,"[FLIP-379|https://cwiki.apache.org/confluence/display/FLINK/FLIP-379%3A+Dynamic+source+parallelism+inference+for+batch+jobs] introduces dynamic source parallelism inference, which, compared to static inference, utilizes runtime information to more accurately determine the source parallelism. The FileSource already possesses the capability for dynamic parallelism inference. As a follow-up task to FLIP-379, this FLIP plans to implement the dynamic parallelism inference interface for HiveSource, and also switches the default static parallelism inference to dynamic parallelism inference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 03:24:03 UTC 2024,,,,,,,,,,"0|z1p1nc:",9223372036854775807,"In Flink 1.20, we have introduced support for dynamic source parallelism inference in batch jobs for the Hive source connector. This allows the connector to dynamically determine parallelism based on the actual partitions with dynamic partition pruning.
Additionally, we have introduced a new configuration option, 'table.exec.hive.infer-source-parallelism.mode,' to enable users to choose between static and dynamic inference modes for source parallelism. By default, the mode is set to 'dynamic'. Users may configure it to 'static' for static inference, 'dynamic' for dynamic inference, or 'none' to disable automatic parallelism inference altogether. It should be noted that in Flink 1.20, the previous configration option 'table.exec.hive.infer-source-parallelism' has been marked as deprecated, but it will continue to serve as a switch for automatic parallelism inference until it is fully phased out.",,,,,,,,,,,,,,,,,,,"14/May/24 02:30;zhuzh;master: ddb5a5355f9aca3d223f1fff6581d83dd317c2de;;;","14/May/24 05:42;zhuzh;The change is merged. Could you add release notes for it and close the ticket? [~xiasun].;;;","14/May/24 11:25;xiasun;Thank you [~zhuzh]  for helping to review the PR! I will add a release note and close the ticket.;;;","15/May/24 01:58;fanrui;Hi [~xiasun] , I saw you added the release note for FLIP-445, would you mind recording FLIP-445 into the 1.20 release doc[1]? It's useful for release managers to follow it, thanks in advance.

[1]https://cwiki.apache.org/confluence/display/FLINK/1.20+Release;;;","22/May/24 03:16;xiasun;[~fanrui] Sorry for my late reply. I have documented FLIP-445 in the 1.20 release document. Thank you for your work!;;;","22/May/24 03:24;fanrui;[~xiasun] Thanks for the update! :);;;",,,,,,,,,,,,,,,,,,,,,,,
Set dummy savepoint path during last-state upgrade,FLINK-35292,13578238,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gyfora,gyfora,gyfora,06/May/24 08:12,06/May/24 08:28,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,"Currently the operator always sets the savepoint path even if last-state (HA metadata) must be used. 

This can be misleading to users as the set savepoint path normally should never take effect and can actually lead to incorrect state restored if the HA metadata is deleted by the user at the wrong moment. 

To avoid this we can set an explicit dummy savepoint path which will prevent restoring from it accidentally. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-06 08:12:50.0,,,,,,,,,,"0|z1p1n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the ROW data deserialization performance of DebeziumEventDeserializationScheme,FLINK-35291,13578191,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,LiuZeshan,LiuZeshan,05/May/24 16:42,09/May/24 02:01,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Flink CDC,,,,0,pull-request-available,,"We are doing performance testing on Flink cdc 3.0 and found through the arthas profile that there is a significant performance bottleneck in the deserialization of row data. The main problem lies in the String. format in the BinaryRecordDataGenerator class, so we have made simple performance optimizations.

test environment:
 * flink: 1.20-SNAPSHOT master
 * flink-cdc: 3.2-SNAPSHOT master
 * 1CU minicluster mode

{code:java}
source:
  type: mysql
  hostname: localhost
  port: 3308
  username: root
  password: 123456
  tables: test.user_behavior
  server-id: 5400-5404
  #server-time-zone: UTC
  scan.startup.mode: earliest-offset
  debezium.poll.interval.ms: 10

sink:
  type: values
  name: Values Sink
  materialized.in.memory: false
  print.enabled: false

pipeline:
  name: Sync MySQL Database to Values
  parallelism: 1{code}
 

*before optimization: 3.5w/s* 
!https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MTRjZGIyNWYyYmVlY2YwNDNmYjExZDE4MjRhMGYyYzlfcVRuM0JBYXpTem9qUWRxdkY0NGZmVkpWc1cxMnlzaE9fVG9rZW46RklTbWJUNkVYb2s0WGF4eEttWWN6M0hIbjJTXzE3MTQ5MjU4OTY6MTcxNDkyOTQ5Nl9WNA|width=361,height=179!

[^cdc-3.0-1c.html]

^Analyzing the flame chart, it can be found that approximately 24.45% of the time is spent on string.format.^

!image-2024-05-06-00-29-34-618.png|width=583,height=171!

 

*after optimization: 5w/s* 

!https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=YjRkMDRmYTkzNzRiNjBmMzVmN2VlYTYyMGRmMGU0ZDRfcFIyNGNGMEViSzRjektpdVFWYTYyUnJQbWJjd1lnb3dfVG9rZW46V2ZXVGJ2T3lDb3dCSmF4WVZvTGMzc2h2bmpmXzE3MTQ5MjU5NTM6MTcxNDkyOTU1M19WNA|width=363,height=174!
 
 [^cdc-3.0-1c-2.html]

After optimization, 4.7%(extractBeforeDataRecord+extractAfterDataRecord) of the time is still spent on org/apache/flink/cdc/runtime/typeutils/BinaryRecordDataGenerator.<init>. Perhaps we can further optimize it.

!image-2024-05-06-00-37-16-028.png|width=379,height=107!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/24 16:36;LiuZeshan;cdc-3.0-1c-2.html;https://issues.apache.org/jira/secure/attachment/13068654/cdc-3.0-1c-2.html","05/May/24 16:21;LiuZeshan;cdc-3.0-1c.html;https://issues.apache.org/jira/secure/attachment/13068656/cdc-3.0-1c.html","05/May/24 16:29;LiuZeshan;image-2024-05-06-00-29-34-618.png;https://issues.apache.org/jira/secure/attachment/13068655/image-2024-05-06-00-29-34-618.png","05/May/24 16:37;LiuZeshan;image-2024-05-06-00-37-16-028.png;https://issues.apache.org/jira/secure/attachment/13068653/image-2024-05-06-00-37-16-028.png",,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-05 16:42:20.0,,,,,,,,,,"0|z1p1co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong Instant type conversion TableAPI to Datastream in thread mode,FLINK-35290,13578186,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,wzorgdrager,wzorgdrager,05/May/24 14:23,05/May/24 14:43,04/Jun/24 20:40,05/May/24 14:43,1.18.1,,,,,,,,,,,API / Python,,,,0,,,"In PyFlink, if you convert a table with a `TIMESTAMP_LTZ(3)` type into a Datastream, we get an `pyflink.common.time.Instant` type. First of all, I'm wondering if this is expected behavior as in the TableAPI, `TIMESTAMP_LTZ` maps to a Python `datetime`. Can't the same be done for the DatastreamAPI? Nevertheless, if we switch from `process` to `thread` mode for execution, the `TIMESTAMP_LTZ(3)` gets mapped to `pemja.PyJObject' (which wraps a `java.time.Instant`) rather than `pyflink.common.time.Instant`. Note that if I only use the DatastreamAPI  and read `Types.Instant()` directly, the conversion in both `thread` and `process` mode seem to work just fine.

Below a minimal example exposing the bug:

```
EXECUTION_MODE = ""thread""  # or ""process""
config = Configuration()
config.set_string(""python.execution-mode"", EXECUTION_MODE)

env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)
t_env.get_config().set(""parallelism.default"", ""1"")
t_env.get_config().set(""python.fn-execution.bundle.size"", ""1"")
t_env.get_config().set(""python.execution-mode"", EXECUTION_MODE)


def to_epoch_ms(row: Row):
    print(type(row[1]))
    return row[1].to_epoch_milli()


t_env.to_data_stream(
    t_env.from_elements(
        [
            (1, datetime(year=2024, day=10, month=9, hour=9)),
            (2, datetime(year=2024, day=10, month=9, hour=12)),
            (3, datetime(year=2024, day=22, month=11, hour=12)),
        ],
        DataTypes.ROW(
            [
                DataTypes.FIELD(""id"", DataTypes.INT()),
                DataTypes.FIELD(""timestamp"", DataTypes.TIMESTAMP_LTZ(3)),
            ]
        ),
    )
).map(to_epoch_ms, output_type=Types.LONG()).print()
env.execute()
```",,,,,,,,,,,,,,,,,,,,FLINK-35180,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 05 14:42:38 UTC 2024,,,,,,,,,,"0|z1p1bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/24 14:42;wzorgdrager;It seems this bug has already been reported here https://issues.apache.org/jira/browse/FLINK-35180. Will close this issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect timestamp of stream elements collected from onTimer in batch mode,FLINK-35289,13578149,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ekanthi,ekanthi,04/May/24 12:13,22/May/24 08:00,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,API / Core,,,,0,pull-request-available,,"In batch mode  all registered timers will fire at the _end of time. Given this, if a user registers a timer for Long.MAX_VALUE, the timestamp assigned to the elements that are collected from the onTimer context ends up being Long.MAX_VALUE. Ideally this should be the time when the batch actually executed  the onTimer function._",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 02:20:57 UTC 2024,,,,,,,,,,"0|z1p13c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 21:44;ekanthi;Any updates on this issue ?;;;","15/May/24 13:31;jeyhunkarimov;Hi [~ekanthi] could you please provide a concrete example? According to your explanation, if

 
{code:java}
ctx.timerService().registerProcessingTimeTimer(Long.MAX_VALUE); {code}
and
{code:java}
@Override
public void onTimer(long timestamp, OnTimerContext ctx, Collector<Integer> out) throws Exception {
    out.collect(111);
} {code}
then, collected value (111) will have Long.MAX_VALUE timestamp. 

However, when we check the implementation of TimestampedCollector#collect, we can see that this is not the case. The stream record will preserve its existing timestamp. 

 

Am I missing something?

 

Thanks!;;;","15/May/24 16:13;ekanthi;When running in Batch Mode If we do:
ctx.timerService().registerEventTimeTimer(Long.MAX_VALUE);

and
@Override
public void onTimer(long timestamp, OnTimerContext ctx, Collector<Integer> out) throws Exception

{ out.collect(111); }

This is a new record being emitted and has no existing timestamp.

Then collected value has timestamp Long.MAX_VALUE. The timestamp registered on the collected element seems to be timestamp when the timer was registered to be fired, which is Long.MAX_VALUE;;;","15/May/24 21:27;jeyhunkarimov; 
{code:java}
carData.assignTimestampsAndWatermarks(
                WatermarkStrategy
                        .<Tuple4<Integer, Integer, Double, Long>>
                                forMonotonousTimestamps()
                        .withTimestampAssigner((car, ts) -> car.f0))
        .keyBy(value -> value.f0)
        .process(new KeyedProcessFunction<Integer, Tuple4<Integer, Integer, Double, Long>, Tuple4<Integer, Integer, Double, Long>>() {
            @Override
            public void processElement(
                    Tuple4<Integer, Integer, Double, Long> value,
                    KeyedProcessFunction<Integer, Tuple4<Integer, Integer, Double, Long>, Tuple4<Integer, Integer, Double, Long>>.Context ctx,
                    Collector<Tuple4<Integer, Integer, Double, Long>> out) throws Exception {


                ctx.timerService().registerProcessingTimeTimer(Long.MAX_VALUE);
            }

            @Override
            public void onTimer(long timestamp, OnTimerContext ctx, Collector<Tuple4<Integer, Integer, Double, Long>> out) throws Exception {
                out.collect(new Tuple4<>(1,1,1.0,1L));
            }
        })
        .process(new ProcessFunction<Tuple4<Integer, Integer, Double, Long>, Tuple4<Integer, Integer, Double, Long>>() {
            @Override
            public void processElement(
                    Tuple4<Integer, Integer, Double, Long> value,
                    ProcessFunction<Tuple4<Integer, Integer, Double, Long>, Tuple4<Integer, Integer, Double, Long>>.Context ctx,
                    Collector<Tuple4<Integer, Integer, Double, Long>> out) throws Exception {
                LOG(ctx.timestamp());
            }
        }){code}
 

If this is the example you mean, then the second process function does not output timestamp of Long.MAX_VALUE. 

 

If you have sth else in mind, then please provide minimum reproducible example. Thanks

 ;;;","16/May/24 02:20;ekanthi;{code:java}
package sample;

import lombok.extern.slf4j.Slf4j;
import org.apache.flink.api.common.RuntimeExecutionMode;
import org.apache.flink.api.common.eventtime.IngestionTimeAssigner;
import org.apache.flink.api.common.eventtime.NoWatermarksGenerator;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.state.MapState;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.configuration.ConfigConstants;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.configuration.MetricOptions;
import org.apache.flink.metrics.jmx.JMXReporterFactory;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.KeyedProcessFunction;
import org.apache.flink.streaming.api.functions.ProcessFunction;
import org.apache.flink.util.Collector;import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;@Slf4j
public class BatchJobTest2 {    private static ParameterTool setupParams() {
        Map<String, String> properties = new HashMap<>();
        properties.put(""security.delegation.token.provider.hadoopfs.enabled"", ""false"");
        properties.put(""security.delegation.token.provider.hbase.enabled"", ""false"");
        return ParameterTool.fromMap(properties);
    }    public static void main(String[] args) throws Exception {
        ParameterTool paramUtils = setupParams();
        Configuration config = new Configuration(paramUtils.getConfiguration());
        config.setString(ConfigConstants.METRICS_REPORTER_PREFIX + ""jmx."" + ConfigConstants.METRICS_REPORTER_FACTORY_CLASS_SUFFIX, JMXReporterFactory.class.getName());
        config.setLong(MetricOptions.METRIC_FETCHER_UPDATE_INTERVAL, paramUtils.getLong(MetricOptions.METRIC_FETCHER_UPDATE_INTERVAL.key(), MetricOptions.METRIC_FETCHER_UPDATE_INTERVAL.defaultValue()));
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(config);        env.setRuntimeMode(RuntimeExecutionMode.BATCH);
        DataStream<Domain> positionData = domainStream(env);
        positionData.keyBy(Domain::getA1)
                .process(new KeyedProcessFunction<String, Domain, Domain>() {
                    private transient MapState<String, Domain> processedInputs;                    @Override
                    public void open(Configuration configuration) {
                        MapStateDescriptor<String, Domain> mapStateDescriptor = new MapStateDescriptor<>(""domain2-input"", TypeInformation.of(String.class),
                                TypeInformation.of(Domain.class));
                        processedInputs = getRuntimeContext().getMapState(mapStateDescriptor);
                    }                    @Override
                    public void processElement(Domain value, KeyedProcessFunction<String, Domain, Domain>.Context context, Collector<Domain> out) throws Exception {
                        processedInputs.put(value.getUniqueId(), value);
                        context.timerService().registerEventTimeTimer(Long.MAX_VALUE);                    }                    

@Override
                    public void onTimer(long timestamp, OnTimerContext ctx, Collector<Domain> collector) throws Exception {
                        processedInputs.iterator().forEachRemaining(entry -> collector.collect(entry.getValue()));
                        processedInputs.clear();
                    }
                }).process(new ProcessFunction<Domain, Void>() {
                    @Override
                    public void processElement(Domain value, ProcessFunction<Domain, Void>.Context ctx, Collector<Void> out) throws Exception {
                        log.info(""Timestamp : {}, element : {}"", ctx.timestamp(), value.getUniqueId());
                    }
                });        env.execute(""FileReadJob"");    

}    

public static DataStream<Domain> domainStream(StreamExecutionEnvironment env) {        /* Not assigning watermarks as program is being run in batch mode and watermarks are irrelevant to batch mode */
        return env.fromCollection(getDataCollection())
                .assignTimestampsAndWatermarks(getNoWatermarkStrategy())
                .returns(TypeInformation.of(Domain.class))
                .name(""test-domain-source"")
                .uid(""test-domain-source"");    
}    

private static List<Domain> getDataCollection() {
        List<Domain> data = new ArrayList<>();
        data.add(new Domain(""A11"", ""123-Z-1""));
        data.add(new Domain(""A11"", ""456-A-2""));
        data.add(new Domain(""A11"", ""456-B-2""));
        data.add(new Domain(""A21"", ""673-9Z-09""));
        data.add(new Domain(""A21"", ""843-09-21""));        
        return data;
    }    

   private static WatermarkStrategy<Domain> getNoWatermarkStrategy() {
        return WatermarkStrategy.<Domain>forGenerator((ctx) -> new NoWatermarksGenerator<>())
                .withTimestampAssigner((ctx) -> new IngestionTimeAssigner<>());
    }    

private static WatermarkStrategy<Domain> getMonotonous() {
        return WatermarkStrategy.<Domain>forMonotonousTimestamps()
                .withTimestampAssigner((ctx) -> new IngestionTimeAssigner<>());
    }    

private static class Domain {
        private String a1;
        private String uniqueId;        
public Domain() {        }        
public Domain(String a1, String uniqueId) {
            this.a1 = a1;
            this.uniqueId = uniqueId;
        }       
 public String getA1() {
            return a1;
        }        
public void setA1(String a1) {
            this.a1 = a1;
        }      
public String getUniqueId() {
            return uniqueId;
        }
    }
}
 {code}
Look at attached program which reproduces issue;;;",,,,,,,,,,,,,,,,,,,,,,,,
Flink Restart Strategy does not work as documented,FLINK-35288,13578133,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,keshavk,keshavk,04/May/24 04:46,07/May/24 03:37,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,"As per the documentation when using the Fixed Delay Restart Strategy, the
*restart-strategy.fixed-delay.attempts* defines the ""The number of times that Flink retries the execution before the job is declared as failed if has been set to fixed-delay"". 

However in reality it is the *maximum-total-task-failures*, i.e. it is possbile that the job does not even attempt to restart. 
This is as per documented in https://cwiki.apache.org/confluence/display/FLINK/FLIP-1%3A+Fine+Grained+Recovery+from+Task+Failures

If there is an outage at a Sink level, for example Elasticsearch outage, all the independent tasks might fail and the job will immediately fail without restart (if restart-strategy.fixed-delay.attempts is set lower or equal to the parallelism of the sink)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 03:37:08 UTC 2024,,,,,,,,,,"0|z1p0zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 03:37;bgeng777;https://cwiki.apache.org/confluence/display/FLINK/FLIP-364%3A+Improve+the+exponential-delay+restart-strategy#FLIP364:Improvetheexponentialdelayrestartstrategy-1.2Differentsemanticsofrestartattemptscauseregionfailovernotasexpected
In the above FLIP, there is some relevant discussion of the 'restart-strategy.fixed-delay.attempts' problem. When 'region-failover' (the default value of *jobmanager.execution.failover-strategy*) is enabled, the org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler#handleFailure method is called once a subtask in a region fails, which consumes the job-level 'restart-strategy.fixed-delay.attempts'. As a result, the restart strategy may not work as the documentation described.
We have also met such case in the production environment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Builder builds NetworkConfig for Elasticsearch connector 8,FLINK-35287,13578033,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liuml07,liuml07,liuml07,03/May/24 07:34,22/May/24 04:16,04/Jun/24 20:40,22/May/24 04:16,,,,,,,,,elasticsearch-3.1.0,,,Connectors / ElasticSearch,,,,0,pull-request-available,,"In FLINK-26088 we added support for ElasticSearch 8.0. It is based on Async sink API and does not use the base module {{flink-connector-elasticsearch-base}}. Regarding the config options (host, username, password, headers, ssl...), we pass all options from the builder to AsyncSink, and last to AsyncWriter. It is less flexible when we add new options and the constructors will get longer and multiple places may validate options unnecessarily. I think it's nice if we make the sink builder builds the NetworkConfig once, and pass it all the way to the writer. This is also how the base module for 6.x / 7.x is implemented. In my recent work adding new options to the network config, this way works simpler.

Let me create a PR to demonstrate the idea. No new features or major code refactoring other than the builder builds the NetworkConfig (code will be shorter). I have a few small fixes which I'll include into the incoming PR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 04:16:02 UTC 2024,,,,,,,,,,"0|z1p0dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/24 04:16;Weijie Guo;main via 0b6d9f823d9831816a81d02c0c1f367746363688.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot discover Hive connector outside Hive catalog,FLINK-35286,13578011,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ryandgoldenberg,ryandgoldenberg,02/May/24 23:21,02/May/24 23:22,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Hive,,,,0,,,"*Problem*

Referencing tables with the 'hive' connector outside of HiveCatalog gives the error
{code:java}
org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'hive' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath{code}
For example, when using a table created like
{code:java}
CREATE TABLE my_table LIKE hive.db.table WITH (...);{code}
Whereas the 'hive' connector is available if the table is referenced via HiveCatalog.

*Desired Behavior*
The 'hive' connector should be available for tables outside of HiveCatalog, for example in the default catalog.

*Benefits*
 * Can refer to Hive tables without fully qualified path `catalog.db.table` outside of HiveCatalog, useful when it is not the only catalog or data source.
 * Can modify Hive tables without changing Hive metastore or using SQL hints [here|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/hive/hive_read_write/#reading]","Flink 1.18.1
Hive 2.3.9
flink-sql-connector-hive-2.3.9_2.12-1.18.1.jar",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-02 23:21:48.0,,,,,,,,,,"0|z1p08w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler key group optimization can interfere with scale-down.max-factor,FLINK-35285,13577975,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,trystan,trystan,02/May/24 16:50,24/May/24 20:41,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,"When setting a less aggressive scale down limit, the key group optimization can prevent a vertex from scaling down at all. It will hunt from target upwards to maxParallelism/2, and will always find currentParallelism again.

 

A simple test trying to scale down from a parallelism of 60 with a scale-down.max-factor of 0.2:
{code:java}
assertEquals(48, JobVertexScaler.scale(60, inputShipStrategies, 360, .8, 8, 360)); {code}
 

It seems reasonable to make a good attempt to spread data across subtasks, but not at the expense of total deadlock. The problem is that during scale down it doesn't actually ensure that newParallelism will be < currentParallelism. The only workaround is to set a scale down factor large enough such that it finds the next lowest divisor of the maxParallelism.

 

Clunky, but something to ensure it can make at least some progress. There is another test that now fails, but just to illustrate the point:
{code:java}
for (int p = newParallelism; p <= maxParallelism / 2 && p <= upperBound; p++) {
    if ((scaleFactor < 1 && p < currentParallelism) || (scaleFactor > 1 && p > currentParallelism)) {
        if (maxParallelism % p == 0) {
            return p;
        }
    }
} {code}
 

Perhaps this is by design and not a bug, but total failure to scale down in order to keep optimized key groups does not seem ideal.

 

Key group optimization block:

[https://github.com/apache/flink-kubernetes-operator/blob/fe3d24e4500d6fcaed55250ccc816546886fd1cf/flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobVertexScaler.java#L296C1-L303C10]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 17:28:14 UTC 2024,,,,,,,,,,"0|z1p00w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/24 17:02;trystan;If this is deemed an actual bug, I'd be happy to submit a PR;;;","03/May/24 15:48;gyfora;I think you make a good point here but we have to be a bit careful in terms of how much key group skew we allow while scaling up or down.

When we are scaling down to a parallelism which doesn't result in an even key distribution then our computed expected throughput will be off (because that is based on the assumption that throughput is linearly dependent on the parallelism but that assumes even key group distribution -> no data skew introduced by the scaling itself).

However this is something we can actually calculate by looking at how uneven the key group distribition is. As long as the introduced skew is within the flexible target rate boundaries then we should be able to scale down (without expecting a ""rebound""). ;;;","03/May/24 16:02;trystan;Makes sense. Maybe it's not a problem with the decision itself, but with the documentation / validation around it? Keeping the keys balanced is a great idea. But it assumes that it _can_ make a decision, when the max-scale (up or down) can actually prevent it from making a decision. This test shows that if you have a max scale up factor of 10% it will actually never be able to scale beyond 60 no matter what.
{code:java}
assertEquals(66, JobVertexScaler.scale(60, inputShipStrategies, 360, 1.1, 8, 360)); {code}
Maybe there could be some validation, or priority setting? I guess it comes down to what's worse: never be able to scale at all or make an inefficient decision?

It's true that it's dependent on even key group distribution but it's also dependent on even key {_}utilization{_}. Which is ideal and I think most people strive for it but it is an easy mistake to make, especially early on when folks are just starting out with Flink.;;;","24/May/24 17:28;trystan;[~gyfora] is there maybe another setting that can help tune this? At least on 1.7.0, I often find that a max scale down factor of 0.5 (which seems to be essentially mandatory given the current computations) leads to an overshoot - so then it scales back up. For example 40 -> 20 -> 40 -> 24.

I'd prefer to be conservative on the scale down and set it to 0.2 or 0.3. In the case of maxParallelism=120, 0.3 would work for _this_ scale down from 40 (sort of - it results in 30), but 0.2 would not - we would effectively have a minParallelism of 40 and never go below it. Yet in the case of current=120, max=120, maxScaleDown=.2, it works just fine - it'll scale to 96. The ""good"" minScaleFactors seem highly dependent on both the maxParallelism and currentParallelism.

It seems that the problem lies in this loop: [https://github.com/apache/flink-kubernetes-operator/blob/fe3d24e4500d6fcaed55250ccc816546886fd1cf/flink-autoscaler/src/main/java/org/apache/flink/autoscaler/JobVertexScaler.java#L299-L303]

If we add 
{code:java}
&& p < currentParallelism {code}
to the loop we get the expected behavior on scale down. Of course, then other keygroup-optimized scale ups break. Perhaps there needs to be different loops for scale up / scale down. On scale down, ensure that p < currentParallelism and on scale up p > currentParallelism. I think this would fix the current scenario as well as the existing ones. I added a few tests locally that confirm it as well. If this is viable I'd be happy to make a PR.

Is there something obvious that I'm missing, something I can tune better?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink end-to-end test times out,FLINK-35284,13577974,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rskraba,rskraba,02/May/24 16:30,16/May/24 08:44,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,test-stability,,"1.20 e2e_2_cron_adaptive_scheduler https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59303&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=3076

{code}
May 01 01:08:42 Test (pid: 127498) did not finish after 900 seconds.
May 01 01:08:42 Printing Flink logs and killing it:
{code}

This looks like a consequence of hundreds of {{RecipientUnreachableException}}s like: 

{code}
2024-05-01 00:55:00,496 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer [] - Slot allocation for allocation 2ec550d8331cd53c32fd899e1e9a0fa5 for job 5654b195450b352be998673f1637fc43 failed.
org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException: Could not send message [RemoteRpcInvocation(TaskExecutorGateway.requestSlot(SlotID, JobID, AllocationID, ResourceProfile, String, ResourceManagerId, Time))] from sender [Actor[pekko://flink/temp/taskmanager_0$De]] to recipient [Actor[pekko.ssl.tcp://flink@localhost:40665/user/rpc/taskmanager_0#-299862847]], because the recipient is unreachable. This can either mean that the recipient has been terminated or that the remote RpcService is currently not reachable.
	at org.apache.flink.runtime.rpc.pekko.DeadLettersActor.handleDeadLetter(DeadLettersActor.java:61) ~[flink-rpc-akkafe85d469-8ced-4732-922e-62c82b554871.jar:1.20-SNAPSHOT]
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) ~[flink-rpc-akkafe85d469-8ced-4732-922e-62c82b554871.jar:1.20-SNAPSHOT]
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) ~[flink-rpc-akkafe85d469-8ced-4732-922e-62c82b554871.jar:1.20-SNAPSHOT]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-rpc-akkafe85d469-8ced-4732-922e-62c82b554871.jar:1.20-SNAPSHOT]
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 08:44:28 UTC 2024,,,,,,,,,,"0|z1p00o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 14:03;rskraba;* 1.20 AdaptiveScheduler / E2E (group 2) https://github.com/apache/flink/actions/runs/9048112585/job/24860957143#step:14:3735
* 1.20 e2e_2_cron_adaptive_scheduler https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59498&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=4404;;;","16/May/24 08:44;rskraba;* 1.20 e2e_2_cron_adaptive_scheduler https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59583&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=3098;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support unique Kafka producer client ids ,FLINK-35283,13577921,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,faltomare,faltomare,02/May/24 09:46,02/May/24 09:48,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,"This issue came out of debuging a warning we're seeing in our Flink logs. We're running Flink 1.18 and have an application that uses Kafka topics as a source and a sink. We're running with several tasks. The warning we're seeing in the logs is:

{{WARN org.apache.kafka.common.utils.AppInfoParser - Error registering AppInfo mbean}}
{{javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=kafka producer client id}}



I've spent a bit of time debugging, and it looks like the root cause of this warning is the Flink {{KafkaSink}} creating multiple {{{}KafkaWriter{}}}s that, in turn, create multiple {{{}KafkaProducer{}}}s with the same Kafka producer `{{{}client.id{}}}`. Since the value for {{client.id}} is used when registering the {{AppInfo}} MBean — when multiple {{{}KafkaProducer{}}}s with the same {{client.id}} are registered we get the above {{{}InstanceAlreadyExistsException{}}}. Since we're running with several tasks and we get a Kafka producer per task this duplicate registration exception makes sense to me.

I'm wondering if the fix would be to update the {{KafkaSink.builder}} by adding a {{setClientIdPrefix}} method, similar to what we have already on the {{{}KafkaSource.builder{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 02 09:48:37 UTC 2024,,,,,,,,,,"0|z1ozow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/24 09:48;faltomare;I've added a PoC here: https://github.com/apache/flink-connector-kafka/pull/101;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink Support for Apache Beam > 2.49,FLINK-35282,13577911,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,antoniovespoli,yaizauga,yaizauga,02/May/24 09:28,04/Jun/24 14:11,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,1.20.0,,,API / Python,,,,0,pull-request-available,,"From what I see PyFlink still has the requirement of Apache Beam => 2.43.0 and <= 2.49.0 which subsequently results in a requirement of PyArrow <= 12.0.0. That keeps us exposed to [https://nvd.nist.gov/vuln/detail/CVE-2023-47248]

I'm not deep enough familiar with the PyFlink code base to understand why Apache Beam's upper dependency limit can't be lifted. From all the existing issues I haven't seen one addressing this. Therefore I created one now. ",,,,,,,,,,,,,,,,,,,,,,FLINK-35520,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 10:56:24 UTC 2024,,,,,,,,,,"0|z1ozmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/24 08:42;hong; merged commit [{{91a9e06}}|https://github.com/apache/flink/commit/91a9e06d3cb611e048274088e56b2c110cd29926] into   apache:master;;;","04/Jun/24 10:56;Sergey Nuyanzin;I noticed that it was reverted by [~hong] at [caa68c2481f7c483d0364206d36654af26f2074f|https://github.com/apache/flink/commit/caa68c2481f7c483d0364206d36654af26f2074f]
for that reason it would make sense to reopen the issue as well


BTW I have a fix for license check issue at https://github.com/snuyanzin/flink/commit/5a4f4d0eb785050552c73fbfc74214f85ee278b0
We could try is once build becomes more stable;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkEnvironmentUtils#addJar add each jar only once,FLINK-35281,13577900,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,loserwang1024,loserwang1024,02/May/24 06:35,09/May/24 06:29,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Current org.apache.flink.cdc.composer.flink.FlinkEnvironmentUtils#addJar will be invoked for each source and sink.
{code:java}

public static void addJar(StreamExecutionEnvironment env, URL jarUrl) {
    try {
        Class<StreamExecutionEnvironment> envClass = StreamExecutionEnvironment.class;
        Field field = envClass.getDeclaredField(""configuration"");
        field.setAccessible(true);
        Configuration configuration = ((Configuration) field.get(env));
        List<String> jars =
                configuration.getOptional(PipelineOptions.JARS).orElse(new ArrayList<>());
        jars.add(jarUrl.toString());
        configuration.set(PipelineOptions.JARS, jars);
    } catch (Exception e) {
        throw new RuntimeException(""Failed to add JAR to Flink execution environment"", e);
    } {code}
if multiple source or sink share same jar, the par path will be added repeatly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-02 06:35:15.0,,,,,,,,,,"0|z1ozk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate HBase Sink connector to use the ASync Sink API,FLINK-35280,13577843,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,martijnvisser,martijnvisser,01/May/24 15:00,03/May/24 12:04,04/Jun/24 20:40,,hbase-3.0.0,hbase-3.0.1,hbase-4.0.0,,,,,,,,,Connectors / HBase,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-05-01 15:00:15.0,,,,,,,,,,"0|z1oz80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support ""last-state"" upgrade mode for FlinkSessionJob ",FLINK-35279,13577773,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,alnzng,alnzng,alnzng,30/Apr/24 17:00,02/May/24 09:08,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,"The ""last-state"" upgrade mode is only supported for Flink application mode today[1], we should provide a consistent / similar user experience in Flink session mode.

[1] [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/#stateful-and-stateless-application-upgrades]
{code:java}
Last state upgrade mode is currently only supported for FlinkDeployments. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-30 17:00:33.0,,,,,,,,,,"0|z1oysg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Occasional NPE on k8s operator status replacement,FLINK-35278,13577760,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ferenc-csaky,mbalassi,mbalassi,30/Apr/24 14:29,04/May/24 09:15,04/Jun/24 20:40,04/May/24 09:15,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,pull-request-available,,"Infrequently we get a null pointer exception on status replacement:
{noformat}
logger: io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher
message: Error during error status handling.   

throwable: { [-]
     class: java.lang.NullPointerException
     msg: null
     stack: [ [-]
       org.apache.flink.kubernetes.operator.utils.StatusRecorder.replaceStatus(StatusRecorder.java:136)
       org.apache.flink.kubernetes.operator.utils.StatusRecorder.patchAndCacheStatus(StatusRecorder.java:97)
       org.apache.flink.kubernetes.operator.reconciler.ReconciliationUtils.toErrorStatusUpdateControl(ReconciliationUtils.java:438)
       org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.updateErrorStatus(FlinkDeploymentController.java:213)
       org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.updateErrorStatus(FlinkDeploymentController.java:60)
       io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleErrorStatusHandler(ReconciliationDispatcher.java:194)
       io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:123)
       io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:91)
       io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:64)
       io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:452)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
       java.lang.Thread.run(Thread.java:829)
     ]
   }{noformat}
I suspect it probably is thrown by getResourceVersion() here:

[https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/StatusRecorder.java#L136]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 04 09:15:38 UTC 2024,,,,,,,,,,"0|z1oypk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/24 09:15;mbalassi;[e73363f|https://github.com/apache/flink-kubernetes-operator/commit/e73363f3486ed9e1df5cc05c9d0baec7c8c3a37f] in main.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the error in the `asncdcaddremove.sql` script for the DB2 test container.,FLINK-35277,13577759,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vinlee,vinlee,30/Apr/24 14:20,24/May/24 09:17,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"1. background

When attempting to use Flink CDC 3.1 in the Flink connector to load data from DB2 to Apache Doris, I set up DB2 using the Docker image `{{{}ruanhang/db2-cdc-demo:v1`{}}}. After configuring the DB2 asynchronous CDC, I tried to capture a table using {{{}CALL ASNCDC.ADDTABLE('MYSCHEMA', 'MYTABLE'){}}}. However, I encountered an error when attempting to add the eleventh table:
[23505][-803] One or more values in the INSERT statement, UPDATE statement, or foreign key update caused by a DELETE statement are not valid because the primary key, unique constraint or unique index identified by ""2"" constrains table ""ASNCDC.IBMSNAP_PRUNCNTL"" from having duplicate values for the index key.. SQLCODE=-803, SQLSTATE=23505, DRIVER=4.26.14
!image-2024-04-30-22-19-17-350.png!

 

2. 

The error indicates that the table {{Asncdc.IBMSNAP_PRUNCNTL}} has a duplicate primary key.

Here is the schema of {{{}Asncdc.IBMSNAP_PRUNCNTL{}}}:
create table IBMSNAP_PRUNCNTL
(
  TARGET_SERVER     CHARACTER(18) not null,
  TARGET_OWNER      VARCHAR(128)  not null,
  TARGET_TABLE      VARCHAR(128)  not null,
  SYNCHTIME         TIMESTAMP(6),
  SYNCHPOINT        VARCHAR(16) FOR BIT DATA,
  SOURCE_OWNER      VARCHAR(128)  not null,
  SOURCE_TABLE      VARCHAR(128)  not null,
  SOURCE_VIEW_QUAL  SMALLINT      not null,
  APPLY_QUAL        CHARACTER(18) not null,
  SET_NAME          CHARACTER(18) not null,
  CNTL_SERVER       CHARACTER(18) not null,
  TARGET_STRUCTURE  SMALLINT      not null,
  CNTL_ALIAS        CHARACTER(8),
  PHYS_CHANGE_OWNER VARCHAR(128),
  PHYS_CHANGE_TABLE VARCHAR(128),
  MAP_ID            VARCHAR(10)   not null
);
​
create unique index IBMSNAP_PRUNCNTLX
   on IBMSNAP_PRUNCNTL (SOURCE_OWNER, SOURCE_TABLE, SOURCE_VIEW_QUAL, APPLY_QUAL, SET_NAME, TARGET_SERVER,
                        TARGET_TABLE, TARGET_OWNER);
​
create unique index IBMSNAP_PRUNCNTLX1
   on IBMSNAP_PRUNCNTL (MAP_ID);
​
create index IBMSNAP_PRUNCNTLX2
   on IBMSNAP_PRUNCNTL (PHYS_CHANGE_OWNER, PHYS_CHANGE_TABLE);
​
create index IBMSNAP_PRUNCNTLX3
   on IBMSNAP_PRUNCNTL (APPLY_QUAL, SET_NAME, TARGET_SERVER);
The issue stems from the logic in {{asncdc.addtable}} not aligning with the {{asncdcaddremove.sql}} script when calling the {{addtable}} procedure. The original insert statement is as follows:
– Original insert statement
SET stmtSQL =   'INSERT INTO ASNCDC.IBMSNAP_PRUNCNTL ( ' || 
               'TARGET_SERVER, ' || 
               'TARGET_OWNER, ' || 
               'TARGET_TABLE, ' || 
               'SYNCHTIME, ' || 
               'SYNCHPOINT, ' || 
               'SOURCE_OWNER, ' || 
               'SOURCE_TABLE, ' || 
               'SOURCE_VIEW_QUAL, ' || 
               'APPLY_QUAL, ' || 
               'SET_NAME, ' || 
               'CNTL_SERVER , ' || 
               'TARGET_STRUCTURE , ' || 
               'CNTL_ALIAS , ' || 
               'PHYS_CHANGE_OWNER , ' || 
               'PHYS_CHANGE_TABLE , ' || 
               'MAP_ID ' || 
               ') VALUES ( ' || 
               '''KAFKA'', ' || 
               '''' || tableschema || ''', ' || 
               '''' || tablename || ''', ' ||
               'NULL, ' || 
               'NULL, ' || 
               '''' || tableschema || ''', ' || 
               '''' || tablename || ''', ' ||
               '0, ' || 
               '''KAFKAQUAL'', ' || 
               '''SET001'', ' || 
               ' (Select CURRENT_SERVER from sysibm.sysdummy1 ), ' || 
               '8, ' || 
               ' (Select CURRENT_SERVER from sysibm.sysdummy1 ), ' || 
               '''ASNCDC'', ' || 
               '''CDC_' || tableschema ||  '_' || tablename || ''', ' ||
               ' ( SELECT CASE WHEN max(CAST(MAP_ID AS INT)) IS NULL THEN CAST(1 AS VARCHAR(10)) ELSE CAST(CAST(max(MAP_ID) AS INT) + 1 AS VARCHAR(10)) END AS MYINT from ASNCDC.IBMSNAP_PRUNCNTL ) ' || 
               '   )';
EXECUTE IMMEDIATE stmtSQL;
The {{max(MAP_ID)}} logic is incorrect, as the correct result should be {{{}CAST(max(CAST(MAP_ID AS INT)) + 1 AS VARCHAR(10)){}}}. This issue prevents the addition of the eleventh table. For more details about {{{}asncdcaddremove.sql{}}}, please refer to: [asncdcaddremove.sql|https://github.com/debezium/debezium-examples/blob/main/tutorial/debezium-db2-init/db2server/asncdcaddremove.sql#L189]","Flink 1.18.0 

Flink CDC 3.1-pre

DB2 11.5.x

 ",,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/24 14:19;vinlee;image-2024-04-30-22-19-17-350.png;https://issues.apache.org/jira/secure/attachment/13068570/image-2024-04-30-22-19-17-350.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 14:21:41 UTC 2024,,,,,,,,,,"0|z1oypc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/24 14:21;vinlee;asncdcaddremove.sql: https://github.com/debezium/debezium-examples/blob/main/tutorial/debezium-db2-init/db2server/asncdcaddremove.sql#L189;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortCodeGeneratorTest.testMultiKeys fails on negative zero,FLINK-35276,13577751,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,30/Apr/24 13:49,30/Apr/24 13:50,04/Jun/24 20:40,,1.19.1,1.20.0,,,,,,,,,,Table SQL / Planner,,,,0,test-stability,,"1.19 AdaptiveScheduler / Test (module: table) [https://github.com/apache/flink/actions/runs/8864296211/job/24339523745#step:10:10757]

SortCodeGeneratorTest can fail if one of the generated random row values is -0.0f.
{code:java}
Apr 28 02:38:03 expect: +I(,SqlRawValue{?},0.0,false); actual: +I(,SqlRawValue{?},-0.0,false)
Apr 28 02:38:03 expect: +I(,SqlRawValue{?},-0.0,false); actual: +I(,SqlRawValue{?},0.0,false)
...
<snip>
...
Apr 28 02:38:04 expect: +I(,null,4.9695407E17,false); actual: +I(,null,4.9695407E17,false)
Apr 28 02:38:04 expect: +I(,null,-3.84924672E18,false); actual: +I(,null,-3.84924672E18,false)
Apr 28 02:38:04 types: [[RAW('java.lang.Integer', ?), FLOAT, BOOLEAN]]
Apr 28 02:38:04 keys: [0, 1]] 
Apr 28 02:38:04 expected: 0.0f
Apr 28 02:38:04  but was: -0.0f
Apr 28 02:38:04 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Apr 28 02:38:04 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Apr 28 02:38:04 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Apr 28 02:38:04 	at org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.testInner(SortCodeGeneratorTest.java:632)
Apr 28 02:38:04 	at org.apache.flink.table.planner.codegen.SortCodeGeneratorTest.testMultiKeys(SortCodeGeneratorTest.java:143)
Apr 28 02:38:04 	at java.lang.reflect.Method.invoke(Method.java:498)
{code}

In the test code, this is extremely unlikely to occur (one in 2²⁴?) but *has* happened at this line (when the {{rnd.nextFloat()}} is {{0.0f}} and {{rnd.nextLong()}} is negative:

[https://github.com/apache/flink/blob/e7ce0a2969633168b9395c683921aa49362ad7a4/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/codegen/SortCodeGeneratorTest.java#L255]

We can reproduce the failure by changing how likely {{0.0f}} is to be generated at that line.",,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/24 13:50;rskraba;job-logs.txt;https://issues.apache.org/jira/secure/attachment/13068569/job-logs.txt",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-30 13:49:37.0,,,,,,,,,,"0|z1oynk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayContainsFunction uses wrong DataType to create element getter,FLINK-35275,13577747,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,banmoy,banmoy,30/Apr/24 13:39,30/Apr/24 13:57,04/Jun/24 20:40,,1.16.0,,,,,,,,,,,Table SQL / Runtime,,,,0,,," 

In ArrayContainsFunction, elementGetter is used to get elements of an array, but it's created from the needle data type rather than the element data type which will lead to wrong results.
{code:java}
public ArrayContainsFunction(SpecializedContext context) {
    super(BuiltInFunctionDefinitions.ARRAY_CONTAINS, context);
    final DataType needleDataType = context.getCallContext().getArgumentDataTypes().get(1);
    elementGetter = ArrayData.createElementGetter(needleDataType.getLogicalType());
    
} {code}
For example, the following sql returns true, but the expected is false. The element type is nullable int, and the needle type is non-nullable int. Using the needle type to create element getter will convert the NULL element to 0, so the result returns true.
{code:java}
SELECT ARRAY_CONTAINS(ARRAY[1, NULL], 0){code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 13:57:01 UTC 2024,,,,,,,,,,"0|z1oymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/24 13:57;banmoy;[~twalthr] Could you help confirm if this is a bug? If so, I'd like to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Occasional failure issue with Flink CDC Db2 UT,FLINK-35274,13577711,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pacinogong,pacinogong,pacinogong,30/Apr/24 10:34,13/May/24 09:31,04/Jun/24 20:40,06/May/24 05:58,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Occasional failure issue with Flink CDC Db2 UT. Because db2 redolog data tableId don't have database name, it will cause table schame occasional not found when task exception restart. I will fix it by supplement database name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 06 05:58:46 UTC 2024,,,,,,,,,,"0|z1oyeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/24 05:58;jiabaosun;Fixed via cdc
* master: a7cb46f7621568486a069a7ae01a7b86ebb0a801
* release-3.1: d556f29475a52234a98bcc65db959483a10beb52;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink's LocalZonedTimestampType should respect timezone set by set_local_timezone,FLINK-35273,13577706,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bgeng777,bgeng777,30/Apr/24 10:01,30/Apr/24 10:25,04/Jun/24 20:40,,,,,,,,,,,,,API / Python,,,,1,,,"The issue is from https://apache-flink.slack.com/archives/C065944F9M2/p1714134880878399
When using TIMESTAMP_LTZ in PyFlink while setting a different time zone, it turns out that the output result does not show the expected result.
Here is my test codes:
{code:python}
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common import Types, Configuration
from pyflink.table import DataTypes, StreamTableEnvironment
from datetime import datetime
import pytz


config = Configuration()
config.set_string(""python.client.executable"", ""/usr/local/Caskroom/miniconda/base/envs/myenv/bin/python"")
config.set_string(""python.executable"", ""/usr/local/Caskroom/miniconda/base/envs/myenv/bin/python"")
env = StreamExecutionEnvironment.get_execution_environment(config)
t_env = StreamTableEnvironment.create(env)
t_env.get_config().set_local_timezone(""UTC"")
# t_env.get_config().set_local_timezone(""GMT-08:00"")

input_table = t_env.from_elements(
    [
        (
            ""elementA"",
            datetime(year=2024, month=4, day=12, hour=8, minute=35),
        ),
        (
            ""elementB"",
            datetime(year=2024, month=4, day=12, hour=8, minute=35, tzinfo=pytz.utc),
            # datetime(year=2024, month=4, day=12, hour=8, minute=35, tzinfo=pytz.timezone('America/New_York')),
        ),
    ],
    DataTypes.ROW(
        [
            DataTypes.FIELD(""name"", DataTypes.STRING()),
            DataTypes.FIELD(""timestamp"", DataTypes.TIMESTAMP_LTZ(3)),
        ]
    ),
)
input_table.execute().print()

# SQL
sql_result = t_env.execute_sql(""CREATE VIEW MyView1 AS SELECT TO_TIMESTAMP_LTZ(1712910900000, 3);"")
t_env.execute_sql(""CREATE TABLE Sink (`t` TIMESTAMP_LTZ) WITH ('connector'='print');"")
t_env.execute_sql(""INSERT INTO Sink SELECT * FROM MyView1;"")
{code}
The output is: 
{code:java}
+----+--------------------------------+-------------------------+
| op |                           name |               timestamp |
+----+--------------------------------+-------------------------+
| +I |                       elementA | 2024-04-12 08:35:00.000 |
| +I |                       elementB | 2024-04-12 16:35:00.000 |
+----+--------------------------------+-------------------------+
2 rows in set
+I[2024-04-12T08:35:00Z]
{code}

In pyflink/tables/types.py, the `LocalZonedTimestampType` class will use follow logic to convert python obj to sql type:

{code:python}
EPOCH_ORDINAL = calendar.timegm(time.localtime(0)) * 10 ** 6
...
def to_sql_type(self, dt):
        if dt is not None:
            seconds = (calendar.timegm(dt.utctimetuple()) if dt.tzinfo
                       else time.mktime(dt.timetuple()))
            return int(seconds) * 10 ** 6 + dt.microsecond + self.EPOCH_ORDINAL
{code}
It shows that the EPOCH_ORDINAL is calculated when the PVM starts but is not decided by the timezone set by `set_local_timezone`.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-30 10:01:18.0,,,,,,,,,,"0|z1oydk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pipeline Transform job supports omitting / renaming calculation column,FLINK-35272,13577652,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xiqian_yu,xiqian_yu,xiqian_yu,30/Apr/24 01:42,24/May/24 09:17,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"Currently, pipeline transform rules require all columns used by expression calculation must be present in final projected schema, and shall not be renamed or omitted.

 

The reason behind this is any column not directly present in projection rules will be filtered out in the PreProjection step, and then the PostProjection process could not find those non-present but indirectly depended columns.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 08:56:05 UTC 2024,,,,,,,,,,"0|z1oy1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/24 01:43;xiqian_yu;[~renqs] I'm willing to implement this.;;;","24/May/24 08:56;leonard;Thanks [~xiqian_yu] for taking this ticket, assigned it to you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for syntax `describe job 'xxx'`,FLINK-35271,13577650,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,30/Apr/24 01:24,30/Apr/24 01:25,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35194,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-30 01:24:34.0,,,,,,,,,,"0|z1oy14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Enrich information in logs, making it easier for debugging",FLINK-35270,13577605,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hctommy,hctommy,hctommy,29/Apr/24 17:23,09/May/24 03:21,04/Jun/24 20:40,09/May/24 03:21,,,,,,,,,1.20.0,,,API / Core,,,,0,pull-request-available,starter,"Good logs helps debug a lot in production environment

Therefore, it'll be better to show more information in logs  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 09 03:21:55 UTC 2024,,,,,,,,,,"0|z1oxr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 03:21;zhuzh;547e4b53ebe36c39066adcf3a98123a1f7890c15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix logging level for errors in AWS connector sinks,FLINK-35269,13577595,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,29/Apr/24 15:54,17/May/24 09:28,04/Jun/24 20:40,17/May/24 09:28,aws-connector-4.2.0,,,,,,,,aws-connector-4.3.0,,,Connectors / AWS,Connectors / Firehose,Connectors / Kinesis,,0,pull-request-available,,"Currently Kinesis and Firehose sinks log information about failed records at DEBUG level.

This makes failures invisible in the logs, limiting ability to investigate source of the issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 17 09:27:30 UTC 2024,,,,,,,,,,"0|z1oxow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/24 09:27;hong; merged commit [{{c688a85}}|https://github.com/apache/flink-connector-aws/commit/c688a8545ac1001c8450e8c9c5fe8bbafa13aeba] into   apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support TTL for Async State API,FLINK-35268,13577563,13574084,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,masteryhx,masteryhx,29/Apr/24 10:20,29/Apr/24 10:20,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-29 10:20:13.0,,,,,,,,,,"0|z1oxhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create documentation for FlinkStateSnapshot CR,FLINK-35267,13577556,13577544,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mateczagany,mateczagany,29/Apr/24 09:05,29/Apr/24 09:05,04/Jun/24 20:40,,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,,,This should cover the new features and migration from the now deprecated methods of taking snapshots.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-29 09:05:49.0,,,,,,,,,,"0|z1oxg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add e2e tests for FlinkStateSnapshot CRs,FLINK-35266,13577554,13577544,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mateczagany,mateczagany,29/Apr/24 09:04,29/Apr/24 09:04,04/Jun/24 20:40,,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-29 09:04:06.0,,,,,,,,,,"0|z1oxfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement FlinkStateSnapshot custom resource,FLINK-35265,13577553,13577544,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mateczagany,mateczagany,29/Apr/24 09:03,29/Apr/24 11:53,04/Jun/24 20:40,,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-29 09:03:31.0,,,,,,,,,,"0|z1oxfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC Pipeline transform rules do not take effect,FLINK-35264,13577551,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,xiqian_yu,xiqian_yu,29/Apr/24 08:52,29/Apr/24 12:24,04/Jun/24 20:40,29/Apr/24 12:24,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"When multiple transform rules are provided, the previous ones do not take effect since transform rules are overwritten in for-loop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 12:24:36 UTC 2024,,,,,,,,,,"0|z1oxf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 12:24;renqs;flink-cdc master: f61f0f44bd56196296193edf9e9fa693e22e6a3c

release-3.1: 71dfad24dffb25f676d732cf9436fb32b88b670b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-446: Kubernetes Operator State Snapshot CRD,FLINK-35263,13577544,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mateczagany,mateczagany,29/Apr/24 08:15,29/Apr/24 08:15,04/Jun/24 20:40,,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,,,"Described in [https://cwiki.apache.org/confluence/display/FLINK/FLIP-446%3A+Kubernetes+Operator+State+Snapshot+CRD]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-29 08:15:49.0,,,,,,,,,,"0|z1oxdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bridge between AsyncKeyedStateBackend and AsyncExecutionController,FLINK-35262,13577541,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,29/Apr/24 08:02,08/May/24 07:11,04/Jun/24 20:40,08/May/24 07:11,,,,,,,,,1.20.0,,,Runtime / State Backends,Runtime / Task,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 08 07:11:31 UTC 2024,,,,,,,,,,"0|z1oxcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/24 07:11;masteryhx;merged 81fd465a into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC pipeline transform doesn't support decimal-type comparison,FLINK-35261,13577514,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiqian_yu,xiqian_yu,29/Apr/24 02:58,29/Apr/24 02:58,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"It would be convenient if we can filter by comparing decimal to number literals like:

{{transform:}}

{{  - source-table: XXX}}

{{    filter: price > 50}}

where price is a Decimal typed column. However currently such expression is not supported, and a runtime exception will be thrown as follows:

 

Caused by: org.apache.flink.api.common.InvalidProgramException: Expression cannot be compiled. This is a bug. Please file an issue.
Expression: import static org.apache.flink.cdc.runtime.functions.SystemFunctionUtils.*;PRICEALPHA > 50
    at org.apache.flink.cdc.runtime.operators.transform.TransformExpressionCompiler.lambda$compileExpression$0(TransformExpressionCompiler.java:62) ~[blob_p-c3b34ad5a5a3a0bc443bb738e308b20b1da04a1f-8d419e2c927baeb0eeb40fb35c0a52dc:3.2-SNAPSHOT]
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4868) ~[blob_p-d4bee45ff47f13d247ff78e6f3d164170cf71835-4816fa00c7fd16a7096498e5ed3caaee:3.2-SNAPSHOT]
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3533) ~[blob_p-d4bee45ff47f13d247ff78e6f3d164170cf71835-4816fa00c7fd16a7096498e5ed3caaee:3.2-SNAPSHOT]
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2282) ~[blob_p-d4bee45ff47f13d247ff78e6f3d164170cf71835-4816fa00c7fd16a7096498e5ed3caaee:3.2-SNAPSHOT]
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2159) ~[blob_p-d4bee45ff47f13d247ff78e6f3d164170cf71835-4816fa00c7fd16a7096498e5ed3caaee:3.2-SNAPSHOT]
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2049) ~[blob_p-d4bee45ff47f13d247ff78e6f3d164170cf71835-4816fa00c7fd16a7096498e5ed3caaee:3.2-SNAPSHOT]
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache.get(LocalCache.java:3966) ~[blob_p-d4bee45ff47f13d247ff78e6f3d164170cf71835-4816fa00c7fd16a7096498e5ed3caaee:3.2-SNAPSHOT]
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4863) ~[blob_p-d4bee45ff47f13d247ff78e6f3d164170cf71835-4816fa00c7fd16a7096498e5ed3caaee:3.2-SNAPSHOT]
    at org.apache.flink.cdc.runtime.operators.transform.TransformExpressionCompiler.compileExpression(TransformExpressionCompiler.java:46) ~[blob_p-c3b34ad5a5a3a0bc443bb738e308b20b1da04a1f-8d419e2c927baeb0eeb40fb35c0a52dc:3.2-SNAPSHOT]
    ... 17 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 1, Column 89: Cannot compare types ""java.math.BigDecimal"" and ""int""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-29 02:58:03.0,,,,,,,,,,"0|z1ox74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Watermark alignment ""page into Chinese",FLINK-35260,13577510,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,maomao,maomao,29/Apr/24 02:20,29/Apr/24 02:23,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,chinese-translation,Documentation,,,0,,,"Watermark alignment lack of chinese translation

!image-2024-04-29-10-21-00-565.png|width=408,height=215!",,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/24 02:21;maomao;image-2024-04-29-10-21-00-565.png;https://issues.apache.org/jira/secure/attachment/13068525/image-2024-04-29-10-21-00-565.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 02:22:50 UTC 2024,,,,,,,,,,"0|z1ox68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 02:22;maomao;[https://nightlies.apache.org/flink/flink-docs-release-1.19/zh/docs/dev/datastream/event-time/generating_watermarks/]

could you assign this to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkCDC Pipeline transform can't deal timestamp field,FLINK-35259,13577486,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wink,wink,wink,28/Apr/24 13:35,17/May/24 03:24,04/Jun/24 20:40,17/May/24 03:24,cdc-3.1.0,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"When the original table contains fields of type Timestamp, it cannot be converted properly.

When the added calculation columns contain fields of type Timestamp, it cannot be converted properly.",,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 03:33:25 UTC 2024,,,,,,,,,,"0|z1ox0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 03:33;renqs;flink-cdc master: 0108d0e5d18c55873c3d67e9caad58b9d2148d6a

release-3.1: e022f4d2592dedc6d570948213fd07f47c1e505c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Broken links to Doris in Flink CDC Documentation,FLINK-35258,13577468,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,renqs,renqs,28/Apr/24 07:59,29/Apr/24 02:30,04/Jun/24 20:40,29/Apr/24 02:30,cdc-3.1.0,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"These broken links are detected by CI: 
 
{code:java}
ERROR: 3 dead links found!
535  [✖] https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD/ → Status: 404
536  [✖] https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE/ → Status: 404
537  [✖] https://doris.apache.org/docs/dev/sql-manual/sql-reference/Data-Types/BOOLEAN/ → Status: 404 


ERROR: 3 dead links found!
1008  [✖] https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD/ → Status: 404
1009  [✖] https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE/ → Status: 404
1010  [✖] https://doris.apache.org/docs/dev/sql-manual/sql-reference/Data-Types/BOOLEAN/ → Status: 404{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 02:29:41 UTC 2024,,,,,,,,,,"0|z1owww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 02:29;renqs;flink-cdc master: a513e9f82e815409a8c91bdc999196d1658137f0

release-3.1: 65a6880e9c67c1dcc2ac506334c475583f6f86b1

release-3.0: 326739905ab8152cbde8289a9f29f18e25a016b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
optimize the handling of schema.change.behavior as EXCEPTION,FLINK-35257,13577463,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yanghuai,yanghuai,28/Apr/24 06:36,28/Apr/24 06:42,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"cdc job will throw exception when schema.change.behavior is exception，The data between the last checkpoint and the current DDL will not be refreshed to the sink, making it difficult for the task to recover this part of the data.
if the sink will flush all data and then throw an exception. This way, the task only needs to recover from the DDL position on the source to ensure data consistency.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 28 06:42:12 UTC 2024,,,,,,,,,,"0|z1owvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/24 06:42;yanghuai;if schema.change.behavior is EXCEPTION，just throw exception before com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistryRequestHandler#applySchemaChange;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pipeline transform ignores column type nullability,FLINK-35256,13577462,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,xiqian_yu,xiqian_yu,28/Apr/24 06:35,29/Apr/24 02:23,04/Jun/24 20:40,29/Apr/24 02:23,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"Flink CDC 3.1.0 brought transform feature, allowing column type / value transformation prior to data routing process. However after the transformation, column type marked as `NOT NULL` lost their annotation, causing some downstream sinks to fail since they require primary key to be NOT NULL.

Here's the minimum reproducible example about this problem:

```yaml
source:
type: mysql
...

sink:
type: starrocks
name: StarRocks Sink
...

pipeline:
name: Sync MySQL Database to StarRocks
parallelism: 4

transform:
 - source-table: reicigo.\.*
projection: ID, UPPER(ID) AS UPID

```

In the MySQL source table, primary key column `ID` is marked as `NOT NULL`, but such information was lost at downstream, causing the following exception (see attachment).",,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/24 06:35;xiqian_yu;log.txt;https://issues.apache.org/jira/secure/attachment/13068504/log.txt",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 28 13:16:48 UTC 2024,,,,,,,,,,"0|z1owvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/24 13:16;renqs;flink-cdc master: 6258bec5bb31533c501bedffef36805b0fd95cf5

flink-cdc release-3.1: de19f7bc6d2745930995846fdfbc6c6b60de5f74;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSinkWriterOperator should override snapshotState and processWatermark method,FLINK-35255,13577460,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yanghuai,yanghuai,yanghuai,28/Apr/24 06:14,29/Apr/24 07:24,04/Jun/24 20:40,29/Apr/24 07:24,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"DataSinkWriterOperator just override org.apache.flink.streaming.api.operators.AbstractStreamOperator#initializeState,but not override org.apache.flink.streaming.api.operators.AbstractStreamOperator#snapshotState and org.apache.flink.streaming.api.operators.AbstractStreamOperator#processWatermark.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 03:52:40 UTC 2024,,,,,,,,,,"0|z1owv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 03:52;renqs;flink-cdc master: 23a67dcdb985f135e984e8e1afa8d1159946f95f

flink-cdc release-3.1: 759b2944968aec6de0aca97d1a5329ecaada1c56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
build_wheels_on_macos failed,FLINK-35254,13577459,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,28/Apr/24 05:36,31/May/24 13:58,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"{code:java}
 ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
          unknown package:
              Expected sha256 f12932e5a6feb5c58192209af1d2607d488cb1d404fbc038ac12ada60327fa34
                   Got        1c61bf307881167fe169de79c02f46d16fc5cd35781e02a40bf1f13671cdc22c
      
      [end of output]

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59219&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=288
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 13:58:45 UTC 2024,,,,,,,,,,"0|z1owuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 14:03;rskraba;* 1.20 build_wheels_on_macos https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59476&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=426
;;;","31/May/24 13:58;rskraba;* 1.18 build_wheels_on_macos https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59986&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=204;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""State Processor API ""page into Chinese",FLINK-35253,13577455,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,drymartini,drymartini,drymartini,28/Apr/24 02:59,20/May/24 03:25,04/Jun/24 20:40,20/May/24 03:24,1.19.0,,,,,,,,1.19.0,,,chinese-translation,Documentation,,,0,pull-request-available,,The links are https://nightlies.apache.org/flink/flink-docs-release-1.19/zh/docs/libs/state_processor_api/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 20 03:24:05 UTC 2024,,,,,,,,,,"0|z1owu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/24 02:59;drymartini; Hi [~jark],could you assign this to me?;;;","28/Apr/24 05:40;Weijie Guo;Thanks, you are assigned.;;;","20/May/24 03:24;Weijie Guo;This page is already translated to Chinese in the master branch, I will close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update the operators marked as deprecated in the instance program on the official website,FLINK-35252,13577454,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,maomao,maomao,28/Apr/24 02:11,29/Apr/24 01:52,04/Jun/24 20:40,,,,,,,,,,,,,Examples,,,,0,,,"Update the operators marked as deprecated in the instance program on the official website.

!image-2024-04-28-10-07-36-248.png|width=386,height=199!

!image-2024-04-28-10-08-14-928.png|width=448,height=82!

The recommended usage now is
Duration.ofSeconds(5)
!image-2024-04-28-10-09-32-184.png|width=474,height=78!",,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/24 02:05;maomao;image-2024-04-28-10-05-37-671.png;https://issues.apache.org/jira/secure/attachment/13068502/image-2024-04-28-10-05-37-671.png","28/Apr/24 02:07;maomao;image-2024-04-28-10-07-11-736.png;https://issues.apache.org/jira/secure/attachment/13068501/image-2024-04-28-10-07-11-736.png","28/Apr/24 02:07;maomao;image-2024-04-28-10-07-36-248.png;https://issues.apache.org/jira/secure/attachment/13068500/image-2024-04-28-10-07-36-248.png","28/Apr/24 02:08;maomao;image-2024-04-28-10-08-14-928.png;https://issues.apache.org/jira/secure/attachment/13068499/image-2024-04-28-10-08-14-928.png","28/Apr/24 02:09;maomao;image-2024-04-28-10-09-32-184.png;https://issues.apache.org/jira/secure/attachment/13068498/image-2024-04-28-10-09-32-184.png",,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 28 02:12:57 UTC 2024,,,,,,,,,,"0|z1owts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/24 02:12;maomao;i am willing to contribute, can you assign this translation task to me.

Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaDerivation serializes mapping incorrectly on checkpoint,FLINK-35251,13577443,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,renqs,renqs,27/Apr/24 15:07,29/Apr/24 02:25,04/Jun/24 20:40,29/Apr/24 02:25,cdc-3.1.0,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"When schema registry is being serialized on checkpoint, SchemaDerivation mistakenly use `out.write()` instead of `out.writeInt()` for serializing the size of derivation mapping. This will cause an EOFException when the job is recovered from checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 28 00:31:31 UTC 2024,,,,,,,,,,"0|z1owrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/24 00:31;leonard;fixed in 
master(3.2) via 9cc3451ddf6546317c4c3af1efbc68c2d745ef7d
3.1 via : 711bc0038cf46a8a07cce7a1d777a4fd370f7ee3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate the ""Flink Operations Playground"" page into Chinese",FLINK-35250,13577441,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,drymartini,drymartini,27/Apr/24 14:09,27/Apr/24 14:21,04/Jun/24 20:40,27/Apr/24 14:21,1.19.0,,,,,,,,1.19.0,,,chinese-translation,Documentation,,,0,,,"The links are 

https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/try-flink/flink-operations-playground/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 27 14:15:46 UTC 2024,,,,,,,,,,"0|z1owqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/24 14:15;drymartini; hi,[~jark] could you please assign this to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support DataGeneratorSource in PyFlink,FLINK-35249,13577421,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vonesec,vonesec,27/Apr/24 07:47,30/Apr/24 02:39,04/Jun/24 20:40,,,,,,,,,,,,,API / Python,,,,0,,,"I notice DataGeneratorSource with Java has submitted in ""flink-connectors/flink-connector-datagen/src/main/java/org/apache/flink/connector/datagen/source/DataGeneratorSource.java"".

We could implement it in PyFlink, since flink-python component is not very active recently,  please assign it to me, I will create a PR for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-27 07:47:30.0,,,,,,,,,,"0|z1owmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support table/column comment to modify DDL,FLINK-35248,13577417,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,27/Apr/24 03:28,27/Apr/24 03:28,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"Table and column comment changes are synchronized to downstream tables：
 
{code:java}
alter table orders comment '订单';  alter table orders modify address varchar(1024) null comment '地址'; {code}
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-27 03:28:48.0,,,,,,,,,,"0|z1owlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade spotless apply to `2.41.1` in flink-connector-parent to work with Java 21,FLINK-35247,13577383,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,masc,masc,26/Apr/24 17:50,26/Apr/24 17:51,04/Jun/24 20:40,,connector-parent-1.1.0,,,,,,,,,,,Build System / CI,Connectors / Common,,,0,,,"Spotless apply version from flink-connector-parent does not work with Java 21

Issue found here: [https://github.com/apache/flink-connector-kafka/pull/98]

This is already fixed by spotless apply: [https://github.com/diffplug/spotless/pull/1920]

but also requires an upgrade to a later `google-java-format`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-26 17:50:33.0,,,,,,,,,,"0|z1oweo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlClientSSLTest.testGatewayMode failed in AZP,FLINK-35246,13577326,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,26/Apr/24 08:57,03/May/24 15:54,04/Jun/24 20:40,26/Apr/24 13:19,,,,,,,,,1.20.0,,,Build System / CI,,,,0,pull-request-available,,"{code:java}
Apr 26 01:51:10 java.lang.IllegalArgumentException: The given host:port ('localhost/<unresolved>:36112') doesn't contain a valid port
Apr 26 01:51:10 	at org.apache.flink.util.NetUtils.validateHostPortString(NetUtils.java:120)
Apr 26 01:51:10 	at org.apache.flink.util.NetUtils.getCorrectHostnamePort(NetUtils.java:81)
Apr 26 01:51:10 	at org.apache.flink.table.client.cli.CliOptionsParser.parseGatewayAddress(CliOptionsParser.java:325)
Apr 26 01:51:10 	at org.apache.flink.table.client.cli.CliOptionsParser.parseGatewayModeClient(CliOptionsParser.java:296)
Apr 26 01:51:10 	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:207)
Apr 26 01:51:10 	at org.apache.flink.table.client.SqlClientTestBase.runSqlClient(SqlClientTestBase.java:111)
Apr 26 01:51:10 	at org.apache.flink.table.client.SqlClientSSLTest.testGatewayMode(SqlClientSSLTest.java:74)
Apr 26 01:51:10 	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
Apr 26 01:51:10 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Apr 26 01:51:10 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
Apr 26 01:51:10 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
Apr 26 01:51:10 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
Apr 26 01:51:10 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
Apr 26 01:51:10 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)

{code}


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59173&view=logs&j=26b84117-e436-5720-913e-3e280ce55cae&t=77cc7e77-39a0-5007-6d65-4137ac13a471&l=12418",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 03 15:54:13 UTC 2024,,,,,,,,,,"0|z1ow28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 08:58;Weijie Guo;{code:java}
InetSocketAddress.createUnresolved(
                                    SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetAddress(),
                                    SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetPort())
                            .toString()
{code}

The construction of  InetSocketAddress fails on Java 17 because the toString representation is not guaranteed to return something of the form host:port.;;;","26/Apr/24 13:19;Weijie Guo;master via 4e6dbe2d1a225a0d0e48fd0997c1f11317402e42.;;;","03/May/24 15:54;rskraba;* 1.20 Java 17 / Test (module: table) https://github.com/apache/flink/actions/runs/8842083488/job/24280428940#step:10:12462
* 1.20 Java 21 / Test (module: table) https://github.com/apache/flink/actions/runs/8842083488/job/24280416340#step:10:12463;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics for flink-connector-tidb-cdc,FLINK-35245,13577325,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,xieyi,xieyi,xieyi,26/Apr/24 08:50,07/May/24 01:15,04/Jun/24 20:40,07/May/24 01:15,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"As [https://github.com/apache/flink-cdc/issues/985] had been closed, but it has not been resolved.

Create  a new issue to track this issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 01:15:21 UTC 2024,,,,,,,,,,"0|z1ow20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 01:15;jiabaosun;Implemented via cdc-master: fa6e7ea51258dcd90f06036196618224156df367;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Correct the package for flink-connector-tidb-cdc test,FLINK-35244,13577320,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xieyi,xieyi,xieyi,26/Apr/24 08:36,06/May/24 01:41,04/Jun/24 20:40,06/May/24 01:40,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"test case for flink-connector-tidb-cdc should under
*org.apache.flink.cdc.connectors.tidb* package
instead of *org.apache.flink.cdc.connectors*
!image-2024-04-26-16-19-39-297.png!
 
 
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/24 08:19;xieyi;image-2024-04-26-16-19-39-297.png;https://issues.apache.org/jira/secure/attachment/13068466/image-2024-04-26-16-19-39-297.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 06 01:40:58 UTC 2024,,,,,,,,,,"0|z1ow0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/24 01:40;jiabaosun;Resolved via cdc-master: 002b16ed4e155b01374040ff302b7536d9c41245;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Carry pre-schema payload with SchemaChangeEvents,FLINK-35243,13577310,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiqian_yu,xiqian_yu,26/Apr/24 07:38,10/May/24 06:15,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Currently, Flink CDC 3.x SchemaChangeEvent provides no information about the previous schema state before applying changes.

Most pipeline sources can't provide PreSchema info (because it's not recorded in the binlog / oplog / ...), but some pipeline sinks require it to perform validation checks and apply schema change. This ticket suggests adding framework-level support to filling pre-schema payload to any SchemaChangeEvent that requires such info.",,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-26 07:38:54.0,,,,,,,,,,"0|z1ovyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add fine-grained schema evolution strategy,FLINK-35242,13577309,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiqian_yu,xiqian_yu,26/Apr/24 07:31,25/May/24 03:32,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Currently, Flink CDC allows three SE strategy: evolving it, ignoring it, or throwing an exception. However such configuration strategy doesn't cover all user cases and requires want more fine-grained strategy configuration.

This ticket suggests adding one more strategy ""try_evolve"" or ""evolve_when_available"". It's basically like ""evolving"" option, but doesn't throw an exception if such operation fails, which provides more flexibility.

Also, this ticket suggests allowing user to configure per-schema-event strategy, so users could evolve some types of event (like rename column) and reject some dangerous events (like truncate table, remove column).",,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-26 07:31:55.0,,,,,,,,,,"0|z1ovyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SQL FLOOR and CEIL functions with SECOND and MINUTE for TIMESTAMP_TLZ,FLINK-35241,13577243,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,AlexeyLV,AlexeyLV,25/Apr/24 18:01,25/Apr/24 18:02,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,0,,,"We need a fix for both SECOND and MINUTE.

The following query doesn't work:
{code:java}
SELECT
  FLOOR(
      CAST(TIMESTAMP '2024-04-25 17:19:42.654' AS TIMESTAMP_LTZ(3))
  TO MINUTE) {code}
These two queries work:
{code:java}
SELECT
  FLOOR(
      CAST(TIMESTAMP '2024-04-25 17:19:42.654' AS TIMESTAMP_LTZ(3))
  TO HOUR) {code}
 
{code:java}
SELECT
  FLOOR(
      TIMESTAMP '2024-04-25 17:19:42.654'
  TO MINUTE) {code}

Stack trace for the first not working query from above:
{code:java}
Caused by: io.confluent.flink.table.utils.CleanedException: org.codehaus.commons.compiler.CompileException: Line 41, Column 69: No applicable constructor/method found for actual parameters ""org.apache.flink.table.data.TimestampData, org.apache.flink.table.data.TimestampData""; candidates are: ""public static long org.apache.flink.table.runtime.functions.SqlFunctionUtils.floor(long, long)"", ""public static float org.apache.flink.table.runtime.functions.SqlFunctionUtils.floor(float)"", ""public static org.apache.flink.table.data.DecimalData org.apache.flink.table.runtime.functions.SqlFunctionUtils.floor(org.apache.flink.table.data.DecimalData)"", ""public static int org.apache.flink.table.runtime.functions.SqlFunctionUtils.floor(int, int)"", ""public static double org.apache.flink.table.runtime.functions.SqlFunctionUtils.floor(double)""
at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:13080)
at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9646)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9506)
at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9422)
at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5263)
... {code}
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-25 18:01:59.0,,,,,,,,,,"0|z1ovk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable FLUSH_AFTER_WRITE_VALUE to avoid flush per record,FLINK-35240,13577230,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,gongzhongqiang,gongzhongqiang,25/Apr/24 16:26,29/May/24 09:00,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / FileSystem,,,,0,pull-request-available,,"*Reproduce：*
* According to user email: https://lists.apache.org/thread/9j5z8hv4vjkd54dkzqy1ryyvm0l5rxhc
*  !image-2024-04-26-00-23-29-975.png! 


*Analysis：*
* `org.apache.flink.formats.csv.CsvBulkWriter#addElement` will flush per record.

*Solution：*
* I think maybe we can disable `FLUSH_AFTER_WRITE_VALUE` to avoid flush when a record added.

",,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/24 16:23;gongzhongqiang;image-2024-04-26-00-23-29-975.png;https://issues.apache.org/jira/secure/attachment/13068439/image-2024-04-26-00-23-29-975.png","26/Apr/24 15:16;afedulov;image-2024-04-26-17-16-07-925.png;https://issues.apache.org/jira/secure/attachment/13068483/image-2024-04-26-17-16-07-925.png","26/Apr/24 15:16;afedulov;image-2024-04-26-17-16-20-647.png;https://issues.apache.org/jira/secure/attachment/13068484/image-2024-04-26-17-16-20-647.png","26/Apr/24 15:16;afedulov;image-2024-04-26-17-16-30-293.png;https://issues.apache.org/jira/secure/attachment/13068485/image-2024-04-26-17-16-30-293.png","26/Apr/24 06:25;gongzhongqiang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13068465/screenshot-1.png",,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 16:53:02 UTC 2024,,,,,,,,,,"0|z1ovhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 16:35;gongzhongqiang;[~afedulov] Can you give some suggestion? 

Solution:
Disable `_FLUSH_AFTER_WRITE_VALUE_` to avoid flush per record added.;;;","25/Apr/24 19:24;afedulov;[~gongzhongqiang] this is pretty strange, because I remember that I ran into this issue during development and this is exactly why this line is there:
[https://github.com/apache/flink/blob/c0bf0ac3fb1fe4814bff09807ed2040bb13da052/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvBulkWriter.java#L60]

Maybe Jackson version got bumped and something in the internal initialization of the _generatorFactory and the state of the ""parent"" mapper has changed. The pointer to it is derived from the parent mapper, so it should, in theory, not make a difference whether the setting is applied before or after the {{.writer()}} call in the CsvBulkWriter constructor:  [ObjectWriter.java#L105|https://github.com/FasterXML/jackson-databind/blob/jackson-databind-2.14.2/src/main/java/com/fasterxml/jackson/databind/ObjectWriter.java#L105]
Try to move the {{JsonGenerator.Feature#AUTO_CLOSE_TARGET}} configuration above the .writer() call. 

It seems the better way could also be to configure it on the mapper itself, not the generator, i.e.:
{{mapper.configure(JsonGenerator.Feature.AUTO_CLOSE_TARGET, false);}}

I am pretty sure *com.fasterxml.jackson.core.JsonGenerator.Feature#AUTO_CLOSE_TARGET* is the right property that is supposed to control this behavior: [JsonFactory.java#L1474-L1482|https://github.com/FasterXML/jackson-core/blob/2.18/src/main/java/com/fasterxml/jackson/core/JsonFactory.java#L1474-L1482]

 ;;;","26/Apr/24 03:22;robyoung;Hi, I've been looking into this.

1. [FLUSH_AFTER_WRITE_VALUE|https://github.com/FasterXML/jackson-databind/blob/52c74def0e487e1149bdc63783b1086ddcb095b7/src/main/java/com/fasterxml/jackson/databind/SerializationFeature.java#L167] is only applied to a subset of writeValue methods of ObjectMapper and ObjectWriter.
> Feature that determines whether JsonGenerator. flush() is called after writeValue() method that takes JsonGenerator as an argument completes (i. e. does NOT affect methods that use other destinations);
So it only applies if JsonGenerator is an argument to writeValue. The method we're using is passing OutputStream.

2. Disabling AUTO_CLOSE_TARGET prevents the underlying stream being closed during writeValue, which is what we want.

3. There's another feature [FLUSH_PASSED_TO_STREAM|https://github.com/FasterXML/jackson-core/blob/02aba8a36aa4c62e02196ee1d64c027a2d03ecdd/src/main/java/com/fasterxml/jackson/core/JsonGenerator.java#L105] being applied by the Jackson CsvGenerator [here|https://github.com/FasterXML/jackson-dataformats-text/blob/3d3165e58b90618a5fbccf630f1604a383afe78c/csv/src/main/java/com/fasterxml/jackson/dataformat/csv/CsvGenerator.java#L505] which is enabled by default. This causes a flush on every writeValue, when the CsvGenerator is closed. I think this is what's provoking the flush.

Experimentally disabling FLUSH_PASSED_TO_STREAM broke integration tests because Jackson is internally wrapping the stream in it's own Writer class which has it's own buffering. So if you tell Jackson not to flush, the jackson writer isn't flushed, and the CSV bytes are never written to the underlying stream.

One workaround I found is wrapping the stream in an implementation that ignores the flush call and passing that to Jackson. https://github.com/robobario/flink/commit/ae3fdb1ca9de748df791af232bba57d6d7289a79;;;","26/Apr/24 06:41;gongzhongqiang;Welcome [~robyoung] to join this disscussion. And you understand what i want to express.

[~afedulov] [~robyoung] I had try to solove this issue on my local. https://github.com/GOODBOY008/flink/commit/4f78be92b5bdebcf92a1e32736434517ccc6f561

 

 ;;;","26/Apr/24 09:23;afedulov;I don't think touching any of the flush-specific properties should be necessary. You can see in the FlameGraph that flush calls are due to close being called and, as [~robyoung] mentioned, this is what JsonGenerator.Feature#AUTO_CLOSE_TARGET is there for. ;;;","26/Apr/24 13:22;gongzhongqiang;  !screenshot-1.png! 

[~afedulov] The close methed always be invorked, in close methed will read *AUTO_CLOSE_TARGET* to determine whether to close or not . ;;;","26/Apr/24 13:25;gongzhongqiang;Base on the logic of _writer. close，We can disable FLUSH_PASSED_TO_STREAM too. So we can control  flush in CsvBulkWriter. ;;;","26/Apr/24 15:21;afedulov;[~gongzhongqiang] and [~robyoung]  you are right!

Unfortunately the docs of the {{FLUSH_PASSED_TO_STREAM}} are lying about it only being relevant for the {{flush}} method, whereas it indeed controls individual flushes from the {{close()}} method
{code:java}
/**
* Feature that specifies that calls to {@link #flush} will cause
* matching <code>flush()</code> to underlying {@link OutputStream}
* or {@link Writer}; if disabled this will not be done.
* Main reason to disable this feature is to prevent flushing at
* generator level, if it is not possible to prevent method being
* called by other code (like <code>ObjectMapper</code> or third
* party libraries).
*<p>
* Feature is enabled by default.
*/
FLUSH_PASSED_TO_STREAM(true){code};;;","26/Apr/24 16:53;gongzhongqiang;[~afedulov] [~robyoung] I opened a pr to patch this issue. Would like help me review? Thank you~ :);;;",,,,,,,,,,,,,,,,,,,,
1.19 docs show outdated warning,FLINK-35239,13577213,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,uce,uce,uce,25/Apr/24 13:06,26/Apr/24 06:48,04/Jun/24 20:40,26/Apr/24 06:48,1.19.0,,,,,,,,1.19.0,,,Documentation,,,,0,pull-request-available,,"The docs for 1.19 are currently marked as outdated although it's the currently stable release.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/24 13:02;uce;Screenshot 2024-04-25 at 15.01.57.png;https://issues.apache.org/jira/secure/attachment/13068436/Screenshot+2024-04-25+at+15.01.57.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-25 13:06:25.0,,,,,,,,,,"0|z1ovdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZP is not working since some ci agent unhealthy,FLINK-35238,13577186,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,Weijie Guo,Weijie Guo,25/Apr/24 09:33,26/Apr/24 02:11,04/Jun/24 20:40,26/Apr/24 02:11,,,,,,,,,,,,Build System / CI,,,,0,,,"	{code:java}
##[error]We stopped hearing from agent AlibabaCI006-agent03. Verify the agent machine is running and has a healthy network connection. Anything that terminates an agent process, starves it for CPU, or blocks its network access can cause this error. For more information, see: https://go.microsoft.com/fwlink/?linkid=846610
Agent: AlibabaCI006-agent03
Started: Today at 10:30
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59135&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=fc5181b0-e452-5c8f-68de-1097947f6483",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 02:11:20 UTC 2024,,,,,,,,,,"0|z1ov7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 09:36;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59134&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=0da23115-68bb-5dcd-192c-bd4c8adebde1;;;","25/Apr/24 09:36;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59135&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=fc5181b0-e452-5c8f-68de-1097947f6483;;;","26/Apr/24 02:11;Weijie Guo;It looks like this has recovered on its own. It should be a network issue. Close this then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Allow Sink to Choose HashFunction in PrePartitionOperator,FLINK-35237,13577185,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhangdingxin,zhangdingxin,25/Apr/24 09:06,13/May/24 03:04,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,"The {{PrePartitionOperator}} in its current implementation only supports a fixed {{HashFunction}} ({{{}org.apache.flink.cdc.runtime.partitioning.PrePartitionOperator.HashFunction{}}}). This limits the ability of Sink implementations to customize the partitioning logic for {{{}DataChangeEvent{}}}s. For example, in the case of partitioned tables, it would be advantageous to allow hashing based on partition keys, hashing according to table names, or using the database engine's internal primary key hash functions (such as with MaxCompute DataSink).

When users require such custom partitioning logic, they are compelled to implement their PartitionOperator, which undermines the utility of {{{}PrePartitionOperator{}}}.

To address this limitation, it would be highly desirable to enable the {{PrePartitionOperator}} to support user-specified custom {{{}HashFunction{}}}s (Function<DataChangeEvent, Integer>). A possible solution could involve a mechanism analogous to the {{DataSink}} interface, allowing the specification of a {{HashFunctionProvider}} class path in the configuration file. This enhancement would greatly facilitate users in tailoring partition strategies to meet their specific application needs.

In this case, I want to create new class {{HashFunctionProvider}} and {{{}HashFunction{}}}:
{code:java}
public interface HashFunctionProvider {
    HashFunction getHashFunction(Schema schema);
}

public interface HashFunction extends Function<DataChangeEvent, Integer> {
    Integer apply(DataChangeEvent event);
} {code}
add {{getHashFunctionProvider}} method to {{DataSink}}

 
{code:java}
public interface DataSink {

    /** Get the {@link EventSinkProvider} for writing changed data to external systems. */
    EventSinkProvider getEventSinkProvider();

    /** Get the {@link MetadataApplier} for applying metadata changes to external systems. */
    MetadataApplier getMetadataApplier();

    default HashFunctionProvider getHashFunctionProvider() {
        return new DefaultHashFunctionProvider();
    }
} {code}
and re-implement {{PrePartitionOperator}} {{recreateHashFunction}} method.
{code:java}
private HashFunction recreateHashFunction(TableId tableId) {
    return hashFunctionProvider.getHashFunction(loadLatestSchemaFromRegistry(tableId));
} {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 09:37:31 UTC 2024,,,,,,,,,,"0|z1ov7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/24 09:37;zhangdingxin;If the maintainers agree that this improvement is worthwhile, I would be happy to take it on. Please feel free to assign the issue to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.19 Translation error on the execution_mode/order-of-processing,FLINK-35236,13577183,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,maomao,maomao,25/Apr/24 08:52,29/Apr/24 06:16,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,chinese-translation,,,,0,pull-request-available,,"[https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/datastream/execution_mode/#order-of-processing]

!image-2024-04-25-16-53-34-007.png!

!image-2024-04-25-16-53-49-052.png!

应为，常规输入：既不从广播输入也不从 keyed 输入",,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/24 08:53;maomao;image-2024-04-25-16-53-34-007.png;https://issues.apache.org/jira/secure/attachment/13068431/image-2024-04-25-16-53-34-007.png","25/Apr/24 08:53;maomao;image-2024-04-25-16-53-49-052.png;https://issues.apache.org/jira/secure/attachment/13068430/image-2024-04-25-16-53-49-052.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 06:16:05 UTC 2024,,,,,,,,,,"0|z1ov6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 09:40;Weijie Guo;Would you mind filing a PR to fix this?;;;","26/Apr/24 03:13;maomao;#24726

I filing a PR to fix this,again.

Thanks [~Weijie Guo] .;;;","29/Apr/24 01:50;maomao;hi [~Weijie Guo],can you help me to review it.

thanks. ;;;","29/Apr/24 06:16;maomao; [GitHub Pull Request #24726|https://github.com/apache/flink/pull/24726];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Fix missing dependencies in the uber jar,FLINK-35235,13577162,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kunni,kunni,kunni,25/Apr/24 07:18,13/May/24 09:32,04/Jun/24 20:40,26/Apr/24 09:02,cdc-3.1.0,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"Some class of Kafka were not included in fat jar.
!image-2024-04-25-15-17-34-717.png!",,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/24 07:17;kunni;image-2024-04-25-15-17-20-987.png;https://issues.apache.org/jira/secure/attachment/13068427/image-2024-04-25-15-17-20-987.png","25/Apr/24 07:17;kunni;image-2024-04-25-15-17-34-717.png;https://issues.apache.org/jira/secure/attachment/13068426/image-2024-04-25-15-17-34-717.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 09:02:19 UTC 2024,,,,,,,,,,"0|z1ov28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 09:02;jiabaosun;Resolved via

* cdc master: ec643c9dd7365261f3cee620d4d6bd5d042917e0
* cdc release-3.1: b96ea11cc7df6c3d57a155573f29c18bf9d787ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NullPointerException of org.apache.flink.cdc.common.configuration.ConfigurationUtils#convertToString,FLINK-35234,13577151,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,loserwang1024,loserwang1024,25/Apr/24 05:56,08/May/24 02:28,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Exception like this:
{code:java}
Caused by: java.lang.NullPointerException    at org.apache.flink.cdc.common.configuration.ConfigurationUtils.convertToString(ConfigurationUtils.java:133) ~[?:?]    at org.apache.flink.cdc.common.configuration.Configuration.toMap(Configuration.java:138) ~[?:?] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-25 05:56:28.0,,,,,,,,,,"0|z1ouzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase lookup result is wrong when lookup cache is enabled,FLINK-35233,13577140,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,tanjialiang,tanjialiang,25/Apr/24 02:16,29/Apr/24 14:37,04/Jun/24 20:40,,hbase-3.0.0,,,,,,,,,,,Connectors / HBase,,,,0,pull-request-available,,"HBase table
||rowkey||name||age||
|1|ben|18|
|2|ken|19|
|3|mark|20|

 
FlinkSQL lookup join with lookup cahce
{code:java}
CREATE TABLE dim_user (
  rowkey STRING,
  info ROW<name STRING, age STRING>,
  PRIMARY KEY (rowkey) NOT ENFORCED
) WITH (
  'connector' = 'hbase-2.2',
  'zookeeper.quorum' = 'localhost:2181',
  'zookeeper.znode.parent' = '/hbase',
  'table-name' = 'default:test',
  'lookup.cache' = 'PARTIAL',
  'lookup.partial-cache.max-rows' = '1000',
  'lookup.partial-cache.expire-after-write' = '1h'
);

CREATE VIEW user_click AS 
SELECT user_id, proctime() AS proc_time
FROM (
  VALUES('1'), ('2'), ('3'), ('1'), ('2')
) AS t (user_id);

SELECT 
    user_id, 
    info.name, 
    info.age
FROM user_click INNER JOIN dim_user
FOR SYSTEM_TIME AS OF user_click.proc_time
ON dim_user.rowkey = user_click.user_id;{code}
 
Expect Result
||rowkey||name||age||
|1|ben|18|
|2|ken|19|
|3|mark|20|
|1|ben|18|
|2|ken|19|

 

Actual Result
||rowkey||name||age||
|1|ben|18|
|2|ken|19|
|3|mark|20|
|1|mark|20|
|2|mark|20|

 
Wrong result when we lookup user_id 1 and 2 the second time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-25 02:16:16.0,,,,,,,,,,"0|z1ouxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for retry settings on GCS connector,FLINK-35232,13577108,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Oleksandr Nitavskyi,vikasmb,vikasmb,24/Apr/24 19:06,09/May/24 01:21,04/Jun/24 20:40,09/May/24 01:21,,,,,,,,,1.20.0,,,Connectors / FileSystem,,,,0,pull-request-available,,"https://issues.apache.org/jira/browse/FLINK-32877 is tracking ability to specify transport options in GCS connector. While setting the params enabled here reduced read timeouts, we still see 503 errors leading to Flink job restarts.

Thus, in this ticket, we want to specify additional retry settings as noted in [https://cloud.google.com/storage/docs/retry-strategy#customize-retries.|https://cloud.google.com/storage/docs/retry-strategy#customize-retries]

We need [these|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#methods] methods available for Flink users so that they can customize their deployment. In particular next settings seems to be the minimum required to adjust GCS timeout with Job's checkpoint config:
 * [maxAttempts|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getMaxAttempts__]
 * [initialRpcTimeout|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getInitialRpcTimeout__]
 * [rpcTimeoutMultiplier|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getRpcTimeoutMultiplier__]
 * [maxRpcTimeout|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getMaxRpcTimeout__]
 * [totalTimeout|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getTotalTimeout__]

 

Basically the proposal is to be able to tune the timeout via multiplier, maxAttemts + totalTimeout mechanisms.

All of the config options should be optional and the default one should be used in case some of configs are not provided.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-32877,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 09 01:21:25 UTC 2024,,,,,,,,,,"0|z1ouq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 16:04;galenwarren;In case it's relevant, reading/writing to GCS happens in two different ways in Flink.

First, the basic connector wraps a Google-provided Hadoop connector and leverages Flink's support for reading/writing to Hadoop file systems.

Second, the RecoverableWriter support leverages the Google Java library directly – the extra features associated with RecoverableWriter require this.

Just mentioning this, because the linked issue says that Hadoop libraries aren't used, which isn't always the case. I'm not sure which mode you're using the connector, from your description.;;;","25/Apr/24 19:35;vikasmb;Thanks for the reply [~galenwarren].

In this case, we see errors from the `RecoverableWriter support leveraging the Google Java library directly`.

Sample error stack traces we see in our Flink deployment:


h4. Path 1: FileCommitter.commit -> GSRecoverableWriterCommitter.commitAfterRecovery
```
h4. com.google.cloud.storage.StorageException: Read timed out
    at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:233)
    at com.google.cloud.storage.spi.v1.HttpStorageRpc.compose(HttpStorageRpc.java:625)
    at com.google.cloud.storage.StorageImpl.lambda$compose$20(StorageImpl.java:482)
    at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105)
    at com.google.cloud.RetryHelper.run(RetryHelper.java:76)
    at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
    at com.google.cloud.storage.Retrying.run(Retrying.java:51)
    at com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1374)
    at com.google.cloud.storage.StorageImpl.compose(StorageImpl.java:480)
    at org.apache.flink.fs.gs.storage.GSBlobStorageImpl.compose(GSBlobStorageImpl.java:158)
    at org.apache.flink.fs.gs.writer.GSRecoverableWriterCommitter.composeBlobs(GSRecoverableWriterCommitter.java:158)
    at org.apache.flink.fs.gs.writer.GSRecoverableWriterCommitter.writeFinalBlob(GSRecoverableWriterCommitter.java:189)
    at org.apache.flink.fs.gs.writer.GSRecoverableWriterCommitter.commitAfterRecovery(GSRecoverableWriterCommitter.java:109)
    at org.apache.flink.streaming.api.functions.sink.filesystem.OutputStreamBasedPartFileWriter$OutputStreamBasedPendingFile.commitAfterRecovery(OutputStreamBasedPartFileWriter.java:350)
    at org.apache.flink.connector.file.sink.committer.FileCommitter.commit(FileCommitter.java:62)
```

Path 2: FileWriter.write
```
com.google.cloud.storage.StorageException: Read timed out
    at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:233)
    at com.google.cloud.storage.spi.v1.HttpStorageRpc.open(HttpStorageRpc.java:949)
    at com.google.cloud.storage.ResumableMedia.lambda$null$0(ResumableMedia.java:37)
    at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105)
    at com.google.cloud.RetryHelper.run(RetryHelper.java:76)
    at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50)
    at com.google.cloud.storage.Retrying.run(Retrying.java:51)
    at com.google.cloud.storage.ResumableMedia.lambda$startUploadForBlobInfo$1(ResumableMedia.java:34)
    at com.google.cloud.storage.BlobWriteChannel$Builder.build(BlobWriteChannel.java:315)
    at com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:577)
    at com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:547)
    at com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:95)
    at org.apache.flink.fs.gs.storage.GSBlobStorageImpl.writeBlob(GSBlobStorageImpl.java:64)
    at org.apache.flink.fs.gs.writer.GSRecoverableFsDataOutputStream.createWriteChannel(GSRecoverableFsDataOutputStream.java:229)
    at org.apache.flink.fs.gs.writer.GSRecoverableFsDataOutputStream.write(GSRecoverableFsDataOutputStream.java:152)
    at org.apache.flink.formats.parquet.PositionOutputStreamAdapter.write(PositionOutputStreamAdapter.java:58)

```

Similar to path1, we do see these errors in `FileWriter.prepareCommit` as well..;;;","25/Apr/24 19:43;galenwarren;Yes, that's using the RecoverableWriter.

The default retry settings are here: [Retry strategy  |  Cloud Storage  |  Google Cloud|https://cloud.google.com/storage/docs/retry-strategy#java], so it should be retrying 6 times for up to 50 seconds I believe, as is. Which settings would you be looking to implement? And you're sure nothing else is going on that's causing the failures (network connectivity, etc.)?

Full disclosure, I worked on RecoverableWriter but I'm not a committer, so I can't really ok anything on my own. But being able to adjust these settings seems reasonable to me.;;;","26/Apr/24 15:12;Oleksandr Nitavskyi;[~galenwarren] thanks. We have reduced the amount of methods to the bare minimum. Reflected in the description: 
 * [maxAttempts|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getMaxAttempts__]
 * [initialRpcTimeout|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getInitialRpcTimeout__]
 * [rpcTimeoutMultiplier|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getRpcTimeoutMultiplier__]
 * [maxRpcTimeout|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getMaxRpcTimeout__]
 * [totalTimeout|https://cloud.google.com/java/docs/reference/gax/latest/com.google.api.gax.retrying.RetrySettings#com_google_api_gax_retrying_RetrySettings_getTotalTimeout__]

Thus Flink user will be able to adjust the total timeout time to the checkpoint timeout, so job does it best before it gave up to commit the data.;;;","29/Apr/24 14:11;galenwarren;[~xtsong] You helped with the original implementation of the GCS RecoverableWriter implementation. This proposal – to allow people to adjust the retry settings – makes sense to me, but I don't really know what guidance to give the people proposing it as far as potential next steps might be. How would something like this potentially move forward? Thanks.;;;","06/May/24 02:52;xtsong;I think we can just move forward with this Jira ticket.

I noticed there's already a PR opened. I can help review and merge it. And it would be helpful if [~galenwarren] can also take a look at it. Not only committers but everyone can help review PRs. Committers are only needed for the final merge.

BTW, the ticket is currently assigned to [~singhravidutt] , who is neither the reporter of the ticket nor the author of the PR. Anyone knows the context?;;;","06/May/24 03:44;vikasmb;> BTW, the ticket is currently assigned to [~singhravidutt] , who is neither the reporter of the ticket nor the author of the PR. Anyone knows the context?

Regarding above, it occurred as I cloned https://issues.apache.org/jira/browse/FLINK-32877 to create this ticket and it did not let me change the assignee of this ticket once it got created. Please feel free to change the assignee as appropriate, thanks.;;;","06/May/24 04:03;xtsong;Thanks for the information. I'm assigning this to [~Oleksandr Nitavskyi] who provided the PR.;;;","09/May/24 01:21;xtsong;master (1.20): f2c0c3ddcdd78c1e2876087139e56534fe3f8421;;;",,,,,,,,,,,,,,,,,,,,
Tests rely on listJobsFunction in TestingClusterClient are broken,FLINK-35231,13577092,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aplyusnin,aplyusnin,24/Apr/24 16:33,24/Apr/24 16:47,04/Jun/24 20:40,,kubernetes-operator-1.9.0,,,,,,,,,,,Kubernetes Operator,,,,0,,,"Supplier listJobsFunction is never called in TestingClusterClient therefore some assertions are never called.

Affected tests: 
 * FlinkSessionJobObserverTest#testObserveWithEffectiveConfig
 * FlinkDeploymentControllerTest#verifyReconcileWithBadConfig
 * FlinkSessionJobControllerTest#verifyReconcileWithBadConfig

Reason:

When listJobs() requested TestingClusterClient calls TestingFlinkClient#getMultipleJobsDetails instead of listJobsFunction",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-24 16:33:43.0,,,,,,,,,,"0|z1oumo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split FlinkSqlParserImplTest to reduce the code lines.,FLINK-35230,13577078,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hackergin,hackergin,24/Apr/24 14:36,25/Apr/24 01:38,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,0,,,"With the increasing extension of Calcite syntax, the current FlinkSqlParserImplTest has reached nearly 3000 lines of code.

If it exceeds the current limit, it will result in errors in the code style check.
{code:java}
Unable to find source-code formatter for language: log. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yaml08:33:19.679 [ERROR] src/test/java/org/apache/flink/sql/parser/FlinkSqlParserImplTest.java:[1] (sizes) FileLength: File length is 3,166 lines (max allowed is 3,100).
{code}
To facilitate future syntax extends, I suggest that we split FlinkSqlParserImplTest and place the same type of syntax in separate Java tests for the convenience of avoiding the continuous growth of the original test class.

My current idea is: 
Since *FlinkSqlParserImplTest* currently inherits {*}SqlParserTest{*}, and *SqlParserTest* itself contains many unit tests, for the convenience of future test splits, we should introduce a basic *ParserTestBase* inheriting {*}SqlParserTest{*}, and disable the original related unit tests in {*}SqlParserTest{*}.

This will facilitate writing relevant unit tests more quickly during subsequent splitting, without the need to repeatedly execute the unit tests inside SqlParserTest.

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59113&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 01:38:59 UTC 2024,,,,,,,,,,"0|z1oujk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 01:35;lsy;Good point, +1.;;;","25/Apr/24 01:38;xu_shuai_;[~lsy]  I'd like to take this, would you assign it to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
join An error occurred when the table was empty,FLINK-35229,13577044,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leexu,leexu,24/Apr/24 10:12,03/Jun/24 07:40,04/Jun/24 20:40,,1.17.2,1.18.0,1.19.0,,,,,,1.17.3,1.18.1,1.19.2,Table SQL / API,,,,0,,,"{code:java}
//代码占位符
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setRuntimeMode(RuntimeExecutionMode.BATCH).setParallelism(1);
StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env);

Table ticker = tableEnvironment.fromValues(
        DataTypes.ROW(
                DataTypes.FIELD(""symbol"", DataTypes.STRING()),
                DataTypes.FIELD(""price"", DataTypes.BIGINT())
        ),
        row(""A"", 12L),
        row(""B"", 17L)
);
tableEnvironment.createTemporaryView(""ticker_t"", ticker);

Table ticker1 = tableEnvironment.fromValues(
        DataTypes.ROW(
                DataTypes.FIELD(""symbol"", DataTypes.STRING()),
                DataTypes.FIELD(""price"", DataTypes.BIGINT())
        )
);
tableEnvironment.createTemporaryView(""ticker_y"", ticker1);

Table ticker2 = tableEnvironment.fromValues(
        DataTypes.ROW(
                DataTypes.FIELD(""symbol"", DataTypes.STRING()),
                DataTypes.FIELD(""price"", DataTypes.BIGINT())
        ),
        row(""A"", 12L),
        row(""B"", 17L)
);
tableEnvironment.createTemporaryView(""ticker_z"", ticker2);

tableEnvironment.sqlQuery(""select coalesce(t.symbol, y.symbol, z.symbol) as symbol, "" +
                "" t.price as price_t, y.price as price_y,  z.price as price_z "" +
                ""from ticker_t t FULL OUTER JOIN ticker_y y ON t.symbol = y.symbol "" +
                ""FULL OUTER JOIN ticker_z z ON y.symbol = z.symbol"")
        .execute().print(); {code}
+----+--------------------------------+----------------------+----------------------+----------------------+
| op |                         symbol |              price_t |              price_y |              price_z |
+----+--------------------------------+----------------------+----------------------+----------------------+
| +I |                              A |                   12 |               <NULL> |               <NULL> |
| +I |                              B |                   17 |               <NULL> |               <NULL> |
| +I |                              A |               <NULL> |               <NULL> |                   12 |
| +I |                              B |               <NULL> |               <NULL> |                   17 |
+----+--------------------------------+----------------------+----------------------+----------------------+",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-24 10:12:12.0,,,,,,,,,,"0|z1ouc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicKafkaSource does not read re-added topic for the same cluster,FLINK-35228,13577036,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ignasd,ignasd,ignasd,24/Apr/24 08:43,30/Apr/24 02:26,04/Jun/24 20:40,30/Apr/24 02:26,kafka-3.1.0,,,,,,,,kafka-3.3.0,,,Connectors / Kafka,,,,0,pull-request-available,,"When using DynamicKafkaSource, if topic is removed from the cluster (that has more active topics remaining) and then re-added back, consumption from that topic won't be happening.

However, if the topic in question is the only topic in that cluster, then everything works as expected - consumption restarts once cluster-topic is re-added.

Steps to reproduce:
 # Have a DynamicKafkaSource.
 # Have KafkaMetadataService report a single cluster with two topics (A and B) for the subscribed stream/streams.
 # Consume some data, topics A and B are consumed as expected.
 # Have KafkaMetadataService remove topic A.
 # Continue consuming data, only topic B consumed as expected.
 # Have KafkaMetadataService re-add topic A.
 # Continue consuming data, however only topic B is actually consumed - this is not expected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 02:26:13 UTC 2024,,,,,,,,,,"0|z1oua8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/24 02:26;masc;CI passes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove execution-mode in ExecutionConfigInfo,FLINK-35227,13577028,13483948,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,24/Apr/24 07:55,24/Apr/24 07:55,04/Jun/24 20:40,,,,,,,,,,2.0.0,,,Runtime / REST,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-24 07:55:23.0,,,,,,,,,,"0|z1ou8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate execution-mode in ExecutionConfigInfo related rest api,FLINK-35226,13577027,13483948,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,24/Apr/24 07:54,16/May/24 10:08,04/Jun/24 20:40,16/May/24 10:08,,,,,,,,,1.20.0,,,Runtime / REST,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 10:08:12 UTC 2024,,,,,,,,,,"0|z1ou88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 10:08;fanrui;merged to master(1.20.0) via: 0c28e1ca7ea1548536d4a5849f57a40c961ee1cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Execution mode in Flink WebUI,FLINK-35225,13577026,13483948,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,24/Apr/24 07:54,16/May/24 10:08,04/Jun/24 20:40,16/May/24 10:08,,,,,,,,,1.20.0,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 10:08:35 UTC 2024,,,,,,,,,,"0|z1ou80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 10:08;fanrui;merged to master(1.20.0) via: 48c14fe5d2b1a9263c1c59d713c6833cc919fd24;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show the JobType on Flink WebUI,FLINK-35224,13577025,13483948,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,24/Apr/24 07:53,29/Apr/24 06:08,04/Jun/24 20:40,29/Apr/24 06:08,,,,,,,,,1.20.0,,,Runtime / Web Frontend,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 06:07:58 UTC 2024,,,,,,,,,,"0|z1ou7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 06:07;fanrui;Merged to master(1.20.0) via: 86de622c807413f2a7a1664113e39798f0fed81d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add jobType in JobDetailsInfo related rest api,FLINK-35223,13577024,13483948,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,24/Apr/24 07:53,29/Apr/24 06:07,04/Jun/24 20:40,29/Apr/24 06:07,,,,,,,,,1.20.0,,,Runtime / REST,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 06:07:37 UTC 2024,,,,,,,,,,"0|z1ou7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 06:07;fanrui;Merged to master(1.20.0) via: 917352ed35eff562a032a6e3038aaaf1177d2a8a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding getJobType for AccessExecutionGraph,FLINK-35222,13577023,13483948,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,24/Apr/24 07:53,29/Apr/24 06:07,04/Jun/24 20:40,29/Apr/24 06:07,,,,,,,,,1.20.0,,,Runtime / Web Frontend,,,,0,pull-request-available,,"Adding getJobType for AccessExecutionGraph interface, and all implementations need to overrite it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 06:07:15 UTC 2024,,,,,,,,,,"0|z1ou7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 06:07;fanrui;Merged to master(1.20.0) via: 5263f9cbf20536e9a81a0044f22b033e78a908ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SQL 2011 reserved keywords as identifiers in Flink HiveParser ,FLINK-35221,13577010,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,Wencong Liu,Wencong Liu,24/Apr/24 05:59,24/May/24 01:36,04/Jun/24 20:40,24/May/24 01:36,1.20.0,,,,,,,,hive-3.0.0,,,Connectors / Hive,,,,0,pull-request-available,,"According to Hive user documentation[1], starting from version 0.13.0, Hive prohibits the use of reserved keywords as identifiers. Moreover, versions 2.1.0 and earlier allow using SQL11 reserved keywords as identifiers by setting {{hive.support.sql11.reserved.keywords=false}} in hive-site.xml. This compatibility feature facilitates jobs that utilize keywords as identifiers.

HiveParser in Flink, relying on Hive version 2.3.9, lacks the option to treat SQL11 reserved keywords as identifiers. This poses a challenge for users migrating SQL from Hive 1.x to Flink SQL, as they might encounter scenarios where keywords are used as identifiers. Addressing this issue is necessary to support such cases.

[1] [LanguageManual DDL - Apache Hive - Apache Software Foundation|https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 01:35:44 UTC 2024,,,,,,,,,,"0|z1ou4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 01:35;luoyuxia;master: bb9510914f7f2bfe3c19b40a6aaf658879dbd4c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
the error of mysql cdc ,FLINK-35220,13577001,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,xiaotouming,xiaotouming,24/Apr/24 03:14,25/Apr/24 05:52,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"When we listen for incremental data on a table,there is a error occur once。

the log is:

2024-04-15 04:52:36
java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:261)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:131)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:157)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:419)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: com.ververica.cdc.connectors.shaded.org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped.
    at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:50)
    at com.ververica.cdc.connectors.mysql.debezium.task.context.MySqlErrorHandler.setProducerThrowable(MySqlErrorHandler.java:85)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$ReaderThreadLifecycleListener.onCommunicationFailure(MySqlStreamingChangeEventSource.java:1544)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1079)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631)
    at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932)
    ... 1 more
Caused by: io.debezium.DebeziumException: Failed to deserialize data of EventHeaderV4\{timestamp=1713126350000, eventType=EXT_UPDATE_ROWS, serverId=1655776775, headerLength=19, dataLength=3291, nextPosition=647993741, flags=0}
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.wrap(MySqlStreamingChangeEventSource.java:1488)
    ... 5 more
Caused by: com.github.shyiko.mysql.binlog.event.deserialization.EventDataDeserializationException: Failed to deserialize data of EventHeaderV4\{timestamp=1713126350000, eventType=EXT_UPDATE_ROWS, serverId=1655776775, headerLength=19, dataLength=3291, nextPosition=647993741, flags=0}
    at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:341)
    at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.nextEvent(EventDeserializer.java:244)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$1.nextEvent(MySqlStreamingChangeEventSource.java:259)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1051)
    ... 3 more
Caused by: java.io.EOFException: Failed to read remaining 1 of 5 bytes from position 15847639. Block length: 173. Initial block length: 3287.
    at com.github.shyiko.mysql.binlog.io.ByteArrayInputStream.fill(ByteArrayInputStream.java:115)
    at com.github.shyiko.mysql.binlog.io.ByteArrayInputStream.read(ByteArrayInputStream.java:105)
    at io.debezium.connector.mysql.RowDeserializers.deserializeDatetimeV2(RowDeserializers.java:407)
    at io.debezium.connector.mysql.RowDeserializers$UpdateRowsDeserializer.deserializeDatetimeV2(RowDeserializers.java:145)
    at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeCell(AbstractRowsEventDataDeserializer.java:183)
    at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeRow(AbstractRowsEventDataDeserializer.java:143)
    at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserializeRows(UpdateRowsEventDataDeserializer.java:72)
    at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserialize(UpdateRowsEventDataDeserializer.java:58)
    at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserialize(UpdateRowsEventDataDeserializer.java:33)
    at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:335)
    ... 6 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Thu Apr 25 05:52:48 UTC 2024,,,,,,,,,,"0|z1ou2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 05:52;loserwang1024;See https://nightlies.apache.org/flink/flink-cdc-docs-master/docs/faq/faq/#q13-the-job-reports-an-error-eventdatadeserializationexception-failed-to-deserialize-data-of-eventheaderv4--caused-by-javanetsocketexception-connection-resetwhat-should-i-do;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink SQL]Support deserialize json string into Map,FLINK-35219,13576997,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,izhangzhihao,izhangzhihao,24/Apr/24 02:25,24/Apr/24 02:25,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,0,,,"like Spark's `from_json` sql function:https://spark.apache.org/docs/latest/api/sql/index.html#from_json
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-24 02:25:10.0,,,,,,,,,,"0|z1ou1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicated values caused by expired state TTL   ,FLINK-35218,13576924,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,j.garcia,j.garcia,23/Apr/24 13:37,24/Apr/24 14:39,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,"Hi,

We utilize the state TTL to clean our Flink input tables through the `table.exec.state.ttl` configuration. 
However, we encountered an issue when the TTL expires, as illustrated in our scenario:

Given this input_table
{code:java}
{
  ""$schema"": ""http://json-schema.org/draft-07/schema"",
  ""$id"": ""http://example.com/example.json"",
  ""type"": ""object"",
  ""title"": ""bet placed schema"",
  ""required"": [
    ""placement_date""
  ],
  ""properties"": {
    ""bet_id"": {
      ""$id"": ""#/properties/bet_id"",
      ""type"": ""string""
    },
    ""regulator"": {
      ""$id"": ""#/properties/regulator"",
      ""type"": ""string""
    },
    ""match_id"": {
      ""$id"": ""#/properties/match_id"",
      ""type"": ""integer""
    },
    ""combo_id"": {
      ""$id"": ""#/properties/combo_id"",
      ""type"": ""integer""
    },
    ""is_live"": {
      ""$id"": ""#/properties/is_live"",
      ""type"": ""boolean""
    },
    ""offer_catalog"": {
      ""$id"": ""#/properties/offer_catalog"",
      ""type"": ""string""
    },
    ""combo_selection_nbr"": {
      ""$id"": ""#/properties/combo_selection_nbr"",
      ""type"": ""integer""
    }
  },
  ""additionalProperties"": true
} {code}
This configuration: 
{code:java}
import org.apache.flink.streaming.api.environment.{StreamExecutionEnvironment => JavaStreamExecutionEnvironment}
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}val streamEnv = new StreamExecutionEnvironment(JavaStreamExecutionEnvironment.getExecutionEnvironment(conf))val tableEnv = StreamTableEnvironment.create(env)
   tableEnv.getConfig.getConfiguration
     .setString(""table.local-time-zone"", ""UTC"")
   tableEnv.getConfig.getConfiguration
     .setString(""table.exec.mini-batch.enabled"", ""true"")
   tableEnv.getConfig.getConfiguration
     .setString(""table.exec.mini-batch.allow-latency"", ""5 s"")
   tableEnv.getConfig.getConfiguration
     .setString(""table.exec.mini-batch.size"", ""5000"")
   tableEnv.getConfig.getConfiguration
     .setString(""table.exec.state.ttl"", TimeUnit.MILLISECONDS.convert(1, TimeUnit.MINUTES).toString) {code}
And this query (simplified query): 
{code:java}
WITH exploded_combos AS (
    select
        event_timestamp,
        CAST(JSON_VALUE(combos.combo, '$.match_id') AS BIGINT) as match_id,
        CAST(
            JSON_VALUE(combos.combo, '$.combo_selection_id') AS BIGINT
        ) as combo_id,
        CAST(JSON_VALUE(combos.combo, '$.is_live') AS BOOLEAN) as is_live,
        CAST(RegulatorToCatalog(regulator) AS VARCHAR) as offer_catalog,
        CARDINALITY(
            JsonToArray(JSON_QUERY(combos.combo, '$.bet_selections'))
        ) as combo_selections_nbr,
        combo_bet_selections
    from
        bet_placed_view
        CROSS JOIN UNNEST(JsonToArray(combo_bet_selections)) AS combos(combo)
),
agg_match AS (
    SELECT
        match_id,
        LOWER(offer_catalog) as offer_catalog,
        MAX(event_timestamp) AS last_event_time_utc,
        COUNT(*) AS bet_count
    FROM
        exploded_combos
    WHERE
        match_id IS NOT NULL
        AND combo_id IS NOT NULL
        AND offer_catalog IS NOT NULL
        AND combo_bet_selections IS NOT NULL
    GROUP BY
        match_id,
        LOWER(offer_catalog)
),
agg_combo AS (
    SELECT
        match_id,
        combo_id,
        combo_selections_nbr,
        is_live,
        LOWER(offer_catalog) AS offer_catalog,
        MAX(event_timestamp) AS last_event_time_utc,
        COUNT(*) as bet_count
    FROM
        exploded_combos
    WHERE
        match_id IS NOT NULL
        AND combo_id IS NOT NULL
        AND (
            combo_selections_nbr = 3
            OR combo_selections_nbr = 2
        )
        AND offer_catalog IS NOT NULL
    GROUP BY
        match_id,
        combo_id,
        combo_selections_nbr,
        is_live,
        LOWER(offer_catalog)
),
ranked_filtered_agg_combo_main_page AS (
    SELECT
        match_id,
        combo_id,
        offer_catalog,
        bet_count,
        ROW_NUMBER() OVER (
            PARTITION BY match_id,
            offer_catalog
            ORDER BY
                bet_count DESC,
                combo_id DESC
        ) AS rank_combo
    FROM
        agg_combo
    WHERE
        combo_selections_nbr = 3
),
joined_filtered_agg_match_main_page AS (
    SELECT
        ranked_filtered_agg_combo_main_page.match_id,
        ranked_filtered_agg_combo_main_page.offer_catalog,
        ranked_filtered_agg_combo_main_page.bet_count,
        ranked_filtered_agg_combo_main_page.combo_id,
        ROW_NUMBER() OVER (
            PARTITION BY agg_match.offer_catalog
            ORDER BY
                agg_match.bet_count DESC,
                agg_match.match_id DESC
        ) AS rank_match
    FROM
        agg_match
        INNER JOIN ranked_filtered_agg_combo_main_page ON ranked_filtered_agg_combo_main_page.match_id = agg_match.match_id
        AND ranked_filtered_agg_combo_main_page.offer_catalog = agg_match.offer_catalog
    WHERE
        ranked_filtered_agg_combo_main_page.rank_combo = 1
)
SELECT
    partition_key,
    match_id,
    offer_catalog,
    false as live,
    LAST_VALUE(last_event_utc) AS last_event_utc,
    LAST_VALUE(last_event_utc) AS max_last_event_utc,
    LAST_VALUE(top) AS topFROM (
        SELECT
            '<all>' as match_id,
            offer_catalog,
            '<all>' || '#openmatch#' || offer_catalog as partition_key,
            CAST('MAX(very_last_event_time_utc)' AS VARCHAR) AS last_event_utc,
            '[' || LISTAGG(
                '{""match_id"":' || CAST(match_id AS VARCHAR) || ',""rank"":' || CAST(rank_match AS VARCHAR) || ',""count"":' || CAST(bet_count AS VARCHAR) || ',""combo_id"":""' || CAST(combo_id AS VARCHAR) || '"",""offer_catalog"":""' || CAST(offer_catalog AS VARCHAR) || '"",""live"": ' || '}'
            ) || ']' AS top
        FROM
            joined_filtered_agg_match_main_page
        WHERE
            rank_match <= 5
        GROUP BY
            offer_catalog
    )
GROUP BY
    partition_key,
    match_id,
    offer_catalog{code}
As you can see in the result below, when the TTL is reached, we have duplicate values in our output.

 

 
{code:java}
+I[<all>#openmatch#fr, <all>, fr,
    false, MAX(very_last_event_time_utc), MAX(very_last_event_time_utc),
    [
        {
            ""match_id"": 2,
            ""rank"": 1,
            ""count"": 2,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 5,
            ""rank"": 2,
            ""count"": 1,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        }
    ]
]
+U[<all>#openmatch#fr, <all>, fr,
    false, MAX(very_last_event_time_utc), MAX(very_last_event_time_utc),
    [
        {
            ""match_id"": 5,
            ""rank"": 2,
            ""count"": 1,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 2,
            ""rank"": 1,
            ""count"": 3,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 3,
            ""rank"": 3,
            ""count"": 2,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 1,
            ""rank"": 4,
            ""count"": 3,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 4,
            ""rank"": 5,
            ""count"": 1,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        }
    ]
]

...
// DUPLICATED VALUES (multiple rank) : 
+U[<all>#openmatch#fr, <all>, fr,
    false, MAX(very_last_event_time_utc), MAX(very_last_event_time_utc),
    [
        {
            ""match_id"": 3,
            ""rank"": 1,
            ""count"": 16,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 5,
            ""rank"": 3,
            ""count"": 14,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 2,
            ""rank"": 2,
            ""count"": 15,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 4,
            ""rank"": 4,
            ""count"": 14,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 1,
            ""rank"": 5,
            ""count"": 12,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 2,
            ""rank"": 2,
            ""count"": 15,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 1,
            ""rank"": 5,
            ""count"": 12,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 5,
            ""rank"": 3,
            ""count"": 16,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 3,
            ""rank"": 1,
            ""count"": 18,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        },
        {
            ""match_id"": 4,
            ""rank"": 4,
            ""count"": 15,
            ""combo_id"": ""123"",
            ""offer_catalog"": ""fr"",
            ""live"":
        }
    ]
]
... {code}
 

*Is this a bug or did we misunderstand how to implement the ttl?*

Flink is also sending to this warning: 

 

!image-2024-04-23-15-34-32-860.png!

 

We actually found 2 workaround: 
 * set a verry large ttl (1year) This will eliminate most of the problems thanks to our business logic
 * Use sliding window (but if we do that we need window of 50days sliding each minute)

We're afraid that these two solutions will be very expensive

Can you tell us if this is a normal behavior or if we miss understand something about the TTL?

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/24 13:34;j.garcia;image-2024-04-23-15-34-32-860.png;https://issues.apache.org/jira/secure/attachment/13068383/image-2024-04-23-15-34-32-860.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 14:39:42 UTC 2024,,,,,,,,,,"0|z1otlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 14:11;martijnvisser;From what the docs state https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/config/

{quote}and will be cleared at some time after it was idle.{quote} 

I would expect (like all TTLs in Flink) that they aren't immediately triggered, but lazily. ;;;","24/Apr/24 14:39;j.garcia;Hello [~martijnvisser],

Thank you for your prompt reply.

Could the observed duplicate ranks in our ROW_NUMBER results be justified by the fact that the state TTL is triggered lazily? If I understand correctly, does this mean we are seeing both the current rank results and the previous ones, which will eventually be purged?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing fsync in FileSystemCheckpointStorage,FLINK-35217,13576920,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,srichter,planet9,planet9,23/Apr/24 13:28,01/May/24 08:42,04/Jun/24 20:40,30/Apr/24 11:37,1.17.0,1.18.0,1.19.0,,,,,,1.18.2,1.19.1,1.20.0,FileSystems,Runtime / Checkpointing,,,0,pull-request-available,,"While running Flink on a system with unstable power supply checkpoints were regularly corrupted in the form of ""_metadata"" files with a file size of 0 bytes. In all cases the previous checkpoint data had already been deleted, causing progress to be lost completely.

Further investigation revealed that the ""FileSystemCheckpointStorage"" doesn't perform ""fsync"" when writing a new checkpoint to disk. This means the old checkpoint gets removed without making sure that the new one is durably persisted on disk. ""strace"" on the jobmanager's process confirms this behavior:
 # The checkpoint chk-60's in-progress metadata is written at ""openat""
 # The checkpoint chk-60's in-progress metadata is atomically renamed at ""rename""
 # The old checkpoint chk-59 is deleted at ""unlink""

For durable persistence an ""fsync"" call is missing before step 3.
Full ""strace"" log:
{code:java}
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60"", 0x7fd2ad5fc970) = -1 ENOENT (No such file or directory)
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60"", 0x7fd2ad5fca00) = -1 ENOENT (No such file or directory)
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc"", {st_mode=S_IFDIR|0755, st_size=42, ...}) = 0
[pid 51618] 11:44:30 mkdir(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60"", 0777) = 0
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60/_metadata"", 0x7fd2ad5fc860) = -1 ENOENT (No such file or directory)
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60/_metadata"", 0x7fd2ad5fc740) = -1 ENOENT (No such file or directory)
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60/._metadata.inprogress.bf9518dc-2100-4524-9e67-e42913c2b8e8"", 0x7fd2ad5fc7d0) = -1 ENOENT (No such file or directory)
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 51618] 11:44:30 openat(AT_FDCWD, ""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60/._metadata.inprogress.bf9518dc-2100-4524-9e67-e42913c2b8e8"", O_WRONLY|O_CREAT|O_EXCL, 0666) = 168
[pid 51618] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60/._metadata.inprogress.bf9518dc-2100-4524-9e67-e42913c2b8e8"", {st_mode=S_IFREG|0644, st_size=23378, ...}) = 0
[pid 51618] 11:44:30 rename(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60/._metadata.inprogress.bf9518dc-2100-4524-9e67-e42913c2b8e8"", ""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-60/_metadata"") = 0
[pid 51644] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59/_metadata"", {st_mode=S_IFREG|0644, st_size=23378, ...}) = 0
[pid 51644] 11:44:30 unlink(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59/_metadata"") = 0
[pid 51644] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 51644] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 51644] 11:44:30 openat(AT_FDCWD, ""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59"", O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 168
[pid 51644] 11:44:30 newfstatat(168, """", {st_mode=S_IFDIR|0755, st_size=0, ...}, AT_EMPTY_PATH) = 0
[pid 51644] 11:44:30 stat(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 51644] 11:44:30 openat(AT_FDCWD, ""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59"", O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 168
[pid 51644] 11:44:30 newfstatat(168, """", {st_mode=S_IFDIR|0755, st_size=0, ...}, AT_EMPTY_PATH) = 0
[pid 51644] 11:44:30 unlink(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59"") = -1 EISDIR (Is a directory)
[pid 51644] 11:44:30 rmdir(""/opt/flink/statestore/e1c541c4568515e77df32d82727e20dc/chk-59"") = 0 {code}
To fix this I'm currently testing the following commit: [https://github.com/Planet-X/flink/commit/24196cc897533b654f44e2b612543ff023cdb123]

""strace"" can confirm that ""fsync"" is now called before the previous checkpoint is removed at ""unlink"":
{code:java}
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50"",  <unfinished ...>
[pid 40393] 11:30:17 <... stat resumed>0x7fc887efc970) = -1 ENOENT (No such file or directory)
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50"", 0x7fc887efca00) = -1 ENOENT (No such file or directory)
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e"", {st_mode=S_IFDIR|0755, st_size=42, ...}) = 0
[pid 40393] 11:30:17 mkdir(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50"", 0777) = 0
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50/_metadata"", 0x7fc887efc870) = -1 ENOENT (No such file or directory)
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50/_metadata"", 0x7fc887efc750) = -1 ENOENT (No such file or directory)
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50/._metadata.inprogress.24b0ea02-a05c-4297-89ff-08340e8cfa90"", 0x7fc887efc7e0) = -1 ENOENT (No such file or directory)
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 40393] 11:30:17 openat(AT_FDCWD, ""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50/._metadata.inprogress.24b0ea02-a05c-4297-89ff-08340e8cfa90"", O_WRONLY|O_CREAT|O_EXCL, 0666) = 194
[pid 40393] 11:30:17 fsync(194)         = 0
[pid 40393] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50/._metadata.inprogress.24b0ea02-a05c-4297-89ff-08340e8cfa90"", {st_mode=S_IFREG|0644, st_size=23366, ...}) = 0
[pid 40393] 11:30:17 rename(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50/._metadata.inprogress.24b0ea02-a05c-4297-89ff-08340e8cfa90"", ""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-50/_metadata"") = 0
[pid 39230] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49/_metadata"", {st_mode=S_IFREG|0644, st_size=23366, ...}) = 0
[pid 39230] 11:30:17 unlink(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49/_metadata"") = 0
[pid 39230] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 39230] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 39230] 11:30:17 openat(AT_FDCWD, ""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49"", O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 194
[pid 39230] 11:30:17 newfstatat(194, """", {st_mode=S_IFDIR|0755, st_size=0, ...}, AT_EMPTY_PATH) = 0
[pid 39230] 11:30:17 stat(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
[pid 39230] 11:30:17 openat(AT_FDCWD, ""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49"", O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 194
[pid 39230] 11:30:17 newfstatat(194, """", {st_mode=S_IFDIR|0755, st_size=0, ...}, AT_EMPTY_PATH) = 0
[pid 39230] 11:30:17 unlink(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49"") = -1 EISDIR (Is a directory)
[pid 39230] 11:30:17 rmdir(""/opt/flink/statestore/28be342d7d6b7cfd8883799cab99576e/chk-49"") = 0 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 01 08:42:57 UTC 2024,,,,,,,,,,"0|z1otkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 14:27;srichter;Hi, the code is calling close on the output stream which usually implies that it's flushed and synced. I'm wondering if this is a OS or Java version specific problem?;;;","23/Apr/24 16:38;planet9;Hey there, thanks for the quick response!

I've checked the docs and it doesn't seem like the OutputStream gives any guarantees on data persistence when calling close(). The doc only states that close() frees resources. For java 17 this can be found e.g. here: [https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/io/OutputStream.html#close()]

It may be possible that the actual behavior changed with the recent switch to java 17, however the documentation states the same for java 11 and 17. All things considered I'm quite sure we cannot safely rely on OutputStream's close() method for data durability as there don't seem to be any guarantees.

 

The flink job was deployed using podman with this exact base image from docker hub:
{code:java}
flink:1.19.0-scala_2.12-java17@sha256:4135661d32caae437ba1a6d328e95910800c640e078bb49ee3789bdccd8a7792
{code}
It's a standalone deployment of one jobmanager and one taskmanager container. The straces were recorded on fedora kinoite with kernel version 6.7.
If it's of any help I can also get an strace of an older flink 1.17 / java 11 version of the job.;;;","25/Apr/24 13:00;srichter;I think you are right, close will only guarantee a flush, i.e. passing all data to the OS, but not forcing the OS to write to disk.;;;","30/Apr/24 11:37;srichter;Merged to master in [{{80af4d5}}|https://github.com/apache/flink/commit/80af4d502318348ba15a8f75a2a622ce9dbdc968]  ;;;","30/Apr/24 12:37;roman;[~srichter] would you mind backporting the fix to previous releases?

It should at least be ported to one previous release according to support policy, ideally to two.;;;","01/May/24 08:42;roman;Backported

to 1.18 as e6726d3b962383d9a2576fe117d7566b205f514a and

to 1.19 as ac4aa35c6e2e2da87760ffbf45d85888b1976c2f.;;;",,,,,,,,,,,,,,,,,,,,,,,
Support for RETURNING clause of JSON_QUERY,FLINK-35216,13576900,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,23/Apr/24 11:08,23/May/24 14:12,04/Jun/24 20:40,23/May/24 14:12,,,,,,,,,1.20.0,,,Table SQL / Planner,,,,0,pull-request-available,,"SQL standard says JSON_QUERY should support RETURNING clause similar to JSON_VALUE. Calcite supports the clause for JSON_VALUE already, but not for the JSON_QUERY.

{code}
<JSON query> ::=
  JSON_QUERY <left paren>
      <JSON API common syntax>
      [ <JSON output clause> ]
      [ <JSON query wrapper behavior> WRAPPER ]
      [ <JSON query quotes behavior> QUOTES [ ON SCALAR STRING ] ]
      [ <JSON query empty behavior> ON EMPTY ]
      [ <JSON query error behavior> ON ERROR ]
      <right paren>

<JSON output clause> ::=
  RETURNING <data type>
      [ FORMAT <JSON representation> ]
{code}",,,,,,,,,,,,,,,,,,,,,,CALCITE-6365,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 14:12:46 UTC 2024,,,,,,,,,,"0|z1otg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/24 14:12;dwysakowicz;Implemented in 0737220959fe52ee22535e7db55b015a46a6294e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The performance of serializerKryo and serializerKryoWithoutRegistration are regressed,FLINK-35215,13576866,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,fanrui,fanrui,23/Apr/24 05:57,31/May/24 03:24,04/Jun/24 20:40,31/May/24 03:24,1.20.0,,,,,,,,1.20.0,,,API / Type Serialization System,,,,1,pull-request-available,,"The performance of serializerKryo and serializerKryoWithoutRegistration are regressed[1][2], I checked recent commits, and found FLINK-34954 changed related logic.

 

[1] http://flink-speed.xyz/timeline/#/?exe=1,6,12&ben=serializerKryo&extr=on&quarts=on&equid=off&env=3&revs=200

[2] http://flink-speed.xyz/timeline/#/?exe=1,6,12&ben=serializerKryoWithoutRegistration&extr=on&quarts=on&equid=off&env=3&revs=200

 

 ",,,,,,,,,,,,,,,,,,,,,FLINK-34954,,,,,"25/Apr/24 06:57;fanrui;image-2024-04-25-14-57-55-231.png;https://issues.apache.org/jira/secure/attachment/13068422/image-2024-04-25-14-57-55-231.png","25/Apr/24 07:00;fanrui;image-2024-04-25-15-00-32-410.png;https://issues.apache.org/jira/secure/attachment/13068423/image-2024-04-25-15-00-32-410.png","31/May/24 03:22;fanrui;image-2024-05-31-11-22-49-226.png;https://issues.apache.org/jira/secure/attachment/13069217/image-2024-05-31-11-22-49-226.png",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 03:24:10 UTC 2024,,,,,,,,,,"0|z1ot8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 06:01;fanrui;Hi [~q.xu] , would you mind taking a look this issue in your free time? thank you in advance.;;;","23/Apr/24 21:06;q.xu;Hello [~fanrui] 

I have a few questions:
 * I'm trying to understand the numbers. How should I interprete them?
 ** For [serializerKryoWithoutRegistration|[http://flink-speed.xyz/timeline/#/?exe=1,6,12&ben=serializerKryoWithoutRegistration&extr=on&quarts=on&equid=off&env=3&revs=50]:|http://flink-speed.xyz/timeline/#/?exe=1,6,12&ben=serializerKryoWithoutRegistration&extr=on&quarts=on&equid=off&env=3&revs=50],] the number for java17 had dropped before my commit was merged, and was a bit better after; the number for default jdk (java8?) dropped as well before my commit, between 04-16 and 04-18, there's a drop about 5%, and then the drop between 04-18 and 04-20 is less than 2% (I skipped 04-19 because it looks like an outlier).
 ** For [serializerKryo|[http://flink-speed.xyz/timeline/#/?exe=1,6,12&ben=serializerKryo&extr=on&quarts=on&equid=off&env=3&revs=50],] the number for java8 seems like normal fluctuation if I compare with the past; for java17 the drop between 04-17 and 04-19 (before my commit) is around 5%, and numbers are not much changed after the merge of my commit comparing to 04-19.
 ** For the two dashboards, only java11 seems have dropped in a statistically meaningful way. But in the past, the fluctuation of this trend has always been significantly higher, so I'm not sure if it's really related to my commit.
 * How are the benchmarks done? Do you have a documentation to reproduce that? ;;;","24/Apr/24 01:23;kkrugler;Hi [~fanrui] - I haven't been following how Flink determines the severity & nature of performance regressions, but labeling this a Blocker Bug seems...odd to me. The speedtest I looked at showed a delta that was close to normal fluctuations in performance results.;;;","25/Apr/24 07:08;fanrui;Hi [~q.xu]  [~kkrugler] , flink community has a performance regression detector to detect them, and send the result to Slack channel every day. I noticed these regression there. If you are interested in it, you can can more details from here[1][2].

Sometimes the performance regression detector will also send an alert when some benchmarks fluctuate only occasionally.

But according to my observation, serializerKryo and serializerKryoWithoutRegistration really have performance regression.(it's not a big regression.) Let us focus on the java 17 and serializerKryoWithoutRegistration(I saw java 8 and java11 are similar).
 * After April 19, average performance is lower than before April 19.
 ** Note: let us focus on the average performance instead of single day.
 ** The reason for single-day performance regression may be caused by fluctuation.
 ** In general, the performance will be recovered in a few days if the performance regression is caused by fluctuation.
 ** All of them (serializerKryoWithoutRegistration and serializerKryo, java 8, java11 and java17) don't recovered for now.
 * And FLINK-34954 is merged at April 19
 * Also, I reverts the FLINK-34954 on my flink fork repo, and try to re-run the benchmark 3 times.
 ** The performance is recovered after reverting, that's why I'm sure FLINK-34954 causes it.
 ** I will add the result later.

!image-2024-04-25-14-57-55-231.png|width=968,height=365!

!image-2024-04-25-15-00-32-410.png|width=974,height=363!

 

[1][https://github.com/apache/flink-benchmarks/blob/master/regression_report_v2.py]

[2][https://docs.google.com/document/d/1Bvzvq79Ll5yxd1UtC0YzczgFbZPAgPcN3cI0MjVkIag]

 ;;;","25/Apr/24 07:59;fanrui;Here is the benchmark result for my test:

 
||Code||serializerKryo||serializerKryoWithoutRegistration||Comment||
|Case1: current master branch|167.163333333 ops/ms|135.03  ops/ms|Lower about 10% than reverts FLINK-34954|
|Case2: master branch reverts FLINK-34954  |190.636025667 ops/ms|147.184592333  ops/ms| |
|Case3: reverts FLINK-34954 and 
with [https://github.com/apache/flink/pull/24717]|189.621157333 ops/ms|146.914767 ops/ms|Lower about 0.5% than reverts FLINK-34954 |

 

 

Note: I choose 3 samples for each cases.
h1. Case1: current master branch

data from benchmark result WebUI.
 * serializerKryo (avg: 167.163333333 ops/ms)
 ** [http://flink-speed.xyz/timeline/#/?exe=6&ben=serializerKryo&extr=on&quarts=on&equid=off&env=3&revs=50]
 ** 164.68 ops/ms (Apr 22)
 ** 168.80 ops/ms (Apr 23)
 ** 168.01 ops/ms (Apr 24)
 * serializerKryoWithoutRegistration (avg: 135.03  ops/ms)
 **  [http://flink-speed.xyz/timeline/#/?exe=12&ben=serializerKryoWithoutRegistration&extr=on&quarts=on&equid=off&env=3&revs=50]
 ** 134.93 ops/ms (Apr 22)
 ** 134.06 ops/ms (Apr 23)
 ** 136.10 ops/ms (Apr 24)

h1. Case2: master branch reverts FLINK-34954  

I trigger it manually, the code branch is : [https://github.com/1996fanrui/flink/tree/revert-kyro-fix]

Note: the result of case2 and case3 isn't public, I paste the result from the Flink Community jenkins.
 * serializerKryo (avg: 190.636025667 ops/ms)
 ** 194.929708 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/81/artifact/jmh-result.csv/*view*/]
 ** 186.023176 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/83/artifact/jmh-result.csv/*view*/]
 ** 190.955193 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/85/artifact/jmh-result.csv/*view*/]
 * serializerKryoWithoutRegistration (avg: 147.184592333  ops/ms)
 ** 149.782267 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/81/artifact/jmh-result.csv/*view*/]
 ** 146.312711 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/83/artifact/jmh-result.csv/*view*/]
 ** 145.458799 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/85/artifact/jmh-result.csv/*view*/]

h1. Case3: reverts FLINK-34954 and with [https://github.com/apache/flink/pull/24717]

I trigger it manually, the code branch is : [https://github.com/1996fanrui/flink/tree/revert-kyro-fix-and-return-if-0]
 * serializerKryo  (avg: 189.621157333 ops/ms)
 ** 182.683889 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/82/artifact/jmh-result.csv/*view*/]
 ** 191.713318 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/84/artifact/jmh-result.csv/*view*/]
 ** 194.466265 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/86/artifact/jmh-result.csv/*view*/]
 * serializerKryoWithoutRegistration  (avg: 146.914767 ops/ms)
 ** 146.514048 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/82/artifact/jmh-result.csv/*view*/]
 ** 145.825161 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/84/artifact/jmh-result.csv/*view*/]
 ** 148.405092 ops/ms     link: [http://jenkins.flink-speed.xyz/job/flink-benchmark-request/86/artifact/jmh-result.csv/*view*/]

Note: I try to use other solution to fix the bug that FLINK-34954 mentioned, but I'm not sure whether the logic is right/suitable. If the solution is fine, the benchmark result is better than FLINK-34954.

Code change: [https://github.com/1996fanrui/flink/commit/ebce171a806a579225897a2d29e4cd28e9164c4f]

The change is pretty easy.;;;","28/Apr/24 00:03;kkrugler;Hi [~fanrui] - thanks for the detailed writeup! Two comments...

# I was surprised that Case2 (above) didn't cause a test to fail, until I realized that the previous fix hadn't added a test for failure with 0 serialized bytes. I would recommend adding this, so that your changes don't accidentally re-introduce this bug.
# I still am suspicious of the change in performance. The initial fix changes a while(true) loop to one that has a simple comparison, and inside the loop there are calls to methods that are going to be doing significant work. So I really don't see how that change could have caused a significant performance regression, unless I'm missing something.;;;","08/May/24 03:05;fanrui;Thanks [~kkrugler] for the feedback!
{quote} * I was surprised that Case2 (above) didn't cause a test to fail, until I realized that the previous fix hadn't added a test for failure with 0 serialized bytes. I would recommend adding this, so that your changes don't accidentally re-introduce this bug.{quote}
Good catch, I will add test later. Also, would you mind helping review the new PR in your free time? thanks a lot.

 
{quote} * I still am suspicious of the change in performance. The initial fix changes a while(true) loop to one that has a simple comparison, and inside the loop there are calls to methods that are going to be doing significant work. So I really don't see how that change could have caused a significant performance regression, unless I'm missing something.{quote}
See this comment: https://github.com/apache/flink/pull/24717#discussion_r1579063045

 ;;;","08/May/24 13:38;kkrugler;Hi [~fanrui] - I added a comment to the PR.;;;","22/May/24 01:57;fanrui;Merged to master via:
 * 46db92641602313c4f5c1390807222c344880e2d (Revert the commit of  FLINK-34954)
 * c1baf07d7601a683f42997dc35dfaef4e41bc928
 * 5b535e1410782a2d42127e00d77815d6ab7068ed (Add test)

I will follow the benchmark result in the following days, and close this Jira after the performance is recovered.;;;","31/May/24 03:24;fanrui;The performance seems already recovered, let me close this JIRA.

 

!image-2024-05-31-11-22-49-226.png|width=1121,height=389!;;;",,,,,,,,,,,,,,,,,,,
Update result partition id for remote input channel when unknown input channel is updated,FLINK-35214,13576859,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,23/Apr/24 04:31,23/Apr/24 12:07,04/Jun/24 20:40,23/Apr/24 12:05,1.20.0,,,,,,,,1.20.0,,,Runtime / Network,,,,0,pull-request-available,,"In [FLINK-29768|https://issues.apache.org/jira/browse/FLINK-29768], the result partition in the local input channel has been updated to support speculation. It is necessary to similarly update the result partition ID in the remote input channel.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 12:05:14 UTC 2024,,,,,,,,,,"0|z1ot74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 12:05;Weijie Guo;master(1.20) via ea2eabe7da555653ac946b185ed62a1d3bfd6c00.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
A potential inconsistent table structure issue,FLINK-35213,13576857,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kunni,kunni,23/Apr/24 03:49,24/May/24 09:17,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"Currently, DataSinkWriterOperator will [request CreateTableEvent|https://github.com/apache/flink-cdc/blob/313726b09690e82aa56fb5b42e89b535d24dadd7/flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/sink/DataSinkWriterOperator.java#L149] from SchemaRegistry when restarted. 
However, If a SchemeChangeEvent is received during this process, SchemaOperator will

1. [request SchemaRegistry|https://github.com/apache/flink-cdc/blob/313726b09690e82aa56fb5b42e89b535d24dadd7/flink-cdc-runtime/src/main/java/org/apache/flink/cdc/runtime/operators/schema/SchemaOperator.java#L252] to update the schema, 
2. and then send FlushEvent. 

As the network situation is quite complex, SchemaRegistry may update the schema first, and then send a CreateTableEvent with the new schema, which is incompatible with DatachangeEvent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-23 03:49:01.0,,,,,,,,,,"0|z1ot6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink thread mode process just can run once in standalonesession mode,FLINK-35212,13576854,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,vonesec,vonesec,23/Apr/24 03:28,30/Apr/24 08:33,04/Jun/24 20:40,,,,,,,,,,,,,API / Python,,,,0,,,"{code:java}
from pyflink.common.types import Row
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common import Types, WatermarkStrategy, Configuration
from pyflink.table import EnvironmentSettings, TableEnvironment
from pyflink.table import StreamTableEnvironment, Schema
from pyflink.datastream.functions import ProcessFunction, MapFunction
from pyflink.common.time import Instant


# init task env
config = Configuration()
config.set_string(""python.execution-mode"", ""thread"")
# config.set_string(""python.execution-mode"", ""process"")
config.set_string(""python.client.executable"", ""/root/miniconda3/bin/python3"")
config.set_string(""python.executable"", ""/root/miniconda3/bin/python3"")

env = StreamExecutionEnvironment.get_execution_environment(config)
table_env = StreamTableEnvironment.create(env)

# create a batch TableEnvironment
table = table_env.from_elements([(1, 'Hi'), (2, 'Hello')]).alias(""id"", ""content"")
table_env.create_temporary_view(""test_table"", table)

result_table = table_env.sql_query(""select *, NOW() as dt from test_table"")
result_ds = table_env.to_data_stream(result_table)

# def test_func(row):
#     return row

# result_ds.map(test_func).print()
result_ds.print()

env.execute()
{code}
Start a standalone session mode cluster by command: 
{code:java}
/root/miniconda3/lib/python3.10/site-packages/pyflink/bin/bin/start-cluster.sh{code}
Submit thread mode job for the first time, this job will success fnished.
{code:java}
/root/miniconda3/lib/python3.10/site-packages/pyflink/bin/flink run -py bug.py {code}
Use above command to submit job for the second time, an error occured:
{code:java}
Job has been submitted with JobID a4f2728199277bba0500796f7925fa26
Traceback (most recent call last):
  File ""/home/disk1/bug.py"", line 34, in <module>
    env.execute()
  File ""/root/miniconda3/lib/python3.10/site-packages/pyflink/datastream/stream_execution_environment.py"", line 773, in execute
    return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
  File ""/root/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py"", line 1322, in __call__
    return_value = get_return_value(
  File ""/root/miniconda3/lib/python3.10/site-packages/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/root/miniconda3/lib/python3.10/site-packages/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o7.execute.
: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: a4f2728199277bba0500796f7925fa26)
        at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
        at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2005)
        at org.apache.flink.client.program.StreamContextEnvironment.getJobExecutionResult(StreamContextEnvironment.java:171)
        at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:122)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
        at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
        at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: a4f2728199277bba0500796f7925fa26)
        at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:130)
        at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2079)
        at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:302)
        at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
        at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2079)
        at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$33(RestClusterClient.java:794)
        at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
        at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2079)
        at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:302)
        at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
        at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:610)
        at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1085)
        at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        ... 1 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
        at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
        at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:128)
        ... 23 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)
        at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)
        at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)
        at jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
        at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
        at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
        at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
        at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
        at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
        at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
        at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
        at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
        at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
        at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
        at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
        at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
        at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
        at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
        at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: java.lang.Error: java.lang.UnsatisfiedLinkError: 'void pemja.core.PythonInterpreter$MainInterpreter.initialize()'
        at pemja.core.PythonInterpreter$MainInterpreter.initialize(PythonInterpreter.java:429)
        at pemja.core.PythonInterpreter.initialize(PythonInterpreter.java:145)
        at pemja.core.PythonInterpreter.<init>(PythonInterpreter.java:46)
        at org.apache.flink.streaming.api.operators.python.embedded.AbstractEmbeddedPythonFunctionOperator.open(AbstractEmbeddedPythonFunctionOperator.java:72)
        at org.apache.flink.streaming.api.operators.python.embedded.AbstractEmbeddedDataStreamPythonFunctionOperator.open(AbstractEmbeddedDataStreamPythonFunctionOperator.java:88)
        at org.apache.flink.streaming.api.operators.python.embedded.AbstractOneInputEmbeddedPythonFunctionOperator.open(AbstractOneInputEmbeddedPythonFunctionOperator.java:68)
        at org.apache.flink.streaming.api.operators.python.embedded.EmbeddedPythonProcessOperator.open(EmbeddedPythonProcessOperator.java:67)
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:753)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:728)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:693)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:922)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.UnsatisfiedLinkError: 'void pemja.core.PythonInterpreter$MainInterpreter.initialize()'
        at pemja.core.PythonInterpreter$MainInterpreter.initialize(Native Method)
        at pemja.core.PythonInterpreter$MainInterpreter.access$100(PythonInterpreter.java:332)
        at pemja.core.PythonInterpreter$MainInterpreter$1.run(PythonInterpreter.java:400)org.apache.flink.client.program.ProgramAbortException: java.lang.RuntimeException: Python process exits with code: 1
        at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:140)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
        at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
        at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.RuntimeException: Python process exits with code: 1
        at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:130)
        ... 14 more {code}
I guess maybe something wrong in taskmanager process when python and pemja shared libraries have already loaded in first time. 

 

I think the thread mode of PyFlink will not be available in the standalone session cluster if this issue is not resolved, so I have set the priority to critical. Please feel free to modify if have different opinions.","Python 3.10.14

PyFlink==1.18.1

openjdk version ""11.0.21"" 2023-10-17 LTS
OpenJDK Runtime Environment (Red_Hat-11.0.21.0.9-1.el7_9) (build 11.0.21+9-LTS)
OpenJDK 64-Bit Server VM (Red_Hat-11.0.21.0.9-1.el7_9) (build 11.0.21+9-LTS, mixed mode, sharing)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 08:33:06 UTC 2024,,,,,,,,,,"0|z1ot60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 03:30;vonesec;I also tested this issue with Java8 and Python 3.8.6 and another similar pemja error ocurred.;;;","23/Apr/24 09:20;vonesec;After debug this test case, I found an ""Native Library /root/miniconda3/lib/python3.10/site-packages/pemja_core.cpython-310-x86_64-linux-gnu.so already loaded in another classloader"" error throwed from pemja when taskmanager load shared libraries on the second time.
 
Does the component owner can fix it? [~hxbks2ks] 
Or we should solve it in Flink, it looks like is a common problem in other Java language projects.;;;","29/Apr/24 10:21;bgeng777;Hi [~vonesec], thanks for creating the detailed bug report! 
I create a brand new env in my local computer and followed the instructions but I cannot reproduce the exception you have met.
I take a look at the [pemja code|https://github.com/alibaba/pemja/blob/release-0.4-1-rc1/src/main/java/pemja/core/PythonInterpreter.java#L336], as we can see, the MainInterpreter is a singleton which means System.load('pemja_core.xxx.so') should happen only once in the JVM. It should get rid of the exception in the jira, whose cause is loading the same .so twice.
So it is somehow strange for you to meet such exception. Have you ever changed something under your python site-packages of pyflink?


But when running your codes, I do meet another exception when trying to `flink run` for a second time:

{quote}2024-04-29 17:41:03,054 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: *anonymous_python-input-format$1*[1] -> Calc[2] -> ConstraintEnforcer[3] -> TableToDataSteam -> Map -> Sink: Print to Std. Out (1/1)#0 (4b45f9997b18155682ed0218dcf0afbb_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAILED with failure cause:
java.lang.ClassCastException: pemja.core.object.PyIterator cannot be cast to pemja.core.object.PyIterator
	at org.apache.flink.streaming.api.operators.python.embedded.AbstractOneInputEmbeddedPythonFunctionOperator.processElement(AbstractOneInputEmbeddedPythonFunctionOperator.java:156) ~[blob_p-ce9fc762fcfc43a2cf28c0d69a738da7efd36b10-323be2d12056985907101ebb52aff326:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.table.runtime.operators.sink.OutputConversionOperator.processElement(OutputConversionOperator.java:105) ~[flink-table-runtime-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.table.runtime.util.StreamRecordCollector.collect(StreamRecordCollector.java:44) ~[flink-table-runtime-1.18.1.jar:1.18.1]
	at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processElement(ConstraintEnforcer.java:247) ~[flink-table-runtime-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist-1.18.1.jar:1.18.1]
	at StreamExecCalc$6.processElement(Unknown Source) ~[?:?]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:425) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:520) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:110) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:99) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:114) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:71) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:338) ~[flink-dist-1.18.1.jar:1.18.1]
2024-04-29 17:41:03,056 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: *anonymous_python-input-format$1*[1] -> Calc[2] -> ConstraintEnforcer[3] -> TableToDataSteam -> Map -> Sink: Print to Std. Out (1/1)#0 (4b45f9997b18155682ed0218dcf0afbb_cbc357ccb763df2852fee8c4fc7d55f2_0_0).{quote}
This exception is caused by a classloader issue. It looks like the pemja's class PyIterator is loaded by different classloaders. I figured out a workaround: putting the flink-python-1.18.1.jar under the flink's lib dir.
But I agree such exception would prevent the PyFlink thread mode from being out-of-box available in session clusters.
I would spend some time to dive into the problem and see if we can improve it. cc [~dianfu] [~hxbks2ks]

;;;","30/Apr/24 08:33;vonesec;Hi Biao Geng, as you say, maybe the exception which I submitted caused by I modified same files under python site-packages of pyflink when I try to solve FLINK-35180. And I met the same exception in commit #0 with Java8 and Python 3.8.6 as you pasted .

Looking forward for your improvement for this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"when synchronize LOB fields using oracle cdc, an error occurs",FLINK-35211,13576844,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,wangsw,wangsw,23/Apr/24 01:54,23/Apr/24 05:08,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"{code:java}
Caused by: java.sql.SQLException: ORA-01291: 缺失日志文件
ORA-06512: 在 ""SYS.DBMS_LOGMNR"", line 58
ORA-06512: 在 line 1oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1205)
oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1823)
oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1778)
oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:303)
io.debezium.jdbc.JdbcConnection.executeWithoutCommitting(JdbcConnection.java:1446)
io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource.startMiningSession(LogMinerStreamingChangeEventSource.java:677)
io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource.execute(LogMinerStreamingChangeEventSource.java:244) {code}
when synchronize LOB fields using oracle cdc 3.0.1 , an error occurs. 

I checked that the debezium version of CDC is 1.9.7.Final. When I used the same version of debezium and the same configuration to synchronize LOB data, the error did not occur, so I am asking for help.

Please view flink sql in the attachment。

Without adding parameter 'debezium.lob.enabled' = 'true', the error does not occur.

 ","Flink 18.1 

Oracle12c

Oracle cdc 3.0.1",,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/24 01:54;wangsw;cdcOracleToPrint.sql;https://issues.apache.org/jira/secure/attachment/13068362/cdcOracleToPrint.sql",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-23 01:54:14.0,,,,,,,,,,"0|z1ot3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Give the option to set automatically the parallelism of the KafkaSource to the number of kafka partitions,FLINK-35210,13576779,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,npfp,npfp,22/Apr/24 14:49,23/Apr/24 07:04,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,"Currently the setting of the `KafkaSource` Flink's operator parallelism needs to be manually chosen which can leads to highly skewed tasks if the developer doesn't do this job.

To avoid this issue, I propose to:
-  retrieve dynamically the number of partitions of the topic using `KafkaConsumer.
partitionsFor(topic).size()`,
- set the parallelism of the stream built from the source based on this value.

 This way there won't be any idle tasks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Tue Apr 23 07:04:16 UTC 2024,,,,,,,,,,"0|z1ospc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 15:19;Oleksandr Nitavskyi;Thanks [~npfp] for suggestion. I believe what you proposed is often resolve with some wrapper around KafkaSource, which could be a layer of indirection to do a lot of things, e.g. parallelism config.

Meanwhile could you please elaborate how could bad parallelism lead to the Idle tasks? Do you mean the case where Source parallelism is lower than the amount of partitions and thus you have Source which consumes nothing and thus you have no watermark advancement unless [Idleness|https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/datastream/event-time/generating_watermarks/#dealing-with-idle-sources] is not configured.;;;","22/Apr/24 15:45;npfp;That's indeed how I proceeded in our case.

 I'm sorry, my choice of words was a bit confusing.

We run a Flink job on EMR and we had a lot of issues where some task instances would be randomly killed. From what we understood, one of the reason was that we used a Flink parallelism for the application equals to a multiple of the number of instances. So some of the task instances could end up reading one or more partitions while some others would read nothing while having the dedicated resources. This behaviour was amplified by the fact that we've got several kafka sources.
It appeared to us that this was a source of instability, so we ended up forcing the parallelism of the source operator to the number of kafka partitions.;;;","22/Apr/24 18:42;martijnvisser;There isn't enough information about used/tested versions of Flink, but I think the actual solution should be fixing bugs (if there are any) instead of finding workarounds like the one proposed in the ticket. It sounds trivial from the start, but the moment you have to take things like new partitions being added on a source that's already in use, this becomes less of an easy fix. https://cwiki.apache.org/confluence/display/FLINK/FLIP-288%3A+Enable+Dynamic+Partition+Discovery+by+Default+in+Kafka+Source already showed some considerations for those edge cases as well.;;;","23/Apr/24 07:04;npfp;I see your point.


Unfortunately, the issue only appeared in production settings in the context of a 1.15 to 1.16 migration and we only could guess that this migration implied more memory load on the instances, leading to disk spilling and eventually unresponsive instances. We tested tweaking all the parameters that changed with the migration so the behaviour would be the same as it used to be but nothing worked apart reducing the local parallelism for the kafka source operators and the global parallelism to match the number of instances.

Just to be sure I understand it well: when the parallelism of a kafka source is bigger that the number of Kafka partitions, the reading task from no partition will:
 * be part of a task slot in case of slot sharing and therefore won't waste memory as this task slot will be used by other tasks,
 * have a dedicated task slot in case of non slot sharing and therefore will waste the dedicated memory.

However, in the first case the memory footprint won't be the same across nodes as the nodes that contain a task slot reading from an existing partition will require more memory than the others.

Is my understanding correct?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add a DeserializationSchema decorator that counts deserialize errors,FLINK-35209,13576757,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,npfp,npfp,22/Apr/24 12:38,22/Apr/24 12:38,04/Jun/24 20:40,,,,,,,,,,,,,API / Core,,,,0,,,"I would like to propose a PR that implements a decorator for `DeserializationSchema`.

The decorator decorates an `DeserializationSchema` object. The purpose of this decorator is to catch any deserialization errors that could occur when deserializing messages. The decorator has a flag to decide to fail or not in this case. If it makes the error silent, then it would count them in a `flink.metrics.Counter` so the user can monitor the silent errors. This PR is ready to be created.

This decorator could be improved by having a sink that would be used to sink all the messages causing deserialization errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,2024-04-22 12:38:36.0,,,,,,,,,,"0|z1oskg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect pipeline.cached-files during processing Python dependencies,FLINK-35208,13576726,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,22/Apr/24 09:22,28/Apr/24 07:40,04/Jun/24 20:40,28/Apr/24 07:40,,,,,,,,,1.20.0,,,API / Python,,,,0,pull-request-available,,"Currently, PyFlink will make use of distributed cache (StreamExecutionEnvironment#cachedFiles) during handling the Python dependencies(See [https://github.com/apache/flink/blob/master/flink-python/src/main/java/org/apache/flink/python/util/PythonDependencyUtils.java#L339] for more details). 

However, if pipeline.cached-files is configured, it will clear StreamExecutionEnvironment#cachedFiles(see [https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java#L1132] for more details) which may break the above functionalities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 28 07:40:35 UTC 2024,,,,,,,,,,"0|z1osdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/24 07:40;dianfu;Merged to master via 0f01a0e9a6856781d5a0e33b26172bb913ec1928;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes session E2E test fails to fetch packages.,FLINK-35207,13576717,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,22/Apr/24 08:32,22/Apr/24 08:32,04/Jun/24 20:40,,1.18.2,,,,,,,,,,,,,,,0,test-stability,,"1.18 Default (Java 8) / E2E (group 1) https://github.com/apache/flink/commit/aacc735806acf1d63fa732706e079bc2ca1bb4fc/checks/24027142976/logs

Looks like some flakiness when fetching packages to install (and just to track if this happens again)

{code}
2024-04-19T14:28:15.9116531Z Apr 19 14:28:15 ==============================================================================
2024-04-19T14:28:15.9117204Z Apr 19 14:28:15 Running 'Run kubernetes session test (custom fs plugin)'
2024-04-19T14:28:15.9118209Z Apr 19 14:28:15 ==============================================================================
2024-04-19T14:28:15.9119866Z Apr 19 14:28:15 TEST_DATA_DIR: /home/runner/work/flink/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-15907928199
2024-04-19T14:28:16.1477984Z Apr 19 14:28:16 Flink dist directory: /home/runner/work/flink/flink/flink-dist/target/flink-1.18-SNAPSHOT-bin/flink-1.18-SNAPSHOT
2024-04-19T14:28:16.1546131Z Apr 19 14:28:16 Flink dist directory: /home/runner/work/flink/flink/flink-dist/target/flink-1.18-SNAPSHOT-bin/flink-1.18-SNAPSHOT
2024-04-19T14:28:16.1670878Z Apr 19 14:28:16 Docker version 24.0.9, build 2936816
2024-04-19T14:28:16.5441575Z Apr 19 14:28:16 docker-compose version 1.29.2, build 5becea4c
2024-04-19T14:28:16.7073581Z Apr 19 14:28:16 Reading package lists...
2024-04-19T14:28:16.8529977Z Apr 19 14:28:16 Building dependency tree...
2024-04-19T14:28:16.8541118Z Apr 19 14:28:16 Reading state information...
2024-04-19T14:28:16.9872695Z Apr 19 14:28:16 conntrack is already the newest version (1:1.4.6-2build2).
2024-04-19T14:28:16.9873637Z Apr 19 14:28:16 0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.
2024-04-19T14:28:17.5567699Z 2024-04-19 14:28:17 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/80172100/7186c302-3766-4ed5-920a-f85c9d6334ac?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240419%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240419T142817Z&X-Amz-Expires=300&X-Amz-Signature=fe759ee1ce1eb3ebeaee7d8e714aedcedbc9035c75bc656f3ecda57836820bdf&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=80172100&response-content-disposition=attachment%3B%20filename%3Dcrictl-v1.24.2-linux-amd64.tar.gz&response-content-type=application%2Foctet-stream [14553934/14553934] -> ""crictl-v1.24.2-linux-amd64.tar.gz"" [1]
2024-04-19T14:28:17.5668524Z Apr 19 14:28:17 crictl
2024-04-19T14:28:18.1236206Z 2024-04-19 14:28:18 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/318491505/e304ee45-ccad-4438-bc2c-039c8f6755d1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240419%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240419T142817Z&X-Amz-Expires=300&X-Amz-Signature=43ca222d979f2595126d94c344045c628c246597bfc9339d6b4dbf223e8b6be3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=318491505&response-content-disposition=attachment%3B%20filename%3Dcri-dockerd-0.2.3.amd64.tgz&response-content-type=application%2Foctet-stream [23042323/23042323] -> ""cri-dockerd-0.2.3.amd64.tgz.2"" [1]
2024-04-19T14:28:18.1292589Z Apr 19 14:28:18 cri-dockerd/cri-dockerd
2024-04-19T14:28:18.6786614Z 2024-04-19 14:28:18 URL:https://raw.githubusercontent.com/Mirantis/cri-dockerd/v0.2.3/packaging/systemd/cri-docker.service [1337/1337] -> ""cri-docker.service"" [1]
2024-04-19T14:28:18.8479307Z 2024-04-19 14:28:18 URL:https://raw.githubusercontent.com/Mirantis/cri-dockerd/v0.2.3/packaging/systemd/cri-docker.socket [204/204] -> ""cri-docker.socket"" [1]
2024-04-19T14:28:19.5285798Z Apr 19 14:28:19 fs.protected_regular = 0
2024-04-19T14:28:19.6167026Z Apr 19 14:28:19 minikube
2024-04-19T14:28:19.6167602Z Apr 19 14:28:19 type: Control Plane
2024-04-19T14:28:19.6168146Z Apr 19 14:28:19 host: Stopped
2024-04-19T14:28:19.6170872Z Apr 19 14:28:19 kubelet: Stopped
2024-04-19T14:28:19.6175184Z Apr 19 14:28:19 apiserver: Stopped
2024-04-19T14:28:19.6179746Z Apr 19 14:28:19 kubeconfig: Stopped
2024-04-19T14:28:19.6180518Z Apr 19 14:28:19 
2024-04-19T14:28:19.6211870Z Apr 19 14:28:19 Starting minikube ...
2024-04-19T14:28:19.6893918Z Apr 19 14:28:19 * minikube v1.28.0 on Ubuntu 22.04
2024-04-19T14:28:19.6934942Z Apr 19 14:28:19 * Using the none driver based on existing profile
2024-04-19T14:28:19.6951845Z Apr 19 14:28:19 * Starting control plane node minikube in cluster minikube
2024-04-19T14:28:19.7246076Z Apr 19 14:28:19 * Restarting existing none bare metal machine for ""minikube"" ...
2024-04-19T14:28:19.7365028Z Apr 19 14:28:19 * OS release is Ubuntu 22.04.4 LTS
2024-04-19T14:28:22.1596670Z Apr 19 14:28:22 * Preparing Kubernetes v1.25.3 on Docker 24.0.9 ...
2024-04-19T14:28:22.1618992Z Apr 19 14:28:22   - kubelet.image-gc-high-threshold=99
2024-04-19T14:28:22.1622618Z Apr 19 14:28:22   - kubelet.image-gc-low-threshold=98
2024-04-19T14:28:22.1626821Z Apr 19 14:28:22   - kubelet.minimum-container-ttl-duration=120m
2024-04-19T14:28:22.1631924Z Apr 19 14:28:22   - kubelet.eviction-hard=memory.available<5Mi,nodefs.available<1Mi,imagefs.available<1Mi
2024-04-19T14:28:22.1636080Z Apr 19 14:28:22   - kubelet.eviction-soft=memory.available<5Mi,nodefs.available<2Mi,imagefs.available<2Mi
2024-04-19T14:28:22.1639814Z Apr 19 14:28:22   - kubelet.eviction-soft-grace-period=memory.available=2h,nodefs.available=2h,imagefs.available=2h
2024-04-19T14:28:22.1643313Z Apr 19 14:28:22   - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf
2024-04-19T14:28:31.7841374Z Apr 19 14:28:31 * Configuring local host environment ...
2024-04-19T14:28:31.7842730Z * 
2024-04-19T14:28:31.7843827Z ! The 'none' driver is designed for experts who need to integrate with an existing VM
2024-04-19T14:28:31.7845787Z * Most users should use the newer 'docker' driver instead, which does not require root!
2024-04-19T14:28:31.7847126Z * For more information, see: https://minikube.sigs.k8s.io/docs/reference/drivers/none/
2024-04-19T14:28:31.7848044Z * 
2024-04-19T14:28:31.7857408Z Apr 19 14:28:31 * Verifying Kubernetes components...
2024-04-19T14:28:31.8378492Z Apr 19 14:28:31   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
2024-04-19T14:28:32.3823850Z Apr 19 14:28:32 * Enabled addons: default-storageclass, storage-provisioner
2024-04-19T14:28:32.4230194Z Apr 19 14:28:32 
2024-04-19T14:28:32.4234195Z ! /usr/bin/kubectl is version 1.29.3, which may have incompatibilities with Kubernetes 1.25.3.
2024-04-19T14:28:32.4239061Z Apr 19 14:28:32   - Want kubectl v1.25.3? Try 'minikube kubectl -- get pods -A'
2024-04-19T14:28:32.4244093Z Apr 19 14:28:32 * Done! kubectl is now configured to use ""minikube"" cluster and ""default"" namespace by default
2024-04-19T14:28:32.5337488Z Apr 19 14:28:32 ! No changes required for the ""minikube"" context
2024-04-19T14:28:32.5351056Z Apr 19 14:28:32 * Current context is ""minikube""
2024-04-19T14:28:32.6869707Z Apr 19 14:28:32 minikube
2024-04-19T14:28:32.6870490Z Apr 19 14:28:32 type: Control Plane
2024-04-19T14:28:32.6871311Z Apr 19 14:28:32 host: Running
2024-04-19T14:28:32.6871820Z Apr 19 14:28:32 kubelet: Running
2024-04-19T14:28:32.6872515Z Apr 19 14:28:32 apiserver: Running
2024-04-19T14:28:32.6873229Z Apr 19 14:28:32 kubeconfig: Configured
2024-04-19T14:28:32.6873992Z Apr 19 14:28:32 
2024-04-19T14:28:32.6931789Z Apr 19 14:28:32 Starting fileserver for Flink distribution
2024-04-19T14:28:32.6933558Z Apr 19 14:28:32 ~/work/flink/flink/flink-dist/target/flink-1.18-SNAPSHOT-bin ~/work/flink/flink
2024-04-19T14:28:48.1402939Z Apr 19 14:28:48 ~/work/flink/flink
2024-04-19T14:28:48.1404207Z Apr 19 14:28:48 ~/work/flink/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-15907928199 ~/work/flink/flink
2024-04-19T14:28:48.1406579Z Apr 19 14:28:48 Preparing Dockeriles
2024-04-19T14:28:48.1409441Z Apr 19 14:28:48 Executing command: git clone https://github.com/apache/flink-docker.git --branch dev-1.18 --single-branch
2024-04-19T14:28:48.1425633Z Cloning into 'flink-docker'...
2024-04-19T14:28:48.6908924Z Generating Dockerfiles... done.
2024-04-19T14:28:48.6912547Z Apr 19 14:28:48 Building images
2024-04-19T14:28:49.2107019Z #0 building with ""default"" instance using docker driver
2024-04-19T14:28:49.2107976Z 
2024-04-19T14:28:49.2112523Z #1 [internal] load .dockerignore
2024-04-19T14:28:49.6193679Z #1 transferring context: 2B done
2024-04-19T14:28:49.6194161Z #1 DONE 0.4s
2024-04-19T14:28:49.6194328Z 
2024-04-19T14:28:49.6194523Z #2 [internal] load build definition from Dockerfile
2024-04-19T14:28:49.6194971Z #2 transferring dockerfile: 4.06kB done
2024-04-19T14:28:49.6195333Z #2 DONE 0.4s
2024-04-19T14:28:49.6195498Z 
2024-04-19T14:28:49.6196025Z #3 [internal] load metadata for docker.io/library/eclipse-temurin:8-jre-jammy
2024-04-19T14:28:49.8448074Z #3 ...
2024-04-19T14:28:49.8448393Z 
2024-04-19T14:28:49.8449407Z #4 [auth] library/eclipse-temurin:pull token for registry-1.docker.io
2024-04-19T14:28:49.8450215Z #4 DONE 0.0s
2024-04-19T14:28:49.9943404Z 
2024-04-19T14:28:49.9944552Z #3 [internal] load metadata for docker.io/library/eclipse-temurin:8-jre-jammy
2024-04-19T14:28:50.0634964Z #3 DONE 0.6s
2024-04-19T14:28:50.2720736Z 
2024-04-19T14:28:50.2722415Z #5 [1/7] FROM docker.io/library/eclipse-temurin:8-jre-jammy@sha256:ed01c07af919817182c3ba5153f6c16a9a4719554610ee06bd40b8724be9e495
2024-04-19T14:28:50.2723747Z #5 CACHED
2024-04-19T14:28:50.2723987Z 
2024-04-19T14:28:50.2724186Z #6 [internal] load build context
2024-04-19T14:28:50.2724755Z #6 transferring context: 5.33kB done
2024-04-19T14:28:50.2727079Z #6 DONE 0.0s
2024-04-19T14:28:50.2727360Z 
2024-04-19T14:28:50.2728096Z #7 [2/7] RUN set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*
2024-04-19T14:28:50.2729204Z #7 0.055 + apt-get update
2024-04-19T14:28:51.1436123Z #7 1.076 Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]
2024-04-19T14:28:51.7944897Z #7 1.727 Get:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]
2024-04-19T14:28:52.9367576Z #7 2.870 Get:3 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]
2024-04-19T14:28:53.6994739Z #7 3.632 Get:4 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,739 kB]
2024-04-19T14:28:53.9295181Z #7 3.712 Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]
2024-04-19T14:28:54.9547182Z #7 4.888 Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]
2024-04-19T14:28:58.4269258Z #7 8.360 Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]
2024-04-19T14:29:36.3677738Z #7 46.30 Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,228 kB]
2024-04-19T14:30:25.7290610Z #7 95.66 Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]
2024-04-19T14:32:30.9312640Z #7 220.9 Get:10 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]
2024-04-19T14:32:32.4662263Z #7 222.4 Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1,792 kB]
2024-04-19T14:32:57.2221456Z #7 247.2 Get:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]
2024-04-19T14:32:57.6880543Z #7 247.6 Ign:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages
2024-04-19T14:32:57.8661891Z #7 247.7 Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.1 kB]
2024-04-19T14:32:57.8663187Z #7 247.8 Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,307 kB]
2024-04-19T14:32:58.2384289Z #7 248.2 Ign:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages
2024-04-19T14:32:58.4071654Z #7 248.3 Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [80.9 kB]
2024-04-19T14:32:58.4073239Z #7 248.3 Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.3 kB]
2024-04-19T14:32:58.6430864Z #7 248.4 Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,019 kB]
2024-04-19T14:32:58.6432668Z #7 248.4 Err:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages
2024-04-19T14:32:58.6433526Z #7 248.4   File has unexpected size (2018947 != 2018948). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:32:58.6435275Z #7 248.4   Hashes of expected file:
2024-04-19T14:32:58.6436726Z #7 248.4    - Filesize:2018948 [weak]
2024-04-19T14:32:58.6437763Z #7 248.4    - SHA256:3a92474950b3c669eaa9e15ec6df5616c9e2e164a72a9b74f0c8c340e471f905
2024-04-19T14:32:58.6438934Z #7 248.4    - SHA1:7cf8264da76c47b0fe05450b1d7319a333c3b728 [weak]
2024-04-19T14:32:58.6439811Z #7 248.4    - MD5Sum:1f3caac4bca7187c33ea80c2bf8e5f5c [weak]
2024-04-19T14:32:58.6440585Z #7 248.4   Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:32:59.4572320Z #7 249.4 Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]
2024-04-19T14:33:16.1984264Z #7 266.1 Fetched 29.2 MB in 4min 26s (110 kB/s)
2024-04-19T14:33:16.8213327Z #7 266.1 Reading package lists...
2024-04-19T14:33:16.8509521Z #7 266.8 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/main/binary-amd64/Packages.gz  File has unexpected size (2018947 != 2018948). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:33:16.8511321Z #7 266.8    Hashes of expected file:
2024-04-19T14:33:16.8511910Z #7 266.8     - Filesize:2018948 [weak]
2024-04-19T14:33:16.8517391Z #7 266.8     - SHA256:3a92474950b3c669eaa9e15ec6df5616c9e2e164a72a9b74f0c8c340e471f905
2024-04-19T14:33:16.8518590Z #7 266.8     - SHA1:7cf8264da76c47b0fe05450b1d7319a333c3b728 [weak]
2024-04-19T14:33:16.8519370Z #7 266.8     - MD5Sum:1f3caac4bca7187c33ea80c2bf8e5f5c [weak]
2024-04-19T14:33:16.8519907Z #7 266.8    Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:33:16.8520601Z #7 266.8 E: Some index files failed to download. They have been ignored, or old ones used instead.
2024-04-19T14:33:16.8521969Z #7 ERROR: process ""/bin/sh -c set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*"" did not complete successfully: exit code: 100
2024-04-19T14:33:16.8522939Z ------
2024-04-19T14:33:16.8523653Z  > [2/7] RUN set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*:
2024-04-19T14:33:16.8524684Z 249.4 Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]
2024-04-19T14:33:16.8525147Z 
2024-04-19T14:33:16.8526414Z 266.8 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/main/binary-amd64/Packages.gz  File has unexpected size (2018947 != 2018948). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:33:16.8527564Z 266.8    Hashes of expected file:
2024-04-19T14:33:16.8527953Z 266.8     - Filesize:2018948 [weak]
2024-04-19T14:33:16.8528534Z 266.8     - SHA256:3a92474950b3c669eaa9e15ec6df5616c9e2e164a72a9b74f0c8c340e471f905
2024-04-19T14:33:16.8530964Z 266.8     - SHA1:7cf8264da76c47b0fe05450b1d7319a333c3b728 [weak]
2024-04-19T14:33:16.8531540Z 266.8     - MD5Sum:1f3caac4bca7187c33ea80c2bf8e5f5c [weak]
2024-04-19T14:33:16.8532050Z 266.8    Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:33:16.8532716Z 266.8 E: Some index files failed to download. They have been ignored, or old ones used instead.
2024-04-19T14:33:16.8533279Z ------
2024-04-19T14:33:16.8533508Z Dockerfile:22
2024-04-19T14:33:16.8533770Z --------------------
2024-04-19T14:33:16.8534049Z   21 |     # Install dependencies
2024-04-19T14:33:16.8534400Z   22 | >>> RUN set -ex; \
2024-04-19T14:33:16.8534728Z   23 | >>>   apt-get update; \
2024-04-19T14:33:16.8535258Z   24 | >>>   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev; \
2024-04-19T14:33:16.8535810Z   25 | >>>   rm -rf /var/lib/apt/lists/*
2024-04-19T14:33:16.8536134Z   26 |     
2024-04-19T14:33:16.8536383Z --------------------
2024-04-19T14:33:16.8537493Z ERROR: failed to solve: process ""/bin/sh -c set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*"" did not complete successfully: exit code: 100
2024-04-19T14:33:16.8566969Z Apr 19 14:33:16 ~/work/flink/flink
2024-04-19T14:33:16.8569678Z Apr 19 14:33:16 Command: build_image test_kubernetes_session 10.1.0.63 failed. Retrying...
2024-04-19T14:33:18.8586467Z Apr 19 14:33:18 Starting fileserver for Flink distribution
2024-04-19T14:33:18.8588071Z Apr 19 14:33:18 ~/work/flink/flink/flink-dist/target/flink-1.18-SNAPSHOT-bin ~/work/flink/flink
2024-04-19T14:33:34.3295853Z Apr 19 14:33:34 ~/work/flink/flink
2024-04-19T14:33:34.3297499Z Apr 19 14:33:34 ~/work/flink/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-15907928199 ~/work/flink/flink
2024-04-19T14:33:34.3300545Z Apr 19 14:33:34 Preparing Dockeriles
2024-04-19T14:33:34.3301986Z Apr 19 14:33:34 Executing command: git clone https://github.com/apache/flink-docker.git --branch dev-1.18 --single-branch
2024-04-19T14:33:34.3318525Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:33:34.3325238Z Apr 19 14:33:34 Retry 1/5 exited 128, retrying in 1 seconds...
2024-04-19T14:33:34.4271357Z Traceback (most recent call last):
2024-04-19T14:33:34.4273159Z   File ""/home/runner/work/flink/flink/flink-end-to-end-tests/test-scripts/python3_fileserver.py"", line 26, in <module>
2024-04-19T14:33:34.4274577Z     httpd = socketserver.TCPServer(("""", 9999), handler)
2024-04-19T14:33:34.4275419Z   File ""/usr/lib/python3.10/socketserver.py"", line 452, in __init__
2024-04-19T14:33:34.4276152Z     self.server_bind()
2024-04-19T14:33:34.4276778Z   File ""/usr/lib/python3.10/socketserver.py"", line 466, in server_bind
2024-04-19T14:33:34.4277544Z     self.socket.bind(self.server_address)
2024-04-19T14:33:34.4278145Z OSError: [Errno 98] Address already in use
2024-04-19T14:33:35.3360844Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:33:35.3363869Z Apr 19 14:33:35 Retry 2/5 exited 128, retrying in 2 seconds...
2024-04-19T14:33:37.3397265Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:33:37.3399988Z Apr 19 14:33:37 Retry 3/5 exited 128, retrying in 4 seconds...
2024-04-19T14:33:41.3431511Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:33:41.3434164Z Apr 19 14:33:41 Retry 4/5 exited 128, retrying in 8 seconds...
2024-04-19T14:33:49.3465591Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:33:49.3468893Z Apr 19 14:33:49 Retry 5/5 exited 128, no more retries left.
2024-04-19T14:33:49.3595146Z Generating Dockerfiles... done.
2024-04-19T14:33:49.3597938Z Apr 19 14:33:49 Building images
2024-04-19T14:33:49.6898354Z #0 building with ""default"" instance using docker driver
2024-04-19T14:33:49.6898938Z 
2024-04-19T14:33:49.6899156Z #1 [internal] load .dockerignore
2024-04-19T14:33:49.6899734Z #1 transferring context: 2B done
2024-04-19T14:33:49.6900254Z #1 DONE 0.0s
2024-04-19T14:33:49.6900499Z 
2024-04-19T14:33:49.6900769Z #2 [internal] load build definition from Dockerfile
2024-04-19T14:33:49.6901477Z #2 transferring dockerfile: 4.06kB done
2024-04-19T14:33:49.6902045Z #2 DONE 0.0s
2024-04-19T14:33:49.6902281Z 
2024-04-19T14:33:49.6902980Z #3 [auth] library/eclipse-temurin:pull token for registry-1.docker.io
2024-04-19T14:33:49.6903758Z #3 DONE 0.0s
2024-04-19T14:33:49.6904002Z 
2024-04-19T14:33:49.6904580Z #4 [internal] load metadata for docker.io/library/eclipse-temurin:8-jre-jammy
2024-04-19T14:33:50.0594746Z #4 DONE 0.5s
2024-04-19T14:33:50.2625719Z 
2024-04-19T14:33:50.2627377Z #5 [1/7] FROM docker.io/library/eclipse-temurin:8-jre-jammy@sha256:ed01c07af919817182c3ba5153f6c16a9a4719554610ee06bd40b8724be9e495
2024-04-19T14:33:50.2628674Z #5 CACHED
2024-04-19T14:33:50.2628902Z 
2024-04-19T14:33:50.2629095Z #6 [internal] load build context
2024-04-19T14:33:50.2629575Z #6 transferring context: 5.33kB done
2024-04-19T14:33:50.2630268Z #6 DONE 0.0s
2024-04-19T14:33:50.2630426Z 
2024-04-19T14:33:50.2631127Z #7 [2/7] RUN set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*
2024-04-19T14:33:50.2631943Z #7 0.050 + apt-get update
2024-04-19T14:33:50.4521237Z #7 0.389 Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]
2024-04-19T14:33:50.8895575Z #7 0.827 Get:2 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]
2024-04-19T14:33:51.0040904Z #7 0.941 Get:3 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]
2024-04-19T14:33:51.1335201Z #7 1.071 Get:4 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]
2024-04-19T14:33:51.3021599Z #7 1.240 Get:5 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,228 kB]
2024-04-19T14:33:51.4337491Z #7 1.371 Get:6 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,739 kB]
2024-04-19T14:33:57.0574558Z #7 6.995 Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]
2024-04-19T14:33:59.0047600Z #7 8.942 Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]
2024-04-19T14:34:01.5106201Z #7 11.45 Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1,792 kB]
2024-04-19T14:34:02.2099249Z #7 12.15 Get:10 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]
2024-04-19T14:34:02.3899195Z #7 12.24 Get:11 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]
2024-04-19T14:34:02.3900106Z #7 12.33 Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]
2024-04-19T14:34:03.0054803Z #7 12.94 Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.1 kB]
2024-04-19T14:34:03.1949206Z #7 13.03 Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,307 kB]
2024-04-19T14:34:03.1950207Z #7 13.13 Ign:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages
2024-04-19T14:34:03.3621871Z #7 13.22 Ign:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages
2024-04-19T14:34:03.3623385Z #7 13.30 Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [80.9 kB]
2024-04-19T14:34:03.5330341Z #7 13.38 Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.3 kB]
2024-04-19T14:34:03.5332067Z #7 13.47 Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]
2024-04-19T14:34:03.6834842Z #7 13.47 Err:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages
2024-04-19T14:34:03.6836417Z #7 13.47   File has unexpected size (1369166 != 1369168). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:34:03.6837037Z #7 13.47   Hashes of expected file:
2024-04-19T14:34:03.6837463Z #7 13.47    - Filesize:1369168 [weak]
2024-04-19T14:34:03.6838066Z #7 13.47    - SHA256:50cd837b83825232586619378da6fbcf1f763f0c7fc8fb4100429822cbab2c23
2024-04-19T14:34:03.6838762Z #7 13.47    - SHA1:cdf607a214a03a5cc987b19407a45e9d382331fa [weak]
2024-04-19T14:34:03.6839338Z #7 13.47    - MD5Sum:252b19f60d48d9398f5966c8dcd0b08f [weak]
2024-04-19T14:34:03.6839877Z #7 13.47   Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:34:03.9527997Z #7 13.89 Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,019 kB]
2024-04-19T14:34:04.1130999Z #7 13.89 Err:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages
2024-04-19T14:34:04.1131939Z #7 13.89   
2024-04-19T14:34:04.1132399Z #7 13.90 Fetched 27.9 MB in 14s (2,015 kB/s)
2024-04-19T14:34:04.5336463Z #7 13.90 Reading package lists...
2024-04-19T14:34:04.5943961Z #7 14.48 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/universe/binary-amd64/Packages.gz  File has unexpected size (1369166 != 1369168). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:34:04.5946002Z #7 14.48    Hashes of expected file:
2024-04-19T14:34:04.5947353Z #7 14.48     - Filesize:1369168 [weak]
2024-04-19T14:34:04.5949131Z #7 14.48     - SHA256:50cd837b83825232586619378da6fbcf1f763f0c7fc8fb4100429822cbab2c23
2024-04-19T14:34:04.5950298Z #7 14.48     - SHA1:cdf607a214a03a5cc987b19407a45e9d382331fa [weak]
2024-04-19T14:34:04.5951254Z #7 14.48     - MD5Sum:252b19f60d48d9398f5966c8dcd0b08f [weak]
2024-04-19T14:34:04.5952120Z #7 14.48    Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:34:04.5953597Z #7 14.48 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/main/binary-amd64/Packages.gz  
2024-04-19T14:34:04.5955181Z #7 14.48 E: Some index files failed to download. They have been ignored, or old ones used instead.
2024-04-19T14:34:04.5957600Z #7 ERROR: process ""/bin/sh -c set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*"" did not complete successfully: exit code: 100
2024-04-19T14:34:04.5959334Z ------
2024-04-19T14:34:04.5960562Z  > [2/7] RUN set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*:
2024-04-19T14:34:04.5961627Z 
2024-04-19T14:34:04.5963326Z 14.48 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/universe/binary-amd64/Packages.gz  File has unexpected size (1369166 != 1369168). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:34:04.5965140Z 14.48    Hashes of expected file:
2024-04-19T14:34:04.5965755Z 14.48     - Filesize:1369168 [weak]
2024-04-19T14:34:04.5966671Z 14.48     - SHA256:50cd837b83825232586619378da6fbcf1f763f0c7fc8fb4100429822cbab2c23
2024-04-19T14:34:04.5967725Z 14.48     - SHA1:cdf607a214a03a5cc987b19407a45e9d382331fa [weak]
2024-04-19T14:34:04.5968615Z 14.48     - MD5Sum:252b19f60d48d9398f5966c8dcd0b08f [weak]
2024-04-19T14:34:04.5969656Z 14.48    Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:34:04.5971176Z 14.48 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/main/binary-amd64/Packages.gz  
2024-04-19T14:34:04.5972677Z 14.48 E: Some index files failed to download. They have been ignored, or old ones used instead.
2024-04-19T14:34:04.5973666Z ------
2024-04-19T14:34:04.5974028Z Dockerfile:22
2024-04-19T14:34:04.5974441Z --------------------
2024-04-19T14:34:04.5974891Z   21 |     # Install dependencies
2024-04-19T14:34:04.5975442Z   22 | >>> RUN set -ex; \
2024-04-19T14:34:04.5975938Z   23 | >>>   apt-get update; \
2024-04-19T14:34:04.5976803Z   24 | >>>   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev; \
2024-04-19T14:34:04.5977646Z   25 | >>>   rm -rf /var/lib/apt/lists/*
2024-04-19T14:34:04.5978621Z   26 |     
2024-04-19T14:34:04.5979052Z --------------------
2024-04-19T14:34:04.5980933Z ERROR: failed to solve: process ""/bin/sh -c set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*"" did not complete successfully: exit code: 100
2024-04-19T14:34:04.5997999Z Apr 19 14:34:04 ~/work/flink/flink
2024-04-19T14:34:04.5998916Z Apr 19 14:34:04 Command: build_image test_kubernetes_session 10.1.0.63 failed. Retrying...
2024-04-19T14:34:06.6017382Z Apr 19 14:34:06 Starting fileserver for Flink distribution
2024-04-19T14:34:06.6018973Z Apr 19 14:34:06 ~/work/flink/flink/flink-dist/target/flink-1.18-SNAPSHOT-bin ~/work/flink/flink
2024-04-19T14:34:22.1250175Z Apr 19 14:34:22 ~/work/flink/flink
2024-04-19T14:34:22.1252227Z Apr 19 14:34:22 ~/work/flink/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-15907928199 ~/work/flink/flink
2024-04-19T14:34:22.1255283Z Apr 19 14:34:22 Preparing Dockeriles
2024-04-19T14:34:22.1257161Z Apr 19 14:34:22 Executing command: git clone https://github.com/apache/flink-docker.git --branch dev-1.18 --single-branch
2024-04-19T14:34:22.1270759Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:34:22.1273940Z Apr 19 14:34:22 Retry 1/5 exited 128, retrying in 1 seconds...
2024-04-19T14:34:22.1946947Z Traceback (most recent call last):
2024-04-19T14:34:22.1948629Z   File ""/home/runner/work/flink/flink/flink-end-to-end-tests/test-scripts/python3_fileserver.py"", line 26, in <module>
2024-04-19T14:34:22.1950000Z     httpd = socketserver.TCPServer(("""", 9999), handler)
2024-04-19T14:34:22.1951598Z   File ""/usr/lib/python3.10/socketserver.py"", line 452, in __init__
2024-04-19T14:34:22.1952387Z     self.server_bind()
2024-04-19T14:34:22.1953086Z   File ""/usr/lib/python3.10/socketserver.py"", line 466, in server_bind
2024-04-19T14:34:22.1953925Z     self.socket.bind(self.server_address)
2024-04-19T14:34:22.1954556Z OSError: [Errno 98] Address already in use
2024-04-19T14:34:23.1305042Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:34:23.1308316Z Apr 19 14:34:23 Retry 2/5 exited 128, retrying in 2 seconds...
2024-04-19T14:34:25.1337258Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:34:25.1340530Z Apr 19 14:34:25 Retry 3/5 exited 128, retrying in 4 seconds...
2024-04-19T14:34:29.1370612Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:34:29.1374572Z Apr 19 14:34:29 Retry 4/5 exited 128, retrying in 8 seconds...
2024-04-19T14:34:37.1402285Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2024-04-19T14:34:37.1405010Z Apr 19 14:34:37 Retry 5/5 exited 128, no more retries left.
2024-04-19T14:34:37.1542869Z Generating Dockerfiles... done.
2024-04-19T14:34:37.1545358Z Apr 19 14:34:37 Building images
2024-04-19T14:34:37.4732168Z #0 building with ""default"" instance using docker driver
2024-04-19T14:34:37.4732793Z 
2024-04-19T14:34:37.4733349Z #1 [internal] load .dockerignore
2024-04-19T14:34:37.4733909Z #1 transferring context: 2B done
2024-04-19T14:34:37.4734431Z #1 DONE 0.0s
2024-04-19T14:34:37.4734685Z 
2024-04-19T14:34:37.4734974Z #2 [internal] load build definition from Dockerfile
2024-04-19T14:34:37.4735706Z #2 transferring dockerfile: 4.06kB done
2024-04-19T14:34:37.4736276Z #2 DONE 0.0s
2024-04-19T14:34:37.4736521Z 
2024-04-19T14:34:37.4737280Z #3 [internal] load metadata for docker.io/library/eclipse-temurin:8-jre-jammy
2024-04-19T14:34:37.6125541Z #3 DONE 0.3s
2024-04-19T14:34:37.8246150Z 
2024-04-19T14:34:37.8247889Z #4 [1/7] FROM docker.io/library/eclipse-temurin:8-jre-jammy@sha256:ed01c07af919817182c3ba5153f6c16a9a4719554610ee06bd40b8724be9e495
2024-04-19T14:34:37.8248716Z #4 CACHED
2024-04-19T14:34:37.8249133Z 
2024-04-19T14:34:37.8249265Z #5 [internal] load build context
2024-04-19T14:34:37.8249616Z #5 transferring context: 5.33kB done
2024-04-19T14:34:37.8250360Z #5 DONE 0.0s
2024-04-19T14:34:37.8250712Z 
2024-04-19T14:34:37.8251359Z #6 [2/7] RUN set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*
2024-04-19T14:34:37.8252111Z #6 0.059 + apt-get update
2024-04-19T14:34:37.9496116Z #6 0.334 Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]
2024-04-19T14:34:38.3850307Z #6 0.770 Get:2 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,739 kB]
2024-04-19T14:34:38.8486903Z #6 1.233 Get:3 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,228 kB]
2024-04-19T14:34:39.0971196Z #6 1.331 Get:4 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]
2024-04-19T14:34:39.0972195Z #6 1.332 Get:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]
2024-04-19T14:34:43.6469689Z #6 6.032 Get:6 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]
2024-04-19T14:34:44.9023918Z #6 7.287 Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]
2024-04-19T14:34:47.1965332Z #6 9.581 Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]
2024-04-19T14:34:47.5489469Z #6 9.934 Get:9 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]
2024-04-19T14:34:47.7884282Z #6 10.17 Get:10 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]
2024-04-19T14:34:47.9099440Z #6 10.29 Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]
2024-04-19T14:34:48.7121093Z #6 11.10 Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1,792 kB]
2024-04-19T14:34:48.8957785Z #6 11.20 Ign:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages
2024-04-19T14:34:48.8959237Z #6 11.28 Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,307 kB]
2024-04-19T14:34:49.0007073Z #6 11.39 Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.1 kB]
2024-04-19T14:34:49.1681160Z #6 11.47 Ign:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages
2024-04-19T14:34:49.1682159Z #6 11.55 Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.3 kB]
2024-04-19T14:34:49.3361032Z #6 11.64 Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [80.9 kB]
2024-04-19T14:34:49.3362101Z #6 11.72 Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]
2024-04-19T14:34:49.4876033Z #6 11.72 Err:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages
2024-04-19T14:34:49.4877389Z #6 11.72   File has unexpected size (1369166 != 1369168). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:34:49.4877992Z #6 11.72   Hashes of expected file:
2024-04-19T14:34:49.4878426Z #6 11.72    - Filesize:1369168 [weak]
2024-04-19T14:34:49.4879026Z #6 11.72    - SHA256:50cd837b83825232586619378da6fbcf1f763f0c7fc8fb4100429822cbab2c23
2024-04-19T14:34:49.4879802Z #6 11.72    - SHA1:cdf607a214a03a5cc987b19407a45e9d382331fa [weak]
2024-04-19T14:34:49.4880368Z #6 11.72    - MD5Sum:252b19f60d48d9398f5966c8dcd0b08f [weak]
2024-04-19T14:34:49.4880883Z #6 11.72   Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:34:49.6446821Z #6 12.03 Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,019 kB]
2024-04-19T14:34:49.8054282Z #6 12.03 Err:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages
2024-04-19T14:34:49.8055282Z #6 12.03   
2024-04-19T14:34:49.8055745Z #6 12.04 Fetched 27.9 MB in 12s (2,330 kB/s)
2024-04-19T14:34:50.2894451Z #6 12.04 Reading package lists...
2024-04-19T14:34:50.3607021Z #6 12.68 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/universe/binary-amd64/Packages.gz  File has unexpected size (1369166 != 1369168). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:34:50.3609240Z #6 12.68    Hashes of expected file:
2024-04-19T14:34:50.3610239Z #6 12.68     - Filesize:1369168 [weak]
2024-04-19T14:34:50.3611510Z #6 12.68     - SHA256:50cd837b83825232586619378da6fbcf1f763f0c7fc8fb4100429822cbab2c23
2024-04-19T14:34:50.3612635Z #6 12.68     - SHA1:cdf607a214a03a5cc987b19407a45e9d382331fa [weak]
2024-04-19T14:34:50.3613616Z #6 12.68     - MD5Sum:252b19f60d48d9398f5966c8dcd0b08f [weak]
2024-04-19T14:34:50.3614490Z #6 12.68    Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:34:50.3615953Z #6 12.68 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/main/binary-amd64/Packages.gz  
2024-04-19T14:34:50.3617483Z #6 12.68 E: Some index files failed to download. They have been ignored, or old ones used instead.
2024-04-19T14:34:50.3619915Z #6 ERROR: process ""/bin/sh -c set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*"" did not complete successfully: exit code: 100
2024-04-19T14:34:50.3621626Z ------
2024-04-19T14:34:50.3622867Z  > [2/7] RUN set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*:
2024-04-19T14:34:50.3623953Z 
2024-04-19T14:34:50.3625670Z 12.68 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/universe/binary-amd64/Packages.gz  File has unexpected size (1369166 != 1369168). Mirror sync in progress? [IP: 91.189.91.83 80]
2024-04-19T14:34:50.3627715Z 12.68    Hashes of expected file:
2024-04-19T14:34:50.3628370Z 12.68     - Filesize:1369168 [weak]
2024-04-19T14:34:50.3629338Z 12.68     - SHA256:50cd837b83825232586619378da6fbcf1f763f0c7fc8fb4100429822cbab2c23
2024-04-19T14:34:50.3630454Z 12.68     - SHA1:cdf607a214a03a5cc987b19407a45e9d382331fa [weak]
2024-04-19T14:34:50.3631383Z 12.68     - MD5Sum:252b19f60d48d9398f5966c8dcd0b08f [weak]
2024-04-19T14:34:50.3632230Z 12.68    Release file created at: Fri, 19 Apr 2024 13:54:56 +0000
2024-04-19T14:34:50.3633670Z 12.68 E: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/main/binary-amd64/Packages.gz  
2024-04-19T14:34:50.3635118Z 12.68 E: Some index files failed to download. They have been ignored, or old ones used instead.
2024-04-19T14:34:50.3636035Z ------
2024-04-19T14:34:50.3636404Z Dockerfile:22
2024-04-19T14:34:50.3636849Z --------------------
2024-04-19T14:34:50.3637339Z   21 |     # Install dependencies
2024-04-19T14:34:50.3637928Z   22 | >>> RUN set -ex; \
2024-04-19T14:34:50.3638482Z   23 | >>>   apt-get update; \
2024-04-19T14:34:50.3639334Z   24 | >>>   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev; \
2024-04-19T14:34:50.3640236Z   25 | >>>   rm -rf /var/lib/apt/lists/*
2024-04-19T14:34:50.3640763Z   26 |     
2024-04-19T14:34:50.3641150Z --------------------
2024-04-19T14:34:50.3642875Z ERROR: failed to solve: process ""/bin/sh -c set -ex;   apt-get update;   apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev;   rm -rf /var/lib/apt/lists/*"" did not complete successfully: exit code: 100
2024-04-19T14:34:50.3670846Z Apr 19 14:34:50 ~/work/flink/flink
2024-04-19T14:34:50.3671796Z Apr 19 14:34:50 Command: build_image test_kubernetes_session 10.1.0.63 failed. Retrying...
2024-04-19T14:34:52.3689189Z Apr 19 14:34:52 Command: build_image test_kubernetes_session 10.1.0.63 failed 3 times.
2024-04-19T14:34:52.3690109Z Apr 19 14:34:52 ERROR: Could not build image. Aborting...
2024-04-19T14:34:52.3714360Z Apr 19 14:34:52 Stopping job timeout watchdog (with pid=187311)
2024-04-19T14:34:52.3737049Z Apr 19 14:34:52 No watchdog process with pid=187311 present, anymore. No action required to clean the watchdog process up.
2024-04-19T14:34:52.3756941Z Apr 19 14:34:52 No watchdog process with pid=187311 present, anymore. No action required to clean the watchdog process up.
2024-04-19T14:34:52.3758222Z Apr 19 14:34:52 Debugging failed Kubernetes test:
2024-04-19T14:34:52.3758987Z Apr 19 14:34:52 Currently existing Kubernetes resources
2024-04-19T14:34:52.4432745Z Apr 19 14:34:52 NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
2024-04-19T14:34:52.4434525Z Apr 19 14:34:52 service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   14m
2024-04-19T14:34:52.5147025Z Apr 19 14:34:52 Name:              kubernetes
2024-04-19T14:34:52.5148161Z Apr 19 14:34:52 Namespace:         default
2024-04-19T14:34:52.5149199Z Apr 19 14:34:52 Labels:            component=apiserver
2024-04-19T14:34:52.5150223Z Apr 19 14:34:52                    provider=kubernetes
2024-04-19T14:34:52.5150956Z Apr 19 14:34:52 Annotations:       <none>
2024-04-19T14:34:52.5151596Z Apr 19 14:34:52 Selector:          <none>
2024-04-19T14:34:52.5152231Z Apr 19 14:34:52 Type:              ClusterIP
2024-04-19T14:34:52.5152910Z Apr 19 14:34:52 IP Family Policy:  SingleStack
2024-04-19T14:34:52.5153570Z Apr 19 14:34:52 IP Families:       IPv4
2024-04-19T14:34:52.5154182Z Apr 19 14:34:52 IP:                10.96.0.1
2024-04-19T14:34:52.5154815Z Apr 19 14:34:52 IPs:               10.96.0.1
2024-04-19T14:34:52.5155505Z Apr 19 14:34:52 Port:              https  443/TCP
2024-04-19T14:34:52.5156214Z Apr 19 14:34:52 TargetPort:        8443/TCP
2024-04-19T14:34:52.5156893Z Apr 19 14:34:52 Endpoints:         10.1.0.63:8443
2024-04-19T14:34:52.5157551Z Apr 19 14:34:52 Session Affinity:  None
2024-04-19T14:34:52.5158160Z Apr 19 14:34:52 Events:            <none>
2024-04-19T14:34:52.5173307Z Apr 19 14:34:52 Flink logs:
2024-04-19T14:34:52.6199811Z Error from server (NotFound): deployments.apps ""flink-native-k8s-session-1"" not found
2024-04-19T14:34:52.6684592Z Error from server (NotFound): clusterrolebindings.rbac.authorization.k8s.io ""flink-role-binding-default"" not found
2024-04-19T14:34:52.7169754Z Apr 19 14:34:52 Stopping minikube ...
2024-04-19T14:34:52.7837442Z Apr 19 14:34:52 * Stopping node ""minikube""  ...
2024-04-19T14:34:57.9784290Z Apr 19 14:34:57 * 1 node stopped.
2024-04-19T14:34:57.9822439Z Apr 19 14:34:57 [FAIL] Test script contains errors.
2024-04-19T14:34:57.9829276Z Apr 19 14:34:57 Checking for errors...
2024-04-19T14:34:57.9994278Z Apr 19 14:34:57 No errors in log files.
2024-04-19T14:34:57.9995033Z Apr 19 14:34:57 Checking for exceptions...
2024-04-19T14:34:58.0181145Z Apr 19 14:34:58 No exceptions in log files.
2024-04-19T14:34:58.0182302Z Apr 19 14:34:58 Checking for non-empty .out files...
2024-04-19T14:34:58.0200187Z grep: /home/runner/work/_temp/debug_files/flink-logs/*.out: No such file or directory
2024-04-19T14:34:58.0203658Z Apr 19 14:34:58 No non-empty .out files.
2024-04-19T14:34:58.0204722Z Apr 19 14:34:58 
2024-04-19T14:34:58.0206128Z Apr 19 14:34:58 [FAIL] 'Run kubernetes session test (custom fs plugin)' failed after 6 minutes and 41 seconds! Test exited with exit code 1
2024-04-19T14:34:58.0207427Z Apr 19 14:34:58 
{code}

Notably the {{Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/main/binary-amd64/Packages.gz  File has unexpected size (2018947 != 2018948). Mirror sync in progress? [IP: 91.189.91.83 80]}}. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 08:32:44.0,,,,,,,,,,"0|z1osbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Desc table syntax support materialized table,FLINK-35206,13576716,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,22/Apr/24 08:26,22/Apr/24 09:59,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,,,"The desc syntax supports materialized table:

{code:SQL}
DESC Orders
{code}

Return the following description:

root
 |-- user: BIGINT NOT NULL COMMENT 'this is primary key'
 |-- product: VARCHAR(32)
 |-- amount: INT
 |-- ts: TIMESTAMP(3) *ROWTIME* COMMENT 'notice: watermark'
 |-- ptime: TIMESTAMP(3) NOT NULL *PROCTIME* AS PROCTIME() COMMENT 'this is a computed column'
 |-- WATERMARK FOR ts AS `ts` - INTERVAL '1' SECOND
 |-- CONSTRAINT PK_3599338 PRIMARY KEY (user)
 |-- LOGICAL_REFRESH_MODE: FULL
 |-- REFRESH_MODE: FULL
 |-- REFRESH_STATUS: ACTIVATED
 |-- REFRESH_HANDLER_DESC: ""{jobId: xxx}""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 08:26:16.0,,,,,,,,,,"0|z1osbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the execution of alter materialized table reset options,FLINK-35205,13576714,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,22/Apr/24 08:19,22/Apr/24 08:44,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / API,Table SQL / Gateway,,,0,,,"
{code:SQL}
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name RESET ('key');
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 08:19:41.0,,,,,,,,,,"0|z1osaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the execution of alter materialized table set options,FLINK-35204,13576713,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,22/Apr/24 08:18,22/Apr/24 08:43,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / API,Table SQL / Gateway,,,0,,,"
{code:SQL}
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name  SET ('key' = 'val');
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 08:18:40.0,,,,,,,,,,"0|z1osao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the execution of alter materialized table set refresh mode,FLINK-35203,13576712,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,22/Apr/24 08:17,22/Apr/24 08:42,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / API,Table SQL / Gateway,,,0,,,"
{code:SQL}
ALTER MATERIALIZED TABLE  [catalog_name.][db_name.]table_name 
 
  SET REFRESH_MODE = { FULL | CONTINUOUS }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 08:17:31.0,,,,,,,,,,"0|z1osag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the execution of alter materialized table set freshness,FLINK-35202,13576711,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,22/Apr/24 08:15,22/Apr/24 08:41,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / API,Table SQL / Gateway,,,0,,,"
{code:SQL}
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name 
 
  SET FRESHNESS = INTERVAL '<num>' { SECOND | MINUTE | HOUR | DAY }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 08:15:30.0,,,,,,,,,,"0|z1osa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the execution of drop materialized table in full refresh mode,FLINK-35201,13576710,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,22/Apr/24 08:14,03/Jun/24 06:26,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Gateway,,,0,,,"In full refresh mode, support drop materialized table and the background refresh workflow.
{code:SQL}
DROP MATERIALIZED TABLE [ IF EXISTS ] [catalog_name.][db_name.]table_name
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 06:26:40 UTC 2024,,,,,,,,,,"0|z1osa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/24 06:26;hackergin;[~lsy]  I would like to take this ticket. Please assign it to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support the execution of suspend, resume materialized table in full refresh mode",FLINK-35200,13576709,13576688,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hackergin,lsy,lsy,22/Apr/24 08:13,03/Jun/24 15:03,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Gateway,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 08:13:16.0,,,,,,,,,,"0|z1os9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the execution of create materialized table in full refresh mode,FLINK-35199,13576708,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,lsy,lsy,22/Apr/24 08:11,01/Jun/24 14:53,04/Jun/24 20:40,01/Jun/24 14:53,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Gateway,,,0,pull-request-available,,"In full refresh mode, support creates materialized table and its background refresh workflow:
{code:SQL}
CREATE MATERIALIZED TABLE [catalog_name.][db_name.]table_name
 
[ ([ <table_constraint> ]) ]
 
[COMMENT table_comment]
 
[PARTITIONED BY (partition_column_name1, partition_column_name2, ...)]
 
[WITH (key1=val1, key2=val2, ...)]
 
FRESHNESS = INTERVAL '<num>' { SECOND | MINUTE | HOUR | DAY }
 
[REFRESH_MODE = { CONTINUOUS | FULL }]
 
AS <select_statement>
 
<table_constraint>:
  [CONSTRAINT constraint_name] PRIMARY KEY (column_name, ...) NOT ENFORCED
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 01 14:51:16 UTC 2024,,,,,,,,,,"0|z1os9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/24 13:11;liyubin117;[~lsy] Hi, This indeed is a valuable feature, Could I take this ?;;;","29/May/24 09:32;lsy;[~liyubin117] Hi, sorry for late response, [~hackergin] is doing this ticket now. You can take other related issues after this ticket is finished.;;;","01/Jun/24 14:51;lsy;Merged in master: e4fa72d9e480664656818395741c37a9995f9334;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support the execution of refresh materialized table,FLINK-35198,13576706,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,lsy,lsy,22/Apr/24 08:09,11/May/24 13:12,04/Jun/24 20:40,11/May/24 13:12,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,pull-request-available,,"
{code:SQL}
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name REFRESH
 [PARTITION (key1=val1, key2=val2, ...)]
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 11 13:12:21 UTC 2024,,,,,,,,,,"0|z1os94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 01:52;xuyangzhong;Hi, [~lsy] Can I take this jira?;;;","11/May/24 13:12;lsy;Merged in master: 9fe8d7bf870987bf43bad63078e2590a38e4faf6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support the execution of suspend, resume materialized table in continuous refresh mode",FLINK-35197,13576705,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,lsy,lsy,22/Apr/24 08:08,13/May/24 01:50,04/Jun/24 20:40,13/May/24 01:50,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Gateway,,,0,pull-request-available,,"In continuous refresh mode, support suspend, resume the background refresh job of materialized table.

{code:SQL}
// suspend
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name SUSPEND

// resume
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name RESUME
[WITH('key1' = 'val1', 'key2' = 'val2')]
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 01:50:08 UTC 2024,,,,,,,,,,"0|z1os8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 01:50;lsy;Merged in master: e4972c003f68da6dc4066459d4c6e5d981f07e96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
bouncycastle class not found for flink pulsar connector,FLINK-35196,13576703,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Problem,,shenwenbing,shenwenbing,22/Apr/24 08:05,28/Apr/24 07:56,04/Jun/24 20:40,28/Apr/24 06:50,pulsar-4.1.0,,,,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,"2024-04-22 14:45:48
java.lang.NoClassDefFoundError: org/apache/pulsar/shade/org/bouncycastle/util/Arrays
    at org.apache.flink.connector.pulsar.table.sink.PulsarWritableMetadata.readMetadata(PulsarWritableMetadata.java:67)
    at org.apache.flink.connector.pulsar.table.sink.PulsarWritableMetadata.applyWritableMetadataInMessage(PulsarWritableMetadata.java:55)
    at org.apache.flink.connector.pulsar.table.sink.PulsarTableSerializationSchema.serialize(PulsarTableSerializationSchema.java:106)
    at org.apache.flink.connector.pulsar.table.sink.PulsarTableSerializationSchema.serialize(PulsarTableSerializationSchema.java:40)
    at org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.write(PulsarWriter.java:143)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:160)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 08:05:17.0,,,,,,,,,,"0|z1os8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the execution of create materialized table in continuous refresh mode,FLINK-35195,13576702,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,22/Apr/24 08:04,07/May/24 12:29,04/Jun/24 20:40,07/May/24 12:29,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Gateway,,,0,pull-request-available,,"In continuous refresh mode, support creates materialized table and its background refresh job:
{code:SQL}
CREATE MATERIALIZED TABLE [catalog_name.][db_name.]table_name
 
[ ([ <table_constraint> ]) ]
 
[COMMENT table_comment]
 
[PARTITIONED BY (partition_column_name1, partition_column_name2, ...)]
 
[WITH (key1=val1, key2=val2, ...)]
 
FRESHNESS = INTERVAL '<num>' { SECOND | MINUTE | HOUR | DAY }
 
[REFRESH_MODE = { CONTINUOUS | FULL }]
 
AS <select_statement>
 
<table_constraint>:
  [CONSTRAINT constraint_name] PRIMARY KEY (column_name, ...) NOT ENFORCED
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 12:29:24 UTC 2024,,,,,,,,,,"0|z1os88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 12:29;lsy;Merged in master: 29736b8c01924b7da03d4bcbfd9c812a8e5a08b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support describe job syntax and execution,FLINK-35194,13576701,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,lsy,lsy,22/Apr/24 08:02,30/Apr/24 06:41,04/Jun/24 20:40,30/Apr/24 06:41,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,pull-request-available,,"
{code:java}
{ DESCRIBE | DESC } JOB 'xxx'
{code}
",,,,,,,,,,,,,,,,,,,,,,,FLINK-35271,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 06:41:41 UTC 2024,,,,,,,,,,"0|z1os80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 09:06;xuyangzhong;Hi, can I take this jira?;;;","26/Apr/24 09:35;lsy;Yeah, assigned to you.;;;","30/Apr/24 06:41;lsy;Merged in master: 44528e0ee9fbed11b5417253534078d60fed3a12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support drop materialized table syntax and execution in continuous refresh mode,FLINK-35193,13576699,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,lsy,lsy,22/Apr/24 08:00,14/May/24 13:00,04/Jun/24 20:40,14/May/24 13:00,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,pull-request-available,,"In continuous refresh mode, support drop materialized table and the background refresh job.
{code:SQL}
DROP MATERIALIZED TABLE [ IF EXISTS ] [catalog_name.][db_name.]table_name
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 12:59:53 UTC 2024,,,,,,,,,,"0|z1os7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 01:52;xuyangzhong;Hi, [~lsy] I'd like to take this task.;;;","14/May/24 12:59;lsy;Merged in master: 94d861b08fef1e350d80a3f5f0f63168d327bc64;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator oom,FLINK-35192,13576698,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,stupid_pig,stupid_pig,22/Apr/24 08:00,31/May/24 14:32,04/Jun/24 20:40,31/May/24 14:32,kubernetes-operator-1.6.1,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,pull-request-available,,"The kubernetest operator docker process was killed by kernel cause out of memory(the time is 2024.04.03: 18:16)


 !image-2024-04-22-15-47-49-455.png! 


Metrics:

the pod memory (RSS) is increasing slowly in the past 7 days:
 !screenshot-1.png! 


However the jvm memory metrics of operator not shown obvious anomaly：
 !image-2024-04-22-15-58-23-269.png! 
 !image-2024-04-22-15-58-42-850.png! 

","jdk: openjdk11
operator version: 1.6.1
",,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/24 07:47;stupid_pig;image-2024-04-22-15-47-49-455.png;https://issues.apache.org/jira/secure/attachment/13068344/image-2024-04-22-15-47-49-455.png","22/Apr/24 07:52;stupid_pig;image-2024-04-22-15-52-51-600.png;https://issues.apache.org/jira/secure/attachment/13068343/image-2024-04-22-15-52-51-600.png","22/Apr/24 07:58;stupid_pig;image-2024-04-22-15-58-23-269.png;https://issues.apache.org/jira/secure/attachment/13068342/image-2024-04-22-15-58-23-269.png","22/Apr/24 07:58;stupid_pig;image-2024-04-22-15-58-42-850.png;https://issues.apache.org/jira/secure/attachment/13068341/image-2024-04-22-15-58-42-850.png","30/Apr/24 08:47;stupid_pig;image-2024-04-30-16-47-07-289.png;https://issues.apache.org/jira/secure/attachment/13068555/image-2024-04-30-16-47-07-289.png","30/Apr/24 09:11;stupid_pig;image-2024-04-30-17-11-24-974.png;https://issues.apache.org/jira/secure/attachment/13068556/image-2024-04-30-17-11-24-974.png","30/Apr/24 12:38;stupid_pig;image-2024-04-30-20-38-25-195.png;https://issues.apache.org/jira/secure/attachment/13068566/image-2024-04-30-20-38-25-195.png","30/Apr/24 12:39;stupid_pig;image-2024-04-30-20-39-05-109.png;https://issues.apache.org/jira/secure/attachment/13068565/image-2024-04-30-20-39-05-109.png","30/Apr/24 12:39;stupid_pig;image-2024-04-30-20-39-34-396.png;https://issues.apache.org/jira/secure/attachment/13068564/image-2024-04-30-20-39-34-396.png","30/Apr/24 12:41;stupid_pig;image-2024-04-30-20-41-51-660.png;https://issues.apache.org/jira/secure/attachment/13068563/image-2024-04-30-20-41-51-660.png","30/Apr/24 12:43;stupid_pig;image-2024-04-30-20-43-20-125.png;https://issues.apache.org/jira/secure/attachment/13068562/image-2024-04-30-20-43-20-125.png","22/Apr/24 08:05;stupid_pig;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13068345/screenshot-1.png","22/Apr/24 08:12;stupid_pig;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13068346/screenshot-2.png","26/Apr/24 09:44;bgeng777;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13068467/screenshot-3.png","08/May/24 04:12;stupid_pig;screenshot-4.png;https://issues.apache.org/jira/secure/attachment/13068724/screenshot-4.png",,15.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 11:41:04 UTC 2024,,,,,,,,,,"0|z1os7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 08:17;stupid_pig;the yaml spec:
{code:yaml}
apiVersion: apps/v1
kind: Deployment
metadata:   annotations:     deployment.kubernetes.io/revision: ""3""
    meta.helm.sh/release-name: flink-kubernetes-operator
    meta.helm.sh/release-namespace: streamfly
  creationTimestamp: ""2024-03-13T02:55:09Z""
  generation: 3
  labels:     app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink-kubernetes-operator
    app.kubernetes.io/version: 1.6.1-GDC1.0.2
    helm.sh/chart: flink-kubernetes-operator-1.6.1-GDC1.0.2
  name: flink-kubernetes-operator
  namespace: streamfly
  resourceVersion: ""8064936654""
  uid: 00418b62-820f-4e4a-a138-1ff81f605787
spec:   progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:     matchLabels:       app.kubernetes.io/name: flink-kubernetes-operator
  strategy:     rollingUpdate:       maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:     metadata:       annotations:         kubectl.kubernetes.io/default-container: flink-kubernetes-operator
      creationTimestamp: null
      labels:         app.kubernetes.io/name: flink-kubernetes-operator
    spec:       containers:       - command:         - /docker-entrypoint.sh
        - operator
        env:         - name: OPERATOR_NAMESPACE
          valueFrom:             fieldRef:               apiVersion: v1
              fieldPath: metadata.namespace
        - name: HOST_IP
          valueFrom:             fieldRef:               apiVersion: v1
              fieldPath: status.hostIP
        - name: POD_IP
          valueFrom:             fieldRef:               apiVersion: v1
              fieldPath: status.podIP
        - name: POD_NAME
          valueFrom:             fieldRef:               apiVersion: v1
              fieldPath: metadata.name
        - name: OPERATOR_NAME
          value: flink-kubernetes-operator
        - name: FLINK_CONF_DIR
          value: /opt/flink/conf
        - name: FLINK_PLUGINS_DIR
          value: /opt/flink/plugins
        - name: LOG_CONFIG
          value: -Dlog4j.configurationFile=/opt/flink/conf/log4j-operator.properties
        - name: JVM_ARGS
          value: -Xmx32g -Xms32g -XX:+UseG1GC
        - name: TZ
          value: Asia/Shanghai
        image: ncr.nie.netease.com/v1-gdcstreaming/gdc-flink-kubernetes-operator:1.6.1-GDC1.0.2
        imagePullPolicy: IfNotPresent
        livenessProbe:           failureThreshold: 3
          httpGet:             path: /
            port: health-port
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: flink-kubernetes-operator
        ports:         - containerPort: 9999
          name: metrics
          protocol: TCP
        - containerPort: 8085
          name: health-port
          protocol: TCP
        resources:           limits:             cpu: ""10""
            memory: 35Gi
          requests:             cpu: ""10""
            memory: 35Gi
        securityContext: {}
        startupProbe:           failureThreshold: 30
          httpGet:             path: /
            port: health-port
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:         - mountPath: /opt/flink/conf
          name: flink-operator-config-volume
        - mountPath: /opt/scheduler/keytab
          name: flink-operator-keytab-volume
        - mountPath: /flink-data
          name: flink-operator-logs-volume
      - command:         - /docker-entrypoint.sh
        - webhook
        env:         - name: WEBHOOK_KEYSTORE_PASSWORD
          valueFrom:             secretKeyRef:               key: password
              name: flink-operator-webhook-secret
        - name: WEBHOOK_KEYSTORE_FILE
          value: /certs/keystore.p12
        - name: WEBHOOK_KEYSTORE_TYPE
          value: pkcs12
        - name: WEBHOOK_SERVER_PORT
          value: ""9443""
        - name: LOG_CONFIG
          value: -Dlog4j.configurationFile=/opt/flink/conf/log4j-operator.properties
        - name: JVM_ARGS
        - name: FLINK_CONF_DIR
          value: /opt/flink/conf
        - name: FLINK_PLUGINS_DIR
          value: /opt/flink/plugins
        - name: OPERATOR_NAMESPACE
          valueFrom:             fieldRef:               apiVersion: v1
              fieldPath: metadata.namespace
        image: ncr.nie.netease.com/v1-gdcstreaming/gdc-flink-kubernetes-operator:1.6.1-GDC1.0.2
        imagePullPolicy: IfNotPresent
        name: flink-webhook
        resources: {}
        securityContext: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:         - mountPath: /certs
          name: keystore
          readOnly: true
        - mountPath: /opt/flink/conf
          name: flink-operator-config-volume
      dnsPolicy: ClusterFirst
      imagePullSecrets:       - name: ncr-pull-secret
      nodeSelector:         node-role.kubernetes.io/edge: """"
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:         runAsGroup: 0
        runAsUser: 0
      serviceAccount: flink-operator
      serviceAccountName: flink-operator
      terminationGracePeriodSeconds: 30
      volumes:       - configMap:           defaultMode: 420
          items:           - key: flink-conf.yaml
            path: flink-conf.yaml
          - key: log4j-operator.properties
            path: log4j-operator.properties
          - key: log4j-console.properties
            path: log4j-console.properties
          name: flink-operator-config
        name: flink-operator-config-volume
      - hostPath:           path: /cfs/flink/keytab
          type: Directory
        name: flink-operator-keytab-volume
      - hostPath:           path: /home/k8s/logs
          type: DirectoryOrCreate
        name: flink-operator-logs-volume
      - name: keystore
        secret:           defaultMode: 420
          items:           - key: keystore.p12
            path: keystore.p12
          secretName: webhook-server-cert
status:   availableReplicas: 2
  conditions:   - lastTransitionTime: ""2024-03-13T02:55:09Z""
    lastUpdateTime: ""2024-03-19T06:48:09Z""
    message: ReplicaSet ""flink-kubernetes-operator-7756945f7"" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: ""True""
    type: Progressing
  - lastTransitionTime: ""2024-04-19T04:08:21Z""
    lastUpdateTime: ""2024-04-19T04:08:21Z""
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: ""True""
    type: Available
  observedGeneration: 3
  readyReplicas: 2
  replicas: 2
  updatedReplicas: 2
{code}
and try to analyze the heap memory using MAT, here are the results of the analysis

!screenshot-2.png!

 

It seems to point to a bug in the jdk [deleteOnExit Api|https://bugs.openjdk.org/browse/JDK-4513817], but the theory from this bug is that it would result in not enough heap memory, whereas according to the jvm memory metrics, there is enough heap memory for an exception exit. It's strange.;;;","22/Apr/24 08:22;stupid_pig; Could you help to have look at this operator problem when you have time? I'd be very grateful.

[~gyfora];;;","26/Apr/24 09:51;bgeng777; !screenshot-3.png! 
According to the flink k8s op's codes, the deleteOnExit() is called when create config files or pod template files. It looks like that it is possible to lead the memory leak if the operator pod runs for a long time. In the operator's FlinkConfigManager implementation, we would clean up these temp files/dirs. Maybe we can safely remove the deleteOnExit() usage? cc [~gyfora]

Also, from the attached yaml, it looks like a custom flink k8s op image(gdc-flink-kubernetes-operator:1.6.1-GDC1.0.2) is used.  [~stupid_pig] would you mind checking if your codes call methods like deleteOnExit if you have some customized changes to the operator?;;;","30/Apr/24 05:31;gyfora;That makes sense [~bgeng777] this would be a good improvement;;;","30/Apr/24 08:32;bgeng777;Sure, created a [pr|https://github.com/apache/flink-kubernetes-operator/pull/822] for this.;;;","30/Apr/24 09:16;stupid_pig; 

The op image we use is to modify the basic image of the DockerFile file from eclipse-temurin:11-jre-jammy to maven:3.8.4-eclipse-temurin-11 (convenient to use jdk tools). There is no modification at the code level [~bgeng777] 

Recently,  we using pmap cmd observed the memory growth of operator and made a comparison  and checking the process rss monitoring of the op host. The comparison time was 04.24 20:00-04.25 01:20.

The process rss occupation increased by 95 MB(34.725G-34.630G):

!image-2024-04-30-20-39-34-396.png|width=611,height=343!


!image-2024-04-30-20-39-05-109.png|width=678,height=397!

 

Observe the pmap results at these two time points and compare:

!https://office.netease.com/api/admin/file/download?path=cowork/2024/04/25/38d9386430814e8282683930ab72daea.png|width=682,height=148!
 

 

It can be found that the growth mainly comes from the front memory area, and there are many 65536KB memory blocks. At the same time, the front memory area increased from 74 MB (35444 KB). + 39668 KB) grows to

165 MB (65536 KB + 65536 KB + 35708 KB),  the increaing result  91MB（165MB - 74MB）basically matching the memory growth of the process.

Referring to an issue in hadoop, it is suspected that glibc has a memory leak due to memory fragmentation in multiple arenas under multi-threading. At present, we refer to the [DockerFile|https://github.com/apache/flink-docker/blob/master/1.18/scala_2.12-java11-ubuntu/Dockerfile#L24] of flink-docker, which using jemalloc as memory assigner.

We replacing the glibc used by the operator with jemalloc4.5, and adjust the Xmx of the operator from 32g to 5.25g.

Here is the memory usage of the process after adjustment:

 

!image-2024-04-30-20-43-20-125.png|width=632,height=250!

 

Therefore, it is suspected that it is caused by a memory leak in glibc in a multi-threaded environment. Does the community consider changing the memory allocation of the op from glibc to jemalloc? If so, I will be happy to provide a PR to optimize this issue.

[~gyfora]

 

ENV

glibc-version: 2.31

jemalloc-version: 4.5;;;","30/Apr/24 09:56;bgeng777;[~stupid_pig], it looks like that pictures of the pmap is broken. But the glibc issue is well known, if that's the case, it makes sense to me to introduce jemalloc.;;;","30/Apr/24 12:45;stupid_pig;It's probably not that the images aren't broken, but that they're too big and I've re-scaled them to view them
[~bgeng777] ;;;","01/May/24 11:29;gyfora;Merged b1f4b3f35f907e1f6425ad2401dbf24309e6fb59 to main;;;","08/May/24 06:04;stupid_pig;In our environment, the operator has been running for more than ten days after switching to jemalloc. We can see that rss will not keep growing, basically confirm that OOM is caused by glibc memory fragmentation.

!screenshot-4.png|width=794,height=253!

I have submitted the relevant code changes back to the community.;;;","21/May/24 06:24;gaborgsomogyi;8b789ee on main;;;","21/May/24 06:33;gaborgsomogyi;[~stupid_pig] I've read through the whole conversation here. Do I understand correctly that this jira can be resolved?;;;","31/May/24 11:41;stupid_pig;I've picked up the relevant pr and running it in my local environment for more than 1 week without any further OOM issues. Thus I think this Jira has been solved!
 
[~gaborgsomogyi] 
 
cc [~gyfora] [~bgeng777] ;;;",,,,,,,,,,,,,,,,
"Support alter materialized table related syntaxes: suspend, resume, refresh, set and reset",FLINK-35191,13576697,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,lsy,lsy,22/Apr/24 07:59,30/Apr/24 01:32,04/Jun/24 20:40,30/Apr/24 01:32,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,pull-request-available,,"SQL statement as follows:

{code:SQL}
// suspend
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name SUSPEND

// resume
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name RESUME
 
[WITH('key1' = 'val1', 'key2' = 'val2')]

// refresh
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name REFRESH
 
[PARTITION (key1=val1, key2=val2, ...)]

// set freshness
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name 
 
  SET FRESHNESS = INTERVAL '<num>' { SECOND | MINUTE | HOUR | DAY }

// set refresh mode

ALTER MATERIALIZED TABLE  [catalog_name.][db_name.]table_name 
 
  SET REFRESH_MODE = { FULL | CONTINUOUS }

// set options
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name  SET ('key' = 'val');

// reset options
ALTER MATERIALIZED TABLE [catalog_name.][db_name.]table_name RESET ('key');
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 01:32:03 UTC 2024,,,,,,,,,,"0|z1os74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 09:01;hackergin;[~lsy] I would like to take this issue. Please assign this task to me. ;;;","22/Apr/24 09:48;lsy;Thanks for taking this ticket, assign to you.;;;","30/Apr/24 01:32;lsy;Merged in master: 330f524d185d575ceb679a6c587e9c39612e844c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support create materialized table syntax,FLINK-35190,13576695,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,lsy,lsy,22/Apr/24 07:55,26/Apr/24 02:14,04/Jun/24 20:40,26/Apr/24 01:55,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,pull-request-available,,"Create materialized table syntax:
{code:SQL}
CREATE MATERIALIZED TABLE [catalog_name.][db_name.]table_name
 
[ ([ <table_constraint> ]) ]
 
[COMMENT table_comment]
 
[PARTITIONED BY (partition_column_name1, partition_column_name2, ...)]
 
[WITH (key1=val1, key2=val2, ...)]
 
FRESHNESS = INTERVAL '<num>' { SECOND | MINUTE | HOUR | DAY }
 
[REFRESH_MODE = { CONTINUOUS | FULL }]
 
AS <select_statement>
 
<table_constraint>:
  [CONSTRAINT constraint_name] PRIMARY KEY (column_name, ...) NOT ENFORCED
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 01:55:29 UTC 2024,,,,,,,,,,"0|z1os6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 09:01;hackergin;[~lsy]  I would like to take this issue. Please assign this task to me.;;;","22/Apr/24 09:48;lsy;Thanks for taking this ticket, assign to you.;;;","26/Apr/24 01:55;lsy;Merged in master: 512f3fbaac8b538f7edda152af0846dc2b213557;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce test-filesystem Catalog based on FileSystem Connector to support materialized table,FLINK-35189,13576692,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,22/Apr/24 07:52,26/Apr/24 09:42,04/Jun/24 20:40,26/Apr/24 09:42,1.20.0,,,,,,,,1.20.0,,,Connectors / FileSystem,Table SQL / API,Tests,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 09:42:33 UTC 2024,,,,,,,,,,"0|z1os60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 09:42;lsy;Merged in master: 714d1cb2e0bd0df03393492dc87cbd800af63e1b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce CatalogMaterializedTable and related interface to support materialized table,FLINK-35188,13576691,13576688,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,22/Apr/24 07:50,25/Apr/24 01:25,04/Jun/24 20:40,25/Apr/24 01:25,1.20.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 01:25:44 UTC 2024,,,,,,,,,,"0|z1os5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 01:25;lsy;Merged in master: 8576178c4c084c38897e395479df11f15a4ea402;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-435: Introduce a New Materialized Table for Simplifying Data Pipelines,FLINK-35187,13576688,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lsy,lsy,lsy,22/Apr/24 07:39,14/May/24 05:32,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / API,Table SQL / Gateway,,,0,,,"This is an umbrella issue for FLIP-435: Introduce a New Materialized Table for Simplifying Data Pipelines, see FLIP design doc for more detail: https://cwiki.apache.org/confluence/display/FLINK/FLIP-435%3A+Introduce+a+New+Materialized+Table+for+Simplifying+Data+Pipelines",,,,,,,,,,,,,,,FLINK-35345,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 07:02:25 UTC 2024,,,,,,,,,,"0|z1os54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 03:03;fanrui;Hi [~lsy] , I wanna check with you, will this great feature be completed in 1.20.0 or 2.0.0? What's your plan?

If it's 1.20 , would you mind recording it into the 1.20 release doc[1]? thank you in advance.

[1] https://cwiki.apache.org/confluence/display/FLINK/1.20+Release

 ;;;","23/Apr/24 06:50;lsy;[~fanrui]Thanks for your reminder, we will complete the MVP version in 1.20.0. I will update the release doc.;;;","23/Apr/24 07:02;fanrui;Thank you very much for the feedback and update!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Create State V2 from new StateDescriptor,FLINK-35186,13576678,13574084,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,22/Apr/24 06:46,22/Apr/24 07:05,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,Runtime / Task,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 06:46:13.0,,,,,,,,,,"0|z1os2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Externalized Checkpoint(rocks, incremental, no parallelism change) end-to-end test failed ",FLINK-35185,13576667,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,22/Apr/24 01:51,22/Apr/24 01:54,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"
{code:java}
pr 21 00:52:11 Running externalized checkpoints test,   with ORIGINAL_DOP=2 NEW_DOP=2   and STATE_BACKEND_TYPE=rocks STATE_BACKEND_FILE_ASYNC=true   STATE_BACKEND_ROCKSDB_INCREMENTAL=true SIMULATE_FAILURE=false ...
Apr 21 00:52:20 Job (8a6bda88c7c422823bcf0d6f7a1e8cae) is running.
Apr 21 00:52:20 Waiting for job (8a6bda88c7c422823bcf0d6f7a1e8cae) to have at least 1 completed checkpoints ...
Apr 21 00:52:20 Waiting for job to process up to 200 records, current progress: 0 records ...
Apr 21 00:52:22 Waiting for job to process up to 200 records, current progress: 139 records ...
Apr 21 00:52:23 Waiting for job to process up to 200 records, current progress: 196 records ...
Apr 21 00:52:26 Cancelling job 8a6bda88c7c422823bcf0d6f7a1e8cae.
Apr 21 00:52:27 Cancelled job 8a6bda88c7c422823bcf0d6f7a1e8cae.
Apr 21 00:52:27 Restoring job with externalized checkpoint at /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-00385748458/externalized-chckpt-e2e-backend-dir/8a6bda88c7c422823bcf0d6f7a1e8cae/chk-8 ...
Apr 21 00:52:32 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:52:36 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:52:39 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:52:42 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:52:45 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:52:49 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:52:52 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:52:55 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:52:58 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:53:01 Job (47369cbdc25eea5a9cc6d26c91c4b141) is not yet running.
Apr 21 00:53:02 Job (47369cbdc25eea5a9cc6d26c91c4b141) has not started within a timeout of 10 sec
Apr 21 00:53:02 Stopping job timeout watchdog (with pid=173454)
Apr 21 00:53:02 [FAIL] Test script contains errors.
Apr 21 00:53:02 Checking of logs skipped.
Apr 21 00:53:02 
Apr 21 00:53:02 [FAIL] 'Resuming Externalized Checkpoint (rocks, incremental, no parallelism change) end-to-end test' failed after 1 minutes and 2 seconds! Test exited with exit code 1

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59046&view=logs&j=b31992a1-93b0-59f3-2c17-4a9deb43d11c&t=36dcb94b-d88d-5832-815b-4b36f2c7af14&l=3508
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-22 01:51:26.0,,,,,,,,,,"0|z1os0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hash collision inside MiniBatchStreamingJoin operator,FLINK-35184,13576651,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rovboyko,rovboyko,rovboyko,21/Apr/24 12:31,10/May/24 03:58,04/Jun/24 20:40,30/Apr/24 02:16,1.19.0,,,,,,,,1.20.0,,,Table SQL / Runtime,,,,0,pull-request-available,,"The hash collision is possible for InputSideHasNoUniqueKeyBundle. To reproduce it just launch the following test within StreamingMiniBatchJoinOperatorTest:

 
{code:java}
@Tag(""miniBatchSize=6"")
@Test
public void testInnerJoinWithNoUniqueKeyHashCollision(TestInfo testInfo) throws Exception {

    leftTypeInfo =
            InternalTypeInfo.of(
                    RowType.of(
                            new LogicalType[] {new IntType(), new BigIntType()},
                            new String[] {""id1"", ""val1""}));

    rightTypeInfo =
            InternalTypeInfo.of(
                    RowType.of(
                            new LogicalType[] {new IntType(), new BigIntType()},
                            new String[] {""id2"", ""val2""}));

    leftKeySelector =
            HandwrittenSelectorUtil.getRowDataSelector(
                    new int[] {0},
                    leftTypeInfo.toRowType().getChildren().toArray(new LogicalType[0]));
    rightKeySelector =
            HandwrittenSelectorUtil.getRowDataSelector(
                    new int[] {0},
                    rightTypeInfo.toRowType().getChildren().toArray(new LogicalType[0]));

    joinKeyTypeInfo = InternalTypeInfo.of(new IntType());

    super.beforeEach(testInfo);

    testHarness.setStateTtlProcessingTime(1);
    testHarness.processElement2(insertRecord(1, 1L));
    testHarness.processElement1(insertRecord(1, 4294967296L));
    testHarness.processElement2(insertRecord(1, 4294967296L));
    testHarness.processElement2(deleteRecord(1, 1L));

    testHarness.close();

    assertor.shouldEmit(testHarness, rowOfKind(RowKind.INSERT, 1, 4294967296L, 1, 4294967296L));
} {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 03:58:16 UTC 2024,,,,,,,,,,"0|z1orww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/24 12:35;rovboyko;please assign this bug on me, I'm working on it.;;;","22/Apr/24 03:46;xu_shuai_;Hi [~rovboyko] , thx for reporting this bug which is caused by the hashcode() in GenericRowData. 
Could you please give a rough explanation of your solutions before implementing it?;;;","22/Apr/24 08:09;rovboyko;Most probably the easiest way to fix it would be to replace the Map<hashCode, List<record>> to Map<record, List<rowKind>> inside InputSideHasNoUniqueKeyBundle. In such attempt we should store every record as map key with constant rowKind=+I, while rowkinds would be stored as values. And before providing the record to processing we should set the proper rowKind back to it.;;;","22/Apr/24 09:33;rovboyko;""bug which is caused by the hashcode() in GenericRowData""

And by the way - why do you think that BinaryRowData can't produce the same collision?;;;","23/Apr/24 02:46;xu_shuai_;Hi [~rovboyko] , actually it can't be avoid hash collision even if using BinaryRowData which can only reduce the probability to some extent. And the solution you mentioned works for me.;;;","23/Apr/24 04:15;rovboyko;[~xu_shuai_] , Ok agree with you. So may I start the implementation?;;;","23/Apr/24 05:46;xu_shuai_;[~rovboyko] , absolutely, please feel free to start the implementation.;;;","23/Apr/24 07:02;rovboyko;I've found even simplier solution without changing the storage schema - https://github.com/apache/flink/pull/24703;;;","30/Apr/24 02:15;lsy;Merged in master: f543cc543e9b0eb05415095190e86d3b22cdf1a4;;;","30/Apr/24 02:16;lsy;[~rovboyko] Can you help create a backport pr to release-1.19?;;;","30/Apr/24 04:03;rovboyko;[~lsy] , done - https://github.com/apache/flink/pull/24749;;;","10/May/24 03:58;lsy;Release-1.19: 17e7c3eaf14b6c63f55d28a308e30ad6a3a80c95;;;",,,,,,,,,,,,,,,,,
Expose FlinkMinorVersion as metric for applications in operator,FLINK-35183,13576626,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,mbalassi,mbalassi,20/Apr/24 18:21,09/May/24 08:54,04/Jun/24 20:40,09/May/24 08:54,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,pull-request-available,,This is a convenience feature on top of the existing Flink version grouping. When implementing platform overview dashboards for aggregating metrics from multiple operators it comes in handy.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 09 08:54:33 UTC 2024,,,,,,,,,,"0|z1orrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 08:54;mbalassi;[{{84d9b74}}|https://github.com/apache/flink-kubernetes-operator/commit/84d9b7459b01dab434b2ea8a43f9c9163c753273] in main.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.apache.commons:commons-compress from 1.24.0 to 1.26.1 for Flink Pulsar connector,FLINK-35182,13576619,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gongzhongqiang,gongzhongqiang,gongzhongqiang,20/Apr/24 14:13,03/May/24 07:03,04/Jun/24 20:40,03/May/24 07:03,,,,,,,,,pulsar-4.2.0,,,Connectors / Pulsar,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 03 07:03:19 UTC 2024,,,,,,,,,,"0|z1orps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/24 07:03;tison;master via https://github.com/apache/flink-connector-pulsar/commit/b37a8b32f30683664ff25888d403c4de414043e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.apache.commons:commons-compress from 1.24.0 to 1.26.1 for Flink Elasticsearch connector,FLINK-35181,13576614,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,gongzhongqiang,gongzhongqiang,20/Apr/24 12:35,20/Apr/24 12:43,04/Jun/24 20:40,20/Apr/24 12:43,,,,,,,,,,,,Connectors / ElasticSearch,,,,0,,,"Bump org.apache.commons:commons-compress from 1.24.0 to 1.26.1 for Flink Elasticsearch connector

 ",,,,,,,,,,,,,,,,,FLINK-35174,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-20 12:35:15.0,,,,,,,,,,"0|z1oroo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Instant in row doesn't convert to correct type in python thread mode,FLINK-35180,13576610,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vonesec,vonesec,20/Apr/24 10:45,05/May/24 14:44,04/Jun/24 20:40,,,,,,,,,,,,,API / Python,,,,0,,,"{code:java}
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common import Types, WatermarkStrategy, Configuration
from pyflink.table import EnvironmentSettings, TableEnvironment
from pyflink.table import StreamTableEnvironment, Schema
from pyflink.datastream.functions import ProcessFunction, MapFunction

# init task env
config = Configuration()
# config.set_string(""python.execution-mode"", ""thread"")
config.set_string(""python.execution-mode"", ""process"")
config.set_string(""python.client.executable"", ""/root/miniconda3/bin/python3"")
config.set_string(""python.executable"", ""/root/miniconda3/bin/python3"")
env = StreamExecutionEnvironment.get_execution_environment(config)
table_env = StreamTableEnvironment.create(env)

table = table_env.from_elements([(1, 'Hi'), (2, 'Hello')]).alias(""id"", ""content"")
table_env.create_temporary_view(""test_table"", table)
result_table = table_env.sql_query(""select *, NOW() as dt from test_table"")

result_ds = table_env.to_data_stream(result_table)
def test_func(row):
    print(row)
    return row

result_ds.map(test_func)
env.execute(){code}
output in process mode:
{code:java}
Row(id=1, content='Hi', dt=Instant<1713609386, 271000000>)
Row(id=2, content='Hello', dt=Instant<1713609386, 580000000>) {code}
output in thread mode:
{code:java}
 
Row(id=1, content='Hi', dt=<pemja.PyJObject object at 0x7f113dc92f10>)
Traceback (most recent call last):
  File ""/home/disk1/yuanwei/bug.py"", line 31, in <module>
    env.execute()
  File ""/root/miniconda3/lib/python3.10/site-packages/pyflink/datastream/stream_execution_environment.py"", line 773, in execute
    return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
  File ""/root/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py"", line 1322, in {}call{}
    return_value = get_return_value(
  File ""/root/miniconda3/lib/python3.10/site-packages/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/root/miniconda3/lib/python3.10/site-packages/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o7.execute.
: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
        at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
        at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
        at org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
        at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
        at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)
        at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)
        at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)
        at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)
        at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
        at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
        at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
        at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
        at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
        at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)
        at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)
        at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
        at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
        at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
        at org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)
        at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
        at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)
        at org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)
        at org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)
        at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)
        at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
        at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
        at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
        at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
        at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
        at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
        at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
        at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
        at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
        at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
        at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
        ... 4 more
Caused by: pemja.core.PythonException: <class 'SystemError'>: <built-in function dumps> returned NULL without setting an exception
        at /root/miniconda3/lib/python3.10/site-packages/pyflink/fn_execution/embedded/operations.process_element(operations.py:93)
        at /root/miniconda3/lib/python3.10/site-packages/pyflink/fn_execution/embedded/operations._output_elements(operations.py:58)
        at /root/miniconda3/lib/python3.10/site-packages/pyflink/fn_execution/embedded/converters.to_external(converters.py:72)
        at pemja.core.object.PyIterator.next(Native Method)
        at pemja.core.object.PyIterator.hasNext(PyIterator.java:40)
        at org.apache.flink.streaming.api.operators.python.embedded.AbstractOneInputEmbeddedPythonFunctionOperator.processElement(AbstractOneInputEmbeddedPythonFunctionOperator.java:161)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
        at org.apache.flink.table.runtime.operators.sink.OutputConversionOperator.processElement(OutputConversionOperator.java:105)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
        at org.apache.flink.table.runtime.util.StreamRecordCollector.collect(StreamRecordCollector.java:44)
        at org.apache.flink.table.runtime.operators.sink.ConstraintEnforcer.processElement(ConstraintEnforcer.java:247)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
        at StreamExecCalc$6.processElement(Unknown Source)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:425)
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:520)
        at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:110)
        at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:99)
        at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:114)
        at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:71)
        at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:338) {code}
I thought it because Instant object not convert correctly in outputDataConverter, it should be convert to pyflink.common.time.Instant as same as in process mode.",,,,,,,,,,,,,,,,,,,FLINK-35290,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 05 14:44:13 UTC 2024,,,,,,,,,,"0|z1orns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/24 14:44;wzorgdrager;I have encountered the same bug. The only addition I can make is that in `thread` mode, the object is a `pemja.PyJObject` which wraps `java.time.Instant`. That should be a `pyflink.common.time.Instant`. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
add postgres pipeline data sink connector,FLINK-35179,13576602,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,20/Apr/24 07:11,20/Apr/24 07:11,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-20 07:11:51.0,,,,,,,,,,"0|z1orm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint CLAIM mode does not fully control snapshot ownership,FLINK-35178,13576601,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,elon,elon,20/Apr/24 06:56,26/Apr/24 02:32,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,Runtime / Checkpointing,,,,0,,,"When I enable incremental checkpointing, and the task fails or is canceled for some reason, restarting the task from {{-s checkpoint_path}} with {{restoreMode CLAIM}} allows the Flink job to recover from the last checkpoint, it just discards the previous checkpoint.

Then I found that this leads to the following two cases:

1. If the new checkpoint_x meta file does not reference files in the shared directory under the previous jobID:         

the shared and taskowned directories from the previous Job will be left as empty directories, and these two directories will persist without being deleted by Flink. !image-2024-04-20-14-51-21-062.png!

2. If the new checkpoint_x meta file references files in the shared directory under the previous jobID:

the chk-(x-1) from the previous job will be discarded, but there will still be state data in the shared directory under that job, which might persist for a relatively long time. Here arises the question: the previous job is no longer running, and it's unclear whether users should delete the state data. Deleting it could lead to errors when the task is restarted, as the meta might reference files that can no longer be found; this could be confusing for users.

 

The potential solution might be to reuse the previous job's jobID when restoring from {{{}-s checkpoint_path{}}}, or to add a new parameter that allows users to specify the jobID they want to recover from;

 

Please correct me if there's anything I've misunderstood.",,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/24 06:51;elon;image-2024-04-20-14-51-21-062.png;https://issues.apache.org/jira/secure/attachment/13068329/image-2024-04-20-14-51-21-062.png","22/Apr/24 07:16;elon;image-2024-04-22-15-16-02-381.png;https://issues.apache.org/jira/secure/attachment/13068337/image-2024-04-22-15-16-02-381.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 02:32:34 UTC 2024,,,,,,,,,,"0|z1orls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 03:09;lijinzhong;[~elon]  Thanks for reporting this.

For case 1: Let me confirm the job configuration.   You configured the parameter ""state.checkpoints.create-subdir: true"", right? 

 

For case 2:  This is the expected behavior of the ""CLAIM-MODE + incremental checkpoint"" for current RocksdbStateBackend.

The reason behind this is that the new job will perform incremental-checkpoint based on the old sst-files belonging to previous job. So the checkpoint folder of old job needs to be preserved until the checkpoint of new job no longer references any sst-files of the old job.

In this case, i think, the lifecycle of the checkpoint-dir for both old and new jobs should be managed by the fink framework, and it should not be the user's responsibility to delete/cleanup it.;;;","22/Apr/24 07:19;elon;[~lijinzhong] 

Thank you for your response.

I have been using the default value (true) for the ""state.checkpoints.create-subdir"" parameter. However, when I tested by setting this value to false, the result was the same, which might indicate I'm doing something wrong ?

Additionally, I've encountered another issue. Even though I set {{{}state.checkpoints.num-retained=3{}}}, the older job's checkpoint versions are not being discarded even if they are not referenced. Only the checkpoint specified by the {{-s}} option (chk-x) is discarded.

As shown in the diagram below, I restored from chk-34, but only chk-34 was discarded, while chk-32 and chk-33 continue to exist indefinitely.

!image-2024-04-22-15-16-02-381.png!;;;","26/Apr/24 02:32;Yanfei Lei;[~elon] 

> the result was the same?

 

Does it mean that empty shared/ and taskowned/ directories still exist?  In our practice, these folders are generally managed with the help of external platforms.

 

>  {{{}state.checkpoints.num-retained=3{}}}, the older job's checkpoint versions are not being discarded even if they are not referenced.

 

The chk-x information is recorded in the job manager. When the job is restored, those information is lost. The job manager only knows the chk-x you specified, but not the other chk-(x-1), chk-(x-2), so they are not deleted.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Datagen examples in documentation do not compile,FLINK-35177,13576590,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,morozov,morozov,20/Apr/24 00:51,20/Apr/24 01:06,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Documentation,,,,0,pull-request-available,,"Currently, the [examples|https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/connectors/datastream/datagen/#rate-limiting] look like this:
{code:java}
GeneratorFunction<Long, Long> generatorFunction = index -> index;
double recordsPerSecond = 100;

DataGeneratorSource<String> source =
        new DataGeneratorSource<>(
             generatorFunction,
             Long.MAX_VALUE,
             RateLimiterStrategy.perSecond(recordsPerSecond),
             Types.STRING);
{code}
The generator function returns Long but the DataGeneratorSource uses String, so their types do not match.

Either the generator function needs to be changed to return a string, or the source needs to use Long.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-20 00:51:21.0,,,,,,,,,,"0|z1orjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support property authentication connection for JDBC catalog & dynamic table,FLINK-35176,13576536,13419032,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,RocMarshal,RocMarshal,19/Apr/24 15:30,07/May/24 02:50,04/Jun/24 20:40,07/May/24 02:50,,,,,,,,,jdbc-3.3.0,,,Connectors / JDBC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 02:50:55 UTC 2024,,,,,,,,,,"0|z1or7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 02:50;fanrui;Merged to main(3.3.0) via:
 * 5a8060b39d50490dc317ac941782df36d9a1d42d
 * a9135f98bd708525c1474d76da98dfce149fb4d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopDataInputStream can't compile with Hadoop 3.2.3,FLINK-35175,13576523,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,masteryhx,rskraba,rskraba,19/Apr/24 13:13,22/Apr/24 11:54,04/Jun/24 20:40,22/Apr/24 01:51,1.20.0,,,,,,,,1.20.0,,,,,,,0,pull-request-available,,"Unfortunately, introduced in FLINK-35045: [PREADWRITEBUFFER|https://github.com/apache/flink/commit/a312a3bdd258e0ff7d6f94e979b32e2bc762b82f#diff-3ed57be01895ba0f792110e40f4283427c55528f11a5105b4bf34ebd4e6fef0dR182] was added in Hadoop releases [3.3.0|https://github.com/apache/hadoop/blob/rel/release-3.3.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StreamCapabilities.java#L72] and [2.10.0|https://github.com/apache/hadoop/blob/rel/release-2.10.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StreamCapabilities.java#L72].

It doesn't exist in flink.hadoop.version [3.2.3|https://github.com/apache/hadoop/blob/rel/release-3.2.3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StreamCapabilities.java], which we are using in end-to-end tests.
{code:java}
00:23:55.093 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project flink-hadoop-fs: Compilation failure: Compilation failure: 
00:23:55.093 [ERROR] /home/vsts/work/1/s/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopDataInputStream.java:[151,63] cannot find symbol
00:23:55.094 [ERROR]   symbol:   variable READBYTEBUFFER
00:23:55.094 [ERROR]   location: interface org.apache.hadoop.fs.StreamCapabilities
00:23:55.094 [ERROR] /home/vsts/work/1/s/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopDataInputStream.java:[182,63] cannot find symbol
00:23:55.094 [ERROR]   symbol:   variable PREADBYTEBUFFER
00:23:55.094 [ERROR]   location: interface org.apache.hadoop.fs.StreamCapabilities
00:23:55.094 [ERROR] /home/vsts/work/1/s/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopDataInputStream.java:[183,43] incompatible types: long cannot be converted to org.apache.hadoop.io.ByteBufferPool
00:23:55.094 [ERROR] -> [Help 1] {code}
* 1.20 compile_cron_hadoop313 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59012&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=3630",,,,,,,,,,,,,,,,,,,,,,,,FLINK-35045,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 22 11:54:58 UTC 2024,,,,,,,,,,"0|z1or4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/24 13:21;rskraba;[~hangxiang]: what do you think? Is it better to bump the supported version of Hadoop or fix the class that relies on this capability?;;;","19/Apr/24 13:34;rskraba;1.20 Hadoop 3.1.3 / Compile https://github.com/apache/flink/actions/runs/8747381080/job/24005737445#step:6:1560;;;","19/Apr/24 17:27;masteryhx;[~rskraba] Thanks for reporting this!

I think it's better if we could upgrade the hadoop version, and I also saw before ticket about it: https://issues.apache.org/jira/browse/FLINK-33584

But seems there still are some blockers.

So I'd prefer to prepare a PR to remove the optimization currently and add TODO here. 

I will also try to help to push forward the upgradation.
Thanks again.;;;","22/Apr/24 01:43;Weijie Guo;1.20 compile_cron_hadoop313

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59037&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691&l=3641;;;","22/Apr/24 01:48;Weijie Guo;hadoop313:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59046&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691&l=3893;;;","22/Apr/24 01:51;masteryhx;merged a4c71c8d into master;;;","22/Apr/24 06:47;Weijie Guo;Not include the fix:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59049&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691&l=3893;;;","22/Apr/24 11:54;rskraba;Not including the fix: 
1.20 Hadoop 3.1.3 / Compile https://github.com/apache/flink/actions/runs/8747381080/job/24005737445#step:6:1560
1.20 Hadoop 3.1.3 / Compile https://github.com/apache/flink/actions/runs/8769422914/job/24064887346#step:6:1759
;;;",,,,,,,,,,,,,,,,,,,,,
Bump org.apache.commons:commons-compress from 1.24.0 to 1.26.1 for Flink RabbitMQ connector,FLINK-35174,13576497,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,19/Apr/24 10:11,20/Apr/24 12:35,04/Jun/24 20:40,,,,,,,,,,rabbitmq-3.1.0,,,Connectors/ RabbitMQ,,,,0,pull-request-available,,"Bump org.apache.commons:commons-compress from 1.24.0 to 1.26.1 for Flink RabbitMQ connector

 ",,,,,,,,,,,,,,,,FLINK-35181,FLINK-35113,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 10:57:56 UTC 2024,,,,,,,,,,"0|z1oqyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/24 10:57;dannycranmer;Merged commit 66e323a into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debezium for Mysql connector Custom Time Serializer ,FLINK-35173,13576495,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,czy006,czy006,czy006,19/Apr/24 09:51,26/Apr/24 08:49,04/Jun/24 20:40,26/Apr/24 08:48,cdc-3.1.0,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,CDC,pull-request-available,"Currently, Flink CDC Time encounters time type errors (including DateTime, Time, Date, TimeStamp) when using MySQL Connector (JsonDebeziumDeserializationSchema) as deserialization, and the converted time is wrong. The essential reason is that the timestamp returned by the bottom layer of debezium is UTC (such as io.debezium.time.Timestamp). The community has already had some [PR|https://github.com/apache/flink-cdc/pull/1366/files#diff-e129e9fae3eea0bb32f0019debb4932413c91088d6dae656e2ecb63913badae4], but they are not work.

Now a way is provided to provide a solution based on Debezium's custom Convert interface (https://debezium.io/documentation/reference/1.9/development/converters.html),
Users can choose to convert the above four time types into STRING according to the specified time format to ensure that users can correctly convert JSON when using the Flink DataStream API.


When the user enables this converter, we need to configure it according to the parameters, That's some datastream use case:
{code:java}
Properties debeziumProperties = new Properties();
debeziumProperties.setProperty(""converters"", ""datetime"");
debeziumProperties.setProperty(""datetime.database.type"", DataBaseType.MYSQL.getType());
debeziumProperties.setProperty(""datetime.type"", ""cn.xxx.sources.cdc.MysqlDebeziumConverter"");
debeziumProperties.setProperty(""datetime.format.date"", ""yyyy-MM-dd"");
debeziumProperties.setProperty(""datetime.format.time"", ""HH:mm:ss"");
debeziumProperties.setProperty(""datetime.format.datetime"", ""yyyy-MM-dd HH:mm:ss"");
debeziumProperties.setProperty(""datetime.format.timestamp"", ""yyyy-MM-dd HH:mm:ss"");
debeziumProperties.setProperty(""datetime.format.timestamp.zone"", ""UTC+8"");
MySqlSourceBuilder<String> builder = MySqlSource.<String>builder()
        .hostname(url[0])
        .port(Integer.parseInt(url[1]))
        .databaseList(table.getDatabase())
        .tableList(getTablePattern(table))
        .username(table.getUserName())
        .password(table.getPassword())
        .debeziumProperties(debeziumProperties); {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 08:48:29 UTC 2024,,,,,,,,,,"0|z1oqy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 08:48;renqs;flink-cdc release-3.1: 90511b3a65f5a3646f70cfca73e54df363e2d119

flink-cdc master: 6232d84052422aa88299f28074a8437e91db2988;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DDL statement is added to the Schema Change Event,FLINK-35172,13576485,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,19/Apr/24 09:17,07/May/24 09:46,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"The current implementation of the [kafka pipeline data sink connector |https://github.com/apache/flink-cdc/pull/2938]does not write ddl statements to the topic because the original dddl statements are missing. ddl cannot be generated backwards using a Schema Change Event.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 09:19:21 UTC 2024,,,,,,,,,,"0|z1oqw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/24 09:19;melin;cc [~1365976815@qq.com];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class 'FlinkSqlParserImpl' is missed,FLINK-35171,13576478,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chenyu-opensource,chenyu-opensource,19/Apr/24 07:56,19/Apr/24 09:30,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,,0,,,!image-2024-04-19-15-55-54-035.png!,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/24 07:55;chenyu-opensource;image-2024-04-19-15-55-54-035.png;https://issues.apache.org/jira/secure/attachment/13068310/image-2024-04-19-15-55-54-035.png","19/Apr/24 07:57;chenyu-opensource;missed in github.png;https://issues.apache.org/jira/secure/attachment/13068312/missed+in+github.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 09:30:39 UTC 2024,,,,,,,,,,"0|z1oqug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/24 08:05;chenyu-opensource;Or I misunderstood? please give me a reply.;;;","19/Apr/24 08:14;chenyu-opensource;Generated sources?;;;","19/Apr/24 09:30;hackergin;[~chenyu-opensource]  {color:#000000}You should build the flink-sql-parser module to generate it.{color};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SqlServer connector support scanNewlyAddedTableEnabled param,FLINK-35170,13576471,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yandufeng,yandufeng,19/Apr/24 07:07,24/May/24 09:17,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,SqlServer connector support scanNewlyAddedTableEnabled param,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-19 07:07:05.0,,,,,,,,,,"0|z1oqsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recycle buffers to freeSegments before releasing data buffer for sort accumulator,FLINK-35169,13576462,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,19/Apr/24 05:54,23/Apr/24 05:17,04/Jun/24 20:40,22/Apr/24 03:42,1.20.0,,,,,,,,1.18.2,1.19.1,1.20.0,Runtime / Network,,,,0,pull-request-available,,"When using sortBufferAccumulator, we should recycle the buffers to freeSegments before releasing the data buffer. The reason is that when getting buffers from the DataBuffer, it may require more buffers than the current quantity available in freeSegments. Consequently, to ensure adequate buffers from DataBuffer, the flushed and recycled buffers should also be added to freeSegments for reuse.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 22 03:42:47 UTC 2024,,,,,,,,,,"0|z1oqqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 03:42;Weijie Guo;master via 68a84fd02fb8e288ff7605073f55834468dcf53a.
1.18 via 1b3849c196c25784dc662398385e0b30f2c23a03.
1.19 via 6045bd8bf935b12c8056e98945e054511da9c251.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic State Iterator for async processing,FLINK-35168,13576451,13574084,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,19/Apr/24 04:06,13/May/24 12:32,04/Jun/24 20:40,13/May/24 12:32,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 12:32:22 UTC 2024,,,,,,,,,,"0|z1oqog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 12:32;zakelly;Merged into master via 01586780;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce MaxCompute pipeline DataSink,FLINK-35167,13576447,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhangdingxin,zhangdingxin,zhangdingxin,19/Apr/24 02:15,23/May/24 12:22,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"By integrating the MaxCompute DataSink, we enable the precise and efficient synchronization of data from Flink's Change Data Capture (CDC) into MaxCompute.",,";19/Apr/24 02:24;zhangdingxin;1814400",";19/Apr/24 02:24;zhangdingxin;1814400",1814400,604800,3628800,200%,1814400,604800,3628800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Fri Apr 19 02:22:29 UTC 2024,,,,,,,,,,"0|z1oqnk:",9223372036854775807,Introduce MaxCompute pipeline DataSink,,,,,,,,,,,,,,,,,,,"19/Apr/24 02:22;zhangdingxin;I'd like to do it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the performance of Hybrid Shuffle when enable memory decoupling,FLINK-35166,13576446,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Jiang Xin,Jiang Xin,Jiang Xin,19/Apr/24 02:03,24/Apr/24 03:52,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,Runtime / Network,,,,0,pull-request-available,,"Currently, the tiered result partition creates the SortBufferAccumulator with the number of expected buffers as min(numSubpartitions+1, 512), thus the SortBufferAccumulator may obtain very few buffers when the parallelism is small. We can easily make the number of expected buffers 512 by default to have a better performance when the buffers are sufficient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 03:52:52 UTC 2024,,,,,,,,,,"0|z1oqnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/24 03:52;Weijie Guo;master(1.20) via 712d9800e6c02e66b51747ca65331ad9e5295ffb.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveBatch Scheduler should not restrict the default source parallelism to the max parallelism set,FLINK-35165,13576421,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vsowrirajan,vsowrirajan,18/Apr/24 19:15,14/May/24 01:52,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,"Copy-pasting the reasoning mentioned on this [discussion thread|https://lists.apache.org/thread/o887xhvvmn2rg5tyymw348yl2mqt23o7].

Let me state why I think ""{_}jobmanager.adaptive-batch-scheduler.default-source-parallelism{_}"" should not be bound by the ""{_}jobmanager.adaptive-batch-scheduler.max-parallelism{_}"".
 *  Source vertex is unique and does not have any upstream vertices - Downstream vertices read shuffled data partitioned by key, which is not the case for the Source vertex
 * Limiting source parallelism by downstream vertices' max parallelism is incorrect
 * If we say for """"semantic consistency"" the source vertex parallelism has to be bound by the overall job's max parallelism, it can lead to following issues:
 ** High filter selectivity with huge amounts of data to read
 ** Setting high ""*jobmanager.adaptive-batch-scheduler.max-parallelism*"" so that source parallelism can be set higher can lead to small blocks and sub-optimal performance.
 ** Setting high ""*jobmanager.adaptive-batch-scheduler.max-parallelism*"" requires careful tuning of network buffer configurations which is unnecessary in cases where it is not required just so that the source parallelism can be set high.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 22 18:03:13 UTC 2024,,,,,,,,,,"0|z1oqi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 18:03;vsowrirajan;JFYI, I am working on the fix for this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support `ALTER CATALOG RESET` syntax,FLINK-35164,13576391,13572934,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liyubin117,liyubin117,18/Apr/24 15:27,09/May/24 02:44,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,"h3. ALTER CATALOG catalog_name RESET (key1, key2, ...)

Reset one or more properties to its default value in the specified catalog.

!image-2024-04-18-23-26-59-854.png|width=781,height=527!",,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/24 15:26;liyubin117;image-2024-04-18-23-26-59-854.png;https://issues.apache.org/jira/secure/attachment/13068301/image-2024-04-18-23-26-59-854.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-18 15:27:05.0,,,,,,,,,,"0|z1oqbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utilize ForSt's native MultiGet API to optimize remote state access,FLINK-35163,13576383,13574094,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lijinzhong,lijinzhong,18/Apr/24 13:57,19/Apr/24 02:14,04/Jun/24 20:40,,,,,,,,,,2.0.0,,,Runtime / State Backends,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-18 13:57:29.0,,,,,,,,,,"0|z1oq9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support grouping state get and put access,FLINK-35162,13576382,13574094,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,lijinzhong,lijinzhong,18/Apr/24 13:55,26/Apr/24 02:06,04/Jun/24 20:40,26/Apr/24 02:06,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 02:06:24 UTC 2024,,,,,,,,,,"0|z1oq9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 02:06;masteryhx;merged 1c4eb914 and fb583804 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement StateExecutor for ForStStateBackend,FLINK-35161,13576380,13574094,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,lijinzhong,lijinzhong,18/Apr/24 13:51,07/May/24 10:07,04/Jun/24 20:40,07/May/24 10:07,,,,,,,,,2.0.0,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 10:07:41 UTC 2024,,,,,,,,,,"0|z1oq8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 10:07;zakelly;Merged via ea4112aefa72e9d15525a72157ced3e3da3650ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for Thread Dump provides a convenient way to display issues of thread deadlocks in tasks,FLINK-35160,13576377,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,elon,elon,18/Apr/24 13:35,20/Apr/24 06:43,04/Jun/24 20:40,,1.16.0,1.17.1,1.18.1,1.19.0,,,,,,,,Runtime / REST,,,,1,,,"After receiving feedback from the business side about performance issues in their tasks, we attempted to troubleshoot and discovered that their tasks had issues with thread deadlocks. However, the Thread Dump entry on the Flink page only shows thread stacks. Since the users are not very familiar with Java stacks, they couldn't clearly identify that the deadlocks were due to issues in the business logic code and mistakenly thought they were problems with the Flink framework

!image-2024-04-18-20-57-52-440.png!

!image-2024-04-18-20-58-09-872.png!

the JVM's jstack command can clearly display thread deadlocks, unfortunately, the business team does not have the permissions to log into the machines.  hear is the jstack log

!image-2024-04-20-14-43-36-939.png!

FlameGraph are excellent for visualizing performance bottlenecks and hotspots in application profiling but are not designed to pinpoint the exact lines of code where thread deadlocks occur.

!image-2024-04-18-21-01-22-881.png!

Perhaps we could enhance the Thread Dump feature to display thread deadlocks, similar to what the {{jstack}} command provides.

 

!image-2024-04-18-21-34-41-014.png!",,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/24 12:57;elon;image-2024-04-18-20-57-52-440.png;https://issues.apache.org/jira/secure/attachment/13068299/image-2024-04-18-20-57-52-440.png","18/Apr/24 12:58;elon;image-2024-04-18-20-58-09-872.png;https://issues.apache.org/jira/secure/attachment/13068298/image-2024-04-18-20-58-09-872.png","18/Apr/24 13:01;elon;image-2024-04-18-21-01-22-881.png;https://issues.apache.org/jira/secure/attachment/13068296/image-2024-04-18-21-01-22-881.png","18/Apr/24 13:34;elon;image-2024-04-18-21-34-41-014.png;https://issues.apache.org/jira/secure/attachment/13068295/image-2024-04-18-21-34-41-014.png","20/Apr/24 06:43;elon;image-2024-04-20-14-43-36-939.png;https://issues.apache.org/jira/secure/attachment/13068328/image-2024-04-20-14-43-36-939.png",,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-18 13:35:20.0,,,,,,,,,,"0|z1oq88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CreatingExecutionGraph can leak CheckpointCoordinator and cause JM crash,FLINK-35159,13576375,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,18/Apr/24 13:12,19/Apr/24 15:03,04/Jun/24 20:40,19/Apr/24 15:03,1.18.0,,,,,,,,1.18.2,1.19.1,1.20.0,Runtime / Coordination,,,,0,pull-request-available,,"When a task manager dies while the JM is generating an ExecutionGraph in the background then {{CreatingExecutionGraph#handleExecutionGraphCreation}} can transition back into WaitingForResources if the TM hosted one of the slots that we planned to use in {{tryToAssignSlots}}.

At this point the ExecutionGraph was already transitioned to running, which implicitly kicks of periodic checkpointing by the CheckpointCoordinator, without the operator coordinator holders being initialized yet (as this happens after we assigned slots).

This effectively leaks that CheckpointCoordinator, including the timer thread that will continue to try triggering checkpoints, which will naturally fail to trigger.
This can cause a JM crash because it results in {{OperatorCoordinatorHolder#abortCurrentTriggering}} to be called, which fails with an NPE since the {{mainThreadExecutor}} was not initialized yet.

{code}
java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.lang.NullPointerException
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$8(CheckpointCoordinator.java:707)
	at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986)
	at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:610)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:910)
	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.util.concurrent.CompletionException: java.lang.NullPointerException
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:932)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
	... 7 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.abortCurrentTriggering(OperatorCoordinatorHolder.java:388)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1085)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:985)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:961)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$7(CheckpointCoordinator.java:693)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
	... 8 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 08:20:06 UTC 2024,,,,,,,,,,"0|z1oq7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/24 08:20;chesnay;master: 131358b918ba511064c23dfee96969a628d07d2a
1.19: 87ed9ccc2103457ba91f6ca45adfd2bfcc75c9ac
1.18: aacc735806acf1d63fa732706e079bc2ca1bb4fc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error handling in StateFuture's callback,FLINK-35158,13576368,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,18/Apr/24 12:07,08/May/24 12:15,04/Jun/24 20:40,08/May/24 12:12,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 08 12:15:36 UTC 2024,,,,,,,,,,"0|z1oq68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/24 12:15;Yanfei Lei;pr [24698|https://github.com/apache/flink/pull/24698] closed via bb0f442

pr [24702|https://github.com/apache/flink/pull/24702] merged via 8475d28;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sources with watermark alignment get stuck once some subtasks finish,FLINK-35157,13576363,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,elon,gyfora,gyfora,18/Apr/24 11:35,04/Jun/24 03:34,04/Jun/24 20:40,,1.17.2,1.18.1,1.19.0,,,,,,,,,Runtime / Coordination,,,,1,pull-request-available,,"The current watermark alignment logic can easily get stuck if some subtasks finish while others are still running.

The reason is that once a source subtask finishes, the subtask is not excluded from alignment, effectively blocking the rest of the job to make progress beyond last wm + alignment time for the finished sources.

This can be easily reproduced by the following simple pipeline:
{noformat}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(2);
    DataStream<Long> s = env.fromSource(new NumberSequenceSource(0, 100),
            WatermarkStrategy.<Long>forMonotonousTimestamps().withTimestampAssigner((SerializableTimestampAssigner<Long>) (aLong, l) -> aLong).withWatermarkAlignment(""g1"", Duration.ofMillis(10), Duration.ofSeconds(2)),
            ""Sequence Source"").filter((FilterFunction<Long>) aLong -> {
        Thread.sleep(200);
        return true;
    }
);

    s.print();
    env.execute();{noformat}
The solution could be to send out a max watermark event once the sources finish or to exclude them from the source coordinator",,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/24 13:36;elon;image-2024-04-24-21-36-16-146.png;https://issues.apache.org/jira/secure/attachment/13068405/image-2024-04-24-21-36-16-146.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 13:36:25 UTC 2024,,,,,,,,,,"0|z1oq54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/24 13:36;elon;hi, [~fanrui] Can you assign this issue to me? I can fix this problem, thank you so so much !

!image-2024-04-24-21-36-16-146.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wire new operators for async state with DataStream V2,FLINK-35156,13576355,13574083,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,18/Apr/24 10:12,25/Apr/24 06:16,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 06:16:42 UTC 2024,,,,,,,,,,"0|z1oq3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 06:16;Yanfei Lei;Merged into master via a87db93.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TableRuntimeException,FLINK-35155,13576354,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,18/Apr/24 09:56,06/May/24 09:01,04/Jun/24 20:40,06/May/24 09:01,,,,,,,,,1.20.0,,,Table SQL / Runtime,,,,0,pull-request-available,,"The `throwException` internal function throws a {{RuntimeException}}. It would be nice to have a specific kind of exception thrown from there, so that it's easier to classify those.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 06 09:01:25 UTC 2024,,,,,,,,,,"0|z1oq34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/24 09:01;dwysakowicz;Implemented in 1904b215e36e4fd48e48ece7ffdf2f1470653130;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc aggregate fails,FLINK-35154,13576335,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,lda-dima,lda-dima,18/Apr/24 08:17,19/Apr/24 05:33,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,,"Javadoc plugin fails with error. Using
{code:java}
javadoc:aggregate{code}
ERROR:
{code:java}
[WARNING] The requested profile ""include-hadoop"" could not be activated because it does not exist.
[WARNING] The requested profile ""arm"" could not be activated because it does not exist.
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.9.1:aggregate (default-cli) on project flink-parent: An error has occurred in JavaDocs report generation: 
[ERROR] Exit code: 1 - /{flinkProjectDir}/flink-end-to-end-tests/flink-confluent-schema-registry/target/generated-sources/example/avro/EventType.java:8: error: type GenericEnumSymbol does not take parameters
[ERROR] public enum EventType implements org.apache.avro.generic.GenericEnumSymbol<EventType> {
[ERROR]    {code}
 

For our flink 1.17 ERROR
{code:java}
[ERROR] /{flinkProjectDir}/flink-connectors/flink-sql-connector-hive-3.1.3/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java:21: error: cannot find symbol
[ERROR] import static org.apache.hadoop.hive.metastore.Warehouse.DEFAULT_DATABASE_NAME;
[ERROR] ^
[ERROR]   symbol:   static DEFAULT_DATABASE_NAME
[ERROR]   location: class{code}
 

Need to increase version of javadoc plugin from 2.9.1 to 2.10.4 (In current version exists bug same with [this|https://github.com/checkstyle/checkstyle/issues/291])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 08:18:54 UTC 2024,,,,,,,,,,"0|z1opyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/24 08:18;lda-dima;[~MartijnVisser] [~trohrmann] 

Hi,

Please, assign me to this task;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal Async State Implementation and StateDescriptor for Map/List State,FLINK-35153,13576324,13574084,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,18/Apr/24 07:21,15/May/24 09:31,04/Jun/24 20:40,15/May/24 09:31,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 15 09:31:16 UTC 2024,,,,,,,,,,"0|z1opwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/24 09:31;zakelly;Merged into master via 73a7e1cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC  Doris Sink Auto create table event should support setting auto partition fields for each table,FLINK-35152,13576317,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,liulangdeerhu,liulangdeerhu,18/Apr/24 06:51,24/May/24 09:17,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,Doris,,"In some scenarios, when creating a physical table in Doris, appropriate partition fields need to be selected to speed up the efficiency of data query and calculation. In addition, partition tables support more applications, such as hot and cold data layering and so on.


The current Flink CDC Doris Sink's create table event creates a table with no partitions set.


The Auto Partition function supported by doris 2.1.x simplifies the creation and management of partitions. We just need to add some configuration items to the Flink CDC job. To tell Flink CDC which fields Doris Sink will use in the create table event to create partitions, you can get a partition table in Doris.

Here's an example:
source: Mysql
source_table:
CREATE TABLE table1 (
col1 INT AUTO_INCREMENT PRIMARY KEY,
col2 DECIMAL(18, 2),
col3 VARCHAR(500),
col4 TEXT,
col5 DATETIME DEFAULT CURRENT_TIMESTAMP
);


If you want to specify the partition of table test.table1, you need to add sink-table-partition-keys , sink-table-partition-type information ,...., to mysql_to_doris.yaml

route:

source-table: test.table1
sink-table:ods.ods_table1
sink-table-partition-key:col5
sink-table-partition-func-call-expr:date_trunc(`col5`, 'month')
sink-table-partition-type:auto range

The auto range partition in Doris 2.1.x does not support null partitions. So you need to set test.table1.col5 == null then '1990-01-01 00:00:00' else test.table1.col5 end

Now after submitting the mysql_to_doris.ymal Flink CDC job, an ods.ods_table1 data table should appear in the Doris database
The data table DDL is as follows:
CREATE TABLE table1 (
col1 INT ,
col5 DATETIME not null,
col2 DECIMAL(18, 2),
col3 VARCHAR(500),
col4 STRING
) unique KEY(`col1`,`col5`)
AUTO PARTITION BY RANGE date_trunc(`col5`, 'month')()
DISTRIBUTED BY HASH (`id`) BUCKETS AUTO
PROPERTIES (
...
);",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,2024-04-18 06:51:31.0,,,,,,,,,,"0|z1opuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink mysql cdc will  stuck when suspend binlog split and ChangeEventQueue is full,FLINK-35151,13576306,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pacinogong,pacinogong,18/Apr/24 04:35,23/Apr/24 10:54,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"Flink mysql cdc will  stuck when suspend binlog split and ChangeEventQueue is full.

Reason is that producing binlog is too fast.  MySqlSplitReader#suspendBinlogReaderIfNeed will execute BinlogSplitReader#stopBinlogReadTask to set 

currentTaskRunning to be false after MysqSourceReader receives binlog split update event.

MySqlSplitReader#pollSplitRecords is executed and 

dataIt is null to execute closeBinlogReader when currentReader is BinlogSplitReader. closeBinlogReader will execute statefulTaskContext.getBinaryLogClient().disconnect(), it could dead lock. Because BinaryLogClient#connectLock is not release  when MySqlStreamingChangeEventSource add element to full queue.

 

You can set StatefulTaskContext#queue to be 1 and run UT NewlyAddedTableITCase#testRemoveAndAddNewTable.

 ",I use master branch reproduce it.,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/24 04:35;pacinogong;dumpstack.txt;https://issues.apache.org/jira/secure/attachment/13068278/dumpstack.txt",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 10:54:23 UTC 2024,,,,,,,,,,"0|z1opsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/24 04:40;pacinogong;I get an idea to address this issue by setting 

""currentTaskRunning || queue.remainingCapacity() == 0"" for BinlogSplitReader#pollSplitRecords. [~Leonard] PTAL;;;","18/Apr/24 07:27;leonard;Thanks [~pacinogong]for the report, [~ruanhang1993] Would you like to take a look this issue?;;;","23/Apr/24 10:54;yangguozhen;We encountered the same issue recently.

It may be caused by using the same BinaryLogClient instance wrapped in 
MySqlTaskContextImpl in producer thread named ""blc-hostname:port"" and consumer thread named like ""Source Data Fetcher"".
When the ChangeEventQueue is full, the BinaryLogClient in ""blc-hostname:port"" thread blocks at call stack shown as below (line number may be wrong since I added some logs)
{code:java}
""blc-hostname:port"" Id=3271 TIMED_WAITING on io.debezium.connector.base.ChangeEventQueue@5e00c81c
    at java.base@11.0.22/java.lang.Object.wait(Native Method)
    -  waiting on io.debezium.connector.base.ChangeEventQueue@5e00c81c
    at app//io.debezium.connector.base.ChangeEventQueue.doEnqueue(ChangeEventQueue.java:204)
    at app//io.debezium.connector.base.ChangeEventQueue.enqueue(ChangeEventQueue.java:169)
    at app//io.debezium.pipeline.EventDispatcher$StreamingChangeRecordReceiver.changeRecord(EventDispatcher.java:405)
    at app//io.debezium.pipeline.EventDispatcher$2.changeRecord(EventDispatcher.java:229)
    at app//io.debezium.relational.RelationalChangeRecordEmitter.emitUpdateRecord(RelationalChangeRecordEmitter.java:160)
    at app//io.debezium.relational.RelationalChangeRecordEmitter.emitChangeRecords(RelationalChangeRecordEmitter.java:60)
    at app//io.debezium.pipeline.EventDispatcher.dispatchDataChangeEvent(EventDispatcher.java:209) {code}
You can dig into the call stack all the way down to [BinaryLogClient.java#L631|https://github.com/osheroff/mysql-binlog-connector-java/blob/0.27.2/src/main/java/com/github/shyiko/mysql/binlog/BinaryLogClient.java#L631]. The ""blc-hostname:port"" thread blocks at listenForEventPackets method call and never enters the finally block. So connectLock never got released.

The ""blc-hostname:port"" producer thread blocks waiting for queue space, after which it will release connectLock. And ""Source Data Fetcher"" consumer thread blocks waiting for lock to be released, after which it will release queue space. So there's dead lock.

We simply stop using MySqlTaskContextImpl in StatefulTaskContext, and there won't be any BinaryLogClient reusing and don't see the issue any further.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The specified upload does not exist. The upload ID may be invalid,FLINK-35150,13576303,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,qyw919867774,qyw919867774,18/Apr/24 03:15,18/Apr/24 03:22,04/Jun/24 20:40,,1.15.0,,,,,,,,,,,Connectors / FileSystem,,,,0,,,"Flink S3 hadoop, write S3 in csv mode, I used this patch  FLINK-28513 .   But I don't understand why S3RecoverableFsDataOutputStream ""sync"" method of this class to be ""completeMultipartUpload"" operation, if ""completeMultipartUpload"" here, Calling close later to upload the rest of the stream will inevitably result in an error.   The part corresponding to uploadID has been merged.
Therefore, when the message in csv is larger than ""S3_MULTIPART_MIN_PART_SIZE"", the uploadPart will be started when switching files, then when BulkPartWriter performs closeForCommit, Due to the sync S3RecoverableFsDataOutputStream method call completeMultipartUpload, So S3RecoverableFsDataOutputStream ""closeForCommit"" method due to the uploadPart, at this time will lead to errors.

 

BulkPartWriter:

!image-2024-04-18-11-03-08-998.png!

CsvBulkWriter:

!image-2024-04-18-11-20-25-583.png!
S3RecoverableFsDataOutputStream:
!image-2024-04-18-10-51-05-071.png!
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/24 02:51;qyw919867774;image-2024-04-18-10-51-05-071.png;https://issues.apache.org/jira/secure/attachment/13068275/image-2024-04-18-10-51-05-071.png","18/Apr/24 03:03;qyw919867774;image-2024-04-18-11-03-08-998.png;https://issues.apache.org/jira/secure/attachment/13068274/image-2024-04-18-11-03-08-998.png","18/Apr/24 03:20;qyw919867774;image-2024-04-18-11-20-25-583.png;https://issues.apache.org/jira/secure/attachment/13068276/image-2024-04-18-11-20-25-583.png",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-18 03:15:10.0,,,,,,,,,,"0|z1oprs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix DataSinkTranslator#sinkTo ignoring pre-write topology if not TwoPhaseCommittingSink,FLINK-35149,13576300,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,loserwang1024,loserwang1024,loserwang1024,18/Apr/24 02:14,04/Jun/24 08:42,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.1.1,cdc-3.2.0,,Flink CDC,,,,1,pull-request-available,,"Current , when sink is not instanceof TwoPhaseCommittingSink, use input.transform rather than stream. It means that pre-write topology will be ignored.
{code:java}
private void sinkTo(
        DataStream<Event> input,
        Sink<Event> sink,
        String sinkName,
        OperatorID schemaOperatorID) {
    DataStream<Event> stream = input;
    // Pre write topology
    if (sink instanceof WithPreWriteTopology) {
        stream = ((WithPreWriteTopology<Event>) sink).addPreWriteTopology(stream);
    }

    if (sink instanceof TwoPhaseCommittingSink) {
        addCommittingTopology(sink, stream, sinkName, schemaOperatorID);
    } else {
        input.transform(
                SINK_WRITER_PREFIX + sinkName,
                CommittableMessageTypeInfo.noOutput(),
                new DataSinkWriterOperatorFactory<>(sink, schemaOperatorID));
    }
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 08:42:33 UTC 2024,,,,,,,,,,"0|z1opr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/24 08:42;renqs;flink-cdc master: 33891869a9fffa2abf8b8ae03915d0ddccdaf5ec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve InstantiationUtil for checking nullary public constructor,FLINK-35148,13576284,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,liuml07,liuml07,liuml07,17/Apr/24 22:28,24/Apr/24 10:10,04/Jun/24 20:40,24/Apr/24 10:10,,,,,,,,,1.20.0,,,API / Core,,,,0,pull-request-available,,"{{InstantiationUtil#hasPublicNullaryConstructor}} checks whether the given class has a public nullary constructor. The implementation can be improved a bit: the `Modifier#isPublic` check within the for-loop can be skipped as the {{Class#getConstructors()}} only returns public constructors.

We can also add a negative unit test for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 10:10:23 UTC 2024,,,,,,,,,,"0|z1opnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/24 10:10;xtsong;master (1.20): 9cc649898d018ca87fa83ae9831ca9bee856f70b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkMaterializer throws StateMigrationException when widening the field type in the output table,FLINK-35147,13576278,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sharonxr55,sharonxr55,17/Apr/24 21:32,17/Apr/24 21:36,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,0,,,"When a field type in the output table is changed from int -> bigint or timestamp(3) -> timestamp(6), SinkMaterializer would fail to restore state. This is unexpected as the change is backward compatible. The new type should be able to ""accept"" all the old values that had narrower type. 

Note that the planner works fine and would accept such change. 

To reproduce

1. run the below SQL 
{code:sql}
CREATE TABLE ltable (
    `id` integer primary key,
    `num` int
) WITH (
    'connector' = 'upsert-kafka',
    'properties.bootstrap.servers' = 'kafka.test:9092',
    'key.format' = 'json',
    'value.format' = 'json',
    'topic' = 'test1'
);

CREATE TABLE rtable (
    `id` integer primary key,
    `ts` timestamp(3)
) WITH (
    'connector' = 'upsert-kafka',
    'properties.bootstrap.servers' = 'kafka.test:9092',
    'key.format' = 'json',
    'value.format' = 'json',
    'topic' = 'test2'
);

CREATE TABLE output (
    `id` integer primary key,
    `num` int,
    `ts` timestamp(3)
) WITH (
    'connector' = 'upsert-kafka',
    'properties.bootstrap.servers' = 'kafka.test:9092',
    'key.format' = 'json',
    'value.format' = 'json',
    'topic' = 'test3'
);

insert into
    `output`
select
    ltable.id,
    num,
    ts
from
    ltable
    join rtable on ltable.id = rtable.id
 {code}
 

2. Stop with a savepoint, then update output table with 
{code:sql}
CREATE TABLE output (
    `id` integer primary key,
    – change one of the type below would cause the issue
    `num` bigint,
    `ts` timestamp(6)
) WITH (
    'connector' = 'upsert-kafka',
    'properties.bootstrap.servers' = 'kafka.test:9092',
    'key.format' = 'json',
    'value.format' = 'json',
    'topic' = 'test3'
);
{code}
3. Restart the job with the savepoint created 

Sample screenshots

!image-2024-04-17-14-15-35-297.png|width=911,height=352!

!image-2024-04-17-14-15-21-647.png|width=1172,height=458!",,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/24 21:15;sharonxr55;image-2024-04-17-14-15-21-647.png;https://issues.apache.org/jira/secure/attachment/13068263/image-2024-04-17-14-15-21-647.png","17/Apr/24 21:15;sharonxr55;image-2024-04-17-14-15-35-297.png;https://issues.apache.org/jira/secure/attachment/13068262/image-2024-04-17-14-15-35-297.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-17 21:32:58.0,,,,,,,,,,"0|z1opm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompileAndExecuteRemotePlanITCase.testCompileAndExecutePlan,FLINK-35146,13576225,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,17/Apr/24 14:54,17/Apr/24 14:55,04/Jun/24 20:40,,1.19.1,,,,,,,,,,,,,,,0,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58960&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=16690

{code}
Apr 17 06:27:47 06:27:47.363 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 64.51 s <<< FAILURE! -- in org.apache.flink.table.sql.CompileAndExecuteRemotePlanITCase
Apr 17 06:27:47 06:27:47.364 [ERROR] org.apache.flink.table.sql.CompileAndExecuteRemotePlanITCase.testCompileAndExecutePlan[executionMode] -- Time elapsed: 56.55 s <<< FAILURE!
Apr 17 06:27:47 org.opentest4j.AssertionFailedError: Did not get expected results before timeout, actual result: null. ==> expected: <true> but was: <false>
Apr 17 06:27:47 	at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
Apr 17 06:27:47 	at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
Apr 17 06:27:47 	at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
Apr 17 06:27:47 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
Apr 17 06:27:47 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214)
Apr 17 06:27:47 	at org.apache.flink.table.sql.SqlITCaseBase.checkResultFile(SqlITCaseBase.java:216)
Apr 17 06:27:47 	at org.apache.flink.table.sql.SqlITCaseBase.runAndCheckSQL(SqlITCaseBase.java:149)
Apr 17 06:27:47 	at org.apache.flink.table.sql.SqlITCaseBase.runAndCheckSQL(SqlITCaseBase.java:133)
Apr 17 06:27:47 	at org.apache.flink.table.sql.CompileAndExecuteRemotePlanITCase.testCompileAndExecutePlan(CompileAndExecuteRemotePlanITCase.java:70)
Apr 17 06:27:47 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 17 06:27:47 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 17 06:27:47 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 17 06:27:47 

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-17 14:54:57.0,,,,,,,,,,"0|z1opag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add timeout for cluster termination,FLINK-35145,13576223,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,17/Apr/24 14:28,17/Apr/24 14:30,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Runtime / Coordination,,,,0,,,"Currently, cluster termination may be blocked forever as there's no timeout for that. For example, for an Application cluster with ZK HA enabled, when ZK cluster is down, the cluster will reach termination status, but the termination process will be blocked when trying to clean up HA data on ZK, where the ZK client will retry connecting to ZK forever. Similar phenomenon can be observed when an HDFS outage occurs.

I propose adding a timeout for the cluster termination process in ClusterEntryPoint#
shutDownAsync method. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 14:29:57 UTC 2024,,,,,,,,,,"0|z1opa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 14:29;Zhanghao Chen;[~fangyong] [~huweihua] WDYT about this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support various sources sync for FlinkCDC in one pipeline,FLINK-35144,13576201,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,klion26,klion26,17/Apr/24 10:44,20/Apr/24 06:42,04/Jun/24 20:40,18/Apr/24 09:57,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,,,"Currently, the FlinkCDC pipeline can only support a single source in one pipeline, we need to start multiple pipelines when there are various sources. 

For upstream which uses sharding, we need to sync multiple sources in one pipeline, the current pipeline can't do this because it can only support a single source.

This issue wants to support the sync of multiple sources in one pipeline.",,,,,,,,,,,,,,,,,,,,FLINK-35099,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 20 06:42:58 UTC 2024,,,,,,,,,,"0|z1op54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/24 01:57;loserwang1024;You mean that sync multiple sources to same sink?;;;","20/Apr/24 06:42;melin;Yes, the business system database consists of multiple database instances, up to 16 service instances, merged and synchronized to kafka.

分库分表业务场景，数据库分布是在多个服务器实例上;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose newly added tables capture in mysql pipeline connector,FLINK-35143,13576200,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,loserwang1024,loserwang1024,loserwang1024,17/Apr/24 10:16,27/May/24 10:22,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Currently, mysql pipeline connector still don't allowed to capture newly added tables.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 02:27:05 UTC 2024,,,,,,,,,,"0|z1op4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/24 02:27;loserwang1024;I'd like to do it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-rabbitmq v3.1.0 for Flink 1.19,FLINK-35142,13576185,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:13,19/Apr/24 10:34,04/Jun/24 20:40,,,,,,,,,,rabbitmq-3.1.0,,,Connectors/ RabbitMQ,,,,0,pull-request-available,,https://github.com/apache/flink-connector-rabbitmq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-17 09:13:27.0,,,,,,,,,,"0|z1op1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-pulsar v4.2.0 for Flink 1.19,FLINK-35141,13576183,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:12,24/Apr/24 08:32,04/Jun/24 20:40,,,,,,,,,,pulsar-4.2.0,,,Connectors / Pulsar,,,,0,pull-request-available,,https://github.com/apache/flink-connector-pulsar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-17 09:12:56.0,,,,,,,,,,"0|z1op14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-opensearch vX.X.X for Flink 1.19,FLINK-35140,13576182,13576170,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,snuyanzin,dannycranmer,dannycranmer,17/Apr/24 09:12,17/Apr/24 09:15,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Opensearch,,,,0,,,"[https://github.com/apache/flink-connector-opensearch]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-17 09:12:11.0,,,,,,,,,,"0|z1op0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-mongodb v1.2.0 for Flink 1.19,FLINK-35139,13576181,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:11,19/Apr/24 10:28,04/Jun/24 20:40,,,,,,,,,,mongodb-1.2.0,,,Connectors / MongoDB,,,,0,pull-request-available,,https://github.com/apache/flink-connector-mongodb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 10:34:11 UTC 2024,,,,,,,,,,"0|z1op0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 09:57;jiabaosun;mongodb main: 660ffe4f33f3ce60da139159741644f48295652d;;;","17/Apr/24 10:17;dannycranmer;RC1 Vote: https://lists.apache.org/thread/s18g7obgp4sbdtl73571976vqvy1ftk8;;;","18/Apr/24 10:34;dannycranmer;RC2 Vote: https://lists.apache.org/thread/2982v6n5q0bgldrp919t5t6d19xsl710;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-kafka v3.2.0 for Flink 1.19,FLINK-35138,13576180,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:11,22/May/24 09:57,04/Jun/24 20:40,,,,,,,,,,kafka-3.2.0,,,Connectors / Kafka,,,,0,pull-request-available,,https://github.com/apache/flink-connector-kafka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 09:57:29 UTC 2024,,,,,,,,,,"0|z1op0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 09:06;dannycranmer;RC1 Vote: https://lists.apache.org/thread/7shs2wzb0jkfdyst3mh6d9pn3z1bo93c;;;","13/May/24 12:36;yazgoo;Hello, do you have an update on this ticket ?
Looks like the vote was fine ?

Thanks !;;;","20/May/24 05:29;tamirsagi;Hi 

Any update about the status here? 

Thanks!;;;","22/May/24 09:57;dannycranmer;We require 3 binding votes to release, we currently have 0. I am pinging the PMC to try to get some traction ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-jdbc v3.2.0 for Flink 1.19,FLINK-35137,13576179,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:10,19/Apr/24 10:28,04/Jun/24 20:40,,,,,,,,,,jdbc-3.2.0,,,Connectors / JDBC,,,,0,,,https://github.com/apache/flink-connector-jdbc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 10:33:52 UTC 2024,,,,,,,,,,"0|z1op08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 13:16;dannycranmer;RC1 Vote: https://lists.apache.org/thread/q6dmc5dbz7kcfvpo99pj2sh5mzhffgl5;;;","18/Apr/24 10:33;dannycranmer;RC2 Vote: https://lists.apache.org/thread/b7xbjo4crt1527ldksw4nkwo8vs56csy;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-hbase v4.0.0 for Flink 1.19,FLINK-35136,13576178,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,dannycranmer,dannycranmer,17/Apr/24 09:10,24/Apr/24 10:33,04/Jun/24 20:40,,,,,,,,,,hbase-4.0.0,,,Connectors / HBase,,,,0,pull-request-available,,https://github.com/apache/flink-connector-hbase,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 13:18:06 UTC 2024,,,,,,,,,,"0|z1op00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 10:55;ferenc-csaky;Currently we have a v3.0.0 HBase connector release, that supports Flink 1.16 and 1.17. There were some added features, but since then HBase 1.x support [got removed|https://github.com/apache/flink-connector-hbase/commit/9cbc109b368e26770891e31750c370d295d629e9] from the connector, so my understanding is the new version should be v4.0.0, supporting Flink 1.18 and 1.19. WDYT?

I can take care of the necessary changes and create a PR, feel free to assign the ticket to me.;;;","17/Apr/24 13:18;dannycranmer;Thanks for picking this up [~ferenc-csaky] . I see that v4.0.0 fix version is already created, and agree it makes sense to bump major version since we are dropping HBase 1.x support. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-gcp-pubsub v3.1.0 for Flink 1.19,FLINK-35135,13576177,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:09,19/Apr/24 10:28,04/Jun/24 20:40,,,,,,,,,,gcp-pubsub-3.1.0,,,Connectors / Google Cloud PubSub,,,,0,pull-request-available,,https://github.com/apache/flink-connector-gcp-pubsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 10:33:38 UTC 2024,,,,,,,,,,"0|z1oozs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/24 10:33;dannycranmer;RC1 Vote: https://lists.apache.org/thread/b7l1r0y7nwox2vhf2z3kwjn41clf6w1v;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-elasticsearch vX.X.X for Flink 1.19,FLINK-35134,13576176,13576170,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hong,dannycranmer,dannycranmer,17/Apr/24 09:08,25/Apr/24 08:45,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,https://github.com/apache/flink-connector-elasticsearch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 08:45:31 UTC 2024,,,,,,,,,,"0|z1oozk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 08:45;Weijie Guo;Drop support for Flink 1.17 via: 44a0a144e2e135641ed3fe1144ad5ddf4cf5c5ff.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-cassandra v3.2.0 for Flink 1.19,FLINK-35133,13576175,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:06,22/Apr/24 13:13,04/Jun/24 20:40,,,,,,,,,,cassandra-3.2.0,,,Connectors / Cassandra,,,,0,pull-request-available,,https://github.com/apache/flink-connector-cassandra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 22 13:05:34 UTC 2024,,,,,,,,,,"0|z1oozc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 13:05;dannycranmer;3.2.0-RC1 Vote thread: https://lists.apache.org/thread/28x1n2p5cxl66t4w6vrq06o9c1j050fj;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release flink-connector-aws v4.3.0 for Flink 1.19,FLINK-35132,13576171,13576170,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:05,19/Apr/24 10:27,04/Jun/24 20:40,,,,,,,,,,aws-connector-4.3.0,,,Connectors / AWS,,,,0,,,https://github.com/apache/flink-connector-aws,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 10:27:42 UTC 2024,,,,,,,,,,"0|z1ooyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 09:17;dannycranmer;RC1 Vote: [https://lists.apache.org/thread/0nw9smt23crx4gwkf6p1dd4jwvp1g5s0];;;","19/Apr/24 10:27;dannycranmer;RC2 Vote: https://lists.apache.org/thread/dof3tprw5s19jfwhk6yxs5qxbb4x5ss8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support and Release Connectors for Flink 1.19,FLINK-35131,13576170,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,17/Apr/24 09:05,17/Apr/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,This is the parent task to contain connector support and releases for Flink 1.19.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-17 09:05:03.0,,,,,,,,,,"0|z1ooy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify AvailabilityNotifierImpl to support speculative scheduler and improve performance,FLINK-35130,13576165,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,17/Apr/24 08:28,21/May/24 10:35,04/Jun/24 20:40,21/May/24 10:35,1.20.0,,,,,,,,1.20.0,,,Runtime / Network,,,,0,pull-request-available,,"The AvailabilityNotifierImpl in SingleInputGate has maps storing the channel ids. But the map key is the result partition id, which will change according to the different attempt numbers when speculation is enabled.  This can be resolved by using `inputChannels` to get channel and the map key of inputChannels will not vary with the attempts. 
In addition, using that map instead can also improve performance for large scale jobs because no extra maps are created.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 10:35:38 UTC 2024,,,,,,,,,,"0|z1oox4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/24 10:35;tanyuxin;master(1.20.0): 46aaea8083047fc86c35491336d795ddcd565128;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Postgres source commits the offset after every multiple checkpoint cycles.,FLINK-35129,13576136,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,m.orazow,loserwang1024,loserwang1024,17/Apr/24 03:03,04/Jun/24 08:56,04/Jun/24 20:40,04/Jun/24 08:56,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"After entering the Stream phase, the offset consumed by the global slot is committed upon the completion of each checkpoint, preventing log files from being unable to be recycled continuously, which could lead to insufficient disk space.

However, the job can only restart from the latest checkpoint or savepoint. if restored from an earlier state, WAL may already have been recycled.

 

The way to solve it is to commit the offset after every multiple checkpoint cycles. The number of checkpoint cycles is determine by connector option, and the default value is 3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 08:56:48 UTC 2024,,,,,,,,,,"0|z1ooqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 03:06;loserwang1024;[~m.orazow] , would you like to take it?;;;","18/Apr/24 02:23;m.orazow;Hey [~loserwang1024] , I would be happy to work on it! Yes, could you please assign it to me?;;;","04/Jun/24 08:56;leonard;Implemented via master: 5b28d1a579919b29acac6acded46d9bee5596bde;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Re-calculate the starting change log offset after the new table added,FLINK-35128,13576129,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,loserwang1024,loserwang1024,loserwang1024,17/Apr/24 02:02,26/Apr/24 03:30,04/Jun/24 20:40,26/Apr/24 03:30,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"In mysql cdc, re-calculate the starting binlog offset after the new table added in MySqlBinlogSplit#appendFinishedSplitInfos, while there lack of same action in StreamSplit#appendFinishedSplitInfos. This will cause data loss if any newly added table snapshot split's highwatermark is smaller.

 

Some unstable test problem occurs because of it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 06:04:21 UTC 2024,,,,,,,,,,"0|z1oop4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 06:04;renqs;flink-cdc master: bdca0e328bce0aa2dc05153bc67da4c4875586fd

flink-cdc release-3.1: aa2ffd9c2747cde21d09305c2852debb8937b1fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC ValuesDataSourceITCase crashed due to OutOfMemoryError,FLINK-35127,13576127,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kunni,jiabaosun,jiabaosun,17/Apr/24 01:53,29/Apr/24 08:24,04/Jun/24 20:40,29/Apr/24 08:24,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,test-stability,"{code}
[INFO] Running org.apache.flink.cdc.connectors.values.source.ValuesDataSourceITCase
Error: Exception in thread ""surefire-forkedjvm-command-thread"" java.lang.OutOfMemoryError: Java heap space
Error:  
Error:  Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""taskmanager_4-main-scheduler-thread-2""
Error:  
Error:  Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""System Time Trigger for Source: values (1/4)#0""
{code}

https://github.com/apache/flink-cdc/actions/runs/8698450229/job/23858750352?pr=3221#step:6:1949
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 08:24:15 UTC 2024,,,,,,,,,,"0|z1oooo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 01:54;jiabaosun;Hi [~kunni],
Could you help take a look?;;;","18/Apr/24 09:55;renqs;I increased the priority to blocker as many PRs are waiting for CI results.;;;","29/Apr/24 08:24;renqs;flink-cdc master: d4ed7db8dca18906bb8a1d7738cb158da0222bc3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Improve checkpoint progress health check config and enable by default,FLINK-35126,13576054,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gyfora,gyfora,gyfora,16/Apr/24 15:31,13/May/24 11:51,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,"Currently the checkpoint progress health check window is configurable by Duration. This makes it hard to enable by default as the sensible interval depends on the checkpoint interval.

We should rework the config and add an alternative checkpoint interval multiplier based config which could be set by default to 3 (default window is 3x checkpoint interval )

If checkpointing is not enabled in config the health check would be disabled of course.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-16 15:31:02.0,,,,,,,,,,"0|z1oo8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement ValueState for ForStStateBackend,FLINK-35125,13576022,13574085,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,lijinzhong,lijinzhong,16/Apr/24 12:07,26/Apr/24 02:06,04/Jun/24 20:40,26/Apr/24 02:06,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 02:06:59 UTC 2024,,,,,,,,,,"0|z1oo1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 02:06;masteryhx;merged e0570102 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector Release Fails to run Checkstyle,FLINK-35124,13576010,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,16/Apr/24 10:24,18/Apr/24 10:33,04/Jun/24 20:40,18/Apr/24 10:33,,,,,,,,,,,,Build System,,,,0,pull-request-available,,"During a release of the AWS connectors the build was failing at the \{{./tools/releasing/shared/stage_jars.sh}} step due to a checkstyle error.

 
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:3.1.2:check (validate) on project flink-connector-aws: Failed during checkstyle execution: Unable to find suppressions file at location: /tools/maven/suppressions.xml: Could not find resource '/tools/maven/suppressions.xml'. -> [Help 1] {code}
 

Looks like it is caused by this [https://github.com/apache/flink-connector-shared-utils/commit/a75b89ee3f8c9a03e97ead2d0bd9d5b7bb02b51a]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 09:54:21 UTC 2024,,,,,,,,,,"0|z1onyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 10:25;dannycranmer;[~echauchot] do you recall why the tools directory was removed from the clone? ;;;","17/Apr/24 08:48;echauchot;[~dannycranmer] yes, it was because it lead to an empty tools directory in the source release. I'll take a look for the suppressions.xml;;;","17/Apr/24 13:22;dannycranmer;Thanks [~echauchot]. I believe it is possible to have other tool configurations in there. Maybe we can tell rsync it to ignore empty folders, this might work {{--prune-empty-dirs}} (https://serverfault.com/questions/598662/copy-directory-tree-without-empty-directories);;;","17/Apr/24 13:40;echauchot;To be more precise, I meant: before function create_pristine_source had _--exclude ""tools/releasing/shared""_ which was to avoid having the submodule release utils scripts inside the source release. But it lead to having the _tools_ directory in the release. In the case of regular connectors it was containing ci and maven subdirs and in case of connector-parent, the tools directory ended up empty. So it was better to remove the whole tools directory from the release. 

 

I'm currently checking on the cassandra connector for the checkstyle error you mentioned. I'll get back to you in that thread

 ;;;","17/Apr/24 13:50;echauchot;Fount it ! The suppression.xml issue you mentioned is not related to the change in the _utils.sh script ([https://github.com/apache/flink-connector-shared-utils/commit/a75b89ee3f8c9a03e97ead2d0bd9d5b7bb02b51a]) at all. It is a bug in the conf. Take a close look, there is an extra / in the path:

{code:java}
Unable to find suppressions file at location: /tools/maven/suppressions.xml
{code}

 
It should refer  to *tools* directory from the current dir and not to *tools* directory from */*

 ;;;","17/Apr/24 13:56;dannycranmer;ok cool. This sounds like a better fix. Btw I was reverting your change locally and it also worked ;)

Got the same error for AWS/MongoDB and JDBC connector;;;","17/Apr/24 14:40;dannycranmer;Actually I still think this is the issue, since it runs the maven commands on the ""pristine source"", which is missing the maven config directory: https://github.com/apache/flink-connector-shared-utils/blob/release_utils/stage_jars.sh#L54. 

 

If it were an invalid path, it would fail on all builds, not just during the release. ;;;","17/Apr/24 16:21;echauchot;It was failing on cassandra before the _utils.sh change, at the time I did a quick workaround by copying the suppression.xml file to /tools.

I think it was failing also for the other connectors.;;;","18/Apr/24 08:42;dannycranmer;This resulted in bad source archives being generated for JDBC and MongoDB connectors. I am going to revert this change for now:
 * [https://lists.apache.org/thread/q6dmc5dbz7kcfvpo99pj2sh5mzhffgl5]
 * [https://lists.apache.org/thread/s18g7obgp4sbdtl73571976vqvy1ftk8];;;","18/Apr/24 09:14;dannycranmer;>  In the case of regular connectors it was containing ci and maven subdirs


I think this is ok, since it is _source code_ (configuration) within the git repository after all;;;","18/Apr/24 09:45;echauchot;Ok fair enough to put back {_}ci{_},  _maven_ and _releasing_ dirs in the pristine source and exclude only shared (because it refers to an external repo). But did you find the reason why the suppressions.xml path ends up being /tools/maven/suppressions.xml and not tools/maven/suppressions.xml ?

 

 ;;;","18/Apr/24 09:54;dannycranmer;Merged commit [{{c411561}}|https://github.com/apache/flink-connector-shared-utils/commit/c4115618085ac046033368e8e3a7eee59874608f] into apache:release_utils ;;;",,,,,,,,,,,,,,,,,
Flink Kubernetes Operator should not do deleteHAData ,FLINK-35123,13575985,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Fei Feng,Fei Feng,16/Apr/24 08:18,16/Apr/24 08:54,04/Jun/24 20:40,,kubernetes-operator-1.7.0,kubernetes-operator-1.8.0,,,,,,,,,,Kubernetes Operator,,,,0,,,"we use flink HA based on zookeeper. when a lots of FlinkDeployment was deleting, operator will be spend to many time in cleanHaData. the jstack show that reconcile thread was hang on disconnect with zookeeper. this made deleting flinkdeployment was slowly. 

!image-2024-04-16-15-56-33-426.png|width=502,height=263!

 

I don't understand why flink kubernetes operator need cleanHAdata , as [~aitozi] comment in PR  [FLINK-26336 Call cancel on deletion & clean up configmaps as well #28|https://github.com/apache/flink-kubernetes-operator/pull/28#discussion_r815968841]
{quote}it's a bit of out of scope of the operator responsibility or ability
{quote}
and I'm totally agree with his point. 

and I want to know why we call don't call RestClusterClient#shutDownCluster interface, which is

1. more graceful and reasonable (operator need not care whether flink app enable ha or not) 2. compatible across flink versions .   

 

 ",,,,,,,,,,,,,,,,,,,,,,,FLINK-27273,,,"16/Apr/24 07:56;Fei Feng;image-2024-04-16-15-56-33-426.png;https://issues.apache.org/jira/secure/attachment/13068219/image-2024-04-16-15-56-33-426.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 08:54:29 UTC 2024,,,,,,,,,,"0|z1ontc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 08:54;gyfora;I agree that if the rest api is accessible we could call shutdown and not touch the HA metadata. But there are some cases when you Need to delete HA metadata explicitly:
 - Cluster is not in a healthy state (rest api not available)
 - Job is previously suspended with last-state upgrade mode where HA metadata is left

Also in Kubernetes HA configuration which is much more common than ZK the HA metadata cleanup is much faster than anything else. It's a simple ConfigMap deletion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Watermark Alignment for DynamicKafkaSource,FLINK-35122,13575972,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masc,masc,masc,16/Apr/24 07:50,16/Apr/24 07:50,04/Jun/24 20:40,,kafka-3.1.0,,,,,,,,,,,Connectors / Kafka,,,,0,,,Implement Watermark Alignment for DynamicKafkaSource,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-16 07:50:47.0,,,,,,,,,,"0|z1onqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC pipeline connector should verify requiredOptions and optionalOptions,FLINK-35121,13575971,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,loserwang1024,loserwang1024,16/Apr/24 07:39,30/May/24 08:22,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"At present, though we provide org.apache.flink.cdc.common.factories.Factory#requiredOptions and org.apache.flink.cdc.common.factories.Factory#optionalOptions, but both are not used anywhere. This means not verifying requiredOptions and optionalOptions.

Thus, like what DynamicTableFactory does, provide 
FactoryHelper to help verify requiredOptions and optionalOptions.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 01:54:12 UTC 2024,,,,,,,,,,"0|z1onq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 07:39;loserwang1024;[~renqs] , CC;;;","28/May/24 01:54;kwafor;[~loserwang1024] I'm willing to take this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Doris Pipeline connector integration test cases,FLINK-35120,13575961,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xiqian_yu,xiqian_yu,xiqian_yu,16/Apr/24 06:46,26/Apr/24 07:19,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Currently, Flink CDC Doris pipeline connector has very limited test cases (which only covers row convertion). Adding an ITCase testing its data pipeline and metadata applier should help improving connector's reliability.",,,,,,,,,,,,,,FLINK-35092,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-16 06:46:29.0,,,,,,,,,,"0|z1ono0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UPDATE DataChangeEvent deserialized data is incorrect,FLINK-35119,13575951,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhongqishang,zhongqishang,16/Apr/24 03:44,27/Apr/24 06:14,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"When DebeziumChangelogMode is upsert, the before of DataChangeEvent is null, deserialized data is incorrect.
 
Add test data  in org.apache.flink.cdc.runtime.serializer.event.DataChangeEventSerializerTest
 
{code:java}
DataChangeEvent.updateEvent(
        TableId.tableId(""namespace"", ""schema"", ""table""), null, after),
DataChangeEvent.updateEvent(
        TableId.tableId(""namespace"", ""schema"", ""table""), null, after, meta) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-16 03:44:05.0,,,,,,,,,,"0|z1onls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StreamingHiveSource cannot track tables that have more than 32,767 partitions",FLINK-35118,13575949,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,heywxl,heywxl,16/Apr/24 02:43,16/Apr/24 02:43,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Connectors / Hive,,,,0,,,"*Description:*

The Streaming Hive Source cannot track tables that have more than 32,767 partitions.

 *Root Cause:*

The Streaming Hive Source uses the following lines to get all partitions of a table:

([git hub link|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HivePartitionFetcherContextBase.java#L130])

HivePartitionFetcherContextBase.java:
{code:java}
    @Override
    public List<ComparablePartitionValue> getComparablePartitionValueList() throws Exception {
        List<ComparablePartitionValue> partitionValueList = new ArrayList<>();
        switch (partitionOrder) {
            case PARTITION_NAME:
                List<String> partitionNames =
                        metaStoreClient.listPartitionNames(
                                tablePath.getDatabaseName(),
                                tablePath.getObjectName(),
                                Short.MAX_VALUE);
                for (String partitionName : partitionNames) {
                    partitionValueList.add(getComparablePartitionByName(partitionName));
                }
                break;
            case CREATE_TIME:
                Map<List<String>, Long> partValuesToCreateTime = new HashMap<>();
                partitionNames =
                        metaStoreClient.listPartitionNames(
                                tablePath.getDatabaseName(),
                                tablePath.getObjectName(),
                                Short.MAX_VALUE); {code}
Where the `metaStoreClient` is a wrapper of the `IMetaStoreClient`, and the function `listPartitionNames` can only list no more than `Short.MAX_VALUE` partitions, which is 32,767.

 

For tables that have more partitions, the source fails to track new partitions and read from it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-16 02:43:29.0,,,,,,,,,,"0|z1onlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncScalarFunction has a dependency issue.,FLINK-35117,13575948,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,heywxl,heywxl,16/Apr/24 02:14,16/Apr/24 02:14,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Table SQL / Planner,,,,0,easyfix,,"Hi,
 
I found a ClassNotFound exception when using Flink 1.19's AsyncScalarFunction. 
 
*Stack trace:*
 
{quote}Caused by: java.lang.ClassNotFoundException: org.apache.commons.text.StringSubstitutor

at java.net.URLClassLoader.findClass(Unknown Source) ~[?:?]

at java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]

at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:150) ~[flink-dist-1.19.0.jar:1.19.0]

at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:113) ~[flink-dist-1.19.0.jar:1.19.0]

at java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]

at org.apache.flink.table.planner.codegen.AsyncCodeGenerator.generateProcessCode(AsyncCodeGenerator.java:173) ~[?:?]

at org.apache.flink.table.planner.codegen.AsyncCodeGenerator.generateFunction(AsyncCodeGenerator.java:77) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecAsyncCalc.getAsyncFunctionOperator(CommonExecAsyncCalc.java:146) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecAsyncCalc.createAsyncOneInputTransformation(CommonExecAsyncCalc.java:126) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecAsyncCalc.translateToPlanInternal(CommonExecAsyncCalc.java:89) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:168) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:259) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:168) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:259) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:177) ~[?:?]

at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:168) ~[?:?]

at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85) ~[?:?]
{quote}
 
*Root cause:*
 
`PlannerModule` uses a URLClassloader to load class and has an exceptional list to load owner owned classes:
 
{code:java}
class PlannerModule {

    /**
     * The name of the table planner dependency jar, bundled with flink-table-planner-loader module
     * artifact.
     */
    static final String FLINK_TABLE_PLANNER_FAT_JAR = ""flink-table-planner.jar"";

    private static final String HINT_USAGE =
            ""mvn clean package -pl flink-table/flink-table-planner,flink-table/flink-table-planner-loader -DskipTests"";

    private static final String[] OWNER_CLASSPATH =
            Stream.concat(
                            Arrays.stream(CoreOptions.PARENT_FIRST_LOGGING_PATTERNS),
                            Stream.of(
                                    // These packages are shipped either by
                                    // flink-table-runtime or flink-dist itself
                                    ""org.codehaus.janino"",
                                    ""org.codehaus.commons"",
                                    ""org.apache.commons.lang3"",
                                    ""org.apache.commons.math3"",
                                    // with hive dialect, hadoop jar should be in classpath,
                                    // also, we should make it loaded by owner classloader,
                                    // otherwise, it'll throw class not found exception
                                    // when initialize HiveParser which requires hadoop
                                    ""org.apache.hadoop""))
                    .toArray(String[]::new);  {code}
But the group of `org.apache.commons.text` is not on the list.

 

*Fix:*

Add `org.apache.commons.text` to the list",image: 1.19.0-scala_2.12-java11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2024-04-16 02:14:26.0,,,,,,,,,,"0|z1onl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade JOSDK dependency to 4.8.3,FLINK-35116,13575918,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mbalassi,mbalassi,mbalassi,15/Apr/24 19:00,17/Apr/24 16:29,04/Jun/24 20:40,,,,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,pull-request-available,,"This bring a much needed fix for the operator HA behaviour:

https://github.com/operator-framework/java-operator-sdk/releases/tag/v4.8.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-15 19:00:58.0,,,,,,,,,,"0|z1oneg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis connector writes wrong Kinesis sequence number at stop with savepoint,FLINK-35115,13575894,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,a.pilipenko,vadim.vararu,vadim.vararu,15/Apr/24 14:32,23/May/24 07:23,04/Jun/24 20:40,19/Apr/24 09:51,1.15.4,1.16.3,1.17.2,1.18.1,aws-connector-4.0.0,aws-connector-4.1.0,aws-connector-4.2.0,,aws-connector-4.3.0,,,Connectors / Kinesis,,,,0,kinesis,pull-request-available,"Having an exactly-once Kinesis -> Flink -> Kafka job and triggering a stop-with-savepoint, Flink duplicates in Kafka all the records between the last checkpoint and the savepoint at resume:
 * Event1 is written to Kinesis
 * Event1 is processed by Flink 
 * Event1 is committed to Kafka at the checkpoint
 * ............................................................................
 * Event2 is written to Kinesis
 * Event2 is processed by Flink
 * Stop with savepoint is triggered manually
 * Event2 is committed to Kafka
 * ............................................................................
 * Job is resumed from the savepoint
 * *{color:#FF0000}Event2 is written again to Kafka at the first checkpoint{color}*

 

{color:#172b4d}I believe that it's a Kinesis connector issue for 2 reasons:{color}
 * I've checked the actual Kinesis sequence number in the _metadata file generated at stop-with-savepoint and it's the one from the checkpoint before the savepoint  instead of being the one of the last record committed to Kafka.
 * I've tested exactly the save job with Kafka as source instead of Kinesis as source and the behaviour does not reproduce.","The issue happens in a *Kinesis -> Flink -> Kafka* exactly-once setup with:
 * Flink versions checked 1.16.3 and 1.18.1
 * Kinesis connector checked 1.16.3 and 4.2.0-1.18
 * checkpointing configured at 1 minute with EXACTLY_ONCE mode: 
{code:java}
StreamExecutionEnvironment execEnv = StreamExecutionEnvironment.getExecutionEnvironment (); execEnv.enableCheckpointing (60000,EXACTLY_ONCE); execEnv.getCheckpointConfig ().setCheckpointTimeout (90000); execEnv.getCheckpointConfig ().setCheckpointStorage (CHECKPOINTS_PATH); {code}

 * Kafka sink configured with EXACTLY_ONCE semantic/delivery guarantee:
{code:java}
Properties sinkConfig = new Properties ();
sinkConfig.put (""transaction.timeout.ms"", 480000);

KafkaSink<String> sink = KafkaSink.<String>builder ()
    .setBootstrapServers (""localhost:9092"")
    .setTransactionalIdPrefix (""test-prefix"")
    .setDeliverGuarantee (EXACTLY_ONCE)
    .setKafkaProducerConfig (sinkConfig)
    .setRecordSerializer (
        (KafkaRecordSerializationSchema<String>) (element, context, timestamp) -> new ProducerRecord<> (
            ""test-output-topic"", null, element.getBytes ()))
    .build (); {code}

 * Kinesis consumer defined as: 
{code:java}
FlinkKinesisConsumer<ByteBuffer> flinkKinesisConsumer = new
    FlinkKinesisConsumer<> (""test-stream"",
    new AbstractDeserializationSchema<> () {
        @Override
        public ByteBuffer deserialize (byte[] bytes) {
            // Return
            return ByteBuffer.wrap (bytes);
        }
    }, props); {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Thu May 23 07:23:09 UTC 2024,,,,,,,,,,"0|z1on94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 15:01;hong;[~a.pilipenko]  Assigned to you, as you mentioned you are looking into it;;;","15/Apr/24 15:12;dannycranmer;Stop with savepoint checkpointing logic is no different to regular checkpoints. This would be a more fundamental issue than limited to stop-with-savepoint. 

 ;;;","15/Apr/24 15:14;hong;[~dannycranmer]  That is true, unless it is a problem with 2PC on the sink. The main difference for stop-with-savepoint is on the side effects executed on the sink.

Let's try and replicate this first!;;;","15/Apr/24 15:34;a.pilipenko;[~dannycranmer] Agree, trying to reproduce it now;;;","16/Apr/24 06:56;vadim.vararu;FYI it's reproducible in Kinesis -> Flink -> S3 sink also.;;;","16/Apr/24 07:49;a.pilipenko;[~vadim.vararu] are you able to reproduce this issue consistently?

Can you enable debug logs and share these logs here? Kinesis connectors logs information about sequence numbers it is storing during checkpointing, which may shed some light on the issue.;;;","17/Apr/24 11:28;vadim.vararu;[~a.pilipenko] Yes, I can reproduce this consistently.

 

I've enabled this logger:

 
{code:java}
logger.kinesis.name = org.apache.flink.streaming.connectors.kinesis
logger.kinesis.level = DEBUG {code}
and got these last logs on TM before triggering the stop-with-savepoint (the log at 2024-04-17 14:05:11,753 is the last checkpoint):

 

 
{code:java}
2024-04-17 14:05:06,330 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 is trying to discover new shards that were created due to resharding ...

2024-04-17 14:05:11,753 DEBUG org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer [] - Snapshotting state ...

2024-04-17 14:05:11,753 DEBUG org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer [] - Snapshotted state, last processed sequence numbers: {StreamShardMetadata{streamName='kinesis-dev-1-20210513-v3-contract-impression', shardId='shardId-000000000000', parentShardId='null', adjacentParentShardId='null', startingHashKey='0', endingHashKey='340282366920938463463374607431768211455', startingSequenceNumber='49618213417511572504838906841289148356109207047268990978', endingSequenceNumber='null'}=49646826022549514041791139259235973731492142339223191554}, checkpoint id: 1, timestamp: 1713351911711

2024-04-17 14:05:16,652 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 is trying to discover new shards that were created due to resharding ...

2024-04-17 14:05:26,930 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 is trying to discover new shards that were created due to resharding ...

2024-04-17 14:05:27,032 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer [] - stream: kinesis-dev-1-20210513-v3-contract-impression, shard: shardId-000000000000, millis behind latest: 0, batch size: 120

24-04-17 14:05:37,229 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 is trying to discover new shards that were created due to resharding ...

2024-04-17 14:05:43,079 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer [] - stream: kinesis-dev-1-20210513-v3-contract-impression, shard: shardId-000000000000, millis behind latest: 0, batch size: 1

2024-04-17 14:05:47,752 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 is trying to discover new shards that were created due to resharding ...

2024-04-17 14:05:50,677 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer [] - stream: kinesis-dev-1-20210513-v3-contract-impression, shard: shardId-000000000000, millis behind latest: 0, batch size: 1{code}
now I trigger the stop-with-savepoint:
{code:java}
2024-04-17 14:05:52,168 INFO org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Starting shutdown of shard consumer threads and AWS SDK resources of subtask 0 ...

2024-04-17 14:05:52,169 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Cancelled discovery

2024-04-17 14:05:52,169 INFO org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Shutting down the shard consumer threads of subtask 0 ...

2024-04-17 14:05:52,645 DEBUG org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer [] - snapshotState() called on closed source; returning null.

2024-04-17 14:05:52,669 INFO org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Starting shutdown of shard consumer threads and AWS SDK resources of subtask 0 ...

2024-04-17 14:05:52,670 INFO org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Shutting down the shard consumer threads of subtask 0 ...  {code}
and here I start from the savepoint:
{code:java}
2024-04-17 14:12:56,691 INFO  org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer [] - Setting restore state in the FlinkKinesisConsumer. Using the following offsets: {org.apache.flink.streaming.connectors.kinesis.model.StreamShardMetadata$EquivalenceWrapper@f5191c51=49646826022549514041791139259235973731492142339223191554}

2024-04-17 14:12:58,370 INFO  org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer [] - Subtask 0 is seeding the fetcher with restored shard StreamShardHandle{streamName='kinesis-dev-1-20210513-v3-contract-impression', shard='{ShardId: shardId-000000000000,HashKeyRange: {StartingHashKey: 0,EndingHashKey: 340282366920938463463374607431768211455},SequenceNumberRange: {StartingSequenceNumber: 49618213417511572504838906841289148356109207047268990978,}}'}, starting state set to the restored sequence number 49646826022549514041791139259235973731492142339223191554

2024-04-17 14:12:58,371 INFO  org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 will start consuming seeded shard StreamShardHandle{streamName='kinesis-dev-1-20210513-v3-contract-impression', shard='{ShardId: shardId-000000000000,HashKeyRange: {StartingHashKey: 0,EndingHashKey: 340282366920938463463374607431768211455},SequenceNumberRange: {StartingSequenceNumber: 49618213417511572504838906841289148356109207047268990978,}}'} from sequence number 49646826022549514041791139259235973731492142339223191554 with ShardConsumer 0

2024-04-17 14:12:59,211 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Subtask 0 is trying to discover new shards that were created due to resharding ...

2024-04-17 14:12:59,297 DEBUG org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer [] - stream: kinesis-dev-1-20210513-v3-contract-impression, shard: shardId-000000000000, millis behind latest: 0, batch size: 3 {code}
There are no other logs between the above actions.

 

I did not dive into the source code, but it seems odd to me that these type of line:
{code:java}
2024-04-17 14:05:11,753 DEBUG org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer [] - Snapshotted state, last processed sequence numbers: {StreamShardMetadata{streamName='kinesis-dev-1-20210513-v3-contract-impression', shardId='shardId-000000000000', parentShardId='null', adjacentParentShardId='null', startingHashKey='0', endingHashKey='340282366920938463463374607431768211455', startingSequenceNumber='49618213417511572504838906841289148356109207047268990978', endingSequenceNumber='null'}=49646826022549514041791139259235973731492142339223191554}, checkpoint id: 1, timestamp: 1713351911711 {code}
has been logged at the last checkpoint but there is not anything similar at the stop-with-savepoint.

 

I've let pass only the logs for _org.apache.flink.streaming.connectors.kinesis._ Let me know if you need any other logs. 

 ;;;","17/Apr/24 14:09;a.pilipenko;[~vadim.vararu] thank you for information. Savepoint should produce similar log to one produced for checkpoint - mechanism is the same.;;;","17/Apr/24 14:28;a.pilipenko;Investigating further;;;","17/Apr/24 19:00;a.pilipenko;snapshotState method in FlinkKinesisConsumer skip saving state if operator had been cancelled:
{noformat}
2024-04-17 14:05:52,645 DEBUG org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer [] - snapshotState() called on closed source; returning null.{noformat}
This leads to state not being updated in state backend during stop-with-savepoint workflow. Created a PR to resolve this.;;;","19/Apr/24 04:49;vadim.vararu;[~a.pilipenko] 4.3.0 will be released for Flink 1.18 as well, right?;;;","19/Apr/24 07:04;dannycranmer;[~vadim.vararu] yes it will. 4.3.0 will support Flink 1.19 and 1.18;;;","19/Apr/24 07:48;vadim.vararu;Great, thanks for quick fix (y);;;","19/Apr/24 09:43;dannycranmer;Merged commit [{{8d29147}}|https://github.com/apache/flink-connector-aws/commit/8d29147b9e6c0a7d27399662c6023ad634363764] into apache:main ;;;","23/May/24 07:23;vadim.vararu;Hi guys,

 

Any idea when the aws-connector-4.3.0 is going to be released?;;;",,,,,,,,,,,,,,
Remove old Table API implementations,FLINK-35114,13575885,13573180,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,ferenc-csaky,ferenc-csaky,15/Apr/24 13:26,14/May/24 07:08,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Kudu,,,,0,,,"At the moment, the connector has both the old Table sink/source/catalog implementations and the matching Dynamic... implementations as well.

Going forward, the deprecated old implementation should be removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-15 13:26:49.0,,,,,,,,,,"0|z1on74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.apache.commons:commons-compress from 1.25.0 to 1.26.1 for Flink AWS connectors,FLINK-35113,13575874,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,15/Apr/24 11:57,19/Apr/24 10:11,04/Jun/24 20:40,15/Apr/24 13:20,,,,,,,,,aws-connector-4.3.0,,,Connectors / AWS,,,,0,pull-request-available,,Bump org.apache.commons:commons-compress from 1.25.0 to 1.26.1 for Flink AWS connectors,,,,,,,,,,,,,,,,FLINK-35174,,,,,,,,FLINK-35008,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 13:20:51 UTC 2024,,,,,,,,,,"0|z1on4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 13:20;dannycranmer;Merged commit [{{6717cba}}|https://github.com/apache/flink-connector-aws/commit/6717cbae8bf7b88478f96e1845bae68df2f33c86] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Membership for Row class does not include field names,FLINK-35112,13575862,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wzorgdrager,wzorgdrager,wzorgdrager,15/Apr/24 10:11,11/May/24 02:56,04/Jun/24 20:40,11/May/24 02:54,1.18.1,,,,,,,,1.19.1,1.20.0,,API / Python,,,,0,pull-request-available,,"In the Row class in PyFlink I cannot do a membership check for field names. This minimal example will show the unexpected behavior:

```

from pyflink.common import Row

row = Row(name=""Alice"", age=11)
# Expected to be True, but is False
print(""name"" in row)

person = Row(""name"", ""age"")
# This is True, as expected
print('name' in person)

```

The related code in the Row class is:
```
    def __contains__(self, item):
        return item in self._values
```


It should be relatively easy to fix with the following code:
```
    def __contains__(self, item):
        if hasattr(self, ""_fields""):
            return item in self._fields
        else:
            return item in self._values
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 11 02:54:17 UTC 2024,,,,,,,,,,"0|z1on20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 13:30;martijnvisser;[~dianfu] [~hxbks2ks] WDYT?;;;","16/Apr/24 05:27;dianfu;Sounds reasonable for me. [~wzorgdrager] Do you want to submit a PR for this issue?;;;","16/Apr/24 07:06;wzorgdrager;[~dianfu] sure, I can submit a PR for this! ;;;","17/Apr/24 05:16;dianfu;Great (y). Thanks [~wzorgdrager] ;;;","11/May/24 02:54;dianfu;Fix in 
- master via b4d71144de8e3772257804b6ed8ad688076430d6

- release-1.19 via d16b20e4fb4fb906927188ca11af599edd0953c1;;;",,,,,,,,,,,,,,,,,,,,,,,,
Modify the spelling mistakes in the taskmanager html,FLINK-35111,13575861,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,wczhu,wczhu,15/Apr/24 10:09,19/Apr/24 06:04,04/Jun/24 20:40,16/Apr/24 07:56,,,,,,,,,1.19.0,,,Runtime / Web Frontend,,,,0,,,"Fix the spelling error from ""profiler""  to ""profiling""",,,,,,,,,,,,,,,,,,,FLINK-35110,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 07:56:32 UTC 2024,,,,,,,,,,"0|z1on1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 12:36;dannycranmer;[~wczhu] shall I assign this to you? Can you link to the page with the typo please?

 ;;;","15/Apr/24 13:31;martijnvisser;Do note that grammar/spelling doesn't need a Jira ticket as explained in the code contribution process https://flink.apache.org/how-to-contribute/code-style-and-quality-pull-requests/;;;","16/Apr/24 02:43;wczhu;[~dannycranmer] Thank you for your reply, but I found that has been [24276|https://github.com/apache/flink/pull/24276] fixed;;;","16/Apr/24 07:56;dannycranmer;Ok great, thanks for the update [~wczhu] . Resolving this ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Modify the spelling mistakes in the taskmanager html,FLINK-35110,13575860,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,wczhu,wczhu,15/Apr/24 10:09,15/Apr/24 12:37,04/Jun/24 20:40,15/Apr/24 12:37,1.19.0,,,,,,,,,,,Runtime / Web Frontend,,,,0,,,"Fix the spelling error from ""profiler""  to ""profiling""",,,,,,,,,,,,,,,,,,,,FLINK-35111,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 12:37:53 UTC 2024,,,,,,,,,,"0|z1on1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 12:37;dannycranmer;Resolving this as duplicate;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Flink 1.20-SNAPSHOT in Flink Kafka connector and drop support for 1.17 and 1.18,FLINK-35109,13575846,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,ruanhang1993,martijnvisser,martijnvisser,15/Apr/24 09:20,15/May/24 07:23,04/Jun/24 20:40,,,,,,,,,,kafka-4.0.0,,,Connectors / Kafka,,,,0,pull-request-available,,"The Flink Kafka connector currently can't compile against Flink 1.20-SNAPSHOT. An example failure can be found at https://github.com/apache/flink-connector-kafka/actions/runs/8659822490/job/23746484721#step:15:169

The {code:java} TypeSerializerUpgradeTestBase{code} has had issues before, see FLINK-32455. See also specifically the comment in https://issues.apache.org/jira/browse/FLINK-32455?focusedCommentId=17739785&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17739785

Next to that, there's also FLINK-25509 which can only be supported with Flink 1.19 and higher. 

So we should:
* Drop support for 1.17 and 1.18
* Refactor the Flink Kafka connector to use the new {code:java}MigrationTest{code}

We will support the Flink Kafka connector for Flink 1.18 via the v3.1 branch; this change will be a new v4.0 version with support for Flink 1.19 and the upcoming Flink 1.20",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 02:06:39 UTC 2024,,,,,,,,,,"0|z1omyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/24 03:15;ruanhang1993;I would like to help it. ;;;","13/May/24 11:09;fpaul;[~ruanhang1993] sure always happy to take some help. I will assign the ticket to you. Feel free to ping me for the review.;;;","14/May/24 02:06;ruanhang1993;[~fpaul] Thanks for your quick reply. I will work on it later.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Deployment recovery is triggered on terminal jobs after jm shutdown ttl,FLINK-35108,13575831,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,15/Apr/24 08:37,15/Apr/24 12:21,04/Jun/24 20:40,15/Apr/24 12:21,kubernetes-operator-1.7.0,kubernetes-operator-1.8.0,,,,,,,kubernetes-operator-1.9.0,,,Kubernetes Operator,,,,0,pull-request-available,,"The deployment recovery mechanism is incorrectly triggered for terminal jobs once the JM deployment is deleted after the TTL period. 

This causes jobs to be resubmitted. This affects only batch jobs.

The workaround is to set 
kubernetes.operator.jm-deployment-recovery.enabled: false

 for batch jobs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 12:21:30 UTC 2024,,,,,,,,,,"0|z1omv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 12:21;gyfora;merged to main be3b79b64440065c9ae9d3eb0267e412d81ef67e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
rename flink-connector-datagen-test module folder to flink-connector-datagen-tests,FLINK-35107,13575826,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xleoken,xleoken,15/Apr/24 08:12,15/Apr/24 08:30,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-15 08:12:56.0,,,,,,,,,,"0|z1omu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes Operator ignores checkpoint type configuration,FLINK-35106,13575821,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mateczagany,mateczagany,15/Apr/24 07:48,15/Apr/24 07:48,04/Jun/24 20:40,,kubernetes-operator-1.8.0,,,,,,,,,,,Kubernetes Operator,,,,0,,,"There is a configuration for checkpoint type that will be taken if perioid checkpointing is enabled or a manual checkpoint is triggered.

However, the configuration value `kubernetes.operator.checkpoint.type` is completely ignored when any checkpoint is triggered.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-15 07:48:33.0,,,,,,,,,,"0|z1omsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support setting default Autoscaler options at autoscaler standalone level,FLINK-35105,13575814,13556703,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,15/Apr/24 07:24,07/May/24 04:59,04/Jun/24 20:40,07/May/24 04:59,,,,,,,,,kubernetes-operator-1.9.0,,,Autoscaler,,,,0,pull-request-available,,"Currently, autoscaler standalone doesn't support set [autoscaler options|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.8/docs/operations/configuration/#autoscaler-configuration]. We must set them at job level when we use autoscaler standalone. It's not convenient if platform administrator wanna change the default value for some autoscaler options, such as:
 * job.autoscaler.enabled
 * job.autoscaler.metrics.window
 * etc

This Jira supports setting Autoscaler options at autoscaler standalone level, it's similar with flink kubernetes operator.

The  autoscaler options of autoscaler standalone will be as the base configuration, and the configuration at job-level can override the default value provided in the autoscaler standalone.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 04:58:57 UTC 2024,,,,,,,,,,"0|z1omrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 04:58;fanrui;Merged to main(1.9.0) via: 249a6945aaa847a506c42111d7332ad072578095;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add kafka pipeline data source connector,FLINK-35104,13575811,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,15/Apr/24 07:21,15/Apr/24 07:25,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"There is already a kafka pipeline data sink connector : [https://github.com/apache/flink-cdc/pull/2938], and there should also be a kafka pipeline data source connector.

First collect cdc data in real time and write it to kafka, then write it to multiple different datasources in real time

 

 
 
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-15 07:21:38.0,,,,,,,,,,"0|z1omqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Plugin] Enhancing Flink Failure Management in Kubernetes with Dynamic Termination Log Integration,FLINK-35103,13575809,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,Swathi Chandrashekar,Swathi Chandrashekar,15/Apr/24 07:07,16/Apr/24 07:08,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,API / Core,,,,0,pull-request-available,,"Currently, whenever we have flink failures, we need to manually do the triaging by looking into the flink logs even for the initial analysis. It would have been better, if the user/admin directly gets the initial failure information even before looking into the logs.

To address this, we've developed a comprehensive solution via a plugin aimed at helping fetch the Flink failures, ensuring critical data is preserved for subsequent analysis and action.

 

In Kubernetes environments, troubleshooting pod failures can be challenging without checking the pod/flink logs. Fortunately, Kubernetes offers a robust mechanism to enhance debugging capabilities by leveraging the /dev/termination-log file.

[https://kubernetes.io/docs/tasks/debug/debug-application/determine-reason-pod-failure/]

By writing failure information to this log, Kubernetes automatically incorporates it into the container status, providing administrators and developers with valuable insights into the root cause of failures.

Our solution capitalizes on this Kubernetes feature to seamlessly integrate Flink failure reporting within the container ecosystem. Whenever a Flink encounters an issue, our plugin dynamically captures and logs the pertinent failure information into the /dev/termination-log file. This ensures that Kubernetes recognizes and propagates the failure status throughout the container ecosystem, enabling efficient monitoring and response mechanisms.

By leveraging Kubernetes' native functionality in this manner, our plugin ensures that Flink failure incidents are promptly identified and reflected in the pod status. This technical integration streamlines the debugging process, empowering operators to swiftly diagnose and address issues, thereby minimizing downtime and maximizing system reliability.

 

In-order to make this plugin generic, by default it doesn't do any action.  We can configure this by using

*external.log.factory.class : org.apache.flink.externalresource.log.K8SSupportTerminationLog*
in our flink-conf file.

This will be present in the plugins directory

Sample output of the flink pod container status when there is a flink failure.
 !screenshot-1.png! 

here, we can see that , the user can clearly understand there was a Auth issue and resolve it instead of checking the complete underlying logs.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/24 07:05;Swathi Chandrashekar;Status-pod.png;https://issues.apache.org/jira/secure/attachment/13068190/Status-pod.png","15/Apr/24 07:08;Swathi Chandrashekar;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13068191/screenshot-1.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 07:08:13 UTC 2024,,,,,,,,,,"0|z1omq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 07:13;Swathi Chandrashekar;Pull request : https://github.com/apache/flink/pull/24664
;;;","15/Apr/24 13:34;martijnvisser;I would argue that this requires a FLIP, and more then just a Jira. ;;;","16/Apr/24 07:08;Swathi Chandrashekar;Thanks [~martijnvisser] . Have started a discussion for FLIP-XXX .
[https://docs.google.com/document/d/1tWR0Fi3w7VQeD_9VUORh8EEOva3q-V0XhymTkNaXHOc/edit?usp=sharing|https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1tWR0Fi3w7VQeD_9VUORh8EEOva3q-V0XhymTkNaXHOc%2Fedit%3Fusp%3Dsharing&data=05%7C02%7Ccswathi%40microsoft.com%7Ce982d402ae5c48aa986e08dc5ddd71eb%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638488453000029058%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&sdata=UFYkYcpWPgaHhBagGKkrLPHZXh%2FivLd05YdmQcbVZaY%3D&reserved=0]
[ DISCUSS ] FLIP-XXX : [Plugin] Enhancing Flink Failure Management in Kubernetes with Dynamic Termination Log Integration
Please share your inputs for the proposal.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Incorret Type mapping for Flink CDC Doris connector,FLINK-35102,13575791,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,xiqian_yu,xiqian_yu,15/Apr/24 03:16,22/Apr/24 07:54,04/Jun/24 20:40,22/Apr/24 07:54,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"According to Flink CDC Doris connector docs, CHAR and VARCHAR are mapped to 3-bytes since Doris uses UTF-8 variable-length encoding internally.
|CHAR(n)|CHAR(n*3)|In Doris, strings are stored in UTF-8 encoding, so English characters occupy 1 byte and Chinese characters occupy 3 bytes. The length here is multiplied by 3. The maximum length of CHAR is 255. Once exceeded, it will automatically be converted to VARCHAR type.|
|VARCHAR(n)|VARCHAR(n*3)|Same as above. The length here is multiplied by 3. The maximum length of VARCHAR is 65533. Once exceeded, it will automatically be converted to STRING type.|

However, currently Doris connector maps `CHAR(n)` to `CHAR(n)` and `VARCHAR(n)` to `VARCHAR(n * 4)`, which is inconsistent with specification in docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 22 07:54:30 UTC 2024,,,,,,,,,,"0|z1omm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 07:54;leonard;fixed via 5e52d8620cee995b55f003956c3f7d7a3bdf4a22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing method parameter annotation for ResourceManagerGateway.sendSlotReport,FLINK-35101,13575789,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,chenyu-opensource,chenyu-opensource,15/Apr/24 02:17,15/Apr/24 03:00,04/Jun/24 20:40,15/Apr/24 02:22,1.18.0,1.19.0,1.20.0,,,,,,1.20.0,,,Runtime / Coordination,,,,0,pull-request-available,,"The parameter of method 'ResourceManagerGateway.sendSlotReport' has missed.

!image-2024-04-15-10-17-53-803.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/24 02:17;chenyu-opensource;image-2024-04-15-10-17-53-803.png;https://issues.apache.org/jira/secure/attachment/13068188/image-2024-04-15-10-17-53-803.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 02:38:54 UTC 2024,,,,,,,,,,"0|z1omls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 02:18;chenyu-opensource;Please, assign this issue to me. I can fix it.;;;","15/Apr/24 02:21;Weijie Guo;Yeap, this should be aligned. A hot-fix is good enough, I will close this then.;;;","15/Apr/24 02:22;Weijie Guo;Just feel free to open a hotfix PR.;;;","15/Apr/24 02:38;chenyu-opensource;[~Weijie Guo] Thank you so much for your review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Quickstarts Java nightly end-to-end test failed on Azure,FLINK-35100,13575787,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,15/Apr/24 02:13,15/Apr/24 02:14,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"
{code:java}
java.lang.IllegalStateException: Trying to open gateway for unseen checkpoint: latest known checkpoint = 1, incoming checkpoint = 2
Apr 13 01:11:42 	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.openGatewayAndUnmarkCheckpoint(SubtaskGatewayImpl.java:223) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$null$3(OperatorCoordinatorHolder.java:276) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at java.util.HashMap$Values.forEach(HashMap.java:982) ~[?:1.8.0_402]
Apr 13 01:11:42 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$notifyCheckpointAborted$4(OperatorCoordinatorHolder.java:276) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:460) ~[flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:460) ~[flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:225) ~[flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:88) ~[flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:174) ~[flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]
Apr 13 01:11:42 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akkae9795546-8599-4711-866f-44d3f5c3d377.jar:1.20-SNAPSHOT]

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58895&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=5714
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-15 02:13:24.0,,,,,,,,,,"0|z1omlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for multiple datasources,FLINK-35099,13575783,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,15/Apr/24 01:18,18/Apr/24 09:57,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"In a hyperscale scenario, databases are distributed on multiple mysql database servers instead of one database server. In our case, there are up to 16 servers. Support for multiple datasources in a single flink cdc task.",,,,,,,,,,,,,,,,,,,FLINK-35144,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-15 01:18:48.0,,,,,,,,,,"0|z1omkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Incorrect results for queries like ""10 >= y"" on tables using Filesystem connector and Orc format",FLINK-35098,13575734,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,empathy87,empathy87,empathy87,13/Apr/24 15:47,14/May/24 07:13,04/Jun/24 20:40,14/May/24 07:11,1.12.7,1.13.6,1.14.6,1.15.4,1.16.3,1.17.2,1.18.1,1.19.0,1.18.2,1.19.1,1.20.0,Connectors / ORC,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,pull-request-available,,"When working with ORC files, there is an issue with evaluation of SQL queries containing expressions with a literal as the first operand. Specifically, the query *10 >= y* does not always return the correct result.

This test added to OrcFileSystemITCase.java fails on the second check:

 
{code:java}
@TestTemplate
void testOrcFilterPushDownLiteralFirst() throws ExecutionException, InterruptedException {
    super.tableEnv()
            .executeSql(""insert into orcLimitTable values('a', 10, 10)"")
            .await();

    List<Row> expected = Collections.singletonList(Row.of(10));
    check(""select y from orcLimitTable where y <= 10"", expected);
    check(""select y from orcLimitTable where 10 >= y"", expected);
}

Results do not match for query:
  select y from orcLimitTable where 10 >= y
Results
 == Correct Result - 1 ==   == Actual Result - 0 ==
!+I[10]    {code}
The checks are equivalent and should evaluate to the same result. But the second query doesn't return the record with y=10.

The table is defined as:
{code:java}
create table orcLimitTable (
    x string,
    y int,
    a int) 
with (
    'connector' = 'filesystem',
    'path' = '/tmp/junit4374176500101507155/junit7109291529844202275/',
    'format'='orc'){code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 07:13:01 UTC 2024,,,,,,,,,,"0|z1om9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/24 15:51;empathy87;Please, assign this issue to me. I know where it could be fixed.;;;","13/Apr/24 21:34;jeyhunkarimov;Hi [~empathy87] sorry, I have been working on the issue in the time between you created the issue and you commented here, so I did not track your comment here. In any case, submission might have some issues (now closed), so you are welcome to submit your PR. Thanks;;;","14/Apr/24 03:41;empathy87;[~jeyhunkarimov], thank you! I submitted my PR.;;;","14/May/24 07:13;Sergey Nuyanzin;Merged as
1.18: [1f604da2dfc831d04826a20b3cb272d2ad9dfb56|https://github.com/apache/flink/commit/1f604da2dfc831d04826a20b3cb272d2ad9dfb56]
1.19: [e16da86dfb1fbeee541cd9dfccd5f5f4520b7396|https://github.com/apache/flink/commit/e16da86dfb1fbeee541cd9dfccd5f5f4520b7396]
master: [4165bac27bda4457e5940a994d923242d4a271dc|https://github.com/apache/flink/commit/4165bac27bda4457e5940a994d923242d4a271dc];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Table API Filesystem connector with 'raw' format repeats last line,FLINK-35097,13575691,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mallikarjuna,david.perkins,david.perkins,12/Apr/24 21:04,26/Apr/24 12:30,04/Jun/24 20:40,25/Apr/24 05:44,1.17.1,,,,,,,,1.18.2,1.19.1,1.20.0,Connectors / FileSystem,,,,0,pull-request-available,,"When using the Filesystem connector with 'raw' format to read text data that contains new lines, a row is returned for every line, but always contains the contents of the last line.

For example, with the following file.
{quote}
line 1
line 2
line 3
{quote}

And table definition
{quote}
create TABLE MyRawTable (
     `doc` string,
 ) WITH (
      'path' = 'file:///path/to/data',
      'format' = 'raw',
       'connector' = 'filesystem'
);
{quote}

Selecting `*` from the table produces three rows all with ""line 3"" for `doc`.","I ran the above test with 1.17.1. I checked for existing bug tickets and release notes, but did not find anything, so assuming this effects 1.18 and 1.19.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 12:30:29 UTC 2024,,,,,,,,,,"0|z1om00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/24 12:46;mallikarjuna;> assuming this effects 1.18 and 1.19.

This is reproducible on `master`. I'll look into a fix.;;;","14/Apr/24 13:47;mallikarjuna;[~david.perkins] , I've raised a fix here:

[https://github.com/apache/flink/pull/24661]

 

I just realised, I need to look for a committer and get the issue assigned first!;;;","15/Apr/24 13:39;david.perkins;Great! Thanks for the fast response.;;;","23/Apr/24 10:29;mallikarjuna;[~david.perkins] , the fix has been merged to master.;;;","25/Apr/24 05:44;twalthr;Fixed in master:
3a56c2f7cccc796557328ef33b6caae57bca5c18
8ffe6d698fb4f37c68a3914e3903dc874dfcb999
51d015b570497c57f56bea64371ecec23fd454c1

Fixed in 1.19:
e7816f714ef5298e1ca978aeddf62732794bb93f

Fixed in 1.18:
cf216f03c71433d21af39d125b4ff9d804cb0b38;;;","26/Apr/24 12:30;david.perkins;Can the fix get applied to 1.17 also?;;;",,,,,,,,,,,,,,,,,,,,,,,
Flink OpenSearch Connector,FLINK-35096,13575674,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hksharma,hksharma,12/Apr/24 17:40,12/Apr/24 17:59,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Opensearch,,,,0,,,"Hi Team,

I am trying to connect with the OS using the FLINK Java but as I dont find a way to pass the IAM-ROLE for the OS cluster I am not able to create the index also it does not throw any exception.



Here is my code which I am trying to


final DataStream<Tuple4<String, String, Long, Long>> tupleSource = env.fromCollection(users);
final OpensearchSink<Tuple4<String, String, Long, Long>> sink =
new OpensearchSinkBuilder<Tuple4<String, String, Long, Long>>()
.setBulkFlushMaxActions(1)
.setHosts(new HttpHost(""aws-cluster"", 443, ""https""))
.setEmitter( (element, ctx, indexer) -> {
indexer.add(
Requests
.indexRequest()
.index(""users"")
.id(element.f0)
.source(Map.ofEntries(
Map.entry(""user_id"", element.f0),
Map.entry(""user_name"", element.f1),
Map.entry(""uv"", element.f2),
Map.entry(""pv"", element.f3)
)));
})
.setAllowInsecure(true)
.setBulkFlushMaxActions(1)
.build();

Idly we do pass the assumeRole of the user once we wan to connect any of the managed service but I dont find a way role anywhere in the OS connector",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,2024-04-12 17:40:26.0,,,,,,,,,,"0|z1olw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionEnvironmentImplTest.testFromSource failure on GitHub CI,FLINK-35095,13575639,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,12/Apr/24 12:17,31/May/24 13:24,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,test-stability,,"1.20 Java 17: Test (module: misc) https://github.com/apache/flink/actions/runs/8655935935/job/23735920630#step:10:22223
{code}
Error: 02:29:05 02:29:05.708 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.360 s <<< FAILURE! -- in org.apache.flink.datastream.impl.ExecutionEnvironmentImplTest
Error: 02:29:05 02:29:05.708 [ERROR] org.apache.flink.datastream.impl.ExecutionEnvironmentImplTest.testFromSource -- Time elapsed: 0.131 s <<< FAILURE!
Apr 12 02:29:05 java.lang.AssertionError: 
Apr 12 02:29:05 
Apr 12 02:29:05 Expecting actual:
Apr 12 02:29:05   [47]
Apr 12 02:29:05 to contain exactly (and in same order):
Apr 12 02:29:05   [48]
Apr 12 02:29:05 but some elements were not found:
Apr 12 02:29:05   [48]
Apr 12 02:29:05 and others were not expected:
Apr 12 02:29:05   [47]
Apr 12 02:29:05 
Apr 12 02:29:05 	at org.apache.flink.datastream.impl.ExecutionEnvironmentImplTest.testFromSource(ExecutionEnvironmentImplTest.java:97)
Apr 12 02:29:05 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Apr 12 02:29:05 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Apr 12 02:29:05 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
Apr 12 02:29:05 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
Apr 12 02:29:05 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
Apr 12 02:29:05 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
Apr 12 02:29:05 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
Apr 12 02:29:05 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 13:24:29 UTC 2024,,,,,,,,,,"0|z1olog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 13:43;rskraba;1.20 Java 17: Test (module: misc) https://github.com/apache/flink/actions/runs/8682562266/job/23807568934#step:10:22199
;;;","03/May/24 15:54;rskraba;* 1.20 Java 21 / Test (module: misc) https://github.com/apache/flink/commit/80af4d502318348ba15a8f75a2a622ce9dbdc968/checks/24453751708/logs
* 1.20 Hadoop 3.1.3 / Test (module: misc) https://github.com/apache/flink/actions/runs/8809949034/job/24182253915#step:10:22352;;;","22/May/24 12:45;rskraba;* 1.20 Java 17 / Test (module: misc) https://github.com/apache/flink/actions/runs/9184288079/job/25256627599#step:10:22297;;;","31/May/24 13:24;rskraba;* 1.20 Default (Java 8) / Test (module: misc) https://github.com/apache/flink/actions/runs/9314653645/job/25640760952#step:10:22281
* 1.20 Java 11 / Test (module: misc) [https://github.com/apache/flink/actions/runs/9295906525/job/25583844788#step:10:22256]

Same symptom and error message happening on {{ExecutionEnvironmentImplTest.testAddWrapSource}};;;",,,,,,,,,,,,,,,,,,,,,,,,,
SinkTestSuiteBase.testScaleDown is hanging for 1.20-SNAPSHOT,FLINK-35094,13575613,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,12/Apr/24 08:23,15/May/24 20:30,04/Jun/24 20:40,15/Apr/24 15:20,1.20.0,elasticsearch-3.1.0,,,,,,,elasticsearch-3.2.0,opensearch-1.2.0,opensearch-2.0.0,Connectors / ElasticSearch,Tests,,,0,pull-request-available,test-stability,"Currently it is reproduced for elastic search connector
all the ci jobs (for all jdks) against 1.20-SNAPSHOT are hanging on 
{noformat}
2024-04-12T05:56:50.6179284Z ""main"" #1 prio=5 os_prio=0 cpu=18726.96ms elapsed=2522.03s tid=0x00007f670c025a50 nid=0x3c6d waiting on condition  [0x00007f6712513000]
2024-04-12T05:56:50.6180667Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2024-04-12T05:56:50.6181497Z 	at java.lang.Thread.sleep(java.base@17.0.10/Native Method)
2024-04-12T05:56:50.6182762Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:152)
2024-04-12T05:56:50.6184456Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2024-04-12T05:56:50.6186346Z 	at org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.checkResultWithSemantic(SinkTestSuiteBase.java:504)
2024-04-12T05:56:50.6188474Z 	at org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.restartFromSavepoint(SinkTestSuiteBase.java:327)
2024-04-12T05:56:50.6190145Z 	at org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.testScaleDown(SinkTestSuiteBase.java:224)
2024-04-12T05:56:50.6191247Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@17.0.10/Native Method)
2024-04-12T05:56:50.6192806Z 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@17.0.10/NativeMethodAccessorImpl.java:77)
2024-04-12T05:56:50.6193863Z 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@17.0.10/DelegatingMethodAccessorImpl.java:43)
2024-04-12T05:56:50.6194834Z 	at java.lang.reflect.Method.invoke(java.base@17.0.10/Method.java:568)
{noformat}

for 1.17, 1.18, 1.19 there is no such issue and everything is ok
https://github.com/apache/flink-connector-elasticsearch/actions/runs/8538572134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 15:21:24 UTC 2024,,,,,,,,,,"0|z1olio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 15:20;Sergey Nuyanzin;Merged as [516a89864b38e35261675f873e37453df761324d|https://github.com/apache/flink-connector-elasticsearch/commit/516a89864b38e35261675f873e37453df761324d];;;","15/Apr/24 15:21;Sergey Nuyanzin;Opensearch connector was fixed within https://github.com/apache/flink-connector-opensearch/commit/00f1a5b13bfbadcb8efce8e16fb06ddea0d8e48e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Postgres source connector support SPECIFIC_OFFSETS start up mode from an existed replication slot.,FLINK-35093,13575588,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,loserwang1024,loserwang1024,loserwang1024,12/Apr/24 03:33,08/May/24 02:44,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,"Current, Postgres source connector  only support INITIAL and LATEST mode.

However, sometimes, user want to restart from an existed replication slot's confiermed_lsn.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 12 03:33:36 UTC 2024,,,,,,,,,,"0|z1old4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/24 03:33;loserwang1024;I'd like to do it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add integrated test for Doris / Starrocks sink pipeline connector,FLINK-35092,13575585,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xiqian_yu,xiqian_yu,xiqian_yu,12/Apr/24 02:22,26/Apr/24 07:19,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Currently, no integrated test are being applied to Doris pipeline connector (there's only one DorisRowConverterTest case for now). Adding ITcases would improving Doris connector's code quality and reliability.",,,,,,,,,,,,,,,FLINK-35120,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-12 02:22:24.0,,,,,,,,,,"0|z1olcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect warning msg in JM when use metric reporter,FLINK-35091,13575584,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,rongtongliu,rongtongliu,12/Apr/24 02:04,12/Apr/24 02:27,04/Jun/24 20:40,,,,,,,,,,1.16.3,,,,,,,0,pull-request-available,,"Hello,

I encountered an issue while upgrading Flink from version 1.14 to 1.18. After the upgrade, I noticed that some monitoring metrics were not being reported to InfluxDB.

Upon checking the Job Manager (JM) logs, I found an error indicating that the previously used classes are no longer supported. However, there seems to be an oddly phrased error message that looks like it might have been written incorrectly.

The error message reads: ""The reporter configuration of '{}' configures the reporter class, which is no a no longer supported approach to configure reporters."" + "" Please configure a factory class instead:""

I believe the correct phrasing should be: ""The reporter configuration of '{}' configures the reporter class, which is a no longer supported approach to configure reporters."" + "" Please configure a factory class instead:""

It appears that the words ""no a"" were accidentally added, making the sentence grammatically incorrect and potentially confusing for users.

 

!image-2024-04-12-10-02-20-142.png!",,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/24 02:02;rongtongliu;image-2024-04-12-10-02-20-142.png;https://issues.apache.org/jira/secure/attachment/13068154/image-2024-04-12-10-02-20-142.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-12 02:04:20.0,,,,,,,,,,"0|z1olc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doris sink fails to create table when database does not exist,FLINK-35090,13575582,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xiqian_yu,xiqian_yu,xiqian_yu,12/Apr/24 01:46,23/May/24 13:15,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Currently, Doris sink connector doesn't support creating database automatically. When user specifies a sink namespace with non-existing database in YAML config, Doris connector will crash.

Expected behaviour: Doris sink connector should create both database and table automatically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 12 01:51:03 UTC 2024,,,,,,,,,,"0|z1olbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/24 01:51;xiqian_yu;[~renqs] I'm willing to take this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Two input AbstractStreamOperator may throw NPE when receiving RecordAttributes,FLINK-35089,13575581,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuannan,xuannan,xuannan,12/Apr/24 01:41,10/May/24 08:30,04/Jun/24 20:40,17/Apr/24 03:58,1.19.0,,,,,,,,1.19.1,1.20.0,,Runtime / Task,,,,0,pull-request-available,,"Currently the `lastRecordAttributes1` and `lastRecordAttributes2` in the `AbstractStreamOperator` are transient. The two fields will be null when it is deserialized in TaskManager, which may cause an NPE.

To fix it, we will initialize the two fields in the {{setup}} method.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 03:58:02 UTC 2024,,,,,,,,,,"0|z1olbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 03:58;xtsong;- master (1.20): 7a90a05e82ddfb3438e611d44fd329858255de6b
- release-1.19: a2c3d27f5dced2ba73307e8230cd07a11b26c401;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
watermark alignment maxAllowedWatermarkDrift and updateInterval param need check,FLINK-35088,13575479,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,elon,elon,elon,11/Apr/24 12:19,25/Apr/24 01:20,04/Jun/24 20:40,25/Apr/24 01:20,1.16.1,,,,,,,,1.20.0,,,API / Core,Runtime / Coordination,,,0,pull-request-available,,"When I use watermark alignment,

1.I found that setting maxAllowedWatermarkDrift to a negative number initially led me to believe it could support delaying the consumption of the source, so I tried it. Then, the upstream data flow would hang indefinitely.

Root cause:
{code:java}
long maxAllowedWatermark = globalCombinedWatermark.getTimestamp()                 + watermarkAlignmentParams.getMaxAllowedWatermarkDrift();  {code}
If maxAllowedWatermarkDrift is negative, SourceOperator: maxAllowedWatermark < lastEmittedWatermark, then the SourceReader will be blocked indefinitely and cannot recover.

I'm not sure if this is a supported feature of watermark alignment. If it's not, I think an additional parameter validation should be implemented to throw an exception on the client side if the value is negative.

2.The updateInterval parameter also lacks validation. If I set it to 0, the task will throw an exception when starting the job manager. The JDK class java.util.concurrent.ScheduledThreadPoolExecutor performs the validation and throws the exception.
{code:java}
java.lang.IllegalArgumentException: null
	at java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:565) ~[?:1.8.0_351]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.<init>(SourceCoordinator.java:191) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider.getCoordinator(SourceCoordinatorProvider.java:92) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.createNewInternalCoordinator(RecreateOnResetOperatorCoordinator.java:333) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.<init>(RecreateOnResetOperatorCoordinator.java:59) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.<init>(RecreateOnResetOperatorCoordinator.java:42) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$Provider.create(RecreateOnResetOperatorCoordinator.java:201) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$Provider.create(RecreateOnResetOperatorCoordinator.java:195) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:529) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:494) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.createOperatorCoordinatorHolder(ExecutionJobVertex.java:286) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.initialize(ExecutionJobVertex.java:223) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertex(DefaultExecutionGraph.java:901) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertices(DefaultExecutionGraph.java:891) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:848) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:830) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:203) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:156) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:365) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:208) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:134) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:369) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:346) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) ~[flink-dist_2.12-1.16.1.jar:1.16.1]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) [?:1.8.0_351]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_351]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_351]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_351]{code}
Therefore, I believe it's necessary to validate these two parameters to ensure that exceptions are thrown on the client side to alert the user.

!image-2024-04-11-20-12-29-951.png!",,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/24 12:12;elon;image-2024-04-11-20-12-29-951.png;https://issues.apache.org/jira/secure/attachment/13068140/image-2024-04-11-20-12-29-951.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 01:20:02 UTC 2024,,,,,,,,,,"0|z1okow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 21:35;martijnvisser;[~elon] Please verify this with later version of Flink, since there have been many bugfixes since. ;;;","16/Apr/24 07:57;masc; 

Watermark alignment was moved out of beta during the 1.18.0 release. I'd recommend that version and onwards. https://issues.apache.org/jira/browse/FLINK-32705;;;","18/Apr/24 08:36;elon;[~martijnvisser] [~masc]  I'm sorry for the late reply. I have conducted a retest based on Flink version 1.18 and found that the problem still persists. Then, I checked the latest code on the main Flink branch and found that there is no validation for these two parameters. What are your thoughts on this?;;;","18/Apr/24 08:56;martijnvisser;[~fanrui] What are your thoughts on this?;;;","23/Apr/24 01:36;fanrui;Thanks [~martijnvisser] for the ping, I will take a look in detail this week.;;;","23/Apr/24 09:39;fanrui;Hi [~elon] , thanks for reporting this issue.

It makes sense to add a check maxAllowedWatermarkDrift and updateInterval inside of WatermarkStrategy#withWatermarkAlignment method, they are duration, they should > 0 ms.

And I found WatermarkStrategy#withIdleness has a similar logic.

Would you like to fix this issue? If yes , I can assign this Jira to you and review. If no, I will pick it up.;;;","23/Apr/24 09:45;elon;[~fanrui] I am very happy to fix this issue, please assign it to me, thank you very much:D;;;","23/Apr/24 09:50;fanrui;[~elon] thank you for your volunteering, assigned.;;;","25/Apr/24 01:20;fanrui;Merged to master(1.20) via: 09531994e3cde35ec159315389cf20396c8e21bd;;;",,,,,,,,,,,,,,,,,,,,
CompileException when watermark definition contains coalesce and to_timestamp built-in functions,FLINK-35081,13575449,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,grzegorz.kolakowski,grzegorz.kolakowski,11/Apr/24 09:16,18/Apr/24 02:00,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,Table SQL / Runtime,,,,0,,,"I have a data stream in which event-time column can have two data formats. To be able to define watermark on the table, I used coalesce and to_timestamp built-in functions as shown below:
{code:sql}
create table test (
    `@timestamp` VARCHAR,
    __rowtime AS coalesce(
        to_timestamp(`@timestamp`, 'yyyy-MM-dd''T''HH:mm:ss'),
        to_timestamp(`@timestamp`, 'yyyy-MM-dd''T''HH:mm:ss.SSS')
    ),
    watermark for __rowtime as __rowtime - INTERVAL '30' SECOND,
    ...
) with ( ... )
{code}
The job failed with the following stacktrace:
{noformat}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242)
    at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748)
    at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
    at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
    at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.base/java.lang.reflect.Method.invoke(Unknown Source)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
    at akka.actor.ActorCell.invoke(ActorCell.scala:547)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
    at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
    at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
    at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'WatermarkGenerator$32'
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
    at org.apache.flink.table.runtime.generated.GeneratedWatermarkGeneratorSupplier.createWatermarkGenerator(GeneratedWatermarkGeneratorSupplier.java:62)
    at org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.createMainOutput(ProgressiveTimestampsAndWatermarks.java:109)
    at org.apache.flink.streaming.api.operators.SourceOperator.initializeMainOutput(SourceOperator.java:460)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNextNotReading(SourceOperator.java:436)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:412)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
    ... 16 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
    ... 18 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
    ... 21 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 31, Column 130: Cannot determine simple type name ""org""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
    at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7121)
    at org.codehaus.janino.UnitCompiler.access$17000(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$2.visitNewClassInstance(UnitCompiler.java:6529)
    at org.codehaus.janino.UnitCompiler$22$2.visitNewClassInstance(UnitCompiler.java:6490)
    at org.codehaus.janino.Java$NewClassInstance.accept(Java.java:5190)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9237)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783)
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
    ... 27 more
{noformat}
 

 
{code:java}
/* 1 */
/* 2 */      public final class WatermarkGenerator$64
/* 3 */          extends org.apache.flink.table.runtime.generated.WatermarkGenerator {
/* 4 */
/* 5 */        private transient org.apache.flink.table.runtime.typeutils.StringDataSerializer typeSerializer$66;
/* 6 */        
/* 7 */        private final org.apache.flink.table.data.binary.BinaryStringData str$68 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""yyyy-MM-dd'T'HH:mm:ss"");
/* 8 */                   
/* 9 */        
/* 10 */        private final org.apache.flink.table.data.binary.BinaryStringData str$72 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""yyyy-MM-dd'T'HH:mm:ss.SSS"");
/* 11 */                   
/* 12 */        private transient org.apache.flink.table.runtime.functions.scalar.CoalesceFunction function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096;
/* 13 */        
/* 14 */        private transient org.apache.flink.api.common.eventtime.WatermarkGeneratorSupplier.Context
/* 15 */        context;
/* 16 */        
/* 17 */
/* 18 */        public WatermarkGenerator$64(Object[] references) throws Exception {
/* 19 */          typeSerializer$66 = (((org.apache.flink.table.runtime.typeutils.StringDataSerializer) references[0]));
/* 20 */          function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096 = (((org.apache.flink.table.runtime.functions.scalar.CoalesceFunction) references[1]));
/* 21 */          
/* 22 */          int len = references.length;
/* 23 */          context =
/* 24 */          (org.apache.flink.api.common.eventtime.WatermarkGeneratorSupplier.Context) references[len-1];
/* 25 */          
/* 26 */        }
/* 27 */
/* 28 */        @Override
/* 29 */        public void open(org.apache.flink.configuration.Configuration parameters) throws Exception {
/* 30 */          
/* 31 */          function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096.open(new org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGeneratorFunctionContextWrapper(context));
/* 32 */                 
/* 33 */        }
/* 34 */
/* 35 */        @Override
/* 36 */        public Long currentWatermark(org.apache.flink.table.data.RowData row) throws Exception {
/* 37 */          
/* 38 */          org.apache.flink.table.data.binary.BinaryStringData field$65;
/* 39 */          boolean isNull$65;
/* 40 */          org.apache.flink.table.data.binary.BinaryStringData field$67;
/* 41 */          boolean isNull$69;
/* 42 */          org.apache.flink.table.data.TimestampData result$70;
/* 43 */          boolean isNull$73;
/* 44 */          org.apache.flink.table.data.TimestampData result$74;
/* 45 */          org.apache.flink.table.data.TimestampData externalResult$76;
/* 46 */          org.apache.flink.table.data.TimestampData result$77;
/* 47 */          boolean isNull$77;
/* 48 */          boolean isNull$78;
/* 49 */          org.apache.flink.table.data.TimestampData result$79;
/* 50 */          
/* 51 */          isNull$65 = row.isNullAt(0);
/* 52 */          field$65 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
/* 53 */          if (!isNull$65) {
/* 54 */            field$65 = ((org.apache.flink.table.data.binary.BinaryStringData) row.getString(0));
/* 55 */          }
/* 56 */          field$67 = field$65;
/* 57 */          if (!isNull$65) {
/* 58 */            field$67 = (org.apache.flink.table.data.binary.BinaryStringData) (typeSerializer$66.copy(field$67));
/* 59 */          }
/* 60 */                  
/* 61 */          
/* 62 */          
/* 63 */          
/* 64 */          
/* 65 */          
/* 66 */          
/* 67 */          isNull$69 = isNull$65 || false;
/* 68 */          result$70 = null;
/* 69 */          if (!isNull$69) {
/* 70 */            
/* 71 */          try {
/* 72 */            
/* 73 */            result$70 = 
/* 74 */          org.apache.flink.table.utils.DateTimeUtils.parseTimestampData(field$67.toString(), ((org.apache.flink.table.data.binary.BinaryStringData) str$68).toString())
/* 75 */                     ;
/* 76 */          } catch (Throwable ignored$71) {
/* 77 */            isNull$69 = true;
/* 78 */            result$70 = null;
/* 79 */          }
/* 80 */          
/* 81 */            isNull$69 = (result$70 == null);
/* 82 */          }
/* 83 */          
/* 84 */          
/* 85 */          
/* 86 */          
/* 87 */          isNull$73 = isNull$65 || false;
/* 88 */          result$74 = null;
/* 89 */          if (!isNull$73) {
/* 90 */            
/* 91 */          try {
/* 92 */            
/* 93 */            result$74 = 
/* 94 */          org.apache.flink.table.utils.DateTimeUtils.parseTimestampData(field$67.toString(), ((org.apache.flink.table.data.binary.BinaryStringData) str$72).toString())
/* 95 */                     ;
/* 96 */          } catch (Throwable ignored$75) {
/* 97 */            isNull$73 = true;
/* 98 */            result$74 = null;
/* 99 */          }
/* 100 */          
/* 101 */            isNull$73 = (result$74 == null);
/* 102 */          }
/* 103 */          
/* 104 */          externalResult$76 = (org.apache.flink.table.data.TimestampData) function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096
/* 105 */            .eval(isNull$69 ? null : ((org.apache.flink.table.data.TimestampData) result$70), isNull$73 ? null : ((org.apache.flink.table.data.TimestampData) result$74));
/* 106 */          
/* 107 */          isNull$77 = externalResult$76 == null;
/* 108 */          result$77 = null;
/* 109 */          if (!isNull$77) {
/* 110 */            result$77 = externalResult$76;
/* 111 */          }
/* 112 */          
/* 113 */          
/* 114 */          isNull$78 = isNull$77 || false;
/* 115 */          result$79 = null;
/* 116 */          if (!isNull$78) {
/* 117 */            
/* 118 */          
/* 119 */          result$79 = org.apache.flink.table.data.TimestampData.fromEpochMillis(result$77.getMillisecond() - ((long) 30000L), result$77.getNanoOfMillisecond());
/* 120 */          
/* 121 */            
/* 122 */          }
/* 123 */          
/* 124 */          if (isNull$78) {
/* 125 */            return null;
/* 126 */          } else {
/* 127 */            return result$79.getMillisecond();
/* 128 */          }
/* 129 */        }
/* 130 */
/* 131 */        @Override
/* 132 */        public void close() throws Exception {
/* 133 */          function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096.close();
/* 134 */        }
/* 135 */      }
/* 136 */    
/* 1 */
/* 2 */      public final class WatermarkGenerator$64
/* 3 */          extends org.apache.flink.table.runtime.generated.WatermarkGenerator {
/* 4 */
/* 5 */        private transient org.apache.flink.table.runtime.typeutils.StringDataSerializer typeSerializer$66;
/* 6 */        
/* 7 */        private final org.apache.flink.table.data.binary.BinaryStringData str$68 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""yyyy-MM-dd'T'HH:mm:ss"");
/* 8 */                   
/* 9 */        
/* 10 */        private final org.apache.flink.table.data.binary.BinaryStringData str$72 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""yyyy-MM-dd'T'HH:mm:ss.SSS"");
/* 11 */                   
/* 12 */        private transient org.apache.flink.table.runtime.functions.scalar.CoalesceFunction function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096;
/* 13 */        
/* 14 */        private transient org.apache.flink.api.common.eventtime.WatermarkGeneratorSupplier.Context
/* 15 */        context;
/* 16 */        
/* 17 */
/* 18 */        public WatermarkGenerator$64(Object[] references) throws Exception {
/* 19 */          typeSerializer$66 = (((org.apache.flink.table.runtime.typeutils.StringDataSerializer) references[0]));
/* 20 */          function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096 = (((org.apache.flink.table.runtime.functions.scalar.CoalesceFunction) references[1]));
/* 21 */          
/* 22 */          int len = references.length;
/* 23 */          context =
/* 24 */          (org.apache.flink.api.common.eventtime.WatermarkGeneratorSupplier.Context) references[len-1];
/* 25 */          
/* 26 */        }
/* 27 */
/* 28 */        @Override
/* 29 */        public void open(org.apache.flink.configuration.Configuration parameters) throws Exception {
/* 30 */          
/* 31 */          function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096.open(new org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGeneratorFunctionContextWrapper(context));
/* 32 */                 
/* 33 */        }
/* 34 */
/* 35 */        @Override
/* 36 */        public Long currentWatermark(org.apache.flink.table.data.RowData row) throws Exception {
/* 37 */          
/* 38 */          org.apache.flink.table.data.binary.BinaryStringData field$65;
/* 39 */          boolean isNull$65;
/* 40 */          org.apache.flink.table.data.binary.BinaryStringData field$67;
/* 41 */          boolean isNull$69;
/* 42 */          org.apache.flink.table.data.TimestampData result$70;
/* 43 */          boolean isNull$73;
/* 44 */          org.apache.flink.table.data.TimestampData result$74;
/* 45 */          org.apache.flink.table.data.TimestampData externalResult$76;
/* 46 */          org.apache.flink.table.data.TimestampData result$77;
/* 47 */          boolean isNull$77;
/* 48 */          boolean isNull$78;
/* 49 */          org.apache.flink.table.data.TimestampData result$79;
/* 50 */          
/* 51 */          isNull$65 = row.isNullAt(0);
/* 52 */          field$65 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
/* 53 */          if (!isNull$65) {
/* 54 */            field$65 = ((org.apache.flink.table.data.binary.BinaryStringData) row.getString(0));
/* 55 */          }
/* 56 */          field$67 = field$65;
/* 57 */          if (!isNull$65) {
/* 58 */            field$67 = (org.apache.flink.table.data.binary.BinaryStringData) (typeSerializer$66.copy(field$67));
/* 59 */          }
/* 60 */                  
/* 61 */          
/* 62 */          
/* 63 */          
/* 64 */          
/* 65 */          
/* 66 */          
/* 67 */          isNull$69 = isNull$65 || false;
/* 68 */          result$70 = null;
/* 69 */          if (!isNull$69) {
/* 70 */            
/* 71 */          try {
/* 72 */            
/* 73 */            result$70 = 
/* 74 */          org.apache.flink.table.utils.DateTimeUtils.parseTimestampData(field$67.toString(), ((org.apache.flink.table.data.binary.BinaryStringData) str$68).toString())
/* 75 */                     ;
/* 76 */          } catch (Throwable ignored$71) {
/* 77 */            isNull$69 = true;
/* 78 */            result$70 = null;
/* 79 */          }
/* 80 */          
/* 81 */            isNull$69 = (result$70 == null);
/* 82 */          }
/* 83 */          
/* 84 */          
/* 85 */          
/* 86 */          
/* 87 */          isNull$73 = isNull$65 || false;
/* 88 */          result$74 = null;
/* 89 */          if (!isNull$73) {
/* 90 */            
/* 91 */          try {
/* 92 */            
/* 93 */            result$74 = 
/* 94 */          org.apache.flink.table.utils.DateTimeUtils.parseTimestampData(field$67.toString(), ((org.apache.flink.table.data.binary.BinaryStringData) str$72).toString())
/* 95 */                     ;
/* 96 */          } catch (Throwable ignored$75) {
/* 97 */            isNull$73 = true;
/* 98 */            result$74 = null;
/* 99 */          }
/* 100 */          
/* 101 */            isNull$73 = (result$74 == null);
/* 102 */          }
/* 103 */          
/* 104 */          externalResult$76 = (org.apache.flink.table.data.TimestampData) function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096
/* 105 */            .eval(isNull$69 ? null : ((org.apache.flink.table.data.TimestampData) result$70), isNull$73 ? null : ((org.apache.flink.table.data.TimestampData) result$74));
/* 106 */          
/* 107 */          isNull$77 = externalResult$76 == null;
/* 108 */          result$77 = null;
/* 109 */          if (!isNull$77) {
/* 110 */            result$77 = externalResult$76;
/* 111 */          }
/* 112 */          
/* 113 */          
/* 114 */          isNull$78 = isNull$77 || false;
/* 115 */          result$79 = null;
/* 116 */          if (!isNull$78) {
/* 117 */            
/* 118 */          
/* 119 */          result$79 = org.apache.flink.table.data.TimestampData.fromEpochMillis(result$77.getMillisecond() - ((long) 30000L), result$77.getNanoOfMillisecond());
/* 120 */          
/* 121 */            
/* 122 */          }
/* 123 */          
/* 124 */          if (isNull$78) {
/* 125 */            return null;
/* 126 */          } else {
/* 127 */            return result$79.getMillisecond();
/* 128 */          }
/* 129 */        }
/* 130 */
/* 131 */        @Override
/* 132 */        public void close() throws Exception {
/* 133 */          function_org$apache$flink$table$runtime$functions$scalar$CoalesceFunction$081acce440d9567a963370b8fb1c1096.close();
/* 134 */        }
/* 135 */      }
/* 136 */    

{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28693,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 01:59:43 UTC 2024,,,,,,,,,,"0|z1okjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/24 01:59;xuyangzhong;I think this bug is same with FLINK-28693;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow to specify additional pushdown filter in table properties,FLINK-35080,13575444,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,grzegorz.kolakowski,grzegorz.kolakowski,11/Apr/24 08:29,11/Apr/24 08:33,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,"Currently, flink-connector-jdbc supports filter pushdown only for basic operators such as: comparison operators, logical operators, is null, is not null and like.

In some use-cases more complex filters need to be applied against the data, which cannot be pushed down by the connector, so a lot of data is fetched by Flink unnecessarily. What is more, expressions using PostgreSQL's jsonb or stored procedures, for instance, need to be recreated in Flink SQL with built-in functions or with UDFs. In addition, you not always own the target relational database in order to create a view which already contains needed predicates.

 

It would be nice, if it is possible to specify additional pushdown parameter in table definition, e.g.:
{code:java}
CREATE TABLE test ( ... )
WITH (
    ...
    'extra-pushdown-predicate' = 'some_jsonb_column ? ''xyz''' 
);{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-11 08:29:36.0,,,,,,,,,,"0|z1okig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MongoConnector failed to resume token when current collection removed,FLINK-35079,13575437,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,xiqian_yu,xiqian_yu,11/Apr/24 07:43,17/Apr/24 02:33,04/Jun/24 20:40,17/Apr/24 02:33,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"When connector tries to create cursor with an expired resuming token during stream task fetching stage, MongoDB connector will crash with such message: ""error due to Command failed with error 280 (ChangeStreamFatalError): 'cannot resume stream; the resume token was not found.""
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 02:33:22 UTC 2024,,,,,,,,,,"0|z1okgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 07:44;xiqian_yu;[~renqs] I'm glad to help investigating this.;;;","17/Apr/24 02:33;jiabaosun;resolved via cdc master: 0562e35da75fb2c8e512d438adb8f80a87964dc4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Include Flink CDC pipeline connector sub-modules in the CI runs,FLINK-35078,13575432,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,m.orazow,m.orazow,m.orazow,11/Apr/24 06:22,12/Apr/24 07:43,04/Jun/24 20:40,12/Apr/24 07:43,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"At the moment, on the the Flink CDC pipeline connector parent is build using the GHA workflow. This misses the maven plugin checks on the sub-modules.

Similar to the other modules we should list the pipeline connector modules separately in the CI built.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 12 07:43:27 UTC 2024,,,,,,,,,,"0|z1okfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 06:22;m.orazow;I will be happy to address this.;;;","12/Apr/24 07:43;renqs;flink-cdc master: adf6667907e61f1554a5aba12db4700224ba2e19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add package license check for Flink CDC modules.,FLINK-35077,13575428,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xiqian_yu,xiqian_yu,xiqian_yu,11/Apr/24 04:08,24/Apr/24 07:35,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,pull-request-available,,"Currently, Flink project has CI scripts checking if dependencies with incompatible licenses are introduced.

Flink CDC module heavily relies on external libraries (especially connectors), so running similar checking scripts during every CI would be helpful preventing developers introducing questionable dependencies by accident.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-11 04:08:27.0,,,,,,,,,,"0|z1okew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Watermark alignment will cause data flow to experience serious shake,FLINK-35076,13575343,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,elon,elon,10/Apr/24 12:23,11/Apr/24 19:09,04/Jun/24 20:40,,1.16.1,,,,,,,,,,,Runtime / Coordination,,,,0,,,"In our company, there is a requirement scenario for multi-stream join operations, we are making modifications based on Flink watermark alignment, then I found that the final join output would experience serious shake.

and I analyzed the reasons: an upstream topic has more than 300 partitions. The number of partitions requested for this topic is too large, causing some partitions to frequently experience intermittent writes with QPS=0. This phenomenon is more serious between 2 am and 5 am.However, the overall topic writing is very smooth.

!image-2024-04-10-20-29-13-835.png!

The final join output will experience serious shake, as shown in the following diagram:

!image-2024-04-10-20-15-05-731.png!

Root cause:
 # The {{SourceOperator#emitLatestWatermark}} reports the lastEmittedWatermark to the SourceCoordinator.
 # If the partition write is zero during a certain period, the lastEmittedWatermark sent by the subtask corresponding to that partition remains unchanged.
 # The SourceCoordinator aggregates the watermarks of all subtasks according to the watermark group and takes the smallest watermark. This means that the maxAllowedWatermark may remain unchanged for some time, even though the overall upstream data flow is moving forward. until that minimum value is updated, only then will everything change, which will manifest as serious shake in the output data stream.

I think choosing the global minimum might not be a good option. Using min/max could more likely encounter some edge cases. Perhaps choosing a median value would be more appropriate? Or a more complex selection strategy?

If replaced with a median value, it can ensure that the overall data flow is very smooth:

!image-2024-04-10-20-23-13-872.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/24 12:15;elon;image-2024-04-10-20-15-05-731.png;https://issues.apache.org/jira/secure/attachment/13068095/image-2024-04-10-20-15-05-731.png","10/Apr/24 12:23;elon;image-2024-04-10-20-23-13-872.png;https://issues.apache.org/jira/secure/attachment/13068094/image-2024-04-10-20-23-13-872.png","10/Apr/24 12:26;elon;image-2024-04-10-20-25-59-387.png;https://issues.apache.org/jira/secure/attachment/13068097/image-2024-04-10-20-25-59-387.png","10/Apr/24 12:29;elon;image-2024-04-10-20-29-13-835.png;https://issues.apache.org/jira/secure/attachment/13068098/image-2024-04-10-20-29-13-835.png",,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 19:09:56 UTC 2024,,,,,,,,,,"0|z1ojw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 20:28;kkrugler;There are logical reasons why picking the minimum value is a requirement. In your situation, you could either set up the watermark strategy for your Kafka source to have a ""[max idleness|https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/datastream/event-time/generating_watermarks/#dealing-with-idle-sources]"", or you could shuffle the stream (via a rebalance()), which would avoid the problem of an idle partition.;;;","11/Apr/24 15:20;elon;[~kkrugler] 

Thank you for your reply.
Setting the idle time is not very controllable in terms of the specific timing. For example, setting it to 10 seconds, the minimum watermark will still not change within these 10 seconds unless the idle time is set as small as possible. I'm not sure if this could solve the problem and further testing is needed;
For the solution of shuffling the stream, I didn't quite understand. In the Flink API:
DataStream xx = env.fromSource(Source<OUT, ?, ?> source, WatermarkStrategy<OUT> timestampsAndWatermarks, String sourceName)
Only DataStream supports rebalance, Source can't rebalance. I'm not quite sure how to shuffle the data source before {{{}fromSource{}}}.;;;","11/Apr/24 19:09;kkrugler;Hi [~elon] - please post these questions about the impact of idleness and how to rebalance on Stack Overflow, or the Flink user list. That way the Q&A can benefit the entire community, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate TwoStageOptimizedAggregateRule,FLINK-35075,13575335,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,10/Apr/24 10:59,27/May/24 03:03,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 09:47:57 UTC 2024,,,,,,,,,,"0|z1oju8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 09:47;jackylau;hi [~snuyanzin] will you help review this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted,FLINK-35074,13575330,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,10/Apr/24 10:00,10/Apr/24 10:00,04/Jun/24 20:40,,1.18.2,,,,,,,,,,,Runtime / Checkpointing,,,,0,test-stability,,"AdaptiveScheduler: Test (module: tests) https://github.com/apache/flink/actions/runs/8609297979/job/23593291616#step:10:7708

{code}
Error: 02:38:03 02:38:03.567 [ERROR] org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted  Time elapsed: 0.62 s  <<< ERROR!
Apr 09 02:38:03 java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.
Apr 09 02:38:03 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Apr 09 02:38:03 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Apr 09 02:38:03 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted(SavepointITCase.java:1072)
Apr 09 02:38:03 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 09 02:38:03 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 09 02:38:03 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 09 02:38:03 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 09 02:38:03 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Apr 09 02:38:03 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 09 02:38:03 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Apr 09 02:38:03 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 09 02:38:03 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 09 02:38:03 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Apr 09 02:38:03 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Apr 09 02:38:03 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 09 02:38:03 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Apr 09 02:38:03 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Apr 09 02:38:03 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Apr 09 02:38:03 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Apr 09 02:38:03 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Apr 09 02:38:03 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Apr 09 02:38:03 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Apr 09 02:38:03 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Apr 09 02:38:03 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Apr 09 02:38:03 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Apr 09 02:38:03 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Apr 09 02:38:03 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Apr 09 02:38:03 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Apr 09 02:38:03 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Apr 09 02:38:03 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Apr 09 02:38:03 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
Apr 09 02:38:03 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Apr 09 02:38:03 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Apr 09 02:38:03 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Apr 09 02:38:03 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Apr 09 02:38:03 Caused by: org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.
Apr 09 02:38:03 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.onLeave(StopWithSavepoint.java:146)
Apr 09 02:38:03 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1294)
Apr 09 02:38:03 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToRestarting(AdaptiveScheduler.java:977)
Apr 09 02:38:03 	at org.apache.flink.runtime.scheduler.adaptive.FailureResultUtil.restartOrFail(FailureResultUtil.java:28)
Apr 09 02:38:03 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.lambda$onFailure$3(StopWithSavepoint.java:233)
Apr 09 02:38:03 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
Apr 09 02:38:03 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
Apr 09 02:38:03 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 09 02:38:03 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 09 02:38:03 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.lambda$null$1(StopWithSavepoint.java:134)
Apr 09 02:38:03 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:1246)
Apr 09 02:38:03 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$29(AdaptiveScheduler.java:1261)
Apr 09 02:38:03 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Apr 09 02:38:03 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Apr 09 02:38:03 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
Apr 09 02:38:03 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Apr 09 02:38:03 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
Apr 09 02:38:03 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
Apr 09 02:38:03 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
Apr 09 02:38:03 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
Apr 09 02:38:03 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
Apr 09 02:38:03 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
Apr 09 02:38:03 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
Apr 09 02:38:03 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
Apr 09 02:38:03 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
Apr 09 02:38:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
Apr 09 02:38:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Apr 09 02:38:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Apr 09 02:38:03 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
Apr 09 02:38:03 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
Apr 09 02:38:03 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
Apr 09 02:38:03 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
Apr 09 02:38:03 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
Apr 09 02:38:03 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
Apr 09 02:38:03 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
Apr 09 02:38:03 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
Apr 09 02:38:03 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Apr 09 02:38:03 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Apr 09 02:38:03 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Apr 09 02:38:03 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Apr 09 02:38:03 Caused by: org.apache.flink.util.FlinkRuntimeException: Stop-with-savepoint failed.
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointAbortAsync$18(StreamTask.java:1404)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$20(StreamTask.java:1429)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:888)
Apr 09 02:38:03 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:813)
Apr 09 02:38:03 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
Apr 09 02:38:03 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
Apr 09 02:38:03 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
Apr 09 02:38:03 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
Apr 09 02:38:03 	at java.lang.Thread.run(Thread.java:750)
{code}

This is awkward to test because we *are* expecting an ExecutionException (artificially added in this ITCase by {{failPath}}), but we are getting a different exception from a different location.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-10 10:00:31.0,,,,,,,,,,"0|z1ojt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock in LocalBufferPool when segments become available in the global pool,FLINK-35073,13575314,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Workaround,,jto,jto,10/Apr/24 08:24,15/Apr/24 13:32,04/Jun/24 20:40,15/Apr/24 13:31,1.19.0,,,,,,,,,,,Runtime / Network,,,,0,deadlock,network,"The reported issue is easy to reproduce in batch mode using hybrid shuffle and a somewhat large total number of slots in the cluster. Low parallelism (60) still triggers it.

Note: Joined a partial threaddump to illustrate the issue.

When `NetworkBufferPool.internalRecycleMemorySegments` is called. The following chain of call may happen:
{code:java}
NetworkBufferPool.internalRecycleMemorySegments -> 
LocalBufferPool.onGlobalPoolAvailable ->
LocalBufferPool.checkAndUpdateAvailability -> 
LocalBufferPool.requestMemorySegmentFromGlobalWhenAvailable{code}
Several instances of `LocalBufferPool` will be notified as soon as a segment becomes available in the global pool because of the implementation of `requestMemorySegmentFromGlobalWhenAvailable`:
{code:java}
networkBufferPool.getAvailableFuture().thenRun(this::onGlobalPoolAvailable));{code}
The issue arises when 2 or more threads go through this specific code path at the same time.

Each thread will notify the same instances of `LocalBufferPool` by invoking `onGlobalPoolAvailable` on each of them and in the process try to acquire a series of locks

As an example, assume there are 6 `LocalBufferPool` instance A, B, C, D, E and F:
Thread 1 calls `onGlobalPoolAvailable` on A, B, C and D. it locks A, B, C and tries to lock D
Thread 2 calls `onGlobalPoolAvailable` on D, E, F, and A. It locks D, E, F and tries to lock A
==> Threads 1 and 2 are mutually blocked.

The example threadump captured this issue:
First thread locked java.util.ArrayDeque@41d6a3bb and is blocked on java.util.ArrayDeque@e2b5e34
Second thread locked java.util.ArrayDeque@e2b5e34 and is blocked on java.util.ArrayDeque@41d6a3bb

 

Note that I'm not familiar enough with Flink internals to know what the fix should be but I'm happy to submit a PR if someone tells me what the correct behaviour should be.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/24 08:08;jto;deadlock_threaddump_extract.json;https://issues.apache.org/jira/secure/attachment/13068084/deadlock_threaddump_extract.json",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 13:31:33 UTC 2024,,,,,,,,,,"0|z1ojpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 09:01;a.pilipenko;[~jto] could you clarify which Flink version is affected with this issue?;;;","10/Apr/24 09:06;jto;[~a.pilipenko] updated the issue :)
I haven't tested older versions of Flink but I'd expect them to be affected too.;;;","15/Apr/24 13:31;jto;Closing this issue because even though I captured a deadlock, that was not what was causing my job to block. Instead it looks like there's a scheduling bug when using the default scheduler with hybrid shuffle. 
Adaptive scheduler + hybrid shuffle works fine.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Doris pipeline sink does not support applying AlterColumnTypeEvent,FLINK-35072,13575312,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,xiqian_yu,xiqian_yu,xiqian_yu,10/Apr/24 08:24,26/Apr/24 07:20,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"According to [Doris documentation|https://doris.apache.org/docs/sql-manual/sql-reference/Data-Definition-Statements/Alter/ALTER-TABLE-COLUMN/], altering column types dynamically is supported (via ALTER TABLE ... MODIFY COLUMN statement) when lossless conversion is available. However, now Doris pipeline connector has no support to AlterColumnTypeEvent, and raises RuntimeException all the time.

It would be convenient for users if they can sync compatible type conversions, and could be easily implemented by extending Doris' SchemaChangeManager helper class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-10 08:24:23.0,,,,,,,,,,"0|z1ojp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove dependency on flink-shaded from cdc source connector,FLINK-35071,13575293,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,loserwang1024,loserwang1024,10/Apr/24 06:30,13/May/24 09:32,04/Jun/24 20:40,,,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,pull-request-available,,"Like what flink-connector-aws does in FLINK-32208, the flink cdc source connectors depend on flink-shaded-guava. With the externalization of connector, connectors shouldn't rely on Flink-Shaded but instead shade dependencies such as this one themselves.

 

More over,  flink-shaded-guava will be included in the final jar package, so maybe cause dependency conflict.

Now we can see why dependency conflict occurs:
 * Since Flink 1.18, version of guava is upgrade from 30 to 31 (using 31.1-jre-17.0)

[!https://private-user-images.githubusercontent.com/125648852/308026228-5425d959-089a-46bd-9e34-0c262cd04457.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI3MzEwMDEsIm5iZiI6MTcxMjczMDcwMSwicGF0aCI6Ii8xMjU2NDg4NTIvMzA4MDI2MjI4LTU0MjVkOTU5LTA4OWEtNDZiZC05ZTM0LTBjMjYyY2QwNDQ1Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNDEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDQxMFQwNjMxNDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05MWNjYjIxYWQ2OTE1ODdkMmRlMjFlNzRkOWFhMmM5OTZmNTk1OWQzZDE5NWMwNGUxNjI1Y2VkMzQ4MTQ3YTkxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.NK51eb_rBFmtrNJgMj1b6YVPuoOrTLtnACpGrwW_CgI!|https://private-user-images.githubusercontent.com/125648852/308026228-5425d959-089a-46bd-9e34-0c262cd04457.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI3MzEwMDEsIm5iZiI6MTcxMjczMDcwMSwicGF0aCI6Ii8xMjU2NDg4NTIvMzA4MDI2MjI4LTU0MjVkOTU5LTA4OWEtNDZiZC05ZTM0LTBjMjYyY2QwNDQ1Ny5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNDEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDQxMFQwNjMxNDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT05MWNjYjIxYWQ2OTE1ODdkMmRlMjFlNzRkOWFhMmM5OTZmNTk1OWQzZDE5NWMwNGUxNjI1Y2VkMzQ4MTQ3YTkxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.NK51eb_rBFmtrNJgMj1b6YVPuoOrTLtnACpGrwW_CgI]
 * Since CDC 3.0.1, version of guava is also upgrade from 30 to 31 (using 31.1-jre-17.0)

 * So CDC 3.0.1 is compatible with Flink 1.18, CDC 2.x is is compatible with Flink 1.13-1.17. CDC 2.x and Flink 1.18 dependency conflict, CDC 3.0.1 and Flink 1.17(or earlier version) dependency conflict.
([https://maven.apache.org/plugins/maven-shade-plugin/shade-mojo.html#relocations]) in pom.xml.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 10 06:32:45 UTC 2024,,,,,,,,,,"0|z1ojkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 06:32;loserwang1024;[~renqs] I would like to do it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove unused duplicate code,FLINK-35070,13575173,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,niliushall,niliushall,09/Apr/24 12:25,09/Apr/24 12:29,04/Jun/24 20:40,,1.16.3,1.17.2,1.18.1,,,,,,1.16.3,,,Documentation,,,,0,documentation,,remove unused duplicate code that used directly in function,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,PR: https://github.com/niliushall/flink/commit/0993d79d3f832f08e5c262daa3a90c85f63332f2,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Tue Apr 09 12:29:39 UTC 2024,,,,,,,,,,"0|z1oiu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 12:29;niliushall;@[~mapohl], hi~, please have a look at this issue in your free time, thanks!

It has been fixed in [https://github.com/niliushall/flink/commit/0993d79d3f832f08e5c262daa3a90c85f63332f2].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContinuousProcessingTimeTrigger continuously registers timers in a loop at the end of the window,FLINK-35069,13575172,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lijinzhong,lijinzhong,09/Apr/24 12:24,19/Apr/24 11:26,04/Jun/24 20:40,,1.17.0,,,,,,,,,,,API / DataStream,,,,0,pull-request-available,,"In our production environment,  when TumblingEventTimeWindows and ContinuousProcessingTimeTrigger are used in combination within a WindowOperator, we observe a situation where the timers are continuously registered in a loop at the end of the window, leading to the job being perpetually stuck in timer processing.

!image-2024-04-09-20-23-54-415.png|width=516,height=205!

This issue can be reproduced using the [UT|https://github.com/apache/flink/blob/8e80ff889701ed1abbb5c15cd3943b254f1fb5cc/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/ContinuousProcessingTimeTriggerTest.java#L177] provided by the pr.",,,,,,,,,,,,,,,,,,,,,,,FLINK-20443,,,"09/Apr/24 12:23;lijinzhong;image-2024-04-09-20-23-54-415.png;https://issues.apache.org/jira/secure/attachment/13068064/image-2024-04-09-20-23-54-415.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 11:26:59 UTC 2024,,,,,,,,,,"0|z1oiu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 12:46;lijinzhong;cc [~Weijie Guo]   Could you please take a look at this issue?;;;","19/Apr/24 11:26;hong;Thanks [~lijinzhong] for the fix! Are we planning to backport this fix to 1.18 and 1.19 as well?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce built-in serialization support for Set,FLINK-35068,13575164,13564005,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,09/Apr/24 11:39,30/May/24 08:51,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,API / Type Serialization System,,,,0,pull-request-available,,"Introduce built-in serialization support for {{{}Set{}}}, another common Java collection type. We'll need to add a new built-in serializer for it ({{{}MultiSetTypeInformation{}}} utilizes {{MapSerializer}} underneath, but it could be more efficient for common {{{}Set{}}}).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 02:04:17 UTC 2024,,,,,,,,,,"0|z1ois8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 11:40;Zhanghao Chen;[~fangyong] May I take this task?;;;","27/May/24 02:04;Weijie Guo;Thanks [~Zhanghao Chen], you are assigned.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
 Support metadata 'op_type' virtual column for Postgres CDC Connector. ,FLINK-35067,13575155,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,walls.flink.m,loserwang1024,loserwang1024,09/Apr/24 10:02,24/May/24 05:42,04/Jun/24 20:40,,cdc-3.1.0,,,,,,,,cdc-3.2.0,,,Flink CDC,,,,0,,,"Like [https://github.com/apache/flink-cdc/pull/2913,] Support metadata 'op_type' virtual column for Postgres CDC Connector. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 05:42:18 UTC 2024,,,,,,,,,,"0|z1oiq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 22:40;walls.flink.m;[~renqs] Can I take this up?;;;","24/May/24 05:42;loserwang1024;[~walls.flink.m], I hope you’re doing well. I’ve been eagerly waiting for this feature for quite some time now. If you're swamped and don't have the bandwidth to work on it, I’d be more than happy to take over. Just let me know.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
TwoInputOperator in IterationBody cannot use keyBy,FLINK-35066,13575151,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,09/Apr/24 09:38,10/Apr/24 02:41,04/Jun/24 20:40,,ml-2.3.0,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,"Implementing a UDF KeyedRichCoProcessFunction or CoFlatMapFunction inside IterationBody yields a “java.lang.ClassCastException: org.apache.flink.iteration.IterationRecord cannot be cast to class org.apache.flink.api.java.tuple.Tuple” error.
More details about this bug can be found at https://lists.apache.org/thread/bgkw1g2tdgnp1xy1clsqtcfs3h18pkd6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-09 09:38:32.0,,,,,,,,,,"0|z1oipc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add numFiredTimers and numFiredTimersPerSecond metrics,FLINK-35065,13575144,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,09/Apr/24 09:07,09/Apr/24 15:28,04/Jun/24 20:40,09/Apr/24 15:28,1.19.0,,,,,,,,1.20.0,,,Runtime / Metrics,Runtime / Task,,,0,pull-request-available,,"Currently there is now way of knowing how many timers are being fired by Flink, so it's impossible to distinguish, even using code profiling, if operator is firing only a couple of heavy timers per second using ~100% of the CPU time, vs firing thousands of timer per seconds.

We could add the following metrics to address this issue:
* numFiredTimers - total number of fired timers per operator
* numFiredTimersPerSecond - per second rate of firing timers per operator",,,,,,,,,,,,,,,,,,,,,,,FLINK-32954,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 15:28:16 UTC 2024,,,,,,,,,,"0|z1oins:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 15:28;pnowojski;Merged to master as 98bd2659f5b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink sql connector pulsar/hive com.fasterxml.jackson.annotation.JsonFormat$Value conflict,FLINK-35064,13575143,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,elon,elon,09/Apr/24 08:55,15/Apr/24 07:57,04/Jun/24 20:40,,1.16.1,,,,,,,,,,,Connectors / Hive,Connectors / Pulsar,,,0,pull-request-available,,"When I compile and package {{flink-sql-connector-pulsar}} & {{{}flink-sql-connector-hive{}}}, and then put these two jar files into the Flink lib directory, I execute the following SQL statement through {{{}bin/sql-client.sh{}}}:

 
{code:java}
// code placeholder
CREATE TABLE
pulsar_table (
content string,
proc_time AS PROCTIME ()
)
WITH
(
'connector' = 'pulsar',
'topics' = 'persistent://xxx',
'service-url' = 'pulsar://xxx',
'source.subscription-name' = 'xxx',
'source.start.message-id' = 'latest',
'format' = 'csv',
'pulsar.client.authPluginClassName' = 'org.apache.pulsar.client.impl.auth.AuthenticationToken',
'pulsar.client.authParams' = 'token:xxx'
);
 
select * from pulsar_table; {code}
The task error exception stack is as follows：

 
{code:java}
Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.annotation.JsonFormat$Value.empty()Lcom/fasterxml/jackson/annotation/JsonFormat$Value;    at org.apache.pulsar.shade.com.fasterxml.jackson.databind.cfg.MapperConfig.<clinit>(MapperConfig.java:56) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.shade.com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:660) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.shade.com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:576) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.common.util.ObjectMapperFactory.createObjectMapperInstance(ObjectMapperFactory.java:151) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.common.util.ObjectMapperFactory.<clinit>(ObjectMapperFactory.java:142) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.client.impl.conf.ConfigurationDataUtils.create(ConfigurationDataUtils.java:35) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.client.impl.conf.ConfigurationDataUtils.<clinit>(ConfigurationDataUtils.java:43) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.client.impl.ClientBuilderImpl.loadConf(ClientBuilderImpl.java:77) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.createClient(PulsarClientFactory.java:105) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.<init>(PulsarSourceEnumerator.java:95) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.<init>(PulsarSourceEnumerator.java:76) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.connector.pulsar.source.PulsarSource.createEnumerator(PulsarSource.java:144) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:213) ~[flink-dist_2.12-1.16.1.jar:1.16.1]

{code}
 

The exception shows a conflict with {{{}com.fasterxml.jackson.annotation.JsonFormat$Value{}}}. I investigated and found that {{flink-sql-connector-pulsar}} and {{flink-sql-connector-hive}} depend on different versions, leading to this conflict.
{code:java}
// flink-sql-connector-pulsar pom.xml
<dependency>
    <groupId>com.fasterxml.jackson</groupId>
    <artifactId>jackson-bom</artifactId>
    <type>pom</type>
    <scope>import</scope>
    <version>2.13.4.20221013</version>
</dependency> 

// flink-sql-connector-hive pom.xml
<dependency>
    <groupId>com.fasterxml.jackson</groupId>
    <artifactId>jackson-bom</artifactId>
    <type>pom</type>
    <scope>import</scope>
    <version>2.15.3</version>
</dependency>{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 07:57:32 UTC 2024,,,,,,,,,,"0|z1oink:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 12:20;chalixar;While we try to maintain dependency upgrades across connectors this deflection is expected since each connector still evolves independently.
I would use maven relocation ([https://maven.apache.org/plugins/maven-shade-plugin/examples/class-relocation.html)] to resolve the issue.
If you want to open another ticket for upgrade for pulsar this can be done but this will only take effect on the next minor version release of the connector;;;","11/Apr/24 03:08;elon;[~chalixar]

Thank you for your reply. I also used relocation to solve this problem, and I am very willing to upgrade for pulsar. My question is, do other connectors also need relocation, or is it just for handling flink-connector-pulsar?;;;","11/Apr/24 07:52;chalixar;AFAIK, We don't specifically do them unless there is a reason for it (CVEs/ deprecations/ part of new change).;;;","15/Apr/24 05:40;syhily;I have made a comment in the related PR. I just copy/paste it here for reference.

The {{pulsar-client-all}} bundle its Jackson internally with a package named {{org.apache.pulsar.shade.com.fasterxml}}. The jackson used in flink shouldn't be shaded into the same package name. Or it may cause more severe issues in class conflicting. I don't think this is a good idea.

The root cause is from pulsar client side with the Jackson shading issues. Shade a jackson causes a lot of issues in developing and running the flink-sql-connector-pulsar before we (StreamNative) donate it to flink community. I think we should ask pulsar do not shade the {{jackson-annotation}} in client for fixing this problem.;;;","15/Apr/24 07:57;syhily;AFAICS, the {{pulsar-client-all}} should be replaced to {{pulsar-client}} in the flink-connector-pulsar. The {{jackson-annotations}} and other common dependencies don't get shaded into the client. So we can use the Flink's jackson freely. I think this may be the best approach to solve this problem.

BTW, the initial adoption of using {{pulsar-client-all}} is because we need the pulsar admin api to accomplish some heavy operations. Since we have drop the usage of pulsar admin api. This should also be avoided to use such a heavy pulsar client indeed.

WDYT, [~elon]. And [~Tison], can you help me double confirm this?;;;",,,,,,,,,,,,,,,,,,,,,,,,
Flink sql connector pulsar/hive com.fasterxml.jackson.annotation.JsonFormat$Value conflict,FLINK-35063,13575141,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,elon,elon,09/Apr/24 08:53,09/Apr/24 08:59,04/Jun/24 20:40,09/Apr/24 08:59,1.16.1,,,,,,,,,,,Connectors / Hive,Connectors / Pulsar,,,0,,,"When I compile and package {{flink-sql-connector-pulsar}} & {{{}flink-sql-connector-hive{}}}, and then put these two jar files into the Flink lib directory, I execute the following SQL statement through {{{}bin/sql-client.sh{}}}:

 
{code:java}
// code placeholder
CREATE TABLE
pulsar_table (
content string,
proc_time AS PROCTIME ()
)
WITH
(
'connector' = 'pulsar',
'topics' = 'persistent://xxx',
'service-url' = 'pulsar://xxx',
'source.subscription-name' = 'xxx',
'source.start.message-id' = 'latest',
'format' = 'csv',
'pulsar.client.authPluginClassName' = 'org.apache.pulsar.client.impl.auth.AuthenticationToken',
'pulsar.client.authParams' = 'token:xxx'
);
 
select * from pulsar_table; {code}
The task error exception stack is as follows：

 
{code:java}
Caused by: java.lang.NoSuchMethodError: com.fasterxml.jackson.annotation.JsonFormat$Value.empty()Lcom/fasterxml/jackson/annotation/JsonFormat$Value;    at org.apache.pulsar.shade.com.fasterxml.jackson.databind.cfg.MapperConfig.<clinit>(MapperConfig.java:56) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.shade.com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:660) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.shade.com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:576) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.common.util.ObjectMapperFactory.createObjectMapperInstance(ObjectMapperFactory.java:151) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.common.util.ObjectMapperFactory.<clinit>(ObjectMapperFactory.java:142) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.client.impl.conf.ConfigurationDataUtils.create(ConfigurationDataUtils.java:35) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.client.impl.conf.ConfigurationDataUtils.<clinit>(ConfigurationDataUtils.java:43) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.pulsar.client.impl.ClientBuilderImpl.loadConf(ClientBuilderImpl.java:77) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.createClient(PulsarClientFactory.java:105) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.<init>(PulsarSourceEnumerator.java:95) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.connector.pulsar.source.enumerator.PulsarSourceEnumerator.<init>(PulsarSourceEnumerator.java:76) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.connector.pulsar.source.PulsarSource.createEnumerator(PulsarSource.java:144) ~[flink-sql-connector-pulsar-4.0-SNAPSHOT.jar:4.0-SNAPSHOT]    at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:213) ~[flink-dist_2.12-1.16.1.jar:1.16.1]

{code}
 

The exception shows a conflict with {{{}com.fasterxml.jackson.annotation.JsonFormat$Value{}}}. I investigated and found that {{flink-sql-connector-pulsar}} and {{flink-sql-connector-hive}} depend on different versions, leading to this conflict.
{code:java}
// flink-sql-connector-pulsar pom.xml
<dependency>
    <groupId>com.fasterxml.jackson</groupId>
    <artifactId>jackson-bom</artifactId>
    <type>pom</type>
    <scope>import</scope>
    <version>2.13.4.20221013</version>
</dependency> 

// flink-sql-connector-hive pom.xml
<dependency>
    <groupId>com.fasterxml.jackson</groupId>
    <artifactId>jackson-bom</artifactId>
    <type>pom</type>
    <scope>import</scope>
    <version>2.15.3</version>
</dependency>{code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-09 08:53:46.0,,,,,,,,,,"0|z1oin4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate RewriteMultiJoinConditionRule,FLINK-35062,13575135,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,09/Apr/24 08:16,15/Apr/24 09:39,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-09 08:16:32.0,,,,,,,,,,"0|z1oils:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate TemporalJoinUtil,FLINK-35061,13575133,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,09/Apr/24 08:11,09/Apr/24 08:17,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-09 08:11:15.0,,,,,,,,,,"0|z1oilc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide compatibility of old CheckpointMode for connector testing framework,FLINK-35060,13575074,13566461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,09/Apr/24 06:59,11/Apr/24 07:30,04/Jun/24 20:40,11/Apr/24 07:30,,,,,,,,,1.20.0,,,Runtime / Checkpointing,Tests,,,0,pull-request-available,,"After FLINK-34516, the {{org.apache.flink.streaming.api.CheckpointingMode}} has been moved to {{org.apache.flink.core.execution.CheckpointingMode}}. It introduced a breaking change to connector testing framework as well as to externalized connector repos by mistake. This should be fixed.",,,,,,,,,,,,,,,,,,,,,FLINK-34516,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 07:30:30 UTC 2024,,,,,,,,,,"0|z1oik0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 07:30;Sergey Nuyanzin;Merged as [dfb827a38bc81fe4610cd0c88c66b8d5da1c0147|https://github.com/apache/flink/commit/dfb827a38bc81fe4610cd0c88c66b8d5da1c0147];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.postgresql:postgresql from 42.5.1 to 42.7.3 in flink-connector-jdbc,FLINK-35059,13575073,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,09/Apr/24 06:58,09/Apr/24 09:05,04/Jun/24 20:40,09/Apr/24 09:05,,,,,,,,,jdbc-3.2.0,,,Connectors / JDBC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 09:05:06 UTC 2024,,,,,,,,,,"0|z1oijs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 09:05;Sergey Nuyanzin;Merged as [12f778da715635be21ca48cbaa2cd10490d09235|https://github.com/apache/flink-connector-jdbc/commit/12f778da715635be21ca48cbaa2cd10490d09235];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encountered change event for table db.table whose schema isn't known to this connector,FLINK-35058,13575064,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,MOBIN,MOBIN,09/Apr/24 05:49,22/Apr/24 03:18,04/Jun/24 20:40,22/Apr/24 03:18,1.17.1,,,,,,,,,,,Flink CDC,,,,0,,,"Flink1.17.1

flink-cdc：flink-sql-connector-mysql-cdc-2.4.1.jar
{code:java}
CREATE TABLE `test_cdc_timestamp` (
  `id` BIGINT COMMENT '主键id',
   ....
   proctime AS PROCTIME(),
   PRIMARY KEY(id) NOT ENFORCED
) WITH (
      'connector' = 'mysql-cdc',
      'hostname' = 'xxxxx',
      'scan.startup.mode' = 'timestamp',
      'scan.startup.timestamp-millis' = '1712419200000' ,
      'port' = '3306',
      'username' = 'xxx',
      'password' = 'xxx',
      'database-name' = 'xxtablename',
      'table-name' = 'xxdatabase',
      'scan.incremental.snapshot.enabled' = 'false',
       'debezium.snapshot.locking.mode' = 'none',
       'server-id' = '5701',
     'server-time-zone' = 'Asia/Shanghai',
    'debezium.skipped.operations' = 'd'
      ); {code}
When I use 'scan.startup.mode' = 'latent-offset 'or'initial' to synchronize data normally, when I use 'scan.startup.mode' = 'timestamp', the following error is reported
{code:java}
2024-04-09 11:11:15.619 [debezium-engine] INFO  io.debezium.util.Threads  - Requested thread factory for connector MySqlConnector, id = mysql_binlog_source named = change-event-source-coordinator
2024-04-09 11:11:15.621 [debezium-engine] INFO  io.debezium.util.Threads  - Creating thread debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator
2024-04-09 11:11:15.629 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.pipeline.ChangeEventSourceCoordinator  - Metrics registered
2024-04-09 11:11:15.630 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.pipeline.ChangeEventSourceCoordinator  - Context created
2024-04-09 11:11:15.642 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.connector.mysql.MySqlSnapshotChangeEventSource  - No previous offset has been found
2024-04-09 11:11:15.642 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.connector.mysql.MySqlSnapshotChangeEventSource  - According to the connector configuration only schema will be snapshotted
2024-04-09 11:11:15.644 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.pipeline.ChangeEventSourceCoordinator  - Snapshot ended with SnapshotResult [status=SKIPPED, offset=null]
2024-04-09 11:11:15.652 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.util.Threads  - Requested thread factory for connector MySqlConnector, id = mysql_binlog_source named = binlog-client
2024-04-09 11:11:15.656 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.pipeline.ChangeEventSourceCoordinator  - Starting streaming
2024-04-09 11:11:15.682 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - GTID set purged on server: 0969640a-1d48-11ed-b6cf-28dee561557c:1-27603868993,70958f24-2253-11eb-891d-f875a48ad7b1:1-50323,ec1e6593-2251-11eb-9c18-f875a48ad539:1-25345454762
2024-04-09 11:11:15.682 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Skip 0 events on streaming start
2024-04-09 11:11:15.682 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Skip 0 rows on streaming start
2024-04-09 11:11:15.683 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.util.Threads  - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-04-09 11:11:15.686 [blcxxxx.mysql.com:3306] INFO  io.debezium.util.Threads  - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-04-09 11:11:15.700 [blcxxxx.mysql.com:3306] INFO  io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Connected to MySQL binlog at xxx.mysql.com:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=, currentBinlogPosition=0, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=, restartBinlogPosition=0, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]
2024-04-09 11:11:15.700 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Waiting for keepalive thread to start
2024-04-09 11:11:15.701 [blcxxxx.mysql.com:3306] INFO  io.debezium.util.Threads  - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2024-04-09 11:11:15.701 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Keepalive thread is running
2024-04-09 11:11:15.818 [blcxxxx.mysql.com:3306] ERROR io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Encountered change event 'Event{header=EventHeaderV4{timestamp=1711939504000, eventType=TABLE_MAP, serverId=431777, headerLength=19, dataLength=81, nextPosition=981190, flags=0}, data=TableMapEventData{tableId=933966, database='xxxdatabase', table='xxxtablename', columnTypes=8, 15, 8, 15, 8, 3, 2, 1, -11, 18, 18, 1, columnMetadata=0, 400, 0, 1024, 0, 0, 0, 0, 4, 0, 0, 0, columnNullability={4, 6, 8, 11}, eventMetadata=null}}' at offset {transaction_id=null, file=mysql-bin.016482, pos=981027, gtids=0969640a-1d48-11ed-b6cf-28dee561557c:27603868994-27603870019, server_id=431777, event=1} for table xxxdatabase.xxxtablename whose schema isn't known to this connector. One possible cause is an incomplete database history topic. Take a new snapshot in this case.
Use the mysqlbinlog tool to view the problematic event: mysqlbinlog --start-position=981090 --stop-position=981190 --verbose mysql-bin.016482
2024-04-09 11:11:15.819 [blcxxxx.mysql.com:3306] ERROR io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Error during binlog processing. Last offset stored = {transaction_id=null, file=mysql-bin.016482, pos=981027, gtids=0969640a-1d48-11ed-b6cf-28dee561557c:27603868994-27603870019, server_id=431777, event=1}, binlog reader near position = mysql-bin.016482/981090
2024-04-09 11:11:15.821 [blcxxxx.mysql.com:3306] ERROR io.debezium.pipeline.ErrorHandler  - Producer failure
io.debezium.DebeziumException: Error processing binlog event
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleEvent(MySqlStreamingChangeEventSource.java:429)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.lambda$execute$25(MySqlStreamingChangeEventSource.java:1095)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1246)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1072)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631)
    at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932)
    at java.lang.Thread.run(Thread.java:745)
Caused by: io.debezium.DebeziumException: Encountered change event for table xxdatabase.xxtablename whose schema isn't known to this connector
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.informAboutUnknownTableIfRequired(MySqlStreamingChangeEventSource.java:753)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.informAboutUnknownTableIfRequired(MySqlStreamingChangeEventSource.java:819)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleUpdateTableMetadata(MySqlStreamingChangeEventSource.java:720)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.lambda$execute$13(MySqlStreamingChangeEventSource.java:1046)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleEvent(MySqlStreamingChangeEventSource.java:408)
    ... 6 common frames omitted
2024-04-09 11:11:15.821 [blcxxxx.mysql.com:3306] INFO  io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Error processing binlog event, and propagating to Kafka Connect so it stops this connector. Future binlog events read before connector is shutdown will be ignored.
2024-04-09 11:11:16.124 [debezium-engine] INFO  io.debezium.embedded.EmbeddedEngine  - Stopping the task and engine
2024-04-09 11:11:16.124 [debezium-engine] INFO  io.debezium.connector.common.BaseSourceTask  - Stopping down connector
2024-04-09 11:11:16.216 [debezium-mysqlconnector-mysql_binlog_source-change-event-source-coordinator] INFO  io.debezium.pipeline.ChangeEventSourceCoordinator  - Finished streaming
2024-04-09 11:11:16.216 [blcxxxx.mysql.com:3306] INFO  io.debezium.connector.mysql.MySqlStreamingChangeEventSource  - Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, file=mysql-bin.016482, pos=981027, gtids=0969640a-1d48-11ed-b6cf-28dee561557c:27603868994-27603870019, server_id=431777, event=71904}
2024-04-09 11:11:16.218 [pool-10-thread-1] INFO  io.debezium.jdbc.JdbcConnection  - Connection gracefully closed
2024-04-09 11:11:16.221 [debezium-engine] ERROR com.ververica.cdc.debezium.internal.Handover  - Reporting error:
com.ververica.cdc.connectors.shaded.org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped.
    at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:50)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleEvent(MySqlStreamingChangeEventSource.java:429)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.lambda$execute$25(MySqlStreamingChangeEventSource.java:1095)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1246)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1072)
    at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631)
    at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932)
    at java.lang.Thread.run(Thread.java:745)
Caused by: io.debezium.DebeziumException: Error processing binlog event
    ... 7 common frames omitted
Caused by: io.debezium.DebeziumException: Encountered change event for table xxdatabase.xxtablename whose schema isn't known to this connector
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.informAboutUnknownTableIfRequired(MySqlStreamingChangeEventSource.java:753)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.informAboutUnknownTableIfRequired(MySqlStreamingChangeEventSource.java:819)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleUpdateTableMetadata(MySqlStreamingChangeEventSource.java:720)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.lambda$execute$13(MySqlStreamingChangeEventSource.java:1046)
    at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleEvent(MySqlStreamingChangeEventSource.java:408)
    ... 6 common frames omitted
2024-04-09 11:11:16.223 [Source: source_ascribe_all_result_data[1] -> Calc[2] -> Sink: print[3] (1/1)#0] INFO  io.debezium.embedded.EmbeddedEngine  - Stopping the embedded engine
2024-04-09 11:11:16.227 [Source: source_ascribe_all_result_data[1] -> Calc[2] -> Sink: print[3] (1/1)#0] INFO  io.debezium.embedded.EmbeddedEngine  - Stopping the embedded engine
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-09 05:49:43.0,,,,,,,,,,"0|z1oihs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.apache.commons:commons-compress from 1.25.0 to 1.26.1 for Flink jdbc connector,FLINK-35057,13575060,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,09/Apr/24 05:33,09/Apr/24 06:36,04/Jun/24 20:40,09/Apr/24 06:35,,,,,,,,,jdbc-3.2.0,,,Connectors / JDBC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 06:35:46 UTC 2024,,,,,,,,,,"0|z1oigw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 06:35;Sergey Nuyanzin;Merged as [cd48c4a5b88a34934d51dac805d0ce0c42c4ea02|https://github.com/apache/flink-connector-jdbc/commit/cd48c4a5b88a34934d51dac805d0ce0c42c4ea02];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when initial sqlserver table that's primary key is datetime type,  it org.apache.flink.table.api.ValidationException: Timestamp precision must be between 0 and 9 (both inclusive)",FLINK-35056,13575056,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,yandufeng,yandufeng,09/Apr/24 05:21,15/Apr/24 12:47,04/Jun/24 20:40,,2.0.0,,,,,,,,,,,Flink CDC,,,,0,,,"when initial sqlserver table that's primary key is datetime type.
it error:

 org.apache.flink.table.api.ValidationException: Timestamp precision must be between 0 and 9 (both inclusive)

 

i find datetime's length is 23. it exceed 9. so it errors.",,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/24 07:30;yandufeng;sqlserver-bug.png;https://issues.apache.org/jira/secure/attachment/13068060/sqlserver-bug.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 12:46:32 UTC 2024,,,,,,,,,,"0|z1oig0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 12:46;fffy;Hi,can u give me a example and let me to reappear it。;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC connector release contains dependency with incompatible licenses,FLINK-35055,13575051,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,xiqian_yu,xiqian_yu,09/Apr/24 03:04,29/Apr/24 08:28,04/Jun/24 20:40,23/Apr/24 10:05,,,,,,,,,cdc-3.1.0,,,Flink CDC,,,,0,pull-request-available,,"Currently, Flink CDC connector releases both slim and fat jars. Apart from CDC itself, all of its dependencies are packaged into fat jars, including some with incompatible licenses:
 * Db2 connector: `com.ibm.db2.jcc:db2jcc:db2jcc4` licensed with a non-FOSS license (International Program License Agreement).
 * MySQL connector: `mysql:mysql-connector-java` licensed with GPLv2 license, which is incompatible with Apache 2.0.
 * Oracle connector: `com.oracle.ojdbc` licensed with a non-FOSS license (Oracle Free Use Terms and Conditions).
 * OceanBase connector: `mysql:mysql-connector-java` licensed with GPLv2 license, which is incompatible with Apache 2.0.

To fix this problem we may:
 # Exclude questionable dependencies from released jar;
 # Add docs to guide user download & place dependencies manually.",,,,,,,,,,,,,,,,,,FLINK-34185,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 10:05:36 UTC 2024,,,,,,,,,,"0|z1oiew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 03:06;xiqian_yu;[~renqs] I'm willing to take this ticket if needed.;;;","23/Apr/24 10:05;renqs;flink-cdc master: 01ec7da98954db4a985d5c01862b430f9844d218;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate TemporalJoinRewriteWithUniqueKeyRule,FLINK-35054,13575050,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,09/Apr/24 03:02,09/Apr/24 08:01,04/Jun/24 20:40,,1.20.0,,,,,,,,1.20.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-09 03:02:12.0,,,,,,,,,,"0|z1oieo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TIMESTAMP with LOCAL TIME ZONE not supported by JDBC connector for Postgres,FLINK-35053,13575000,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pietro97,pietro97,08/Apr/24 14:38,30/Apr/24 09:39,04/Jun/24 20:40,,1.18.1,1.19.0,jdbc-3.1.2,,,,,,,,,Connectors / JDBC,,,,0,,,"The JDBC sink for Postgres does not support {{{}TIMESTAMP_LTZ{}}}, nor {{TIMESTAMP WITH TIME ZONE}} types.

Related issues: FLINK-22199, FLINK-20869
h2. Problem Explanation

A Postgres {{target_table}} has a field {{tm_tz}} of type {{timestamptz}} .
{code:sql}
-- Postgres DDL
CREATE TABLE target_table (
    tm_tz TIMESTAMP WITH TIME ZONE
)
{code}
In Flink we have a table with a column of type {{{}TIMESTAMP_LTZ(6){}}}, and our goal is to sink it to {{{}target_table{}}}.
{code:sql}
-- Flink DDL
CREATE TABLE sink (
    tm_tz TIMESTAMP_LTZ(6)
) WITH (
    'connector' = 'jdbc',
    'table-name' = 'target_table'
    ...
)
{code}
According to [AbstractPostgresCompatibleDialect.supportedTypes()|https://github.com/apache/flink-connector-jdbc/blob/7025642d88ff661e486745b23569595e1813a1d0/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractPostgresCompatibleDialect.java#L109], {{TIMESTAMP_WITH_LOCAL_TIME_ZONE}} is supported, while {{TIMESTAMP_WITH_TIME_ZONE}} is not.

However, when the converter is created via [AbstractJdbcRowConverter.externalConverter()|https://github.com/apache/flink-connector-jdbc/blob/7025642d88ff661e486745b23569595e1813a1d0/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/converter/AbstractJdbcRowConverter.java#L246], it throws an {{UnsupportedOperationException}} since {{TIMESTAMP_WITH_LOCAL_TIME_ZONE}} is *not* among the available types, while [{{TIMESTAMP_WITH_TIME_ZONE}}|https://github.com/apache/flink-connector-jdbc/blob/7025642d88ff661e486745b23569595e1813a1d0/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/converter/AbstractJdbcRowConverter.java#L168] is.
{code:java}
Exception in thread ""main"" java.lang.UnsupportedOperationException: Unsupported type:TIMESTAMP_LTZ(6)
	at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.createInternalConverter(AbstractJdbcRowConverter.java:186)
	at org.apache.flink.connector.jdbc.databases.postgres.dialect.PostgresRowConverter.createPrimitiveConverter(PostgresRowConverter.java:99)
	at org.apache.flink.connector.jdbc.databases.postgres.dialect.PostgresRowConverter.createInternalConverter(PostgresRowConverter.java:58)
	at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.createNullableInternalConverter(AbstractJdbcRowConverter.java:118)
	at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.<init>(AbstractJdbcRowConverter.java:68)
	at org.apache.flink.connector.jdbc.databases.postgres.dialect.PostgresRowConverter.<init>(PostgresRowConverter.java:47)
	at org.apache.flink.connector.jdbc.databases.postgres.dialect.PostgresDialect.getRowConverter(PostgresDialect.java:51)
	at org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.getScanRuntimeProvider(JdbcDynamicTableSource.java:184)
	at org.apache.flink.table.planner.connectors.DynamicSourceUtils.validateScanSource(DynamicSourceUtils.java:478)
	at org.apache.flink.table.planner.connectors.DynamicSourceUtils.prepareDynamicSource(DynamicSourceUtils.java:161)
	at org.apache.flink.table.planner.connectors.DynamicSourceUtils.convertSourceToRel(DynamicSourceUtils.java:125)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:118)
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:4002)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2872)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2432)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2346)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2291)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:728)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:714)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3848)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:618)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:229)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:205)
	at org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:69)
	at org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)
	at org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:73)
	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:272)
	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:262)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:708)
{code}
h3. Using TIMESTAMP WITH TIME ZONE

Defining {{tm_tz}} in Flink as {{TIMESTAMP(6) WITH TIME ZONE}} instead of {{TIMESTAMP_LTZ(6)}} does not solve the issue, and returns the following error instead:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered ""TIME"" at line 1, column 66.
Was expecting:
    ""LOCAL"" ...
    
	at org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:81)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:102)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)
{code}
h3. Using TIMESTAMP

Defining {{tm_tz}} in Flink as {{TIMESTAMP(6)}} can lead to potentially incorrect time zone conversions.

For instance, assume that the local time is GMT+2 and we have a row in Flink with {{tm_tz}} equal to {{'2024-04-01 00:00:00'}} (UTC). When the {{toTimestamp()}} method ([reference|https://github.com/apache/flink-connector-jdbc/blob/ab5d6159141bdbe8aed78e24c9500a136efbfac0/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/converter/AbstractJdbcRowConverter.java#L251]) used by {{AbstractJdbcRowConverter.createExternalConverter()}} is invoked
!createExternalConverter.png|width=80%!

it adds the local timezone to it, instead of ""+00"":

!TimestampData.png|width=80%!

!Timestamp.png|width=80%!

Postgres will therefore receive {{'2024-04-01 00:00:00+02'}} (instead of +00) and will convert it to {{'2024-03-31 22:00:00+00'.}}
h2. Possible Solutions
 # Make the JDBC connector support {{TIMESTAMP_LTZ}} by adding a proper converter to [AbstractJdbcRowConverter.externalConverter()|https://github.com/apache/flink-connector-jdbc/blob/7025642d88ff661e486745b23569595e1813a1d0/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/converter/AbstractJdbcRowConverter.java#L246].
 # Fix the behavior of the [converter|https://github.com/apache/flink-connector-jdbc/blob/ab5d6159141bdbe8aed78e24c9500a136efbfac0/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/converter/AbstractJdbcRowConverter.java#L251] for {{TIMESTAMP}} types, so that:
 ## it either forces UTC timezone (like adding ""+00"" to timestamps) or
 ## it removes timezone information from the timestamp passed to the external system",,,,,,,,,,,,,,,,,,,,,,,FLINK-22199,,,"08/Apr/24 16:09;pietro97;Timestamp.png;https://issues.apache.org/jira/secure/attachment/13067955/Timestamp.png","08/Apr/24 16:09;pietro97;TimestampData.png;https://issues.apache.org/jira/secure/attachment/13067956/TimestampData.png","08/Apr/24 16:23;pietro97;createExternalConverter.png;https://issues.apache.org/jira/secure/attachment/13067961/createExternalConverter.png",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-08 14:38:56.0,,,,,,,,,,"0|z1oi3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Webhook validator should reject unsupported Flink versions,FLINK-35052,13574986,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,08/Apr/24 12:27,24/May/24 13:27,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,The admission webhook currently does not verify if FlinkDeployment CR utilizes Flink versions that are not supported by the Operator. This causes the CR to be accepted and the failure to be postponed until the reconciliation phase. We should instead fail fast and provide users direct feedback.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-08 12:27:20.0,,,,,,,,,,"0|z1oi0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Weird priorities when processing unaligned checkpoints,FLINK-35051,13574981,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pnowojski,pnowojski,08/Apr/24 12:01,09/Apr/24 05:04,04/Jun/24 20:40,,1.16.3,1.17.2,1.18.1,1.19.0,,,,,,,,Runtime / Checkpointing,Runtime / Network,Runtime / Task,,0,,,"While looking through the code I noticed that `StreamTask` is processing unaligned checkpoints in strange order/priority. The end result is that unaligned checkpoint `Start Delay` /  triggering checkpoints in `StreamTask` can be unnecessary delayed by other mailbox actions in the system, like for example:
* processing time timers
* `AsyncWaitOperator` results
* ... 

Incoming UC barrier is treated as a priority event by the network stack (it will be polled from the input before anything else). This is what we want, but polling elements from network stack has lower priority then processing enqueued mailbox actions.

Secondly, if AC barrier timeout to UC, that's done via a mailbox action, but this mailbox action is also not prioritised in any way, so other mailbox actions could be unnecessarily executed first. 

On top of that there is a clash of two separate concepts here:
# Mailbox priority. yieldToDownstream - so in a sense reverse to what we would like to have for triggering checkpoint, but that only kicks in #yield() calls, where it's actually correct, that operator in a middle of execution can not yield to checkpoint - it should only yield to downstream.
# Control mails in mailbox executor - cancellation is done via that, it bypasses whole mailbox queue.
# Priority events in the network stack.

It's unfortunate that 1. vs 3. has a naming clash, as priority name is used in both things, and highest network priority event containing UC barrier, when executed via mailbox has actually the lowest mailbox priority.

Control mails mechanism is a kind of priority mails executed out of order, but doesn't generalise well for use in checkpointing.

This whole thing should be re-worked at some point. Ideally what we would like have is that:
* mail to convert AC barriers to UC
* polling UC barrier from the network input
* checkpoint trigger via RPC for source tasks

should be processed first, with an exception of yieldToDownstream, where current mailbox priorities should be adhered.",,,,,,,,,,,,,,,,,,,,,,,FLINK-34704,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 05:03:27 UTC 2024,,,,,,,,,,"0|z1ohzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 05:03;pvary;FLINK-34704 is one of the ways this issue materializes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Lazy Initialization of DynamoDbBeanElementConverter,FLINK-35050,13574962,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,chalixar,chalixar,08/Apr/24 08:50,08/Apr/24 10:23,04/Jun/24 20:40,08/Apr/24 10:23,aws-connector-4.3.0,,,,,,,,,,,Connectors / DynamoDB,,,,0,,,"h2. Description

{{DynamoDbBeanElementConverter}} implements {{ElementConverter}} which now supports open method as of FLINK-29938, we need to remove lazy initialization given that it is now unblocked.",,,,,,,,,,,,,,,,,,,,FLINK-30388,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 09:27:59 UTC 2024,,,,,,,,,,"0|z1ohvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 09:27;chalixar;So apparently this is a Duplicate of https://issues.apache.org/jira/browse/FLINK-30388 
We can close this one, I will update the PR to address the original issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Map Async State API for ForStStateBackend,FLINK-35049,13574947,13574085,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yanfei Lei,masteryhx,masteryhx,08/Apr/24 06:52,27/May/24 13:23,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-08 06:52:50.0,,,,,,,,,,"0|z1ohs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement all methods of AsyncKeyedStateBakend ,FLINK-35048,13574946,13574085,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,08/Apr/24 06:50,08/May/24 07:14,04/Jun/24 20:40,08/May/24 07:14,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,,,,,,,,,,,,,,,FLINK-35046,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 08 07:14:15 UTC 2024,,,,,,,,,,"0|z1ohrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/24 07:14;masteryhx;merged 17d89054 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ForStStateBackend to manage ForSt,FLINK-35047,13574945,13574085,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,08/Apr/24 06:45,23/May/24 04:11,04/Jun/24 20:40,26/Apr/24 01:51,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,"A ForStStateBackend is introduced to leverage ForSt as state store for Flink.

This ticket includes:
 # Life cycle of ForSt, including initlization/closing
 # basic options, resource control, metrics like RocksDBStateBackend

doesn't include the implementation of new AsyncKeyedStateBackend and Async State API which will be resolved in other tickets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 01:51:33 UTC 2024,,,,,,,,,,"0|z1ohrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 01:51;masteryhx;merged 629c3571...19577b0d into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce New KeyedStateBackend related Async interfaces,FLINK-35046,13574944,13574084,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,08/Apr/24 06:41,17/Apr/24 02:31,04/Jun/24 20:40,17/Apr/24 02:31,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,"Since we have introduced new State API, the async version of some classes should be introduced to support it, e.g. AsyncKeyedStateBackend, new State Descriptor.",,,,,,,,,,,,,FLINK-35048,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 02:31:49 UTC 2024,,,,,,,,,,"0|z1ohrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 02:31;masteryhx;merged 691cc671 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ForStFileSystem to support reading and writing with ByteBuffer,FLINK-35045,13574943,13574085,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,08/Apr/24 06:32,19/Apr/24 13:19,04/Jun/24 20:40,18/Apr/24 13:12,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,"As described in FLIP-427, ForStFileSystem is introduced to support reading and writing with ByteBuffer, which will bridge between ForSt FileSystem and Flink FileSystem.",,,,,,,,,,,,,,,,,,,,,,,FLINK-35175,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 13:12:13 UTC 2024,,,,,,,,,,"0|z1ohr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/24 13:12;masteryhx;merged 369bbb8f...a312a3bd into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce statebackend-forst module,FLINK-35044,13574942,13574085,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,08/Apr/24 06:29,10/Apr/24 02:16,04/Jun/24 20:40,10/Apr/24 02:16,,,,,,,,,1.20.0,,,Runtime / State Backends,,,,0,pull-request-available,,"A simple work to introduce a new module for ForStStateBackend, then several subsequent PR e.g. ForStFileSystem, Implementation of Async API, ForStStateBackend could be commited/reviewed in parallel based on it.
See FLIP-427 for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 10 02:16:00 UTC 2024,,,,,,,,,,"0|z1ohqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 02:16;masteryhx;merged ee81c79f into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release beta version of ForSt,FLINK-35043,13574941,13574085,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,08/Apr/24 06:27,10/Apr/24 02:15,04/Jun/24 20:40,10/Apr/24 02:15,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 10 02:15:04 UTC 2024,,,,,,,,,,"0|z1ohqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 02:15;masteryhx;The beta version could be found in: https://mvnrepository.com/artifact/com.ververica/forstjni/0.1.0-beta;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming File Sink s3 end-to-end test failed as TM lost,FLINK-35042,13574937,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,08/Apr/24 05:43,08/Apr/24 05:45,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Build System / CI,,,,0,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58782&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=14344

FAIL 'Streaming File Sink s3 end-to-end test' failed after 15 minutes and 20 seconds! Test exited with exit code 1

I have checked the JM log, it seems that a taskmanager is no longer reachable:

{code:java}
2024-04-08T01:12:04.3922210Z Apr 08 01:12:04 2024-04-08 00:58:15,517 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (4/4) (14b44f534745ffb2f1ef03fca34f7f0d_0a448493b4782967b150582570326227_3_0) switched from RUNNING to FAILED on localhost:44987-47f5af @ localhost (dataPort=34489).
2024-04-08T01:12:04.3924522Z Apr 08 01:12:04 org.apache.flink.runtime.jobmaster.JobMasterException: TaskManager with id localhost:44987-47f5af is no longer reachable.
2024-04-08T01:12:04.3925421Z Apr 08 01:12:04 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyTargetUnreachable(JobMaster.java:1511) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3926185Z Apr 08 01:12:04 	at org.apache.flink.runtime.heartbeat.DefaultHeartbeatMonitor.reportHeartbeatRpcFailure(DefaultHeartbeatMonitor.java:126) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3926925Z Apr 08 01:12:04 	at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.runIfHeartbeatMonitorExists(HeartbeatManagerImpl.java:275) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3929898Z Apr 08 01:12:04 	at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.reportHeartbeatTargetUnreachable(HeartbeatManagerImpl.java:267) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3930692Z Apr 08 01:12:04 	at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.handleHeartbeatRpcFailure(HeartbeatManagerImpl.java:262) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3931442Z Apr 08 01:12:04 	at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.lambda$handleHeartbeatRpc$0(HeartbeatManagerImpl.java:248) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3931917Z Apr 08 01:12:04 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_402]
2024-04-08T01:12:04.3934759Z Apr 08 01:12:04 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_402]
2024-04-08T01:12:04.3935252Z Apr 08 01:12:04 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_402]
2024-04-08T01:12:04.3935989Z Apr 08 01:12:04 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:460) ~[flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3936731Z Apr 08 01:12:04 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3938103Z Apr 08 01:12:04 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:460) ~[flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3942549Z Apr 08 01:12:04 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:225) ~[flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3945371Z Apr 08 01:12:04 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:88) ~[flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3946244Z Apr 08 01:12:04 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:174) ~[flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3946960Z Apr 08 01:12:04 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3947664Z Apr 08 01:12:04 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3950764Z Apr 08 01:12:04 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3952816Z Apr 08 01:12:04 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3953526Z Apr 08 01:12:04 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3954214Z Apr 08 01:12:04 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3954988Z Apr 08 01:12:04 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3964997Z Apr 08 01:12:04 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3965715Z Apr 08 01:12:04 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3966385Z Apr 08 01:12:04 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3967066Z Apr 08 01:12:04 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3969968Z Apr 08 01:12:04 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3970656Z Apr 08 01:12:04 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3971333Z Apr 08 01:12:04 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3971987Z Apr 08 01:12:04 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3974909Z Apr 08 01:12:04 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) [flink-rpc-akka9681a48a-ca1a-45b0-bb71-4bdb5d2aed93.jar:1.20-SNAPSHOT]
2024-04-08T01:12:04.3975317Z Apr 08 01:12:04 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_402]
2024-04-08T01:12:04.3975691Z Apr 08 01:12:04 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_402]
2024-04-08T01:12:04.3976056Z Apr 08 01:12:04 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_402]
2024-04-08T01:12:04.3976415Z Apr 08 01:12:04 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_402]
{code}

But I didn't found any valuable message from the corresponding TM log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-08 05:43:30.0,,,,,,,,,,"0|z1ohps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncrementalRemoteKeyedStateHandleTest.testSharedStateReRegistration failed,FLINK-35041,13574936,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,Weijie Guo,Weijie Guo,08/Apr/24 05:31,13/May/24 14:02,04/Jun/24 20:40,11/May/24 12:38,1.20.0,,,,,,,,1.20.0,,,Build System / CI,,,,0,pull-request-available,,"{code:java}
Apr 08 03:22:45 03:22:45.450 [ERROR] org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandleTest.testSharedStateReRegistration -- Time elapsed: 0.034 s <<< FAILURE!
Apr 08 03:22:45 org.opentest4j.AssertionFailedError: 
Apr 08 03:22:45 
Apr 08 03:22:45 expected: false
Apr 08 03:22:45  but was: true
Apr 08 03:22:45 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Apr 08 03:22:45 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Apr 08 03:22:45 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(K.java:45)
Apr 08 03:22:45 	at org.apache.flink.runtime.state.DiscardRecordedStateObject.verifyDiscard(DiscardRecordedStateObject.java:34)
Apr 08 03:22:45 	at org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandleTest.testSharedStateReRegistration(IncrementalRemoteKeyedStateHandleTest.java:211)
Apr 08 03:22:45 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 08 03:22:45 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Apr 08 03:22:45 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Apr 08 03:22:45 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Apr 08 03:22:45 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Apr 08 03:22:45 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)

{code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58782&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9238]
 ",,,,,,,,,,,,,,,,,,,,,FLINK-32079,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 14:02:53 UTC 2024,,,,,,,,,,"0|z1ohpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 14:52;rskraba;jdk8 https://github.com/apache/flink/actions/runs/8593664911/job/23545710057#step:10:8958
jdk8 https://github.com/apache/flink/actions/runs/8597596125/job/23557044226#step:10:8780;;;","09/Apr/24 03:23;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58794&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9323;;;","10/Apr/24 10:12;rskraba;1.20 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58842&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9221
1.20 jdk17 https://github.com/apache/flink/actions/runs/8609297774/job/23593490296#step:10:8926
1.20 jdk8 https://github.com/apache/flink/actions/runs/8624933967/job/23640952596#step:10:8596
1.20 jdk21 https://github.com/apache/flink/actions/runs/8624933967/job/23640965630#step:10:7863;;;","11/Apr/24 02:54;Weijie Guo;Hi [~Feifan Wang], I saw that you modified the test code for the last time, any thoughts about this?;;;","11/Apr/24 03:15;fanrui;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58863&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9523;;;","12/Apr/24 12:13;rskraba;1.20 Java 17: Test (module: core) https://github.com/apache/flink/actions/runs/8655935935/job/23735919404#step:10:8255;;;","12/Apr/24 12:29;rskraba;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58891&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8875;;;","12/Apr/24 13:16;rskraba;Ok, I *can't* reproduce this by running {{IncrementalRemoteKeyedStateHandleTest}} in the mode ""Repeat until failure"" in IntelliJ, but I *can* reliably reproduce it (within about a minute) by running the entire {{org.apache.flink.runtime.state}} package in the mode ""Repeat until stopped"".  I'm not entirely sure how to interpret this: is another unit test interfering sufficiently to cause this one to be flaky?;;;","15/Apr/24 13:43;rskraba;* 1.20 Test (module:core) https://github.com/apache/flink/actions/runs/8684468749/job/23812888125#step:10:8283
* 1.20 Hadoop 3.1.1: Test (module: core) https://github.com/apache/flink/actions/runs/8682562266/job/23807645978#step:10:8187
* 1.20 Java 8: Test (module: core) https://github.com/apache/flink/actions/runs/8677831281/job/23794122267#step:10:9108
* 1.20 Java 21: Test (module: core) https://github.com/apache/flink/actions/runs/8677831281/job/23794373235#step:10:8456


;;;","16/Apr/24 10:06;rskraba;* 1.20 Java 11: Test (module: core) https://github.com/apache/flink/actions/runs/8698882161/job/23856834635#step:10:9180
* 1.20 Default (Java 8): Test (module: core) https://github.com/apache/flink/actions/runs/8689117008/job/23826646737#step:10:7553;;;","16/Apr/24 10:10;rskraba;1.20 test_ci core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58947&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8901;;;","17/Apr/24 14:49;rskraba;1.20 Java 8: Test (module: core) https://github.com/apache/flink/actions/runs/8715237422/job/23907053858#step:10:9074;;;","17/Apr/24 14:59;rskraba;1.20 test_ci_core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58969&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8933
1.20 test_ci_core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58971&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8897;;;","18/Apr/24 09:05;rskraba;* 1.20 Java 8 / Test (module: core) [https://github.com/apache/flink/actions/runs/8731358306/job/23956957736#step:10:8376]
 * 1.20 Java 17 / Test (module: core) [https://github.com/apache/flink/actions/runs/8731358306/job/23956870590#step:10:8521]

 ;;;","22/Apr/24 01:44;Weijie Guo;test_cron_jdk21 core

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59037&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=8911;;;","22/Apr/24 11:55;rskraba;1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8766910657/job/24059706708#step:10:8640
1.20 Java 21 / Test (module: core) https://github.com/apache/flink/actions/runs/8769422914/job/24065044031#step:10:9006
1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/8769422914/job/24065049005#step:10:8464
;;;","24/Apr/24 07:47;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59104&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9567;;;","26/Apr/24 08:46;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59173&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9556

;;;","30/Apr/24 08:24;rskraba;1.20 test_ci_core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59281&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8885;;;","02/May/24 16:33;rskraba;1.20 test_cron_hadoop313 core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59303&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9001;;;","03/May/24 15:54;rskraba;Going through some of the older GitHub actions from the last week, there are a lot of these:
 
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/8917610620/job/24491172511#step:10:8154
* 1.20 Java 21 / Test (module: core) https://github.com/apache/flink/actions/runs/8917610620/job/24491154789#step:10:8873
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/8888221960/job/24404966761#step:10:7787
* 1.20 AdaptiveScheduler / Test (module: core) https://github.com/apache/flink/actions/runs/8888221960/job/24404939797#step:10:8361
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8874021289/job/24361049250#step:10:8308
* 1.20 Java 17 / Test (module: core) https://github.com/apache/flink/actions/runs/8872328953/job/24356752585#step:10:8911
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/8864296312/job/24339779126#step:10:9083
* 1.20 Java 21 / Test (module: core) https://github.com/apache/flink/actions/runs/8856547891/job/24323115199#step:10:8933
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/8842083488/job/24280420760#step:10:8265
* 1.20 Java 17 / Test (module: core) https://github.com/apache/flink/actions/runs/8825970497/job/24231219571#step:10:9087
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8825652254/job/24230389260#step:10:9141
* 1.20 Java 21 / Test (module: core) https://github.com/apache/flink/actions/runs/8809949034/job/24182328046#step:10:8078
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8800044378/job/24153034222#step:10:8261
* 1.20 Java 17 / Test (module: core) https://github.com/apache/flink/actions/runs/8793750647/job/24132431375#step:10:7754
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8784906766/job/24104618074#step:10:8444
;;;","06/May/24 08:09;rskraba;* 1.20 Java 17 / Test (module: core) https://github.com/apache/flink/commit/beb0b167bdcf95f27be87a214a69a174fd49d256/checks/24613040802/logs
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/8954955141/job/24595323637#step:10:7795
* 1.20 Hadoop 3.1.3 / Test (module: core) https://github.com/apache/flink/actions/runs/8954955141/job/24595381176#step:10:8858
;;;","07/May/24 08:03;rskraba;* 1.20 test_cron_jdk21 core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59356&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=8870;;;","07/May/24 08:58;fanrui;I have asked [~Feifan Wang] offline, he will check&fix it this week with high priority, thanks Feifan in advance.;;;","09/May/24 07:43;fanrui;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59410&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9464]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59408&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9463;;;","10/May/24 09:41;rskraba;* 1.20 Hadoop 3.1.3 / Test (module: core) https://github.com/apache/flink/actions/runs/9026237714/job/24803537384#step:10:8419
* 1.20 Java 21 / Test (module: core) https://github.com/apache/flink/actions/runs/9011311875/job/24758973855#step:10:8334
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8999811164/job/24723153060#step:10:8487
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8997755665/job/24716975457#step:10:9046
* 1.20 Java 11 / Test (module: core) https://github.com/apache/flink/actions/runs/8995101420/job/24709819637#step:10:8738
* 1.20 Java 21 / Test (module: core) https://github.com/apache/flink/actions/runs/8995101420/job/24709801069#step:10:8940
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8985327313/job/24679590248#step:10:8686
;;;","11/May/24 12:36;fanrui;Merged to master (1.20.0) via:  86c8304d735581518ce666be4896b7d5e48a1e42;;;","11/May/24 12:38;fanrui;IIUC, this issue has been fixed in master  branch, so I will close it first.

If it still happens, feel free to re-open  it.;;;","13/May/24 14:02;rskraba;Thanks so much!  I've verified that I can no longer reproduce this error by repeatedly running the entire package of tests.;;;"
The performance of serializerHeavyString regresses since April 3,FLINK-35040,13574933,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,fanrui,fanrui,fanrui,08/Apr/24 02:52,27/May/24 08:22,04/Jun/24 20:40,27/May/24 08:22,1.20.0,,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,"The performance of serializerHeavyString regresses since April 3, and had not yet recovered on April 8th.

It seems Java 11 regresses, and Java 8 and Java 17 are fine.

http://flink-speed.xyz/timeline/#/?exe=1,6,12&ben=serializerHeavyString&extr=on&quarts=on&equid=off&env=3&revs=200


 !screenshot-1.png! ",,,,,,,,,,,,,,,,,,,,,FLINK-34955,,,,,"08/Apr/24 02:51;fanrui;image-2024-04-08-10-51-07-403.png;https://issues.apache.org/jira/secure/attachment/13067930/image-2024-04-08-10-51-07-403.png","11/Apr/24 04:53;fanrui;image-2024-04-11-12-53-53-353.png;https://issues.apache.org/jira/secure/attachment/13068132/image-2024-04-11-12-53-53-353.png","09/May/24 07:43;fanrui;result1.html;https://issues.apache.org/jira/secure/attachment/13068760/result1.html","09/May/24 07:43;fanrui;result2.html;https://issues.apache.org/jira/secure/attachment/13068761/result2.html","09/May/24 07:43;fanrui;result3.html;https://issues.apache.org/jira/secure/attachment/13068762/result3.html","08/Apr/24 03:32;fanrui;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13067931/screenshot-1.png",,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 08:22:57 UTC 2024,,,,,,,,,,"0|z1ohow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 09:15;fanrui;After running a series of benchmarks, I found FLINK-34955 causes the performance regression.

The commit of FLINK-34955 is : [163b9cca6d2ccac0ff89dd985e3232667ddfb14f|https://github.com/apache/flink/commit/163b9cca6d2ccac0ff89dd985e3232667ddfb14f] , and its previous commit is [8a18b119c958568d58f346c0e0868784a0ab9653|https://github.com/apache/flink/commit/8a18b119c958568d58f346c0e0868784a0ab9653].

I ran serializerHeavyString in the flink community benchmark jenkins for these 2 commits, the following is the result:
{code:java}
163b9cca6d2ccac0ff89dd985e3232667ddfb14f:
The First round:  129.695712 ops/ms , link: http://jenkins.flink-speed.xyz/job/flink-benchmark-request/65/
The Second round: 134.800001 ops/ms , link: http://jenkins.flink-speed.xyz/job/flink-benchmark-request/67/
The third round:  135.937003 ops/ms , link: http://jenkins.flink-speed.xyz/job/flink-benchmark-request/70/
 ""org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerHeavyString"",""thrpt"",1,30,129.695712,4.497077,""ops/ms""
""org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerHeavyString"",""thrpt"",1,30,134.800001,0.685663,""ops/ms"" ""org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerHeavyString"",""thrpt"",1,30,135.937003,1.208447,""ops/ms""


8a18b119c958568d58f346c0e0868784a0ab9653:
The First round:  156.328850 ops/ms , link: http://jenkins.flink-speed.xyz/job/flink-benchmark-request/66/
The Second round: 154.229828 ops/ms , link: http://jenkins.flink-speed.xyz/job/flink-benchmark-request/68/
The third round:  156.893234 ops/ms , link: http://jenkins.flink-speed.xyz/job/flink-benchmark-request/69/
 
""org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerHeavyString"",""thrpt"",1,30,156.328850,1.880390,""ops/ms""
""org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerHeavyString"",""thrpt"",1,30,154.229828,3.453092,""ops/ms""
""org.apache.flink.benchmark.SerializationFrameworkMiniBenchmarks.serializerHeavyString"",""thrpt"",1,30,156.893234,1.308621,""ops/ms"" 

{code}
The performance is regressed with commit 163b9cca6d2ccac0ff89dd985e3232667ddfb14f, and the performance is fine without it. So I think FLINK-34955 causes the performance regression.

The jdk8 and jdk17 are fine, and my test server with jdk11 doesn't have any regression. So I'm not sure if it's an issue.;;;","11/Apr/24 02:57;Weijie Guo;> The performance is regressed with commit 163b9cca6d2ccac0ff89dd985e3232667ddfb14f, and the performance is fine without it. 

Can this result be consistently reproduced?;;;","11/Apr/24 02:59;fanrui;Also, I revert [163b9cca6d2ccac0ff89dd985e3232667ddfb14f|https://github.com/apache/flink/commit/163b9cca6d2ccac0ff89dd985e3232667ddfb14f] in the latest master branch, and the benchmark result is recovered to 155.761059 ops/ms.

See: http://jenkins.flink-speed.xyz/job/flink-benchmark-request/71/;;;","11/Apr/24 04:20;Weijie Guo;> http://jenkins.flink-speed.xyz/job/flink-benchmark-request/71/

This link require username and password. Would you mind sharing the screenshot  :)
;;;","11/Apr/24 04:56;fanrui;{quote}Can this result be consistently reproduced?
{quote}
Yes, it can be reproduced in flink benchmark server.
{quote}This link require username and password. Would you mind sharing the screenshot :)
{quote}
Following is the result, and the corresponding code branch is : [https://github.com/1996fanrui/flink/commits/revert-163b9cca6d2ccac0ff89dd985e3232667ddfb14f/]

The branch is based on flink master branch, and revert the 163b9cca6d2ccac0ff89dd985e3232667ddfb14f.

!image-2024-04-11-12-53-53-353.png!;;;","11/Apr/24 04:57;fanrui;Hi [~slfan1989] , would you mind helping take a look as well? thanks in advanced.;;;","11/Apr/24 05:16;slfan1989;[~fanrui] I received your message, and I apologize for any inconvenience caused. The reason for upgrading commons-compress to version 1.26.0 is indeed due to the known CVE issues in commons-compress 1.24.0.

We can refer to the following link: [https://mvnrepository.com/artifact/org.apache.commons/commons-compress/1.24.0]

Direct vulnerabilities: CVE-2024-26308 CVE-2024-25710

Addressing the CVE issue is indeed necessary to prevent vulnerabilities in our system. I suggest we consider upgrading to version 1.26.1 to address this problem. Reverting to 1.24.0 might only serve as a temporary solution.;;;","11/Apr/24 05:18;slfan1989;[~fanrui] [~Weijie Guo] How is serializerHeavyString tested? I haven't seen any testing scripts here. Could you provide some hints?

I found the project at [https://github.com/apache/flink-benchmarks]. I will take a look at the relevant tests as soon as possible.;;;","11/Apr/24 10:10;fanrui;Hi [~slfan1989] , thanks for your quick feedback!

FLINK-34955 wants to fix CVE issues of {{{}common-compress{}}}, but it upgrades the {{commons-io}} together. I try to revert {{commons-io}} to 2.11.0, and the performance is recovered.

My question is why do you upgrade the commons-io in FLINK-34955, and I didn't see any vulnerabilities for commons-io. Could I revert {{commons-io}} to 2.11.0?

 

Note: I revert {{commons-io}} to 2.11.0, and upgrade commons-compress to 2.16.1, then run the benchmark once, the performance is recovered.

I try to only revert {{commons-io}} to 2.11.0(See the PR), trigger benchmark twice, and see the performance result later. (The benchmark server is busy, so the result may be finished tomorrow.);;;","11/Apr/24 10:22;slfan1989;[~fanrui] During the compilation process, we found that commons-compress requires a higher version of commons-io, otherwise there will be a class not found exception.;;;","13/Apr/24 17:35;rmetzger;Do we know what causes the performance degradation with commons-io? Maybe there's a ticket in the commons-io project that helps us understand what is going on? If not, it might make sense to report to commons-io, so that they are aware that they have a performance degradation.;;;","15/Apr/24 02:48;fanrui;Thanks [~slfan1989] and [~rmetzger] for the comment!

I didn't find any related issue from commons-io JIRA[1]. Also, I ran benchmark on my Mac with jdk11, and try to analyze why code path causes this regression. I use async-profiler wall mode to analyze the benchmark, and didn't find any code from commons.io package.

Do you have any idea to troubleshooting?

[1]https://issues.apache.org/jira/browse/IO-855?jql=project%20%3D%20IO%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened);;;","09/May/24 07:51;fanrui;I attached 3 flamegraphs here, these flamegraphs are generated on flink benchmark server, many thanks to [~Zakelly] for the help.

From these flamegraphs, we cannot find any org.apache.commons.io code is called (I search commons in flamegraph). But the performance regresses after commons.io is upgraded. It's a little strange.

cc [~Weijie Guo] [~rmetzger] [~uce] ;;;","27/May/24 08:22;fanrui;Close this Jira first, you can get more details from https://lists.apache.org/thread/moo2b67qk77ysoofs9j1ojzjk0rhrr9h;;;",,,,,,,,,,,,,,,
Create Profiling JobManager/TaskManager Instance failed,FLINK-35039,13574932,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,wczhu,wczhu,wczhu,08/Apr/24 02:31,30/Apr/24 05:51,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,"I'm test the ""async-profiler"" feature in version 1.19, but when I submit a task in yarn per-job mode, I get an error  when I click Create Profiling Instance on the flink Web UI page.
!image-2024-04-08-10-21-31-066.png!

!image-2024-04-08-10-21-48-417.png!

The error message obviously means that the yarn proxy server does not support *POST* calls. I checked the code of _*WebAppProxyServlet.java*_ and found that the *POST* method is indeed not supported, so I changed it to *PUT* method and the call was successful.

!image-2024-04-08-10-30-16-683.png!

 ","Hadoop 3.2.2
Flink 1.19",,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/24 02:21;wczhu;image-2024-04-08-10-21-31-066.png;https://issues.apache.org/jira/secure/attachment/13067929/image-2024-04-08-10-21-31-066.png","08/Apr/24 02:21;wczhu;image-2024-04-08-10-21-48-417.png;https://issues.apache.org/jira/secure/attachment/13067928/image-2024-04-08-10-21-48-417.png","08/Apr/24 02:30;wczhu;image-2024-04-08-10-30-16-683.png;https://issues.apache.org/jira/secure/attachment/13067927/image-2024-04-08-10-30-16-683.png","30/Apr/24 03:12;Yu Chen;image-2024-04-30-11-12-34-734.png;https://issues.apache.org/jira/secure/attachment/13068547/image-2024-04-30-11-12-34-734.png","30/Apr/24 03:14;Yu Chen;image-2024-04-30-11-14-44-335.png;https://issues.apache.org/jira/secure/attachment/13068548/image-2024-04-30-11-14-44-335.png",,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 03:31:18 UTC 2024,,,,,,,,,,"0|z1ohoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 02:38;wczhu;[~Yu Chen]，[~yunta]  Please have a look, If my modification plan is fine, please give me Assign.;;;","19/Apr/24 06:00;yunta;[~wczhu] Thanks for finding this problem in YARN environment. I just feel curious why YARN does not support POST?;;;","19/Apr/24 08:19;wczhu;Hi [~yunta] , I don't know why YARN doesn't support HTTP POST,I guess the GET and PUT methods are sufficient, so the POST method is not overridden;;;","19/Apr/24 10:48;yunta;[~wczhu] Already assigned to you.;;;","30/Apr/24 03:31;Yu Chen;Hi [~wczhu] , sorry for the late response.

It does surprise me that YARN doesn't support POST. But I have a confusing point: POST requests are already used in many places in the Flink interfaces, such as Stop-with-savepoint. Are these interfaces currently not accessible on YARN?

Moreover, according to the principles of RESTful Interface [1], the biggest difference between POST and PUT is that PUT requests are idempotent, the same request is submitted N times and the result is still the same, but the Profiling interface should be a new request each time and the result will be added to the server.

Therefore, POST may be more in line with the semantics of this interface.

So I wonder if it is appropriate to do this compatibility in Flink. WDYT [~yunta] ?


 [1] https://restfulapi.net/rest-put-vs-post/

!image-2024-04-30-11-12-34-734.png|width=414,height=496!;;;",,,,,,,,,,,,,,,,,,,,,,,,
Bump test dependency org.yaml:snakeyaml to 2.2 for Flink Kafka connector,FLINK-35038,13574920,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,uce,uce,uce,07/Apr/24 18:33,24/May/24 09:17,04/Jun/24 20:40,11/Apr/24 11:49,,,,,,,,,kafka-3.2.0,kafka-4.0.0,,Connectors / Kafka,,,,0,pull-request-available,,"Usage of SnakeYAML via {{flink-shaded}} was replaced by an explicit test scope dependency on {{org.yaml:snakeyaml:1.31}} with FLINK-34193.

This outdated version of SnakeYAML triggers security warnings. These should not be an actual issue given the test scope, but we should consider bumping the version for security hygiene purposes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 11:49:16 UTC 2024,,,,,,,,,,"0|z1ohm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 11:49;martijnvisser;Fixed in apache/flink-connector-kafka:

main: 369e7be46a70fd50d68746498aed82105741e7d6
v3.1: ad798fc5387ba3582f92516697d60d0f523e86cb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize uniqueKeys and upsertKeys inference of windows with ROW_NUMBER,FLINK-35037,13574911,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,nilerzhou,nilerzhou,nilerzhou,07/Apr/24 12:21,21/May/24 04:02,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,"In current Implementation, relNodes with Window type will only deliver upsert/unique keys of their inputs.

However windows with ROW_NUMBER can also produce upsert/unique keys.

For example:
{code:java}
select id, name, score, age, class,
    row_number() over(partition by class order by name) as rn,
    rank() over (partition by class order by score) as rk,
    dense_rank() over (partition by class order by score) as drk,
    avg(score) over (partition by class order by score) as avg_score,
    max(score) over (partition by age) as max_score,
    count(id) over (partition by age) as cnt
from student {code}
(class, rn) is a valid upsert/unique keys candidate. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 07:27:33 UTC 2024,,,,,,,,,,"0|z1ohk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 04:32;nilerzhou;Hi [~libenchao] , could you please assign this task to me? I've already prepared a PR.;;;","09/Apr/24 06:00;libenchao;[~nilerzhou]  Assigned to you :);;;","15/Apr/24 07:27;nilerzhou;[~libenchao]  The PR has been submitted. Please help reviewing it when you have time. Thx.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC Job cancel with savepoint failed,FLINK-35036,13574907,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fly365,fly365,07/Apr/24 09:35,08/Apr/24 10:27,04/Jun/24 20:40,,,,,,,,,,,,,Flink CDC,,,,0,,,"English Description: With the Flink CDC job, I want oracle data to doris, in the  snapshot，canel the Flink CDC Job with savepoint,the job cancel failed.

Using Flink CDC, synchronize the data tables of Oracle 19C to Doris. During the initialization snapshot phase, some data was synchronized but has not yet reached the incremental phase. At this time, cancel the CDC task and save Flink Savepoint, but the task cancellation fails; After the task enters the incremental phase, it is possible to cancel the task and save the savepoint. Why did the savepoint fail during the stock data synchronization phase?

中文描述： 使用Flink CDC,将Oracle 19C的数据表同步到Doris中，在初始化快照阶段，同步了一部分数据但还没有到增量阶段，此时取消CDC任务并保存Flink Savepoint,取消任务失败；而在任务进入增量阶段后，取消任务并保存savepoint是可以的，请问存量数据同步阶段，为何savepoint失败？

!image-2024-04-07-17-35-23-136.png!

 ","Flink 1.15.2

Flink CDC 2.4.2

Oracle 19C

Doris 2.0.3",,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/24 09:35;fly365;image-2024-04-07-17-35-23-136.png;https://issues.apache.org/jira/secure/attachment/13067922/image-2024-04-07-17-35-23-136.png","08/Apr/24 09:20;fly365;image-2024-04-08-17-20-37-320.png;https://issues.apache.org/jira/secure/attachment/13067934/image-2024-04-08-17-20-37-320.png","08/Apr/24 09:21;fly365;image-2024-04-08-17-21-25-111.png;https://issues.apache.org/jira/secure/attachment/13067935/image-2024-04-08-17-21-25-111.png","08/Apr/24 09:22;fly365;image-2024-04-08-17-21-58-127.png;https://issues.apache.org/jira/secure/attachment/13067936/image-2024-04-08-17-21-58-127.png","08/Apr/24 09:23;fly365;image-2024-04-08-17-23-29-139.png;https://issues.apache.org/jira/secure/attachment/13067939/image-2024-04-08-17-23-29-139.png","08/Apr/24 09:24;fly365;jobmanager_log.txt;https://issues.apache.org/jira/secure/attachment/13067937/jobmanager_log.txt","08/Apr/24 09:24;fly365;taskmanager_20.13.33.33_35939-bfbb67_log;https://issues.apache.org/jira/secure/attachment/13067938/taskmanager_20.13.33.33_35939-bfbb67_log",,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 09:22:11 UTC 2024,,,,,,,,,,"0|z1ohjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 02:08;bgeng777;Hi [~fly365], according to the attached screenshot, the failure is caused by a timeout in flink client side. 
IIUC, in the full volume phase of a flink cdc job, it needs to process lots of data and typically due to the back pressure, the state may be much larger than the incremental phase(you can check the state size in flink's web ui).
As a result, it would take longer time for the flink to complete the savepoint. The client's [default timeout is 60s|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#client-timeout], so maybe you can increase the value to see if the savepoint can succeed.
;;;","08/Apr/24 02:28;renqs;[~fly365] Thanks for reporting the issue! I agree with [~bgeng777]'s idea and maybe you can have a try. 

And could you rewrite the description in English considering we have contributors around the world? Thanks;;;","08/Apr/24 09:22;fly365;[~bgeng777] [~renqs] ，Thank you for your help！

Eenglish Description： I adjusted the timeout of the Flink client to 180s; There are 5 million data records in the Oracle database table. When the data records were captured to 833120, I used a command to cancel the task and save the savepoint. At this time, the CDC client timed out for a long time, and the task cancellation failed. At this time, the CDC task is still in the full data capture stage. During the initial full data capture, the Oracle CDC task defaults to being done through rowId. The checkpoint of the task is saved normally, but when canceling and saving the savepoint, why can't the task status be stored normally?
After the initial full data capture is completed, the task enters the incremental data capture phase. At this point, the task can be cancelled and saved normally. Is this a Flink issue or a Flink CDC related issue? Request help to answer, thank you!

 
中文描述： 我将flink client 的超时时间调整成了180s；Oracle数据库表有500万条数据，当数据记录抓取到833,120条时，我使用命令取消任务并保存savepoint，此时CDC客户端长时间直到超时，任务取消失败，此时CDC任务仍在全量数据抓取阶段，在初始全量数据抓取时，Oracle CDC任务默认是通过rowId进行做的chunk，任务的checkpoint保存正常，但是取消并存储savepoint时，为何不能正常存储任务状态呢？
当初始全量数据抓取完成后，任务进入到增量数据抓取阶段，此时任务能够正常取消并保存savepoint，这种情况是Flink 问题还是 Flink CDC 相关的问题呢？请求帮助解答，谢谢！
!image-2024-04-08-17-20-37-320.png!
!image-2024-04-08-17-21-25-111.png!
!image-2024-04-08-17-21-58-127.png!
 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce job pause time when cluster resources are expanded in adaptive mode,FLINK-35035,13574898,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,heigebupahei,heigebupahei,07/Apr/24 07:31,12/Apr/24 09:10,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Runtime / Task,,,,0,,,"When 'jobmanager.scheduler = adaptive' , job graph changes triggered by cluster expansion will cause long-term task stagnation. We should reduce this impact.
As an example:
I have jobgraph for : [v1 (maxp=10 minp = 1)] -> [v2 (maxp=10, minp=1)]
When my cluster has 5 slots, the job will be executed as [v1 p5]->[v2 p5]
When I add slots the task will trigger jobgraph changes，by
org.apache.flink.runtime.scheduler.adaptive.ResourceListener#onNewResourcesAvailable，
However, the five new slots I added were not discovered at the same time (for convenience, I assume that a taskmanager has one slot), because no matter what environment we add, we cannot guarantee that the new slots will be added at once, so this will cause onNewResourcesAvailable triggers repeatedly
，If each new slot action has a certain interval, then the jobgraph will continue to change during this period. What I hope is that there will be a stable time to configure the cluster resources  and then go to it after the number of cluster slots has been stable for a certain period of time. Trigger jobgraph changes to avoid this situation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 12 09:09:54 UTC 2024,,,,,,,,,,"0|z1ohhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 02:24;bgeng777;I am not very familiar with adaptive scheduler, maybe others can share more insights. I just want to ask a question to make sure we are on the same page.
Do you mean that instead of triggering onNewResourcesAvailable repeatedly once a new slot is found(in your example, 5 new slots so 5 times of rescheduling), you are expecting the JM can discover the 5 new slots at the same time after a configurable period of time and only trigger 1 time of rescheduling?;;;","09/Apr/24 02:19;heigebupahei;[~bgeng777] 
Thank you for your comment. As you understand, I hope that if a new tm occurs during the running of the task, the task should not restart immediately, but wait for a period of time.
Below I will explain this process in detail:
Taking the example I gave at the beginning, assume that the current task is [v1 (maxp=10 minp = 1)]  > [v2 (maxp=10, minp=1)], but the total number of slots in the cluster is 5, so the task is running [v1 p5]>[v2 p5] runs. I have now added 5 slots. I hope the task will run with [v1 p10]->[v2 p10]. When the number of cluster slots becomes 6, the task will immediately trigger a restart. At this time, according to the jobmanager.adaptive-scheduler.resource-stabilization-timeout parameter, the task will wait for a period of time during the restart phase for resources. If the slot does not reach the target slot number of 10 during this period, the task will run with a lower degree of parallelism. , but my slots will be added to 10 over a period of time, so this will trigger another expansion and restart process. In this process, I have one more restart process and one more resource waiting process. Why don't we start before the first restart? Should I wait for a period of time or determine that the number of slots meets my p=10 before triggering the restart (scale up) action?

 
 [~echauchot]  hi, can you help me look into this issue, it seems similar to FLINK-21883.
 
Thanks
 
 ;;;","09/Apr/24 14:29;echauchot;FLINK-21883 cooldown period was mainly designed to avoid too frequent rescales. Here is how it works when new slots are available:
 - Flink should rescale immediately only if last rescale was done more than scaling-interval.min (default 30s) ago.
 - Otherwise it should schedule a rescale at (now + scaling-interval.min) point in time.
The rescale is done like this:
 - if minimum scaling requirements are met (AdaptiveScheduler#shouldRescale default to minimum 1 slot added), the job is restarted with new parallelism
 - if minimum scaling requirements are not met
 -- if last rescale was done more than scaling-interval.max ago (default disabled), a rescale is forced.
 -- otherwise, schedule a forced rescale in scaling-interval.max

So in your case of slots arriving partially during the resource stabilization timeout leading to a rescale with only a portion of the ideal number of slots, what I see is that you can either:
1. increase the stabilization timeout hopping you'll get all the slots during that time
2. set min-parallelism-increase to 5 instead of default 1 and set scaling-interval.max. That way the first slots additions will not trigger a rescale but the rescale will be issued only when the 5th slot arrives and you will still get a security force rescale scheduled no matter what (as long as the parallelism has changed) after scaling-interval.max;;;","10/Apr/24 02:07;heigebupahei;[~echauchot] 
Thank you for your reply, but I have some questions:

jobmanager.adaptive-scheduler.min-parallelism-increase is a parameter on jobmanager, so I cannot update this value after the cluster is started. Assuming it is set to 5, this time it causes some problems:

The original task is [v1 (maxp=10 minp = 1)] -> [v2 (maxp=10, minp=1)]. If I call restapi, the parallelism is overwritten to the new [v1 (maxp=12 minp = 1)] -> [v2 (maxp=12, minp=1)], then I added slots to the cluster, but obviously I only need to add 2 slots to meet the requirements, but because min-parallelism-increase was not reached, So this will not cause the task to trigger expansion. It needs to wait until scaling-interval.max is reached before triggering (scaling-interval.max needs to be set first). I think in this case, should I add a configuration item to support its triggering?

 

Maybe can add a switch similar to jobmanager.adaptive-scheduler.min-parallelism-increase. When the resource changes, it will be judged whether the current resource fully meets the parallelism requirements of the job. If it is satisfied, rescheduling will be triggered directly. If it is not satisfied, it will be rescheduled in after scaling-interval.max . WDYT? [~echauchot] 

Looking forward to your reply!

 ;;;","10/Apr/24 10:17;echauchot;> It needs to wait until scaling-interval.max is reached before triggering

Yes or wait until 5 slots are there. 

>When the resource changes, it will be judged whether the current resource fully meets the parallelism requirements of the job. If it is satisfied, rescheduling will be triggered directly. If it is not satisfied, it will be rescheduled in after scaling-interval.max

It is already how things work when you set min-parallelism-increase and scaling-interval.max;;;","12/Apr/24 02:43;heigebupahei;[~echauchot] 

Thank you for your reply.
I think you are looking at this scene from the perspective of Reactive Mode, because Reactive Mode only uses the resources of the cluster as a criterion for task parallelism. I don’t know if I understand it correctly.

But my above scenario is in non-Reactive Mode. I just use the adaptive scheduler, which means that I increase the parallelism of the running task from 10 to 12. However, because min-parallelism-increase=5, I am satisfied in the cluster slot. When the condition of 12 is met, the expansion of the task cannot be triggered immediately, but it needs to wait for scaling-interval.max before the expansion can be triggered. My purpose is to trigger the expansion when the parallelism of 12 is met, instead of having to after scaling-interval.max or min-parallelism-increase;;;","12/Apr/24 08:09;echauchot;with adaptive scheduler, the jobMaster declares resources needed with a min and a max. The only difference with reactive mode is that the max is +INF. Here we are talking about declaring min resources needed. So unless there is something I missed, I'm not sure reactive mode is relevant here.

If I understand correctly,  what you want in the end is to use whatever new slots arrive in the cluster with a minimal waiting period.  So why not just leave default min-parallelism-increase=1, leave default scaling-interval.max unset and change default scaling-interval.min of 30s to 0s ?

The only thing is that you will have more frequent rescales (each time a slot is added to the cluster) modulo slots that are added during the stabilization period that do not lead to a rescale.;;;","12/Apr/24 08:31;heigebupahei;[~echauchot] 
Thank you for your patience in tracking this issue!
 
> The only thing is that you will have more frequent rescales (each time a slot is added to the cluster) modulo slots that are added during the stabilization period that do not lead to a rescale.

 
This is the problem. Imagine that I originally wanted to adjust the parallelism degree from 10 to 12. My execution step is to first adjust the maximum parallelism degree of my job to 12 through the rest api, and then I add tm to the cluster. If min-parallelism-increase=1 Then my job may trigger the scaling process twice when I change the number of slots from 10 to 12. This process may last for minutes, if min-parallelism-increase > 2, such as 5, Then my job has to wait until scaling-interval.max before scaling. I think we can optimize this process ，let the job trigger scaling exactly when slot becomes 12
 
 ;;;","12/Apr/24 08:46;echauchot;> if min-parallelism-increase=1 Then my job may trigger the scaling process twice when I change the number of slots from 10 to 12

It is not slot per slot rescale, there will be only one rescale in these cases:
 * if the TM comes with 2 slots at once
 * if the second slot comes during the stabilization timeout

That being said, I know there is an ongoing reflection in the community to decrease the overall timeouts during rescale.;;;","12/Apr/24 08:55;heigebupahei;> It is not slot per slot rescale, there will be only one rescale in these cases:
 * if the TM comes with 2 slots at once
 * if the second slot comes during the stabilization timeout

 
Yes, there is only one scaling in both cases. However, if a tm has only one slot and the time interval between registration and jm is relatively long, it will be triggered twice.
 
> That being said, I know there is an ongoing reflection in the community to decrease the overall timeouts during rescale.

good, Can you please help me @ the person who worked on this? Maybe can follow up on this question, thanks!
 
 ;;;","12/Apr/24 09:09;echauchot;I think [~dmvk] has started to think about that.  He might already have suggestions for improving the overall rescale timeout;;;",,,,,,,,,,,,,,,,,,
codegen compile error raised when use kafka connector and protobuf format,FLINK-35034,13574897,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,adam.sun,adam.sun,07/Apr/24 07:27,07/Apr/24 07:39,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,Connectors / Kafka,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,,,"The following error messages and stack were encountered When i using Flink SQL with Kafka connector and protobuf format:
{code:java}
// code placeholder
2024-03-23 23:23:38,852 ERROR org.apache.flink.formats.protobuf.util.PbCodegenUtils        [] - Protobuf codegen compile error: 
package org.apache.flink.formats.protobuf.deserialize;
import org.apache.flink.table.data.RowData;
import org.apache.flink.table.data.ArrayData;
import org.apache.flink.table.data.binary.BinaryStringData;
import org.apache.flink.table.data.GenericRowData;
import org.apache.flink.table.data.GenericMapData;
import org.apache.flink.table.data.GenericArrayData;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.HashMap;
import com.google.protobuf.ByteString;
public class GeneratedProtoToRow_916e09b8a900477390c1f944e4a36da6{
public static RowData decode(.UserProtoBuf.User message){
RowData rowData=null;
.UserProtoBuf.User message0 = message;
GenericRowData rowData0 = new GenericRowData(7);
Object elementDataVar1 = null;
elementDataVar1 = message0.getAge();

rowData0.setField(0, elementDataVar1);
Object elementDataVar2 = null;
elementDataVar2 = message0.getTimestamp();

rowData0.setField(1, elementDataVar2);
Object elementDataVar3 = null;
elementDataVar3 = message0.getEnabled();

rowData0.setField(2, elementDataVar3);
Object elementDataVar4 = null;
elementDataVar4 = message0.getHeight();

rowData0.setField(3, elementDataVar4);
Object elementDataVar5 = null;
elementDataVar5 = message0.getWeight();

rowData0.setField(4, elementDataVar5);
Object elementDataVar6 = null;
elementDataVar6 = BinaryStringData.fromString(message0.getUserName().toString());

rowData0.setField(5, elementDataVar6);
Object elementDataVar7 = null;
elementDataVar7 = BinaryStringData.fromString(message0.getFullAddress().toString());

rowData0.setField(6, elementDataVar7);
rowData = rowData0;

return rowData;
}
}

2024-03-23 23:23:38,856 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: simple_test[2153] -> Sink: print_sink[2154] (1/1)#0 (c4aaed5ad4c63a8ba82a47979ffce386_717c7b8afebbfb7137f6f0f99beb2a94_0_0) switched from INITIALIZING to FAILED with failure cause:
org.apache.flink.formats.protobuf.PbCodegenException: org.apache.flink.api.common.InvalidProgramException: Program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:124) ~[protobufTest-1.0-SNAPSHOT-1.jar:?]
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.open(PbRowDataDeserializationSchema.java:64) ~[protobufTest-1.0-SNAPSHOT-1.jar:?]
    at org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema.open(DynamicKafkaDeserializationSchema.java:94) ~[ververica-connector-kafka-1.17-vvr-8.0.5-SNAPSHOT-jar-with-dependencies.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.open(KafkaDeserializationSchemaWrapper.java:47) ~[ververica-connector-kafka-1.17-vvr-8.0.5-SNAPSHOT-jar-with-dependencies.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.connector.kafka.source.KafkaSource.createReader(KafkaSource.java:144) ~[ververica-connector-kafka-1.17-vvr-8.0.5-SNAPSHOT-jar-with-dependencies.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.connector.kafka.source.KafkaSource.createReader(KafkaSource.java:135) ~[ververica-connector-kafka-1.17-vvr-8.0.5-SNAPSHOT-jar-with-dependencies.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.SourceOperator.initReader(SourceOperator.java:318) ~[flink-dist-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.init(SourceOperatorStreamTask.java:93) ~[flink-dist-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:778) ~[flink-dist-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:745) ~[flink-dist-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:959) ~[flink-dist-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:928) [flink-dist-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:751) [flink-dist-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:567) [flink-dist-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at java.lang.Thread.run(Thread.java:879) [?:1.8.0_372]
Caused by: org.apache.flink.api.common.InvalidProgramException: Program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.formats.protobuf.util.PbCodegenUtils.compileClass(PbCodegenUtils.java:262) ~[protobufTest-1.0-SNAPSHOT-1.jar:?]
    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:116) ~[protobufTest-1.0-SNAPSHOT-1.jar:?]
    ... 14 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 14, Column 30: IDENTIFIER expected instead of '.'
    at org.codehaus.janino.TokenStreamImpl.read(TokenStreamImpl.java:195) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.read(Parser.java:3313) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseQualifiedIdentifier(Parser.java:326) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseReferenceType(Parser.java:2342) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseType(Parser.java:2326) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseFormalParameter(Parser.java:1519) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseFormalParameters(Parser.java:1488) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseMethodDeclarationRest(Parser.java:1392) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseClassBodyDeclaration(Parser.java:938) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseClassBody(Parser.java:736) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseClassDeclarationRest(Parser.java:642) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parsePackageMemberTypeDeclarationRest(Parser.java:370) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.Parser.parseCompilationUnit(Parser.java:241) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table-runtime-1.17-vvr-8.0.5-SNAPSHOT.jar:1.17-vvr-8.0.5-SNAPSHOT]
    at org.apache.flink.formats.protobuf.util.PbCodegenUtils.compileClass(PbCodegenUtils.java:259) ~[protobufTest-1.0-SNAPSHOT-1.jar:?]
    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.<init>(ProtoToRowConverter.java:116) ~[protobufTest-1.0-SNAPSHOT-1.jar:?]
    ... 14 more{code}
proto file:
{code:java}
syntax = ""proto3"";
option java_outer_classname = ""UserProtoBuf"";
message User {
  int32 age = 1;
  int64 timestamp = 2;
  bool enabled = 3;
  float height = 4;
  double weight = 5;
  string userName = 6;
  string Full_Address = 7;
} {code}
Flink SQL:
{code:java}
CREATE TEMPORARY TABLE test (
  ...
) WITH (
  'connector' = 'kafka',
  'topic' = '',
  'properties.bootstrap.servers' = '',
  'properties.group.id' = '',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'protobuf',
  'protobuf.message-class-name' = 'org.example.UserProtoBuf$User',
  'protobuf.ignore-parse-errors' = 'true'
)
; {code}
according to the error message, the type of the parameter `message` which is used in method `decode` was lost package info.
{code:java}
public static RowData decode(.UserProtoBuf.User message){} {code}
After analyzing the following method calls, i found that the above exception will occur when neither `package` nor `option java_package` is specified in the proto file, at this time, the variable `javaPackageName` in method `getOuterProtoPrefix` will be an empty string.

!https://intranetproxy.alipay.com/skylark/lark/0/2024/png/59256556/1712473173927-c277b275-08cc-4bb3-8322-f0c8937700b3.png|width=657,height=349!
{code:java}
org.apache.flink.formats.protobuf.util.PbCodegenUtils#compileClass
org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter#ProtoToRowConverter
 - Class generatedClass = PbCodegenUtils.compileClass(Thread.currentThread().getContextClassLoader(), generatedPackageName + ""."" + generatedClassName, codegenAppender.code());
 - codegenAppender.appendSegment(""public static RowData decode("" + fullMessageClassName + "" message){"");
 - String fullMessageClassName = PbFormatUtils.getFullJavaName(descriptor);
org.apache.flink.formats.protobuf.util.PbFormatUtils#getFullJavaName(com.google.protobuf.Descriptors.Descriptor)
org.apache.flink.formats.protobuf.util.PbFormatUtils#getOuterProtoPrefix {code}",,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/24 07:30;adam.sun;flinksql.txt;https://issues.apache.org/jira/secure/attachment/13067919/flinksql.txt","07/Apr/24 07:30;adam.sun;logfile.txt;https://issues.apache.org/jira/secure/attachment/13067920/logfile.txt","07/Apr/24 07:30;adam.sun;protofile.txt;https://issues.apache.org/jira/secure/attachment/13067921/protofile.txt",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-07 07:27:22.0,,,,,,,,,,"0|z1ohh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Latency marker emitting under async execution model,FLINK-35031,13574894,13574083,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Yanfei Lei,Yanfei Lei,07/Apr/24 06:58,17/Apr/24 08:07,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / Task,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 08:05:01 UTC 2024,,,,,,,,,,"0|z1ohgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 06:21;yunfengzhou;What is the relationship between this ticket and FLINK-35028? The PR of FLINK-35028 has provided implementations to support event time triggers.;;;","17/Apr/24 08:05;Yanfei Lei;[~yunfengzhou] At first I planned to implement event timer and processing timer separately, but during the implementation process, I found that they were very closely related, so I implemented them together.

I will change this ticket to preserve the order between records and ""Latency Marker"".;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Epoch Manager for async execution,FLINK-35030,13574893,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,07/Apr/24 06:57,21/May/24 04:24,04/Jun/24 20:40,21/May/24 04:24,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 04:24:20 UTC 2024,,,,,,,,,,"0|z1ohgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/24 01:02;yunfengzhou;[~Yanfei Lei] According to the description in FLIP-425, Non-record stream elements other than watermark and checkpoint will also be supported through the epoch mechanism. Thus shall we modify the scope of this ticket, not just for watermark, but for all possible events?

Besides, I suppose checkpoint barriers will also use the strict-order mode of the Epoch mechanism. If this is the case, do we still need the checkpoint draining introduced in FLINK-35027?;;;","24/Apr/24 03:47;Yanfei Lei;[~yunfengzhou] 

> Thus shall we modify the scope of this ticket, not just for watermark, but for all possible events?

Thanks for the suggestion, I'll change the scope of this ticket.

 

> I suppose checkpoint barriers will also use the strict-order mode of the Epoch mechanism. If this is the case, do we still need the checkpoint draining introduced in FLINK-35027?

 

We are considering storing some state requests in AEC into checkpoints(see [faster checkpoint drain|https://cwiki.apache.org/confluence/display/FLINK/FLIP-425%3A+Asynchronous+Execution+Model#FLIP425:AsynchronousExecutionModel-FasterCheckpointDrain] ), like unaligned checkpoint. Therefore we handle checkpoint barrier separately first.;;;","21/May/24 04:24;Yanfei Lei;Merged into master via f1ecb9e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Store timer in JVM heap when async execution enabled,FLINK-35029,13574892,13574083,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yanfei Lei,Yanfei Lei,Yanfei Lei,07/Apr/24 06:49,09/May/24 10:53,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,Runtime / Task,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-07 06:49:58.0,,,,,,,,,,"0|z1ohgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timer firing under async execution model,FLINK-35028,13574890,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,07/Apr/24 06:35,22/Apr/24 13:28,04/Jun/24 20:40,22/Apr/24 13:27,,,,,,,,,,,,Runtime / State Backends,Runtime / Task,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 22 13:28:16 UTC 2024,,,,,,,,,,"0|z1ohg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 13:28;Yanfei Lei;Merged into master via 0b2e988;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement checkpoint drain in AsyncExecutionController,FLINK-35027,13574885,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,07/Apr/24 06:06,22/Apr/24 13:29,04/Jun/24 20:40,22/Apr/24 03:02,,,,,,,,,,,,Runtime / Checkpointing,Runtime / Task,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 22 13:29:03 UTC 2024,,,,,,,,,,"0|z1ohew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 13:29;Yanfei Lei;Merged into master via 9336760;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce async execution configurations,FLINK-35026,13574884,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,07/Apr/24 04:58,26/Apr/24 11:51,04/Jun/24 20:40,26/Apr/24 11:51,,,,,,,,,,,,Runtime / Configuration,Runtime / Task,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 26 11:51:32 UTC 2024,,,,,,,,,,"0|z1oheo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 11:51;Yanfei Lei;Merged into master via 713c30f..3ff2ba4 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wire AsyncExecutionController to AbstractStreamOperator,FLINK-35025,13574883,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,Yanfei Lei,Yanfei Lei,07/Apr/24 04:56,10/May/24 05:47,04/Jun/24 20:40,17/Apr/24 10:36,,,,,,,,,,,,Runtime / Task,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 05:47:57 UTC 2024,,,,,,,,,,"0|z1oheg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 10:36;zakelly;Merged into master via c7be45d0...fe8dde4e;;;","10/May/24 03:58;yunfengzhou;Hi [~zakelly], I just noticed that the current implementation might not be able to work correctly if object reuse is enabled, given that multiple StreamRecords would be cached. We may need to force copy the streamRecord before processElement(),like that in AsyncWaitOperator.

https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/async/AsyncWaitOperator.java#L246;;;","10/May/24 04:05;zakelly;[~yunfengzhou] I think you are right. Thanks for the reminder! Would you like to fix this?;;;","10/May/24 05:47;yunfengzhou;Sure. I'll submit a fix for it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Implement record order preservation and buffering of AsyncExecutionController,FLINK-35024,13574882,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,07/Apr/24 04:55,15/Apr/24 06:13,04/Jun/24 20:40,15/Apr/24 06:12,,,,,,,,,,,,Runtime / Task,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 06:13:11 UTC 2024,,,,,,,,,,"0|z1ohe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 06:13;Yanfei Lei;Merged 6d139a1 into master ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNApplicationITCase failed on Azure,FLINK-35023,13574878,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,Weijie Guo,Weijie Guo,07/Apr/24 02:20,11/Apr/24 02:51,04/Jun/24 20:40,10/Apr/24 07:32,1.20.0,,,,,,,,1.20.0,,,Build System / CI,,,,0,pull-request-available,,"1. YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion

{code:java}
Apr 06 02:19:44 02:19:44.063 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion -- Time elapsed: 9.727 s <<< FAILURE!
Apr 06 02:19:44 java.lang.AssertionError: Application became FAILED or KILLED while expecting FINISHED
Apr 06 02:19:44 	at org.apache.flink.yarn.YarnTestBase.waitApplicationFinishedElseKillIt(YarnTestBase.java:1282)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:116)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion$1(YARNApplicationITCase.java:72)
Apr 06 02:19:44 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion(YARNApplicationITCase.java:70)
Apr 06 02:19:44 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Apr 06 02:19:44 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)

{code}


2. YARNApplicationITCase.testApplicationClusterWithRemoteUserJar


{code:java}
Apr 06 02:19:44 java.lang.AssertionError: Application became FAILED or KILLED while expecting FINISHED
Apr 06 02:19:44 	at org.apache.flink.yarn.YarnTestBase.waitApplicationFinishedElseKillIt(YarnTestBase.java:1282)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:116)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithRemoteUserJar$2(YARNApplicationITCase.java:86)
Apr 06 02:19:44 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithRemoteUserJar(YARNApplicationITCase.java:84)
Apr 06 02:19:44 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Apr 06 02:19:44 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
{code}



3. YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion


{code:java}
Apr 06 02:19:44 java.lang.AssertionError: Application became FAILED or KILLED while expecting FINISHED
Apr 06 02:19:44 	at org.apache.flink.yarn.YarnTestBase.waitApplicationFinishedElseKillIt(YarnTestBase.java:1282)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:116)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion$0(YARNApplicationITCase.java:62)
Apr 06 02:19:44 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
Apr 06 02:19:44 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion(YARNApplicationITCase.java:60)
Apr 06 02:19:44 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Apr 06 02:19:44 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
Apr 06 02:19:44 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)

{code}





https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58754&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=28469

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58754&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=28484

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58754&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=28499",,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/24 07:55;rskraba;jobmanager.log;https://issues.apache.org/jira/secure/attachment/13067933/jobmanager.log",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 02:51:03 UTC 2024,,,,,,,,,,"0|z1ohdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/24 09:23;Weijie Guo;jdk17:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58759&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=28322

jdk21:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58759&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=28612;;;","08/Apr/24 05:33;Weijie Guo;jdk17 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58782&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=28689
;;;","08/Apr/24 08:15;rskraba;I've attached a the failing  [^jobmanager.log] -- for all three of the errors this looks like another classloading problem related to FLINK-34988.

{code}
... snip ...
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not ""opens java.util"" to unnamed module @4b55eca5
	at java.lang.reflect.AccessibleObject.throwInaccessibleObjectException(AccessibleObject.java:391) ~[?:?]
	at java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:367) ~[?:?]
	at java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:315) ~[?:?]
	at java.lang.reflect.Field.checkCanSetAccessible(Field.java:183) ~[?:?]
	at java.lang.reflect.Field.setAccessible(Field.java:177) ~[?:?]
	at org.apache.flink.streaming.runtime.translators.DataStreamV2SinkTransformationTranslator.registerSinkTransformationTranslator(DataStreamV2SinkTransformationTranslator.java:104) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.datastream.impl.ExecutionEnvironmentImpl.<clinit>(ExecutionEnvironmentImpl.java:96) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:109) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:301) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
	... 13 more
{code}

in the maven logs, you can also see 

{code}
org.apache.hadoop.util.Shell$ExitCodeException: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:998) ~[hadoop-common-2.10.2.jar:?]
	at org.apache.hadoop.util.Shell.run(Shell.java:884) ~[hadoop-common-2.10.2.jar:?]
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1216) ~[hadoop-common-2.10.2.jar:?]
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:294) ~[hadoop-yarn-server-nodemanager-2.10.2.jar:?]
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:447) ~[hadoop-yarn-server-nodemanager-2.10.2.jar:?]
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:298) ~[hadoop-yarn-server-nodemanager-2.10.2.jar:?]
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:99) ~[hadoop-yarn-server-nodemanager-2.10.2.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
{code}

Do we just need to add the {{surefire.module.config}} to flink-yarn-tests?;;;","08/Apr/24 14:52;rskraba;1.20 jdk17 https://github.com/apache/flink/actions/runs/8577964065/job/23511387767#step:10:28144
1.20 jdk21 https://github.com/apache/flink/actions/runs/8577964065/job/23511380428#step:10:28113
1.20 jdk17 https://github.com/apache/flink/actions/runs/8585464694/job/23527040006#step:10:28255
1.20 jdk21 https://github.com/apache/flink/actions/runs/8585464694/job/23527032769#step:10:27764
1.20 jdk17 https://github.com/apache/flink/actions/runs/8593534375/job/23545425992#step:10:27947
1.20 jdk21 https://github.com/apache/flink/actions/runs/8593534375/job/23545418325#step:10:27914

;;;","08/Apr/24 16:58;rskraba;Ooops -- there's is already a {{surefire.module.config}} in the *flink-yarn-tests* module, but the MiniYARNCluster isn't taking the specified {{--add-opens}}.  Apparently this might be automatic in more recent Hadoop versions (MAPREDUCE-7449).  

I can reproduce this locally.  Here's an example of the JVM flags used in the current Java 17 run: 

{code}
2024-04-06 02:39:51,076 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
2024-04-06 02:39:51,083 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting YarnApplicationClusterEntryPoint (Version: 1.20-SNAPSHOT, Scala: 2.12, Rev:DeadD0d0, Date:1970-01-01T01:00:00+01:00)
2024-04-06 02:39:51,083 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: root
2024-04-06 02:39:51,242 WARN  org.apache.hadoop.util.NativeCodeLoader                      [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-04-06 02:39:51,343 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: root
2024-04-06 02:39:51,344 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: OpenJDK 64-Bit Server VM - Eclipse Adoptium - 17/17.0.7+7
2024-04-06 02:39:51,344 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Arch: amd64
2024-04-06 02:39:51,346 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 192 MiBytes
2024-04-06 02:39:51,346 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /usr/lib/jvm/jdk-17.0.7+7
2024-04-06 02:39:51,347 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Hadoop version: 2.10.2
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:+HeapDumpOnOutOfMemoryError
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx201326592
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms201326592
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:+IgnoreUnrecognizedVMOptions
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/root/flink/flink-yarn-tests/target/test/data/flink-yarn-tests-application/yarn-1405946/flink-yarn-tests-application-logDir-nm-0_0/application_1712371182743_0001/container_1712371182743_0001_01_000001/jobmanager.log
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:log4j.properties
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:log4j.properties
2024-04-06 02:39:51,348 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:
2024-04-06 02:39:51,349 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2024-04-06 02:39:51,349 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=201326592b
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b
2024-04-06 02:39:51,350 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: :lib/flink-cep-1.20-SNAPSHOT.jar:lib/flink-connector-files-1.20-SNAPSHOT.jar:lib/flink-csv-1.20-SNAPSHOT.jar:lib/flink-json-1.20-SNAPSHOT.jar:lib/flink-scala_2.12-1.20-SNAPSHOT.jar:lib/flink-table-api-java-uber-1.20-SNAPSHOT.jar:lib/flink-table-planner-loader-1.20-SNAPSHOT.jar:lib/flink-table-runtime-1.20-SNAPSHOT.jar:lib/log4j-1.2-api-2.17.1.jar:lib/log4j-api-2.17.1.jar:lib/log4j-core-2.17.1.jar:lib/log4j-slf4j-impl-2.17.1.jar:flink-dist-1.20-SNAPSHOT.jar:config.yaml::/root/.m2/repository/org/testcontainers/testcontainers/1.19.1/testcontainers-1.19.1.jar:/root/.m2/repository/org/rnorth/duct-tape/duct-tape/1.0.8/duct-tape-1.0.8.jar:/root/.m2/repository/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/root/.m2/repository/com/github/docker-java/docker-java-api/3.3.3/docker-java-api-3.3.3.jar:/root/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.15.3/jackson-annotations-2.15.3.jar:/root/.m2/repository/com/github/docker-java/docker-java-transport-zerodep/3.3.3/docker-java-transport-zerodep-3.3.3.jar:/root/.m2/repository/com/github/docker-java/docker-java-transport/3.3.3/docker-java-transport-3.3.3.jar:/root/.m2/repository/net/java/dev/jna/jna/5.12.1/jna-5.12.1.jar:/root/.m2/repository/com/ibm/icu/icu4j/67.1/icu4j-67.1.jar:/root/.m2/repository/org/snakeyaml/snakeyaml-engine/2.6/snakeyaml-engine-2.6.jar:/root/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar:/root/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/root/.m2/repository/com/ververica/frocksdbjni/6.20.3-ververica-2.0/frocksdbjni-6.20.3-ververica-2.0.jar:/root/.m2/repository/com/twitter/chill-java/0.7.6/chill-java-0.7.6.jar:/root/.m2/repository/commons-io/commons-io/2.15.1/commons-io-2.15.1.jar:/root/.m2/repository/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/root/.m2/repository/org/apache/commons/commons-text/1.10.0/commons-text-1.10.0.jar:/root/.m2/repository/commons-cli/commons-cli/1.5.0/commons-cli-1.5.0.jar:/root/.m2/repository/org/javassist/javassist/3.24.0-GA/javassist-3.24.0-GA.jar:/root/.m2/repository/org/xerial/snappy/snappy-java/1.1.10.4/snappy-java-1.1.10.4.jar:/root/.m2/repository/tools/profiler/async-profiler/2.9/async-profiler-2.9.jar:/root/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/root/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/root/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.10.2/hadoop-hdfs-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-hdfs-client/2.10.2/hadoop-hdfs-client-2.10.2.jar:/root/.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar:/root/.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar:/root/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/root/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar:/root/.m2/repository/io/netty/netty-all/4.1.100.Final/netty-all-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-buffer/4.1.100.Final/netty-buffer-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec/4.1.100.Final/netty-codec-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-dns/4.1.100.Final/netty-codec-dns-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-haproxy/4.1.100.Final/netty-codec-haproxy-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-http/4.1.100.Final/netty-codec-http-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-http2/4.1.100.Final/netty-codec-http2-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-memcache/4.1.100.Final/netty-codec-memcache-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-mqtt/4.1.100.Final/netty-codec-mqtt-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-redis/4.1.100.Final/netty-codec-redis-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-smtp/4.1.100.Final/netty-codec-smtp-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-socks/4.1.100.Final/netty-codec-socks-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-stomp/4.1.100.Final/netty-codec-stomp-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-codec-xml/4.1.100.Final/netty-codec-xml-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-common/4.1.100.Final/netty-common-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-handler/4.1.100.Final/netty-handler-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.100.Final/netty-transport-native-unix-common-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-handler-proxy/4.1.100.Final/netty-handler-proxy-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-handler-ssl-ocsp/4.1.100.Final/netty-handler-ssl-ocsp-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-resolver/4.1.100.Final/netty-resolver-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-resolver-dns/4.1.100.Final/netty-resolver-dns-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-transport/4.1.100.Final/netty-transport-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-transport-rxtx/4.1.100.Final/netty-transport-rxtx-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-transport-sctp/4.1.100.Final/netty-transport-sctp-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-transport-udt/4.1.100.Final/netty-transport-udt-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.100.Final/netty-transport-classes-epoll-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-transport-classes-kqueue/4.1.100.Final/netty-transport-classes-kqueue-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-resolver-dns-classes-macos/4.1.100.Final/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/root/.m2/repository/io/netty/netty-transport-native-epoll/4.1.100.Final/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/root/.m2/repository/io/netty/netty-transport-native-epoll/4.1.100.Final/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/root/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.100.Final/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/root/.m2/repository/io/netty/netty-transport-native-kqueue/4.1.100.Final/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/root/.m2/repository/io/netty/netty-resolver-dns-native-macos/4.1.100.Final/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/root/.m2/repository/io/netty/netty-resolver-dns-native-macos/4.1.100.Final/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/root/.m2/repository/xerces/xercesImpl/2.12.0/xercesImpl-2.12.0.jar:/root/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/root/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/root/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.15.3/jackson-databind-2.15.3.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.10.2/hadoop-yarn-common-2.10.2.jar:/root/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/root/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/root/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/root/.m2/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/root/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/root/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/root/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/root/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/root/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.10.2/hadoop-mapreduce-client-core-2.10.2.jar:/root/.m2/repository/org/apache/kafka/kafka-clients/3.2.3/kafka-clients-3.2.3.jar:/root/.m2/repository/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar:/root/.m2/repository/org/jcuda/jcuda/10.0.0/jcuda-10.0.0.jar:/root/.m2/repository/org/jcuda/jcublas/10.0.0/jcublas-10.0.0.jar:/root/.m2/repository/org/apache/curator/curator-test/5.4.0/curator-test-5.4.0.jar:/root/.m2/repository/org/apache/zookeeper/zookeeper/3.7.1/zookeeper-3.7.1.jar:/root/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.7.1/zookeeper-jute-3.7.1.jar:/root/.m2/repository/org/apache/yetus/audience-annotations/0.12.0/audience-annotations-0.12.0.jar:/root/.m2/repository/io/dropwizard/metrics/metrics-core/3.2.5/metrics-core-3.2.5.jar:/root/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.10.1/junit-jupiter-api-5.10.1.jar:/root/.m2/repository/org/opentest4j/opentest4j/1.3.0/opentest4j-1.3.0.jar:/root/.m2/repository/org/junit/platform/junit-platform-commons/1.10.1/junit-platform-commons-1.10.1.jar:/root/.m2/repository/org/apache/hadoop/hadoop-common/2.10.2/hadoop-common-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-annotations/2.10.2/hadoop-annotations-2.10.2.jar:/root/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar:/root/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/root/.m2/repository/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar:/root/.m2/repository/org/apache/httpcomponents/httpcore/4.4.14/httpcore-4.4.14.jar:/root/.m2/repository/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/root/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/root/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/root/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/root/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/root/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/root/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/root/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/root/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/root/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/root/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/root/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/root/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/root/.m2/repository/asm/asm/3.1/asm-3.1.jar:/root/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/root/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/root/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/root/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/root/.m2/repository/commons-configuration/commons-configuration/1.7/commons-configuration-1.7.jar:/root/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/root/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/root/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/root/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/root/.m2/repository/org/apache/avro/avro/1.11.3/avro-1.11.3.jar:/root/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.15.3/jackson-core-2.15.3.jar:/root/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/root/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/root/.m2/repository/org/apache/hadoop/hadoop-auth/2.10.2/hadoop-auth-2.10.2.jar:/root/.m2/repository/com/nimbusds/nimbus-jose-jwt/7.9/nimbus-jose-jwt-7.9.jar:/root/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/root/.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar:/root/.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar:/root/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/root/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/root/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/root/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/root/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/root/.m2/repository/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/root/.m2/repository/com/jcraft/jsch/0.1.55/jsch-0.1.55.jar:/root/.m2/repository/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/root/.m2/repository/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/root/.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar:/root/.m2/repository/org/apache/commons/commons-compress/1.26.0/commons-compress-1.26.0.jar:/root/.m2/repository/org/codehaus/woodstox/stax2-api/4.2.1/stax2-api-4.2.1.jar:/root/.m2/repository/com/fasterxml/woodstox/woodstox-core/5.3.0/woodstox-core-5.3.0.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.10.2/hadoop-yarn-client-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.10.2/hadoop-yarn-api-2.10.2.jar:/root/.m2/repository/javax/xml/bind/jaxb-api/2.3.1/jaxb-api-2.3.1.jar:/root/.m2/repository/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar:/root/.m2/repository/org/apache/hadoop/hadoop-minicluster/2.10.2/hadoop-minicluster-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-common/2.10.2/hadoop-common-2.10.2-tests.jar:/root/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.10.2/hadoop-hdfs-2.10.2-tests.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-server-tests/2.10.2/hadoop-yarn-server-tests-2.10.2-tests.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.10.2/hadoop-yarn-server-common-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-registry/2.10.2/hadoop-yarn-registry-2.10.2.jar:/root/.m2/repository/org/apache/geronimo/specs/geronimo-jcache_1.0_spec/1.0-alpha-1/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/.m2/repository/org/ehcache/ehcache/3.3.1/ehcache-3.3.1.jar:/root/.m2/repository/com/zaxxer/HikariCP-java7/2.4.12/HikariCP-java7-2.4.12.jar:/root/.m2/repository/com/microsoft/sqlserver/mssql-jdbc/6.2.1.jre7/mssql-jdbc-6.2.1.jre7.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.10.2/hadoop-yarn-server-nodemanager-2.10.2.jar:/root/.m2/repository/com/codahale/metrics/metrics-core/3.0.1/metrics-core-3.0.1.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.10.2/hadoop-yarn-server-resourcemanager-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.10.2/hadoop-yarn-server-applicationhistoryservice-2.10.2.jar:/root/.m2/repository/de/ruedigermoeller/fst/2.50/fst-2.50.jar:/root/.m2/repository/com/cedarsoftware/java-util/1.9.0/java-util-1.9.0.jar:/root/.m2/repository/com/cedarsoftware/json-io/2.5.1/json-io-2.5.1.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-server-timelineservice/2.10.2/hadoop-yarn-server-timelineservice-2.10.2.jar:/root/.m2/repository/org/apache/commons/commons-csv/1.0/commons-csv-1.0.jar:/root/.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar:/root/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.10.2/hadoop-mapreduce-client-jobclient-2.10.2-tests.jar:/root/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.10.2/hadoop-mapreduce-client-common-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.10.2/hadoop-mapreduce-client-shuffle-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.10.2/hadoop-mapreduce-client-app-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.10.2/hadoop-yarn-server-web-proxy-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.10.2/hadoop-mapreduce-client-jobclient-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-hs/2.10.2/hadoop-mapreduce-client-hs-2.10.2.jar:/root/.m2/repository/org/apache/hadoop/hadoop-minikdc/3.2.4/hadoop-minikdc-3.2.4.jar:/root/.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar:/root/.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar:/root/.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar:/root/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/root/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/root/.m2/repository/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar:/root/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/root/.m2/repository/org/junit/jupiter/junit-jupiter/5.10.1/junit-jupiter-5.10.1.jar:/root/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.10.1/junit-jupiter-params-5.10.1.jar:/root/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.10.1/junit-jupiter-engine-5.10.1.jar:/root/.m2/repository/org/junit/vintage/junit-vintage-engine/5.10.1/junit-vintage-engine-5.10.1.jar:/root/.m2/repository/org/junit/platform/junit-platform-engine/1.10.1/junit-platform-engine-1.10.1.jar:/root/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/root/.m2/repository/org/assertj/assertj-core/3.23.1/assertj-core-3.23.1.jar:/root/.m2/repository/net/bytebuddy/byte-buddy/1.14.4/byte-buddy-1.14.4.jar:/root/.m2/repository/org/mockito/mockito-core/3.4.6/mockito-core-3.4.6.jar:/root/.m2/repository/net/bytebuddy/byte-buddy-agent/1.14.4/byte-buddy-agent-1.14.4.jar:/root/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/root/.m2/repository/org/mockito/mockito-junit-jupiter/3.4.6/mockito-junit-jupiter-3.4.6.jar:/root/.m2/repository/org/hamcrest/hamcrest-all/1.3/hamcrest-all-1.3.jar:/root/.m2/repository/org/testcontainers/junit-jupiter/1.19.1/junit-jupiter-1.19.1.jar:/root/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.17.1/log4j-slf4j-impl-2.17.1.jar:/root/.m2/repository/org/apache/logging/log4j/log4j-api/2.17.1/log4j-api-2.17.1.jar:/root/.m2/repository/org/apache/logging/log4j/log4j-core/2.17.1/log4j-core-2.17.1.jar:/root/.m2/repository/org/apache/logging/log4j/log4j-1.2-api/2.17.1/log4j-1.2-api-2.17.1.jar
2024-04-06 02:39:51,352 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -
{code};;;","09/Apr/24 04:11;Weijie Guo;jdk17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58811&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=28695

jkd21: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58811&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=28266;;;","09/Apr/24 04:12;Weijie Guo;Thanks [~rskraba], good find!

> Apparently this might be automatic in more recent Hadoop versions

Does this mean we need to upgrade the bumped hadoop version?;;;","09/Apr/24 10:07;rskraba;It looks like these JVM args are meant to be applied to the Flink JVM, not the YARN processes, so we don't need a hadoop bump! 

(The MAPREDUCE Jira that I mentioned isn't relevant after all.);;;","10/Apr/24 07:32;Sergey Nuyanzin;Merged as [dec5e9e659dd09346781c97c940a20a6cbc63678|https://github.com/apache/flink/commit/dec5e9e659dd09346781c97c940a20a6cbc63678];;;","10/Apr/24 10:12;rskraba;(before the fix on master)
1.20 jdk17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58842&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=28467
1.20 jdk21: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58842&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=28019
1.20 jdk17: https://github.com/apache/flink/actions/runs/8609297774/job/23593490958#step:10:27893
1.20 jdk21: https://github.com/apache/flink/actions/runs/8609297774/job/23593252830#step:10:27865
1.20 jdk17: https://github.com/apache/flink/actions/runs/8624933967/job/23640971283
1.20 jdk21: https://github.com/apache/flink/actions/runs/8624933967/job/23640966448#step:10:28119
;;;","11/Apr/24 02:51;Weijie Guo;Thanks [~rskraba] for the fix, good job!;;;",,,,,,,,,,,,,,,,,,
Add TypeInformed Element Converter for DynamoDbSink,FLINK-35022,13574865,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chalixar,chalixar,chalixar,06/Apr/24 15:44,26/Apr/24 14:25,04/Jun/24 20:40,,aws-connector-4.3.0,,,,,,,,,,,Connectors / DynamoDB,,,,0,pull-request-available,,"h2. Context
{{DynamoDbSink}} as an extentsion of {{AsyncSinkBase}} depends on {{org.apache.flink.connector.base.sink.writer.ElementConverter}} to convert Flink stream objects to DynamoDb write requests, where item is represented as {{Map<String, AttributeValue[1]>}}.

{{AttributeValue}} is the wrapper for the DynamoDb comprehendable Object in a format similar with type identification properties as in
{M"": {""Name"" : {""S"": Joe }, ""Age"" : {""N"": 35 }}}.

Since TypeInformation is already natively supported in Flink, many implementations of the DynamoDb ElementConverted is just a boiler plate. 
For example 
{code:title=""Simple POJO Element Conversion""}
 public class Order {
        String id;
        int quantity;
        double total;
}
{code}

The implementation of the converter must be 

{code:title=""Simple POJO DDB Element Converter""}
public static class SimplePojoElementConverter implements ElementConverter<Order, DynamoDbWriteRequest> {

        @Override
        public DynamoDbWriteRequest apply(Order order, SinkWriter.Context context) {
            Map<String, AttributeValue> itemMap = new HashMap<>();
            itemMap.put(""id"", AttributeValue.builder().s(order.id).build());
            itemMap.put(""quantity"", AttributeValue.builder().n(String.valueOf(order.quantity)).build());
            itemMap.put(""total"", AttributeValue.builder().n(String.valueOf(order.total)).build());
            return DynamoDbWriteRequest.builder()
                    .setType(DynamoDbWriteRequestType.PUT)
                    .setItem(itemMap)
                    .build();
        }

        @Override
        public void open(Sink.InitContext context) {
            
        }
    }
{code}

while this might not be too much of work, however it is a fairly common case in Flink and this implementation requires some fair knowledge of DDB model for new users.

h2. Proposal 

Introduce {{ DynamoDbTypeInformedElementConverter}} as follows:

{code:title=""TypeInformedElementconverter""} 
public class DynamoDbTypeInformedElementConverter<inputT> implements ElementConverter<inputT, DynamoDbWriteRequest> {
DynamoDbTypeInformedElementConverter(CompositeType<inputT> typeInfo);
    public DynamoDbWriteRequest convertElement(input) {
    switch this.typeInfo{
        case: BasicTypeInfo.STRING_TYPE_INFO: return input -> AttributeValue.fromS(o.toString())
        case: BasicTypeInfo.SHORT_TYPE_INFO: 
        case: BasicTypeInfo.INTEGER_TYPE_INFO: input -> AttributeValue.fromN(o.toString())
       case: TupleTypeInfo: input -> AttributeValue.fromL(converTuple(input))
      .....
    }
}
}

// User Code
public static void main(String []args) {
  DynamoDbTypeInformedElementConverter elementConverter = new DynamoDbTypeInformedElementConverter(TypeInformation.of(Order.class));
DdbSink.setElementConverter(elementConverter); 
}

{code}

We will start by supporting all Pojo/ basic/ Tuple/ Array typeInfo which should be enough to cover all DDB supported types (s,n,bool,b,ss,ns,bs,bools,m,l)

1- https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/dynamodb/model/AttributeValue.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 09:37:08 UTC 2024,,,,,,,,,,"0|z1ohag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/24 15:45;chalixar;[~danny.cranmer] Wdyt about this proposal? 
Additionally could please assign it to myself;;;","15/Apr/24 09:18;dannycranmer;Hey [~chalixar], thanks for the contribution. Can you elaborate on when a user would use this instead of the [DynamoDbBeanElementConverter|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/main/java/org/apache/flink/connector/dynamodb/sink/DynamoDbBeanElementConverter.java]? You can simply annotate your class with {{@DynamoDbBean}} and then use the converter like so (example [Order|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws/flink-connector-dynamodb/src/test/java/org/apache/flink/connector/dynamodb/util/Order.java]):

{code}
ElementConverter<Order, DynamoDbWriteRequest> elementConverter =
                new DynamoDbBeanElementConverter<>(Order.class);
{code}

Are you targeting the case when the user does not want/cannot change the model? I would rather leave the type transform to the AWS SDK if possible. However, I like the idea here. Please elaborate on the motivation;;;","15/Apr/24 09:37;chalixar;Hi [~dannycranmer] 
Thanks for the reply, I agree most POJOs could just be converted using bean converter. 
To be very honest, the main motivation came from Pyflink where type informed data types are commonly used, 
The BeanConverter couldn't be used in pyflink after supporting DDB Pyflink https://issues.apache.org/jira/browse/FLINK-32007 (Actually this is a blocker to this task) since defined pojos are python classes not java, this is resolved in datastream functions using TypeInformed functions as in [map function here.|https://github.com/apache/flink/blob/f74dc57561a058696bd2bd42593f862a9b490474/flink-python/pyflink/datastream/data_stream.py#L273];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
AggregateQueryOperations produces wrong asSerializableString representation,FLINK-35021,13574746,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,05/Apr/24 09:06,05/Apr/24 12:18,04/Jun/24 20:40,05/Apr/24 12:18,1.19.0,,,,,,,,1.20.0,,,Table SQL / API,,,,0,pull-request-available,,"A table API query:
{code}
        env.fromValues(1, 2, 3)
                .as(""number"")
                .select(col(""number"").count())
                .insertInto(TEST_TABLE_API)
{code}

produces

{code}
INSERT INTO `default`.`timo_eu_west_1`.`table_api_basic_api` SELECT `EXPR$0` FROM (
    SELECT (COUNT(`number`)) AS `EXPR$0` FROM (
        SELECT `f0` AS `number` FROM (
            SELECT `f0` FROM (VALUES 
                (1),
                (2),
                (3)
            ) VAL$0(`f0`)
        )
    )
    GROUP BY 
)
{code}

which is missing a grouping expression",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 05 12:18:17 UTC 2024,,,,,,,,,,"0|z1ogk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Apr/24 12:18;dwysakowicz;Fixed in 77215aaf6ca7ccbff7bd3752e59068ac9956d549;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Model Catalog implementation in Hive etc,FLINK-35020,13574707,13574379,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lihaosky,lihaosky,04/Apr/24 22:46,04/Apr/24 22:46,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-04 22:46:13.0,,,,,,,,,,"0|z1ogbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support show create model syntax,FLINK-35019,13574694,13574379,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lihaosky,lihaosky,04/Apr/24 19:20,04/Apr/24 19:20,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,show options in addition to input/output schema,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-04 19:20:59.0,,,,,,,,,,"0|z1og8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML_EVALUATE function,FLINK-35018,13574682,13574379,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lihaosky,lihaosky,04/Apr/24 17:55,04/Apr/24 17:55,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-04 17:55:42.0,,,,,,,,,,"0|z1og60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML_PREDICT function,FLINK-35017,13574680,13574379,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lihaosky,lihaosky,04/Apr/24 17:55,04/Apr/24 17:55,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-04 17:55:28.0,,,,,,,,,,"0|z1og5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catalog changes for model CRUD,FLINK-35016,13574679,13574379,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lihaosky,lihaosky,04/Apr/24 17:55,04/Apr/24 17:55,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-04 17:55:03.0,,,,,,,,,,"0|z1og5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Parquet Reader doesn't honor parquet configuration,FLINK-35015,13574677,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,04/Apr/24 17:53,10/Apr/24 10:48,04/Jun/24 20:40,10/Apr/24 10:48,1.17.2,1.18.1,1.19.0,,,,,,1.20.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,"For example, To access parquet files in legacy standard, users to need to use READ_INT96_AS_FIXED flag to read deprecated INT96 columns. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 10 10:48:40 UTC 2024,,,,,,,,,,"0|z1og4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/24 18:18;ZhenqiuHuang;After taking deeper look on the code, AvroSchemaConverter takes hadoop configuration. So users just need to specify hadoop config in flink conf to enable the flag. ;;;","10/Apr/24 10:48;mbalassi;[{{c0891cf}}|https://github.com/apache/flink/commit/c0891cfe28903c5a362a35ea3bb8702bdc92cf43] in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlNode to operation conversion for models,FLINK-35014,13574676,13574379,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lihaosky,lihaosky,04/Apr/24 17:52,04/Apr/24 17:52,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-04 17:52:40.0,,,,,,,,,,"0|z1og4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
