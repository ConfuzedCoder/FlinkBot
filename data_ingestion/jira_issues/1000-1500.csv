Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Issue split),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
GroupAggregateRestoreTest.testRestore fails,FLINK-34513,13569798,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,bvarghese,mapohl,mapohl,26/Feb/24 07:13,03/Jun/24 01:45,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57828&view=logs&j=26b84117-e436-5720-913e-3e280ce55cae&t=77cc7e77-39a0-5007-6d65-4137ac13a471&l=10881

{code}
Feb 24 01:12:01 01:12:01.384 [ERROR] Tests run: 10, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 2.957 s <<< FAILURE! -- in org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateRestoreTest
Feb 24 01:12:01 01:12:01.384 [ERROR] org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateRestoreTest.testRestore(TableTestProgram, ExecNodeMetadata)[4] -- Time elapsed: 0.653 s <<< FAILURE!
Feb 24 01:12:01 java.lang.AssertionError: 
Feb 24 01:12:01 
Feb 24 01:12:01 Expecting actual:
Feb 24 01:12:01   [""+I[3, 1, 2, 8, 31, 10.0, 3]"",
Feb 24 01:12:01     ""+I[2, 1, 4, 14, 42, 7.0, 6]"",
Feb 24 01:12:01     ""+I[1, 1, 4, 12, 24, 6.0, 4]"",
Feb 24 01:12:01     ""+U[2, 1, 4, 14, 57, 8.0, 7]"",
Feb 24 01:12:01     ""+U[1, 1, 4, 12, 32, 6.0, 5]"",
Feb 24 01:12:01     ""+I[7, 0, 1, 7, 7, 7.0, 1]"",
Feb 24 01:12:01     ""+U[2, 1, 4, 14, 57, 7.0, 7]"",
Feb 24 01:12:01     ""+U[1, 1, 4, 12, 32, 5.0, 5]"",
Feb 24 01:12:01     ""+U[3, 1, 2, 8, 31, 9.0, 3]"",
Feb 24 01:12:01     ""+U[7, 0, 1, 7, 7, 7.0, 2]""]
Feb 24 01:12:01 to contain exactly in any order:
Feb 24 01:12:01   [""+I[3, 1, 2, 8, 31, 10.0, 3]"",
Feb 24 01:12:01     ""+I[2, 1, 4, 14, 42, 7.0, 6]"",
Feb 24 01:12:01     ""+I[1, 1, 4, 12, 24, 6.0, 4]"",
Feb 24 01:12:01     ""+U[2, 1, 4, 14, 57, 8.0, 7]"",
Feb 24 01:12:01     ""+U[1, 1, 4, 12, 32, 6.0, 5]"",
Feb 24 01:12:01     ""+U[3, 1, 2, 8, 31, 9.0, 3]"",
Feb 24 01:12:01     ""+U[2, 1, 4, 14, 57, 7.0, 7]"",
Feb 24 01:12:01     ""+I[7, 0, 1, 7, 7, 7.0, 2]"",
Feb 24 01:12:01     ""+U[1, 1, 4, 12, 32, 5.0, 5]""]
Feb 24 01:12:01 elements not found:
Feb 24 01:12:01   [""+I[7, 0, 1, 7, 7, 7.0, 2]""]
Feb 24 01:12:01 and elements not expected:
Feb 24 01:12:01   [""+I[7, 0, 1, 7, 7, 7.0, 1]"", ""+U[7, 0, 1, 7, 7, 7.0, 2]""]
Feb 24 01:12:01 
Feb 24 01:12:01 	at org.apache.flink.table.planner.plan.nodes.exec.testutils.RestoreTestBase.testRestore(RestoreTestBase.java:313)
Feb 24 01:12:01 	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33421,,,,,,FLINK-35012,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 01:45:54 UTC 2024,,,,,,,,,,"0|z1nm1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 07:14;mapohl;[~bvarghese] can you have a look at this? I assume it's related to the work you've done in FLINK-33421? Please correct me if I'm wrong.;;;","27/Feb/24 07:40;mapohl;https://github.com/apache/flink/actions/runs/8058552438/job/22011758865#step:10:11241;;;","04/Mar/24 07:46;mapohl;https://github.com/apache/flink/actions/runs/8127069864/job/22212009057#step:10:10308;;;","25/Mar/24 18:22;jhughes;From looking at this quickly, here are some notes:

First, there are at least two different test cases which are failing.  Both seem to be related to enabling mini.batch.

Second, I tried to fix this via `.testMaterializedData()`.  This approach (presently) fails since records are updated between between and after the restore.  We could extend the idea of `.testMaterializedData()` check only the records specified in `.consumedAfterRestore()`.;;;","04/Apr/24 16:11;rskraba;I just created FLINK-35012 which  might be related or a duplicate since it inherits from the same test base {{RestoreTestBase}}.  ;;;","21/May/24 14:26;rskraba;* 1.20 Java 8 / Test (module: table) [https://github.com/apache/flink/actions/runs/9144334458/job/25142198437#step:10:10702];;;","22/May/24 12:44;rskraba;* 1.19 Java 21 / Test (module: table) https://github.com/apache/flink/actions/runs/9184288072/job/25256618916#step:10:10748;;;","29/May/24 14:46;mapohl;* 1.20 (Java 8): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59935&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11686;;;","03/Jun/24 01:44;Weijie Guo;1.20 test_cron_jdk21 table

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=60008&view=logs&j=26b84117-e436-5720-913e-3e280ce55cae&t=77cc7e77-39a0-5007-6d65-4137ac13a471&l=11513;;;","03/Jun/24 01:45;Weijie Guo;Hi [~bvarghese], Is there any progress about this?;;;",,,,,,,,,,,,,,,,,,,,,,,
Thrown root cause for  HandlerRequestException,FLINK-34512,13569797,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,masteryhx,masteryhx,masteryhx,26/Feb/24 07:09,28/Feb/24 02:18,04/Jun/24 20:40,28/Feb/24 02:18,,,,,,,,,,,,Runtime / REST,,,,0,pull-request-available,,,,"My Flink job only thrown below exception without the root cause:

 
{code:java}
org.apache.flink.runtime.rest.handler.HandlerRequestException: Cannot resolve path parameter (triggerid) from value ""4c729cc8-ccc1-80dc-6c8d-9d84d52cfe7b"". {code}
It's better for HandlerRequest to thrown the root cause of HandlerRequestException.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-26 07:09:06.0,,,,,,,,,,"0|z1nm1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 2.0: Deprecate legacy State&Checkpointing&Recovery options,FLINK-34511,13569796,13566461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zakelly,zakelly,26/Feb/24 07:01,26/Feb/24 07:01,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-26 07:01:29.0,,,,,,,,,,"0|z1nm14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 2.0: Rename RestoreMode to RecoveryClaimMode,FLINK-34510,13569795,13566461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zakelly,zakelly,26/Feb/24 07:00,29/May/24 12:43,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 29 12:43:35 UTC 2024,,,,,,,,,,"0|z1nm0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/24 06:06;spoon-lz;Hi [~zakelly] ,this issue has not started yet. Do we have to wait until the 1.20 release?;;;","29/May/24 12:43;zakelly;[~spoon-lz] Yes, we should wait until 1.20 release (when we can do something for flink 2.0);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs: Fix Debezium JSON/AVRO opts in examples ,FLINK-34509,13569661,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,lorenzo.affetti,lorenzo.affetti,23/Feb/24 15:23,05/Mar/24 22:46,04/Jun/24 20:40,05/Mar/24 22:46,1.19.0,1.20.0,,,,,,1.20.0,,,,Documentation,,,,0,pull-request-available,,,,"Problem found here: [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/debezium/#how-to-use-debezium-format.]

Here is the example provided:
{code:java}
CREATE TABLE topic_products (
  -- schema is totally the same to the MySQL ""products"" table
  id BIGINT,
  name STRING,
  description STRING,
  weight DECIMAL(10, 2)
) WITH (
 'connector' = 'kafka',
 'topic' = 'products_binlog',
 'properties.bootstrap.servers' = 'localhost:9092',
 'properties.group.id' = 'testGroup',
 -- using 'debezium-json' as the format to interpret Debezium JSON messages
 -- please use 'debezium-avro-confluent' if Debezium encodes messages in Avro format
 'format' = 'debezium-json'
) {code}
Actually, `debezium-json` would require `'debezium-json.schema-include' = 'true'` to work (as, by default, Debezium includes the schema. See [https://www.markhneedham.com/blog/2023/01/24/flink-sql-could-not-execute-sql-statement-corrupt-debezium-message/).]

On the other hand `debezium-avro` would require the URL of the Confluent schema registry: `'debezium-avro-confluent.url' = '[http://...:8081'.]

I propose to split the single example in 2 with the correct default options.",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 22:45:58 UTC 2024,,,,,,,,,,"0|z1nl74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/24 22:45;jingge;master: 641f4f4d0d0156b84bdb9ba528b1dd96f7ae9d9c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate S3-related ITCases and e2e tests to Minio ,FLINK-34508,13569645,13562450,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,m.orazow,mapohl,mapohl,23/Feb/24 13:08,08/May/24 20:37,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,,Build System / CI,,,,0,github-actions,pull-request-available,,,Anything that uses {{org.apache.flink.testutils.s3.S3TestCredentials}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34324,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 05:45:01 UTC 2024,,,,,,,,,,"0|z1nl3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 12:21;m.orazow;Hey [~mapohl], let me take this task also. Could you please assign it to me?;;;","16/Apr/24 02:48;m.orazow;Hey all,

I have add changes to remove the `IT_CASE_S3_*` variables. There is one issue I want to discuss.

There is an integration test case in `flink-yarn` module, it is `[YarnFileStateTestS3ITCase.java|https://github.com/apache/flink/blob/master/flink-yarn/src/test/java/org/apache/flink/yarn/YarnFileStageTestS3ITCase.java]`. This file also depends on the S3 variables. It also uses the S3 Native (s3n) in one of the tests, when run this test fails.

 

-However, it is not enabled in the CI. The `flink-yarn` module is not used in the tests (or I could not find). It is not listed on the [stage.sh|https://github.com/apache/flink/blob/master/tools/ci/stage.sh] file.-

It is enabled. But still none of two tests is run, from one of recent logs:
{code:java}
 Apr 16 02:17:12 02:17:12.837 [INFO] Running org.apache.flink.yarn.YarnFileStageTestS3ITCase
27911Apr 16 02:17:12 02:17:12.936 [INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.089 s -- in org.apache.flink.yarn.YarnFileStageTestS3ITCase{code}
 

The NativeS3Filesystem does not work with Minio, since it directly goes to the default `.amazonaws.` endpoint. The S3 native library is deprecated in the 3.3.4+ version of `hadoop-aws` library, but not used for yarn module for now.

 

I would suggest to remove the `s3n` test from the YarnFileStageTestS3ITCase test. What do you think?;;;","16/Apr/24 06:36;m.orazow;In the Azure Pipeline the YarnFileStageTestS3ITCase is skipped:
{code:java}
Apr 16 03:01:16 03:01:16.491 [INFO] Running org.apache.flink.yarn.YarnFileStageTestS3ITCase
Apr 16 03:01:24 03:01:24.104 [WARNING] Tests run: 8, Failures: 0, Errors: 0, Skipped: 8, Time elapsed: 7.611 s -- in org.apache.flink.yarn.YarnFileStageTestS3ITCase {code}
 ;;;","16/Apr/24 06:42;m.orazow;Hey [~mapohl],

On the GitHub CI, the S3 `IT_CASE_S3_*` parameters are not set? They seem to be empty, and tests depending on it are skipped.


For secrets they should have been displayed with `*****`:
{code:java}
Run PROFILE=""-Dinclude_hadoop_aws"" PROFILE=""$PROFILE -Pgithub-actions"" ./tools/azure-pipelines/uploading_watchdog.sh \
2  PROFILE=""-Dinclude_hadoop_aws"" PROFILE=""$PROFILE -Pgithub-actions"" ./tools/azure-pipelines/uploading_watchdog.sh \
3      ./tools/ci/test_controller.sh connect
4  shell: sh -e {0}
5  env:
6    MOUNTED_WORKING_DIR: /__w/flink/flink
7    CONTAINER_LOCAL_WORKING_DIR: /root/flink
8    FLINK_ARTIFACT_DIR: /root/artifact-directory
9    FLINK_ARTIFACT_FILENAME: flink_artifacts.tar.gz
10    MAVEN_REPO_FOLDER: /root/.m2/repository
11    MAVEN_ARGS: -Dmaven.repo.local=/root/.m2/repository
12    DOCKER_IMAGES_CACHE_FOLDER: /root/.docker-cache
13    GHA_JOB_TIMEOUT: 240
14    GHA_PIPELINE_START_TIME: 2024-04-16 01:46:07+00:00
15    JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64
16    PATH: /usr/lib/jvm/java-8-openjdk-amd64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
17    IT_CASE_S3_BUCKET: 
18    IT_CASE_S3_ACCESS_KEY: 
19    IT_CASE_S3_SECRET_KEY: 
20[INFO] GitHub Actions environment detected: ./tools/azure-pipelines/uploading_watchdog.sh will rely on GHA-specific environment variables.
21[INFO] GitHub Actions environment detected: ./tools/azure-pipelines/uploading_watchdog.sh will export the variables in the GHA-specific way. {code}


Please let me know if we need to create a new issue to set them and enable tests. This issue is about removing them :) Maybe we can discuss how we want to proceed.

I would vote to remove the `s3n` from the Yarn integration test and use Minio for all of them;;;","18/Apr/24 05:45;m.orazow;The [YarnFileStageTestS3ITCase|https://github.com/apache/flink/blob/master/flink-yarn/src/test/java/org/apache/flink/yarn/YarnFileStageTestS3ITCase.java#L131] test is skipped because of the assumption UUID.randomUUID() path does not exist.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSON functions have wrong operand checker,FLINK-34507,13569633,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dwysakowicz,dwysakowicz,23/Feb/24 11:14,23/Feb/24 11:14,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"I believe that all JSON functions (`JSON_VALUE`, `JSON_QUERY`, ...) have wrong operand checker.

As far as I can tell the first argument (the JSON) should be a `STRING` argument. That's what all other systems do (some accept clob/blob additionally e.g. ORACLE).

We via Calcite accept `ANY` type there, which I believe is wrong: https://github.com/apache/calcite/blob/c49792f9c72159571f898c5fca1e26cba9870b07/core/src/main/java/org/apache/calcite/sql/fun/SqlJsonValueFunction.java#L61",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-23 11:14:53.0,,,,,,,,,,"0|z1nl0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Do not copy ""file://"" schemed artifact in standalone application modes",FLINK-34506,13569625,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ferenc-csaky,ferenc-csaky,23/Feb/24 09:57,23/Feb/24 09:57,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Client / Job Submission,,,,0,,,,,"In standalone application mode, if an artifact is passed via a path witohut prefix, the file will be copied to `user.artifacts.base-dir`, although it should not be, as it can accessable locally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-23 09:57:38.0,,,,,,,,,,"0|z1nkz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate WindowGroupReorderRule,FLINK-34505,13569615,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,23/Feb/24 09:26,08/Apr/24 09:27,04/Jun/24 20:40,,1.20.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-23 09:26:50.0,,,,,,,,,,"0|z1nkww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid the parallelism adjustment when the upstream shuffle type doesn't have keyBy,FLINK-34504,13569610,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,23/Feb/24 08:17,26/Feb/24 13:53,04/Jun/24 20:40,26/Feb/24 13:53,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,,"JobVertexScaler#scale has a optimization: Try to adjust the parallelism such that it divides the number of key groups without a remainder =>  data is evenly spread across subtasks.

 

It's only useful when the upstream shuffle type has keyBy. We should avoid this optimization when the upstream shuffle type doesn't have keyBy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 13:52:56 UTC 2024,,,,,,,,,,"0|z1nkvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 13:52;fanrui;Merged to main(1.8.0) via: f4b04ec5f47358829ee279806d2f74b1f1641ee2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate JoinDeriveNullFilterRule,FLINK-34503,13569583,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,23/Feb/24 04:26,15/May/24 05:30,04/Jun/24 20:40,,1.20.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-23 04:26:52.0,,,,,,,,,,"0|z1nkps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support calculating network memory for forward and rescale edge,FLINK-34502,13569576,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,23/Feb/24 02:46,23/Feb/24 15:49,04/Jun/24 20:40,23/Feb/24 15:49,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,,"This is follow up Jira of FLINK-34471.

FLINK-34471 assuming all connections type are ALL_TO_ALL. This Jira will optimize it to save some network memory for forward and rescale connection.

Following is the optimization logic:
 * When the connection type is FORWARD and the parallelism of 2 tasks are same. We calculate network memory based on FORWARD connection.
 * When the connection type is FORWARD and the parallelism of 2 tasks are not same. We calculate network memory based on RESCALE connection.
 * When the connection type is RESCALE. We calculate network memory based on RESCALE connection.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 23 15:49:41 UTC 2024,,,,,,,,,,"0|z1nko8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/24 15:49;fanrui;Merged via main(1.8.0) via : 304fca82ccc153c0745c468893a8930e9b9be806;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In AsyncWaitOperator, ResultHandler.completeExceptionally() triggeres task/job restarts after max retry is reached",FLINK-34501,13569566,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sdineshkumar1985,sdineshkumar1985,22/Feb/24 22:41,28/Feb/24 02:55,04/Jun/24 20:40,,,,,,,,,,,,,API / DataStream,,,,0,pull-request-available,,,,"In AsyncWaitOperator, ResultHandler.completeExceptionally() triggeres task/job restarts after max retry is reached.

Instead can we allow user to decide on how to handle the exception to avoid job restarts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-22 22:41:20.0,,,,,,,,,,"0|z1nkm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33261 Support Setting Parallelism for Table/SQL Sources,FLINK-34500,13569527,13566678,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,sudewei.sdw,sudewei.sdw,22/Feb/24 16:16,27/Feb/24 15:26,04/Jun/24 20:40,27/Feb/24 15:26,1.19.0,,,,,,,1.19.0,,,,Connectors / Parent,Table SQL / API,,,0,,,,,"This issue aims to verify [FLIP-367|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=263429150].


Volunteers can verify it by following the [doc changes|https://github.com/apache/flink/pull/24234]. Since currently only the pre-defined DataGen connector and user-defined connector supports setting source parallelism, volunteers can verify it through DataGen Connector.

The basic steps include:
1. Start a Flink cluster and submit a Flink SQL Job to the cluster.
2. In this Flink Job, use the DataGen SQL Connector to generate data.
3. Specify the parameter scan.parallelism in DataGen connector options as user-defined parallelism instead of default parallelism.
4. Observe whether the parallelism of the source has changed on the job graph of the Flink Application UI, and whether the shuffle mode is correct.


If everything is normal, you will see that the parallelism of the source operator is indeed different from that of downstream, and the shuffle mode is rebalanced by default.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33261,,FLINK-34305,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 15:26:26 UTC 2024,,,,,,,,,,"0|z1nkdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/24 15:26;yunta;I started a local standalone cluster, and submitted the SQL queries via sql-client.
DataGen source with parallelism different with `parallelism.default`, no matter larger or smaller, the source operator will be the one with exact parallelism just as `scan.parallelism` configured. 
If we did not add `scan.parallelism` option, the source-parallelism will be the same as the `parallelism.default` option.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration#toString should hide sensitive values,FLINK-34499,13569492,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Feb/24 11:00,29/Feb/24 07:00,04/Jun/24 20:40,29/Feb/24 07:00,1.17.2,1.18.1,1.19.0,1.20.0,,,,1.17.3,1.18.2,1.19.0,,Runtime / Configuration,,,,0,pull-request-available,,,,"Time and time again people log the entire Flink configuration for no reason, risking that sensitive values are logged in plain text.

We should make this harder by changing {{Configuration#toString}} to automatically hide sensitive values, for example like this:

{code}
    @Override
    public String toString() {
        return ConfigurationUtils
                .hideSensitiveValues(this.confData.entrySet().stream().collect(
                        Collectors.toMap(Map.Entry::getKey, entry -> entry.getValue().toString())))
                .toString();
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 29 07:00:38 UTC 2024,,,,,,,,,,"0|z1nk5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Feb/24 07:00;mapohl;master: [9e81177e44d63501b360b1a246a16ea3faecb548|https://github.com/apache/flink/commit/9e81177e44d63501b360b1a246a16ea3faecb548]
1.19: [5016325b6ec930a3f5cd3b186c185e004ede8691|https://github.com/apache/flink/commit/5016325b6ec930a3f5cd3b186c185e004ede8691]
1.18: [e770cef5d1df282fe28deb5f1309873b8342d046|https://github.com/apache/flink/commit/e770cef5d1df282fe28deb5f1309873b8342d046]
1.17: [72fbc89777286d9cda46a80231b2db11c21ced0d|https://github.com/apache/flink/commit/72fbc89777286d9cda46a80231b2db11c21ced0d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GSFileSystemFactory logs full Flink config,FLINK-34498,13569491,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jeyhunkarimov,chesnay,chesnay,22/Feb/24 10:56,29/Feb/24 06:58,04/Jun/24 20:40,29/Feb/24 06:57,1.18.1,,,,,,,1.19.0,,,,Connectors / FileSystem,,,,0,pull-request-available,,,,"This can cause secrets from the config to be logged.
{code}
    @Override
    public void configure(Configuration flinkConfig) {
        LOGGER.info(""Configuring GSFileSystemFactory with Flink configuration {}"", flinkConfig);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 29 06:57:55 UTC 2024,,,,,,,,,,"0|z1nk5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 14:08;mapohl;Can we agree on closing this issue as a duplicate of FLINK-34499? It feels like FLINK-34499 would fix the same issue on a larger scale. Or am I missing something here?;;;","29/Feb/24 06:57;mapohl;master: [8bb4457b6cf13d85639e914a8d959e39c2257d66|https://github.com/apache/flink/commit/8bb4457b6cf13d85639e914a8d959e39c2257d66]
1.19: [628ae78ab304a7543a8d5566a7dd6cbcb336d61b|https://github.com/apache/flink/commit/628ae78ab304a7543a8d5566a7dd6cbcb336d61b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid using system classloader in SerializedThrowableDeserializer  ,FLINK-34497,13569488,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wangsan,wangsan,22/Feb/24 10:35,22/Feb/24 12:55,04/Jun/24 20:40,,1.17.2,1.18.1,,,,,,,,,,Runtime / REST,,,,0,pull-request-available,,,,"SerializedThrowableDeserializer is now using `ClassLoader.getSystemClassLoader()` when deserializing `SerializedThrowable`. But when using flink-client in systems like spring boot, we will get exceptions like this:
{code:java}
java.lang.ClassNotFoundException: org.apache.flink.util.SerializedThrowable
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78)
	at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2034)
	at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)
	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)
	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)
	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)
	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:539)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:527)
	at org.apache.flink.runtime.rest.messages.json.SerializedThrowableDeserializer.deserialize(SerializedThrowableDeserializer.java:69)
	at org.apache.flink.runtime.rest.messages.json.JobResultDeserializer.deserialize(JobResultDeserializer.java:106)
	at org.apache.flink.runtime.rest.messages.json.JobResultDeserializer.deserialize(JobResultDeserializer.java:50)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:542)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:564)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1405)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:185)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4706)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2948)
	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:635)
	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$6(RestClient.java:626)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1150)
	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:482)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
{code}
This is because Spring Boot is using [The Executable Jar Format,|https://docs.spring.io/spring-boot/docs/current/reference/html/executable-jar.html#appendix.executable-jar.restrictions] which contains Nested JARs, the system classloader is not able to load the class in the nested jar, thus will lead to class not found exception. We should use current context classloader instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-22 10:35:43.0,,,,,,,,,,"0|z1nk4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Classloading deadlock between ExecNodeMetadataUtil and JsonSerdeUtil,FLINK-34496,13569476,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Feb/24 09:43,23/Feb/24 09:32,04/Jun/24 20:40,23/Feb/24 09:32,1.18.1,,,,,,,1.18.2,1.19.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,"This is a fun one!

ExecNodeMetadataUtil and JsonSerdeUtil have a circular dependency in their static initialization, which can cause a classloading lockup when 2 threads are running the class initialization of each class at the same time because during class initialization they hold a lock.

https://ternarysearch.blogspot.com/2013/07/static-initialization-deadlock.html

JsonSerdeUtils#createFlinkTableJacksonModule calls into the ExecNodeMetadataUtil, while ExecNodeMetadataUtil#addToLookupMap calls into the JsonSerdeUtils.

{code}
 ""ForkJoinPool-3-worker-11"" #25 daemon prio=5 os_prio=0 cpu=219.87ms elapsed=995.99s tid=0x00007ff11c50e000 nid=0xf0fc in Object.wait()  [0x00007ff12a4f3000]
    java.lang.Thread.State: RUNNABLE
 	at o.a.f.t.p.plan.nodes.exec.serde.JsonSerdeUtil.createFlinkTableJacksonModule(JsonSerdeUtil.java:133)
 	at o.a.f.t.p.plan.nodes.exec.serde.JsonSerdeUtil.<clinit>(JsonSerdeUtil.java:111)

""ForkJoinPool-3-worker-7"" #23 daemon prio=5 os_prio=0 cpu=54.83ms elapsed=996.00s tid=0x00007ff11c50c000 nid=0xf0fb in Object.wait()  [0x00007ff12a5f4000]
   java.lang.Thread.State: RUNNABLE
	at o.a.f.t.p.plan.utils.ExecNodeMetadataUtil.addToLookupMap(ExecNodeMetadataUtil.java:235)
	at o.a.f.t.p.plan.utils.ExecNodeMetadataUtil.<clinit>(ExecNodeMetadataUtil.java:156)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 23 09:32:49 UTC 2024,,,,,,,,,,"0|z1nk20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 10:10;chesnay;A quick fix would be moving JsonSerdeUtil#hasJsonCreatorAnnotation to a separate class, breaking the cyclic dependency.;;;","23/Feb/24 09:32;chesnay;master: 
2d78c10211272a264712e86192c4dfc59c6a5521
6c8f3a0799c609d8076f782e5334e389e4d92eee
1.19:
dd77ee5a2501a6750387126c347cf540f3fb172b
1.18:
39ed3cf279d61e4472e1c30a17927992236df467;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resuming Savepoint (rocks, scale up, heap timers) end-to-end test failure due to FileNotFoundException",FLINK-34495,13569459,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,mapohl,mapohl,22/Feb/24 07:35,02/Apr/24 16:00,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57760&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=5d91035e-8022-55f2-2d4f-ab121508bf7e&l=2010

{code}
java.util.concurrent.ExecutionException: java.io.IOException: Could not open output stream for state backend
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:?]
        at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:?]
        at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:511) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:54) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) [flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.io.IOException: Could not open output stream for state backend
        at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.createStream(FsCheckpointStreamFactory.java:461) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.flushToFile(FsCheckpointStreamFactory.java:308) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:284) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.contrib.streaming.state.RocksDBStateUploader.uploadLocalFileToCheckpointFs(RocksDBStateUploader.java:148) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.contrib.streaming.state.RocksDBStateUploader.lambda$createUploadFutures$0(RocksDBStateUploader.java:111) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:32) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[?:?]
        ... 3 more
Caused by: java.io.FileNotFoundException: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-32462964103/savepoint-e2e-test-chckpt-dir/3c9ffc670ead2cb3c4118410cbef3b72/chk-12/3415a2f2-b0c8-4a07-b4a1-bb6cc58a7c56 (No such file or directory)
        at java.io.FileOutputStream.open0(Native Method) ~[?:?]
        at java.io.FileOutputStream.open(FileOutputStream.java:298) ~[?:?]
        at java.io.FileOutputStream.<init>(FileOutputStream.java:237) ~[?:?]
        at java.io.FileOutputStream.<init>(FileOutputStream.java:187) ~[?:?]
        at org.apache.flink.core.fs.local.LocalDataOutputStream.<init>(LocalDataOutputStream.java:50) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.core.fs.local.LocalFileSystem.create(LocalFileSystem.java:266) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.create(SafetyNetWrapperFileSystem.java:130) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.core.fs.EntropyInjector.createEntropyAware(EntropyInjector.java:76) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.createStream(FsCheckpointStreamFactory.java:451) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.flushToFile(FsCheckpointStreamFactory.java:308) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:284) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.contrib.streaming.state.RocksDBStateUploader.uploadLocalFileToCheckpointFs(RocksDBStateUploader.java:148) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.contrib.streaming.state.RocksDBStateUploader.lambda$createUploadFutures$0(RocksDBStateUploader.java:111) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:32) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[?:?]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 16:00:50 UTC 2024,,,,,,,,,,"0|z1njy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 09:13;zakelly;Thanks for reporting this! But I'm afraid it is probably not caused by the checkpoint failure, since the 'o.a.f.r.c.CheckpointException' is an [allowed exception|https://github.com/apache/flink/blob/80090c76f881634cf2472d9f65f2fa835eef1fc3/flink-end-to-end-tests/test-scripts/common.sh#L477]. There's only first 500 lines of log print. We can see full logs for more details.;;;","22/Feb/24 10:31;mapohl;Ah, thanks for pointing it out. I was already looking for the whitelist but didn't find anything there, yet. I'm gonna update the Jira issue description. It's actually caused by a {{FileNotFoundException}} Do you have capacity to look into the issue?;;;","22/Feb/24 11:36;zakelly;[~mapohl] I would like to, but I have no idea how to retrieve the original logs. Is there any guide on this?;;;","22/Feb/24 12:34;mapohl;# The failed step {{Run e2e tests}} can be retrieved from the link I shared in the Jira description
# From there you can jump to the job level [e2e_1_cron_jdk11|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57760&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe] (jdk11 is the job profile label and e2e_1 is the stage name)
# The logs now include a link [1 artifact|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57760&view=artifacts&pathAsName=false]
# Following the link will bring you to the artifact overview of this job (which, of course, doesn't have only 1 artifact as claimed in the previous step :-D )
# In the overview, you have to look for the correct build artifact based on the name: It should have the job profile label {{jdk11}} and the stage name {{e2e_1}} which is the 40th entry in the list leading to the download link (three dots button on the right side of the entry when hovering over it): https://artprodsu6weu.artifacts.visualstudio.com/A2d3c0ac8-fecf-45be-8407-6d87302181a9/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/artifact/cGlwZWxpbmVhcnRpZmFjdDovL2FwYWNoZS1mbGluay9wcm9qZWN0SWQvOTg0NjM0OTYtMWFmMi00NjIwLThlYWItYTJlY2MxYTJlNmZlL2J1aWxkSWQvNTc3NjAvYXJ0aWZhY3ROYW1lL2xvZ3MtY3Jvbl9qZGsxMS1lMmVfMV9jcm9uX2pkazExLTE3MDg1NjI1MDE1/content?format=zip;;;","23/Feb/24 06:33;zakelly;[~mapohl]  That's very helpful, thanks a lot! I'll look into it.;;;","26/Feb/24 06:22;zakelly;Well, the exception is shown in the description. I guess this comes from:
# When triggering stop-with-savepoint, the on-going periodic checkpoint is abort.
# The JM deletes the private state directory on cp abort.
# While in the meantime, the TMs are still writing to that directory, throwing FileNotFoundException.

So a possible walk-around is enable incremental checkpoint for this test, since the TM will write into the shared directory which won't be deleted on cp abort.;;;","26/Feb/24 09:27;mapohl;are we sure that we can just ignore this error? It feels odd to me that we don't handle this concurrency issue properly within the production code. I also cannot find any FileNotFoundException being white listed in other test cases (e.g. in the list of allowed exceptions you already [mentioned above|https://issues.apache.org/jira/browse/FLINK-34495?focusedCommentId=17819546&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17819546]) which makes me believe that this is rather a failure we want to look into. [~roman] do you have an opinion on that?;;;","26/Feb/24 10:28;zakelly;[~mapohl] It is not convenient to avoid this, since the checkpoint notification is best-effort and there is no ack from TM to JM. JM does not know when it's 'safe' to delete the private state directory. But yes, it should be addressed and the notification or the state file ownership should better be re-designed.

I suggest a dedicated test to reproduce this since this may happen rarely in this test I guess. ;;;","06/Mar/24 10:18;mapohl;1.19: https://github.com/apache/flink/actions/runs/8165883662/job/22324004048#step:14:1440;;;","02/Apr/24 16:00;rskraba;1.19, JDK21: [https://github.com/apache/flink/actions/runs/8502821702/job/23287735549#step:14:2374]

 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Migrate ReplaceIntersectWithSemiJoinRule,FLINK-34494,13569438,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,22/Feb/24 04:13,11/Mar/24 20:28,04/Jun/24 20:40,11/Mar/24 20:28,1.20.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 20:27:52 UTC 2024,,,,,,,,,,"0|z1njtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 09:36;jackylau;hi [~dwysakowicz] will you help review this?;;;","11/Mar/24 20:27;Sergey Nuyanzin;Merged as [5f38a5702fc3629e29cae0d531912b1ae9b47e08|https://github.com/apache/flink/commit/5f38a5702fc3629e29cae0d531912b1ae9b47e08];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate ReplaceMinusWithAntiJoinRule,FLINK-34493,13569437,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jackylau,jackylau,jackylau,22/Feb/24 04:12,06/Mar/24 09:47,04/Jun/24 20:40,06/Mar/24 09:47,1.20.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 09:47:02 UTC 2024,,,,,,,,,,"0|z1njtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 09:47;dwysakowicz;Implemented in 6a12668bcfe651fa938517eb2da4d537ce6ce668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix scala style comment link when migrate scala to java,FLINK-34492,13569433,13565339,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,22/Feb/24 03:16,01/Mar/24 09:54,04/Jun/24 20:40,01/Mar/24 09:54,1.20.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,," 

scala [[org.apache.calcite.rel.rules.CalcMergeRule]]

java  {@link org.apache.calcite.rel.rules.CalcMergeRule}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 01 09:54:27 UTC 2024,,,,,,,,,,"0|z1njsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 09:54;jiabaosun;master: 46cbf22147d783fb68f77fad95161dc5ef036c96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move from experimental support to production support for Java 17,FLINK-34491,13569425,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dhrapate,dhrapate,22/Feb/24 01:26,29/Feb/24 22:44,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,,,,,0,,,,,"This task is to move away from experimental support for Java 17 to production support so that teams running Flink in production can migrate to Java 17 successfully

*Background:*

Flink supports protobuf dataformat to exchange messages between different operators and the serialization and deserialization of these protobufs messages are performed by library called ""Kryo"". In order to move away from experimental support of Java 17 released as part of Flink 1.18.1, the Kryo library in Flink 1.18.1 needs to be updated from 2.24.0 to 5.5.0 because Kryo 2.24.0 does not support Java 17. This improvement plan is tracked as part of this ticket [https://cwiki.apache.org/confluence/display/FLINK/FLIP-317%3A+Upgrade+Kryo+from+2.24.0+to+5.5.0]

All Flink applications using protobuf currently generate state with Kryo v2. Once the above improvement plan is complete all Flink applications will fully support reading that state and write newer state with Kryo v5. However, latest Kryo v5 doesn't support snapshots made by a previous Kryo v2. This will prevent applications which are using snapshot mechanism to deploy their jobs to latest Flink version with Kryo v5 support without a bridge version running on Java 11. Applications will have to run on a bridge release version that will read their state with Kryo v2 data and write it with Kryo v5 data before upgrading to a future version of Flink that completely drops support for Kryo v2.

Basically, Flink applications using protobuf dataformat cannot move directly from Java 8 to Java 17 without downtime after the kryo v5 release in Flink. Applications will need to first move to Java 11 (bridging version) and then move to Java 17 to have a safe deployment.

*Blocker for this task:*

Upgrade to Kryo 5.5.0 which supports Java 17 and a path for snapshot migration
https://issues.apache.org/jira/browse/FLINK-3154.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-22 01:26:17.0,,,,,,,,,,"0|z1njqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-kinesis not correctly supporting credential chaining,FLINK-34490,13569412,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,a.pilipenko,eddieramirez,eddieramirez,21/Feb/24 22:33,23/Feb/24 21:19,04/Jun/24 20:40,,1.17.2,aws-connector-4.2.0,,,,,,,,,,Connectors / Kinesis,,,,0,,,,,"When using AWS credential chaining, `{{{}flink-connector-kinesis{}}}` does not correctly follow the chain of credentials.

 

*Expected Result*

 `{{{}flink-connector-kinesis{}}}`  should follow the `{{{}source_profile{}}}` for each respective profile in `{{{}~/.aws/config{}}}` to ultimately determine credentials.

 

*Observed Result*

 `{{{}flink-connector-kinesis{}}}` only follows the first matching `{{{}source_profile{}}}` specified in `{{{}~/.aws/config{}}}` and then errors out because there is no credentials for that profile.
{code:java}
org.apache.flink.kinesis.shaded.com.amazonaws.SdkClientException: Unable to load credentials into profile [profile intermediate-role]: AWS Access Key ID is not specified
{code}
 

*Configuration*

connector config
{code:java}
aws.credentials.provider: PROFILE
aws.credentials.profile.name: flink-access-role{code}
 

aws `{{{}~/.aws/config{}}}` file
{code:java}
[profile flink-access-role]
role_arn = arn:aws:iam::xxxxxxxxx:role/flink-access-role
source_profile = intermediate-role

[profile intermediate-role]
role_arn = arn:aws:iam::xxxxxxxxx:role/intermediate-role
source_profile = aws-sso-role

[profile aws-sso-role]
sso_session = idc
sso_role_name = xxxxx
sso_account_id = xxxxx
credential_process = aws configure export-credentials --profile=aws-sso-role

[sso-session idc]
sso_start_url = xxxxx
sso_region = xxxxx
sso_registration_scopes = sso:account:access
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/24 22:23;eddieramirez;Flink Credential Chaining.png;https://issues.apache.org/jira/secure/attachment/13066951/Flink+Credential+Chaining.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 12:55:20 UTC 2024,,,,,,,,,,"0|z1njns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 12:55;dannycranmer;Assigning to [~a.pilipenko] to take a look.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New File Sink end-to-end test timed out,FLINK-34489,13569374,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mapohl,mapohl,21/Feb/24 16:11,27/Feb/24 14:33,04/Jun/24 20:40,27/Feb/24 14:33,1.19.0,1.20.0,,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57707&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3726

{code}
eb 21 07:26:03 Number of produced values 10770/60000
Feb 21 07:39:50 Test (pid: 151375) did not finish after 900 seconds.
Feb 21 07:39:50 Printing Flink logs and killing it:
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 13:56:48 UTC 2024,,,,,,,,,,"0|z1njfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 16:16;mapohl;Same issue in {{Streaming File Sink end-to-end test}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57702&view=logs&j=dc1bf4ed-4646-531a-f094-e103042be549&t=fb3d654d-52f8-5b98-fe9d-b18dd2e2b790&l=3313;;;","27/Feb/24 12:10;mapohl;The first case is caused by a TaskManager not coming up on time leaving the job with an expected parallelism of 4 hanging on 3 TaskManagers/Slots. The failed TM reveals the following error logs:
{code}
[...]
2024-02-21 07:30:28,915 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address pekko.ssl.tcp://flink@localhost:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address pekko.ssl
.tcp://flink@localhost:6123/user/rpc/resourcemanager_*.
2024-02-21 07:30:38,939 WARN  org.apache.pekko.remote.EndpointReader                       [] - Discarding inbound message to [Actor[pekko://flink/temp/_user_rpc_resourcemanager_*$o]] in read-only association to [pekko.ssl.tcp://flink@localhost:6123]. If this happens often you may 
consider using pekko.remote.use-passive-connections=off or use Artery TCP.
2024-02-21 07:30:48,954 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address pekko.ssl.tcp://flink@localhost:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address pekko.ssl
.tcp://flink@localhost:6123/user/rpc/resourcemanager_*.
2024-02-21 07:30:58,353 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Fatal error occurred in TaskExecutor pekko.ssl.tcp://flink@localhost:33341/user/rpc/taskmanager_0.
org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration PT5M. This indicates a problem with this instance. Terminating now.
        at org.apache.flink.runtime.taskexecutor.TaskExecutor.registrationTimeout(TaskExecutor.java:1614) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$startRegistrationTimeout$18(TaskExecutor.java:1599) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451) ~[flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451) ~[flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218) ~[flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168) ~[flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) [flink-rpc-akka1f9d7113-8709-4799-8cb2-0d9f15b6f908.jar:1.20-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_402]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_402]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_402]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_402]
[...]
{code};;;","27/Feb/24 13:56;mapohl;Same with the 2nd case - The TaskManager #4 failed fatally:
{code}
[...]
2024-02-21 05:34:44,967 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address pekko.ssl.tcp://flink@localhost:6123/user/rpc/resourcemanager_*, retrying in 10000 ms: Could not connect to rpc endpoint under address pekko.ssl.tcp://flink@localhost:6123/user/rpc/resourcemanager_*.
2024-02-21 05:34:54,407 ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Fatal error occurred in TaskExecutor pekko.ssl.tcp://flink@localhost:45603/user/rpc/taskmanager_0.
org.apache.flink.runtime.taskexecutor.exceptions.RegistrationTimeoutException: Could not register at the ResourceManager within the specified maximum registration duration PT5M. This indicates a problem with this instance. Terminating now.
        at org.apache.flink.runtime.taskexecutor.TaskExecutor.registrationTimeout(TaskExecutor.java:1614) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$startRegistrationTimeout$18(TaskExecutor.java:1599) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451) ~[flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451) ~[flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218) ~[flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168) ~[flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) [flink-rpc-akkafc9c0aa2-35db-418c-8ff3-f8872cca24d7.jar:1.19-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387) [?:?]
        at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312) [?:?]
        at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843) [?:?]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808) [?:?]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188) [?:?]
[...]
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate snapshot deployment into GHA nightly workflow,FLINK-34488,13569362,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,21/Feb/24 14:20,22/Feb/24 09:40,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,,Build System / CI,,,,0,github-actions,,,,"Analogously to the [Azure Pipelines nightly config|https://github.com/apache/flink/blob/e923d4060b6dabe650a8950774d176d3e92437c2/tools/azure-pipelines/build-apache-repo.yml#L103] we want to deploy the snapshot artifacts in the GHA nightly workflow as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 09:40:30 UTC 2024,,,,,,,,,,"0|z1njco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 09:40;mapohl;We should use the Apache Infra nightly snapshot repo instead of pushing to an s3 bucket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate tools/azure-pipelines/build-python-wheels.yml into GHA nightly workflow,FLINK-34487,13569360,13562450,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,m.orazow,mapohl,mapohl,21/Feb/24 14:16,04/Jun/24 14:27,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,,Build System / CI,,,,0,github-actions,pull-request-available,,,"Analogously to the [Azure Pipelines nightly config|https://github.com/apache/flink/blob/e923d4060b6dabe650a8950774d176d3e92437c2/tools/azure-pipelines/build-apache-repo.yml#L183] we want to generate the wheels artifacts in the GHA nightly workflow as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 24 14:11:18 UTC 2024,,,,,,,,,,"0|z1njc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 07:16;m.orazow;Hey [~mapohl], I'd like to work on this. Could you please assign this to me? Thanks!;;;","01/Mar/24 10:40;mapohl;Thanks for contributing to this part of the code base. That will help us moving away from Azure Pipelines. :);;;","01/Mar/24 11:14;m.orazow;Yes definitely! It is good initiative to move to GitHub actions as open-source project (y);;;","24/Mar/24 14:11;m.orazow;Hello [~mapohl], could you please have a look to the attached PR? Thanks a lot (y) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation on how to add the shared utils as a submodule to the connector repo,FLINK-34486,13569342,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,21/Feb/24 11:57,21/Feb/24 11:57,04/Jun/24 20:40,,connector-parent-1.1.0,,,,,,,,,,,Connectors / Common,,,,0,starter,,,,"[apache/flink-connector-shared-utils:README.md|https://github.com/apache/flink-connector-shared-utils/blob/release_utils/README.md] doesn't state how a the shared utils shall be added as a submodule to a connector repository. But this is expected from within [connector release documentation|https://cwiki.apache.org/confluence/display/FLINK/Creating+a+flink-connector+release#Creatingaflinkconnectorrelease-Buildareleasecandidate]:
{quote}
The following sections assume that the release_utils branch from flink-connector-shared-utils is mounted as a git submodule under tools/releasing/shared, you can update the submodule by running  git submodule update --remote (or git submodule update --init --recursive if the submodule wasn't initialized, yet) to use latest release utils, you need to mount the  flink-connector-shared-utils  as a submodule under the tools/releasing/shared if it hasn't been mounted in the connector repository. See the README for details.
{quote}

Let's update the README accordingly and add a link to {{README}} in the connector release documentation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-21 11:57:19.0,,,,,,,,,,"0|z1nj88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Token delegation doesn't work with Presto S3 filesystem,FLINK-34485,13569337,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Feb/24 11:37,22/Feb/24 10:13,04/Jun/24 20:40,22/Feb/24 10:13,1.18.1,,,,,,,1.20.0,,,,Connectors / FileSystem,,,,0,pull-request-available,,,,"AFAICT it's not possible to use token delegation with the Presto filesystem.
The token delegation relies on the {{DynamicTemporaryAWSCredentialsProvider}}, but it doesn't have a constructor that presto required (ruling out presto.s3.credentials-provider), and other providers can't be used due to FLINK-13602.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 10:13:47 UTC 2024,,,,,,,,,,"0|z1nj74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 10:13;chesnay;master: cf5bb80b5fd965928ff75d415dd93e0b12fa1b49;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split 'state.backend.local-recovery' into two options for checkpointing and recovery,FLINK-34484,13569334,13566461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,zakelly,zakelly,21/Feb/24 10:50,15/Mar/24 04:24,04/Jun/24 20:40,15/Mar/24 04:24,,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,,,,,,,,,,,,,FLINK-34454,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 15 04:24:05 UTC 2024,,,,,,,,,,"0|z1nj6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 04:24;zakelly;Merged into master via 649e2b412e10c7f698721cbbd8697f9b02248977;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the documentation of 'state.checkpoints.dir' and 'state.checkpoint-storage',FLINK-34483,13569332,13566461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,zakelly,zakelly,21/Feb/24 10:49,08/Mar/24 09:30,04/Jun/24 20:40,08/Mar/24 09:30,,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 09:29:41 UTC 2024,,,,,,,,,,"0|z1nj60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 09:29;Yanfei Lei;Merged 1d8cbe9 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename options for checkpointing,FLINK-34482,13569331,13566461,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,zakelly,zakelly,21/Feb/24 10:48,03/Jun/24 03:51,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,,,,,,,,,,,,,FLINK-34456,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-21 10:48:30.0,,,,,,,,,,"0|z1nj5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate SetOpRewriteUtil,FLINK-34481,13569316,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,21/Feb/24 09:37,01/Mar/24 07:48,04/Jun/24 20:40,01/Mar/24 07:48,1.20.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"we should Migrate SetOpRewriteUtil for 
ReplaceMinusWithAntiJoinRule

ReplaceMinusWithAntiJoinRule
RewriteIntersectAllRule
RewriteMinusAllRule",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 01 07:48:29 UTC 2024,,,,,,,,,,"0|z1nj2g:",9223372036854775807,merged f523b9d6191ecb584e36aa2aeffcd0659ce231f7,,,,,,,,,,,,,,,,,,,"01/Mar/24 07:48;jackylau;Merged f523b9d6191ecb584e36aa2aeffcd0659ce231f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add method to support user jar overwrite flink inner jar class when same class,FLINK-34480,13569315,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,JinxinTang,JinxinTang,21/Feb/24 09:36,21/Feb/24 09:52,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-21 09:36:55.0,,,,,,,,,,"0|z1nj28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix missed changelog configs in the documentation,FLINK-34479,13569287,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,21/Feb/24 05:39,21/Feb/24 09:55,04/Jun/24 20:40,21/Feb/24 09:55,1.19.0,1.20.0,,,,,,1.19.0,1.20.0,,,Documentation,,,,0,pull-request-available,,,,state_backend_changelog_section has been missed in the documentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 09:55:01 UTC 2024,,,,,,,,,,"0|z1niw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 09:55;masteryhx;Merged [{{6f7e841}}|https://github.com/apache/flink/commit/6f7e841dd4ba060092f4ea35ef616a204d593904] into master and [{{d19e886}}|https://github.com/apache/flink/commit/d19e886b3d26985f64a1cdcd9520c1802bdc0bf8] into release-1.19.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NoSuchMethod error for ""flink cancel $jobId"" via Command Line",FLINK-34478,13569281,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liuyi,liuyi,21/Feb/24 04:18,05/Mar/24 14:34,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Command Line Client,,,,0,,,,,"On 1.18.1 standalone mode (launched by ""\{flink}/bin/start-cluster.sh""), I hit "" [java.lang.NoSuchMethodError: 'boolean org.apache.commons.cli.CommandLine.hasOption(org.apache.commons.cli.Option)'|https://www.google.com/search?q=java.lang.NoSuchMethodError:+%27boolean+org.apache.commons.cli.CommandLine.hasOption%28org.apache.commons.cli.Option%29%27] "" when trying to cancel a job submitted via the UI by executing the Command Line ""{*}{flink}/bin/flink cancel _$jobId_{*}"". While clicking on ""Cancel Job"" link in the UI can cancel the job just fine, and ""flink run"" command line also works fine.

Has anyone seen same/similar behavior?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 14:34:25 UTC 2024,,,,,,,,,,"0|z1niuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/24 14:34;ferenc-csaky;Just tried this with the {{TopSpeedWindowing.jar}} streaming example, worked fine for me on an M1 macbook. Are you trying with a custom JAR? On what system?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support capture groups in REGEXP_REPLACE,FLINK-34477,13569232,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jeyhunkarimov,danderson,danderson,20/Feb/24 17:42,03/Mar/24 23:36,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,"For example, I would expect this query
{code:java}
select REGEXP_REPLACE('ERR1,ERR2', '([^,]+)', 'AA$1AA'); {code}
to produce
{code:java}
AAERR1AA,AAERR2AA{code}
but instead it produces
{code:java}
AA$1AA,AA$1AA{code}
With FLINK-9990 support was added for REGEXP_EXTRACT, which does provide access to the capture groups, but for many use cases supporting this in the way that users expect, in REGEXP_REPLACE, would be more natural and convenient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-20 17:42:25.0,,,,,,,,,,"0|z1nijs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Window TVFs with named parameters don't support column expansion,FLINK-34476,13569230,13549560,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,20/Feb/24 17:01,22/Feb/24 11:30,04/Jun/24 20:40,22/Feb/24 11:30,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"It seems named parameters still have issues with column expansion of virtual metadata column:

{code}

SELECT * FROM TABLE(  TUMBLE( DATA => TABLE gaming_player_activity_source, TIMECOL => DESCRIPTOR(meta_col), SIZE => INTERVAL '10' MINUTES));

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 11:30:28 UTC 2024,,,,,,,,,,"0|z1nijc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 11:30;twalthr;Fixed in master: e622205d0b74c6cbaf6fef6d8c11a397cdc30284
Fixed in release-1.19: f21ee01bc5969e86e014d90ac4c5196b63e5a959;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionDriverTest failed with exit code 2,FLINK-34475,13569215,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,20/Feb/24 14:29,22/Mar/24 16:07,04/Jun/24 20:40,20/Feb/24 15:37,1.18.1,,,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57649&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8746]
{code:java}
Feb 20 01:20:02 01:20:02.369 [ERROR] Process Exit Code: 2
Feb 20 01:20:02 01:20:02.369 [ERROR] Crashed tests:
Feb 20 01:20:02 01:20:02.369 [ERROR] org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverTest
Feb 20 01:20:02 01:20:02.369 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-34920,,,,,,,,,,,,,,,,,,"20/Feb/24 15:20;mapohl;FLINK-34475.log;https://issues.apache.org/jira/secure/attachment/13066909/FLINK-34475.log","20/Feb/24 16:25;mapohl;FLINK-34475.zookeeper-client.log;https://issues.apache.org/jira/secure/attachment/13066915/FLINK-34475.zookeeper-client.log","20/Feb/24 16:25;mapohl;FLINK-34475.zookeeper-server.log;https://issues.apache.org/jira/secure/attachment/13066914/FLINK-34475.zookeeper-server.log",,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 15:37:33 UTC 2024,,,,,,,,,,"0|z1nig0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 15:24;mapohl;The test that seems to fail in {{{}ZooKeeperLeaderElectionDriverTest#testElectionDriverLosesLeadership{}}}. At least that's the last test method for this class that's started and it doesn't have a corresponding log message stating that the test successfully ran:
{code:java}
================================================================================
01:16:27,800 [ForkJoinPool-39-worker-25] INFO  org.apache.flink.util.TestLoggerExtension                    [] - 
================================================================================
Test org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriverTest.testElectionDriverLosesLeadership[testElectionDriverLosesLeadership()] is running.
--------------------------------------------------------------------------------
01:16:27,802 [ForkJoinPool-39-worker-25] INFO  org.apache.curator.test.ZooKeeperServerEmbeddedAdapter       [] - Configure ZooKeeperServerEmbeddedAdapter with properties: {initLimit=10, syncLimit=5, localSessionsUpgradingEnabled=false, localSessionsEnabled=false, clientPort=37091, minSessionTimeout=1000, maxSessionTimeout=60000, tickTime=1000, dataDir=/tmp/DirectoryUtils760296815203509951, admin.enableServer=false}
01:16:27,833 [FileChannelManagerImpl-netty-shuffle shutdown hook] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-00ef6da7-3349-40d6-93cb-878c81abcd82
[...]{code}
There is an {{IllegalStateException}} being thrown in {{ZooKeeperLeaderElectionDriverTest#testPublishLeaderInformation}}. But that one seems to be unrelated because it can be reproduced when executing the test locally:
{code}
01:16:27,663 [ForkJoinPool-39-worker-25-EventThread] ERROR org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache [] - 
java.lang.IllegalStateException: Expected state [STARTED] was [STOPPED]
        at org.apache.flink.shaded.curator5.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:821) ~[flink-shaded-zookeeper-3-3.7.1-17.0.jar:3.7.1-17.0]
        at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkState(CuratorFrameworkImpl.java:457) ~[flink-shaded-zookeeper-3-3.7.1-17.0.jar:3.7.1-17.0]
        at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:484) ~[flink-shaded-zookeeper-3-3.7.1-17.0.jar:3.7.1-17.0]
        at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.wasDeleted(TreeCache.java:354) ~[flink-shaded-zookeeper-3-3.7.1-17.0.jar:3.7.1-17.0]
        at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.process(TreeCache.java:386) [flink-shaded-zookeeper-3-3.7.1-17.0.jar:3.7.1-17.0]
        at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:77) [flink-shaded-zookeeper-3-3.7.1-17.0.jar:3.7.1-17.0]
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:578) [flink-shaded-zookeeper-3-3.7.1-17.0.jar:3.7.1-17.0]
        at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553) [flink-shaded-zookeeper-3-3.7.1-17.0.jar:3.7.1-17.0]
{code};;;","20/Feb/24 15:31;mapohl;There's a server-side error causing the JVM exiting with code 2:
{code}
01:16:27,812 [  zkservermainrunner] ERROR org.apache.zookeeper.server.embedded.ZooKeeperServerEmbeddedImpl [] - error during server lifecycle
java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method) ~[?:1.8.0_292]
        at sun.nio.ch.Net.bind(Net.java:461) ~[?:1.8.0_292]
        at sun.nio.ch.Net.bind(Net.java:453) ~[?:1.8.0_292]
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222) ~[?:1.8.0_292]
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85) ~[?:1.8.0_292]
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:78) ~[?:1.8.0_292]
        at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:662) ~[zookeeper-3.7.1.jar:3.7.1]
        at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:160) ~[zookeeper-3.7.1.jar:3.7.1]
        at org.apache.zookeeper.server.embedded.ZooKeeperServerEmbeddedImpl$4.run(ZooKeeperServerEmbeddedImpl.java:159) [zookeeper-3.7.1.jar:3.7.1]
01:16:27,813 [  zkservermainrunner] INFO  org.apache.zookeeper.server.ZooKeeperServerMain              [] - Connection factory did not start
01:16:27,822 [  zkservermainrunner] ERROR org.apache.zookeeper.util.ServiceUtils                       [] - Exiting JVM with code 2
01:16:27,832 [  zkservermainrunner] INFO  org.apache.zookeeper.server.embedded.ZooKeeperServerEmbeddedImpl [] - ZK server died. Requesting stop on JVM
{code};;;","20/Feb/24 15:37;mapohl;I consider this a one-time issue. We're already using random ports for ZooKeeper's TestServer in the tests. There's nothing else that can be done here, AFAICS.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
After failed deserialization with ConfluentRegistryAvroDeserializationSchema all subsequent deserialization fails,FLINK-34474,13569206,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gliter,gliter,20/Feb/24 13:47,20/Feb/24 15:39,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,1,,,,,"Steps to reproduce:
 # Create ConfluentRegistryAvroDeserializationSchema instance for specific Avro
 # Parse invalid byte representation of serialized Avro event
 # Parse valid byte representation of serialized Avro event

Expected:
Validation in step 3 is successful

 

Actual:
Validation in step 3 fails

Short code example, I cannot attach full examples at this time:

```
public class DeserializationTest {
    public static void main(String[] args) throws Exception {
       byte[] valid = new byte[]{
               ...
        };
        byte[] invalid = new byte[]{
                ...
        };


        ConfluentRegistryAvroDeserializationSchema<RawEvent> deserializer = ConfluentRegistryAvroDeserializationSchema.forSpecific(RawEvent.class, valid schema registry url);

        System.out.println(""deserialize valid"");
        des(deserializer, valid);
        System.out.println(""deserialize invalid"");
        des(deserializer, invalid);
        System.out.println(""deserialize valid"");
        des(deserializer, valid);
        System.out.println(""deserialize valid"");
        des(deserializer, valid);
    }

    private static void des(ConfluentRegistryAvroDeserializationSchema<RawEvent> deserializer, byte[] bytes) {
        try {
            deserializer.deserialize(bytes);
            System.out.println(""VALID"");
        } catch (Exception e) {
            System.out.println(""FAILED: "" + e);
        }
    }
}
```

Console output:

```
deserialize valid
VALID
deserialize invalid
FAILED: java.lang.ArrayIndexOutOfBoundsException: Index -154 out of bounds for length 2
deserialize valid
FAILED: java.lang.ArrayIndexOutOfBoundsException: Index 24 out of bounds for length 2
deserialize valid
FAILED: java.lang.ArrayIndexOutOfBoundsException: Index 25 out of bounds for length 2
```","* Locally executed, without Flink cluster.
 * Flink on Kubernetes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 15:39:51 UTC 2024,,,,,,,,,,"0|z1nie0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 15:39;gliter;I also found out that if we deserialize a byte array that contains a valid serialized Avro and some extra bytes after that it will deserialize successfully but will fail on next deserialization. Seems like the extra bytes are left over in deserializer.

valid and validWithExtraBytes is the same but in the latter case I have added one extra byte (in my example just 0)

```
System.out.println(""deserialize valid"");
        des(deserializer, valid);
        System.out.println(""deserialize valid with extra bytes"");
        des(deserializer, validWithExtraBytes);
        System.out.println(""deserialize valid"");
        des(deserializer, valid);
```

```
deserialize valid
VALID
deserialize valid with extra bytes
VALID
deserialize valid
FAILED: java.lang.ArrayIndexOutOfBoundsException: Index 36 out of bounds for length 2
```;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate FlinkPruneEmptyRules,FLINK-34473,13569202,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,20/Feb/24 13:10,28/Feb/24 12:26,04/Jun/24 20:40,,1.20.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,,,,,,,,,,,,,,,,,,,FLINK-34156,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 12:26:16 UTC 2024,,,,,,,,,,"0|z1nid4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 12:26;xuyangzhong;Hi, [~jackylau] . This Jira is a subtask instead of a bug, right?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"loading class of protobuf format descriptor by ""Class.forName(className, true, Thread.currentThread().getContextClassLoader())"" may can not find class because the current thread class loader may not contain this class",FLINK-34472,13569200,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jeremymu,jeremymu,20/Feb/24 12:53,21/Feb/24 02:22,04/Jun/24 20:40,21/Feb/24 02:22,1.16.2,1.18.1,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,"1、Stack information：
Caused by: java.lang.IllegalArgumentException: get test.ProtobufTest1 descriptors error!
    at org.apache.flink.formats.protobuf.util.PbFormatUtils.getDescriptor(PbFormatUtils.java:94)
    at org.apache.flink.formats.protobuf.serialize.PbRowDataSerializationSchema.<init>(PbRowDataSerializationSchema.java:49)
    at org.apache.flink.formats.protobuf.PbEncodingFormat.createRuntimeEncoder(PbEncodingFormat.java:47)
    at org.apache.flink.formats.protobuf.PbEncodingFormat.createRuntimeEncoder(PbEncodingFormat.java:31)
    at org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.createSerialization(KafkaDynamicSink.java:388)
    at org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink.getSinkRuntimeProvider(KafkaDynamicSink.java:194)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecSink.createSinkTransformation(CommonExecSink.java:150)
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:176)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:159)
    at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)

2、my suggestion:
Add a member variable to PbEncodingFormat: class loader instance, which is passed through DynamicTableFactory.Context method :getClassLoader() ","1、standalone session
2、flink on k8s ",1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,FLINK-32418,,,,,,,,,,,,,,,"20/Feb/24 12:53;jeremymu;exception1.png;https://issues.apache.org/jira/secure/attachment/13066899/exception1.png","20/Feb/24 12:57;jeremymu;sourcecode.jpg;https://issues.apache.org/jira/secure/attachment/13066900/sourcecode.jpg","20/Feb/24 13:31;jeremymu;sourcecode2.png;https://issues.apache.org/jira/secure/attachment/13066904/sourcecode2.png","20/Feb/24 13:31;jeremymu;sourcecode3.png;https://issues.apache.org/jira/secure/attachment/13066905/sourcecode3.png",,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,Wed Feb 21 02:22:26 UTC 2024,,,,,,,,,,"0|z1nico:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 13:18;libenchao;Is this duplicated with FLINK-32418?;;;","20/Feb/24 13:35;jeremymu;It's the same problem;;;","21/Feb/24 02:22;libenchao;[~jeremymu] Thanks for the confirmation, I'm closing this one as ""duplicated"", further discussions can go to FLINK-32418.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tune network memory as part of Autoscaler Memory Tuning,FLINK-34471,13569175,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,fanrui,fanrui,20/Feb/24 10:09,23/Feb/24 13:13,04/Jun/24 20:40,23/Feb/24 13:13,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,,Design doc: https://docs.google.com/document/d/19HYamwMaYYYOeH3NRbk6l9P-bBLBfgzMYjfGEPWEbeo/edit?usp=sharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 23 13:13:05 UTC 2024,,,,,,,,,,"0|z1ni74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 10:14;fanrui;cc  [~mxm] [~gyfora] 

It is mentioned in this comment: https://github.com/apache/flink-kubernetes-operator/pull/778#pullrequestreview-1889766784;;;","21/Feb/24 13:02;mxm;Thanks Rui!;;;","21/Feb/24 16:04;mxm;I think in addition to the fine-grained approach described in the doc, we can do a first implementation which simply uses [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/network_mem_tuning/#network-buffer-lifecycle] and assumes an ALL_TO_ALL relationship. This may not optimize down to the last byte but still gives great savings over the default.;;;","22/Feb/24 01:47;fanrui;Assuming ALL_TO_ALL in the first version is fine for me.

Also, if all of you don't mind, I can support POINT_TO_POINT in the next version.;;;","22/Feb/24 10:18;mxm;Thanks! That would be very helpful – I don’t mind at all. I realized point to point connections might be a bit tricky when the parallelism between tasks changes from being equal to being different because at runtime we then switch to a different partitioner. So worst case we could run out of network buffers in this scenario. I’m curious how you want to solve this. We probably need to add extra buffers to account for this edge case. ;;;","22/Feb/24 10:57;fanrui;{quote}I realized point to point connections might be a bit tricky when the parallelism between tasks changes from being equal to being different because at runtime we then switch to a different partitioner. So worst case we could run out of network buffers in this scenario. 
{quote}
Do you mean the connection type is  forward between 2 tasks and then one parallelism is changed by rescale api, right? I try to run a demo in 1.18.0 just now, the WebUI still show the FORWARD, but the real connection type is RESCALE.

Based on it, autoscaler can handle it in following logic:
 * When the connection type is FORWARD and the parallelism of 2 tasks are same. We calculate network memory based on FORWARD connection.
 * When the connection type is FORWARD and the parallelism of 2 tasks are not same. We calculate network memory based on RESCALE connection.

Note: this is my temporary idea. I will investigate and document it in detail next month.

 

Also, the webui shows a wrong type. IIUC, FLINK-33123 is fixing it. After that, we can change the calculating logic again. WDYT?;;;","22/Feb/24 10:59;mxm;Yes, that is exactly what I meant: Moving from FORWARD to RESCALE. The workaround you described makes sense.;;;","23/Feb/24 13:13;fanrui;Merged to main(1.8.0) via: 6a8606517fb9f38f3a486c022f925368c9b4d9ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Transactional message + Table api kafka source with 'latest-offset' scan bound mode causes indefinitely hanging,FLINK-34470,13569074,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dongwoo.kim,dongwoo.kim,19/Feb/24 14:22,01/Jun/24 16:04,04/Jun/24 20:40,,kafka-3.1.0,,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"h2. Summary  
Hi we have faced issue with transactional message and table api kafka source. 
If we configure *'scan.bounded.mode'* to *'latest-offset'* flink sql's request timeouts after hanging. We can always reproduce this unexpected behavior by following below steps.
This is related to this [issue|https://issues.apache.org/jira/browse/FLINK-33484] too.

h2. How to reproduce
1. Deploy transactional producer and stop after producing certain amount of messages
2. Configure *'scan.bounded.mode'* to *'latest-offset'* and submit simple query such as getting count of the produced messages
3. Flink sql job gets stucked and timeouts.

h2. Cause
Transaction producer always produces [control records|https://kafka.apache.org/documentation/#controlbatch] at the end of the transaction. And these control messages are not polled by {*}consumer.poll(){*}. (It is filtered internally). In *KafkaPartitionSplitReader* code, split is finished only when 
*lastRecord.offset() >= stoppingOffset - 1* condition is met. This might work well with non transactional messages or streaming environment but in some batch use cases it causes unexpected hanging.

h2. Proposed solution
{code:java}
if (consumer.position(tp) >= stoppingOffset) {
                    recordsBySplits.setPartitionStoppingOffset(tp, stoppingOffset);
                    finishSplitAtRecord(
                            tp,
                            stoppingOffset,
                            lastRecord.offset(),
                            finishedPartitions,
                            recordsBySplits);
                }
{code}
Replacing if condition to *consumer.position(tp) >= stoppingOffset* in [here|https://github.com/apache/flink-connector-kafka/blob/15f2662eccf461d9d539ed87a78c9851cd17fa43/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java#L137] can solve the problem. 
*consumer.position(tp)* gets next record's offset if it exist and the last record's offset if the next record doesn't exist. 
By this KafkaPartitionSplitReader is available to finish the split even when the stopping offset is configured to control record's offset. 



I would be happy to implement about this fix if we can reach on agreement. Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 01 16:04:01 UTC 2024,,,,,,,,,,"0|z1nhko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 14:24;martijnvisser;[~dongwoo.kim] Can you please verify this with the latest version of the Flink Kafka connector?;;;","19/Feb/24 15:11;dongwoo.kim;[~martijnvisser] I have used latest version(flink-sql-connector-kafka-3.1.0-1.18.jar) and verified that issue still exists. This [line|https://github.com/apache/flink-connector-kafka/blob/15f2662eccf461d9d539ed87a78c9851cd17fa43/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaPartitionSplitReader.java#L137] seems to be causing the issue;;;","25/Mar/24 16:07;rmetzger;[~renqs] can you take a look at this?;;;","09/Apr/24 09:16;m.orazow;Hey [~dongwoo.kim] ,

 

Would you like to send a PR for it? It would be good to fix this for all transactional issues (including FLINK-33484), a good solution that works both for transactional, non-transactional and bounded mode cases.

 

I would be happy to collaborate, please let me know.

 

Best,

Muhammet;;;","15/Apr/24 01:15;dongwoo.kim;Hello [~m.orazow], 

I'm glad that you're interested in collaborating, I’ll send a pr soon and keep you updated.
Thanks for reaching out.

Best,

Dongwoo;;;","29/Apr/24 03:24;dongwoo.kim;Hi [~m.orazow], 
I’ve just made a PR and would appreciate your review
I also left a comment about the metric issue in the discussion area and would appreciate any feedback on that.
Thanks

Best,
Dongwoo;;;","22/May/24 08:47;m.orazow;Hey [~dongwoo.kim] , sorry for late reply. Somehow missed the message.

I have added a minor comment, but overall I'd add integration test for this case if possible and let committer to check the PR also ;;;","01/Jun/24 16:04;dongwoo.kim;Hello [~m.orazow] thanks for the review. I've added integration test code for this case. 
I'd be happy to get feedback from added test codes. 
Thanks.

Best,
Dongwoo;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Implement TableDistribution toString,FLINK-34469,13569069,13557293,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jeyhunkarimov,twalthr,twalthr,19/Feb/24 12:42,05/Mar/24 22:47,04/Jun/24 20:40,03/Mar/24 23:33,1.20.0,,,,,,,1.20.0,,,,Table SQL / API,,,,0,pull-request-available,,,,The newly added TableDistribution misses a toString implementation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 03 23:24:34 UTC 2024,,,,,,,,,,"0|z1nhjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/24 23:24;jingge;master: 322a841694968b5b8f7bb7c6cd9dd85d9d17e23f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Lineage Interface in Cassandra Connector,FLINK-34468,13569058,13526635,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,19/Feb/24 11:55,20/Feb/24 08:44,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Cassandra,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-19 11:55:02.0,,,,,,,,,,"0|z1nhh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Lineage Interface in Jdbc Connector,FLINK-34467,13569057,13526635,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,19/Feb/24 11:54,20/Feb/24 08:44,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-19 11:54:25.0,,,,,,,,,,"0|z1nhgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Lineage Interface in Kafka Connector,FLINK-34466,13569056,13526635,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,19/Feb/24 11:53,20/Feb/24 08:44,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-19 11:53:28.0,,,,,,,,,,"0|z1nhgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python py38-cython: commands failed, Bash exited with code '1",FLINK-34465,13569045,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,lincoln,lincoln.86xy,19/Feb/24 10:02,19/Feb/24 12:13,04/Jun/24 20:40,19/Feb/24 10:47,,,,,,,,,,,,API / Python,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57580&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=28104

{noformat}
Feb 18 03:18:05 ERROR:   py38-cython: commands failed
Feb 18 03:18:05   py39-cython: commands succeeded
Feb 18 03:18:05   py310-cython: commands succeeded
Feb 18 03:18:05   py311-cython: commands succeeded
Feb 18 03:18:05 cleanup /__w/2/s/flink-python/.tox/.tmp/package/1/apache-flink-1.20.dev0.zip
Feb 18 03:18:06 ============tox checks... [FAILED]============
Feb 18 03:18:06 Process exited with EXIT CODE: 1.
Feb 18 03:18:06 Trying to KILL watchdog (18662).
/__w/2/s/tools/ci/watchdog.sh: line 100: 18662 Terminated              watchdog
Feb 18 03:18:06 Searching for .dump, .dumpstream and related files in '/__w/2/s'
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '1'.
Finishing: Test - python
{noformat}
",,,,,,,,,,,,,,,,,,,,,,FLINK-31802,,,,,FLINK-26644,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 12:13:00 UTC 2024,,,,,,,,,,"0|z1nhe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 10:47;mapohl;Thanks for creating this issue, [~lincoln] . For the python stage, it's a bit tedious to find the actual issue because the actual hints are located further up in the log output. The description you're giving in this ticket is quite general. That makes it harder to pinpoint the actual cause.

Just as a hint for Python test instabilities: You can search in the logs for the term ""FAILURES"". Usually, this term only appears once at the start of the actual failure log (e.g. [here for the build above|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57580&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=24260]). Anything below that line shows the actual failure and reveals that it's actually FLINK-26644 rather than a new failure. I hope this helps. :)

I'm going to close this one in favor of FLINK-26644;;;","19/Feb/24 12:13;lincoln.86xy;Looks like I didn't find the right keywords(tried 'error' and 'exception') :D, thanks a lot! [~mapohl];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
actions/cache@v4 times out,FLINK-34464,13569041,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,19/Feb/24 09:40,31/May/24 13:23,04/Jun/24 20:40,,,,,,,,,,,,,Build System / CI,Test Infrastructure,,,0,github-actions,test-stability,,,"[https://github.com/apache/flink/actions/runs/7953599167/job/21710058433#step:4:125]

Pulling the docker image stalled. This should be a temporary issue:
{code:java}
/usr/bin/docker exec  601a5a6e68acf3ba38940ec7a07e08d7c57e763ca0364070124f71bc2f708bc3 sh -c ""cat /etc/*release | grep ^ID""
120Received 260046848 of 1429155280 (18.2%), 248.0 MBs/sec
121Received 545259520 of 1429155280 (38.2%), 260.0 MBs/sec
[...]
Received 914358272 of 1429155280 (64.0%), 0.0 MBs/sec
21645Received 914358272 of 1429155280 (64.0%), 0.0 MBs/sec
21646Error: The operation was canceled. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 13:23:19 UTC 2024,,,,,,,,,,"0|z1nhdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 16:00;rskraba;1.18, jdk17: [https://github.com/apache/flink/actions/runs/8502821617/job/23287712701#step:4:125]

As noted above, while the error actually occurs in the *Initialize Job* step, the *Upload build artifacts* step is highlighted in GitHub actions:
{code:java}
 /usr/bin/docker exec  159dd3123de719a469e6dfbee169fd0baba570825f05bf20ce2d7378a3693a39 sh -c ""cat /etc/*release | grep ^ID""
Error: Input required and not supplied: path {code}
 ;;;","31/May/24 13:23;rskraba;* 1.20 Default (Java 8) / Test packaging/licensing https://github.com/apache/flink/actions/runs/9265488039/job/25489064468#step:4:21641;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Open catalog in CatalogManager should use proper context classloader,FLINK-34463,13569040,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wangsan,wangsan,19/Feb/24 09:14,01/Mar/24 08:42,04/Jun/24 20:40,,1.17.2,1.18.1,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,"When we try to create a catalog in CatalogManager, if the catalog jar is added using `ADD JAR` and the catalog itself requires SPI mechanism, the operation may fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-19 09:14:28.0,,,,,,,,,,"0|z1nhd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session window with negative parameter throws unclear exception,FLINK-34462,13569034,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xu_shuai_,xu_shuai_,19/Feb/24 09:01,03/Mar/24 07:14,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Set invalid parameter in session window get unclear error.
{code:java}
// add test in WindowAggregateITCase
def testEventTimeSessionWindowWithInvalidName(): Unit = {
  val sql =
    """"""
      |SELECT
      |  window_start,
      |  window_end,
      |  COUNT(*),
      |  SUM(`bigdec`),
      |  MAX(`double`),
      |  MIN(`float`),
      |  COUNT(DISTINCT `string`),
      |  concat_distinct_agg(`string`)
      |FROM TABLE(
      |   SESSION(TABLE T1, DESCRIPTOR(rowtime), INTERVAL '-5' SECOND))
      |GROUP BY window_start, window_end
    """""".stripMargin

  val sink = new TestingAppendSink
  tEnv.sqlQuery(sql).toDataStream.addSink(sink)
  env.execute()
} 

{code}
{code:java}
java.lang.AssertionError: Sql optimization: Assertion error: null at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:79) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59) at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156) at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156) at scala.collection.Iterator.foreach(Iterator.scala:937) at scala.collection.Iterator.foreach$(Iterator.scala:937) at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) at scala.collection.IterableLike.foreach(IterableLike.scala:70) at scala.collection.IterableLike.foreach$(IterableLike.scala:69) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156) at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154) at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176) at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83) at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87) at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:320) at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178) at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224) at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219) at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:151) at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:128) at org.apache.flink.table.api.bridge.scala.TableConversions.toDataStream(TableConversions.scala:60) at org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.testEventTimeSessionWindowWithInvalidName(WindowAggregateITCase.scala:1239) at java.lang.reflect.Method.invoke(Method.java:498) at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.Iterator.forEachRemaining(Iterator.java:116) at scala.collection.convert.Wrappers$IteratorWrapper.forEachRemaining(Wrappers.scala:26) at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485) at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) Suppressed: org.opentest4j.AssertionFailedError: Expecting value to be true but was false
{code}
while the tumble window throws 
{code:java}
java.lang.IllegalArgumentException: Tumbling Window parameters must satisfy size > 0, but got size -5000ms.
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)	at org.apache.flink.table.runtime.operators.window.tvf.slicing.SliceAssigners$TumblingSliceAssigner.<init>(SliceAssigners.java:152)	at org.apache.flink.table.runtime.operators.window.tvf.slicing.SliceAssigners$TumblingSliceAssigner.<init>(SliceAssigners.java:136)	at org.apache.flink.table.runtime.operators.window.tvf.slicing.SliceAssigners.tumbling(SliceAssigners.java:65) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34346,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-19 09:01:32.0,,,,,,,,,,"0|z1nhbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MongoDB weekly builds fail with time out on Flink 1.18.1 for JDK17,FLINK-34461,13569033,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jiabaosun,martijnvisser,martijnvisser,19/Feb/24 08:56,19/Feb/24 16:27,04/Jun/24 20:40,19/Feb/24 16:27,mongodb-1.1.0,,,,,,,mongodb-1.1.0,,,,Connectors / MongoDB,,,,0,test-stability,,,,"The weekly tests for MongoDB consistently time out for the v1.0 branch while testing Flink 1.18.1 for JDK17:

https://github.com/apache/flink-connector-mongodb/actions/runs/7770329490/job/21190387348

https://github.com/apache/flink-connector-mongodb/actions/runs/7858349600/job/21443232301

https://github.com/apache/flink-connector-mongodb/actions/runs/7945225005/job/21691624903
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 16:23:09 UTC 2024,,,,,,,,,,"0|z1nhbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 16:23;jiabaosun;The reason for this issue is that the v1.0 branch is missing the backport of FLINK-33899. 
It has been fixed in PR-28 via 
v1.0 (5a8b0979d79e1da009115cde7375bf28c45c22ad, a56c003b8c5aca646e47d4950189b81c9e7e75c3).

Since the main branch has update nightly builds against the latest released v1.1 branch which already includes these two commits, the nightly CI will not fail.
main (aaf3867b2a72a61a0511f250c36580842623b6bc);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jdbc driver get rid of flink-core,FLINK-34460,13569009,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,19/Feb/24 05:24,21/Feb/24 08:05,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Table SQL / Client,,,,0,,,,,"Currently jdbc driver depends on flink-core/flink-runtime modules, users need to upgrade jdbc driver version when the flink session cluster for olap is upgraded, this is not very suitable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-19 05:24:33.0,,,,,,,,,,"0|z1nh68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Results column names should match SELECT clause expression names,FLINK-34459,13568979,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jeyhunkarimov,jeyhunkarimov,18/Feb/24 14:51,19/Feb/24 14:14,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / Client,,,,0,,,,,"When printing {{SQL SELECT}} results, Flink will output generated expression name when the expression is not {{column reference or used with alias/over.}}
For example, select a, a + 1 from T would result in 


{code:java}
+----+-------------+-------------+
| op |           a |      EXPR$1 |
+----+-------------+-------------+
| +I |           1 |           2 |
| +I |           1 |           2 |
| +I |           1 |           2 |
+----+-------------+-------------+
{code}

Instead of the generated {{EXPR$1}} it would be nice to have {{a + 1}} (which is the case in some other data processing systems like Spark).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 14:14:22 UTC 2024,,,,,,,,,,"0|z1ngzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 08:11;martijnvisser;Should it? It would potentially result in really large names (if the output is the result of a complex series of functions for example), and it's easily solvable by using ""AS x"" in your SQL statement as well. ;;;","19/Feb/24 09:01;lorenzo.affetti;[~martijnvisser] well, if the name is a long one it can be truncated automatically via a simple check.

I like the proposal as it would increase the quality of result understanding (y);;;","19/Feb/24 09:07;martijnvisser;I still am not convinced this is an actual user improvement: why is this better then doing as ""AS x"" in your SQL statement? Is this really a user problem?

How would you determine where to truncate? Is it a static value, do you base it on the available width of the returned table? Do you return the full result from the planner, and make it a client-option only, or do you want to put this everything? Is this even SQL standard compliant?
;;;","19/Feb/24 10:14;jeyhunkarimov;Hi [~martijnvisser] yes, there are some tradeoffs. Using AS is always a solution, but then a user needs to modify a query [and maybe modify back]. 
And for queries with many projection expressions, user needs to remember the mapping between EXPR$X -> actual expression in the query. 

Some other systems like Spark does not truncate (at least for the large expressions I tried), some like MySQL/SQLite truncate after some point (they decide where and how to truncate for large expressions). 

So, we have multiple options to deal with the very large expressions: fallback to the current (EXPR$X) version, truncate, etc.
WDYT?
;;;","19/Feb/24 10:54;martijnvisser;I think it should be a FLIP and a discussion :) Can you create and start one?;;;","19/Feb/24 14:14;jeyhunkarimov;Yes that makes sense. Sure thing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename options for Generalized incremental checkpoints (changelog),FLINK-34458,13568970,13566461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,18/Feb/24 08:57,22/Feb/24 06:05,04/Jun/24 20:40,22/Feb/24 06:05,,,,,,,,1.20.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 06:05:14 UTC 2024,,,,,,,,,,"0|z1ngxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 06:05;masteryhx;merged e7e973e2 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename options for latency tracking,FLINK-34457,13568969,13566461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,18/Feb/24 08:55,22/Feb/24 02:50,04/Jun/24 20:40,22/Feb/24 02:50,,,,,,,,1.20.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 02:50:57 UTC 2024,,,,,,,,,,"0|z1ngxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 02:50;masteryhx;merged 6be297d1 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move all checkpoint-related options into CheckpointingOptions,FLINK-34456,13568968,13566461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,spoon-lz,zakelly,zakelly,18/Feb/24 08:54,29/May/24 15:20,04/Jun/24 20:40,29/May/24 15:20,,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,Move all checkpoint-related options that are out of `CheckpointingOptions` into `CheckpointingOptions`. Deprecate the original ones.,,,,,,,,,,,,,FLINK-34482,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 29 15:20:41 UTC 2024,,,,,,,,,,"0|z1ngxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 09:55;clevermose;Is there anyone working on this issue? If no one is working on it, I would be happy to help solve it.;;;","21/Feb/24 10:47;zakelly;[~clevermose] Thanks for volunteering! I've just started working on this, it blocks several tickets so it is better to be finished soon. If you have some free time recently, I can assign this to you.;;;","21/Feb/24 10:58;spoon-lz;[~Zakelly] Please assign the issue to me and I will try to complete this task as soon as possible;;;","21/Feb/24 11:10;zakelly;[~spoon-lz] It seems two accounts here. I'm sorry but are you the same person :)?;;;","21/Feb/24 11:16;spoon-lz;[~Zakelly] Yes, please forgive me for using a previous account in my first answer. After I discovered the problem, I changed my current account.;;;","21/Feb/24 11:18;zakelly;[~spoon-lz] Assigned to you. If you need any help, feel free to reach out.;;;","21/Feb/24 12:09;spoon-lz;[~Zakelly] I looked at FLIP-406 and took a quick look at the code. Want to know if what this PR wants to do is to migrate the configuration in ""ExecutionCheckpointingOptions"" to ""CheckpointingOptions"";;;","21/Feb/24 13:06;zakelly;[~spoon-lz] yes you are right. And `ExecutionCheckpointingOptions` should be annotated as @\Deprecated;;;","26/Feb/24 06:34;spoon-lz;[~Zakelly] This PR has been submitted, please check if it meets the requirements.;;;","29/May/24 15:20;zakelly;Merged into master via 89c89d85;;;",,,,,,,,,,,,,,,,,,,,,,,
Move RestoreMode from flink-runtime to flink-core,FLINK-34455,13568962,13566461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,zakelly,zakelly,zakelly,18/Feb/24 06:52,22/Feb/24 08:51,04/Jun/24 20:40,22/Feb/24 08:37,,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"This should be done according to the FLIP-406, since a new RecoveryOptions class will be introduced in flink-core, which contains options for RestoreMode.

Although RestoreMode is @PublicEvolving, there is no public user interface usage of this class (normal user would only use it in configuration), so no breaking change introduced.",,,,,,,,,,,,,FLINK-34454,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 08:37:05 UTC 2024,,,,,,,,,,"0|z1ngw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 08:37;Yanfei Lei;merged 80090c7 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce StateRecoveryOptions in flink-core,FLINK-34454,13568957,13566461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,18/Feb/24 06:21,27/Feb/24 09:32,04/Jun/24 20:40,27/Feb/24 09:32,,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,,,,,,,,,,,,,FLINK-34455,FLINK-34484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 09:32:21 UTC 2024,,,,,,,,,,"0|z1nguw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/24 09:32;masteryhx;merged 63dc5598...b9069227 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Benchmark] Add warmup in SchedulerBenchmarkExecutor,FLINK-34453,13568956,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,zakelly,zakelly,zakelly,18/Feb/24 05:35,19/Feb/24 03:53,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,Benchmarks,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-18 05:35:37.0,,,,,,,,,,"0|z1nguo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-15959 Add min number of slots configuration to limit total number of slots,FLINK-34452,13568825,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,xiangyu0xf,xiangyu0xf,17/Feb/24 05:27,21/Feb/24 07:01,04/Jun/24 20:40,21/Feb/24 07:01,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,release-testing,,,,"Test Suggestion:
 # Prepare configuration options:
 ** taskmanager.numberOfTaskSlots = 2,
 ** slotmanager.number-of-slots.min = 7,
 ** slot.idle.timeout = 50000
 # Setup a Flink session Cluster on Yarn or Native Kubernetes based on following docs:
 ** [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/yarn/#starting-a-flink-session-on-yarn]
 ** [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/#starting-a-flink-session-on-kubernetes]
 # Verify that 4 TaskManagers will be registered even though no jobs has been submitted
 # Verify that these TaskManagers will not be destroyed after 50 seconds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34391,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 07:00:49 UTC 2024,,,,,,,,,,"0|z1ngj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 06:18;Weijie Guo;[~xiangyu0xf] I have tested this feature on a yarn cluster:

1. Set {{taskmanager.numberOfTaskSlots=2}}, {{slotmanager.number-of-slots.min=7}}, and {{slot.idle.timeout=50000}}.
2. Start a yarn session cluster.
3. From ActiveResourceManager's log that 4 workers (TM) were requested and successfully registered.
4. These workers are not be destroyed even after idle timeout.

Everything looks as expected then.;;;","21/Feb/24 06:58;xiangyu0xf;[~Weijie Guo] Thx for your work!;;;","21/Feb/24 07:00;xiangyu0xf;[~lincoln.86xy] Hi, I will close this ticket as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Kubernetes Operator] Job with restarting TaskManagers uses wrong/misleading fallback approach,FLINK-34451,13568819,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,alexdchoffer,alexdchoffer,17/Feb/24 00:01,26/Feb/24 10:17,04/Jun/24 20:40,,kubernetes-operator-1.6.1,,,,,,,,,,,Kubernetes Operator,,,,0,,,,," 

We had a situation where TaskManagers were constantly restarting from OOM. We're using the Adaptive scheduler with the Kubernetes Operator, and a restart strategy of exponential backoff, and so the JobManagers remained alive. We're also using savepoint upgrade mode. 

When we tried to remedy the situation by raising the direct memory allocation to the pods, we were surprised that Flink used the last savepoint taken, rather than the checkpoint. This was unfortunate for us because we are on adaptive scheduler and the job hasn't changed in some time, so this last savepoint was 6 days old! Meanwhile, checkpoints were taken every minute up until failure. I can confirm the HA metadata existed in the configmaps, and the corresponding checkpoints existed in remote storage for it to access. Plus, no Flink version changes were in the deployment.

The Operator logs reported that it was using last-state recovery in this situation:


{code:java}
2024-02-15 19:38:38,252 o.a.f.k.o.l.AuditUtils         [INFO ][job-name] >>> Event  | Info    | SPECCHANGED     | UPGRADE change(s) detected (Diff: FlinkDeploymentSpec[image : image:0a7c41b -> image:ebebc53, restartNonce : null -> 100]), starting reconciliation.
2024-02-15 19:38:38,252 o.a.f.k.o.r.d.AbstractJobReconciler [INFO ][job-name] Upgrading/Restarting running job, suspending first...
2024-02-15 19:38:38,260 o.a.f.k.o.r.d.ApplicationReconciler [INFO ][job-name] Job is not running but HA metadata is available for last state restore, ready for upgrade
2024-02-15 19:38:38,270 o.a.f.k.o.l.AuditUtils         [INFO ][job-name] >>> Event  | Info    | SUSPENDED       | Suspending existing deployment.
2024-02-15 19:38:38,270 o.a.f.k.o.s.NativeFlinkService [INFO ][job-name] Deleting JobManager deployment while preserving HA metadata.	
2024-02-15 19:38:40,431 o.a.f.k.o.l.AuditUtils         [INFO ][job-name] >>> Status | Info    | UPGRADING       | The resource is being upgraded 
2024-02-15 19:38:40,532 o.a.f.k.o.l.AuditUtils         [INFO ][job-name] >>> Event  | Info    | SUBMIT          | Starting deployment
2024-02-15 19:38:40,532 o.a.f.k.o.s.AbstractFlinkService [INFO ][job-name] Deploying application cluster requiring last-state from HA metadata
2024-02-15 19:38:40,538 o.a.f.k.o.u.FlinkUtils         [INFO ][job-name] Job graph in ConfigMap job-name-cluster-config-map is deleted {code}
But when the job booted up, it reported restoring from savepoint:


{code:java}
2024-02-15 19:39:03,887 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 522b3c363499d81ed7922aa30b13e237 from Savepoint 20207 @ 0 for 522b3c363499d81ed7922aa30b13e237 located at abfss://savepoints@storageaccount.dfs.core.windows.net/job-name/savepoint-522b3c-8836a1edc709. {code}

Our expectation was that the Operator logs were true, and that it would be restoring from checkpoint. We had to scramble and manually restore from the checkpoint to restore function.

 

 

It's also worth noting I can recreate this issue in a testing environment. The process for doing so is:

- Boot up HA JobManagers with checkpoints on and savepoint upgrade mode, using adaptive scheduler

- Make a dummy change to trigger a savepoint.

- Allow the TaskManagers to process some data and hit the checkpoint interval.

- Cause the TaskManagers to crash. In our case, we could use up a bunch of memory in the pods and cause it to crash.

- Observe the Operator logs saying it is restoring from last-state, but watch as the pods instead use the last savepoint.","Operator version: 1.7.1

Flink version 1.18.0

HA JobManagers

Adaptive scheduler mode using the operator's autoscaler

Checkpointing at an interval of 60s

Upgrade mode savepoint",,,,,,,,,,,FLINK-34518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 10:16:23 UTC 2024,,,,,,,,,,"0|z1nghs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/24 07:06;gyfora;Before we can investigate the root cause it would be great to get some extra information:
 # Can you please share your spec to see the configuration / HA setup? (could this be related to multiple job managers?)
 # In your repro scenario, does it happen all the time?
 # Does this affect Kubernetes Operator 1.7.0 (or 1.8-SNAPSHOT if possible)?
 # Does this affect Flink 1.17 as well?
 # Does it affect only the adaptive scheduler?

 

I tried reproducing this in the basic-checkpoint-ha example (with flink 1.18.0 and the adaptive scheduler) but I never hit any issue on either savepoint / last-state upgrades, failing TMs or not. 

 ;;;","21/Feb/24 23:08;alexdchoffer;[~gyfora] 
 # Here is my FlinkDeployment:

{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: kafka
  namespace: flink
spec:
  image: [redacted]
  flinkVersion: v1_18
  restartNonce: 25
  flinkConfiguration:
    taskmanager.rpc.port: ""50100""
    taskmanager.numberOfTaskSlots: ""1""
    blob.server.port: ""6124""
    jobmanager.memory.process.size: ""null""
    taskmanager.memory.process.size: ""2gb""
    high-availability.type: kubernetes
    high-availability.storageDir: abfss://job-result-store@[redacted].dfs.core.windows.net/kafka

    state.checkpoints.dir: abfss://checkpoints@[redacted].dfs.core.windows.net/kafka
    execution.checkpointing.interval: ""30000""
    execution.checkpointing.mode: EXACTLY_ONCE
    state.checkpoint-storage: filesystem

    state.savepoints.dir: abfss://savepoints@[redacted].dfs.core.windows.net/kafka

    state.backend.type: rocksdb
    state.backend.incremental: ""true""
    state.backend.rocksdb.localdir: /rocksdb

    fs.azure.account.auth.type: OAuth
    fs.azure.account.oauth.provider.type: org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider
    fs.azure.account.oauth2.client.endpoint: [redacted]
    fs.azure.account.oauth2.client.id: [redacted]

    # Fix bug with hadoop azure that buffers checkpoint blocks in disk rather than memory https://issues.apache.org/jira/browse/HADOOP-18707
    fs.azure.data.blocks.buffer: array

    restart-strategy.type: exponentialdelay
    job.autoscaler.enabled: ""true""
    job.autoscaler.stabilization.interval: 2m
    job.autoscaler.metrics.window: 1m
    job.autoscaler.target.utilization: ""0.6""
    job.autoscaler.target.utilization.boundary: ""0.2""
    job.autoscaler.restart.time: 1m
    job.autoscaler.catch-up.duration: 1m
    job.autoscaler.scale-up.grace-period: 10m
    jobmanager.scheduler: adaptive
    pipeline.max-parallelism: ""12""
    job.autoscaler.vertex.max-parallelism: ""5""

  serviceAccount: flink
  jobManager:
    replicas: 2
    resource:
      memory: ""2gb""
      cpu: 1
    podTemplate:
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - kafka
                  - key: component
                    operator: In
                    values:
                      - jobmanager
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 10
        containers:
          - name: flink-main-container
            resources:
              limits:
                ephemeral-storage: 1Gi
              requests:
                ephemeral-storage: 1Gi
  taskManager:
    resource:
      memory: ""2gb""
      cpu: 1
    podTemplate:
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - kafka
                  - key: component
                    operator: In
                    values:
                      - taskmanager
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 10
        containers:
          - name: flink-main-container
            resources:
              limits:
                ephemeral-storage: 2Gi
              requests:
                ephemeral-storage: 2Gi
            volumeMounts:
              - mountPath: /rocksdb
                name: rocksdb
        volumes:
          - name: rocksdb
            emptyDir:
              sizeLimit: 1Gi
  podTemplate:
    spec:
      containers:
        - name: flink-main-container
          ports:
            - containerPort: 9250
              name: metrics
              protocol: TCP
  job:
    entryClass: ""org.apache.flink.client.python.PythonDriver""
    args: [""-pyclientexec"", ""/usr/bin/python"", ""-py"", ""/opt/flink/usrlib/kafka-k6.py"", ""--kubernetes"", ""--fivemin_bytessent_stream"", ""--kafka_bootstrap_ip"", ""10.177.1.26""]
    upgradeMode: savepoint
    parallelism: 1{code}
      2. Yes, I can recreate this scenario each time I try.

      3. My ticket was mistaken, this was found on operator version 1.7.0 (I will update the ticket). I just recreated it on the latest Flink Operator image available (37ca517).

      4. Just confirmed it occurs on Flink 1.17.0

      5. Did not occur when adaptive scheduler was turned off!

In scenario 5 above, the job correctly flipped back to the last checkpoint.;;;","22/Feb/24 10:04;gyfora;Which 1.18 version are you using? I have only tried to repro this with 1.18.1 (latest 1.18 release) but failed to repro it.;;;","22/Feb/24 10:07;gyfora;I am only asking because there have been fixes / improvements between 1.18.0 -> 1.18.1;;;","22/Feb/24 19:14;alexdchoffer;[~gyfora] I see it on 1.18.1 as well. Only flipping off the adaptive scheduler fixes it for me. Note that my upgrades at this point are just changing the restartNonce.;;;","22/Feb/24 19:26;gyfora;Could this be related to the the Jobmanager HA? Instead of 2 replicas change to 1 and try again?;;;","22/Feb/24 19:59;alexdchoffer;That ""fixed it"", but not in a satisfying way... Using non-HA meant that when it came to the point for me to test upgrading the job while the TMs were crashing, it just wouldn't try:
{code:java}
2024-02-22 19:55:35,338 o.a.f.k.o.r.d.AbstractJobReconciler [INFO ][flink/kafka] Upgrading/Restarting running job, suspending first...
2024-02-22 19:55:35,339 o.a.f.k.o.r.d.ApplicationReconciler [INFO ][flink/kafka] Job is not running and HA metadata is not available or usable for executing the upgrade, waiting for upgradeable state{code};;;","22/Feb/24 20:06;gyfora;I did not mean to turn off HA but only to reduce the JM replicas to 1;;;","22/Feb/24 20:32;alexdchoffer;Putting JMs at 1 with HA on still leads to the same result - it restores from savepoint.;;;","22/Feb/24 20:40;gyfora;Hm, so this really seems to be somehow adaptive scheduler related. My problem is that I cannot repro this locally. Would you please try with the `basic-checkpoint-ha.yaml` example? 

That has checkpointing and HA enabled, you can configure the adaptive scheduler and 1.18. I could not repro it this way.;;;","22/Feb/24 20:42;gyfora;I tried killing TMs and immediately bumping the restartNonce at the same time. You can see in the logs that the operator uses the HA metadata and picks up the latest checkpoint correctly (not the savepoint);;;","22/Feb/24 21:29;gyfora;Also one thing that occurred to me is that the issue could be with your HA storage. Is it possible that the new JM cannot access the HA storage directory? Can you change it to S3/Hdfs maybe?

 

during the upgrade the JM pod is killed and a new one will start so it needs access to the stored files . ;;;","22/Feb/24 21:58;alexdchoffer;I'm bound to Azure unfortunately. Also, why would that only crop up when adaptive scheduler is on? I'll try the basic-checkpoint example.;;;","22/Feb/24 23:58;alexdchoffer;I just got the checkpoint example running and it doesn't show this issue. Hmph. I wonder what in the configuration is causing this;;;","23/Feb/24 17:34;alexdchoffer;[~gyfora] I can recreate on basic-checkpoint-ha by adding restart-strategy.type: exponentialdelay. 
{code:java}
spec:
  restartNonce: 10
  image: flink:1.17
  flinkVersion: v1_17
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: ""2""
    state.savepoints.dir: file:///flink-data/savepoints
    state.checkpoints.dir: file:///flink-data/checkpoints
    high-availability.type: kubernetes
    high-availability.storageDir: file:///flink-data/ha
    jobmanager.scheduler: adaptive
    restart-strategy.type: exponentialdelay {code};;;","23/Feb/24 21:25;gyfora;That's a good catch, if this is a bug related to adaptive scheduler + exponential restart strategy then we should be able to track it down. Do you have any hypothesis what may cause the issue?;;;","24/Feb/24 00:18;alexdchoffer;The clue seems to be in the logs. The operator thinks it is restarting from last-state. The job however thinks it is restarting from savepoint. The job is taking precedence, of course. Have to think some more to hypothesize why the task failure restarting would malfunction in that case;;;","24/Feb/24 04:42;gyfora;To me the logs are not very surprising. The way it is currently implemented the operator sends the last savepoint as the savepoint parameter even if it assumes that last state will be used.

It’s a bit weird and could be removed …

but in any case it expects Flink to pick up the last state through the failover mechanism from the HA metadata (that’s how last state upgrade works)

so to me the logs are not so informative, the big questions is why the adaptive scheduler would ignore the HA metadata in this particular case. The operator checked that it’s there but Flink ignored it and instead starts from the savepoint (which was set somewhat unnecessarily but expected);;;","24/Feb/24 04:59;alexdchoffer;You’re the expert, I’m merely a user!:D I’d be curious if you can recreate with that restart flag on the ha example. It does seem like unintended behavior;;;","24/Feb/24 05:02;gyfora;I will definitely try this on Monday , I was just curious if you have any working hypothesis to validate first :) thanks for narrowing this down;;;","26/Feb/24 07:36;gyfora;I took a closer look at this and it also happens with the default restart strategy. 

The relevant log segment during shutdown (if we simply delete the deployment object as the last-state suspend does in the operator)
{code:java}
2024-02-26 07:05:04,412 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map -> Sink: Unnamed (1/1) (6a8685c5160d149204dd115f396dcb38_90bea66de1c231edf33913ecd54406c1_0_1) switched from RUNNING to FAILED on autoscaling-example-taskmanager-1-1 @ 10.244.0.54 (dataPort=35905).
org.apache.flink.util.FlinkExpectedException: The TaskExecutor is shutting down.
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:481) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:239) ~[flink-dist-1.18.1.jar:1.18.1]
    ...
2024-02-26 07:05:04,415 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
2024-02-26 07:05:04,415 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 6a8685c5160d149204dd115f396dcb38_90bea66de1c231edf33913ecd54406c1_0_1.
2024-02-26 07:05:04,416 DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sending out cancel request, to remove task execution from TaskManager.
2024-02-26 07:05:04,417 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.244.0.54, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.
2024-02-26 07:05:04,417 INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Restarting job.
org.apache.flink.util.FlinkExpectedException: The TaskExecutor is shutting down.
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:481) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:239) ~[flink-dist-1.18.1.jar:1.18.1]
    ...
2024-02-26 07:05:04,417 DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Transition from state Executing to Restarting.
2024-02-26 07:05:04,417 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Autoscaling Example (5ddd0b1ba346d3bfd5ef53a63772e43c) switched from state RUNNING to CANCELLING.
2024-02-26 07:05:04,417 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Sequence Source -> Filter (1/1) (6a8685c5160d149204dd115f396dcb38_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from RUNNING to CANCELING.
2024-02-26 07:05:04,418 DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{cda2d4d04521eed8b88245bb0eb497e0})
2024-02-26 07:05:04,418 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Sequence Source -> Filter (1/1) (6a8685c5160d149204dd115f396dcb38_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from CANCELING to CANCELED.
2024-02-26 07:05:04,418 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 6a8685c5160d149204dd115f396dcb38_cbc357ccb763df2852fee8c4fc7d55f2_0_1.
2024-02-26 07:05:04,419 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 6a8685c5160d149204dd115f396dcb38_cbc357ccb763df2852fee8c4fc7d55f2_0_1.
2024-02-26 07:05:04,419 DEBUG org.apache.flink.runtime.scheduler.adaptive.allocator.SharedSlot [] - Returning logical slot to shared slot (SlotRequestId{cda2d4d04521eed8b88245bb0eb497e0})
2024-02-26 07:05:04,419 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Autoscaling Example (5ddd0b1ba346d3bfd5ef53a63772e43c) switched from state CANCELLING to CANCELED.
2024-02-26 07:05:04,420 DEBUG org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - ExecutionGraph 5ddd0b1ba346d3bfd5ef53a63772e43c reached terminal state CANCELED.
2024-02-26 07:05:04,420 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 5ddd0b1ba346d3bfd5ef53a63772e43c.
2024-02-26 07:05:04,420 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [d3be5ab8662c10de36088fddeb531b59].
org.apache.flink.util.FlinkExpectedException: The TaskExecutor is shutting down.
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:481) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:239) ~[flink-dist-1.18.1.jar:1.18.1]
    ...
2024-02-26 07:05:04,420 DEBUG org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Ignoring scheduled action because expected state org.apache.flink.runtime.scheduler.adaptive.Executing@310e697d is not the actual state org.apache.flink.runtime.scheduler.adaptive.Restarting@5654b461.
2024-02-26 07:05:04,619 DEBUG org.apache.pekko.remote.transport.netty.NettyTransport       [] - Remote connection to [/10.244.0.54:33105] was disconnected because of [id: 0x6431709f, /10.244.0.53:55610 :> /10.244.0.54:33105] DISCONNECTED
2024-02-26 07:05:04,620 DEBUG org.apache.pekko.remote.transport.ProtocolStateActor         [] - Association between local [tcp://flink-metrics@10.244.0.53:55610] and remote [tcp://flink-metrics@10.244.0.54:33105] was disassociated because the ProtocolStateActor failed: Shutdown
2024-02-26 07:05:04,619 DEBUG org.apache.pekko.remote.transport.netty.NettyTransport       [] - Remote connection to [/10.244.0.54:49696] was disconnected because of [id: 0x0e459e2e, /10.244.0.54:49696 :> /10.244.0.53:6123] DISCONNECTED
2024-02-26 07:05:04,700 DEBUG org.apache.pekko.remote.transport.ProtocolStateActor         [] - Association between local [tcp://flink@10.244.0.53:6123] and remote [tcp://flink@10.244.0.54:49696] was disassociated because the ProtocolStateActor failed: Shutdown
2024-02-26 07:05:04,700 DEBUG org.apache.pekko.remote.Remoting                             [] - Remote system with address [pekko.tcp://flink-metrics@10.244.0.54:33105] has shut down. Address is now gated for 50 ms, all messages to this address will be delivered to dead letters.
2024-02-26 07:05:04,707 DEBUG org.apache.pekko.remote.Remoting                             [] - Remote system with address [pekko.tcp://flink@10.244.0.54:6122] has shut down. Address is now gated for 50 ms, all messages to this address will be delivered to dead letters.
2024-02-26 07:05:04,841 DEBUG org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.Watcher [] - Watcher closed
2024-02-26 07:05:05,002 WARN  org.apache.pekko.actor.CoordinatedShutdown                   [] - Could not addJvmShutdownHook, due to: Shutdown in progress
2024-02-26 07:05:05,002 WARN  org.apache.pekko.actor.CoordinatedShutdown                   [] - Could not addJvmShutdownHook, due to: Shutdown in progress {code}
For some reason seems like the job actually goes into a globally terminal CANCELED state and HA metadata is cleaned up / lost. I wonder why this happens with the AdaptiveScheduler and not with the default scheduler. 

cc [~chesnay] [~dmvk] do you guys have any clue why there is a completely different behaviour in this case for the adaptive scheduler? Why does the job end up in a globally terminal state?

Here are also the logs for the default scheduler which clearly doesn't do any cleanup / cancelling as expected (by me):


{code:java}
2024-02-26 07:13:04,449 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Closing TaskExecutor connection autoscaling-example-taskmanager-1-1 because: The TaskExecutor is shutting down.
2024-02-26 07:13:04,449 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor autoscaling-example-taskmanager-1-1 because: The TaskExecutor is shutting down.
2024-02-26 07:13:04,449 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager [] - Unregistering task executor 24ea20d5af9f88b43319c53118f5ce3c from the slot manager.
2024-02-26 07:13:04,449 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer [] - Freeing slot 9f25210e811654cf75f0a813c65fb1b8.
2024-02-26 07:13:04,451 DEBUG org.apache.flink.runtime.scheduler.SharedSlot                [] - Release shared slot (SlotRequestId{09ea3d193e173b8cc62a6c50274926b5})
2024-02-26 07:13:04,454 DEBUG org.apache.flink.runtime.scheduler.SharedSlot                [] - Release logical slot (SlotRequestId{241da77c2fc4f7d840ffa12641bc135f}) for execution vertex (id cbc357ccb763df2852fee8c4fc7d55f2_0) from the physical slot (SlotRequestId{09ea3d193e173b8cc62a6c50274926b5})
2024-02-26 07:13:04,458 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2024-02-26 07:13:04,459 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
2024-02-26 07:13:04,462 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTracker [] - Free allocated slot with allocationId 9f25210e811654cf75f0a813c65fb1b8.
2024-02-26 07:13:04,463 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTracker [] - Remove task manager 24ea20d5af9f88b43319c53118f5ce3c.
2024-02-26 07:13:04,463 DEBUG org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl [] - Processing shutdown of task executor autoscaling-example-taskmanager-1-1.
2024-02-26 07:13:04,463 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Stopping worker autoscaling-example-taskmanager-1-1.
2024-02-26 07:13:04,459 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesApplicationClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2024-02-26 07:13:04,456 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Sequence Source -> Filter (1/1) (a6d599cf375af90bdbffdbbb5363b13a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAILED on autoscaling-example-taskmanager-1-1 @ 10.244.0.56 (dataPort=46729).
org.apache.flink.util.FlinkExpectedException: The TaskExecutor is shutting down.
at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:481) ~[flink-dist-1.18.1.jar:1.18.1]
at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:239) ~[flink-dist-1.18.1.jar:1.18.1]
at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor$StartedState.lambda$terminate$0(PekkoRpcActor.java:574) ~[flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-dist-1.18.1.jar:1.18.1]
at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor$StartedState.terminate(PekkoRpcActor.java:573) ~[flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleControlMessage(PekkoRpcActor.java:196) ~[flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) ~[flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) ~[flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) ~[flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) [flink-rpc-akka44a70f6e-66c7-4ad0-98eb-a736d52e5378.jar:1.18.1]
at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
2024-02-26 07:13:04,709 DEBUG org.apache.pekko.remote.transport.netty.NettyTransport       [] - Remote connection to [/10.244.0.56:35175] was disconnected because of [id: 0xa827118c, /10.244.0.55:51996 :> /10.244.0.56:35175] DISCONNECTED
2024-02-26 07:13:04,712 DEBUG org.apache.pekko.remote.transport.ProtocolStateActor         [] - Association between local [tcp://flink-metrics@10.244.0.55:51996] and remote [tcp://flink-metrics@10.244.0.56:35175] was disassociated because the ProtocolStateActor failed: Shutdown
2024-02-26 07:13:04,709 DEBUG org.apache.pekko.remote.transport.ProtocolStateActor         [] - Association between local [tcp://flink@10.244.0.55:6123] and remote [tcp://flink@10.244.0.56:57044] was disassociated because the ProtocolStateActor failed: Shutdown
2024-02-26 07:13:04,716 DEBUG org.apache.pekko.remote.Remoting                             [] - Remote system with address [pekko.tcp://flink-metrics@10.244.0.56:35175] has shut down. Address is now gated for 50 ms, all messages to this address will be delivered to dead letters.
2024-02-26 07:13:04,718 DEBUG org.apache.pekko.remote.transport.netty.NettyTransport       [] - Remote connection to [/10.244.0.56:57044] was disconnected because of [id: 0xb1bfb799, /10.244.0.56:57044 :> /10.244.0.55:6123] DISCONNECTED
2024-02-26 07:13:04,719 DEBUG org.apache.pekko.remote.Remoting                             [] - Remote system with address [pekko.tcp://flink@10.244.0.56:6122] has shut down. Address is now gated for 50 ms, all messages to this address will be delivered to dead letters.
2024-02-26 07:13:04,804 WARN  org.apache.pekko.remote.transport.netty.NettyTransport       [] - Remote connection to [null] failed with java.net.ConnectException: Connection refused: /10.244.0.56:6122
2024-02-26 07:13:04,804 WARN  org.apache.pekko.remote.ReliableDeliverySupervisor           [] - Association with remote system [pekko.tcp://flink@10.244.0.56:6122] has failed, address is now gated for [50] ms. Reason: [Association failed with [pekko.tcp://flink@10.244.0.56:6122]] Caused by: [java.net.ConnectException: Connection refused: /10.244.0.56:6122]
2024-02-26 07:13:04,840 DEBUG org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.Watcher [] - Watcher closed {code};;;","26/Feb/24 08:07;gyfora;It looks like there is a race condition between handling the TaskManager failure and the JobManager shutdown in the adaptive scheduler which leads to a terminal failed state. 

If I only terminate the JobManager (scale down the k8s Deployment replicas to 0) then basically nothing happens to the HA metadata. (which is expected):


{noformat}
2024-02-26 07:59:07,413 DEBUG org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.informers.impl.cache.Reflector [] - Event received MODIFIED ConfigMap resourceVersion v6719805 for v1/namespaces/default/configmaps
2024-02-26 07:59:08,096 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2024-02-26 07:59:08,097 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesApplicationClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2024-02-26 07:59:08,098 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
2024-02-26 07:59:08,308 DEBUG org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.Watcher [] - Watcher closed
2024-02-26 07:59:08,508 WARN  org.apache.pekko.actor.CoordinatedShutdown                   [] - Could not addJvmShutdownHook, due to: Shutdown in progress{noformat};;;","26/Feb/24 10:16;gyfora;I opened a new ticket to track this issue explicitly for the adaptive scheduler: https://issues.apache.org/jira/browse/FLINK-34518;;;",,,,,,,,,,
TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding failed,FLINK-34450,13568720,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,rmetzger,mapohl,mapohl,16/Feb/24 11:13,08/Apr/24 18:13,04/Jun/24 20:40,,1.20.0,,,,,,,1.20.0,,,,Runtime / Task,,,,0,github-actions,test-stability,,,"https://github.com/XComp/flink/actions/runs/7927275243/job/21643615491#step:10:9880

{code}
Error: 07:48:06 07:48:06.643 [ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.309 s <<< FAILURE! -- in org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest
Error: 07:48:06 07:48:06.646 [ERROR] org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding -- Time elapsed: 0.036 s <<< FAILURE!
Feb 16 07:48:06 Output was not correct.: array lengths differed, expected.length=8 actual.length=7; arrays first differed at element [6]; expected:<Watermark @ 5> but was:<Watermark @ 6>
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:78)
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:28)
Feb 16 07:48:06 	at org.junit.Assert.internalArrayEquals(Assert.java:534)
Feb 16 07:48:06 	at org.junit.Assert.assertArrayEquals(Assert.java:285)
Feb 16 07:48:06 	at org.apache.flink.streaming.util.TestHarnessUtil.assertOutputEquals(TestHarnessUtil.java:59)
Feb 16 07:48:06 	at org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding(TwoInputStreamTaskTest.java:248)
Feb 16 07:48:06 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 16 07:48:06 Caused by: java.lang.AssertionError: expected:<Watermark @ 5> but was:<Watermark @ 6>
Feb 16 07:48:06 	at org.junit.Assert.fail(Assert.java:89)
Feb 16 07:48:06 	at org.junit.Assert.failNotEquals(Assert.java:835)
Feb 16 07:48:06 	at org.junit.Assert.assertEquals(Assert.java:120)
Feb 16 07:48:06 	at org.junit.Assert.assertEquals(Assert.java:146)
Feb 16 07:48:06 	at org.junit.internal.ExactComparisonCriteria.assertElementsEqual(ExactComparisonCriteria.java:8)
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:76)
Feb 16 07:48:06 	... 6 more
{code}

I couldn't reproduce it locally with 20000 runs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 18:13:25 UTC 2024,,,,,,,,,,"0|z1nfvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/24 00:07;roman;1.20 master: https://github.com/apache/flink/actions/runs/8545965922/job/23415690241#step:10:9900;;;","04/Apr/24 19:12;rmetzger;Builds on master are failing with this -- setting to blocker: https://github.com/apache/flink/actions/runs/8545965922/job/23415690241;;;","05/Apr/24 08:22;rmetzger;I'm checking how easy this is to reproduce with GHA and if Romans latest changes have anything to do with it.;;;","05/Apr/24 14:53;rskraba;1.20 Jdk8 https://github.com/apache/flink/actions/runs/8545965922/job/23415690241#step:10:10235
;;;","08/Apr/24 18:13;rmetzger;I was not able to reproduce the issue with a single push, so it is ""just"" a build instability.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink build took too long,FLINK-34449,13568686,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,16/Feb/24 08:02,29/Feb/24 07:11,04/Jun/24 20:40,,1.17.2,1.18.1,,,,,,,,,,Build System / CI,Test Infrastructure,,,0,test-stability,,,,"We saw a timeout when building Flink in e2e1 stage. No logs are available to investigate the issue:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57551&view=logs&j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6

{code}
Nothing to show. Final logs are missing. This can happen when the job is cancelled or times out.
{code}

I'd consider this an infrastructure issue but created the Jira issue for documentation purposes. Let's see whether that pops up again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 29 07:11:39 UTC 2024,,,,,,,,,,"0|z1nfo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 13:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57761&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678;;;","23/Feb/24 11:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57807&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678;;;","29/Feb/24 07:11;mapohl;This time, it happened in the caching step:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57957&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=5f5e3bcf-c82b-57ca-7f80-f293d0ad4448&l=1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogLocalRecoveryITCase#testRestartTM failed fatally with 127 exit code,FLINK-34448,13568683,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,16/Feb/24 07:52,18/Feb/24 06:36,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57550&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8897
\
{code}
Feb 16 02:43:47 02:43:47.142 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.2.2:test (integration-tests) on project flink-tests: 
Feb 16 02:43:47 02:43:47.142 [ERROR] 
Feb 16 02:43:47 02:43:47.142 [ERROR] Please refer to /__w/1/s/flink-tests/target/surefire-reports for the individual test results.
Feb 16 02:43:47 02:43:47.142 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Feb 16 02:43:47 02:43:47.142 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Feb 16 02:43:47 02:43:47.142 [ERROR] Command was /bin/sh -c cd '/__w/1/s/flink-tests' && '/usr/lib/jvm/jdk-11.0.19+7/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/1/s/flink-tests/target/surefire/surefirebooter-20240216015747138_560.jar' '/__w/1/s/flink-tests/target/surefire' '2024-02-16T01-57-43_286-jvmRun4' 'surefire-20240216015747138_558tmp' 'surefire_185-20240216015747138_559tmp'
Feb 16 02:43:47 02:43:47.142 [ERROR] Error occurred in starting fork, check output in log
Feb 16 02:43:47 02:43:47.142 [ERROR] Process Exit Code: 127
Feb 16 02:43:47 02:43:47.142 [ERROR] Crashed tests:
Feb 16 02:43:47 02:43:47.142 [ERROR] org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase
Feb 16 02:43:47 02:43:47.142 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Feb 16 02:43:47 02:43:47.142 [ERROR] Command was /bin/sh -c cd '/__w/1/s/flink-tests' && '/usr/lib/jvm/jdk-11.0.19+7/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/1/s/flink-tests/target/surefire/surefirebooter-20240216015747138_560.jar' '/__w/1/s/flink-tests/target/surefire' '2024-02-16T01-57-43_286-jvmRun4' 'surefire-20240216015747138_558tmp' 'surefire_185-20240216015747138_559tmp'
Feb 16 02:43:47 02:43:47.142 [ERROR] Error occurred in starting fork, check output in log
Feb 16 02:43:47 02:43:47.142 [ERROR] Process Exit Code: 127
Feb 16 02:43:47 02:43:47.142 [ERROR] Crashed tests:
Feb 16 02:43:47 02:43:47.142 [ERROR] org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase
Feb 16 02:43:47 02:43:47.142 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/24 08:52;mapohl;FLINK-34448.head.log.gz;https://issues.apache.org/jira/secure/attachment/13066761/FLINK-34448.head.log.gz",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 18 06:36:36 UTC 2024,,,,,,,,,,"0|z1nfnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/24 08:27;mapohl;FYI: {{GroupCombineITCase}}/{{GroupReduceITCase}} were executed while the error occurred
{code}
grep ""^Test "" mvn-4.log | sed -e 's/ is running.//g' -e 's/ successfully run.//g' | sort | uniq -c | grep -v ""      2""
      1 Test testCheckPartitionShuffleGroupBy[Execution mode = COLLECTION](org.apache.flink.test.operators.GroupCombineITCase)
      1 Test testCorrectnessOfAllGroupReduceForTuplesWithCombine[Execution mode = COLLECTION](org.apache.flink.test.operators.GroupReduceITCase)
      1 Test testCorrectnessOfGroupReduceOnCustomTypeWithKeyExtractorAndCombine[Execution mode = COLLECTION](org.apache.flink.test.operators.GroupReduceITCase)
      1 Test testCorrectnessOfGroupReduceOnTuplesWithCombine[Execution mode = COLLECTION](org.apache.flink.test.operators.GroupReduceITCase)
      1 Test testRestartTM[delegated state backend type = org.apache.flink.runtime.state.hashmap.HashMapStateBackend@29a60c27](org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase)
{code};;;","16/Feb/24 08:49;mapohl;The test execution stops in the middle of nowhere and a new test {{CheckpointIntervalDuringBacklogITCase}} is started without the TestLogger log output (which can be explained by the fact that [CheckpointIntervalDuringBacklogITCase:65|https://github.com/apache/flink/blob/615e824195d23612d4b008e2cada92709d0a14af/flink-tests/src/test/java/org/apache/flink/test/checkpointing/CheckpointIntervalDuringBacklogITCase.java#L65] isn't extending the {{TestLogger}} class:

{code}
[...]
02:20:21,440 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - KeyedProcess (2/2) (d28075ab570d3483e626c635563db92e_0a448493b4782967b150582570326227_1_0) switched from INITIALIZING to RUNNING.
02:20:21,442 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - KeyedProcess (1/2) (d28075ab570d3483e626c635563db92e_0a448493b4782967b150582570326227_0_0) switched from INITIALIZING to RUNNING.
02:20:21,560 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1708050021549 for job 4f5239e2bda1b74f4b8f920dc7c8568e.
02:20:21,633 [KeyedProcess (1/2)#0] INFO  org.apache.flink.state.changelog.ChangelogKeyedStateBackend  [] - Initialize Materialization. Current changelog writers last append to sequence number 1
02:20:21,636 [KeyedProcess (1/2)#0] INFO  org.apache.flink.state.changelog.ChangelogKeyedStateBackend  [] - Starting materialization from 0 : 1
02:20:21,741 [KeyedProcess (2/2)#0] INFO  org.apache.flink.state.changelog.ChangelogKeyedStateBackend  [] - snapshot of KeyedProcess (2/2)#0 for checkpoint 1, change range: 0..1, materialization ID 0
02:21:00,567 [                main] INFO  org.apache.flink.runtime.testutils.PseudoRandomValueSelector [] - Randomly selected false for taskmanager.network.memory.buffer-debloat.enabled
02:21:00,577 [                main] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
02:21:00,577 [                main] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
02:21:00,577 [                main] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
02:21:00,579 [                main] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
02:21:00,579 [                main] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
02:21:00,590 [                main] INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Flink Mini Cluster
02:21:00,599 [                main] INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Metrics Registry
[...]
{code}

The test that runs afterwards can be identified through the artificial error stacktrace in the logs:
{code}
02:21:04,987 [Source: Custom Source -> Sink: Unnamed (1/1)#0] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source -> Sink: Unnamed (1/1)#0 (da1d8472a75b0152e15291de926c34a2_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAIL
ED with failure cause:
java.lang.RuntimeException: Artificial Exception
        at org.apache.flink.test.checkpointing.CheckpointRestoreWithUidHashITCase$StatefulSource.run(CheckpointRestoreWithUidHashITCase.java:256) ~[test-classes/:?]
        at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:113) ~[flink-streaming-java-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:71) ~[flink-streaming-java-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:338) ~[flink-streaming-java-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
{code};;;","16/Feb/24 08:50;mapohl;[~roman] [~Wencong Liu] Can one of you look into this issue or delegate this issue to the right person to investigate?;;;","17/Feb/24 14:14;Wencong Liu;Maybe [~Yanfei Lei] could take a look 😄.;;;","18/Feb/24 03:13;Yanfei Lei;From the watchdog:
{code:java}
Feb 16 02:20:16 02:20:16.815 [INFO] Running org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase
Feb 16 02:20:23 02:20:23.665 [INFO] Running org.apache.flink.test.checkpointing.SavepointFormatITCase
Feb 16 02:20:40 02:20:40.052 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.76 s -- in org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase
Feb 16 02:20:42 02:20:42.775 [INFO] Running org.apache.flink.test.checkpointing.AutoRescalingITCase
Feb 16 02:20:57 02:20:57.705 [INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.04 s -- in org.apache.flink.test.checkpointing.SavepointFormatITCase
Feb 16 02:21:00 02:21:00.066 [INFO] Running org.apache.flink.test.checkpointing.CheckpointRestoreWithUidHashITCase
Feb 16 02:21:06 02:21:06.478 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.408 s -- in org.apache.flink.test.checkpointing.CheckpointRestoreWithUidHashITCase
Feb 16 02:21:08 02:21:08.805 [INFO] Running org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase
Feb 16 02:21:12 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale keyed_different_parallelism from 12 to 7, sourceSleepMs = 0, buffersPerChannel = 0].
Feb 16 02:21:20 Starting org.apache.flink.test.checkpointing.UnalignedCheckpointRescaleITCase#shouldRescaleUnalignedCheckpoint[downscale keyed_different_parallelism from 12 to 7, sourceSleepMs = 0, buffersPerChannel = 0]. {code}
The ChangelogLocalRecoveryITCase started running at {*}02:20:16{*}, and by {*}02:43:47{*},  ChangelogLocalRecoveryITCase had not finished yet, so the VM failed fatally with 127 exit code. While ChangelogLocalRecoveryITCase is stuck, other tests(SavepointFormatITCase,AutoRescalingITCase...) are running.

 ;;;","18/Feb/24 06:36;roman;[This|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57550&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8072] looks strange to me:
{code}
2024-02-16T02:20:16.8162791Z Feb 16 02:20:16 02:20:16.815 [INFO] Running org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase
2024-02-16T02:20:21.8146124Z Inconsistency detected by ld.so: dl-tls.c: 493: _dl_allocate_tls_init: Assertion `listp->slotinfo[cnt].gen <= GL(dl_tls_generation)' failed!
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ActiveResourceManagerTest#testWorkerRegistrationTimeoutNotCountingAllocationTime still fails on slow machines,FLINK-34447,13568571,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,15/Feb/24 11:00,16/Feb/24 11:48,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"This appeared in this [PR CI run|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57529&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7997] of FLINK-34427.
{code}
Feb 14 18:50:01 18:50:01.283 [ERROR] Tests run: 18, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.665 s <<< FAILURE! -- in org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest
Feb 14 18:50:01 18:50:01.283 [ERROR] org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.testWorkerRegistrationTimeoutNotCountingAllocationTime -- Time elapsed: 0.197 s <<< FAILURE!
Feb 14 18:50:01 java.lang.AssertionError: 
Feb 14 18:50:01 
Feb 14 18:50:01 Expecting
Feb 14 18:50:01   <CompletableFuture[Completed: 70e6587e5e4ba9f310031a96bdda2971]>
Feb 14 18:50:01 not to be done.
Feb 14 18:50:01 Be aware that the state of the future in this message might not reflect the one at the time when the assertion was performed as it is evaluated later on
Feb 14 18:50:01 	at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest$15.lambda$new$3(ActiveResourceManagerTest.java:982)
Feb 14 18:50:01 	at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest$Context.runTest(ActiveResourceManagerTest.java:1133)
Feb 14 18:50:01 	at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest$15.<init>(ActiveResourceManagerTest.java:963)
Feb 14 18:50:01 	at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerTest.testWorkerRegistrationTimeoutNotCountingAllocationTime(ActiveResourceManagerTest.java:946)
Feb 14 18:50:01 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 14 18:50:01 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Feb 14 18:50:01 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Feb 14 18:50:01 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Feb 14 18:50:01 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Feb 14 18:50:01 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}

But I was able to reproduce it locally as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-15 11:00:14.0,,,,,,,,,,"0|z1neyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlValidatorException with LATERAL TABLE and JOIN,FLINK-34446,13568562,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,jeyhunkarimov,jingge,jingge,15/Feb/24 09:41,20/Feb/24 11:26,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Ecosystem,Table SQL / Planner,,,0,pull-request-available,,,,"found one regression issue. Query working Flink 1.17.2, but failing with Flink 1.18.+

 
{code:java}
-- Query working Flink 1.17.2, but failing with Flink 1.18.+

-- -- [ERROR] Could not execute SQL statement. Reason:

-- -- org.apache.calcite.sql.validate.SqlValidatorException: Table 's' not found

SELECT * FROM sample as s,
LATERAL TABLE(split(s.id,'[01]'))
CROSS JOIN (VALUES ('A'), ('B'));
{code}

The problem is not related to the the alias scope. Even if we replace split(s.id.. ) with split(id,...) the error

{code:java}
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'id' not found in any table
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
{code}

will be generated. This seems to be Calcite issue, since this test fails on Calcite v1.32 and does not fail on Calcite 1.29.0 and 1.30.0.
We tested it with Calcite versions 1.31.0, 1.32.0, 1.33.0, 1.34.0, 1.35.0, 1.36.0 and the main branch (c774c313a81d01c4e3e77cf296d04839c5ab04c0). The issue still remains",,,,,,,,,,,,,FLINK-31362,,,,,,,,,,,,,,,,,,,,,,,CALCITE-6266,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 12:18:42 UTC 2024,,,,,,,,,,"0|z1newo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/24 12:43;Sergey Nuyanzin;most probably came with 1.31.0 as part of  CALCITE-35;;;","16/Feb/24 05:15;Sergey Nuyanzin;[~jingge] , [~jeyhunkarimov] 
I applied similar fix I submitted for Calcite 
it seems passing the test mentioned in the issue description

please have a look;;;","19/Feb/24 12:18;jingge;Let's wait until the PR [https://github.com/apache/calcite/pull/3690] has been reviewed and is accepted.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate new endpoint with Flink UI metrics section,FLINK-34445,13568474,13564417,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ruanhang1993,mason6345,mason6345,14/Feb/24 19:22,03/Jun/24 17:36,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,Runtime / Web Frontend,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 17:36:07 UTC 2024,,,,,,,,,,"0|z1ned4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/24 19:38;mason6345;[~ruanhang1993] I believe you expressed some interest in this FLIP. Unfortunately, my employer makes it difficult for me to contribute this change to OSS. Is it possible that someone from your side could work on this? I'd be happy to review but need some help for the contribution;;;","27/Mar/24 01:37;ruanhang1993;Sure, I would like to help in this part. When should I finish this feature?;;;","27/Mar/24 05:37;mason6345;Anytime after https://issues.apache.org/jira/browse/FLINK-34444 is resolved for which I already have a PR. I guess anytime before 1.20 Flink release?;;;","27/Mar/24 05:56;ruanhang1993;Fine, I will take a look at this. Thanks ~;;;","23/Apr/24 23:26;masc;[~ruanhang1993] I reassigned to you. Thank you for volunteering, I will help review!;;;","03/Jun/24 17:36;masc;[~ruanhang1993] do you still have time to work on this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new endpoint handler to flink,FLINK-34444,13568473,13564417,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masc,mason6345,mason6345,14/Feb/24 19:21,23/Apr/24 23:25,04/Jun/24 20:40,23/Apr/24 23:25,,,,,,,,1.20.0,,,,Runtime / Metrics,Runtime / REST,,,0,pull-request-available,,,,Add new endpoint handler to flink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 23:25:54 UTC 2024,,,,,,,,,,"0|z1necw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 23:25;masc;Master CI passes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNFileReplicationITCase.testPerJobModeWithCustomizedFileReplication failed when deploying job cluster,FLINK-34443,13568393,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,14/Feb/24 09:51,19/Feb/24 11:44,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Build System / CI,Runtime / Coordination,Test Infrastructure,,0,github-actions,test-stability,,,"https://github.com/apache/flink/actions/runs/7895502206/job/21548246199#step:10:28804

{code}
Error: 03:04:05 03:04:05.066 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 68.10 s <<< FAILURE! -- in org.apache.flink.yarn.YARNFileReplicationITCase
Error: 03:04:05 03:04:05.067 [ERROR] org.apache.flink.yarn.YARNFileReplicationITCase.testPerJobModeWithCustomizedFileReplication -- Time elapsed: 1.982 s <<< ERROR!
Feb 14 03:04:05 org.apache.flink.client.deployment.ClusterDeploymentException: Could not deploy Yarn job cluster.
Feb 14 03:04:05 	at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:566)
Feb 14 03:04:05 	at org.apache.flink.yarn.YARNFileReplicationITCase.deployPerJob(YARNFileReplicationITCase.java:109)
Feb 14 03:04:05 	at org.apache.flink.yarn.YARNFileReplicationITCase.lambda$testPerJobModeWithCustomizedFileReplication$0(YARNFileReplicationITCase.java:73)
Feb 14 03:04:05 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
Feb 14 03:04:05 	at org.apache.flink.yarn.YARNFileReplicationITCase.testPerJobModeWithCustomizedFileReplication(YARNFileReplicationITCase.java:73)
Feb 14 03:04:05 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 14 03:04:05 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Feb 14 03:04:05 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Feb 14 03:04:05 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Feb 14 03:04:05 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Feb 14 03:04:05 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Feb 14 03:04:05 Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/root/.flink/application_1707879779446_0002/log4j-api-2.17.1.jar could only be written to 0 of the 1 minReplication nodes. There are 2 datanode(s) running and 2 node(s) are excluded in this operation.
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2260)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2813)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:908)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:577)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:549)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:518)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)
Feb 14 03:04:05 	at java.security.AccessController.doPrivileged(Native Method)
Feb 14 03:04:05 	at javax.security.auth.Subject.doAs(Subject.java:422)
Feb 14 03:04:05 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)
Feb 14 03:04:05 
Feb 14 03:04:05 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1579)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.Client.call(Client.java:1525)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.Client.call(Client.java:1422)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:231)
Feb 14 03:04:05 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
Feb 14 03:04:05 	at com.sun.proxy.$Proxy113.addBlock(Unknown Source)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:520)
Feb 14 03:04:05 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 14 03:04:05 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
Feb 14 03:04:05 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
Feb 14 03:04:05 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
Feb 14 03:04:05 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
Feb 14 03:04:05 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
Feb 14 03:04:05 	at com.sun.proxy.$Proxy116.addBlock(Unknown Source)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1082)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1898)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1700)
Feb 14 03:04:05 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:707)
{code}

This could be a GHA infrastructure issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34418,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 11:44:38 UTC 2024,,,,,,,,,,"0|z1ndv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/24 09:52;mapohl;Maybe related to FLINK-34418;;;","16/Feb/24 10:38;mapohl;No disk space issues reported here:
* https://github.com/apache/flink/actions/runs/7925038590/job/21637836033#step:10:27178
* https://github.com/apache/flink/actions/runs/7925038590/job/21637828343#step:10:27633;;;","19/Feb/24 09:19;mapohl;* [https://github.com/apache/flink/actions/runs/7938595181/job/21677803913#step:10:28799]
 * [https://github.com/apache/flink/actions/runs/7938595184/job/21677788845#step:10:27633]
 * [https://github.com/apache/flink/actions/runs/7938595184/job/21677813511#step:10:28731]
 * [https://github.com/apache/flink/actions/runs/7938595184/job/21677790189#step:10:27633]
 * [https://github.com/apache/flink/actions/runs/7945888022/job/21693407019#step:10:28813]
 * [https://github.com/apache/flink/actions/runs/7945888201/job/21693403892#step:10:28806]
 * [https://github.com/apache/flink/actions/runs/7945888201/job/21693426922#step:10:27716]
 * [https://github.com/apache/flink/actions/runs/7946115091/job/21693601123#step:10:27716]
 * [https://github.com/apache/flink/actions/runs/7953599167/job/21710055146#step:10:27671]
 * [https://github.com/apache/flink/actions/runs/7953599343/job/21710039735#step:10:27228]
 * [https://github.com/apache/flink/actions/runs/7954185052/job/21711406254#step:10:27707];;;","19/Feb/24 11:44;mapohl;FLINK-34418 is resolved. Let's see whether these kind of failures also disappear. We can close this Jira issue as a duplicate of FLINK-34418 if it doesn't reappear in by the end of the week.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support optimizations for pre-partitioned [external] data sources,FLINK-34442,13568340,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jeyhunkarimov,jeyhunkarimov,jeyhunkarimov,13/Feb/24 18:21,07/Mar/24 09:57,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,1,pull-request-available,,,,"There are some use-cases in which data sources are pre-partitioned:

- Kafka broker is already partitioned w.r.t. some key[s]
- There are multiple [Flink] jobs  that materialize their outputs and read them as input subsequently

One of the main benefits is that we might avoid unnecessary shuffling. 
There is already an experimental feature in DataStream to support a subset of these [1].
We should support this for Flink Table/SQL as well. 

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/experimental/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-13 18:21:21.0,,,,,,,,,,"0|z1ndjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Documentation for flink-sql-runner-example in Kubernetes Operator Documentation,FLINK-34441,13568336,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,prakash.tiwari,prakash.tiwari,13/Feb/24 16:52,15/Feb/24 07:34,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"There isn't a direct way available to submit SQL script based jobs to the Flink Kubernetes Operator. So we have created a [flink-sql-runner-example|https://github.com/apache/flink-kubernetes-operator/tree/release-1.7/examples/flink-sql-runner-example] that helps to run Flink SQL scripts as table API jobs. I believe it's a very useful and important example, and information about this job is missing from the Kubernetes Operator's documentation. Hence I've created this issue to update the documentation to include this example.

The prospect for this issue was discussed here: [https://github.com/apache/flink-kubernetes-operator/pull/596] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-13 16:52:44.0,,,,,,,,,,"0|z1ndig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Debezium Protobuf Confluent Format,FLINK-34440,13568334,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,klam-shop,klam-shop,13/Feb/24 16:45,07/May/24 20:39,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"*Motivation*

Debezium and the Confluent Schema registry can be used to emit Protobuf Encoded messages to Kafka, but Flink does not easily support consuming these messages through a connector.

*Definition of Done*

Add a format `debezium-protobuf-confluent` provided by DebeziumProtobufFormatFactory that supports Debezium messages encoded using Protocol Buffer and the Confluent Schema Registry. 

To consider
 * Mirror the implementation of the `debezium-avro-confluent` format. First implement a `protobuf-confluent` format similar to the existing [Confluent Avro|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/formats/avro-confluent/] format that's provided today, which allows reading/writing protobuf using the Confluent Schema Registry",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 13 16:47:08 UTC 2024,,,,,,,,,,"0|z1ndi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 16:47;klam-shop;Hello! I just added this Jira issue, following the [Contribution Guide.|https://flink.apache.org/how-to-contribute/contribute-code/]

If this ticket warrants a dev@ discussion, I'm happy to open one, please let me know.

Also, I am happy to work on contributing the code to complete this issue.

Looking forward to hearing others' thoughts!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move chown operations to COPY commands in Dockerfile,FLINK-34439,13568325,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mateczagany,mateczagany,mateczagany,13/Feb/24 15:41,14/Feb/24 13:21,04/Jun/24 20:40,14/Feb/24 13:21,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"We can lower the size of the output operator container image if we don't run 'chown' commands in seperate RUN commands inside the Dockerfile, but instead use the '--chown' argument of the COPY command.

Using 'RUN chown...' will copy all the files affected with their whole size to a new layer, duplicating the previous files from the COPY command.

Example:
{code:java}
$ docker image history ghcr.io/apache/flink-kubernetes-operator:ccb10b8
...
<missing>     3 months ago  RUN /bin/sh -c chown -R flink:flink $FLINK...  116MB       buildkit.dockerfile.v0
... {code}
This would mean a 20% reduction in image size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 14 13:21:50 UTC 2024,,,,,,,,,,"0|z1ndg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/24 13:21;gyfora;merged to main 6cab745df9d0a742ab15f341bdf69c8e802b2a39;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes Operator doesn't wait for TaskManager deletion in native mode,FLINK-34438,13568324,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mateczagany,mateczagany,mateczagany,13/Feb/24 15:26,20/Feb/24 11:21,04/Jun/24 20:40,20/Feb/24 11:21,kubernetes-operator-1.6.1,kubernetes-operator-1.7.0,kubernetes-operator-1.8.0,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"This issue was partly fixed in FLINK-32334 but native mode was not included in the fix.

I don't see any downsides with adding the same check to native deployment mode, which would make sure that all TaskManagers were deleted when we shut down a Flink cluster.

There should also be some logs suggesting that the timeout was exceeded instead of silently returning when waiting for the cluster to shut down.

An issue was also mentioned on the mailing list which seems to be related to this: [https://lists.apache.org/thread/4gwj4ob4n9zg7b90vnqohj8x1p0bb5cb]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 11:21:15 UTC 2024,,,,,,,,,,"0|z1ndfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 11:21;gyfora;merged to main a8fd19429e93428e8a2498c32def24aa8ebbd4c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in SQL Client - `s/succeed/succeeded`,FLINK-34437,13568321,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,rmoff,rmoff,13/Feb/24 15:02,28/Feb/24 11:18,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,," 
{code:java}
Flink SQL> CREATE CATALOG c_new WITH ('type'='generic_in_memory');
[INFO] Execute statement succeed. {code}
`{*}Execute statement {color:#FF0000}succeed{color}.{*}` is grammatically incorrect, and should read `{*}Execute statement {color:#FF0000}succeeded{color}.{*}`

 

[https://github.com/apache/flink/blob/5844092408d21023a738077d0922cc75f1e634d7/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java#L214]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 11:18:24 UTC 2024,,,,,,,,,,"0|z1ndf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 11:18;xuyangzhong;Hi, [~jingge] . I see that this PR has been merged. Does this Jira need to be closed?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro schema evolution and compatibility issues in Pulsar connector,FLINK-34436,13568318,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jacek_wislicki,jacek_wislicki,13/Feb/24 14:48,15/Apr/24 09:49,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,,"We noticed a couple of critical issues in the Pulsar-Flink connector related to schema evolution and compatibility. Please see the MRE available at https://github.com/JacekWislicki/test11. More details are in the project's README file, here is the summary:

Library versions:
* Pulsar 3.0.1
* Flink 1.17.2
* Pulsar-Flink connector 4.1.0-1.17

Problems:
* Exception thrown when schema's fields are added/removed
* Avro's enum default value is ignored, instead the last known applied

I believe that I observed the same behaviour in the Pulsar itself, still now we are focusing on the connector, hence I was able to document the problems when using it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 09:49:55 UTC 2024,,,,,,,,,,"0|z1ndeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/24 17:57;jacek_wislicki;Added also a simpler example (without enumerations that seem to be the key problem): https://github.com/JacekWislicki/test12;;;","15/Feb/24 10:15;jacek_wislicki;The problem seems to be in org.apache.flink.connector.pulsar.source.reader.deserializer.PulsarSchemaWrapper.deserialize(Message<byte[]>, Collector<T>):
{code:java}
    @Override
    public void deserialize(Message<byte[]> message, Collector<T> out) throws Exception {
        Schema<T> schema = this.pulsarSchema.getPulsarSchema();
        byte[] bytes = message.getData();
        T instance = schema.decode(bytes);

        out.collect(instance);
    }
{code}
this.pulsarSchema.getPulsarSchema() returns the expected schema, not the schema in which the message was actually encoded. For proper conversion, both source (encoding) and target (decoding) schemas are needed.;;;","19/Feb/24 08:11;jacek_wislicki;A note: in the MRE there is ALWAYS_COMPATIBLE used, although I tested it also with BACKWARD, BACKWARD_TRANSITIVE, FORWARD, FORWARD_TRANSITIVE, FULL and FULL_TRANSITIVE, with the same result.;;;","29/Feb/24 10:19;jacek_wislicki;Please let me know if you have any thought on this. Of course, a good explanation why my understanding of the behaviour is wrong will also be valuable.;;;","15/Apr/24 05:47;syhily;I think this is not a issue. It's just a design on purpose. The schema evolution on Pulsar doesn't get proper designed. We just send the desired schema to Pulsar server and let the validation be performed on server side. The client side shouldn't perform any validation. Instead, we just use the schema provided by user for Pub/Sub messages. Sometimes, we even bypass the schema with pure bytes for (de)serializing messages on flink.;;;","15/Apr/24 09:49;syhily;If you want to consume all the messages with different types from the same topic, the GenericRecordDeserializationSchema may be the best approach to use.

https://github.com/apache/flink-connector-pulsar/blob/main/flink-connector-pulsar/src/main/java/org/apache/flink/connector/pulsar/source/reader/deserializer/GenericRecordDeserializationSchema.java;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.yaml:snakeyaml from 1.31 to 2.2 for flink-connector-elasticsearch,FLINK-34435,13568312,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Feb/24 14:15,21/Mar/24 14:08,04/Jun/24 20:40,21/Mar/24 14:08,,,,,,,,,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,https://github.com/apache/flink-connector-elasticsearch/pull/90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 14:08:04 UTC 2024,,,,,,,,,,"0|z1ndd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 14:08;Sergey Nuyanzin;Merged as [55c5982bba88a3f1806b4939be788d8f77bead72|https://github.com/apache/flink-connector-elasticsearch/commit/55c5982bba88a3f1806b4939be788d8f77bead72];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultSlotStatusSyncer doesn't complete the returned future,FLINK-34434,13568289,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,mapohl,mapohl,13/Feb/24 11:10,20/Feb/24 06:03,04/Jun/24 20:40,20/Feb/24 06:03,1.17.2,1.18.1,1.19.0,1.20.0,,,,1.18.2,1.19.0,1.20.0,,Runtime / Coordination,,,,0,pull-request-available,,,,"When looking into FLINK-34427 (unrelated), I noticed an odd line in [DefaultSlotStatusSyncer:155|https://github.com/apache/flink/blob/15fe1653acec45d7c7bac17071e9773a4aa690a4/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/DefaultSlotStatusSyncer.java#L155] where we complete a future that should be already completed (because the callback is triggered after the {{requestFuture}} is already completed in some way. Shouldn't we complete the {{returnedFuture}} instead?

I'm keeping the priority at {{Major}} because it doesn't seem to have been an issue in the past.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20835,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 06:02:43 UTC 2024,,,,,,,,,,"0|z1nd80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 11:11;mapohl;[~guoyangze] can you have a look at this? Maybe, I'm missing something here.;;;","19/Feb/24 02:25;guoyangze;Good catch [~mapohl]. I think the returnedFuture should be completed here instead.;;;","20/Feb/24 02:42;guoyangze;master: 15af3e49ca42fea1e3f6c53d6d1498315b1322ac
1a494bc1f04571b8b8248d7c2c1af364222a0c61
1.18: e95cb6e73900fbbc2039407be1bd87271b2a950b
21cfe998f4fb21afe24ceb8b6f4fef180e89b9e9;;;","20/Feb/24 06:02;guoyangze;1.19: 8cf29969d9aec4943713f0a6096b703718ce0dd0
45d4dc10248402757e203aa266b19c95e2e93b46;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CollectionFunctionsITCase.test failed due to job restart,FLINK-34433,13568276,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,13/Feb/24 09:38,13/Feb/24 09:38,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Table SQL / Planner,,,,0,github-actions,test-stability,,,"https://github.com/apache/flink/actions/runs/7880739697/job/21503460772#step:10:11312

{code}
Error: 02:33:24 02:33:24.955 [ERROR] Tests run: 439, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 56.57 s <<< FAILURE! -- in org.apache.flink.table.planner.functions.CollectionFunctionsITCase
Error: 02:33:24 02:33:24.956 [ERROR] org.apache.flink.table.planner.functions.CollectionFunctionsITCase.test(TestCase)[81] -- Time elapsed: 1.141 s <<< ERROR!
Feb 13 02:33:24 java.lang.RuntimeException: Job restarted
Feb 13 02:33:24 	at org.apache.flink.streaming.api.operators.collect.UncheckpointedCollectResultBuffer.sinkRestarted(UncheckpointedCollectResultBuffer.java:42)
Feb 13 02:33:24 	at org.apache.flink.streaming.api.operators.collect.AbstractCollectResultBuffer.dealWithResponse(AbstractCollectResultBuffer.java:87)
Feb 13 02:33:24 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:124)
Feb 13 02:33:24 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:126)
Feb 13 02:33:24 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:100)
Feb 13 02:33:24 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:247)
Feb 13 02:33:24 	at org.assertj.core.internal.Iterators.assertHasNext(Iterators.java:49)
Feb 13 02:33:24 	at org.assertj.core.api.AbstractIteratorAssert.hasNext(AbstractIteratorAssert.java:60)
Feb 13 02:33:24 	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$ResultTestItem.test(BuiltInFunctionTestBase.java:383)
Feb 13 02:33:24 	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestSetSpec.lambda$getTestCase$4(BuiltInFunctionTestBase.java:341)
Feb 13 02:33:24 	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestCase.execute(BuiltInFunctionTestBase.java:119)
Feb 13 02:33:24 	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.test(BuiltInFunctionTestBase.java:99)
Feb 13 02:33:24 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 13 02:33:24 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Feb 13 02:33:24 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Feb 13 02:33:24 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Feb 13 02:33:24 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Feb 13 02:33:24 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-13 09:38:01.0,,,,,,,,,,"0|z1nd54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-enable forkReuse for flink-table-planner,FLINK-34432,13568267,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Feb/24 08:09,16/Feb/24 10:30,04/Jun/24 20:40,14/Feb/24 10:45,1.18.2,1.19.0,1.20.0,,,,,1.20.0,,,,Table SQL / Client,Test Infrastructure,Tests,,0,pull-request-available,,,,"With FLINK-18356 resolved, we should re-enable forkReuse for flink-table-planner to speed up the tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,FLINK-29114,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 14 10:45:27 UTC 2024,,,,,,,,,,"0|z1nd34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/24 10:45;martijnvisser;Fixed in apache/flink:master 403694e7b9c213386f3ed9cff21ce2664030ebc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move static BlobWriter methods to separate util,FLINK-34431,13568191,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,12/Feb/24 16:09,12/Feb/24 16:27,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"The BlobWriter interface contains several static methods, some being used, others being de-facto internal methods.
We should move these into a dedicated BlobWriterUtils class so we can properly deal with method visibility.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-12 16:09:16.0,,,,,,,,,,"0|z1ncm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Akka frame size exceeded with many ByteStreamStateHandle being used,FLINK-34430,13568170,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,pnowojski,pnowojski,12/Feb/24 13:06,20/Feb/24 14:48,04/Jun/24 20:40,,1.16.3,1.17.2,1.18.1,1.19.0,,,,,,,,Runtime / Coordination,,,,0,,,,,"The following error can happen
{noformat}
Discarding oversized payload sent to Actor[akka.tcp://flink@xxxx/user/rpc/taskmanager_0#-yyyy]: max allowed size 10485760 bytes, actual size of encoded class org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation was 11212046 bytes.
	
error.stack_trace
akka.remote.OversizedPayloadException: Discarding oversized payload sent to Actor[akka.tcp://flink@xxxx/user/rpc/taskmanager_0#-yyyy]: max allowed size 10485760 bytes, actual size of encoded class org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation was 11212046 bytes.
{noformat}
when https://issues.apache.org/jira/browse/FLINK-26050 is causing large amount of small sst files to be created and never deleted. If those files are small enough to be handled by {{ByteStreamStateHandle}} akka frame size can be exceeded.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26050,FLINK-4399,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 14:48:24 UTC 2024,,,,,,,,,,"0|z1nchk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/24 13:07;pnowojski;Akka frame size can be exceeded as a result of RocksDB creating too many small files.;;;","18/Feb/24 05:57;mayuehappy;[~pnowojski] The reason for so many ssts here is that RocksDB cannot delete these files in auto compaction. Can we trigger a manual Comparison at the appropriate time to avoid having too many sst files and solve this problem from the root?

[~srichter] Just like we choose to trigger async manual compaction after ingest db rescaling to delete extra keys

 

 ;;;","20/Feb/24 14:48;pnowojski;[~roman] is already working on that under FLINK-26050 ticket. However the akka frame size is still an issue. The 10MB limit is dangerously small, especially when the {{ByteStreamHandle}} size limit is bumped to 1MB (which is a pretty reasonable value all things consider). In that case the limit would exceeded with only 10+ files.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding K8S Annotations to Internal Service,FLINK-34429,13568161,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,barakbn@gmail.com,barakbn@gmail.com,barakbn@gmail.com,12/Feb/24 09:40,06/Mar/24 07:48,04/Jun/24 20:40,06/Mar/24 07:48,,,,,,,,1.20.0,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,,"Flink currently supports adding Annotations to the Rest Service (with the configuration key:
kubernetes.rest-service.annotations). 

New configuration key: kubernetes.internal-service.annotations should be added to allow the same functionality for the internal service. 
 
This is useful, for example, when implementing Teleport app discovery. Without special annotation to the service, Teleport floods the JobManger with errors:

 
{code:java}
2024-02-12 09:24:43,747 WARN org.apache.pekko.remote.transport.netty.NettyTransport [] - Remote connection to [/172.16.108.253:39884] failed with org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 10485760: 369295621 - discarded 2024-02-12 09:24:43,753 ERROR org.apache.flink.runtime.blob.BlobServerConnection [] - Error while executing BLOB connection from /172.16.108.253:40954. java.io.IOException: Unknown operation 22 at org.apache.flink.runtime.blob.BlobServerConnection.run(BlobServerConnection.java:116) [flink-dist-1.18.1.jar:1.18.1]  {code}
To avoid these we need to add [special Annotations|https://github.com/gravitational/teleport/blob/master/rfd/0135-kube-apps-discovery.md#annotations] to the internal service.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 07:48:25 UTC 2024,,,,,,,,,,"0|z1ncfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 09:30;dannycranmer;Thanks for raising [~barakbn@gmail.com] , assigning to you.;;;","06/Mar/24 07:48;dannycranmer;Merged commit [{{70975b2}}|https://github.com/apache/flink/commit/70975b258700f12cdb4e9352180be2f4213a6a08] into apache:master ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowAggregateITCase#testEventTimeHopWindow_GroupingSets times out,FLINK-34428,13568156,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,12/Feb/24 08:36,31/May/24 13:22,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Table SQL / API,,,,0,github-actions,test-stability,,,"https://github.com/apache/flink/actions/runs/7866453368/job/21460921339#step:10:15127

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f1770cb7000 nid=0x4ad4d waiting on condition [0x00007f17711f6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000ab48e3a0> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2131)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2099)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2077)
	at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:876)
	at org.apache.flink.table.planner.runtime.stream.sql.WindowAggregateITCase.testTumbleWindowWithoutOutputWindowColumns(WindowAggregateITCase.scala:477)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 13:22:56 UTC 2024,,,,,,,,,,"0|z1nceg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/24 13:22;rskraba;* 1.18 AdaptiveScheduler / Test (module: table) [https://github.com/apache/flink/actions/runs/9295906824/job/25583865489#step:10:17171]

This is the same symptom but occurs on the following tests:
* WindowAggregateITCase.testEventTimeTumbleWindow_Cube
* WindowAggregateITCase.testRelaxFormProctimeCascadeWindowAgg;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManagerTest fails fatally (exit code 239),FLINK-34427,13568155,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,mapohl,mapohl,mapohl,12/Feb/24 08:32,02/Apr/24 10:12,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://github.com/apache/flink/actions/runs/7866453350/job/21460921911#step:10:8959

{code}
Error: 02:28:53 02:28:53.220 [ERROR] Process Exit Code: 239
Error: 02:28:53 02:28:53.220 [ERROR] Crashed tests:
Error: 02:28:53 02:28:53.220 [ERROR] org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest
Error: 02:28:53 02:28:53.220 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Error: 02:28:53 02:28:53.220 [ERROR] Command was /bin/sh -c cd '/root/flink/flink-runtime' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.lang=ALL-UNNAMED' '--add-opens=java.base/java.net=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' '-Xmx768m' '-jar' '/root/flink/flink-runtime/target/surefire/surefirebooter-20240212022332296_94.jar' '/root/flink/flink-runtime/target/surefire' '2024-02-12T02-21-39_495-jvmRun3' 'surefire-20240212022332296_88tmp' 'surefire_26-20240212022332296_91tmp'
Error: 02:28:53 02:28:53.220 [ERROR] Error occurred in starting fork, check output in log
Error: 02:28:53 02:28:53.220 [ERROR] Process Exit Code: 239
Error: 02:28:53 02:28:53.220 [ERROR] Crashed tests:
Error: 02:28:53 02:28:53.221 [ERROR] org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest
Error: 02:28:53 02:28:53.221 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456)
[...]
{code}

The fatal error is triggered most likely within the {{FineGrainedSlotManagerTest}}:
{code}
02:26:39,362 [   pool-643-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'pool-643-thread-1' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@4bbc0b10 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a45cd9a[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:851) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.handleAsync(CompletableFuture.java:2178) ~[?:1.8.0_392]
        at org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer.allocateSlot(DefaultSlotStatusSyncer.java:138) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.allocateSlotsAccordingTo(FineGrainedSlotManager.java:722) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.checkResourceRequirements(FineGrainedSlotManager.java:645) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.lambda$null$12(FineGrainedSlotManager.java:603) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_392]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_392]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_392]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_392]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_392]
Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@4bbc0b10 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a45cd9a[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_392]
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622) ~[?:1.8.0_392]
        at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:826) ~[?:1.8.0_392]
        ... 14 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-33416,,,,,,,,,,,,FLINK-34551,,,,FLINK-34588,FLINK-34589,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 10:12:15 UTC 2024,,,,,,,,,,"0|z1nce8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 11:17;mapohl;[~chesnay] the upstream future {{requestFuture}} is coming from the {{TaskManagerGateway#requestSlot}} RPC call. I would conclude that the RPCEndpoint (considering that the [handleAsync callback|https://github.com/apache/flink/blob/15fe1653acec45d7c7bac17071e9773a4aa690a4/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/DefaultSlotStatusSyncer.java#L138] should be chained up and run in wherever the RPC call is executed) is should down while there's still a scheduled task queued up causing the {{RejectedExecutionException}}. WDYT?;;;","13/Feb/24 11:44;chesnay;The problem is the use of scheduled executors in the FineGrainedSlotManager. It periodically tries to schedule actions unconditionally into the main thread, and this periodic action is also never cancelled.
If the rpc endpoint shuts down during the periodic delay the scheduled action can fire again before the rpc service (and thus scheduled executor) is shut down, running into this error.

This code is plain broken as tt makes assumptions about the lifecycle of the scheduled executor. The loop should be canceled when the FGSM is shut down, and as a safety rail any scheduled action should validate that the FGSM is not shut down yet before scheduling anything into the main thread.;;;","13/Feb/24 12:00;mapohl;Thanks for the clarification. This is an issue that also exists in 1.18. I won't increase the priority to blocker for 1.19 because of that. But we should fix this.;;;","28/Feb/24 07:44;mapohl;The main logs of the following build failure don't reveal it:
https://github.com/apache/flink/actions/runs/8074215483/job/22059411711#step:10:8894

... but the RejectedExecutionException caused a fatal error in {{mvn-4.log}}:
{code}
02:27:03,856 [   pool-720-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'pool-720-thread-1' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@e965fd1[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@77e9006c[Wrapped task = java.util.concur
rent.CompletableFuture$UniHandle@67ac433e]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@579847d[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:315) ~[?:?]
        at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:955) ~[?:?]
        at java.util.concurrent.CompletableFuture.handleAsync(CompletableFuture.java:2382) ~[?:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer.allocateSlot(DefaultSlotStatusSyncer.java:138) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.allocateSlotsAccordingTo(FineGrainedSlotManager.java:722) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.checkResourceRequirements(FineGrainedSlotManager.java:645) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.lambda$checkResourceRequirementsWithDelay$12(FineGrainedSlotManager.java:603) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
        at java.lang.Thread.run(Thread.java:1583) [?:?]
Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@e965fd1[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@77e9006c[Wrapped task = java.util.concurrent.CompletableFuture$UniHandl
e@67ac433e]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@579847d[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:705) ~[?:?]
        at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:754) ~[?:?]
        at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:953) ~[?:?]
        ... 11 more
{code};;;","01/Mar/24 10:27;mapohl;https://github.com/apache/flink/actions/runs/8105495552/job/22154143281#step:10:8956;;;","05/Mar/24 06:28;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58059&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8661;;;","25/Mar/24 14:23;rskraba;1.19 https://github.com/apache/flink/actions/runs/8398671965/job/23003973390#step:10:8944;;;","02/Apr/24 10:12;mapohl;Copied over from FLINK-33416:
* https://github.com/XComp/flink/actions/runs/6472726326/job/17575765131
* 1.19: https://github.com/apache/flink/actions/runs/8467681781/job/23199435037#step:10:8909;;;",,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase.testHybridSelectiveExchangesRestart times out,FLINK-34426,13568154,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,12/Feb/24 08:18,12/Feb/24 08:18,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Runtime / Network,,,,0,github-actions,test-stability,,,"https://github.com/apache/flink/actions/runs/7851900779/job/21429781783#step:10:9052

{code}
""ForkJoinPool-1-worker-3"" #16 daemon prio=5 os_prio=0 cpu=3397.79ms elapsed=11462.88s tid=0x00007f48966b3800 nid=0x7a303 waiting on condition  [0x00007f486e97a000]
   java.lang.Thread.State: WAITING (parking)
	at jdk.internal.misc.Unsafe.park(java.base@11.0.19/Native Method)
	- parking to wait for  <0x00000000a2faa230> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.19/LockSupport.java:194)
	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.19/CompletableFuture.java:1796)
	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.19/ForkJoinPool.java:3118)
	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.19/CompletableFuture.java:1823)
	at java.util.concurrent.CompletableFuture.get(java.base@11.0.19/CompletableFuture.java:1998)
	at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:36)
	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:61)
	at org.apache.flink.test.runtime.BatchShuffleITCaseBase.executeJob(BatchShuffleITCaseBase.java:117)
	at org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart(HybridShuffleITCase.java:79)
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@11.0.19/Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-12 08:18:43.0,,,,,,,,,,"0|z1nce0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerRunnerITCase#testNondeterministicWorkingDirIsDeletedInCaseOfProcessFailure times out,FLINK-34425,13568153,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,12/Feb/24 08:15,13/Feb/24 12:02,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,,"https://github.com/apache/flink/actions/runs/7851900616/job/21429757962#step:10:8844

{code}
Feb 10 03:21:45 ""main"" #1 [498632] prio=5 os_prio=0 cpu=619.91ms elapsed=1653.40s tid=0x00007fbd29695000 nid=498632 waiting on condition  [0x00007fbd2b9f3000]
Feb 10 03:21:45    java.lang.Thread.State: WAITING (parking)
Feb 10 03:21:45 	at jdk.internal.misc.Unsafe.park(java.base@21.0.1/Native Method)
Feb 10 03:21:45 	- parking to wait for  <0x00000000ae6199f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
Feb 10 03:21:45 	at java.util.concurrent.locks.LockSupport.park(java.base@21.0.1/LockSupport.java:371)
Feb 10 03:21:45 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(java.base@21.0.1/AbstractQueuedSynchronizer.java:519)
Feb 10 03:21:45 	at java.util.concurrent.ForkJoinPool.unmanagedBlock(java.base@21.0.1/ForkJoinPool.java:3780)
Feb 10 03:21:45 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@21.0.1/ForkJoinPool.java:3725)
Feb 10 03:21:45 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(java.base@21.0.1/AbstractQueuedSynchronizer.java:1707)
Feb 10 03:21:45 	at java.lang.ProcessImpl.waitFor(java.base@21.0.1/ProcessImpl.java:425)
Feb 10 03:21:45 	at org.apache.flink.test.recovery.TaskManagerRunnerITCase.testNondeterministicWorkingDirIsDeletedInCaseOfProcessFailure(TaskManagerRunnerITCase.java:126)
Feb 10 03:21:45 	at java.lang.invoke.LambdaForm$DMH/0x00007fbccb1b8000.invokeVirtual(java.base@21.0.1/LambdaForm$DMH)
Feb 10 03:21:45 	at java.lang.invoke.LambdaForm$MH/0x00007fbccb1b8800.invoke(java.base@21.0.1/LambdaForm$MH)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 13 11:47:30 UTC 2024,,,,,,,,,,"0|z1ncds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 11:47;mapohl;This looks like a test issue. The TaskManager process is destroyed in [TaskManagerRunnerITCase:124|https://github.com/apache/flink/blob/d6c7eee8243b4fe3e593698f250643534dc79cb5/flink-tests/src/test/java/org/apache/flink/test/recovery/TaskManagerRunnerITCase.java#L124] but doesn't get back properly causing the timeout in {{#waitFor()}} in [TaskManagerRunnerITCase:126|https://github.com/apache/flink/blob/d6c7eee8243b4fe3e593698f250643534dc79cb5/flink-tests/src/test/java/org/apache/flink/test/recovery/TaskManagerRunnerITCase.java#L126].

I'm gonna lower this issue's priority to {{Major}}. I don't consider it in any way problematic for the upcoming 1.19 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedBlockingSubpartitionWriteReadTest#testRead10ConsumersConcurrent times out,FLINK-34424,13568151,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,yunfengzhou,mapohl,mapohl,12/Feb/24 07:54,27/Feb/24 07:58,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Runtime / Network,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57446&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9151

{code}
Feb 11 13:55:29 ""ForkJoinPool-50-worker-25"" #414 daemon prio=5 os_prio=0 tid=0x00007f19503af800 nid=0x284c in Object.wait() [0x00007f191b6db000]
Feb 11 13:55:29    java.lang.Thread.State: WAITING (on object monitor)
Feb 11 13:55:29 	at java.lang.Object.wait(Native Method)
Feb 11 13:55:29 	at java.lang.Thread.join(Thread.java:1252)
Feb 11 13:55:29 	- locked <0x00000000e2e019a8> (a org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest$LongReader)
Feb 11 13:55:29 	at org.apache.flink.core.testutils.CheckedThread.trySync(CheckedThread.java:104)
Feb 11 13:55:29 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:92)
Feb 11 13:55:29 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:81)
Feb 11 13:55:29 	at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest.testRead10ConsumersConcurrent(BoundedBlockingSubpartitionWriteReadTest.java:177)
Feb 11 13:55:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 06:53:04 UTC 2024,,,,,,,,,,"0|z1ncdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 16:34;mapohl;I'm wondering whether that has anything to do with the blocked reader thread:
{code}
Feb 11 13:55:29 ""Thread-76"" #476 daemon prio=5 os_prio=0 tid=0x00007f190bbf1800 nid=0x5a40 waiting for monitor entry [0x00007f191bce4000]
Feb 11 13:55:29    java.lang.Thread.State: BLOCKED (on object monitor)
Feb 11 13:55:29         at net.jpountz.lz4.LZ4JNI.LZ4_decompress_fast(Native Method)
Feb 11 13:55:29         at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:70)
Feb 11 13:55:29         at org.apache.flink.runtime.io.compression.Lz4BlockDecompressor.decompress(Lz4BlockDecompressor.java:68)
Feb 11 13:55:29         at org.apache.flink.runtime.io.network.buffer.BufferDecompressor.decompress(BufferDecompressor.java:126)
Feb 11 13:55:29         at org.apache.flink.runtime.io.network.buffer.BufferDecompressor.decompressToIntermediateBuffer(BufferDecompressor.java:68)
Feb 11 13:55:29         at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest.readLongs(BoundedBlockingSubpartitionWriteReadTest.java:206)
Feb 11 13:55:29         at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest.access$000(BoundedBlockingSubpartitionWriteReadTest.java:55)
Feb 11 13:55:29         at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionWriteReadTest$LongReader.go(BoundedBlockingSubpartitionWriteReadTest.java:323)
Feb 11 13:55:29         at org.apache.flink.core.testutils.CheckedThread.run(CheckedThread.java:67)
{code}

The test was started at 13:32:18.152 and timed out at 13:55:39;;;","13/Feb/24 16:38;mapohl;[~piotr.nowicki] (because it's networking; feel free to delegate) [~yunfengzhou] (because you touched the code in FLINK-33743 recently): Can someone help with investigating the cause of the issue?;;;","13/Feb/24 16:42;piotr.nowicki;[~mapohl] I guess you wanted to mention [~pnowojski] ? :);;;","13/Feb/24 17:03;mapohl;args. Didn't we have this in the past? Sorry again - the auto completion and the guy behind the screen are to blame here. Yes, you're right.;;;","13/Feb/24 17:06;piotr.nowicki;Yes, we did! A week ago was an anniversary of the last mention :D

No problem and good luck with concurrency issues :);;;","14/Feb/24 08:38;pnowojski;:D

[~mapohl] I haven't made any modifications to this area of code since a very long time and I unfortunately recently don't have much time to investigate this kind of issues. I would suggest to ping someone else. Maybe authors of the most recent change to this test file: FLINK-33743

[~yunfengzhou][~tanyuxin]
;;;","19/Feb/24 10:12;yunfengzhou;[~mapohl] [~pnowojski] I'll try to figure out the cause of this problem. FLINK-33743 should not affect Flink's behavior when using bounded blocking shuffle, so the problem should have existed before my commits, but anyway I'll try looking into this.;;;","19/Feb/24 10:41;mapohl;Thank you. Much appreciated :);;;","20/Feb/24 02:48;tanyuxin;[~mapohl] [~pnowojski] Sorry for the late reply. I tried to reproduce the issue, but it can not be reproduced in my local environment. I think it may be an occasional case with a low probability. I and [~yunfengzhou] will continue investigating the cause.;;;","22/Feb/24 06:53;yunfengzhou;Hi [~mapohl]  and [~pnowojski], [~tanyuxin] and I have not been able to find the cause of this problem for now. As far as I have investigated, a Java thread might be blocked without an explicit ""waiting to lock ..."" hint if it involves JNI calls like MonitorEnter/MonitorExit, internal locks like Semaphore and CountDownLatch, or low-level synchronization primitives like LockSupport.park(). However I did not find any match of these patterns in the blocked thread's stack. Besides, it seems that Java's GC might also cause a thread to be in blocking status, so we are not even sure there are blocking or deadlock issues to resolve.

Given that we could not reproduce this error in our local environment, nor have we found a similar error in Flink CI history, it seems that the error is a low-probability issue that can be temporarily ignored. Could we mark this issue as low priority for now and maybe revisit it when we have more inputs on exceptions like this?;;;",,,,,,,,,,,,,,,,,,,,,,,
Make tool/ci/compile_ci.sh not necessarily rely on clean phase,FLINK-34423,13568150,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,12/Feb/24 07:43,12/Feb/24 07:43,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,,Build System / CI,,,,0,github-actions,,,,"The GHA job {{Test packaging/licensing}} job runs [.github/workflows/template.flink-ci.yml:169|https://github.com/apache/flink/blob/85edd784fc72c1784849e2b122cbf3215f89817c/.github/workflows/template.flink-ci.yml#L169] which enables Maven's {{clean}} phase. This triggers redundant work because the {{Test packaging/licensing}} job wouldn't utilize the build artifacts of the previous {{Compile}} job but rerun the {{test-compile}} once more.

Disabling {{clean}} should improve the runtime of the {{Test packaging/licensing}} job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-12 07:43:37.0,,,,,,,,,,"0|z1ncd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchTestBase doesn't actually use MiniClusterExtension,FLINK-34422,13568116,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,11/Feb/24 00:17,12/Feb/24 16:22,04/Jun/24 20:40,12/Feb/24 16:22,1.18.1,,,,,,,1.18.2,1.19.0,1.20.0,,Test Infrastructure,,,,0,pull-request-available,,,,"BatchTestBase sets up a table environment in instance fields, which runs before the BeforeEachCallback from the MiniClusterExtension has time to run.
As a result the table environment internally creates a local stream environment, due to which _all_ test extending the BatchTestBase are spawning separate mini clusters for every single job.

I believe this is on reason why we had troubles with enabling fork-reuse in the planner module",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 12 11:01:29 UTC 2024,,,,,,,,,,"0|z1nc5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/24 11:01;chesnay;master:
65727fb943807f1ff5345419ce389c5734df0cb4
4c4643c3251c284260c96a2110f4b78c8a369723
1.19:
994850d33a32f1ac27cee755f976b86208f911e3
3fcbe3df48904d10ae29a35800474b18af9e7172
1.18:
d69393678efe7e26bd5168407a1c862cd4a0e148;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip post-compile checks in compile.sh if fast profile is active,FLINK-34421,13568101,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,10/Feb/24 12:00,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,,,1.18.2,1.20.0,,,Build System / CI,,,,0,pull-request-available,,,,"We currently waste time in our e2e tests, re-running a bunch of post-compile checks (like packaging/licensing).
Let's couple this to the -Dfast/-Pfast switches.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-10 12:00:22.0,,,,,,,,,,"0|z1nc28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Various YARN tests fail after failing to download hadoop.tar.gz,FLINK-34420,13568095,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,roman,roman,roman,10/Feb/24 08:47,27/Feb/24 14:16,04/Jun/24 20:40,11/Feb/24 12:32,1.18.2,1.19.0,1.20.0,,,,,1.17.3,1.18.2,1.19.0,,Tests,,,,0,,,,,"https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1702&view=logs&j=0e31ee24-31a6-528c-a4bf-45cde9b2a14e&t=696bc156-f753-5888-468e-42d78df39222&l=11334

https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1700&view=logs&j=bbbd0720-137e-5f59-95a5-b5d332f196d3&t=4769aa47-e87b-5ecd-1fb2-14d52396866d&l=9937

```
2024-02-09T19:21:57.8947690Z Feb 09 19:21:57 Pre-downloading Hadoop tarball
2024-02-09T19:21:57.9250518Z   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
2024-02-09T19:21:57.9250844Z                                  Dload  Upload   Total   Spent    Left  Speed
2024-02-09T19:21:57.9251910Z 
2024-02-09T19:21:58.0005684Z   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
2024-02-09T19:21:58.0006583Z 100   288  100   288    0     0   3789      0 --:--:-- --:--:-- --:--:--  3789
```
which is way too small - meaning we got redirection (https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1700&view=logs&j=bbbd0720-137e-5f59-95a5-b5d332f196d3&t=4769aa47-e87b-5ecd-1fb2-14d52396866d&l=8056 )

Later, it can't be unpacked:
https://dev.azure.com/khachatryanroman/flink/_build/results?buildId=1700&view=logs&j=bbbd0720-137e-5f59-95a5-b5d332f196d3&t=4769aa47-e87b-5ecd-1fb2-14d52396866d&l=9657
```
#11 [ 7/28] COPY hadoop.tar.gz /tmp/hadoop.tar.gz
#11 CACHED

#12 [ 8/28] RUN set -x     && mkdir -p /usr/local/hadoop     && tar -xf /tmp/hadoop.tar.gz --strip-components=1 -C /usr/local/hadoop     && rm /tmp/hadoop.tar.gz*
#12 0.175 + mkdir -p /usr/local/hadoop
#12 0.177 + tar -xf /tmp/hadoop.tar.gz --strip-components=1 -C /usr/local/hadoop
#12 0.178 tar: This does not look like a tar archive
#12 0.179 
#12 0.179 gzip: stdin: not in gzip format
#12 0.179 tar: Child returned status 1
#12 0.179 tar: Error is not recoverable: exiting now

```
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 14:15:53 UTC 2024,,,,,,,,,,"0|z1nc0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/24 12:32;chesnay;master: 5277f653ad325649d07072ad255f0e20c41f1b42
1.19: 296f140b62f73d5547ff710d439dd8068b01520d
1.18: 8d162fc2b68fbeca31b9d4b73dfee188b356bba5;;;","27/Feb/24 09:59;mapohl;Is this a flaky issue? I can reproduce it locally but I don't understand why it's not causing build failures in the 1.17 nightly builds.;;;","27/Feb/24 12:37;roman;IIRC, there was a difference in PR builds and master/release branch builds;;;","27/Feb/24 14:15;mapohl;That would explain why it failed in my [1.17 backport|https://github.com/apache/flink/pull/24385] for FLINK-34518 but not in the nightly. Still, I'm puzzled why it is like that. I couldn't find a difference between the two build types.

Anyway, thanks for the clarification. I backported this one also to 1.17

1.17: [e70ab656f7ccba67995cb87f9c65e1f9dec7aad8|https://github.com/apache/flink/commit/e70ab656f7ccba67995cb87f9c65e1f9dec7aad8];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-docker's .github/workflows/snapshot.yml doesn't support JDK 17 and 21,FLINK-34419,13567996,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,m.orazow,mapohl,mapohl,09/Feb/24 11:19,27/Mar/24 14:57,04/Jun/24 20:40,27/Mar/24 14:57,,,,,,,,,,,,Build System / CI,,,,0,pull-request-available,starter,,,"[.github/workflows/snapshot.yml|https://github.com/apache/flink-docker/blob/master/.github/workflows/snapshot.yml#L40] needs to be updated: JDK 17 support was added in 1.18 (FLINK-15736). JDK 21 support was added in 1.19 (FLINK-33163)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 14:52:04 UTC 2024,,,,,,,,,,"0|z1nbew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/24 22:52;m.orazow;Hello [~mapohl] ,

I would like to work on this issue. Could you please assign it to me?;;;","28/Feb/24 06:51;mapohl;Sure, thanks for volanteering;;;","28/Feb/24 12:08;m.orazow;Hello [~mapohl], 

Thanks! Could you please have a look? I have send a PR for master branch.

I would like also to update dev-x branches if this looks good. The {{dev-x}} should be updated at least to test with new JDKs. ;;;","27/Mar/24 14:52;mapohl;master: 9e0041a2c9dace4bf3f32815e3e24e24385b179b
dev-master: 1460077743b29e17edd0a2d7efd3897fa097988d
dev-1.19: 67d7c46ed382a665e941f0cf1f1606d10f87dee5
dev-1.18: d93d911b015e535fc2b6f1426c3b36229ff3d02a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disk space issues for Docker-ized GitHub Action jobs,FLINK-34418,13567976,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,09/Feb/24 08:44,19/Feb/24 11:43,04/Jun/24 20:40,19/Feb/24 11:43,1.18.1,1.19.0,1.20.0,,,,,1.18.2,1.19.0,,,Test Infrastructure,,,,0,github-actions,pull-request-available,test-stability,,"[https://github.com/apache/flink/actions/runs/7838691874/job/21390739806#step:10:27746]
{code:java}
[...]
Feb 09 03:00:13 Caused by: java.io.IOException: No space left on device
27608Feb 09 03:00:13 	at java.io.FileOutputStream.writeBytes(Native Method)
27609Feb 09 03:00:13 	at java.io.FileOutputStream.write(FileOutputStream.java:326)
27610Feb 09 03:00:13 	at org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:250)
27611Feb 09 03:00:13 	... 39 more
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34443,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 11:43:07 UTC 2024,,,,,,,,,,"0|z1nbag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/24 08:12;mapohl;https://github.com/apache/flink/actions/runs/7851900601/job/21429775024;;;","12/Feb/24 08:16;mapohl;https://github.com/apache/flink/actions/runs/7851900616;;;","12/Feb/24 08:19;mapohl;https://github.com/apache/flink/actions/runs/7851900779;;;","12/Feb/24 08:19;mapohl;https://github.com/apache/flink/actions/runs/7859001632/job/21444955041;;;","12/Feb/24 08:20;mapohl;This one still succeeded but got a disk space reaching limits warning: https://github.com/apache/flink/actions/runs/7859001687/job/21445027923#step:1:46;;;","12/Feb/24 08:28;mapohl;https://github.com/apache/flink/actions/runs/7861970334;;;","12/Feb/24 08:36;mapohl;https://github.com/apache/flink/actions/runs/7866453368;;;","12/Feb/24 13:32;mapohl;https://github.com/apache/flink/actions/runs/7869012663/job/21467582110;;;","12/Feb/24 13:32;mapohl;https://github.com/apache/flink/actions/runs/7870763675;;;","13/Feb/24 09:40;mapohl;https://github.com/apache/flink/actions/runs/7880739758;;;","14/Feb/24 09:52;mapohl;https://github.com/apache/flink/actions/runs/7895502206/job/21548178104;;;","14/Feb/24 09:54;mapohl;https://github.com/apache/flink/actions/runs/7895502322/job/21548178211;;;","14/Feb/24 10:02;mapohl;https://github.com/apache/flink/actions/runs/7895502334;;;","19/Feb/24 09:23;mapohl;https://github.com/apache/flink/actions/runs/7938595320/job/21677809941;;;","19/Feb/24 09:32;mapohl;https://github.com/apache/flink/actions/runs/7945888061/job/21693403355#step:10:28837;;;","19/Feb/24 09:33;mapohl;* [https://github.com/apache/flink/actions/runs/7945888201/job/21693426141]
 * [https://github.com/apache/flink/actions/runs/7945888201/job/21693400407]
 * [https://github.com/apache/flink/actions/runs/7953599167/job/21710059779#step:10:28824]
 * [https://github.com/apache/flink/actions/runs/7953599167/job/21710058707#step:10:27748]
 * [https://github.com/apache/flink/actions/runs/7953599337/job/21710025435]
 * [https://github.com/apache/flink/actions/runs/7953599343/job/21710047123]
 * [https://github.com/apache/flink/actions/runs/7953599343/job/21710023829];;;","19/Feb/24 11:43;mapohl;master: [4cc04a0b79152f6731c2118c038a63eb9ba713e9|https://github.com/apache/flink/commit/4cc04a0b79152f6731c2118c038a63eb9ba713e9]
1.19: [c6c1f7984d101a59e923a163fddbd0128dea117a|https://github.com/apache/flink/commit/c6c1f7984d101a59e923a163fddbd0128dea117a]
1.18: [10c8943e49f324abb22c8254e415d9d666c3a6a3|https://github.com/apache/flink/commit/10c8943e49f324abb22c8254e415d9d666c3a6a3];;;",,,,,,,,,,,,,,,,
Add JobID to logging MDC,FLINK-34417,13567930,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,08/Feb/24 19:12,11/Mar/24 14:12,04/Jun/24 20:40,08/Mar/24 14:31,,,,,,,,1.19.0,,,,Runtime / Checkpointing,Runtime / Coordination,Runtime / Task,,0,pull-request-available,,,,"Adding JobID to logging MDC allows to apply Structural Logging 
and analyze Flink logs more efficiently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34643,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 14:31:18 UTC 2024,,,,,,,,,,"0|z1nb08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 14:31;roman;Merged into master as d6a4eb966fbc47277e07b79e7c64939a62eb1d54.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Local recovery and sticky scheduling end-to-end test"" still doesn't work with AdaptiveScheduler",FLINK-34416,13567916,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,08/Feb/24 15:56,08/Feb/24 15:57,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,,"We tried to enable all {{AdaptiveScheduler}}-related tests in FLINK-34409 because it appeared that all Jira issues that were referenced are resolved. That's not the case for the {{""Local recovery and sticky scheduling end-to-end test""}} tests, though.

With the {{AdaptiveScheduler}} being enabled, we run into issues where the test runs forever due to a {{NullPointerException}} continuously triggering a failure:
{code}
Feb 07 19:02:59 2024-02-07 19:02:21,706 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Unnamed (3/4) (54075d3d22edb729e5f396726f777860_20ba6b65f97481d5570070de90e4e791_2_16292) switched from INITIALIZING to FAILED on localhost:40893-09ff7>
Feb 07 19:02:59 java.lang.NullPointerException: Expected to find info here.
Feb 07 19:02:59         at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:76) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob$StateCreatingFlatMap.initializeState(StickyAllocationAndLocalRecoveryTestJob.java:340) ~[?:?]
Feb 07 19:02:59         at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:187) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:169) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:134) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:285) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreStateAndGates(StreamTask.java:799) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$restoreInternal$3(StreamTask.java:753) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:753) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:712) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:751) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) ~[flink-dist-1.20-SNAPSHOT.jar:1.20-SNAPSHOT]
Feb 07 19:02:59         at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
{code}

This error is caused by a Precondition in [StickyAllocationAndLocalRecoveryTestJob:340|https://github.com/apache/flink/blob/0f3470db83c1fddba9ac9a7299b1e61baab4ff12/flink-end-to-end-tests/flink-local-recovery-and-allocation-test/src/main/java/org/apache/flink/streaming/tests/StickyAllocationAndLocalRecoveryTestJob.java#L340]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 08 15:57:17 UTC 2024,,,,,,,,,,"0|z1nax4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/24 15:57;mapohl;We should investigate whether that could be fixed or whether it's a conceptual issue. We should add a proper explanation for why the tests are disabled for the AdaptiveScheduler in the code in the latter case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move away from Kafka-Zookeeper based tests in favor of Kafka-KRaft,FLINK-34415,13567915,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,08/Feb/24 15:44,08/Feb/24 15:54,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"The current Flink Kafka connector still uses Zookeeper for Kafka-based testing. Since Kafka 3.3, KRaft has been marked as production ready [1]. In order to reduce tech debt, we should remove all the dependencies on Zookeeper and only uses KRaft for the Flink Kafka connector. 

[1] https://cwiki.apache.org/confluence/display/KAFKA/KIP-833%3A+Mark+KRaft+as+Production+Ready
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-08 15:44:56.0,,,,,,,,,,"0|z1naww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXACTLY_ONCE guarantee doesn't work properly for Flink/Pulsar connector ,FLINK-34414,13567902,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rtrojczak,rtrojczak,08/Feb/24 14:42,15/Apr/24 05:55,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,,"Using Pulsar connector for Flink (version 4.1.0-1.17) with Flink job (version 1.17.2) when there is an exception thrown within the job, the job gets restarted, starts from the last checkpoint, but the sink writes to the output more events than it should, even though the EXACT_ONCE guarantees are set everywhere. To be more specific, here is my Job's flow:
 * a Pulsar source that reads from the input topic,
 * a simple processing function,
 * and a sink that writes to the output topic.

Here is a fragment of the source creation:
{code:java}
    .setDeserializationSchema(Schema.AVRO(inClass), inClass)
    .setSubscriptionName(subscription)
    .enableSchemaEvolution()
    .setConfig(PulsarOptions.PULSAR_ENABLE_TRANSACTION, true)
    .setConfig(PulsarSourceOptions.PULSAR_ACK_RECEIPT_ENABLED, true)
    .setConfig(PulsarSourceOptions.PULSAR_MAX_FETCH_RECORDS, 1)
    .setConfig(PulsarSourceOptions.PULSAR_ENABLE_AUTO_ACKNOWLEDGE_MESSAGE, false);
{code}
Here is the fragment of the sink creation:
{code:java}
    .setSerializationSchema(Schema.AVRO(outClass), outClass)
    .setConfig(PulsarOptions.PULSAR_ENABLE_TRANSACTION, true)
    .setConfig(PulsarSinkOptions.PULSAR_WRITE_DELIVERY_GUARANTEE, DeliveryGuarantee.EXACTLY_ONCE)
    .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE);
{code}
And here is the Flink environment preparation:
{code:java}
    environment.setRuntimeMode(RuntimeExecutionMode.STREAMING);
    environment.enableCheckpointing(CHECKPOINTING_INTERVAL, CheckpointingMode.EXACTLY_ONCE);
{code}
After sending 1000 events on the input topic, on the output topic I got 1048 events.

I ran the job on my local Kubernetes cluster, using Kubernetes Flink Operator.

Here is the MRE for this problem (mind that there is an internal dependency, but it may be commented out together with the code that relies on it): [https://github.com/trojczak/flink-pulsar-connector-problem]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 05:55:17 UTC 2024,,,,,,,,,,"0|z1nau0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Feb/24 10:14;rtrojczak;Hello All,

I would like to kindly remind you about this issue. With this problem (if in fact this is a problem) the EXACTLY_ONCE delivery guarantee doesn't work for Flink-Pulsar setup and makes this technology stack useless in many applications.;;;","15/Apr/24 05:55;syhily;I don't think this is a issue from my side. First of all {{.setConfig(PulsarOptions.PULSAR_ENABLE_TRANSACTION, true)}} only works on the PulsarSink side. The source didn't use transaction for its poor performance, we have to drop the support of transaction in source. In your sample code, you have set {{.setConfig(PulsarSourceOptions.PULSAR_ACK_RECEIPT_ENABLED, true)}} and {{.setConfig(PulsarSourceOptions.PULSAR_ENABLE_AUTO_ACKNOWLEDGE_MESSAGE, false)}}. This means the duplicated message could be get because the Pulsar can't the acknowledege from connector and resend the messages. So I think this is the root cause.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop support for HBase v1,FLINK-34413,13567862,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,martijnvisser,martijnvisser,08/Feb/24 09:09,06/Mar/24 19:07,04/Jun/24 20:40,06/Mar/24 19:07,,,,,,,,hbase-4.0.0,,,,Connectors / HBase,,,,0,pull-request-available,,,,As discussed in https://lists.apache.org/thread/6663052dmfnqm8wvqoxx9k8jwcshg1zq ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 19:07:20 UTC 2024,,,,,,,,,,"0|z1nal4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/24 16:12;ferenc-csaky;Thanks for creating this ticket, feel free to assign it to me, I will cover this.;;;","06/Mar/24 19:07;martijnvisser;Fixed in apache/flink-connector-hbase:main 9cbc109b368e26770891e31750c370d295d629e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResultPartitionDeploymentDescriptorTest fails due to fatal error (239 exit code),FLINK-34412,13567855,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,08/Feb/24 08:38,08/Feb/24 08:38,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57388&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8323

{code}
Feb 08 04:56:31 [ERROR] org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest
Feb 08 04:56:31 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Feb 08 04:56:31 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter6684124987290515696.jar /__w/1/s/flink-runtime/target/surefire 2024-02-08T04-45-49_396-jvmRun4 surefire6142105262662423760tmp surefire_245661504424247139476tmp
Feb 08 04:56:31 [ERROR] Error occurred in starting fork, check output in log
Feb 08 04:56:31 [ERROR] Process Exit Code: 239
Feb 08 04:56:31 [ERROR] Crashed tests:
Feb 08 04:56:31 [ERROR] org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptorTest
Feb 08 04:56:31 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Feb 08 04:56:31 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
Feb 08 04:56:31 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
Feb 08 04:56:31 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Feb 08 04:56:31 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Feb 08 04:56:31 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Feb 08 04:56:31 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-08 08:38:56.0,,,,,,,,,,"0|z1najk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Wordcount on Docker test (custom fs plugin)"" timed out with some strange issue while setting the test up",FLINK-34411,13567853,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,08/Feb/24 08:14,12/Feb/24 11:33,04/Jun/24 20:40,12/Feb/24 11:33,1.19.0,,,,,,,,,,,Test Infrastructure,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57380&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=5802

{code}
Feb 07 15:22:39 ==============================================================================
Feb 07 15:22:39 Running 'Wordcount on Docker test (custom fs plugin)'
Feb 07 15:22:39 ==============================================================================
Feb 07 15:22:39 TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-39516987853
Feb 07 15:22:40 Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.19-SNAPSHOT-bin/flink-1.19-SNAPSHOT
Feb 07 15:22:40 Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.19-SNAPSHOT-bin/flink-1.19-SNAPSHOT
Feb 07 15:22:41 Docker version 24.0.7, build afdd53b
Feb 07 15:22:44 docker-compose version 1.29.2, build 5becea4c
Feb 07 15:22:44 Starting fileserver for Flink distribution
Feb 07 15:22:44 ~/work/1/s/flink-dist/target/flink-1.19-SNAPSHOT-bin ~/work/1/s
Feb 07 15:23:07 ~/work/1/s
Feb 07 15:23:07 ~/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-39516987853 ~/work/1/s
Feb 07 15:23:07 Preparing Dockeriles
Feb 07 15:23:07 Executing command: git clone https://github.com/apache/flink-docker.git --branch dev-1.19 --single-branch
Cloning into 'flink-docker'...
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/common_docker.sh: line 65: ./add-custom.sh: No such file or directory
Feb 07 15:23:07 Building images
ERROR: unable to prepare context: path ""dev/test_docker_embedded_job-ubuntu"" not found
Feb 07 15:23:09 ~/work/1/s
Feb 07 15:23:09 Command: build_image test_docker_embedded_job failed. Retrying...
Feb 07 15:23:14 Starting fileserver for Flink distribution
Feb 07 15:23:14 ~/work/1/s/flink-dist/target/flink-1.19-SNAPSHOT-bin ~/work/1/s
Feb 07 15:23:36 ~/work/1/s
Feb 07 15:23:36 ~/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-39516987853 ~/work/1/s
Feb 07 15:23:36 Preparing Dockeriles
Feb 07 15:23:36 Executing command: git clone https://github.com/apache/flink-docker.git --branch dev-1.19 --single-branch
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Feb 07 15:23:36 Retry 1/5 exited 128, retrying in 1 seconds...
Traceback (most recent call last):
  File ""/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/python3_fileserver.py"", line 26, in <module>
    httpd = socketserver.TCPServer(("""", 9999), handler)
  File ""/usr/lib/python3.8/socketserver.py"", line 452, in __init__
    self.server_bind()
  File ""/usr/lib/python3.8/socketserver.py"", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 12 11:33:40 UTC 2024,,,,,,,,,,"0|z1naj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/24 10:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57393&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=4921;;;","08/Feb/24 16:15;ferenc-csaky;The current [dev-1.19|https://github.com/apache/flink-docker/tree/dev-1.19] branch in the flink-docker repo was cloned from the {{master}} branch, hence that script does not exists. I also bumped into this today.;;;","09/Feb/24 08:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57410&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=4880;;;","09/Feb/24 08:53;mapohl;Thanks for looking into it, [~ferenc-csaky] . That sounds like this is the issue, yeah.;;;","12/Feb/24 08:44;mapohl;* https://github.com/apache/flink/actions/runs/7838691836/job/21390782645;;;","12/Feb/24 11:33;mapohl;I rebased {{dev-1.19}} to {{dev-master}} and provided a fix for the snapshot CI in {{master}}

apache/flink-docker@master: 2c169b6a83bf83bbe997ed35aaf548de10050b58;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable nightly trigger in forks,FLINK-34410,13567839,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,08/Feb/24 07:16,11/Feb/24 12:31,04/Jun/24 20:40,09/Feb/24 08:36,1.20.0,,,,,,,1.20.0,,,,Build System / CI,,,,0,pull-request-available,,,,"We can disable the automatic triggering of the nightly trigger workflow in fork (see [GHA docs|https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions]s:
{code}
if: github.repository == 'octo-org/octo-repo-prod'
{code}

No backport is needed because the schedule triggers will on fire for {{master}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 09 08:36:17 UTC 2024,,,,,,,,,,"0|z1nag0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/24 08:36;mapohl;master: [d2abd744621c6f0f65e7154a2c1b53bcaf78e90b|https://github.com/apache/flink/commit/d2abd744621c6f0f65e7154a2c1b53bcaf78e90b];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increase test coverage for AdaptiveScheduler,FLINK-34409,13567766,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,07/Feb/24 15:02,25/Mar/24 16:09,04/Jun/24 20:40,25/Mar/24 16:09,1.17.2,1.18.1,1.19.0,1.20.0,,,,1.18.2,1.19.1,1.20.0,,Runtime / Coordination,,,,0,pull-request-available,,,,"There are still several tests disabled for the {{AdaptiveScheduler}} which we can enable now. All the issues seem to have been fixed.

We can even remove the annotation {{@FailsWithAdaptiveScheduler}} now. It's not needed anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21535,FLINK-21450,FLINK-21400,,FLINK-21689,,,,,,FLINK-34921,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 16:09:21 UTC 2024,,,,,,,,,,"0|z1n9zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 15:03;mapohl;Linking FLINK-21535, FLINK-21450 and FLINK-21400. These were the issues that motivated disabling the tests. All of them are resolved now.;;;","07/Feb/24 15:08;mapohl;We can't remove the annotation because of {{MiniClusterITCase#testHandlingNotEnoughSlotsThroughEarlyAbortRequest}}.;;;","25/Mar/24 16:09;mapohl;master: [1aa35b95975560da6afb7fcf0ad80f0a25c5d183|https://github.com/apache/flink/commit/1aa35b95975560da6afb7fcf0ad80f0a25c5d183]
1.19: [f82ff7c656d3eeb3e82b456d284639e59624a849|https://github.com/apache/flink/commit/f82ff7c656d3eeb3e82b456d284639e59624a849]
1.18: [f2a6ff5a97bf27d68be1188c05158e18df810549|https://github.com/apache/flink/commit/f2a6ff5a97bf27d68be1188c05158e18df810549];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VeryBigPbProtoToRowTest#testSimple fails with OOM,FLINK-34408,13567739,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mapohl,mapohl,07/Feb/24 12:42,07/Feb/24 13:48,04/Jun/24 20:40,07/Feb/24 13:48,1.20.0,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57371&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23861

{code}
Feb 07 09:40:16 09:40:16.314 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 29.58 s <<< FAILURE! -- in org.apache.flink.formats.protobuf.VeryBigPbProtoToRowTest
Feb 07 09:40:16 09:40:16.314 [ERROR] org.apache.flink.formats.protobuf.VeryBigPbProtoToRowTest.testSimple -- Time elapsed: 29.57 s <<< ERROR!
Feb 07 09:40:16 org.apache.flink.util.FlinkRuntimeException: Error in serialization.
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:327)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:162)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:1007)
Feb 07 09:40:16 	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:56)
Feb 07 09:40:16 	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:45)
Feb 07 09:40:16 	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:61)
Feb 07 09:40:16 	at org.apache.flink.client.deployment.executors.LocalExecutor.getJobGraph(LocalExecutor.java:104)
Feb 07 09:40:16 	at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:81)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2440)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2421)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollectWithClient(DataStream.java:1495)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1382)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1367)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.validateRow(ProtobufTestHelper.java:66)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:121)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:103)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:98)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.VeryBigPbProtoToRowTest.testSimple(VeryBigPbProtoToRowTest.java:36)
Feb 07 09:40:16 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 07 09:40:16 Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:323)
Feb 07 09:40:16 	... 18 more
Feb 07 09:40:16 Caused by: java.lang.OutOfMemoryError: Java heap space
Feb 07 09:40:16 	at java.util.Arrays.copyOf(Arrays.java:3236)
Feb 07 09:40:16 	at java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:191)
Feb 07 09:40:16 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:555)
Feb 07 09:40:16 	at org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:486)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamConfig.lambda$triggerSerializationAndReturnFuture$0(StreamConfig.java:182)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamConfig$$Lambda$1582/1961611609.accept(Unknown Source)
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
Feb 07 09:40:16 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Feb 07 09:40:16 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Feb 07 09:40:16 	at java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34403,,,FLINK-33611,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 12:47:51 UTC 2024,,,,,,,,,,"0|z1n9ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 12:43;mapohl;I'm linking FLINK-33611 as a cause because that test was added as part of the FLINK-33611 changes.

[~dsaisharath] [~libenchao] can you have a look at this;;;","07/Feb/24 12:47;mapohl;https://github.com/apache/flink/actions/runs/7812519813/job/21310082909#step:10:23503;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky tests causing workflow timeout,FLINK-34407,13567726,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,07/Feb/24 10:37,10/Feb/24 13:22,04/Jun/24 20:40,10/Feb/24 13:22,aws-connector-4.2.0,,,,,,,,,,,Connectors / Kinesis,,,,0,pull-request-available,,,,"Example build: [https://github.com/apache/flink-connector-aws/actions/runs/7735404733]

Tests are stuck retrying due to the following exception:
{code:java}
797445 [main] WARN  org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher [] - Encountered recoverable error TimeoutException. Backing off for 0 millis 000000 (arn)
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber$RecoverableFanOutSubscriberException: java.util.concurrent.TimeoutException: Timed out acquiring subscription - 000000 (arn)
 	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.handleErrorAndRethrow(FanOutShardSubscriber.java:327) ~[classes/:?]
 	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.openSubscriptionToShard(FanOutShardSubscriber.java:283) ~[classes/:?]
 	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.subscribeToShardAndConsumeRecords(FanOutShardSubscriber.java:210) ~[classes/:?]
 	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.runWithBackoff(FanOutRecordPublisher.java:177) ~[classes/:?]
 	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.run(FanOutRecordPublisher.java:130) ~[classes/:?]
 	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisherTest.testCancelExitsGracefully(FanOutRecordPublisherTest.java:595) ~[test-classes/:?]
 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_402]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 10 13:22:14 UTC 2024,,,,,,,,,,"0|z1n9qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/24 13:22;hong;merged commit [{{b207606}}|https://github.com/apache/flink-connector-aws/commit/b207606a95d0ce508c55e69dd0dc6c598eb2fb3c] into   apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose RuntimeContext in FunctionContext,FLINK-34406,13567709,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nilerzhou,nilerzhou,07/Feb/24 08:49,20/Feb/24 03:20,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Table SQL / API,,,,0,,,,,"When I implement a LookupFunction and utilize a RateLimiter in it, I need to open the RateLimiter in the open function. And I can only get a FunctionContext in the function. 

However the RateLimiter needs to call getNumberOfParallelSubtasks of RuntimeContext to get the parallelism of the job, so that it can calculate the flow limitation for each subtask.

Actually, getMetricGroup, getUserCodeClassLoader and so many FunctionContext functionalities all come from RuntimeContext. Why not just expose the RuntimeContext here?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 03:20:16 UTC 2024,,,,,,,,,,"0|z1n9n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 03:20;nilerzhou;Hi [~twalthr] ,  what do you think about this proposal?  I found that most of RuntimeContext functionalities in FunctionContext are introduced by you in https://issues.apache.org/jira/browse/FLINK-22857. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RightOuterJoinTaskTest#testCancelOuterJoinTaskWhileSort2 fails due to an interruption of the RightOuterJoinDriver#prepare method,FLINK-34405,13567705,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,07/Feb/24 08:37,16/May/24 08:39,04/Jun/24 20:40,,1.17.2,1.18.1,1.19.0,1.20.0,,,,,,,,API / Core,,,,0,pull-request-available,starter,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57357&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9027

{code}
Feb 07 03:20:16 03:20:16.223 [ERROR] Failures: 
Feb 07 03:20:16 03:20:16.223 [ERROR] org.apache.flink.runtime.operators.RightOuterJoinTaskTest.testCancelOuterJoinTaskWhileSort2
Feb 07 03:20:16 03:20:16.223 [ERROR]   Run 1: RightOuterJoinTaskTest>AbstractOuterJoinTaskTest.testCancelOuterJoinTaskWhileSort2:435 
Feb 07 03:20:16 expected: 
Feb 07 03:20:16   null
Feb 07 03:20:16  but was: 
Feb 07 03:20:16   java.lang.Exception: The data preparation caused an error: Interrupted
Feb 07 03:20:16   	at org.apache.flink.runtime.operators.testutils.BinaryOperatorTestBase.testDriverInternal(BinaryOperatorTestBase.java:209)
Feb 07 03:20:16   	at org.apache.flink.runtime.operators.testutils.BinaryOperatorTestBase.testDriver(BinaryOperatorTestBase.java:189)
Feb 07 03:20:16   	at org.apache.flink.runtime.operators.AbstractOuterJoinTaskTest.access$100(AbstractOuterJoinTaskTest.java:48)
Feb 07 03:20:16   	...(1 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33114,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 08:39:22 UTC 2024,,,,,,,,,,"0|z1n9m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 09:03;mapohl;[~pnowojski] can you have a look at it or delegate it to the right person?;;;","07/Feb/24 12:35;mapohl;Looks like the issue is not a blocker. My suspicion is that the {{sleep}} in [AbstractOuterJoinTaskTest#testCancelOuterJoinTaskWhileSort2:424|https://github.com/apache/flink/blob/ac61e83dad8ea7d0fc91ad98315b2987275586dd/flink-runtime/src/test/java/org/apache/flink/runtime/operators/AbstractOuterJoinTaskTest.java#L424] is just not properly set which allowed the subsequent [taskRunner.interrupt()|https://github.com/apache/flink/blob/ac61e83dad8ea7d0fc91ad98315b2987275586dd/flink-runtime/src/test/java/org/apache/flink/runtime/operators/AbstractOuterJoinTaskTest.java#L427] be called too early (while {{RightOuterJoinDriver#prepare}} is still not finished.

We need to refactor the test to rely on futures instead of {{Thread#sleep}} to fix this.

No action necessary from your end, anymore [~pnowojski];;;","08/Feb/24 06:27;fanrui;org.apache.flink.runtime.operators.RightOuterJoinTaskTest.testCancelOuterJoinTaskWhileSort1 fails as well.

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57395&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9113;;;","07/Apr/24 04:03;jiabaosun;taskRunner Thread:  testDriver() -> AbstractOuterJoinDriver#prepare() :101 -> WAITING on ExternalSorter#getIterator().

The InterruptedException is always thrown by BinaryOperatorTestBase:209.
It will be dropped after cancel() method called, see BinaryOperatorTestBase:260.;;;","16/May/24 08:39;rskraba;* 1.20 test_cron_adaptive_scheduler core https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59558&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8662;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupWindowAggregateProcTimeRestoreTest#testRestore times out,FLINK-34404,13567703,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,asheinberg,mapohl,mapohl,07/Feb/24 08:27,03/Jun/24 06:44,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Table SQL / Planner,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57357&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11603

{code}
Feb 07 02:17:40 ""ForkJoinPool-74-worker-1"" #382 daemon prio=5 os_prio=0 cpu=282.22ms elapsed=961.78s tid=0x00007f880a485c00 nid=0x6745 waiting on condition  [0x00007f878a6f9000]
Feb 07 02:17:40    java.lang.Thread.State: WAITING (parking)
Feb 07 02:17:40 	at jdk.internal.misc.Unsafe.park(java.base@17.0.7/Native Method)
Feb 07 02:17:40 	- parking to wait for  <0x00000000ff73d060> (a java.util.concurrent.CompletableFuture$Signaller)
Feb 07 02:17:40 	at java.util.concurrent.locks.LockSupport.park(java.base@17.0.7/LockSupport.java:211)
Feb 07 02:17:40 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@17.0.7/CompletableFuture.java:1864)
Feb 07 02:17:40 	at java.util.concurrent.ForkJoinPool.compensatedBlock(java.base@17.0.7/ForkJoinPool.java:3449)
Feb 07 02:17:40 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@17.0.7/ForkJoinPool.java:3432)
Feb 07 02:17:40 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@17.0.7/CompletableFuture.java:1898)
Feb 07 02:17:40 	at java.util.concurrent.CompletableFuture.get(java.base@17.0.7/CompletableFuture.java:2072)
Feb 07 02:17:40 	at org.apache.flink.table.planner.plan.nodes.exec.testutils.RestoreTestBase.testRestore(RestoreTestBase.java:292)
Feb 07 02:17:40 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@17.0.7/Native Method)
Feb 07 02:17:40 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(java.base@17.0.7/NativeMethodAccessorImpl.java:77)
Feb 07 02:17:40 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(java.base@17.0.7/DelegatingMethodAccessorImpl.java:43)
Feb 07 02:17:40 	at java.lang.reflect.Method.invoke(java.base@17.0.7/Method.java:568)
Feb 07 02:17:40 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:728)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/24 12:14;mapohl;FLINK-34404.failure.log;https://issues.apache.org/jira/secure/attachment/13066603/FLINK-34404.failure.log","08/Feb/24 12:14;mapohl;FLINK-34404.success.log;https://issues.apache.org/jira/secure/attachment/13066602/FLINK-34404.success.log",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 06:44:35 UTC 2024,,,,,,,,,,"0|z1n9ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 08:34;mapohl;[~bvarghese] can you check this one as well? It would be good to clarify whether that's production code issue (which would make it a blocker) or whether it's a test code issue.;;;","07/Feb/24 10:56;twalthr;If this is a new error, it might be related to FLINK-34095 which we merged yesterday. Do we know which specific test causes the issue? CC [~asheinberg] ;;;","07/Feb/24 12:02;mapohl;I tried to figure that out when creating the Jira but failed. But thanks to you, I did another try: {{GroupWindowAggregateProcTimeRestoreTest}} is not coming back. I'm gonna update the Jira title.;;;","07/Feb/24 18:37;bvarghese;Unsure how this is related to FLINK-34095. Will see if I can reproduce this locally.;;;","07/Feb/24 20:43;bvarghese;I ran this in repeat mode (200 times) on master branch as well as release-1.19 branch and the test passed.;;;","08/Feb/24 12:16;mapohl;I attached the debug logs for the failed run in CI and a successful local run of the test. Maybe, that helps to get some insights. Without having looked more into it, it's apparent that there is more table-related initialization done in the successful run in comparison to what we're seeing in the failed run.;;;","08/Feb/24 16:59;asheinberg;I did recently add a test, so if for some reason we think we were mistaken on the culprit, I can look.;;;","01/Mar/24 10:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57981&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12625

I verified that it's the GroupWindowAggregateProcTimeRestoreTest again:
{code}
grep '^Test org.apache.flink' mvn-*.log | cut -d' ' -f2 | sort | uniq -c | grep -v ""     [1234567]\?[02468]"" | grep testRestore
      1 org.apache.flink.table.planner.plan.nodes.exec.stream.GroupWindowAggregateProcTimeRestoreTest.testRestore[[1]
{code};;;","11/Mar/24 14:41;mapohl;{{WindowTableFunctionProcTimeRestoreTest}} timed out (I'm not gonna create a new Jira issue): https://github.com/apache/flink/actions/runs/8201251769/job/22429899203#step:10:11641

{code}
Mar 08 09:59:09 ""ForkJoinPool-139-worker-1"" #1489 daemon prio=5 os_prio=0 tid=0x00007f9d9dfbb800 nid=0xea67 waiting on condition [0x00007f9d88de2000]
Mar 08 09:59:09    java.lang.Thread.State: WAITING (parking)
Mar 08 09:59:09 	at sun.misc.Unsafe.park(Native Method)
Mar 08 09:59:09 	- parking to wait for  <0x00000000dc647680> (a java.util.concurrent.CompletableFuture$Signaller)
Mar 08 09:59:09 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Mar 08 09:59:09 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Mar 08 09:59:09 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Mar 08 09:59:09 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Mar 08 09:59:09 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Mar 08 09:59:09 	at org.apache.flink.table.planner.plan.nodes.exec.testutils.RestoreTestBase.testRestore(RestoreTestBase.java:292)
[...]
{code}

Log analysis to identify the specific test:
{code}
cat mvn-*.log | grep testRestore | cut -d' ' -f2 | sort | uniq -c | sort -n | head 
      1 org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionProcTimeRestoreTest.testRestore[[1]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.AsyncCalcRestoreTest.testRestore[[1]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.AsyncCalcRestoreTest.testRestore[[2]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.AsyncCalcRestoreTest.testRestore[[3]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.AsyncCalcRestoreTest.testRestore[[4]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.AsyncCalcRestoreTest.testRestore[[5]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.CalcRestoreTest.testRestore[[1]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.CalcRestoreTest.testRestore[[2]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.CalcRestoreTest.testRestore[[3]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.CalcRestoreTest.testRestore[[4]
{code};;;","19/Mar/24 13:45;rskraba;This also occurs on AZP:
* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58399&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12626 |https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58399&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12626](1.19)

 ;;;","21/Mar/24 10:44;mapohl;https://github.com/apache/flink/actions/runs/8290287716/job/22688320933#step:10:11684
{code}
cat mvn-*.log | grep testRestore | cut -d' ' -f2 | sort | uniq -c | sort -n | head -3
      1 org.apache.flink.table.planner.plan.nodes.exec.stream.WindowTableFunctionProcTimeRestoreTest.testRestore[[1]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.AsyncCalcRestoreTest.testRestore[[1]
      2 org.apache.flink.table.planner.plan.nodes.exec.stream.AsyncCalcRestoreTest.testRestore[[2]
{code};;;","10/May/24 09:40;rskraba;(Using the same command line as above)
* 1.20 Default (Java 8) / Test (module: table) https://github.com/apache/flink/actions/runs/8999811164/job/24723153970#step:10:12716
;;;","31/May/24 13:21;rskraba;* 1.19 Hadoop 3.1.3 / Test (module: table) https://github.com/apache/flink/actions/runs/9295906585/job/25583895678#step:10:11733
* 1.20 Default (Java 8) / Test (module: table) https://github.com/apache/flink/actions/runs/9275522134/job/25520829167#step:10:12142;;;","03/Jun/24 06:44;Weijie Guo;1.20 test_cron_jdk11

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=60013&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11901;;;",,,,,,,,,,,,,,,,,,,
VeryBigPbProtoToRowTest#testSimple cannot pass due to OOM,FLINK-34403,13567689,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,libenchao,libenchao,07/Feb/24 06:20,14/Feb/24 16:56,04/Jun/24 20:40,14/Feb/24 16:56,1.20.0,,,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,test-stability,,,"After FLINK-33611 merged, the misc test on GHA cannot pass due to out of memory error, throwing following exceptions:

{code:java}
Error: 05:43:21 05:43:21.768 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 40.98 s <<< FAILURE! -- in org.apache.flink.formats.protobuf.VeryBigPbRowToProtoTest
Error: 05:43:21 05:43:21.773 [ERROR] org.apache.flink.formats.protobuf.VeryBigPbRowToProtoTest.testSimple -- Time elapsed: 40.97 s <<< ERROR!
Feb 07 05:43:21 org.apache.flink.util.FlinkRuntimeException: Error in serialization.
Feb 07 05:43:21 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:327)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:162)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:1007)
Feb 07 05:43:21 	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:56)
Feb 07 05:43:21 	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:45)
Feb 07 05:43:21 	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:61)
Feb 07 05:43:21 	at org.apache.flink.client.deployment.executors.LocalExecutor.getJobGraph(LocalExecutor.java:104)
Feb 07 05:43:21 	at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:81)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2440)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2421)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollectWithClient(DataStream.java:1495)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1382)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1367)
Feb 07 05:43:21 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.validateRow(ProtobufTestHelper.java:66)
Feb 07 05:43:21 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.rowToPbBytes(ProtobufTestHelper.java:89)
Feb 07 05:43:21 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.rowToPbBytes(ProtobufTestHelper.java:76)
Feb 07 05:43:21 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.rowToPbBytes(ProtobufTestHelper.java:71)
Feb 07 05:43:21 	at org.apache.flink.formats.protobuf.VeryBigPbRowToProtoTest.testSimple(VeryBigPbRowToProtoTest.java:37)
Feb 07 05:43:21 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 07 05:43:21 Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Self-suppression not permitted
Feb 07 05:43:21 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Feb 07 05:43:21 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:323)
Feb 07 05:43:21 	... 18 more
Feb 07 05:43:21 Caused by: java.lang.IllegalArgumentException: Self-suppression not permitted
Feb 07 05:43:21 	at java.lang.Throwable.addSuppressed(Throwable.java:1072)
Feb 07 05:43:21 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:556)
Feb 07 05:43:21 	at org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:486)
Feb 07 05:43:21 	at org.apache.flink.streaming.api.graph.StreamConfig.lambda$triggerSerializationAndReturnFuture$0(StreamConfig.java:182)
Feb 07 05:43:21 	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
Feb 07 05:43:21 	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
Feb 07 05:43:21 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
Feb 07 05:43:21 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Feb 07 05:43:21 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Feb 07 05:43:21 	at java.lang.Thread.run(Thread.java:750)
Feb 07 05:43:21 Caused by: java.lang.OutOfMemoryError: Java heap space
{code}

See more details : https://github.com/apache/flink/actions/runs/7810182427/job/21303415607
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-34408,,,,,,,,,,,,FLINK-33611,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 14 16:56:04 UTC 2024,,,,,,,,,,"0|z1n9io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 06:23;libenchao;CC [~mapohl], is it possible we can increase the java heap size for running these tests? The newly introduced test indeed needs more memory since it verifies an extreme use case.;;;","07/Feb/24 13:50;mapohl;This is not a GHA specific issue. Also Azure Pipelines start to fail consistently due to FLINK-33611 being merged:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57371&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23861
{code}
Feb 07 09:40:16 09:40:16.314 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 29.58 s <<< FAILURE! -- in org.apache.flink.formats.protobuf.VeryBigPbProtoToRowTest
Feb 07 09:40:16 09:40:16.314 [ERROR] org.apache.flink.formats.protobuf.VeryBigPbProtoToRowTest.testSimple -- Time elapsed: 29.57 s <<< ERROR!
Feb 07 09:40:16 org.apache.flink.util.FlinkRuntimeException: Error in serialization.
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:327)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:162)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:1007)
Feb 07 09:40:16 	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:56)
Feb 07 09:40:16 	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:45)
Feb 07 09:40:16 	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:61)
Feb 07 09:40:16 	at org.apache.flink.client.deployment.executors.LocalExecutor.getJobGraph(LocalExecutor.java:104)
Feb 07 09:40:16 	at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:81)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2440)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2421)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollectWithClient(DataStream.java:1495)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1382)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1367)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.validateRow(ProtobufTestHelper.java:66)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:121)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:103)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.ProtobufTestHelper.pbBytesToRow(ProtobufTestHelper.java:98)
Feb 07 09:40:16 	at org.apache.flink.formats.protobuf.VeryBigPbProtoToRowTest.testSimple(VeryBigPbProtoToRowTest.java:36)
Feb 07 09:40:16 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 07 09:40:16 Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:323)
Feb 07 09:40:16 	... 18 more
Feb 07 09:40:16 Caused by: java.lang.OutOfMemoryError: Java heap space
Feb 07 09:40:16 	at java.util.Arrays.copyOf(Arrays.java:3236)
Feb 07 09:40:16 	at java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:191)
Feb 07 09:40:16 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:555)
Feb 07 09:40:16 	at org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:486)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamConfig.lambda$triggerSerializationAndReturnFuture$0(StreamConfig.java:182)
Feb 07 09:40:16 	at org.apache.flink.streaming.api.graph.StreamConfig$$Lambda$1582/1961611609.accept(Unknown Source)
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
Feb 07 09:40:16 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
Feb 07 09:40:16 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Feb 07 09:40:16 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Feb 07 09:40:16 	at java.lang.Thread.run(Thread.java:748)
{code}
;;;","07/Feb/24 13:52;mapohl;[~libenchao] Maybe, I don't fully understand the test case. But can't we reduce the test data size, instead?;;;","07/Feb/24 20:19;dsaisharath;The test was added for an extreme case that would fail without the changes made in the PR. If we reduce the test data size, the test case would pass without the changes in the PR. I've tried hard to make such a test that would meet all the requirements and also pass the Azure pipelines heap size requirement but I'm not sure why it has started failing after passing earlier. Fwiw, the test passes comfortably in my local environment at a much larger size than what was merged into the codebase. I can remove this test altogether if it is not possible to increase the heap size for the tests as the PR already achieved it's goal;;;","08/Feb/24 08:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57387&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","08/Feb/24 08:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57392&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23860;;;","08/Feb/24 13:55;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57399&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","09/Feb/24 08:47;mapohl;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57406&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23861]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57407&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23864]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57417&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=23438]
 * https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57417&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=23492;;;","09/Feb/24 10:49;mapohl;[~dsaisharath] what's a heap size that would allow the test to pass?;;;","12/Feb/24 08:00;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57422&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57428&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57440&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57454&view=results
* https://github.com/apache/flink/actions/runs/7831121355/job/21367169878#step:10:23505
* https://github.com/apache/flink/actions/runs/7823924194/job/21345848746#step:10:23507
* https://github.com/apache/flink/actions/runs/7823895861
* https://github.com/apache/flink/actions/runs/7838691422
* https://github.com/apache/flink/actions/runs/7851900601
* https://github.com/apache/flink/actions/runs/7859002096/job/21444979868#step:10:23510;;;","12/Feb/24 08:16;chesnay;If you need a larger share of the heap size tag is as an ITCase instead.;;;","12/Feb/24 12:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57464&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23485;;;","12/Feb/24 13:22;libenchao;ITCase sounds good to me, +1 for it.;;;","13/Feb/24 07:38;mapohl;* https://dev.azure.com/apache-flink/web/build.aspx?pcguid=2d3c0ac8-fecf-45be-8407-6d87302181a9&builduri=vstfs%3a%2f%2f%2fBuild%2fBuild%2f57469&tracking_data=ew0KICAic291cmNlIjogIlNsYWNrUGlwZWxpbmVzQXBwIiwNCiAgInNvdXJjZV9ldmVudF9uYW1lIjogImJ1aWxkLmNvbXBsZXRlIg0KfQ%3d%3d
* https://dev.azure.com/apache-flink/web/build.aspx?pcguid=2d3c0ac8-fecf-45be-8407-6d87302181a9&builduri=vstfs%3a%2f%2f%2fBuild%2fBuild%2f57489&tracking_data=ew0KICAic291cmNlIjogIlNsYWNrUGlwZWxpbmVzQXBwIiwNCiAgInNvdXJjZV9ldmVudF9uYW1lIjogImJ1aWxkLmNvbXBsZXRlIg0KfQ%3d%3d
* https://dev.azure.com/apache-flink/web/build.aspx?pcguid=2d3c0ac8-fecf-45be-8407-6d87302181a9&builduri=vstfs%3a%2f%2f%2fBuild%2fBuild%2f57491&tracking_data=ew0KICAic291cmNlIjogIlNsYWNrUGlwZWxpbmVzQXBwIiwNCiAgInNvdXJjZV9ldmVudF9uYW1lIjogImJ1aWxkLmNvbXBsZXRlIg0KfQ%3d%3d;;;","13/Feb/24 09:45;mapohl;master: [9a316a5bcc47da7f69e76e0c25ed257adc4298ce|https://github.com/apache/flink/commit/9a316a5bcc47da7f69e76e0c25ed257adc4298ce];;;","13/Feb/24 19:09;dsaisharath;Sorry, I couldn't get back here in time. Thanks [~mapohl] for the fix!;;;","14/Feb/24 09:27;mapohl;No worries;;;","14/Feb/24 09:32;mapohl;Args, all the time I didn't notice that they are two separate tests (with very similar names):
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57499&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23862
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57499&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23862
* https://github.com/apache/flink/actions/runs/7895502334/job/21548207280#step:10:23089
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57518&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=23068
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57518&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=23068
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57518&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=23375

Reopening the issue.;;;","14/Feb/24 16:56;mapohl;* master
** [2298e53f35121602c56845ac8040439fbd1a9ff4|https://github.com/apache/flink/commit/2298e53f35121602c56845ac8040439fbd1a9ff4]
** [9a316a5bcc47da7f69e76e0c25ed257adc4298ce|https://github.com/apache/flink/commit/9a316a5bcc47da7f69e76e0c25ed257adc4298ce];;;",,,,,,,,,,,,,,
 Class loading conflicts when using PowerMock in ITcase.,FLINK-34402,13567679,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,nilerzhou,nilerzhou,nilerzhou,07/Feb/24 03:53,07/Feb/24 09:52,04/Jun/24 20:40,07/Feb/24 09:52,1.19.0,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"Currently when no user jars exist, system classLoader will be used to load classes as default. However, if we use powerMock to create some ITCases, the framework will utilize JavassistMockClassLoader to load classes.  Forcing the use of the system classLoader can lead to class loading conflict issue.

Therefore we should use Thread.currentThread().getContextClassLoader() instead of 
ClassLoader.getSystemClassLoader() here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 09:52:49 UTC 2024,,,,,,,,,,"0|z1n9gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 03:56;nilerzhou;[~zjureel] hi, could you please assign this task to me? I'd like to fix it.;;;","07/Feb/24 04:14;xtsong;Hi [~nilerzhou],

The description of this issue is a bit unclear to me. Could you provide a bit more information?
- In which ITCase did you run into the problem? If it's an ITCase that is not yet exist and you are planning to add, it would be helpful to also provide the codes so others can reproduce the issue.
- Where exactly are you suggesting to replace `ClassLoader.getSystemClassLoader()` with `Thread.currentThread().getContextClassLoader()`?

BTW, it is discouraged to use Mockito for testing. See the Code Style and Quality Guide for more details. 
https://flink.apache.org/how-to-contribute/code-style-and-quality-common/#avoid-mockito---use-reusable-test-implementations;;;","07/Feb/24 06:58;nilerzhou;Hi [~xtsong] ,

For the first question, yes, it's an ITCase that I'm going to add. The code is only for internal use in our company and not intended to be contributed to the community. But I can provide the keys to reproduce the issue. 

It's ITcase for Redis SQL Connector. I added a RunWith and PrepareForTest annotation, and then created a StreamTableEnvironment and used it to execute queries. Those queries would make mocked class to work and be checked. 
{code:java}
@RunWith(PowerMockRunner.class) 
@PrepareForTest(RedisClientTableUtils.class)

···

EnvironmentSettings envSettings =
        EnvironmentSettings.newInstance().inStreamingMode().build();
tEnv = StreamTableEnvironment.create(env, envSettings);{code}
 Then an exception is thrown:
{code:java}
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
    at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
    at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:75)
    at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
    at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
    at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:443)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
    at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1595)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.CompletionException: java.lang.ClassCastException: org.apache.flink.api.common.ExecutionConfig cannot be cast to org.apache.flink.api.common.ExecutionConfig
    at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
    at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592)
    ... 3 more
Caused by: java.lang.ClassCastException: org.apache.flink.api.common.ExecutionConfig cannot be cast to org.apache.flink.api.common.ExecutionConfig
    at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:98)
    at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
    at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:403)
    at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:379)
    at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
    at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
    at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
    ... 3 more {code}
For question 2, I found if no users jars exist the system classLoader will be used in [BlobLibraryCacheManager#242|https://github.com/apache/flink/blob/f1fba33d85a802b896170ff3cdb0107ee082c44a/flink-runtime/src/main/java/org/apache/flink/runtime/execution/librarycache/BlobLibraryCacheManager.java#L242] . The related issue is FLINK-32265

The fixed code should be:
{code:java}
private UserCodeClassLoader getOrResolveClassLoader(
        Collection<PermanentBlobKey> libraries, Collection<URL> classPaths)
        throws IOException {
    synchronized (lockObject) {
        verifyIsNotReleased();

        if (resolvedClassLoader == null) {
            boolean systemClassLoader =
                    wrapsSystemClassLoader && libraries.isEmpty() && classPaths.isEmpty();
            resolvedClassLoader =
                    new ResolvedClassLoader(
                            systemClassLoader
                                    ? Thread.currentThread().getContextClassLoader()                                                             : createUserCodeClassLoader(
                                            jobId, libraries, classPaths),
                            libraries,
                            classPaths,
                            systemClassLoader);
        } else {
            resolvedClassLoader.verifyClassLoader(libraries, classPaths);
        }

        return resolvedClassLoader;
    }
}{code}
And thanks for your advice, for our internal use, we depend on powerMock to check calling times for all APIs and the output RowDatas for the queries. Do you get any advice for the scenario?;;;","07/Feb/24 07:27;xtsong;Thanks for the detailed information.

I think this should not be a bug. The reported issue doesn't really affect any real production use case. And Flink is not designed to be executed with a PowerMockRunner or JavassistMockClassLoader.

I'm also not familiar with other approaches for checking API calling times and query outputs, other than manually implementing them. But if the cases are only for internal usages in your company, you don't really need to follow the community code-style guides.

My suggestion would be to apply the proposed changes only to your internal fork. WDYT?;;;","07/Feb/24 08:41;nilerzhou;[~xtsong] I cannot find other use case which the issue will affect either.  Only applying the changes to my internal fork is ok to me. However I get a minor concern, when I update my Flink version, I should copy this work version by version. If this change can be merged into the master, no copy work should be done. 

Returning to the change, do you have any concern about the replacement? ;;;","07/Feb/24 08:42;nilerzhou;[~fangyong] May I ask for your opinion on discussion between me and Xintong?;;;","07/Feb/24 09:15;martijnvisser;I don't think this is a change that Flink should incorporate like [~xtsong] stated: we don't want Mockito in the Flink code base and recommend against it. If you use that for your internal fork, then it's also up to you to solve any problems that might arise because you're using something that the Flink community discourages. ;;;","07/Feb/24 09:34;nilerzhou;[~martijnvisser]  I totally agree with your opinion that we should not do extra work to support what community discourages. Do you have any idea how to check API calling times for client to external system in SQL connector? Or what's the best practice to do integration testing for a SQL connector?;;;","07/Feb/24 09:44;xtsong; > Returning to the change, do you have any concern about the replacement?

Replacing `ClassLoader.getSystemClassLoader()` with `Thread.currentThread().getContextClassLoader()` implies the assumption that `BlobLibraryCacheManager` should be called from a thread that is created by the system classloader, which is implicit and fragile. If later `BlobLibraryCacheManager` is called from another thread, the assumption can easily be overlooked and broken, leading to unpredictable behaviors. This may not be absolutely unaffordable, but compared to what we gain from the changes, I'd rather not to apply it.;;;","07/Feb/24 09:49;nilerzhou;Thank you for your patience and time, [~xtsong] I'll try to find a better solution for integration test of SQL connectors and not to use PowerMock.;;;","07/Feb/24 09:52;xtsong;Thanks for you understanding.

Closing the ticket as ""Not A Bug"";;;",,,,,,,,,,,,,,,,,,,,,,
"Translate ""Flame Graphs"" page into Chinese",FLINK-34401,13567671,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lxliyou,lxliyou,lxliyou,07/Feb/24 02:27,06/Mar/24 11:00,04/Jun/24 20:40,06/Mar/24 11:00,1.20.0,,,,,,,1.20.0,,,,chinese-translation,,,,0,pull-request-available,,,,"The page is located at _""docs/content.zh/docs/ops/debugging/flame_graphs.md""_",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 10:59:54 UTC 2024,,,,,,,,,,"0|z1n9eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 02:37;lxliyou;I have done with it;;;","06/Mar/24 10:59;jingge;master: 76cce1e9d351eda4e76096707e1bc4302b200922;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka sources with watermark alignment sporadically stop consuming,FLINK-34400,13567624,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,asardaes,asardaes,06/Feb/24 17:10,04/Mar/24 09:32,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,,,,,0,,,,,"I have 2 Kafka sources that read from different topics. I have assigned them to the same watermark alignment group, and I have _not_ enabled idleness explicitly in their watermark strategies. One topic remains pretty much empty most of the time, while the other receives a few events per second all the time. Parallelism of the active source is 2, for the other one it's 1, and checkpoints are once every minute.

This works correctly for some time (10 - 15 minutes in my case) but then 1 of the active sources stops consuming, which causes lag to increase. Weirdly, after another 15 minutes or so, all the backlog is consumed at once, and then everything stops again.

I'm attaching some logs from the Task Manager where the issue appears. You will notice that the Kafka network client reports disconnections (a long time after the deserializer stopped reporting that events were being consumed), I'm not sure if this is related.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/24 09:31;asardaes;alignment_lags.png;https://issues.apache.org/jira/secure/attachment/13066565/alignment_lags.png","06/Feb/24 17:10;asardaes;logs.txt;https://issues.apache.org/jira/secure/attachment/13066526/logs.txt",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 09:31:45 UTC 2024,,,,,,,,,,"0|z1n948:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 19:56;martijnvisser;[~asardaes] Which version of the Flink Kafka connector did you use?;;;","06/Feb/24 19:57;martijnvisser;Also CC [~fanrui] who probably has better insights in this feature :);;;","06/Feb/24 20:08;asardaes;I'm using Kafka connector 3.0.2-1.18;;;","07/Feb/24 08:47;fanrui;Hi [~asardaes] , the flink version is 1.18.1 , right?

I saw your log has `Caused by: org.apache.kafka.clients.consumer.internals.NoAvailableBrokersException`. I'm not sure whether the root cause is about your kafka environment. Or the kafka cluster is fine, but the flink cluster cannot connect the kafka cluster well.

 

Also, is your flink running well when you don't use the watermark alignment?

If this issue is related to watermark alignment. Would you mind providing the simple demo code to reproduce it? I can help troubleshooting it.;;;","07/Feb/24 09:32;asardaes;Hi [~fanrui], yes I'm testing Flink 1.18.1 right now.

I also saw that exception in the logs, but I definitely find it odd. If I disable alignment, the Flink job works without issues. Additionally, the source  has a parallelism of 2, and only 1 of the instances shows issues, the other one continues consuming and committing offsets normally (see the attached graph which corresponds to Strimzi metrics, the yellow line is for the second topic in the alignment group that remains empty). Also, we have a lot of components consuming from different topics in the same Kafka cluster, and only the Flink job shows issues when alignment is enabled.

 !alignment_lags.png!

I unfortunately can't share the whole code, but we're not doing anything special with the Kafka connector, both sources are instantiated like this:

{noformat}
KafkaSource.<GenericChangeMessage>builder()
            .setBootstrapServers(config.internalBroker)
            .setTopics(config.someTopic)
            .setGroupId(config.internalBrokerGroupId)
            .setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.LATEST))
            .setDeserializer(new ...Deserializer()) // this changes between sources of course
            .setProperties(config.getInternalBrokerProperties())
            .setClientIdPrefix(""..."")
            .build()
{noformat}

and it really seems to be a problem at the source - if I look at the {{numRecordsOutPerSecond}} metric, it's the source itself that reports 0/s (for one of the parallel streams).

In case it's relevant, the watermark strategy is either {{noWatermarks}} or 

{noformat}
WatermarkStrategy.<T>forBoundedOutOfOrderness(maxAllowedWatermarkDrift) // 1 second
            .withWatermarkAlignment(alignmentGroup, maxAllowedWatermarkDrift, Duration.ofSeconds(1L))
{noformat}

Our Kafka cluster has 3 brokers and 30 partitions per topic.

I think that if I add {{.withIdleness(...)}} to the watermark strategy, it also works fine, but I will do some more testing.;;;","07/Feb/24 09:42;fanrui;Thanks a lot for the quick feedback!

It sounds a bug of watermark alignment. I will try to reproduce in my Local first, and then try to find the bug. I will feedback here if there is any progress.

Regarding time, I'm not sure if it can be solved in these two days. This Saturday is Chinese New Year, and I will be on vacation starting tomorrow night. If I cannot solve it tomorrow, I will continue analyze it after my vacation.;;;","07/Feb/24 09:58;fanrui;{quote} One topic remains pretty much empty most of the time, while the other receives a few events per second all the time.

I think that if I add {{.withIdleness(...)}} to the watermark strategy, it also works fine, but I will do some more testing.
{quote}
After re-read the background, I guess it may be not a bug.

When one source or topic doesn't have data, the watermark of this source won't be update. And then watermark alignment will let other quick sources to wait for this slow source.

As you said, I think idleness is suitable for your case. WDYT?;;;","07/Feb/24 10:52;asardaes;I actually thought idleness was going to be required for my use case, but while reading through FLINK-32496, I thought alignment implicitly enabled some form of idleness, although I don't know if that's configurable. Moreover, why would it only stall one of the parallel source streams instead of both?;;;","07/Feb/24 11:03;fanrui;{quote}I thought alignment implicitly enabled some form of idleness
{quote}
The idleness is disabled by default, it must be enabled by users. Because user needs to set how long it mark the source to be idle after no new data is received.
{quote}Moreover, why would it only stall one of the parallel source streams instead of both?
{quote}
As you said, one source is always empty. So I think one source doesn't have lag because it doesn't have data.

Other one has lag because it's waiting for the empty source. So only one source is blocked.

Please correct my if my understanding is wrong, thanks~;;;","07/Feb/24 11:24;asardaes;Ah, let me clarify. The setup is roughly like this:
 * Source 1 with parallelism 2 consuming from topic A that continuously receives messages
 ** I have 2 Task Managers, so each source reader should be consuming approximately 15 partitions in each TM.
 * Source 2 with parallelism 1 consuming from topic B that rarely receives messages and has stayed mostly empty during my experiments

Both sources were assigned to the same alignment group.

So, what I meant is that Source 1 is showing lag in only _one_ of its readers, and the corresponding error logs only show in 1 TM, the other reader and its TM never show errors. This is why the graph I posted earlier shows the lag is slightly reduced at one step, but then increases a lot more in the next step (instead of showing increasing steps all the time).

On the other hand, why would lag start showing up only after 15 minutes or so?

I will probably enable idleness anyway, but I was testing both scenarios and I find these inconsistencies kind of unexpected.;;;","18/Feb/24 11:26;fanrui;Hi [~asardaes] , did you try enable idleness?

I try watermark alignment on my Mac with kafka docker, it works well.
 * Don't enable idleness： The quick topic will be blocked when slow topic doesn't have data. (Because quick topic source is waiting for the slow source)
 * Set idleness = 5s for slow topic: The quick topic is only blocked for 5 seconds. After 5 seconds, it start consume.

Here is my test code, you can change a little code to simulate your case.
 * Data generator to Kafka topic:
 ** I generate some data to quick topic and slow topic.
 ** After a while, I restart it and don't write any data to slow topic. (Only write data to quick topic to simulate your case.)
 ** https://github.com/1996fanrui/fanrui-learning/blob/1069662b5fb434928c4141bf2397149cd354489b/module-flink/src/main/java/com/dream/flink/kafka/alignment/KafkaDataGenerator.java
 * Consume data from kafka:
 ** https://github.com/1996fanrui/fanrui-learning/blob/1069662b5fb434928c4141bf2397149cd354489b/module-flink/src/main/java/com/dream/flink/kafka/alignment/KafkaAlignmentDemo.java

 ;;;","04/Mar/24 09:31;asardaes;Hi [~fanrui], sorry for the late reply, it slipped my mind. I did try both approaches back then (with and without idleness), my point was that disabling idleness was behaving strangely: I also expect the quick topic to be blocked by the slow (empty) topic, but in my experiments this didn't happen consistenly, so consumption was unblocked sometimes for unknown reasons, and only one of the streams was blocked, not all of them.

In any case, I imagine this will be an unsupported configuration scenario with a somewhat undefined behavior.;;;",,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33644 Make QueryOperations SQL serializable,FLINK-34399,13567622,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,dwysakowicz,dwysakowicz,06/Feb/24 16:49,26/Feb/24 06:32,04/Jun/24 20:40,26/Feb/24 06:32,,,,,,,,,,,,Table SQL / API,,,,0,release-testing,,,,"Test suggestions:
1. Write a few Table API programs.
2. Call Table.getQueryOperation#asSerializableString, manually verify the produced SQL query
3. Check the produced SQL query is runnable and produces the same results as the Table API program:

{code}
Table table = tEnv.from(""a"") ...

String sqlQuery = table.getQueryOperation().asSerializableString();

//verify the sqlQuery is runnable
tEnv.sqlQuery(sqlQuery).execute().collect()
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33644,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 06:32:23 UTC 2024,,,,,,,,,,"0|z1n93s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 09:46;ferenc-csaky;Hi! I can take this task, would someone be able to help and assign it to me? Thanks!;;;","18/Feb/24 13:47;lincoln.86xy;[~ferenc-csaky] Thanks for taking this! Assigned to you.;;;","20/Feb/24 10:33;ferenc-csaky;Thank you [~lincoln.86xy], I'll proceed with it in the next 1-2 days.;;;","21/Feb/24 18:28;ferenc-csaky;Executed the following test scenarios:
 # Simple datagen table:
{code:sql}
CREATE TABLE IF NOT EXISTS `datagen_table` (
  `col_str` STRING,
  `col_int` INT,
  `col_ts` TIMESTAMP(3),
  WATERMARK FOR `col_ts` AS col_ts - INTERVAL '5' SECOND
) WITH (
  'connector' = 'datagen'
);
{code}
{{asSerializableString()}} result:
{code:sql}
SELECT `col_str`, `col_int`, `col_ts` FROM `default_catalog`.`default_database`.`datagen_table`
{code}

 # Aggreagte view:
{code:sql}
CREATE TABLE IF NOT EXISTS `txn_gen` (
  `id` INT,
  `amount` INT,
  `timestamp` TIMESTAMP(3),
  WATERMARK FOR `timestamp` AS `timestamp` - INTERVAL '1' SECOND
) WITH (
  'connector' = 'datagen',
  'fields.id.max' = '5',
  'fields.id.min' = '1',
  'rows-per-second' = '1'
);

CREATE VIEW IF NOT EXISTS `aggr_five_sec` AS SELECT
  `id`,
  COUNT(`id`) AS `txn_count`,
  TUMBLE_ROWTIME(`timestamp`, INTERVAL '5' SECOND) AS `w_row_time`
FROM `txn_gen`
GROUP BY `id`, TUMBLE(`timestamp`, INTERVAL '5' SECOND)
{code}
{{asSerializableString()}} result:
{code:sql}
SELECT `id`, `txn_count`, `w_row_time` FROM `default_catalog`.`default_database`.`aggr_five_sec`
{code}

 # Join view:
{code:sql}
CREATE TEMPORARY TABLE IF NOT EXISTS `location_updates` (
  `character_id` INT,
  `location` STRING,
  `proctime` AS PROCTIME()
)
WITH (
  'connector' = 'faker', 
  'fields.character_id.expression' = '#{number.numberBetween ''0'',''100''}',
  'fields.location.expression' = '#{harry_potter.location}'
);

CREATE TEMPORARY TABLE IF NOT EXISTS `characters` (
  `character_id` INT,
  `name` STRING
)
WITH (
  'connector' = 'faker', 
  'fields.character_id.expression' = '#{number.numberBetween ''0'',''100''}',
  'fields.name.expression' = '#{harry_potter.characters}'
);

CREATE TEMPORARY VIEW IF NOT EXISTS `joined` AS SELECT 
  c.character_id,
  l.location,
  c.name
FROM location_updates AS l
JOIN characters FOR SYSTEM_TIME AS OF proctime AS c
ON l.character_id = c.character_id;
{code}
{{asSerializableString()}} result:
{code:sql}
SELECT `character_id`, `location`, `name` FROM `default_catalog`.`default_database`.`joined`
{code}

Job execution went fine, all tests gave the expected resulst. Also checked the related PRs for this feature, it is very well covered with unit tests, so I think it looks good and works as desired.;;;","26/Feb/24 06:32;lincoln.86xy;[~ferenc-csaky]Thanks for your testing work!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validation Error in FlinkSessionJob Savepoint UpgradeMode Configuration,FLINK-34398,13567592,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,ddebowczyk,ddebowczyk,06/Feb/24 13:18,07/Feb/24 14:14,04/Jun/24 20:40,07/Feb/24 14:14,kubernetes-operator-1.7.0,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"When configuring a {{FlinkSessionJob}} to utilize the {{savepoint}} {{upgradeMode}}, the {{flink-kubernetes-webhook}} throws an exception indicating that the checkpoint/savepoint directory must be defined. This occurs despite having set the options {{state.checkpoints.dir}} and {{state.savepoints.dir}} in the {{flinkConfiguration}} of {{FlinkSessionJob}}. Interestingly, the validation passes only when these options are set in the {{flinkConfiguration}} of {{FlinkDeployment}}. However, I believe this behavior is flawed. The {{flinkConfiguration}} overrides from the {{FlinkSessionJob}} spec should also be considered during session job spec validation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 14:14:06 UTC 2024,,,,,,,,,,"0|z1n8x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 14:14;gyfora;merged to main f5d9b1917aec875bf04c35d6d3debd93f620c76d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource wait timeout can't be disabled,FLINK-34397,13567584,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,06/Feb/24 12:30,11/Mar/24 12:44,04/Jun/24 20:40,,1.17.2,,,,,,,1.17.3,1.18.2,1.20.0,,Runtime / Configuration,,,,0,,,,,"The documentation for {{jobmanager.adaptive-scheduler.resource-wait-timeout}} states that:

??Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.??

However, we don't support parsing negative durations.

{code}
Could not parse value '-1 s' for key 'jobmanager.adaptive-scheduler.resource-wait-timeout'.
Caused by: java.lang.NumberFormatException: text does not start with a number
	at org.apache.flink.util.TimeUtils.parseDuration(TimeUtils.java:80)
	at org.apache.flink.configuration.ConfigurationUtils.convertToDuration(ConfigurationUtils.java:399)
	at org.apache.flink.configuration.ConfigurationUtils.convertValue(ConfigurationUtils.java:331)
	at org.apache.flink.configuration.Configuration.lambda$getOptional$3(Configuration.java:729)
	at java.base/java.util.Optional.map(Optional.java:260)
	at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:729)
	... 2 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 04:33:39 UTC 2024,,,,,,,,,,"0|z1n8vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 12:41;chesnay;This is a bit irritating to fix. 0 has different semantics (== dont wait at all), so I think we'll need to add a separate option that controls whether the wait timeout is applied or not.;;;","20/Feb/24 04:33;pulkitjain;[~chesnay] 

Is there any update on this issue? We are also facing this issue on Flink version - 1.16.1. Could you confirm is this issue is applicable for this release as well?

 

Thanks

Pulkit;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-32775 Support yarn.provided.lib.dirs to add parent directory to classpath,FLINK-34396,13567573,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,argoyal,lincoln.86xy,lincoln.86xy,06/Feb/24 11:44,07/Feb/24 01:23,04/Jun/24 20:40,07/Feb/24 01:23,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34395,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:45;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066513/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 01:23:01 UTC 2024,,,,,,,,,,"0|z1n8sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:45;lincoln.86xy;[~argoyal]Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","06/Feb/24 23:16;argoyal;Hi [~lincoln.86xy] , this feature doesnot need cross-team testing. Can you please help close this ticket.;;;","07/Feb/24 01:23;lincoln.86xy;[~argoyal] Thanks for confirming :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-32514 Support using larger checkpointing interval when source is processing backlog,FLINK-34395,13567572,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,yunfengzhou,lincoln.86xy,lincoln.86xy,06/Feb/24 11:43,16/Feb/24 11:48,04/Jun/24 20:40,16/Feb/24 11:48,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34396,,FLINK-34394,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:43;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066512/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 16 11:48:11 UTC 2024,,,,,,,,,,"0|z1n8so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:43;lincoln.86xy;[~yunfengzhou]Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","08/Feb/24 15:49;jingge;pinged [~yunfengzhou] in the Slack, waiting for the feedback;;;","15/Feb/24 02:32;yunfengzhou;Hi [~lincoln.86xy] [~jingge] Sorry for the late reply. This feature does not need cross-team testing. Could you please help close this ticket? It seems that I don't have the permission to close it.;;;","16/Feb/24 11:48;lincoln.86xy;[~yunfengzhou] Thanks for your confirming!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33028 Make expanding behavior of virtual metadata columns configurable,FLINK-34394,13567571,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,lincoln.86xy,lincoln.86xy,06/Feb/24 11:40,09/Feb/24 08:36,04/Jun/24 20:40,09/Feb/24 08:36,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34395,,FLINK-34393,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:41;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066511/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 09 08:35:39 UTC 2024,,,,,,,,,,"0|z1n8sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:41;lincoln.86xy;Hi [~twalthr] Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","09/Feb/24 08:35;twalthr;Hi [~lincoln.86xy], this feature is rather small and should be covered well by tests already. Is it used in production already in our internal fork. I will close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-20767 Add nested field support for SupportsFilterPushDown,FLINK-34393,13567570,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,vsowrirajan,lincoln.86xy,lincoln.86xy,06/Feb/24 11:36,08/Feb/24 18:50,04/Jun/24 20:40,08/Feb/24 18:50,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34394,,FLINK-34392,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:37;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066510/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 08 18:50:26 UTC 2024,,,,,,,,,,"0|z1n8s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:37;lincoln.86xy;[~vsowrirajan]Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","08/Feb/24 15:48;jingge;pinged [~vsowrirajan]  in the Slack, waiting for the feedback;;;","08/Feb/24 18:42;vsowrirajan;[~lincoln.86xy] [~jingge] It won't be required in this case. This feature is an internal optimization that doesn't require any user-facing changes. Currently, with this feature Flink SQL framework automatically pushes down nested fields filters to the underlying datasource if supported. Currently there are no datasources that has this support because the new API is yet to be released.;;;","08/Feb/24 18:43;vsowrirajan;Looks like I don't have permissions to close the ticket. Please feel free to close the ticket as cross-testing is not required in this case.;;;","08/Feb/24 18:50;jingge;Thanks for the clarification.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33146 Unify the Representation of TaskManager Location in REST API and Web UI,FLINK-34392,13567568,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,Zhanghao Chen,lincoln.86xy,lincoln.86xy,06/Feb/24 11:33,07/Feb/24 03:43,04/Jun/24 20:40,07/Feb/24 03:43,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34393,,FLINK-34391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 03:43:39 UTC 2024,,,,,,,,,,"0|z1n8rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:34;lincoln.86xy;[~Zhanghao Chen] Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket;;;","07/Feb/24 03:30;Zhanghao Chen;Closing it as it does not require cross-team testing;;;","07/Feb/24 03:31;Zhanghao Chen;[~lincoln.86xy] Hi, I cannot close it. Could you help with that?;;;","07/Feb/24 03:43;lincoln.86xy;[~Zhanghao Chen]Thanks for confirming! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-15959 Add min number of slots configuration to limit total number of slots,FLINK-34391,13567567,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xiangyu0xf,lincoln.86xy,lincoln.86xy,06/Feb/24 11:31,18/Feb/24 13:43,04/Jun/24 20:40,18/Feb/24 13:43,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34392,,FLINK-34367,,,,,,,,,,,,FLINK-15959,,FLINK-34452,,,,,,"06/Feb/24 11:32;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066508/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 18 13:43:28 UTC 2024,,,,,,,,,,"0|z1n8rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:32;lincoln.86xy;[~xiangyu0xf] Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket

 !screenshot-1.png! ;;;","07/Feb/24 06:05;xiangyu0xf;[~lincoln.86xy] Hi, I think this feature needs cross-team testing because it is a user facing feature.;;;","07/Feb/24 09:25;lincoln.86xy;[~xiangyu0xf] Thanks for confirming! 
If the documentation and test instructions are ready, you can create a test ticket.
;;;","08/Feb/24 15:51;jingge;pinged [~xiangyu0xf] in the Slack, waiting for the feedback;;;","18/Feb/24 01:33;xiangyu0xf;[~lincoln.86xy] You can close this ticket now. I've created this ticket for test: https://issues.apache.org/jira/browse/FLINK-34452;;;","18/Feb/24 13:43;lincoln.86xy;[~xiangyu0xf] Thanks for the updating!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33325 Built-in cross-platform powerful java profiler,FLINK-34390,13567566,13566678,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,easonqin,yunta,yunta,06/Feb/24 11:26,18/Feb/24 03:03,04/Jun/24 20:40,18/Feb/24 03:03,1.19.0,,,,,,,1.19.0,,,,Runtime / Web Frontend,,,,0,release-testing,,,,See https://issues.apache.org/jira/browse/FLINK-34310,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/24 02:43;easonqin;image-2024-02-08-10-43-27-679.png;https://issues.apache.org/jira/secure/attachment/13066589/image-2024-02-08-10-43-27-679.png","08/Feb/24 02:44;easonqin;image-2024-02-08-10-44-55-401.png;https://issues.apache.org/jira/secure/attachment/13066590/image-2024-02-08-10-44-55-401.png","08/Feb/24 02:45;easonqin;image-2024-02-08-10-45-13-951.png;https://issues.apache.org/jira/secure/attachment/13066591/image-2024-02-08-10-45-13-951.png","08/Feb/24 02:45;easonqin;image-2024-02-08-10-45-31-564.png;https://issues.apache.org/jira/secure/attachment/13066592/image-2024-02-08-10-45-31-564.png",,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 18 03:03:26 UTC 2024,,,,,,,,,,"0|z1n8rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/24 02:55;easonqin;Hi [~yunta] I have tested it and it meets expectations. Detailed test results are shown in the following screenshots.
h3. 1. Disable rest.profiling.enabled

!image-2024-02-08-10-43-27-679.png!
h3. 2. Enable rest.profiling.enabled

Cannot create another profiling while one is running. (Default profiling duration 30 s)

!image-2024-02-08-10-44-55-401.png!

11 snapshots were triggered, but only 10 will be retained in the end. (CPU mode failed because I tested it on my Mac)

!image-2024-02-08-10-45-13-951.png!

!image-2024-02-08-10-45-31-564.png!;;;","08/Feb/24 08:31;fanrui;Thanks a lot [~easonqin] for the testing! :);;;","18/Feb/24 03:03;yunta;[~easonqin] Thanks for the testing!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcAutoscalerStateStore explicitly writes update_time,FLINK-34389,13567558,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,06/Feb/24 10:32,15/Mar/24 02:25,04/Jun/24 20:40,15/Mar/24 02:25,kubernetes-operator-1.8.0,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,,"JdbcAutoscalerStateStore explicitly writes update_time instead of relying on the database to update.

Some databases doesn't support update the timestamp column automatically. For example, Derby doesn't support update the update_time automatically when we update any data. It's hard to do a general test during I developing the test for JdbcAutoscalerEventHandler.

 

As the common&open source service, in order to support all databases well, it's better to handle it inside of the service.

 

In order to unify the design for JdbcAutoscalerEventHandler and JdbcAutoscalerStateStore, we update the design of JdbcAutoscalerStateStore in this JIRA.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 15 02:25:14 UTC 2024,,,,,,,,,,"0|z1n8pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 02:25;fanrui;Merged to main(1.8.0) via: 733b1b54e23c7486418bdca706f85a97b896f469;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-28915 Support artifact fetching in Standalone and native K8s application mode,FLINK-34388,13567554,13566678,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Yu Chen,ferenc-csaky,ferenc-csaky,06/Feb/24 09:51,23/Feb/24 07:54,04/Jun/24 20:40,23/Feb/24 07:40,1.19.0,,,,,,,1.19.0,,,,Runtime / Metrics,,,,0,release-testing,,,,"This ticket covers testing FLINK-28915. More details and the added docs are accessible on the [PR|https://github.com/apache/flink/pull/24065]

Test 1: Pass {{local://}} job jar in standalone mode, check the artifacts are not actually copied.
Test 2: Pass multiple artifacts in standalone mode.
Test 3: Pass a non-local job jar in native k8s mode. [1]
Test 4: Pass additional remote artifacts in native k8s mode.

Available config options: https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#artifact-fetching

[1] Custom docker image build instructions: https://github.com/apache/flink-docker/tree/dev-master

Note: The docker build instructions also contains a web server example that can be used to serve HTTP artifacts.",,,,,,,,,,,,,,,,,,,,,,FLINK-34387,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 23 07:54:54 UTC 2024,,,,,,,,,,"0|z1n8oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 11:00;Yu Chen;I'd like to take this Release Testing. Is there anyone helps to assinged this to me?;;;","23/Feb/24 03:51;Yu Chen;Hi [~ferenc-csaky]  [~lincoln.86xy]  [~yunta] , 

I have completed this Release Testing. And I think this feature works well in branch `release-1.19`, and this ticket can be closed.

Here are some testing logs as a reference.
*> Test 1: Pass local:// job jar in standalone mode, and check the artifacts are not actually copied.*
*Passed.* Flink will use the original file and not actually copied.
By the way, it also works with an absolute path(without `local://`), but it will copy the file to the `user.artifacts.base-dir`. I'm not sure whether it's expected (In my opinion, it was a local file, maybe we don't need to copy that cc [~ferenc-csaky] ).

*> Test 2: Pass multiple artifacts in standalone mode.*
*Passed.* In StandaloneJobCluster, flink could load the jar with `http://`(copied) and `local://`(not copied) simultaneously.

*> Test 3: Pass a non-local job jar in native k8s mode.*
*Passed.* Tested by starting native k8s application cluster in `minikube` with following command,
{code:java}
./bin/flink run-application \
--target kubernetes-application \
-Dkubernetes.cluster-id=my-first-application-cluster\
-Dkubernetes.container.image.ref=flink:test_community_1.19_SN \
http://localhost:9999/data/WordCount.jar {code}


flink will throw expected exception with hints(set `user.artifacts.raw-http-enabled` to true).
{code:java}
2024-02-23 02:20:48,305 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Could not create application program.
java.lang.RuntimeException: java.lang.IllegalArgumentException: Artifact fetching from raw HTTP endpoints are disabled. Set the 'user.artifacts.raw-http-enabled' property to override.
at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.fetchArtifacts(KubernetesApplicationClusterEntrypoint.java:158) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.getPackagedProgramRetriever(KubernetesApplicationClusterEntrypoint.java:129) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.getPackagedProgram(KubernetesApplicationClusterEntrypoint.java:111) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.lambda$main$0(KubernetesApplicationClusterEntrypoint.java:85) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.main(KubernetesApplicationClusterEntrypoint.java:85) [flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
Caused by: java.lang.IllegalArgumentException: Artifact fetching from raw HTTP endpoints are disabled. Set the 'user.artifacts.raw-http-enabled' property to override.
at org.apache.flink.client.program.artifact.ArtifactFetchManager.isRawHttp(ArtifactFetchManager.java:166) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.client.program.artifact.ArtifactFetchManager.getFetcher(ArtifactFetchManager.java:142) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.client.program.artifact.ArtifactFetchManager.fetchArtifact(ArtifactFetchManager.java:157) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.client.program.artifact.ArtifactFetchManager.fetchArtifacts(ArtifactFetchManager.java:124) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.fetchArtifacts(KubernetesApplicationClusterEntrypoint.java:156) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
... 5 more {code}


After setting the parameter in `flink-conf.yaml`, the jar was copied to `user.artifacts.base-dir` and the job was running as expected.


*> Test 4: Pass additional remote artifacts in native k8s mode.*
*Passed.* Tested by starting native k8s application cluster with commands in `Test 3`.
In addition, we added additional jars in `flink-conf.yaml`, they are copied as expected and the job running well.
{code:java}
user.artifacts.artifact-list: http://10.23.171.97:9999/data/AsyncIO.jar;http://10.23.171.97:9999/data/WordCount.jar; {code}
{code:java}
root@my-first-application-cluster-8479579f45-cdpc9:/opt/flink/artifacts/default/my-first-application-cluster# ls
AsyncIO.jar WindowJoin.jar WordCount.jar {code};;;","23/Feb/24 07:40;yunta;Thanks for [~Yu Chen]'s work!
I'll mark this ticket as resolved first, and we can continue discussing the left question [~ferenc-csaky];;;","23/Feb/24 07:54;ferenc-csaky;Thank you [~Yu Chen] for the thorough testing!

{quote}By the way, it also works with an absolute path(without `local://`), but it will copy the file to the `user.artifacts.base-dir`. I'm not sure whether it's expected (In my opinion, it was a local file, maybe we don't need to copy that cc Ferenc Csaky ){quote}

This is a good point, with the current implementation a simple absolute path without a prefix will be handled as remote, cause there is a strict check to {{local}} scheme, so it ignores {{file}}, which will be present, when a path without a prefix is passed. I agree with your opinion and would ocnsider this a bug. I would not consider this a major thing that should block the 1.19 release, but it would be an easy fix, so I can prepare a PR today.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33695 Introduce TraceReporter and use it to create checkpointing and recovery traces ,FLINK-34387,13567542,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Do,,pnowojski,lincoln.86xy,06/Feb/24 08:52,06/Feb/24 13:06,04/Jun/24 20:40,06/Feb/24 12:54,1.19.0,,,,,,,1.19.0,,,,Runtime / Metrics,,,,0,release-testing,,,,"This ticket covers testing three related features: FLINK-33695, FLINK-33735 and FLINK-33696.

Instructions:
#  Configure Flink to use [Slf4jTraceReporter|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/trace_reporters/#slf4j] and with enabled *INFO* level logging (can be to console or to a file, doesn't matter).
# Start a streaming job with enabled checkpointing.
# Let it run for a couple of checkpoints.
# Verify presence of a single *JobInitialization* [1] trace logged just after job start up.
# Verify presence of a couple of *Checkpoint* [1] traces logged after each successful or failed checkpoint.

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/ops/traces/#checkpointing-and-initialization",,,,,,,,,,,,,,,,,,,,FLINK-34388,,FLINK-34289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 13:06:18 UTC 2024,,,,,,,,,,"0|z1n8m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 12:54;pnowojski;After giving it a second thought I'm closing this ticket, as there is no big need to manually cross test this feature.;;;","06/Feb/24 13:06;lincoln.86xy;[~pnowojski] Thanks a lot for reconfirming.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RocksDB bloom filter metrics,FLINK-34386,13567539,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hejufang001,hejufang001,hejufang001,06/Feb/24 08:33,20/Feb/24 14:42,04/Jun/24 20:40,20/Feb/24 14:42,1.18.0,,,,,,,1.20.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"In our production environment, with RocksDB bloom filter enabled, the performance optimization effect on task state reading is obvious. However, there is a lack of usage metrics for bloom filter, If these Metrics are reported via Metrics reporter, it is easy to monitor the effectiveness of bloom filter optimization.

And these metrics are available from RocksDB Statistics:

BLOOM_FILTER_USEFUL: times bloom filter has avoided file reads.

BLOOM_FILTER_FULL_POSITIVE: times bloom FullFilter has not avoided the reads.

BLOOM_FILTER_FULL_TRUE_POSITIVE: times bloom FullFilter has not avoided the reads and data actually exist.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 14:42:24 UTC 2024,,,,,,,,,,"0|z1n8lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 08:39;hejufang001;[~pnowojski] could you take a look at this? ;;;","20/Feb/24 14:42;pnowojski;merged commit 890a995 into apache:master now

Thanks [~hejufang001]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When using table api to operate a jdbc table whose name is a keyword, exceptions like ""You have an error in your SQL syntax..."" will be reported.",FLINK-34385,13567535,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,chlambda,chlambda,06/Feb/24 07:59,06/Feb/24 12:22,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,"For instance, I created a table named 'table', and when executing {{{}tableEnvironment.from(""table""){}}}, it results in an error message that reads: ""You have an error in your SQL syntax; please consult the manual for the correct syntax to use near 'table' at line 1 corresponding to your MySQL server version."" Even if I enclose the table name with backticks, the error continues to occur because these backticks are stripped within the flink-sql-parser's code.

In FLINK-16067 issue, there is a suggestion stating: ""I propose making the parsing logic for identifiers in the Table API more flexible. We should not force users to escape any SQL identifiers."" This explains why backticks are removed by the flink-sql-parser. However, this aspect seems to be overlooked in the flink-connector-jdbc's code.

If indeed this is a genuine issue, I would be more than happy to submit a Pull Request to fix it. Please assign it to me.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-06 07:59:40.0,,,,,,,,,,"0|z1n8kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33735 Improve the exponential-delay restart-strategy ,FLINK-34384,13567528,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,caicancai,lincoln.86xy,lincoln.86xy,06/Feb/24 06:56,07/Feb/24 06:03,04/Jun/24 20:40,07/Feb/24 06:03,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,release-testing,,,,"Test suggestion:
 # Prepare a datastream job that all tasks throw exception directly.
 ## Set the parallelism to 5 or above
 # Prepare some configuration options:
 ** restart-strategy.type : exponential-delay
 ** restart-strategy.exponential-delay.attempts-before-reset-backoff : 7
 # Start the cluster: ./bin/start-cluster.sh
 # Run the job: ./bin/flink run -c className jarName
 # Check the result
 ** Check whether job will be retried 7 times
 ** Check the exception history, the list has 7 exceptions
 ** Each retries except the last one can see the 5 subtasks(They are concurrent exceptions).

!image-2024-02-06-15-05-05-331.png|width=1624,height=797!  
 

Note: Set these options mentioned at step2 at 2 level separately
 * Cluster level: set them in the config.yaml
 * Job level: Set them in the code

 

Job level demo:
{code:java}
public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();

    conf.setString(""restart-strategy"", ""exponential-delay"");
    conf.setString(""restart-strategy.exponential-delay.attempts-before-reset-backoff"", ""6"");
    StreamExecutionEnvironment env =  StreamExecutionEnvironment.getExecutionEnvironment(conf);
    env.setParallelism(5);

    DataGeneratorSource<Long> generatorSource =
            new DataGeneratorSource<>(
                    value -> value,
                    300,
                    RateLimiterStrategy.perSecond(10),
                    Types.LONG);

    env.fromSource(generatorSource, WatermarkStrategy.noWatermarks(), ""Data Generator"")
            .map(new RichMapFunction<Long, Long>() {
                @Override
                public Long map(Long value) {
                    throw new RuntimeException(
                            ""Excepted testing exception, subtaskIndex: "" + getRuntimeContext().getIndexOfThisSubtask());
                }
            })
            .print();

    env.execute();
} {code}",,,,,,,,,,,,,,,,,,,,,,FLINK-34288,,,,,,,,,,,,,,,,,,,,"06/Feb/24 07:05;fanrui;image-2024-02-06-15-05-05-331.png;https://issues.apache.org/jira/secure/attachment/13066493/image-2024-02-06-15-05-05-331.png","07/Feb/24 05:49;caicancai;image-2024-02-07-13-49-03-024.png;https://issues.apache.org/jira/secure/attachment/13066538/image-2024-02-07-13-49-03-024.png","06/Feb/24 06:58;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066492/screenshot-1.png",,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 06:02:45 UTC 2024,,,,,,,,,,"0|z1n8iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 05:48;caicancai;[~fanrui] [~lincoln.86xy] test success;;;","07/Feb/24 05:49;caicancai;!image-2024-02-07-13-49-03-024.png!;;;","07/Feb/24 06:02;fanrui;Thanks a lot [~caicancai] for the testing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify the comment with incorrect syntax,FLINK-34383,13567527,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,lxliyou,lxliyou,06/Feb/24 06:55,22/Feb/24 02:22,04/Jun/24 20:40,22/Feb/24 02:22,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,,There is an error in the syntax of the comment for the class PermanentBlobCache,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-06 06:55:54.0,,,,,,,,,,"0|z1n8io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33625 Support System out and err to be redirected to LOG or discarded,FLINK-34382,13567526,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,caicancai,lincoln.86xy,lincoln.86xy,06/Feb/24 06:50,07/Feb/24 09:13,04/Jun/24 20:40,06/Feb/24 12:46,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,release-testing,,,,"Test suggestion:
 # Prepare a Flink SQL job and a flink datastream job
 ** they can use the print sink or call System.out.println inside of the UDF
 # Add this config to the config.yaml
 ** taskmanager.system-out.mode : LOG
 # Run the job
 # Check whether the print log is redirected to log file

SQL demo:
{code:java}
./bin/sql-client.sh

CREATE TABLE orders (
  id           INT,
  app          INT,
  channel      INT,
  user_id      STRING,
  ts           TIMESTAMP(3),
  WATERMARK FOR ts AS ts
) WITH (
   'connector' = 'datagen',
   'rows-per-second'='20',
   'fields.app.min'='1',
   'fields.app.max'='10',
   'fields.channel.min'='21',
   'fields.channel.max'='30',
   'fields.user_id.length'='10'
);

create table print_sink ( 
  id           INT,
  app          INT,
  channel      INT,
  user_id      STRING,
  ts           TIMESTAMP(3)
) with ('connector' = 'print' );

insert into print_sink
select id       ,app       ,channel       ,user_id       ,ts   from orders 
{code}",,,,,,,,,,,,,,,,,,,,,,FLINK-34308,,,,,,,,,,,,,,,,,,,,"06/Feb/24 12:11;caicancai;test1.png;https://issues.apache.org/jira/secure/attachment/13066514/test1.png","06/Feb/24 12:11;caicancai;test2.png;https://issues.apache.org/jira/secure/attachment/13066515/test2.png",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 09:13:43 UTC 2024,,,,,,,,,,"0|z1n8ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 12:11;caicancai;[~lincoln.86xy] [~fanrui] test success;;;","06/Feb/24 12:45;fanrui;[~cancai] Thanks a lot for your testing.;;;","07/Feb/24 09:13;lincoln.86xy;[~caicancai] Thank you for testing this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`RelDataType#getFullTypeString` should be used to print in `RelTreeWriterImpl` if `withRowType` is true instead of `Object#toString`,FLINK-34381,13567524,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,xuyangzhong,xuyangzhong,06/Feb/24 06:47,08/Feb/24 09:00,04/Jun/24 20:40,,1.19.0,1.9.0,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Currently `RelTreeWriterImpl` use `rel.getRowType.toString` to print row type.
{code:java}
if (withRowType) {
  s.append("", rowType=["").append(rel.getRowType.toString).append(""]"")
} {code}
However, looking deeper into the code, we should use `rel.getRowType.getFullTypeString` to print the row type. Because the function `getFullTypeString` will print richer type information such as `nullable`. Take `StructuredRelDataType` as an example, the diff is below:
{code:java}
// source
util.addTableSource[(Long, Int, String)](""MyTable"", 'a, 'b, 'c)

// sql
SELECT a, c FROM MyTable

// rel.getRowType.toString
RecordType(BIGINT a, VARCHAR(2147483647) c)

// rel.getRowType.getFullTypeString
RecordType(BIGINT a, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" c) NOT NULL{code}
   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 09:42:22 UTC 2024,,,,,,,,,,"0|z1n8i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 09:42;xuyangzhong;I will try to fix it if this makes sense indeed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strange RowKind and records about intermediate output when using minibatch join,FLINK-34380,13567519,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,06/Feb/24 06:05,24/May/24 07:50,04/Jun/24 20:40,,1.19.0,,,,,,,1.20.0,,,,Table SQL / Runtime,,,,0,,,,,"{code:java}
// Add it in CalcItCase

@Test
  def test(): Unit = {
    env.setParallelism(1)
    val rows = Seq(
      changelogRow(""+I"", java.lang.Integer.valueOf(1), ""1""),
      changelogRow(""-U"", java.lang.Integer.valueOf(1), ""1""),
      changelogRow(""+U"", java.lang.Integer.valueOf(1), ""99""),
      changelogRow(""-D"", java.lang.Integer.valueOf(1), ""99"")
    )
    val dataId = TestValuesTableFactory.registerData(rows)

    val ddl =
      s""""""
         |CREATE TABLE t1 (
         |  a int,
         |  b string
         |) WITH (
         |  'connector' = 'values',
         |  'data-id' = '$dataId',
         |  'bounded' = 'false'
         |)
       """""".stripMargin
    tEnv.executeSql(ddl)

    val ddl2 =
      s""""""
         |CREATE TABLE t2 (
         |  a int,
         |  b string
         |) WITH (
         |  'connector' = 'values',
         |  'data-id' = '$dataId',
         |  'bounded' = 'false'
         |)
       """""".stripMargin
    tEnv.executeSql(ddl2)

    tEnv.getConfig.getConfiguration
      .set(ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ENABLED, Boolean.box(true))
    tEnv.getConfig.getConfiguration
      .set(ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ALLOW_LATENCY, Duration.ofSeconds(5))
    tEnv.getConfig.getConfiguration
      .set(ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_SIZE, Long.box(3L))

    println(tEnv.sqlQuery(""SELECT * from t1 join t2 on t1.a = t2.a"").explain())

    tEnv.executeSql(""SELECT * from t1 join t2 on t1.a = t2.a"").print()
  } {code}
Output:
{code:java}
+----+-------------+-----------------+-------------+---------+
| op |           a |               b |          a0 |      b0 |
+----+-------------+-----------------+-------------+---------+
| +U |           1 |               1 |           1 |      99 |
| +U |           1 |              99 |           1 |      99 |
| -U |           1 |               1 |           1 |      99 |
| -D |           1 |              99 |           1 |      99 |
+----+-------------+-----------------+-------------+---------+{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34349,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 07:50:27 UTC 2024,,,,,,,,,,"0|z1n8gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 06:38;xuyangzhong;Hi,  [~xu_shuai_] . Can you help check it again?;;;","01/Mar/24 07:44;xu_shuai_;Let me take a look.;;;","22/Apr/24 05:21;rovboyko;Hi [~xuyangzhong] , [~xu_shuai_] !

1) The RowKind can't be fixed in current architecture, because +I and +U are separated in different batches in this example. And this would be are bit tricky to fix it.

2) But the records order is really incorrect in this example and it can be easily fixed - https://github.com/rovboyko/flink/tree/fix/FLINK-34380;;;","30/Apr/24 04:55;rovboyko;[~xuyangzhong] , [~xu_shuai_] , what do you think? May I create a PR for fixing it?;;;","14/May/24 02:42;rovboyko;[~xu_shuai_] , [~xuyangzhong] , could you please take a look?;;;","15/May/24 03:48;xuyangzhong;Sorry for this late reply. The results of this repair still don’t seem to meet expectations a little.

Still based on the above test, the result is following. However, the row kind of the first data should be `+I`, right?

 
{code:java}
+----+---+----+---+----+
| op | a | b  | a0| b0 |
+----+---+----+---+----+
| +U | 1 |  1 | 1 | 99 |
| -U | 1 |  1 | 1 | 99 |
| +U | 1 | 99 | 1 | 99 |
| -D | 1 | 99 | 1 | 99 |
+----+---+----+---+----+ {code}
 

 ;;;","15/May/24 07:13;rovboyko;Hi [~xuyangzhong] ! Thank you for your reply.

Yes, you're right - the RowKind still not fixed in this example. But I think we should consider to fix the RowKind in separate issue because:

1) Incorrect RowKind in your example is the common problem of MiniBatch functionality. It happens every time when +I and -U records are assigned to first batch and then +U record is assigned to second batch. And it can't be fixed easily and only for Join operator - we should try to reproduce the same for Aggregate operator and fix it as well

2) While incorrect RowKind is not so serious problem, the incorrect order of output records might be really critical because it leads to incorrect result

So I sugest to fix only incorrect order in this issue and create the separate one for incorrect RowKind.;;;","15/May/24 13:17;xu_shuai_;Hi [~rovboyko] , sorry for late reply. 
For the  incorrect order of output records, the minibatch optimization is designed to guarantee final consistency. And the fix you mentioned has been considered when the pr was reviewed. Flink is a distributed realtime processing system. The order of output could be guaranteed on a node by using LinkedHashMap, however, it could not be guaranteed when join operator runs on multiple nodes. So I think it makes little sense to keep the order here.

For the Rowkind, it was also reviewed. As you mentioned, it is a common problem of MiniBatch functionality. It does not influence final result. From the benefit perspective, this problem could be tolerable.;;;","16/May/24 09:58;rovboyko;Hi [~xu_shuai_] !

I can't agree with you because the order of messages for one key can't be violated even for distributed system. Otherwise it will lead to inconsistent result. You can check it by running the XuYang's example without last -D record for both join sides:
{code:java}
val rows = Seq(
      changelogRow(""+I"", java.lang.Integer.valueOf(1), ""1""),
      changelogRow(""-U"", java.lang.Integer.valueOf(1), ""1""),
      changelogRow(""+U"", java.lang.Integer.valueOf(1), ""99""),
//      changelogRow(""-D"", java.lang.Integer.valueOf(1), ""99"")
    ) {code}
And the result will be:
{code:java}
+----+-------------+--------------------------------+-------------+--------------------------------+
| op |           a |                              b |          a0 |                             b0 |
+----+-------------+--------------------------------+-------------+--------------------------------+
| +U |           1 |                              1 |           1 |                             99 |
| +U |           1 |                             99 |           1 |                             99 |
| -U |           1 |                              1 |           1 |                             99 |
+----+-------------+--------------------------------+-------------+--------------------------------+ {code}
This means that sink or downstream operator will receive the -U (retract record) as last record. And I guess no downstream or sink operator can correctly process elements in such order.;;;","24/May/24 04:12;rovboyko;Hi [~xu_shuai_] , [~xuyangzhong] !

I still think that messages order *for one single key* is the main problem which should be addressed in this issue.

Could you please review my comment above?;;;","24/May/24 07:50;xuyangzhong;Hi, [~rovboyko] . +1 to fix the wrong order. I'll take a look for your pr later.;;;",,,,,,,,,,,,,,,,,,,,,,
table.optimizer.dynamic-filtering.enabled lead to OutOfMemoryError,FLINK-34379,13567518,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jeyhunkarimov,zhuyinjun,zhuyinjun,06/Feb/24 06:05,30/May/24 14:17,04/Jun/24 20:40,30/May/24 14:16,1.17.2,1.18.1,,,,,,1.17.3,1.18.2,1.19.1,1.20.0,Table SQL / Planner,,,,0,pull-request-available,,,,"When using batch computing, I union all about 50 tables and then join other table. When compiling the execution plan, 
there throws OutOfMemoryError: Java heap space, which was no problem in  1.15.2. However, both 1.17.2 and 1.18.1 all throws same errors,This causes jobmanager to restart. Currently,it has been found that this is caused by table.optimizer.dynamic-filtering.enabled, which defaults is true,When I set table.optimizer.dynamic-filtering.enabled to false, it can be compiled and executed normally

code

TableEnvironment.create(EnvironmentSettings.newInstance()
.withConfiguration(configuration)
.inBatchMode().build())

sql=select att,filename,'table0' as mo_name from table0 UNION All select att,filename,'table1' as mo_name from table1 UNION All select att,filename,'table2' as mo_name from table2 UNION All select att,filename,'table3' as mo_name from table3 UNION All select att,filename,'table4' as mo_name from table4 UNION All select att,filename,'table5' as mo_name from table5 UNION All select att,filename,'table6' as mo_name from table6 UNION All select att,filename,'table7' as mo_name from table7 UNION All select att,filename,'table8' as mo_name from table8 UNION All select att,filename,'table9' as mo_name from table9 UNION All select att,filename,'table10' as mo_name from table10 UNION All select att,filename,'table11' as mo_name from table11 UNION All select att,filename,'table12' as mo_name from table12 UNION All select att,filename,'table13' as mo_name from table13 UNION All select att,filename,'table14' as mo_name from table14 UNION All select att,filename,'table15' as mo_name from table15 UNION All select att,filename,'table16' as mo_name from table16 UNION All select att,filename,'table17' as mo_name from table17 UNION All select att,filename,'table18' as mo_name from table18 UNION All select att,filename,'table19' as mo_name from table19 UNION All select att,filename,'table20' as mo_name from table20 UNION All select att,filename,'table21' as mo_name from table21 UNION All select att,filename,'table22' as mo_name from table22 UNION All select att,filename,'table23' as mo_name from table23 UNION All select att,filename,'table24' as mo_name from table24 UNION All select att,filename,'table25' as mo_name from table25 UNION All select att,filename,'table26' as mo_name from table26 UNION All select att,filename,'table27' as mo_name from table27 UNION All select att,filename,'table28' as mo_name from table28 UNION All select att,filename,'table29' as mo_name from table29 UNION All select att,filename,'table30' as mo_name from table30 UNION All select att,filename,'table31' as mo_name from table31 UNION All select att,filename,'table32' as mo_name from table32 UNION All select att,filename,'table33' as mo_name from table33 UNION All select att,filename,'table34' as mo_name from table34 UNION All select att,filename,'table35' as mo_name from table35 UNION All select att,filename,'table36' as mo_name from table36 UNION All select att,filename,'table37' as mo_name from table37 UNION All select att,filename,'table38' as mo_name from table38 UNION All select att,filename,'table39' as mo_name from table39 UNION All select att,filename,'table40' as mo_name from table40 UNION All select att,filename,'table41' as mo_name from table41 UNION All select att,filename,'table42' as mo_name from table42 UNION All select att,filename,'table43' as mo_name from table43 UNION All select att,filename,'table44' as mo_name from table44 UNION All select att,filename,'table45' as mo_name from table45 UNION All select att,filename,'table46' as mo_name from table46 UNION All select att,filename,'table47' as mo_name from table47 UNION All select att,filename,'table48' as mo_name from table48 UNION All select att,filename,'table49' as mo_name from table49 UNION All select att,filename,'table50' as mo_name from table50 UNION All select att,filename,'table51' as mo_name from table51 UNION All select att,filename,'table52' as mo_name from table52 UNION All select att,filename,'table53' as mo_name from table53；

Table allUnionTable = tEnv.sqlQuery(sql);
Table res =
allUnionTable.join(
allUnionTable
.groupBy(col(""att""))
.select(col(""att""), col(""att"").count().as(COUNT_NAME))
.filter(col(COUNT_NAME).isGreater(1))
.select(col(key).as(""l_key"")),
col(key).isEqual(col(""l_key""))
);

res.printExplain(ExplainDetail.JSON_EXECUTION_PLAN);

 

 

Exception trace

Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3181)
    at java.util.ArrayList.grow(ArrayList.java:267)
    at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:241)
    at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:233)
    at java.util.ArrayList.add(ArrayList.java:464)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.setTables(DynamicPartitionPruningUtils.java:240)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:163)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:175)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)
    at org.apache.flink.table.planner.utils.DynamicPartitionPruningUtils$DppDimSideChecker.visitDimSide(DynamicPartitionPruningUtils.java:190)

 

 ",1.17.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 14:17:03 UTC 2024,,,,,,,,,,"0|z1n8go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 08:26;lsy;[~zhuyinjun]Thanks for reporting it, are you testing the OOM that occurs locally in IDEA, or is it a production environment? Can you provide more information to help us reproduce it locally?;;;","06/Feb/24 08:38;zhuyinjun;[~lsy]  It is in the production environment. Once this batch task is run, it will cause the OOM to restart the jobmanager. Then, the local idea  test this batch task. This error is inevitable, errors are consistent with the production environment

Do you need any additional information？;;;","06/Feb/24 09:46;zhuyinjun;[~lsy] 

Some simple code can trigger this situation in both 1.17.1 and 1.18.1. prepare some test data
 
 
TableUtil.registerResCatalog(tEnv);
Catalog resCatalog = tEnv.getCatalog(RES_CATALOG_NAME).get();
resCatalog.createDatabase(""temp"", noConfigDataBase(), true);
List<String> strings = new ArrayList<>();
for (int i = 0; i < 54; i++) {
tEnv.executeSql(""CREATE TABLE IF NOT EXISTS `result`.`temp`.table"" + i + """" +
""(att STRING,filename STRING)"");
tEnv.fromValues(
DataTypes.ROW(
DataTypes.FIELD(""att"", DataTypes.STRING()),
DataTypes.FIELD(""filename"", DataTypes.STRING())
), row(String.valueOf(i), String.valueOf(i + 1))).insertInto(""`result`.`temp`.table"" + i)
.execute();
strings.add(""select att,filename from `result`.`temp`.table"" + i);
}
Table allUnionTable = tEnv.sqlQuery(String.join("" UNION ALL "", strings));
Table res =
allUnionTable.join(
allUnionTable
.groupBy(col(""att""))
.select(col(""att""), col(""att"").count().as(COUNT_NAME))
.filter(col(COUNT_NAME).isGreater(1))
.select(col(""att"").as(""l_key"")),
col(""att"").isEqual(col(""l_key""))
);
res.printExplain(ExplainDetail.JSON_EXECUTION_PLAN);;;;","06/Feb/24 12:47;lsy;[~zhuyinjun]Thanks for your response, I will take a look.;;;","27/Feb/24 12:11;zhuyinjun;Is there any progress now? This seems to be a problem caused by the use of recursive functions

[~lsy] ;;;","29/Feb/24 09:35;lsy;[~zhuyinjun] Sorry for late response, I don't have spent time to locate the root cause because of the current task at hand.;;;","02/Apr/24 02:50;Weijie Guo;I don't think this is a blocker, downgrade it to critical.;;;","29/Apr/24 04:03;lsy;Merged in master: cca14d4210634d481cacb11354e870807d570561;;;","29/Apr/24 04:05;lsy;[~jeyhunkarimov]could you please backport it to release-1.19, 1.18, 1.17?;;;","30/Apr/24 02:22;lsy;Release-1.19: f321970111cfb6f340bd2eb0795cf24b81d583a6
Release-1.18: 9d0858ee745bc835efa78a34d849d5f3ecb89f6d
Realase-1.7: d2f93a5527b05583fc97bbae511ca0ac95325c02;;;","15/May/24 07:35;jeyhunkarimov;Reopening because of patch needed, as a result of the comment: https://github.com/apache/flink/pull/24600#discussion_r1600843684;;;","22/May/24 14:28;hong;[~jeyhunkarimov] Any progress on this patch?;;;","27/May/24 13:51;jingge;master: 87b7193846090897b2feabf716ee5378bcd7585b;;;","28/May/24 07:23;hong;[~jeyhunkarimov] Thanks for the fix! We should backport the patch to 1.19 + 1.18 as well :);;;","30/May/24 07:15;jingge;release-1.19: 663eeaf4092ed8c393b03ff6425f66559d97dc66

release-1.18: 9fb0344e580369692ca404561beddeecbe01c925

release-1.17: 7fe25989a65474fd6184d7936d0c2167f9080528;;;","30/May/24 14:17;hong;Closing Jira as patch has been completed for master branch as well as 1.17, 1.18, 1.19 branch;;;",,,,,,,,,,,,,,,,,
Minibatch join disrupted the original order of input records,FLINK-34378,13567511,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,xuyangzhong,xuyangzhong,06/Feb/24 03:52,27/Feb/24 01:38,04/Jun/24 20:40,27/Feb/24 01:38,1.19.0,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,,,,,"I'm not sure if it's a bug. The following case can re-produce this situation.
{code:java}
// add it in CalcITCase
@Test
def test(): Unit = {
  env.setParallelism(1)
  val rows = Seq(
    row(1, ""1""),
    row(2, ""2""),
    row(3, ""3""),
    row(4, ""4""),
    row(5, ""5""),
    row(6, ""6""),
    row(7, ""7""),
    row(8, ""8""))
  val dataId = TestValuesTableFactory.registerData(rows)

  val ddl =
    s""""""
       |CREATE TABLE t1 (
       |  a int,
       |  b string
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$dataId',
       |  'bounded' = 'false'
       |)
     """""".stripMargin
  tEnv.executeSql(ddl)

  val ddl2 =
    s""""""
       |CREATE TABLE t2 (
       |  a int,
       |  b string
       |) WITH (
       |  'connector' = 'values',
       |  'data-id' = '$dataId',
       |  'bounded' = 'false'
       |)
     """""".stripMargin
  tEnv.executeSql(ddl2)

  tEnv.getConfig.getConfiguration
    .set(ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ENABLED, Boolean.box(true))
  tEnv.getConfig.getConfiguration
    .set(ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_ALLOW_LATENCY, Duration.ofSeconds(5))
  tEnv.getConfig.getConfiguration
    .set(ExecutionConfigOptions.TABLE_EXEC_MINIBATCH_SIZE, Long.box(20L))

  println(tEnv.sqlQuery(""SELECT * from t1 join t2 on t1.a = t2.a"").explain())

  tEnv.executeSql(""SELECT * from t1 join t2 on t1.a = t2.a"").print()
}{code}
Result
{code:java}
+----+---+---+---+---+ 
| op | a | b | a0| b0| 
+----+---+---+---+---+ 
| +I | 3 | 3 | 3 | 3 | 
| +I | 7 | 7 | 7 | 7 | 
| +I | 2 | 2 | 2 | 2 | 
| +I | 5 | 5 | 5 | 5 | 
| +I | 1 | 1 | 1 | 1 | 
| +I | 6 | 6 | 6 | 6 | 
| +I | 4 | 4 | 4 | 4 | 
| +I | 8 | 8 | 8 | 8 | 
+----+---+---+---+---+

{code}
When I do not use minibatch join, the result is :
{code:java}
+----+---+---+----+----+
| op | a | b | a0 | b0 |
+----+---+---+----+----+
| +I | 1 | 1 |  1 |  1 |
| +I | 2 | 2 |  2 |  2 |
| +I | 3 | 3 |  3 |  3 |
| +I | 4 | 4 |  4 |  4 |
| +I | 5 | 5 |  5 |  5 |
| +I | 6 | 6 |  6 |  6 |
| +I | 7 | 7 |  7 |  7 |
| +I | 8 | 8 |  8 |  8 |
+----+---+---+----+----+
 {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34349,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 01:38:04 UTC 2024,,,,,,,,,,"0|z1n8f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 03:53;xuyangzhong;cc [~xu_shuai_] ;;;","06/Feb/24 08:29;lsy;This is a by-design behavior, we only guarantee consistency in the final result.;;;","06/Feb/24 08:48;libenchao;I'm wondering what kind of disorder it is, from the Jira description I cannot get it directly. Could you paste the output in the description, then others can know what the problem is without running the test themselves.;;;","07/Feb/24 02:20;xuyangzhong;[~lsy] The situation is that although I set the parallelism is ""1"", but the order of output in minibatch is still disrupted.

Hi, [~libenchao] . Thanks for reminding, I have attached the diff about results while tuning on and off the minibatch join.;;;","09/Feb/24 23:50;jeyhunkarimov;Hi [~xuyangzhong] the ordering is different even with parallelism 1 because of {{Set}} in {{MiniBatch}} operator. IMO this is expected behavior.  ;;;","19/Feb/24 08:51;xu_shuai_;Hi [~xuyangzhong] . This is an expected behavior. To maintain order, additional data structures would need to be introduced, which would result in a performance degradation and the ordered effect would only materialize when parallelism is set to 1. If order preservation is required with a parallelism of 1, it suffices to simply turn off the minibatch feature.;;;","27/Feb/24 01:38;xuyangzhong;Thanks for your answer, [~xu_shuai_]. That sounds good to me. I'll close this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33297 Support standard YAML for FLINK configuration,FLINK-34377,13567510,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xiasun,JunRuiLi,JunRuiLi,06/Feb/24 03:25,27/Feb/24 14:25,04/Jun/24 20:40,27/Feb/24 14:25,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,release-testing,,,,"This issue aims to verify FLIP-366.

Starting with version 1.19, Flink has officially introduced full support for the standard YAML 1.2 syntax. For detailed information, please refer to the Flink website:https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#flink-configuration-file

We may need to cover the following two types of test cases:

Test 1: For newly created jobs, utilize a config.yaml file to set up the Flink cluster. We need to verify that the job runs as expected with this new configuration.

Test 2: For existing jobs, migrate the legacy flink-conf.yaml to the new config.yaml. Test the job runs just like before post-migration.",,,,,,,,,,,,,,,,,,,,,,FLINK-34294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 14:25:55 UTC 2024,,,,,,,,,,"0|z1n8ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 05:56;xiasun;I'd like to take this ticket. [~zhuzh] Could you help to assign this to me?;;;","26/Feb/24 06:01;zhuzh;Thanks for volunteering! [~xiasun]
I have assigned you the ticket.;;;","27/Feb/24 14:17;xiasun;Hi [~zhuzh] [~lincoln.86xy] , I've completed the testing work as described and all test results align with expectations. The verification process was as follows:

For Test 1, I created a new Flink SQL job on a yarn-session cluster and configured it using the config.yaml file, specifically setting the parallelism.default. The job ran with the expected level of parallelism.

For Test 2, I utilized the migration script 'bin/migrate-config-file.sh' to transition an existing job's flink-conf.yaml to the new config.yaml format. The script successfully converted the configuration as anticipated, and the job ran with the proper settings per the new configuration.

 So I think it looks good and works as desired.;;;","27/Feb/24 14:25;lincoln.86xy;[~xiasun]Thanks for your testing!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLINK SQL SUM() causes a precision error,FLINK-34376,13567507,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liufangliang,liufangliang,06/Feb/24 03:11,07/Feb/24 03:39,04/Jun/24 20:40,,1.14.3,1.18.1,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"{code:java}
select cast(sum(CAST(9.000000000000000011 AS DECIMAL(38,18)) *10 ) as STRING) {code}
The precision is wrong in the Flink 1.14.3 and master branch

!image-2024-02-06-11-15-02-669.png!

 

The accuracy is correct in the Flink 1.13.2 

!image-2024-02-06-11-17-03-399.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/24 03:15;liufangliang;image-2024-02-06-11-15-02-669.png;https://issues.apache.org/jira/secure/attachment/13066485/image-2024-02-06-11-15-02-669.png","06/Feb/24 03:17;liufangliang;image-2024-02-06-11-17-03-399.png;https://issues.apache.org/jira/secure/attachment/13066486/image-2024-02-06-11-17-03-399.png",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 03:39:34 UTC 2024,,,,,,,,,,"0|z1n8e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 03:24;liufangliang;Hi [~matriv], [~twalthr], [~zonli] 

Related issues:

https://issues.apache.org/jira/browse/FLINK-24691;;;","07/Feb/24 03:39;xuyangzhong;This bug is introduced by https://issues.apache.org/jira/browse/FLINK-22586 .

We can simplify the sql to re-produce this bug: 
{code:java}
select cast(9.000000000000000011 AS DECIMAL(38,18)) * 10

+----+------------------------------------------+
| op |                                   EXPR$0 |
+----+------------------------------------------+
| +I |                               90.0000000 |
+----+------------------------------------------+ {code}
This is a designed behavior(but there seems to be some problems). For the multiplication of Decimal types, the following formula is currently used.

 
{code:java}
// ========================= Decimal Precision Deriving ==========================
// Adopted from ""https://docs.microsoft.com/en-us/sql/t-sql/data-types/precision-
// scale-and-length-transact-sql""
//
// Operation    Result Precision                        Result Scale
// e1 + e2      max(s1, s2) + max(p1-s1, p2-s2) + 1     max(s1, s2)
// e1 - e2      max(s1, s2) + max(p1-s1, p2-s2) + 1     max(s1, s2)
// e1 * e2      p1 + p2 + 1                             s1 + s2
// e1 / e2      p1 - s1 + s2 + max(6, s1 + p2 + 1)      max(6, s1 + p2 + 1)
// e1 % e2      min(p1-s1, p2-s2) + max(s1, s2)         max(s1, s2)
//
// Also, if the precision / scale are out of the range, the scale may be sacrificed
// in order to prevent the truncation of the integer part of the decimals. {code}
 

For Integer type, the default precision is 10 and the scale is 0. So the result precision and scale is (49, 18). However, the precision exceeds the max precision 38, then it chooses to adjust scale from 18 to 7:

 
{code:java}
integer part: 49 - 18 = 31
adjusted scale: 38 - 31 = 7{code}
IMO, the original design that choose to keep the integer part of the completion makes sense. But in this case, the result is wrong and we should fix it (by verifying mysql the result is `90.000000000000000110`).

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complete work for syntax `DESCRIBE EXTENDED tableName`,FLINK-34375,13567505,13567498,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,337361684@qq.com,xuyangzhong,xuyangzhong,06/Feb/24 02:59,06/Feb/24 07:18,04/Jun/24 20:40,,1.10.0,1.19.0,,,,,,,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,FLINK-28074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 03:30:40 UTC 2024,,,,,,,,,,"0|z1n8ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 03:30;337361684@qq.com;Hi, [~xuyangzhong] . I worked on the DESCRIBE EXTENDED syntax last year, so I'll take this issue and continue to complete it. Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complete work for syntax `DESCRIBE EXTENDED DATABASE databaseName`,FLINK-34374,13567504,13567498,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,337361684@qq.com,xuyangzhong,xuyangzhong,06/Feb/24 02:58,06/Feb/24 07:17,04/Jun/24 20:40,,1.10.0,1.19.0,,,,,,,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 03:31:59 UTC 2024,,,,,,,,,,"0|z1n8dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 03:31;337361684@qq.com;Hi, [~xuyangzhong] . I want to take this issue, can u assign to me! Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complete work for syntax `DESCRIBE DATABASE databaseName`,FLINK-34373,13567503,13567498,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,xuyangzhong,xuyangzhong,06/Feb/24 02:57,06/Feb/24 07:17,04/Jun/24 20:40,,1.10.0,1.19.0,,,,,,,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 06:51:39 UTC 2024,,,,,,,,,,"0|z1n8dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 06:51;xuyangzhong;I'll try to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complete work for syntax `DESCRIBE CATALOG catalogName`,FLINK-34372,13567502,13567498,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,xuyangzhong,xuyangzhong,06/Feb/24 02:56,04/Mar/24 02:22,04/Jun/24 20:40,,1.10.0,1.19.0,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,"The public api changes are announced in Flip-69 ([https://cwiki.apache.org/confluence/display/FLINK/FLIP-69%3A+Flink+SQL+DDL+Enhancement]). 

The result type of this query is also defined in Flip-69([https://cwiki.apache.org/confluence/display/FLINK/FLIP-69%3A+Flink+SQL+DDL+Enhancement] ) and Flip-84([https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=134745878]). Here I just show them again.

 
{code:java}
// Catalog

/**
* Get a user defined catalog description.
* @return a user-implement catalog detailed explanation
*/
default String explainCatalog() {
    return String.format(""CatalogClass:%s"", this.getClass().getCanonicalName());
} {code}
!image-2024-02-06-16-38-24-085.png|width=751,height=343!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/24 08:38;xuyangzhong;image-2024-02-06-16-38-24-085.png;https://issues.apache.org/jira/secure/attachment/13066496/image-2024-02-06-16-38-24-085.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 03:26:18 UTC 2024,,,,,,,,,,"0|z1n8d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 03:26;xuyangzhong;I'll try to do it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-331: Support EndOfStreamTrigger and isOutputOnlyAfterEndOfStream operator attribute to optimize task deployment,FLINK-34371,13567500,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,yunfengzhou,yunfengzhou,yunfengzhou,06/Feb/24 02:52,28/Feb/24 12:45,04/Jun/24 20:40,28/Feb/24 11:28,,,,,,,,1.20.0,,,,Runtime / Checkpointing,Runtime / Task,,,0,pull-request-available,,,,This is an umbrella ticket for FLIP-331.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 11:28:31 UTC 2024,,,,,,,,,,"0|z1n8co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 11:28;xtsong;master (1.20): 94b5f031a785d16077d870fe9e009d168077430b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Complete work and improve about enhanced Flink SQL DDL ,FLINK-34370,13567498,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,xuyangzhong,xuyangzhong,06/Feb/24 01:41,06/Feb/24 07:16,04/Jun/24 20:40,,1.10.0,1.19.0,,,,,,,,,,Table SQL / API,,,,0,,,,,"This is a umbrella Jira for completing work for [Flip-69]([https://cwiki.apache.org/confluence/display/FLINK/FLIP-69%3A+Flink+SQL+DDL+Enhancement]) about enhanced Flink SQL DDL.

With FLINK-34254(https://issues.apache.org/jira/browse/FLINK-34254), it seems that this flip is not finished yet.

The matrix is below:
|DDL|can be used in sql-client |
|_SHOW CATALOGS_|YES|
|_DESCRIBE_ _CATALOG catalogName_|{color:#de350b}NO{color}|
|_USE_ _CATALOG catalogName_|YES|
|_CREATE DATABASE dataBaseName_|YES|
|_DROP DATABASE dataBaseName_|YES|
|_DROP IF EXISTS DATABASE dataBaseName_|YES|
|_DROP DATABASE dataBaseName RESTRICT_|YES|
|_DROP DATABASE dataBaseName CASCADE_|YES|
|_ALTER DATABASE dataBaseName SET
( name=value [, name=value]*)|YES|
|_USE dataBaseName_|YES|
|_SHOW_ _DATABASES_|YES|
|_DESCRIBE DATABASE dataBaseName_|{color:#de350b}NO{color}|
|_DESCRIBE EXTENDED DATABASE dataBaseName_|{color:#de350b}NO{color}|
|_SHOW_ _TABLES_|YES|
|_DESCRIBE tableName_|YES|
|_DESCRIBE EXTENDED tableName_|{color:#de350b}NO{color}|
|_ALTER_ _TABLE tableName
RENAME TO newTableName|YES|
|_ALTER_ _TABLE tableName
SET ( name=value [, name=value]*)|YES|

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34254,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 06:18:05 UTC 2024,,,,,,,,,,"0|z1n8c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 01:45;337361684@qq.com;Hi, [~xuyangzhong] .I hope I can also participate in the development of the remaining flip features. Please cc me if there are any further developments. Thanks !;;;","06/Feb/24 06:18;xuyangzhong;[~337361684@qq.com] Thanks for your volunteering.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch connector supports SSL context,FLINK-34369,13567477,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,liuml07,liuml07,liuml07,05/Feb/24 20:14,28/May/24 07:17,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,"The current Flink ElasticSearch connector does not support SSL option, causing issues connecting to secure ES clusters.

As SSLContext is not serializable and possibly environment aware, we can add a (serializable) provider of SSL context to the {{NetworkClientConfig}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27054,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 07:17:14 UTC 2024,,,,,,,,,,"0|z1n87k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 20:16;liuml07;Note the Flink ElasticSearch [SQL connector|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/elasticsearch/] is also missing SSL options. That is tracked by FLINK-27054 as that may require different configurations than {{NetworkClientConfig}} API improvement.;;;","04/May/24 09:35;Sergey Nuyanzin;Merged as [5d1f8d03e3cff197ed7fe30b79951e44808b48fe|https://github.com/apache/flink-connector-elasticsearch/commit/5d1f8d03e3cff197ed7fe30b79951e44808b48fe];;;","04/May/24 09:37;Sergey Nuyanzin;The fix version should be after 3.1.0, however 3.1.0 is still in voting stage...
Let's keep it open until there will be another available unreleased version in jira or it will be taken in another RC;;;","28/May/24 07:17;liuml07;Can we close this now?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update GCS filesystems to latest available version v3.0,FLINK-34368,13567449,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,martijnvisser,martijnvisser,martijnvisser,05/Feb/24 16:16,07/Feb/24 10:14,04/Jun/24 20:40,,,,,,,,,,,,,FileSystems,,,,0,pull-request-available,,,,Update to https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v3.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-05 16:16:05.0,,,,,,,,,,"0|z1n81c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-34027 AsyncScalarFunction for asynchronous scalar function support,FLINK-34367,13567434,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,asheinberg,lincoln.86xy,lincoln.86xy,05/Feb/24 15:01,13/Feb/24 08:22,04/Jun/24 20:40,13/Feb/24 08:22,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34391,,FLINK-34310,,,,,,,,,,,,,,,,,,,,"05/Feb/24 15:03;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066470/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 13 00:03:44 UTC 2024,,,,,,,,,,"0|hzzvcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 15:03;lincoln.86xy;[~asheinberg] Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","08/Feb/24 15:48;jingge;Hi [~asheinberg] would you like to create a release testing ticket for your contributed feature? Thanks!;;;","12/Feb/24 09:18;jingge;[~twalthr] would you like to take a look at this ticket?;;;","13/Feb/24 00:03;asheinberg;Sorry, I missed this last week.  I don't believe that this requires cross team testing, since it's a fairly well defined new feature that shouldn't really affect the correctness of existing functionality and shouldn't affect other components.  Can you give me an idea of what sort of functionality you think requires this type of testing? [~lincoln.86xy] [~jingge] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support to group rows by column ordinals,FLINK-34366,13567430,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jeyhunkarimov,martijnvisser,martijnvisser,05/Feb/24 14:31,03/Mar/24 23:54,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,"Reference: BigQuery https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#group_by_col_ordinals

The GROUP BY clause can refer to expression names in the SELECT list. The GROUP BY clause also allows ordinal references to expressions in the SELECT list, using integer values. 1 refers to the first value in the SELECT list, 2 the second, and so forth. The value list can combine ordinals and value names. The following queries are equivalent:

{code:sql}
WITH PlayerStats AS (
  SELECT 'Adams' as LastName, 'Noam' as FirstName, 3 as PointsScored UNION ALL
  SELECT 'Buchanan', 'Jie', 0 UNION ALL
  SELECT 'Coolidge', 'Kiran', 1 UNION ALL
  SELECT 'Adams', 'Noam', 4 UNION ALL
  SELECT 'Buchanan', 'Jie', 13)
SELECT SUM(PointsScored) AS total_points, LastName, FirstName
FROM PlayerStats
GROUP BY LastName, FirstName;

/*--------------+----------+-----------+
 | total_points | LastName | FirstName |
 +--------------+----------+-----------+
 | 7            | Adams    | Noam      |
 | 13           | Buchanan | Jie       |
 | 1            | Coolidge | Kiran     |
 +--------------+----------+-----------*/
{code}

{code:sql}
WITH PlayerStats AS (
  SELECT 'Adams' as LastName, 'Noam' as FirstName, 3 as PointsScored UNION ALL
  SELECT 'Buchanan', 'Jie', 0 UNION ALL
  SELECT 'Coolidge', 'Kiran', 1 UNION ALL
  SELECT 'Adams', 'Noam', 4 UNION ALL
  SELECT 'Buchanan', 'Jie', 13)
SELECT SUM(PointsScored) AS total_points, LastName, FirstName
FROM PlayerStats
GROUP BY 2, 3;

/*--------------+----------+-----------+
 | total_points | LastName | FirstName |
 +--------------+----------+-----------+
 | 7            | Adams    | Noam      |
 | 13           | Buchanan | Jie       |
 | 1            | Coolidge | Kiran     |
 +--------------+----------+-----------*/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 17:10:11 UTC 2024,,,,,,,,,,"0|z1n7xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 17:10;jeyhunkarimov;Hi [~martijnvisser] I worked on this issue. Could you please check the PR in your available time? Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[docs] Delete repeated pages in Chinese Flink website and correct the Paimon url,FLINK-34365,13567424,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,waterking,waterking,waterking,05/Feb/24 13:49,07/Feb/24 02:14,04/Jun/24 20:40,07/Feb/24 02:14,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,,"The ""教程"" column on the [Flink 中文网|https://flink.apache.org/zh/how-to-contribute/contribute-documentation/] currently has two ""[With Paimon(incubating) (formerly Flink Table Store)|https://paimon.apache.org/docs/master/engines/flink/%22]"".

Therefore, I delete one for brevity.

Also, the current link is wrong and I correct it with this link ""[With Paimon(incubating) (formerly Flink Table Store)|https://paimon.apache.org/docs/master/engines/flink]""",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/24 13:49;waterking;微信截图_20240205214854.png;https://issues.apache.org/jira/secure/attachment/13066463/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240205214854.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,713,https://github.com/apache/flink-web/pull/713,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 02:14:38 UTC 2024,,,,,,,,,,"0|z1n7w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 13:51;waterking;This is my first open source contribution to Flink, please assign this issue to me :).

Thank you very much!;;;","06/Feb/24 06:24;wanglijie;Assigned to you [~waterking] :);;;","07/Feb/24 02:14;wanglijie;Fixed via branch asf-site(flink-web): ec2e5c2b4a312fe56e44e13c57b84c6f1331b992;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix release utils mount point to match the release doc and scripts,FLINK-34364,13567412,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,05/Feb/24 11:33,20/Feb/24 06:19,04/Jun/24 20:40,14/Feb/24 10:34,,,,,,,,connector-parent-1.1.0,,,,Connectors / Parent,Release System,,,0,pull-request-available,,,,parent_pom branch refers to an incorrect mount point tools/*release*/shared instead of tools/*releasing*/shared for the release_utils. _tools/releasing_/shared is the one used in the release scripts and in the release docs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 14 10:34:06 UTC 2024,,,,,,,,,,"0|z1n7tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 12:20;echauchot;Notably the source release script was excluding tools/releasing/shared but not tools/release/shared. This is why tools/release/shared was in the source release. 

And by the way I noticed that all the connectors source releases were containing an empty tools/releasing directory because only tools/releasing/shared is excluded in the source release script and not the whole tools/releasing directory. It seems a bit messy to me so I think we should fix that in the release scripts later on for next connectors releases.;;;","14/Feb/24 10:33;echauchot;parent_pom: c806d46ef06c8e46d28a6a8b5db3f5104cfe53bc;;;","14/Feb/24 10:34;echauchot;fixed release utils integration;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connectors release utils should allow to not specify flink version in stage_jars.sh,FLINK-34363,13567411,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,05/Feb/24 11:28,27/Feb/24 10:41,04/Jun/24 20:40,27/Feb/24 10:41,,,,,,,,,,,,Connectors / Parent,Release System,,,0,pull-request-available,,,,"For connectors-parent release, Flink version is not needed. The stage_jars.sh script should allow to specify only ${project_version} and not ${project_version}-${flink_minor_version}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-05 11:28:46.0,,,,,,,,,,"0|z1n7t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add argument to reuse connector docs cache in setup_docs.sh to improve build times,FLINK-34362,13567402,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,337361684@qq.com,qingyue,qingyue,05/Feb/24 10:13,26/Feb/24 06:01,04/Jun/24 20:40,26/Feb/24 06:01,1.19.0,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,"Problem:
The current build process of Flink's documentation involves the `setup_docs.sh` script, which re-clones connector repositories every time the documentation is built. This operation is time-consuming, particularly for developers in regions with slower internet connections or facing network restrictions (like the Great Firewall in China). This results in a build process that can take an excessive amount of time, hindering developer productivity.

 

Proposal:

We could add a command-line argument (e.g., --use-doc-cache) to the `setup_docs.sh` script, which, when set, skips the cloning step if the connector repositories have already been cloned previously. As a result, developers can opt to use the cache when they do not require the latest versions of the connectors' documentation. This change will reduce build times significantly and improve the developer experience for those working on the documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 06:01:21 UTC 2024,,,,,,,,,,"0|z1n7r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 10:18;qingyue;Cc [~martijnvisser], I'd like to hear your opinion.;;;","05/Feb/24 13:28;martijnvisser;[~qingyue] That makes sense to me, I do think that we would have to default to not use the cache (to avoid that the nightly documentation builds might suddenly publish an old artifact). Besides that, +1 for me ;;;","06/Feb/24 01:04;337361684@qq.com;Hi, [~qingyue]  and [~martijnvisser]. I'm looking forward to take this issue, can you assign to me?;;;","06/Feb/24 02:07;qingyue;Hi [~martijnvisser], +1 to not use cache by default. Hi [~337361684@qq.com], thanks for the volunteering. The ticket is assigned to you. ;;;","26/Feb/24 06:01;qingyue;Fixed in master a95b0fb75b6acc57e8cbde2847f26a1c870b03c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink end-to-end test fails in GHA,FLINK-34361,13567383,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,05/Feb/24 08:18,26/Feb/24 07:26,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,API / Python,,,,0,github-actions,test-stability,,,"""PyFlink end-to-end test"" fails:
https://github.com/apache/flink/actions/runs/7778642859/job/21208811659#step:14:7420

The only error I could identify is:
{code}
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
conda 23.5.2 requires ruamel-yaml<0.18,>=0.11.14, but you have ruamel-yaml 0.18.5 which is incompatible.
Feb 05 03:31:54 Successfully installed apache-beam-2.48.0 avro-python3-1.10.2 cloudpickle-2.2.1 crcmod-1.7 cython-3.0.8 dill-0.3.1.1 dnspython-2.5.0 docopt-0.6.2 exceptiongroup-1.2.0 fastavro-1.9.3 fasteners-0.19 find-libpython-0.3.1 grpcio-1.50.0 grpcio-tools-1.50.0 hdfs-2.7.3 httplib2-0.22.0 iniconfig-2.0.0 numpy-1.24.4 objsize-0.6.1 orjson-3.9.13 pandas-2.2.0 pemja-0.4.1 proto-plus-1.23.0 protobuf-4.23.4 py4j-0.10.9.7 pyarrow-11.0.0 pydot-1.4.2 pymongo-4.6.1 pyparsing-3.1.1 pytest-7.4.4 python-dateutil-2.8.2 pytz-2024.1 regex-2023.12.25 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8 tomli-2.0.1 typing-extensions-4.9.0 tzdata-2023.4
/home/runner/work/flink/flink/flink-python/dev/.conda/lib/python3.10/site-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /home/runner/work/flink/flink/flink-python/pyflink/fn_execution/table/window_aggregate_fast.pxd
  tree = Parsing.p_module(s, pxd, full_module_name)
{code}
Not sure whether that's the actual cause.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 07:26:14 UTC 2024,,,,,,,,,,"0|z1n7mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 15:58;mapohl;I tend to argue that this one is also caused by disk space issues (FLINK-34360). But the test completed and the there's still some disk space available. I'm gonna leave this one open for now to see whether something similar appears again.;;;","26/Feb/24 07:26;mapohl;Looks like it still appears: https://github.com/apache/flink/actions/runs/8019572934/job/21908054903#step:14:7622
{code}
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
conda 23.5.2 requires ruamel-yaml<0.18,>=0.11.14, but you have ruamel-yaml 0.18.6 which is incompatible.
Feb 23 14:20:34 Successfully installed apache-beam-2.48.0 avro-python3-1.10.2 cloudpickle-2.2.1 crcmod-1.7 cython-3.0.8 dill-0.3.1.1 dnspython-2.6.1 docopt-0.6.2 exceptiongroup-1.2.0 fastavro-1.9.4 fasteners-0.19 find-libpython-0.3.1 grpcio-1.50.0 grpcio-tools-1.50.0 hdfs-2.7.3 httplib2-0.22.0 iniconfig-2.0.0 numpy-1.24.4 objsize-0.6.1 orjson-3.9.14 pandas-2.2.0 pemja-0.4.1 proto-plus-1.23.0 protobuf-4.23.4 py4j-0.10.9.7 pyarrow-11.0.0 pydot-1.4.2 pymongo-4.6.2 pyparsing-3.1.1 pytest-7.4.4 python-dateutil-2.8.2 pytz-2024.1 regex-2023.12.25 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 tomli-2.0.1 typing-extensions-4.9.0 tzdata-2024.1
/home/runner/work/flink/flink/flink-python/dev/.conda/lib/python3.10/site-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /home/runner/work/flink/flink/flink-python/pyflink/fn_execution/table/window_aggregate_fast.pxd
  tree = Parsing.p_module(s, pxd, full_module_name)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GHA e2e test failure due to no space left on device error,FLINK-34360,13567382,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,05/Feb/24 08:11,19/Feb/24 11:43,04/Jun/24 20:40,07/Feb/24 14:20,1.18.1,1.19.0,,,,,,1.18.2,1.19.0,,,Tests,,,,0,github-actions,pull-request-available,test-stability,,"https://github.com/apache/flink/actions/runs/7763815214

{code}
AdaptiveScheduler / E2E (group 2)
Process completed with exit code 1.
AdaptiveScheduler / E2E (group 2)
You are running out of disk space. The runner will stop working when the machine runs out of disk space. Free space left: 35 MB
{code}

We're only seeing it in GHA which makes me think that it's rather a GHA-related issue.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-34357,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 14:20:10 UTC 2024,,,,,,,,,,"0|z1n7mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 08:11;mapohl;https://github.com/apache/flink/actions/runs/7763815403/job/21176583086;;;","05/Feb/24 08:12;mapohl;https://github.com/apache/flink/actions/runs/7770984519;;;","05/Feb/24 08:13;mapohl;here we also see it in the {{misc}} stage:
https://github.com/apache/flink/actions/runs/7770984575;;;","05/Feb/24 08:14;mapohl;https://github.com/apache/flink/actions/runs/7772852476;;;","05/Feb/24 08:14;mapohl;https://github.com/apache/flink/actions/runs/7773175436/job/21196623732;;;","05/Feb/24 08:14;mapohl;https://github.com/apache/flink/actions/runs/7775040261/job/21200652382;;;","05/Feb/24 08:14;mapohl;https://github.com/apache/flink/actions/runs/7778610011;;;","05/Feb/24 08:14;mapohl;https://github.com/apache/flink/actions/runs/7778592684;;;","05/Feb/24 08:15;mapohl;https://github.com/apache/flink/actions/runs/7778642859;;;","05/Feb/24 08:27;mapohl;https://github.com/apache/flink/actions/runs/7778643029/job/21208793783#step:14:9258;;;","05/Feb/24 08:29;mapohl;https://github.com/apache/flink/actions/runs/7778643029/job/21208808107#step:10:28837;;;","05/Feb/24 08:34;mapohl;https://github.com/apache/flink/actions/runs/7778643029/job/21208794625#step:14:8210;;;","05/Feb/24 08:38;mapohl;https://github.com/apache/flink/actions/runs/7778643029/job/21208808107#step:10:28837;;;","05/Feb/24 08:40;mapohl;https://github.com/apache/flink/actions/runs/7778659004;;;","06/Feb/24 09:02;mapohl;There was a drop in available disk space from 84G (see [workflow run from Feb 2|https://github.com/apache/flink/actions/runs/7750406652/job/21136823036#step:14:11726]) to 73G (a [workflow run from the day afterwards|https://github.com/apache/flink/actions/runs/7763815214/job/21176570116#step:14:11074]).

Looks like we're now close to the disk space that is required by certain tests because we still have succeeding e2e1 runs with the disk space being at 73G (see [most-recent example|https://github.com/apache/flink/actions/runs/7793652557/job/21253945170#step:14:11788]).;;;","06/Feb/24 10:01;mapohl;Essentially, we had 84G with 60G used by GitHub in the old setup (before Feb 2, 2024) and 73G with ~60G used by GitHub in the new setup. There's a drop of free disk space from 24G to 13-14G.

Interestingly, Azure Pipeline runners seem to have 73G as well (see [example build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57317&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=100]). But the Azure runners seem to have less used disk space which seems to be the reason why we're not running into issues there.;;;","07/Feb/24 14:20;mapohl;master: [f468b5bddc1b7179862be285eb22b31c48a35ae5|https://github.com/apache/flink/commit/f468b5bddc1b7179862be285eb22b31c48a35ae5]
1.19: [74057eb09a0b56065e34b8c6dca87c6fbd287410|https://github.com/apache/flink/commit/74057eb09a0b56065e34b8c6dca87c6fbd287410]
1.18: [9f76892380929ae9ee758ed332b1f1e8ef864481|https://github.com/apache/flink/commit/9f76892380929ae9ee758ed332b1f1e8ef864481];;;",,,,,,,,,,,,,,,,
"""Kerberized YARN per-job on Docker test (default input)"" failed due to IllegalStateException",FLINK-34359,13567379,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,05/Feb/24 07:56,05/Feb/24 08:05,04/Jun/24 20:40,05/Feb/24 08:05,1.18.1,,,,,,,,,,,Deployment / YARN,,,,0,github-actions,test-stability,,,"This looks similar to FLINK-34357 because it's also due to some YARN issue. But the e2e test ""Kerberized YARN per-job on Docker test (default input)"" is causing the failure:
{code}
[...]
Exception in thread ""Thread-4"" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:208)
	at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2570)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2801)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2776)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2654)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2636)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1100)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1707)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1688)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
	at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
{code}

https://github.com/apache/flink/actions/runs/7770984519/job/21191905887#step:14:11720",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34357,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 08:05:26 UTC 2024,,,,,,,,,,"0|z1n7m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 08:05;mapohl;I'm closing this one in favor of FLINK-34357. it looks like there is some more fundamental problem with the YARN-related e2e tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-connector-jdbc nightly fails with ""Expecting code to raise a throwable""",FLINK-34358,13567378,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wanglijie,martijnvisser,martijnvisser,05/Feb/24 07:52,20/Feb/24 12:33,04/Jun/24 20:40,20/Feb/24 12:33,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,0,pull-request-available,test-stability,,,"https://github.com/apache/flink-connector-jdbc/actions/runs/7770283211/job/21190280602#step:14:346

{code:java}
[INFO] Running org.apache.flink.connector.jdbc.dialect.cratedb.CrateDBDialectTypeTest
Error:  Tests run: 19, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.554 s <<< FAILURE! - in org.apache.flink.connector.jdbc.dialect.cratedb.CrateDBDialectTypeTest
Error:  org.apache.flink.connector.jdbc.dialect.cratedb.CrateDBDialectTypeTest.testDataTypeValidate(TestItem)[19]  Time elapsed: 0.018 s  <<< FAILURE!
java.lang.AssertionError: 

Expecting code to raise a throwable.
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 s - in org.apache.flink.connector.jdbc.catalog.JdbcCatalogUtilsTest
[INFO] Running org.apache.flink.architecture.ProductionCodeArchitectureTest
[INFO] Running org.apache.flink.architecture.ProductionCodeArchitectureBase
[INFO] Running org.apache.flink.architecture.rules.ApiAnnotationRules
[INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.155 s - in org.apache.flink.connector.jdbc.dialect.JdbcDialectTypeTest
[INFO] Running org.apache.flink.architecture.TestCodeArchitectureTest
[INFO] Running org.apache.flink.architecture.TestCodeArchitectureTestBase
[INFO] Running org.apache.flink.architecture.rules.ITCaseRules
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.109 s - in org.apache.flink.architecture.rules.ApiAnnotationRules
[INFO] Running org.apache.flink.architecture.rules.TableApiRules
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.024 s - in org.apache.flink.architecture.rules.TableApiRules
[INFO] Running org.apache.flink.architecture.rules.ConnectorRules
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.31 s - in org.apache.flink.architecture.rules.ConnectorRules
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.464 s - in org.apache.flink.architecture.ProductionCodeArchitectureBase
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.468 s - in org.apache.flink.architecture.ProductionCodeArchitectureTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.758 s - in org.apache.flink.architecture.rules.ITCaseRules
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.761 s - in org.apache.flink.architecture.TestCodeArchitectureTestBase
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.775 s - in org.apache.flink.architecture.TestCodeArchitectureTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 110.38 s - in org.apache.flink.connector.jdbc.databases.oracle.xa.OracleExactlyOnceSinkE2eTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 172.591 s - in org.apache.flink.connector.jdbc.databases.db2.xa.Db2ExactlyOnceSinkE2eTest
[INFO] 
[INFO] Results:
[INFO] 
Error:  Failures: 
Error:    PostgresDialectTypeTest>JdbcDialectTypeTest.testDataTypeValidate:102 
Expecting code to raise a throwable.
Error:    TrinoDialectTypeTest>JdbcDialectTypeTest.testDataTypeValidate:102 
Expecting code to raise a throwable.
Error:    CrateDBDialectTypeTest>JdbcDialectTypeTest.testDataTypeValidate:102 
Expecting code to raise a throwable.
[INFO] 
Error:  Tests run: 394, Failures: 3, Errors: 0, Skipped: 1
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 12:32:52 UTC 2024,,,,,,,,,,"0|z1n7ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 05:56;wanglijie;The test passes in flink 1.18 but fails in flink 1.19. The root cause is that after FLINK-34316, the code that thrown this exception was not called. 

The exception thrown in {{JdbcDynamicTableSource.getScanRuntimeProvider}} in flink 1.18, which was omitted in FLINK-34316.

Call {{executeSql}} instead of {{sqlQuery}} should fix it. I will prepare a PR soon.;;;","19/Feb/24 06:00;wanglijie;The exception stack in 1.18:
{code:java}
java.lang.UnsupportedOperationException: Unsupported type:TIMESTAMP_LTZ(3)

	at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.createInternalConverter(AbstractJdbcRowConverter.java:187)
	at org.apache.flink.connector.jdbc.databases.trino.dialect.TrinoRowConverter.createInternalConverter(TrinoRowConverter.java:60)
	at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.createNullableInternalConverter(AbstractJdbcRowConverter.java:118)
	at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.<init>(AbstractJdbcRowConverter.java:68)
	at org.apache.flink.connector.jdbc.databases.trino.dialect.TrinoRowConverter.<init>(TrinoRowConverter.java:40)
	at org.apache.flink.connector.jdbc.databases.trino.dialect.TrinoDialect.getRowConverter(TrinoDialect.java:49)
	at org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.getScanRuntimeProvider(JdbcDynamicTableSource.java:184)
	at org.apache.flink.table.planner.connectors.DynamicSourceUtils.validateScanSource(DynamicSourceUtils.java:478)
	at org.apache.flink.table.planner.connectors.DynamicSourceUtils.prepareDynamicSource(DynamicSourceUtils.java:161)
	at org.apache.flink.table.planner.connectors.DynamicSourceUtils.convertSourceToRel(DynamicSourceUtils.java:125)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:118)
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:4002)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2872)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2432)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2346)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2291)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:728)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:714)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3848)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:618)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:229)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:205)
	at org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:69)
	at org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)
	at org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:73)
	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:272)
	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:262)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:708)
	at org.apache.flink.connector.jdbc.dialect.JdbcDialectTypeTest.testDataTypeValidate(JdbcDialectTypeTest.java:101)
{code};;;","20/Feb/24 12:32;Sergey Nuyanzin;Merged to main as [5d6bd2e12bcdd488e182c53a9f97d5685744e6a7|https://github.com/apache/flink-connector-jdbc/commit/5d6bd2e12bcdd488e182c53a9f97d5685744e6a7];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"IllegalAnnotationsException causes ""PyFlink YARN per-job on Docker test"" e2e test to fail",FLINK-34357,13567376,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,05/Feb/24 07:47,06/Feb/24 15:43,04/Jun/24 20:40,06/Feb/24 15:43,1.18.1,,,,,,,,,,,Deployment / YARN,,,,0,github-actions,test-stability,,,"https://github.com/apache/flink/actions/runs/7763815214/job/21176570116#step:14:10009

{code}
Feb 03 03:29:04 SEVERE: Failed to generate the schema for the JAX-B elements
Feb 03 03:29:04 javax.xml.bind.JAXBException
Feb 03 03:29:04  - with linked exception:
Feb 03 03:29:04 [java.lang.reflect.InvocationTargetException]
Feb 03 03:29:04 	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:262)
Feb 03 03:29:04 	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:234)
[...]
Feb 03 03:29:04 	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Feb 03 03:29:04 Caused by: java.lang.reflect.InvocationTargetException
Feb 03 03:29:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 03 03:29:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 03 03:29:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 03 03:29:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 03 03:29:04 	at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.ContextFactory.createContext(ContextFactory.java:44)
Feb 03 03:29:04 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 03 03:29:04 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 03 03:29:04 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 03 03:29:04 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 03 03:29:04 	at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:247)
Feb 03 03:29:04 	... 57 more
Feb 03 03:29:04 Caused by: com.sun.xml.internal.bind.v2.runtime.IllegalAnnotationsException: 1 counts of IllegalAnnotationExceptions
Feb 03 03:29:04 java.util.Set is an interface, and JAXB can't handle interfaces.
Feb 03 03:29:04 	this problem is related to the following location:
Feb 03 03:29:04 		at java.util.Set
Feb 03 03:29:04 		at public java.util.HashMap org.apache.hadoop.yarn.api.records.timeline.TimelineEntity.getPrimaryFiltersJAXB()
Feb 03 03:29:04 		at org.apache.hadoop.yarn.api.records.timeline.TimelineEntity
Feb 03 03:29:04 		at public java.util.List org.apache.hadoop.yarn.api.records.timeline.TimelineEntities.getEntities()
Feb 03 03:29:04 		at org.apache.hadoop.yarn.api.records.timeline.TimelineEntities
Feb 03 03:29:04 
Feb 03 03:29:04 	at com.sun.xml.internal.bind.v2.runtime.IllegalAnnotationsException$Builder.check(IllegalAnnotationsException.java:91)
Feb 03 03:29:04 	at com.sun.xml.internal.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:445)
Feb 03 03:29:04 	at com.sun.xml.internal.bind.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:277)
Feb 03 03:29:04 	at com.sun.xml.internal.bind.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:124)
Feb 03 03:29:04 	at com.sun.xml.internal.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1123)
Feb 03 03:29:04 	at com.sun.xml.internal.bind.v2.ContextFactory.createContext(ContextFactory.java:147)
Feb 03 03:29:04 	... 67 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-34359,,,FLINK-34360,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 15:43:37 UTC 2024,,,,,,,,,,"0|z1n7lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 07:57;mapohl;https://github.com/apache/flink/actions/runs/7770984519/job/21191906001#step:14:11551;;;","05/Feb/24 07:58;mapohl;{{Kerberized YARN per-job on Docker test (default input)'}} failed:
https://github.com/apache/flink/actions/runs/7770984519/job/21191937191#step:14:13497;;;","05/Feb/24 08:02;mapohl;https://github.com/apache/flink/actions/runs/7770984575/job/21191929185#step:14:13787;;;","05/Feb/24 08:04;mapohl;https://github.com/apache/flink/actions/runs/7772411876/job/21196107220#step:14:13386;;;","05/Feb/24 08:24;mapohl;https://github.com/apache/flink/actions/runs/7778642859/job/21208829515#step:14:13750;;;","05/Feb/24 08:25;mapohl;https://github.com/apache/flink/actions/runs/7778643029/job/21208793546#step:14:13376;;;","05/Feb/24 08:29;mapohl;https://github.com/apache/flink/actions/runs/7778643029/job/21208806965;;;","05/Feb/24 08:41;mapohl;https://github.com/apache/flink/actions/runs/7779306433/job/21210375865#step:14:13556;;;","05/Feb/24 08:42;mapohl;These failures are also most likely caused by the no space left issue (FLINK-34360) because the disk usage command at the end of the test reveals a high disk usage (see /dev/root/):
{code}
Environment Information
  Feb 05 04:54:26 Jps
  Feb 05 04:54:26 303871 Jps
  Feb 05 04:54:26 Disk information
  Feb 05 04:54:26 Filesystem      Size  Used Avail Use% Mounted on
  Feb 05 04:54:26 /dev/root        73G   71G  2.2G  98% /
  Feb 05 04:54:26 tmpfs           7.9G  172K  7.9G   1% /dev/shm
  Feb 05 04:54:26 tmpfs           3.2G  1.2M  3.2G   1% /run
  Feb 05 04:54:26 tmpfs           5.0M     0  5.0M   0% /run/lock
  Feb 05 04:54:26 /dev/sdb15      105M  6.1M   99M   6% /boot/efi
  Feb 05 04:54:26 /dev/sda1        74G  4.1G   66G   6% /mnt
  Feb 05 04:54:26 tmpfs           1.6G   12K  1.6G   1% /run/user/1001
{code};;;","06/Feb/24 15:43;mapohl;This test is quite likely caused by the disk space issue of FLINK-34360 because PyFlink YARN per-job on Docker test is one of the test that requires disk space for the Docker images and there for is more likely to fail due to disk space issues.;;;",,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33768  Support dynamic source parallelism inference for batch jobs ,FLINK-34356,13567350,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,JunRuiLi,lincoln.86xy,lincoln.86xy,05/Feb/24 05:13,26/Feb/24 06:00,04/Jun/24 20:40,26/Feb/24 06:00,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,release-testing,,,,"This issue aims to verify FLIP-379.

New Source can implement the interface DynamicParallelismInference to enable dynamic parallelism inference. For detailed information, please refer to the documentation.

We may need to cover the following two types of test cases:
Test 1: FileSource has implemented the dynamic source parallelism inference. Test the automatic parallelism inference of FileSource.
Test 2: Test the dynamic source parallelism inference of a custom Source.",,,,,,,,,,,,,,,,,,,,,,FLINK-34287,,,,,,,,,,,,,,FLINK-33768,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 06:00:20 UTC 2024,,,,,,,,,,"0|z1n7fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 08:42;JunRuiLi;I'd like to take this ticket. [~zhuzh] Could you help to assign this to me?;;;","21/Feb/24 09:10;zhuzh;Assigned. Thanks for volunteering! [~JunRuiLi]
;;;","26/Feb/24 05:38;JunRuiLi;I have conducted tests for both scenarios as instructed:
 # With the FileSource, when parallelism.default is greater than the number of files, the parallelism is set to the number of files. Conversely, when parallelism.default is less than the number of files, the parallelism.default value is used.
 # With a custom source that implemented the DynamicParallelismInference interface, the results indicate that the parallelism is determined by the DynamicParallelismInference interface.

All tests completed successfully. The test results are consistent with expectations.;;;","26/Feb/24 06:00;lincoln.86xy;[~JunRuiLi]Thanks for your testing!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-34054 Support named parameters for functions and procedures,FLINK-34355,13567349,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xu_shuai_,lincoln.86xy,lincoln.86xy,05/Feb/24 05:10,20/Feb/24 06:01,04/Jun/24 20:40,20/Feb/24 06:01,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,release-testing,,,,"Test suggestion:

1. Implement a test UDF or Procedure and support Named Parameters.

2. When calling a function or procedure, use named parameters to verify if the results are as expected.

You can test the following scenarios:
1. Normal usage of named parameters, fully specifying each parameter.
2. Omitting unnecessary parameters.
3. Omitting necessary parameters to confirm if an error is reported.",,,,,,,,,,,,,,,,,,,,,,FLINK-34303,,,,,,,,,,,,,,FLINK-34054,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 06:01:55 UTC 2024,,,,,,,,,,"0|z1n7fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 07:19;xu_shuai_;Hi, I'd like to take this verification. cc [~hackergin] .;;;","07/Feb/24 09:29;yunta;[~xu_shuai_] Already assigned to you, please go ahead.;;;","20/Feb/24 02:08;xu_shuai_;Hi, I have finished this verification.;;;","20/Feb/24 06:01;lincoln.86xy;[~xu_shuai_] Thanks for your testing work!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-34037 Improve Serialization Configuration and Usage in Flink,FLINK-34354,13567322,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zakelly,Zhanghao Chen,Zhanghao Chen,04/Feb/24 11:00,27/Feb/24 05:28,04/Jun/24 20:40,27/Feb/24 05:28,1.19.0,,,,,,,1.19.0,,,,API / Type Serialization System,,,,0,release-testing,,,,"This issue aims to verify [FLIP-398|https://cwiki.apache.org/confluence/display/FLINK/FLIP-398%3A+Improve+Serialization+Configuration+And+Usage+In+Flink].

Volunteers can verify it by following the [doc changes|https://github.com/apache/flink/pull/24230]. Basically, two parts need to be verfied:
 # The old way of configuring serialization via hard-code method calls still works:
 ** ExecutionConfig#registerType (for both POJO and generic types)
 ** ExecutionConfig#addDefaultKryoSerializer
 ** ExecutionConfig#registerTypeWithKryoSerializer
 # The new way of configuring serialization via config option [pipeline.serialization-config|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#pipeline-serialization-config] also works for:
 ** Regieter serializer for POJO types
 ** Register Kryo serializer for generic types
 ** Register Kryo serializer as the default Kryo serializer for types
 ** Register custom type info factories",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34037,,FLINK-34328,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 05:28:51 UTC 2024,,,,,,,,,,"0|z1n79c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/24 07:49;zakelly;Hi, if no one takes, I'd like to help on this.;;;","23/Feb/24 07:51;lincoln.86xy;[~Zakelly] Thanks for taking this! Assigned to you.;;;","25/Feb/24 03:19;Zhanghao Chen;[~Zakelly] Thanks for taking this!;;;","25/Feb/24 17:02;zakelly;Verified by:
 * Checked all the UTs.
 * Configure a streaming job using both hard-code and configuration way:
 ** {{ExecutionConfig#registerPojoType}} and {{pipeline.registered-pojo-types}} seems good.
 ** {{ExecutionConfig#disableGenericTypes}} and {{pipeline.generic-types}} seems good.
 ** {{ExecutionConfig#registerTypeWithKryoSerializer}} and {{pipeline.serialization-config}} as well as {{pipeline.default-kryo-serializers}}  are good.
** {{ExecutionConfig#addDefaultKryoSerializer}} and {{pipeline.serialization-config}} as well as {{pipeline.default-kryo-serializers}} are good.
** Typeinfo factories can be registered via {{pipeline.serialization-config}}.

+1 for release.;;;","27/Feb/24 05:28;lincoln.86xy;[~Zakelly]Thanks for your testing!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
A strange exception will be thrown if minibatch size is not set while using mini-batch join,FLINK-34353,13567316,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,04/Feb/24 09:26,05/Feb/24 03:49,04/Jun/24 20:40,05/Feb/24 03:49,1.19.0,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"{code:java}
java.lang.IllegalArgumentException: maxCount must be greater than 0
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)	at org.apache.flink.table.runtime.operators.bundle.trigger.CountCoBundleTrigger.<init>(CountCoBundleTrigger.java:34)	at org.apache.flink.table.planner.plan.utils.MinibatchUtil.createMiniBatchCoTrigger(MinibatchUtil.java:61)	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecJoin.translateToPlanInternal(StreamExecJoin.java:231)	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:168)	at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85)	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)	at scala.collection.Iterator.foreach(Iterator.scala:937)	at scala.collection.Iterator.foreach$(Iterator.scala:937)	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)	at scala.collection.IterableLike.foreach(IterableLike.scala:70)	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)	at scala.collection.TraversableLike.map(TraversableLike.scala:233)	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)	at scala.collection.AbstractTraversable.map(Traversable.scala:104)	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84)	at org.apache.flink.table.planner.delegation.PlannerBase.getExplainGraphs(PlannerBase.scala:537)	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:103)	at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:51)	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:697)	at org.apache.flink.table.api.internal.TableImpl.explain(TableImpl.java:482)	at org.apache.flink.table.api.Explainable.explain(Explainable.java:40) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34349,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 03:49:17 UTC 2024,,,,,,,,,,"0|z1n780:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 09:50;xuyangzhong;I'll try to fix it.;;;","05/Feb/24 03:49;lsy;Merged in master branch: de2d175decce8defeb7931f449392e47d637e2c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the documentation of allowNonRestoredState,FLINK-34352,13567312,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,04/Feb/24 08:47,04/Mar/24 12:38,04/Jun/24 20:40,04/Mar/24 12:38,,,,,,,,1.20.0,,,,Documentation,,,,0,pull-request-available,,,,"Current documentation of allowNonRestoredState is not clear, we should clarify:
 # It can lead to serious issues with correctness if it's used incorrectly.
 # The correctness is related to the topological order and the logic of job when removing operator by default.
 # For DataStream Job, the operator uid could be assigned explicitly to avoid the reassignment of operator uid.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 12:38:10 UTC 2024,,,,,,,,,,"0|z1n774:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/24 12:38;masteryhx;merged e168cd4d into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-33397 Support Configuring Different State TTLs using SQL Hint,FLINK-34351,13567304,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,liyubin117,qingyue,qingyue,04/Feb/24 05:43,27/Feb/24 05:41,04/Jun/24 20:40,27/Feb/24 05:41,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,release-testing,,,,"This describes how to verify FLINK-33397: Support Configuring Different State TTLs using SQL Hint.

 

The verification steps are as follows.
1. Start the standalone session cluster and sql client.
2. Execute the following DDL statements.
{code:sql}
CREATE TABLE `default_catalog`.`default_database`.`Orders` (
  `order_id` INT,
  `line_order_id` INT
) WITH (
  'connector' = 'datagen', 
  'rows-per-second' = '5'

); 

CREATE TABLE `default_catalog`.`default_database`.`LineOrders` (
  `line_order_id` INT,
  `ship_mode` STRING
) WITH (
  'connector' = 'datagen',
  'rows-per-second' = '5'
);

CREATE TABLE `default_catalog`.`default_database`.`OrdersShipInfo` (
  `order_id` INT,
  `line_order_id` INT,
  `ship_mode` STRING ) WITH (
  'connector' = 'print'
); {code}
3. Compile and verify the INSERT INTO statement with the STATE_TTL hint applied to join
{code:sql}
-- SET the pipeline level state TTL to 24h
SET 'table.exec.state.ttl' = '24h';

-- Configure different state TTL for join operator
COMPILE PLAN '/path/to/join-plan.json' FOR
INSERT INTO OrdersShipInfo 
SELECT /*+STATE_TTL('a' = '2d', 'b' = '12h')*/ a.order_id, a.line_order_id, b.ship_mode 
FROM Orders a JOIN LineOrders b ON a.line_order_id = b.line_order_id;

{code}
The generated JSON file *should* contain the following ""state"" JSON array for StreamJoin ExecNode.
{code:json}
{
    ""id"" : 5,
    ""type"" : ""stream-exec-join_1"",
    ""joinSpec"" : {
      ...
    },
    ""state"" : [ {
      ""index"" : 0,
      ""ttl"" : ""2 d"",
      ""name"" : ""leftState""
    }, {
      ""index"" : 1,
      ""ttl"" : ""12 h"",
      ""name"" : ""rightState""
    } ],
    ""inputProperties"": [...],
    ""outputType"": ...,
    ""description"": ...
}
{code}
4. Compile and verify the INSERT INTO statement with the STATE_TTL hint applied to group aggregate
{code:sql}
CREARE TABLE source_t (
    a INT,
    b BIGINT,
    c STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second' = '5'
);

CREARE TABLE sink_t (
    b BIGINT PRIMARY KEY NOT ENFORCED,
    cnt BIGINT,
    avg_a DOUBLE,
    min_c STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second' = '5'
);
COMPILE PLAN '/path/to/agg-plan.json' FOR
INSERT INTO sink_t SELECT /*+ STATE_TTL('source_t' = '1s') */
b, 
COUNT(*) AS cnt,
AVG(a) FILTER (WHERE a > 1) AS avg_a
MIN(c) AS min_c 
FROM source_t GROUP BY b

{code}
 
The generated JSON file *should* contain the following ""state"" JSON array for StreamExecGroupAggregate ExecNode.
{code:json}
""state"" : [ {
  ""index"" : 0,
  ""ttl"" : ""1 s"",
  ""name"" : ""groupAggregateState""
} ]

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/24 07:24;liyubin117;image-2024-02-21-15-24-22-289.png;https://issues.apache.org/jira/secure/attachment/13066920/image-2024-02-21-15-24-22-289.png","21/Feb/24 07:24;liyubin117;image-2024-02-21-15-24-43-212.png;https://issues.apache.org/jira/secure/attachment/13066921/image-2024-02-21-15-24-43-212.png",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 05:41:04 UTC 2024,,,,,,,,,,"0|z1n75c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 07:26;liyubin117;[~qingyue] I found some typos in the original test scripts, stage 4 should be as follows:
{code:java}
CREATE TABLE source_t (
    a INT,
    b BIGINT,
    c STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second' = '5'
);

CREATE TABLE sink_t (
    b BIGINT PRIMARY KEY NOT ENFORCED,
    cnt BIGINT,
    avg_a DOUBLE,
    min_c STRING
) WITH (
    'connector' = 'print'
);
COMPILE PLAN '/path/to/agg-plan.json' FOR
INSERT INTO sink_t SELECT /*+ STATE_TTL('source_t' = '1s') */
b, 
COUNT(*) AS cnt,
AVG(a) FILTER (WHERE a > 1) AS avg_a,
MIN(c) AS min_c 
FROM source_t GROUP BY b;{code}
After careful tests, the practical feature takes effect actually.

!image-2024-02-21-15-24-43-212.png|width=294,height=197!

!image-2024-02-21-15-24-22-289.png|width=418,height=127!;;;","27/Feb/24 05:29;lincoln.86xy;[~liyubin117] Thanks for your testing! As you posted above, the test table 'sink_t' should be a sink table, the 'datagen' type seems a typo, 'print' sink should work.  Other than that, the test itself was as expected? If so, we can close this ticket.
;;;","27/Feb/24 05:38;liyubin117;[~lincoln.86xy] There is no problem about the test :);;;","27/Feb/24 05:41;lincoln.86xy;[~liyubin117] Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hugo cannot rebuild site for zh doc,FLINK-34350,13567297,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,qingyue,qingyue,04/Feb/24 03:32,04/Feb/24 03:32,04/Jun/24 20:40,,,,,,,,,,,,,Documentation,,,,0,,,,,"For en docs, Hugo can detect changes and automatically rebuild the site.

However, for zh docs, it does not work.

From the console, we can see that Hugo does detect the changes.
{code:java}
Source changed WRITE|CHMOD   ""/Users/jane.cjm/GitHub/flink/docs/content.zh/docs/dev/table/sql/queries/window-tvf.md"" {code}
However, the changes cannot be applied to the site automatically. I have to stop and reinvoke `./build_docs.sh` to make it work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-04 03:32:06.0,,,,,,,,,,"0|z1n73s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-34219 Introduce a new join operator to support minibatch,FLINK-34349,13567296,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xuyangzhong,xu_shuai_,xu_shuai_,04/Feb/24 03:31,06/Feb/24 06:06,04/Jun/24 20:40,06/Feb/24 05:49,1.19.0,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,release-testing,,,,"Minibatch join is ready. Users could improve performance in regular stream join scenarios. 

Someone can verify this feature by following the [doc]([https://github.com/apache/flink/pull/24240)] although it is still being reviewed.

If someone finds some bugs about this feature, you open a Jira linked this one to report them.",,,,,,,,,,,,,,,,,,,,,,FLINK-34304,,,,,,,,,,,,,,FLINK-34353,FLINK-34378,FLINK-34380,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 05:49:12 UTC 2024,,,,,,,,,,"0|z1n73k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 07:36;xuyangzhong;Hi, I'd like to take this verification.;;;","05/Feb/24 05:22;lincoln.86xy;[~xuyangzhong] Thank you for volunteering! Assigned to you.;;;","06/Feb/24 05:43;xuyangzhong;Hi, I have finished this verification. Some unexpected behaviors such as bug and tech doubt have been linked to this jira.;;;","06/Feb/24 05:49;lincoln.86xy;[~xuyangzhong] Thanks for your test work and quick fixing! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-20281 Window aggregation supports changelog stream input,FLINK-34348,13567295,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hackergin,xuyangzhong,xuyangzhong,04/Feb/24 03:28,20/Feb/24 07:37,04/Jun/24 20:40,20/Feb/24 07:37,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,release-testing,,,,"Window TVF aggregation supports changelog stream  is ready for testing. User can add a window tvf aggregation as a down stream after CDC source or some nodes that will produce cdc records.

Someone can verify this feature with:
 # Prepare a mysql table, and insert some data at first.
 # Start sql-client and prepare ddl for this mysql table as a cdc source.
 # You can verify the plan by `EXPLAIN PLAN_ADVICE` to check if there is a window aggregate node and the changelog contains ""UA"" or ""UB"" or ""D"" in its upstream. 
 # Use different kinds of window tvf to test window tvf aggregation while updating the source data to check the data correctness.",,,,,,,,,,,,,,,,,,,,,,FLINK-34301,,,,,,,,,,,,,,,,,,,,"07/Feb/24 09:06;hackergin;截屏2024-02-07 16.21.37.png;https://issues.apache.org/jira/secure/attachment/13066547/%E6%88%AA%E5%B1%8F2024-02-07+16.21.37.png","07/Feb/24 09:06;hackergin;截屏2024-02-07 16.21.55.png;https://issues.apache.org/jira/secure/attachment/13066548/%E6%88%AA%E5%B1%8F2024-02-07+16.21.55.png","07/Feb/24 09:07;hackergin;截屏2024-02-07 16.22.24.png;https://issues.apache.org/jira/secure/attachment/13066549/%E6%88%AA%E5%B1%8F2024-02-07+16.22.24.png","07/Feb/24 09:08;hackergin;截屏2024-02-07 16.23.12.png;https://issues.apache.org/jira/secure/attachment/13066550/%E6%88%AA%E5%B1%8F2024-02-07+16.23.12.png","07/Feb/24 09:08;hackergin;截屏2024-02-07 16.23.27.png;https://issues.apache.org/jira/secure/attachment/13066551/%E6%88%AA%E5%B1%8F2024-02-07+16.23.27.png","07/Feb/24 09:09;hackergin;截屏2024-02-07 16.23.38.png;https://issues.apache.org/jira/secure/attachment/13066552/%E6%88%AA%E5%B1%8F2024-02-07+16.23.38.png","07/Feb/24 09:11;hackergin;截屏2024-02-07 16.29.09.png;https://issues.apache.org/jira/secure/attachment/13066553/%E6%88%AA%E5%B1%8F2024-02-07+16.29.09.png","07/Feb/24 09:11;hackergin;截屏2024-02-07 16.29.21.png;https://issues.apache.org/jira/secure/attachment/13066554/%E6%88%AA%E5%B1%8F2024-02-07+16.29.21.png","07/Feb/24 09:11;hackergin;截屏2024-02-07 16.29.34.png;https://issues.apache.org/jira/secure/attachment/13066555/%E6%88%AA%E5%B1%8F2024-02-07+16.29.34.png","07/Feb/24 09:15;hackergin;截屏2024-02-07 16.46.12.png;https://issues.apache.org/jira/secure/attachment/13066556/%E6%88%AA%E5%B1%8F2024-02-07+16.46.12.png","07/Feb/24 09:15;hackergin;截屏2024-02-07 16.46.23.png;https://issues.apache.org/jira/secure/attachment/13066557/%E6%88%AA%E5%B1%8F2024-02-07+16.46.23.png","07/Feb/24 09:15;hackergin;截屏2024-02-07 16.46.37.png;https://issues.apache.org/jira/secure/attachment/13066558/%E6%88%AA%E5%B1%8F2024-02-07+16.46.37.png","07/Feb/24 09:17;hackergin;截屏2024-02-07 16.53.37.png;https://issues.apache.org/jira/secure/attachment/13066559/%E6%88%AA%E5%B1%8F2024-02-07+16.53.37.png","07/Feb/24 09:17;hackergin;截屏2024-02-07 16.53.47.png;https://issues.apache.org/jira/secure/attachment/13066560/%E6%88%AA%E5%B1%8F2024-02-07+16.53.47.png","07/Feb/24 09:17;hackergin;截屏2024-02-07 16.54.01.png;https://issues.apache.org/jira/secure/attachment/13066561/%E6%88%AA%E5%B1%8F2024-02-07+16.54.01.png","07/Feb/24 09:18;hackergin;截屏2024-02-07 16.59.22.png;https://issues.apache.org/jira/secure/attachment/13066562/%E6%88%AA%E5%B1%8F2024-02-07+16.59.22.png","07/Feb/24 09:18;hackergin;截屏2024-02-07 16.59.33.png;https://issues.apache.org/jira/secure/attachment/13066563/%E6%88%AA%E5%B1%8F2024-02-07+16.59.33.png","07/Feb/24 09:18;hackergin;截屏2024-02-07 16.59.42.png;https://issues.apache.org/jira/secure/attachment/13066564/%E6%88%AA%E5%B1%8F2024-02-07+16.59.42.png",,18.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 07:37:49 UTC 2024,,,,,,,,,,"0|z1n73c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 09:27;hackergin;I'd like to verify this feature. ;;;","05/Feb/24 05:21;lincoln.86xy;[~hackergin] Thank you for volunteering! Assigned to you.;;;","07/Feb/24 09:20;hackergin;{color:#000000}I have completed the test, and overall it meets expectations.{color}

 
h2. *The following is the main process of my test*

1. create a CDC source (here I use the paimon table), 
2. Execute explain plan
3. Write relevant data to confirm if the results meet expectations.
h2. Create The CDC Source

!截屏2024-02-07 16.21.37.png!

!截屏2024-02-07 16.21.55.png!
h2. Execute explain plan

*TUMBLE*

!截屏2024-02-07 16.22.24.png!

*SESSION*

!截屏2024-02-07 16.23.12.png!

*HOP*

!截屏2024-02-07 16.23.27.png!

*CUMULATE*

!截屏2024-02-07 16.23.38.png!
h2. Verify the result

*TUMBLE*

Insert the data and the result as follows.
{code:sql}
SELECT window_start, window_end, SUM(cnt) FROM TABLE(   TUMBLE(TABLE test_source, DESCRIPTOR(`timestamp`), INTERVAL '10' MINUTES)) GROUP BY
 window_start, window_end;
{code}
!截屏2024-02-07 16.29.09.png!

!截屏2024-02-07 16.29.21.png!

!截屏2024-02-07 16.29.34.png!
 * SESSION *

Insert the data and the result as follows.
{code:sql}
SELECT window_start, window_end, SUM(cnt) FROM TABLE(SESSION(TABLE test_source, DESCRIPTOR(`timestamp`), INTERVAL '10' MINUTES)) GROUP BY
 window_start, window_end;
{code}
!截屏2024-02-07 16.46.12.png! 
!截屏2024-02-07 16.46.23.png! 
!截屏2024-02-07 16.46.37.png!
 * HOP *

Insert the data and the result as follows.
{code:sql}
 SELECT window_start, window_end, SUM(cnt) FROM TABLE( HOP(TABLE test_source, DESCRIPTOR(`timestamp`), INTERVAL '10' MINUTES, INTERVAL '20'
MINUTES)) GROUP BY window_start, window_end;
{code}
!截屏2024-02-07 16.53.37.png! 
!截屏2024-02-07 16.53.47.png! 
!截屏2024-02-07 16.54.01.png!
 * CUMULATE *

Insert the data and the result as follows.
{code:sql}
SELECT window_start, window_end, SUM(cnt) FROM TABLE(CUMULATE(TABLE test_source, DESCRIPTOR(`timestamp`), INTERVAL '10' MINUTES, INTERVAL '20' MINUTES)) GROUP BY window_start, window_end;
{code}
!截屏2024-02-07 16.59.22.png! 
!截屏2024-02-07 16.59.33.png! 
!截屏2024-02-07 16.59.42.png!;;;","07/Feb/24 09:21;hackergin;[~xuyangzhong]  {color:#000000}Please help confirm the overall process and let me know if there are any areas I have missed.{color};;;","08/Feb/24 02:30;xuyangzhong;Hi, [~hackergin]. Thanks for your detailed testing. Overall, it seems that nothing is missing, regarding the plan test and IT test. ;;;","20/Feb/24 06:28;hackergin;cc [~lincoln.86xy]  The test has been completed as above.;;;","20/Feb/24 07:37;lincoln.86xy;[~hackergin] Thanks for your testing work!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes native resource manager request wrong spec.,FLINK-34347,13567294,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ruibin,ruibin,04/Feb/24 03:28,04/Feb/24 03:28,04/Jun/24 20:40,,1.18.0,kubernetes-operator-1.6.1,,,,,,,,,,Deployment / Kubernetes,Kubernetes Operator,,,0,,,,,"We had a flink spec in which TM cpu is set to 0.5, then we upgraded it to 4.0. We found the job manager requesting both TM with 0.5 CPU and 4 CPU. Most TMs with 0.5 CPU was released soon, however there was 1 TM with 0.5 CPU remained and caused lag in job.

 

Logs for mixed TM requests:
{code:java}
2024-02-03 10:10:41,414 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Requested worker octopus-16-323-octopus-engine-write-proxy-taskmanager-3-244 with resource spec WorkerResourceSpec {cpuCores=4.0, taskHeapSize=5.637gb (6053219520 bytes), taskOffHeapSize=1024.000mb (1073741824 bytes), networkMemSize=64.000mb (67108864 bytes), managedMemSize=0 bytes, numSlots=4}.02-03 18:10:44.8442024-02-03 10:10:44,844 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Requesting new worker with resource spec WorkerResourceSpec {cpuCores=0.5, taskHeapSize=1.137gb (1221381320 bytes), taskOffHeapSize=1024.000mb (1073741824 bytes), networkMemSize=64.000mb (67108864 bytes), managedMemSize=0 bytes, numSlots=4}, current pending count: 1.02-03 18:10:44.9202024-02-03 10:10:44,920 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Requesting new worker with resource spec WorkerResourceSpec {cpuCores=0.5, taskHeapSize=1.137gb (1221381320 bytes), taskOffHeapSize=1024.000mb (1073741824 bytes), networkMemSize=64.000mb (67108864 bytes), managedMemSize=0 bytes, numSlots=4}, current pending count: 2.02-03 18:10:44.942 {code}
The name of wrong TM: octopus-16-323-octopus-engine-write-proxy-taskmanager-3-326.

Relevant logs are attached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/24 03:28;ruibin;jobmanager.csv;https://issues.apache.org/jira/secure/attachment/13066451/jobmanager.csv","04/Feb/24 03:28;ruibin;taskmanager_octopus-16-323-octopus-engine-write-proxy-taskmanager-3-326.csv;https://issues.apache.org/jira/secure/attachment/13066450/taskmanager_octopus-16-323-octopus-engine-write-proxy-taskmanager-3-326.csv",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-04 03:28:27.0,,,,,,,,,,"0|z1n734:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-24024 Support session Window TVF,FLINK-34346,13567293,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xu_shuai_,xuyangzhong,xuyangzhong,04/Feb/24 03:25,20/Feb/24 06:00,04/Jun/24 20:40,20/Feb/24 06:00,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,release-testing,,,,"Session window TVF is ready. Users can use Session window TVF aggregation instead of using legacy session group window aggregation.

Someone can verify this feature by following the [doc]([https://github.com/apache/flink/pull/24250]) although it is still being reviewed. 

Further more,  although session window join, session window rank and session window deduplicate are in experimental state, If someone finds some bugs about them, you could also open a Jira linked this one to report them.",,,,,,,,,,,,,,,,,,,,,,FLINK-34300,,,,,,,,,,,,,,FLINK-34462,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 06:00:32 UTC 2024,,,,,,,,,,"0|z1n72w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 03:41;lincoln.86xy;[~xu_shuai_] Assigned to you since you comment in https://issues.apache.org/jira/browse/FLINK-34300 that you are willing to take on testing.;;;","20/Feb/24 02:10;xu_shuai_;Hi, I have finished this testing. The exception I think could be improved has been linked to this jira.;;;","20/Feb/24 06:00;lincoln.86xy;[~xu_shuai_] Thanks for testing this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove TaskExecutorManager related logic,FLINK-34345,13567261,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,caicancai,caicancai,caicancai,03/Feb/24 06:23,07/Feb/24 06:07,04/Jun/24 20:40,07/Feb/24 06:07,1.19.0,,,,,,,1.20.0,,,,,,,,0,pull-request-available,,,,"FLINK-31449 removed DeclarativeSlotManager related logic. Some related classes should be removed as well after FLINK-31449, such as:
 * TaskExecutorManager
 * TaskExecutorManagerBuilder
 * TaskExecutorManagerTest
 * {{org.apache.flink.runtime.resourcemanager.slotmanager.TaskManagerRegistration}}
 * PendingTaskManagerSlot
 * TaskManagerSlotId
 * SlotStatusUpdateListener
 * TestingTaskManagerSlotInformation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 06:07:27 UTC 2024,,,,,,,,,,"0|z1n6vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 06:07;fanrui;Merged to master(1.20) via: f1fba33d85a802b896170ff3cdb0107ee082c44a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong JobID in CheckpointStatsTracker,FLINK-34344,13567207,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,roman,roman,roman,02/Feb/24 14:55,12/Feb/24 13:31,04/Jun/24 20:40,12/Feb/24 13:31,1.18.1,1.19.0,1.20.0,,,,,1.18.2,1.19.0,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"The job id is generated randomly:
```
    public CheckpointStatsTracker(int numRememberedCheckpoints, MetricGroup metricGroup) {
        this(numRememberedCheckpoints, metricGroup, new JobID(), Integer.MAX_VALUE);
    }
```
This affects how it is logged (or reported elsewhere).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 12 13:31:26 UTC 2024,,,,,,,,,,"0|z1n6js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/24 13:31;chesnay;master: c84f42c1a7e752eaf8b9c3beb23fb9b01d39443d
1.19: 37756561d99ff73ba8cbf445c57f57fe11250867
1.18: 33fb37aac5fbc709a62d35445879c75a6ba48086;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceManager registration is not completed when registering the JobMaster,FLINK-34343,13567177,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,mapohl,mapohl,02/Feb/24 11:25,05/Feb/24 11:46,04/Jun/24 20:40,05/Feb/24 11:46,1.17.2,1.18.1,1.19.0,,,,,1.17.3,1.18.2,1.19.0,,Runtime / Coordination,Runtime / RPC,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57203&view=logs&j=64debf87-ecdb-5aef-788d-8720d341b5cb&t=2302fb98-0839-5df2-3354-bbae636f81a7&l=8066

The test run failed due to a NullPointerException:
{code}
Feb 02 01:11:55 2024-02-02 01:11:47,791 INFO  org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor       [] - The rpc endpoint org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager has not been started yet. Discarding message LocalFencedMessage(0000000000000000000
0000000000000, LocalRpcInvocation(ResourceManagerGateway.registerJobMaster(JobMasterId, ResourceID, String, JobID, Time))) until processing is started.
Feb 02 01:11:55 2024-02-02 01:11:47,797 WARN  org.apache.flink.runtime.rpc.pekko.SupervisorActor           [] - RpcActor pekko://flink/user/rpc/resourcemanager_2 has failed. Shutting it down now.
Feb 02 01:11:55 java.lang.NullPointerException: Cannot invoke ""org.apache.flink.runtime.rpc.RpcServer.getAddress()"" because ""this.rpcServer"" is null
Feb 02 01:11:55         at org.apache.flink.runtime.rpc.RpcEndpoint.getAddress(RpcEndpoint.java:322) ~[flink-dist-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:182) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) ~[flink-rpc-akka06a9bb81-2e68-483a-b236-a283d0b1d097.jar:1.19-SNAPSHOT]
Feb 02 01:11:55         at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) ~[?:?]
Feb 02 01:11:55         at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) ~[?:?]
Feb 02 01:11:55         at java.util.concurrent.ForkJoinPool.scan(Unknown Source) ~[?:?]
Feb 02 01:11:55         at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) ~[?:?]
Feb 02 01:11:55         at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) ~[?:?]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34007,,,"02/Feb/24 11:36;mapohl;FLINK-34343_k8s_application_cluster_e2e_test.log;https://issues.apache.org/jira/secure/attachment/13066432/FLINK-34343_k8s_application_cluster_e2e_test.log",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 11:46:33 UTC 2024,,,,,,,,,,"0|z1n6d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 11:28;mapohl;Initially, I thought that it would be caused by the FLINK-34007 changes. But that's not the case. The ""Run kubernetes application test"" doesn't rely HA but uses {{StandaloneLeaderElection}} (sessionId: {{00000000000000000000000000000000}}).;;;","02/Feb/24 11:36;mapohl;I attached the logs of the corresponding failure to this issue.;;;","02/Feb/24 11:39;mapohl;[~chesnay] do you have capacity to look at that one? I'm still investigating the FLINK-34007 test instability.;;;","03/Feb/24 10:18;chesnay;[~mapohl] and I looked at this together and concluded that his is one hell of a race condition. There's a short window where the underlying infrastructure for _receiving_ messages already exists, without the rpcServer field being set yet in the RpcEndpoint.
While these messages are correctly rejected by the RpcActor before being passed to the RpcEndpoint (as it hasn't started yet), for logging purposes we access the RpcEndpoint's address; but that one is contingent of the rpcServer field being set.
In the end it boils down to accesses to the RpcEndpoint while it is still being set up.

An easy fix is to just not use the address but the actor path.;;;","05/Feb/24 11:46;chesnay;master: 839f298c3838f2f4981e271554b82fae770747d8
1.18: ee9945cd785577d0f68092823b71abbb53d127f8
1.17: 3f22b6363e6cad4352821f42907ec8a2a181e675;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Address Shard Consistency Issue for DDB Streams Source,FLINK-34342,13567164,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dannycranmer,dannycranmer,02/Feb/24 09:55,02/Feb/24 10:13,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / DynamoDB,,,,0,,,,,"*Problem*

We call [DescribeStream|https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_streams_DescribeStream.html] with the ExclusiveStartShardId parameter set to the last seen shard ID. The issue is that the API is eventually consistent, meaning if we call the API multiple times we might get different results, for example: 
 * Call 1: [A, C]
 * Call 2: [A, B, C]

Since we would set ExclusiveStartShardId to {{{}C{}}}, in the above example the connector would miss shard {{B}}

*Solution*

We need to find a solution to support this gap. This could be to periodically list all shards to find gaps and not start processing new shards until their parents are complete. This feature does not need to be applied to the KDS source.",,,,,,,,,,,,FLINK-34340,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-02 09:55:28.0,,,,,,,,,,"0|z1n6a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement DDB Streams Table API support,FLINK-34341,13567162,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dannycranmer,dannycranmer,02/Feb/24 09:48,02/Feb/24 10:13,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Kinesis,,,,0,pull-request-available,,,,"Implement Table API support for DDB Streams Source.

 

Consider:
 * Configurations to support. Should have customisation parity with DataStream API
 * Testing should include both SQL client + Table API via Java",,,,,,,,,,,,FLINK-34340,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-02 09:48:02.0,,,,,,,,,,"0|z1n69s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for DDB Streams for DataStream API,FLINK-34340,13567158,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dannycranmer,dannycranmer,02/Feb/24 09:27,02/Feb/24 10:13,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / DynamoDB,,,,0,,,,,"In the legacy KDS source we support Amazon DynamoDB streams via an adapter shim. Both KDS and DDB streams have a similar API.

This task builds upon https://issues.apache.org/jira/browse/FLINK-34339 and will add a {{DynamoDBStreamsSource}} which will setup a DDB SDK client shim.",,,,,,,,,,,,FLINK-34339,FLINK-34342,FLINK-34341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-02 09:27:28.0,,,,,,,,,,"0|z1n68w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add connector abstraction layer to remove reliance on AWS SDK classes,FLINK-34339,13567157,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dannycranmer,dannycranmer,02/Feb/24 09:24,02/Feb/24 10:13,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Kinesis,,,,0,,,,,"In order to shim DDB streams we need to be able to support the Stream/Shard/Record etc concepts without tying to a specific implementation. This will allow us to mimic the KDS/DDB streams support in the old connector by providing a shim at the AWS SDK client.
 # Model {{software.amazon.awssdk.services.kinesis}} classes as native concepts
 # Push down any usage of {{software.amazon.awssdk.services.kinesis}} to a KDS specific class
 # Ensure that the bulk of the connector logic is reusable, the top level class would be implementation specific and shim in the write client factories and configuration",,,,,,,,,,,,FLINK-24438,FLINK-31922,FLINK-33180,FLINK-31980,FLINK-32324,FLINK-34340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-02 09:24:51.0,,,,,,,,,,"0|z1n68o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An exception is thrown when some named params change order when using window tvf,FLINK-34338,13567134,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,02/Feb/24 06:28,01/Apr/24 19:05,04/Jun/24 20:40,,1.15.0,,,,,,,1.20.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"This bug can be reproduced by the following sql in `WindowTableFunctionTest`

 
{code:java}
@Test
def test(): Unit = {
  val sql =
    """"""
      |SELECT *
      |FROM TABLE(TUMBLE(
      |   DATA => TABLE MyTable,
      |   SIZE => INTERVAL '15' MINUTE,
      |   TIMECOL => DESCRIPTOR(rowtime)
      |   ))
      |"""""".stripMargin
  util.verifyRelPlan(sql)
}{code}
In Flip-145 and user doc, we can found `the DATA param must be the first`, but it seems that we also can't change the order about other params.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-02 06:28:07.0,,,,,,,,,,"0|z1n63k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink.InitContextWrapper should implement metadataConsumer method,FLINK-34337,13567128,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,02/Feb/24 05:49,06/Feb/24 19:20,04/Jun/24 20:40,06/Feb/24 19:20,1.19.0,,,,,,,1.19.0,,,,API / Core,,,,0,pull-request-available,,,,"Sink.InitContextWrapper should implement metadataConsumer method.

If the metadataConsumer method is not implemented, the behavior of the wrapped WriterInitContext's metadataConsumer will be lost.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34192,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 19:20:05 UTC 2024,,,,,,,,,,"0|z1n628:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 19:20;martijnvisser;Fixed in apache/flink:master 03b4584422826d2819d571871dfef4efced19f01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutoRescalingITCase#testCheckpointRescalingWithKeyedAndNonPartitionedState may hang sometimes,FLINK-34336,13567123,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,02/Feb/24 05:00,20/Feb/24 15:00,04/Jun/24 20:40,20/Feb/24 14:22,1.19.0,1.20.0,,,,,,1.19.0,,,,Tests,,,,0,pull-request-available,test-stability,,,"AutoRescalingITCase#testCheckpointRescalingWithKeyedAndNonPartitionedState may hang in waitForRunningTasks({color:#9876aa}restClusterClient{color}{color:#cc7832}, {color}jobID{color:#cc7832}, {color}parallelism2){color:#cc7832};{color}
h2. Reason:

The job has 2 tasks(vertices), after calling updateJobResourceRequirements. The source parallelism isn't changed (It's parallelism) , and the FlatMapper+Sink is changed from  parallelism to parallelism2.

So we expect the task number should be parallelism + parallelism2 instead of parallelism2.

 
h2. Why it can be passed for now?

Flink 1.19 supports the scaling cooldown, and the cooldown time is 30s by default. It means, flink job will rescale job 30 seconds after updateJobResourceRequirements is called.

 

So the running tasks are old parallelism when we call waitForRunningTasks({color:#9876aa}restClusterClient{color}{color:#cc7832}, {color}jobID{color:#cc7832}, {color}parallelism2){color:#cc7832};. {color}

IIUC, it cannot be guaranteed, and it's unexpected.

 
h2. How to reproduce this bug?

[https://github.com/1996fanrui/flink/commit/ffd713e24d37db2c103e4cd4361d0cd916d0d2f6]
 * Disable the cooldown
 * Sleep for a while before waitForRunningTasks

If so, the job running in new parallelism, so `waitForRunningTasks` will hang forever.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 07:47:30 UTC 2024,,,,,,,,,,"0|z1n614:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/24 09:57;mapohl;* https://github.com/apache/flink/actions/runs/7895502334/job/21548185872#step:10:10193
* https://github.com/apache/flink/actions/runs/7895502334/job/21548208160#step:10:11190
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57518&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=15356;;;","19/Feb/24 09:21;mapohl;* [https://github.com/apache/flink/actions/runs/7938595320/job/21677775512#step:10:11191]
 * [https://github.com/apache/flink/actions/runs/7938595320/job/21677768481#step:10:8721];;;","19/Feb/24 09:34;fanrui;Hi [~mapohl] , thanks for your report these failed CI.

What do you think about the [fix PR|https://github.com/apache/flink/pull/24248]?;;;","19/Feb/24 09:47;mapohl;Yes, I postponed looking into the PR. I will do it today. (y);;;","20/Feb/24 02:45;fanrui;Merged to

master(1.20) via: e2e3de2d48e3f02b746bdbdcb4da7b0477986a11

1.19 via: c91029b0456ddd7635fe0f04ad9ecb4813c5d5a3;;;","20/Feb/24 07:41;mapohl;Thanks [~fanrui]. Can you also create a 1.19 backport?

And just as a hint: The fix version would be only 1.19.0 up to the point where the 1.19.0 release actually happened (even if the change also ended up in {{{}master{}}}). Any change that is backported to 1.19 right now is still considered a 1.19.0 (and not a 1.20.0 fix).;;;","20/Feb/24 07:47;fanrui;Hi [~mapohl] , thanks for your reminder. I have submitted a PR to backport it to 1.19: [https://github.com/apache/flink/pull/24340]

Please correct me if I misunderstand.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Query hints in RexSubQuery could not be printed,FLINK-34335,13567120,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,02/Feb/24 03:00,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,"That is because in `RelTreeWriterImpl`, we don't care about the `RexSubQuery`. And `RexSubQuery` use `RelOptUtil.toString(rel)` to print itself instead of adding extra information such as query hints.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 03:01:09 UTC 2024,,,,,,,,,,"0|z1n60g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 03:01;xuyangzhong;I'll try to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sub-task level RocksDB file count metric,FLINK-34334,13567026,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hejufang001,hejufang001,hejufang001,01/Feb/24 12:08,18/Mar/24 02:23,04/Jun/24 20:40,18/Mar/24 02:23,1.18.0,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"In our production environment, we encountered the problem of task deploy failure. The root cause was that too many sst files of a single sub-task led to too much task deployment information(OperatorSubtaskState), and then caused akka request timeout in the task deploy phase. Therefore, I wanted to add sub-task level RocksDB file count metrics. It is convenient to avoid performance problems caused by too many sst files in time.

RocksDB has provided the JNI (https://javadoc.io/doc/org.rocksdb/rocksdbjni/6.20.3/org/rocksdb/RocksDB.html#getColumnFamilyMetaData ()) We can easily retrieve the file count and report it via metrics reporter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/24 12:07;hejufang001;img_v3_027i_7ed0b8ba-3f12-48eb-aab3-cc368ac47cdg.jpg;https://issues.apache.org/jira/secure/attachment/13066392/img_v3_027i_7ed0b8ba-3f12-48eb-aab3-cc368ac47cdg.jpg",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 02:23:38 UTC 2024,,,,,,,,,,"0|z1n5fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 03:51;hejufang001;[~masteryhx] could you take a look at this? ;;;","06/Feb/24 07:36;masteryhx;I think we could add such a metric.

But I'd suggest to report it by RocksDB Property just like other metrics: [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#rocksdb-native-metrics]

Since RocksDB has supported some related metrics, e.g. num-files-at-level.;;;","06/Feb/24 12:27;hejufang001;[~masteryhx] Thanks for your advice, I intend to get the number of files per level by the RocksDB Property, and report the number of files per level separately through metrics reporter. Those metrics can be enabled by a configuration, such as: state.backend.rocksdb.metrics.num-files-at-level.
can you assign this to me?;;;","18/Feb/24 04:06;hejufang001;cc [~pnowojski] ;;;","18/Mar/24 02:23;masteryhx;Merged [{{0921968}}|https://github.com/apache/flink/commit/0921968bb7d38bfac0b7899ec974a9744a721b22] into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix FLINK-34007 LeaderElector bug in 1.18,FLINK-34333,13567014,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,01/Feb/24 09:42,24/May/24 16:01,04/Jun/24 20:40,13/Feb/24 16:20,1.18.1,,,,,,,1.18.2,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"FLINK-34007 revealed a bug in the k8s client v6.6.2 which we're using since Flink 1.18. This issue was fixed with FLINK-34007 for Flink 1.19 which required an update of the k8s client to v6.9.0.

This Jira issue is about finding a solution in Flink 1.18 for the very same problem FLINK-34007 covered. It's a dedicated Jira issue because we want to unblock the release of 1.19 by resolving FLINK-34007.

Just to summarize why the upgrade to v6.9.0 is desired: There's a bug in v6.6.2 which might prevent the leadership lost event being forwarded to the client ([#5463|https://github.com/fabric8io/kubernetes-client/issues/5463]). An initial proposal where the release call was handled in Flink's {{KubernetesLeaderElector}} didn't work due to the leadership lost event being triggered twice (see [FLINK-34007 PR comment|https://github.com/apache/flink/pull/24132#discussion_r1467175902])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34007,FLINK-31997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 16:01:09 UTC 2024,,,,,,,,,,"0|z1n5cw:",9223372036854775807,Fixes a bug where the leader election wasn't able to pick up leadership again after renewing the lease token caused a leadership loss. This required fabric8io:kubernetes-client to be upgraded from v6.6.2 to v6.9.0.,,,,,,,,,,,,,,,,,,,"01/Feb/24 10:50;mapohl;The reason I was hesitating to backport the FLINK-34007 and upgrade the k8s client to v6.9.0 to Flink 1.18  was the size of the [release notes diff between v6.6.2 and v6.9.0|https://github.com/fabric8io/kubernetes-client/compare/v6.6.2...v6.9.0]. It's hard to overlook what other (breaking; according to the [6.x.0 release notes summaries|https://github.com/fabric8io/kubernetes-client/releases]) changes are introduced that might affect Flink's stability.;;;","01/Feb/24 12:47;mapohl;Breaking changes between v6.6.2 and v6.9.2 based on the {{fabric8io:kubernetes-client}} release notes.
 * Icons:
 ** (x) Change on the Flink side required. This doesn't come with a change of behavior for the user (because it only affects test code)
 ** (!) Not used by Flink but might cause issues with projects which rely on the Flink dependency
 ** [(/)|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.8.0] Deprecation work that doesn't remove API, yet.

 
 * (x) [v6.9.0|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.9.0]
 ** (x) Fix [#5368|https://github.com/fabric8io/kubernetes-client/issues/5368]: ListOptions parameter ordering is now alphabetical. If you are using non-crud mocking for lists with options, you may need to update your parameter order.
 *** This issue causes a change in Flink's {{KubernetesClientTestBase}} because the order in which the GET parameters are processed to match the mocked requests changed.
 ** (!) Fix [#5343|https://github.com/fabric8io/kubernetes-client/issues/5343]: Removed io.fabric8.kubernetes.model.annotation.PrinterColumn, use io.fabric8.crd.generator.annotation.PrinterColumn
 ** (!) Fix [#5391|https://github.com/fabric8io/kubernetes-client/issues/5391]: Removed the vertx-uri-template dependency from the vertx client, if you need that for your application, then introduce your own dependency.
 ** [(/)|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.8.0] Fix [#5220|https://github.com/fabric8io/kubernetes-client/issues/5220]: KubernetesResourceUtil.isValidLabelOrAnnotation has been deprecated because the rules for labels and annotations are different
 * (!) [v6.8.0|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.8.0]
 ** [(/)|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.8.0] Fix [#2718|https://github.com/fabric8io/kubernetes-client/issues/2718]: KubernetesResourceUtil.isResourceReady was deprecated. Use client.resource(item).isReady() or Readiness.getInstance().isReady(item) instead.
 ** (!) Fix [#5171|https://github.com/fabric8io/kubernetes-client/issues/5171]: Removed Camel-K extension, use org.apache.camel.k:camel-k-crds instead.
 ** (!) Fix [#5262|https://github.com/fabric8io/kubernetes-client/issues/5262]: Built-in resources were in-consistent with respect to their serialization or empty collections. In many circumstances this was confusing behavior. In order to be consistent all built-in resources will omit empty collections by default. This is a breaking change if you are relying on an empty collection in a json merge or a strategic merge where the list has a patchStrategy of atomic. In these circumstances the empty collection will no longer be serialized. You may instead use a json patch, server side apply instead, or modify the serialized form of the patch.
 ** (!) Fix [#5279|https://github.com/fabric8io/kubernetes-client/issues/5279]: (java-generator) Add native support for date-time fields, they are now mapped to native java.time.ZonedDateTime
 ** (!) Fix [#5315|https://github.com/fabric8io/kubernetes-client/issues/5315]: kubernetes-junit-jupiter no longer registers the NamespaceExtension and KubernetesExtension extensions to be used in combination with junit-platform.properties>junit.jupiter.extensions.autodetection.enabled=trueconfiguration. If you wish to use these extensions and autodetect them, change your dependency to kubernetes-junit-jupiter-autodetect.
 ** [(/)|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.8.0] Deprecating io.fabric8.kubernetes.model.annotation.PrinterColumn in favor of: io.fabric8.crd.generator.annotation.PrinterColumn
 ** (!) Resource classes in resource.k8s.io/v1alpha1 have been moved to resource.k8s.io/v1alpha2 apiGroup in Kubernetes 1.27. Users are required to change package of the following classes:
 *** io.fabric8.kubernetes.api.model.resource.v1alpha1.PodSchedulingContext -> - io.fabric8.kubernetes.api.model.resource.v1alpha2.PodSchedulingContext
 *** io.fabric8.kubernetes.api.model.resource.v1alpha1.ResourceClaim -> - io.fabric8.kubernetes.api.model.resource.v1alpha2.ResourceClaim
 *** io.fabric8.kubernetes.api.model.resource.v1alpha1.ResourceClaimTemplate -> io.fabric8.kubernetes.api.model.resource.v1alpha2.ResourceClaimTemplate
 *** io.fabric8.kubernetes.api.model.resource.v1alpha1.ResourceClass -> io.fabric8.kubernetes.api.model.resource.v1alpha2.ResourceClass
 * (!) [v6.7.0|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.7.0]
 ** (!) Fix [#4911|https://github.com/fabric8io/kubernetes-client/issues/4911]:
 *** Config/RequestConfig.scaleTimeout has been deprecated along with Scalable.scale(count, wait) and DeployableScalableResource.deployLatest(wait). withTimeout may be called before the operation to control the timeout.
 *** Config/RequestConfig.websocketTimeout has been removed. Config/RequestConfig.requestTimeout will be used for websocket connection timeouts.
 *** HttpClient api/building changes - writeTimeout has been removed, readTimeout has moved to the HttpRequest
 ** (!) Fix [#4662|https://github.com/fabric8io/kubernetes-client/issues/4662]:
 *** removed deprecated classes/methods: ReflectUtils, ReplaceValueStream, ParameterNamespaceListVisitFromServerGetDeleteRecreateWaitApplicable, ResourceCompare, and Serialization methods taking parameters
 *** deprecated serialization static logic: several IOHelpers methods, Serialization methods, such as access to the static jsonMapper. Please use KubernetesSerialization methods instead.
 *** deprecated Helper.getAnnotationValue, use HasMetadata methods instead.
 ** (!) Fix [#5125|https://github.com/fabric8io/kubernetes-client/issues/5125]:
 *** support for TLSv1.3 is now enabled by default
 *** usage of TlsVersion.TLS_1_1, TLS_1_0, and SSL_3_0 have been deprecated
 ** (!) Fix [#1335|https://github.com/fabric8io/kubernetes-client/issues/1335]: The JDK and OkHttp clients will default to using the VM's standard configuration for proxies if an applicable proxy configuration is not found in the Kubernetes client Config;;;","01/Feb/24 13:18;mapohl;I think it's fine to do the upgrade from v6.6.2 to v6.9.0 even for 1.18:
 * The only change that seems to affect Flink is in the Mock-framework and only affects test code
 * Most of the changes are cleanup changes, except for:
 ** Fix [#5262|https://github.com/fabric8io/kubernetes-client/issues/5262]: Changed behavior in certain scenarios
 ** Fix [#5125|https://github.com/fabric8io/kubernetes-client/issues/5125]: Different default TLS version
 ** Fix [#1335|https://github.com/fabric8io/kubernetes-client/issues/1335]: Different fallback used for proxy configuration

Generally, we could argue that downstream project should rely on shaded dependencies provided by Flink. And fixing the bug out-weighs the stability concerns here. Do you see a problem with this argument [~gyfora], [~wangyang0918]?

The alternatives to doing the upgrade are:
 * Reverting the upgrade (i.e. going back from v6.6.2 to v5.12.4). This would allow us to get to the stable version that was tested with 1.17-
 * Provide a Flink-customized implementation of the fabric8io {{LeaderElector}} class with the cherry-picked changes of [8f8c438f|https://github.com/fabric8io/kubernetes-client/commit/8f8c438f] and [0f6c6965|https://github.com/fabric8io/kubernetes-client/commit/0f6c6965]. As a consequence, we would stick to fabric8io:kubernetes-client v.6.6.2;;;","02/Feb/24 09:22;chesnay;??Not used by Flink but might cause issues with projects which rely on the Flink dependency??

I don't think we should limit us by this. For all intents and purposes flink-kubernetes and it's dependencies are an internal thing and not intended to be used externally.
Putting that aside we have 1 small breaking change in a test; I'm +1 on an upgrade.;;;","02/Feb/24 10:11;wangyang0918;I am not aware of other potential issues and in favor of upgrading the k8s client version. ;;;","02/Feb/24 10:15;wangyang0918;+1 for what Chesnay said. We already shade the fabric8 k8s client in kubernetes-client module. It should not be used directly in other projects.;;;","13/Feb/24 16:20;mapohl;* 1.18
** [35c560312efc91dafd1b4674ce1e10acc9320ab1|https://github.com/apache/flink/commit/35c560312efc91dafd1b4674ce1e10acc9320ab1]
** [87560b7cedd6c857612a24b83485f5000b9edbd6|https://github.com/apache/flink/commit/87560b7cedd6c857612a24b83485f5000b9edbd6];;;","23/May/24 11:31;pedromazala;Do you guys know when 1.18.2 will make it? In the meanwhile are there docker images I could use (could be coming from github)
;;;","23/May/24 11:57;mapohl;You can raise the discussion on the ML. Alternatively, you're free to build your own Flink Docker image including the fix and push it to some registry.;;;","24/May/24 16:01;pedromazala;I've just done it. Thank you for porting it back;;;",,,,,,,,,,,,,,,,,,,,,,,
Investigate the permissions,FLINK-34332,13567006,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,01/Feb/24 09:02,01/Feb/24 09:02,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,Build System / CI,,,,0,starter,,,,"We're currently using {{read-all}} for our workflows. We might want to limit the scope and document why certain reads are needed (see [GHA docs|https://docs.github.com/en/actions/using-jobs/assigning-permissions-to-jobs]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-01 09:02:12.0,,,,,,,,,,"0|z1n5b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable Apache INFRA ephemeral runners for nightly builds,FLINK-34331,13567000,13562450,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,01/Feb/24 08:13,02/Apr/24 12:02,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,Build System / CI,,,,0,pull-request-available,,,,"The nightly CI is currently still utilizing the GitHub runners. We want to switch to Apache INFRA's ephemeral runners (see [docs|https://cwiki.apache.org/confluence/display/INFRA/ASF+Infra+provided+self-hosted+runners]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INFRA-25526,FLINK-34989,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 13:44:06 UTC 2024,,,,,,,,,,"0|z1n59s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 13:35;mapohl;I merged the 1.18 backport PR first to monitor the CI first: [Apache Infra runner workflow|https://github.com/apache/flink/actions/runs/7989772037] ([baseline workflow on GitHub runners|https://github.com/apache/flink/actions/runs/7985097005]);;;","21/Feb/24 13:44;mapohl;I reverted the change in [0293639|https://github.com/apache/flink/commit/0293639] because of an [error in the compilation phase|https://github.com/apache/flink/actions/runs/7989772037/job/21817027242#step:7:3509]. We can test CI by pushing the branch to the Apache Flink repo instead of using the fork 🤦 I'm gonna go ahead and do that;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specify code owners for .github/workflows folder,FLINK-34330,13566999,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,01/Feb/24 08:12,02/Feb/24 09:46,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,,,,,0,,,,,"Currently, the workflow files can be modified by any committer. We have to discuss whether we want to limit access to the PMC (or a subset of it) here. That might be a means to protect self-hosted runners.

See the [codeowner documentation|https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners] for further details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 09:44:19 UTC 2024,,,,,,,,,,"0|z1n59k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 08:59;mapohl;I guess it's not really necessary considering that the Apache Infra runners are only ephemeral. I'm gonna remove the {{blocks}} link to FLINK-34331 again.;;;","01/Feb/24 09:33;chesnay;We can't do this as it violates ASF policy. Code changes must not be restricted to specific persons.;;;","02/Feb/24 08:04;mapohl;Interesting, thanks for the pointer. I came up with it because Apache Arrow has a [apache/arrow:.github/CODEOWNERS|https://github.com/apache/arrow/blob/main/.github/CODEOWNERS] configuration file for exactly that purpose. I also found [Apache Geode|https://cwiki.apache.org/confluence/display/GEODE/Introduce+Codeowners+file] using this functionality. They are referring back to the {{.asf.yml}} which allows branch protection and automatic assignment for reviews (see [.asf.yml docs|https://cwiki.apache.org/confluence/display/INFRA/Git+-+.asf.yaml+features#Git.asf.yamlfeatures-Branchprotection]). So, maybe that policy changed or was relaxed?;;;","02/Feb/24 09:25;chesnay;Chances are they just weren't caught; no one is actively looking out this after all.
But anytime this gets brought up on ASF mailing lists it gets shut down quickly.;;;","02/Feb/24 09:44;chesnay;Specifically, what will not fly is that a codeowner review is REQUIRED for something to be merged. But you can use codeowners to auto-request reviews from people (WITHOUT blocking the PR on it) because then it's just automation around notifications.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalingReport format tests fail locally on decimal format,FLINK-34329,13566996,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fanrui,gyfora,gyfora,01/Feb/24 07:27,02/Feb/24 05:54,04/Jun/24 20:40,02/Feb/24 05:54,kubernetes-operator-1.8.0,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,,"The recently introduced scaling event format tests fail locally due to different decimal format:

```
[ERROR]   AutoScalerEventHandlerTest.testScalingReport:55
expected: ""Scaling execution enabled, begin scaling vertices:\{ Vertex ID ea632d67b7d595e5b851708ae9ad79d6 | Parallelism 3 -> 1 | Processing capacity 424.68 -> 123.40 | Target data rate 403.67}{ Vertex ID bc764cd8ddf7a0cff126f51c16239658 | Parallelism 4 -> 2 | Processing capacity Infinity -> Infinity | Target data rate 812.58}\{ Vertex ID 0a448493b4782967b150582570326227 | Parallelism 5 -> 8 | Processing capacity 404.73 -> 645.00 | Target data rate 404.27}""
 but was: ""Scaling execution enabled, begin scaling vertices:\{ Vertex ID ea632d67b7d595e5b851708ae9ad79d6 | Parallelism 3 -> 1 | Processing capacity 424,68 -> 123,40 | Target data rate 403,67}{ Vertex ID bc764cd8ddf7a0cff126f51c16239658 | Parallelism 4 -> 2 | Processing capacity Infinity -> Infinity | Target data rate 812,58}\{ Vertex ID 0a448493b4782967b150582570326227 | Parallelism 5 -> 8 | Processing capacity 404,73 -> 645,00 | Target data rate 404,27}""
[INFO]
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 05:54:21 UTC 2024,,,,,,,,,,"0|z1n58w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 07:28;gyfora;cc [~fanrui] ;;;","01/Feb/24 07:36;fanrui;Thanks [~gyfora] reporting this issue.

 

I don't understand why 812.58 can be formatted to 812,58 in String.format(""{color:#172b4d}%.2f"", 812.58). Could you elaborate on it? I can fix it asap after I {color}know the reason, thanks;;;","01/Feb/24 07:56;gyfora;I don't know the reason yet but the test fails constantly on my mac :(;;;","01/Feb/24 08:34;fanrui;I know the reason, the decimal point format in EU is , instead of . 

 

I don't know it before, I try to fix it asap.

 ;;;","02/Feb/24 05:54;fanrui;Merged to main(1.8.0) via : 398a87c9012ddc79bfe4b2378cea740642283628;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-34037 Improve Serialization Configuration And Usage In Flink,FLINK-34328,13566979,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,01/Feb/24 03:30,04/Feb/24 11:07,04/Jun/24 20:40,04/Feb/24 11:07,1.19.0,,,,,,,1.19.0,,,,API / Type Serialization System,Runtime / Configuration,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34037,FLINK-34354,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 13:43:51 UTC 2024,,,,,,,,,,"0|z1n554:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 03:31;Zhanghao Chen;It needs cross-team testing. [~lincoln.86xy] Could you assign to me?;;;","01/Feb/24 13:43;lincoln.86xy;[~Zhanghao Chen] Assigned to you :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use maven wrapper in operator build,FLINK-34327,13566966,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mason6345,mason6345,01/Feb/24 01:31,01/Feb/24 01:31,04/Jun/24 20:40,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Contributors need to switch between maven versions at times and mvnw can help make this easy. For reference, the build was failing with maven 3.2 but passed when I switched manually to maven 3.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-01 01:31:36.0,,,,,,,,,,"0|z1n528:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Slack integration,FLINK-34326,13566912,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,31/Jan/24 15:13,27/Mar/24 09:47,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,"There are Slack actions (e.g. [actions/slack-github-actions-slack-integration|https://github.com/marketplace/actions/slack-github-actions-slack-integration]) available. They usually require some secrets to be specified. Hence, it might be worth it to resolve FLINK-34322 beforehand to verify that secrets work properly with the workflows.",,,,,,,,,,,,FLINK-34322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 09:47:19 UTC 2024,,,,,,,,,,"0|z1n4q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 09:47;rskraba;It looks like subscribing to failed workflows is  [currently unsupported|https://github.com/integrations/slack/issues/1563] in the GitHub integration for Slack (already installed on the flink slack instance).  If it were, we could simply subscribe to the relevant event in the #builds channel :/

One alternative is to use one of the github slack notification actions (noting ravsamhq/notify-slack-action as an alternative in the issue above).  I can take a look at what INFRA prefers as an action.

Another alternative would be too subscribe to *all* of the workflow completed events from GitHub, including successful and canceled events.  There were about 50 runs over the last week, with only 14 of them were failures to be investigated.  This isn't perfect but it's also not unreasonable!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent state with data loss after OutOfMemoryError,FLINK-34325,13566908,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,asardaes,asardaes,31/Jan/24 14:40,02/Feb/24 16:33,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,,0,,,,,"I have a job that uses broadcast state to maintain a cache of required metadata. I am currently evaluating memory requirements of my specific use case, and I ran into a weird situation that seems worrisome.

All sources in my job are Kafka sources. I wrote a large amount of messages in Kafka to force the broadcast state's cache to grow. At some point, this caused an ""{{java.lang.OutOfMemoryError: Java heap space}}"" error in the -Job- Task Manager. I would have expected the whole java process of the TM to crash, but the job was simply restarted. What's worrisome is that, after 2 checkpoint failures ^1^, the job restarted and subsequently resumed from the latest successful checkpoint and completely ignored all the events I wrote to Kafka, which I can verify because I have a custom metric that exposes the approximate size of this cache, and the fact that the job didn't crashloop at this point after reading all the messages from Kafka over and over again.

I'm attaching an excerpt of the Job Manager's logs. My main concerns are:

# It seems the memory error somehow prevented the Kafka offsets from being ""rolled back"", so eventually the Kafka events that should have ended in the broadcast state's cache were ignored.
# -Is it normal that the state is somehow ""materialized"" in the JM and is thus affected by the size of the JM's heap? Is this something particular due to the use of broadcast state? I found this very surprising.- See comments

^1^ Two failures are expected since {{execution.checkpointing.tolerable-failed-checkpoints=1}}","Flink on Kubernetes with HA, RocksDB with incremental checkpoints on Azure",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/24 14:40;asardaes;jobmanager_log.txt;https://issues.apache.org/jira/secure/attachment/13066379/jobmanager_log.txt",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 13:49:03 UTC 2024,,,,,,,,,,"0|z1n4pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 15:25;martijnvisser;With regards to 2, isn't this as documented as ""Broadcast state is kept in-memory at runtime and memory provisioning should be done accordingly. This holds for all operator states."" at https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/broadcast_state/#important-considerations ? ;;;","31/Jan/24 15:29;asardaes;I had interpreted that as ""kept in Task Manager memory"". I am currently doing my experiments precisely for the purpose of provisioning, so if broadcast state is also kept in the Job Manager's memory, I can take that into account, I just found that rather surprising.;;;","31/Jan/24 15:41;asardaes;I just did a quick test where I increased the memory for the TM without changing the JM, and I could definitely load more metadata into the cache before it crashed. I wonder if the reported OOM error is actually from the TM and it's just reported by the JM?

EDIT: I think this is the case, I was logging a lot in the TM so I didn't see it initially, but I found the exception and its stacktrace in the TM logs after all.;;;","01/Feb/24 13:49;asardaes;I will say that, while I can of course reproduce the OOM problems, I cannot reliably reproduce the state inconsistency, most of the time the job really ends up in a crashloop until I increase memory or clean up the state.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
s3_setup is called in test_file_sink.sh even if the common_s3.sh is not sourced,FLINK-34324,13566896,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,31/Jan/24 13:36,10/May/24 08:07,04/Jun/24 20:40,10/May/24 08:06,1.17.2,1.18.1,1.19.0,,,,,1.18.2,1.19.1,1.20.0,,Connectors / Hadoop Compatibility,Tests,,,0,pull-request-available,test-stability,,,"See example CI run from the FLINK-34150 PR:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56570&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3191
{code}
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_file_sink.sh: line 38: s3_setup: command not found
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34150,,,,FLINK-34508,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 08:06:41 UTC 2024,,,,,,,,,,"0|z1n4mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 07:46;mapohl;I merged the 1.18 backport first to not interrupt the 1.19 release planning (because it's planned to create the release branch soon). Let's check the [corresponding CI run on release-1.18|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57335&view=results] before merging the {{master}} and 1.17 change. ;;;","06/Feb/24 16:36;mapohl;I reverted the fix in release-1.18 with [a4dd5854|https://github.com/apache/flink/commit/a4dd58545d59b59089d9321a743d6c98a7c8e855] because it introduced [a test failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57335&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3551]. I'm gonna get back to this one tomorrow.;;;","10/May/24 08:06;mapohl;* master: [93526c2f3247598ce80854cf65dd4440eb5aaa43|https://github.com/apache/flink/commit/93526c2f3247598ce80854cf65dd4440eb5aaa43]
* 1.19: [8707c63ee147085671a9ae1b294854bac03fc914|https://github.com/apache/flink/commit/8707c63ee147085671a9ae1b294854bac03fc914]
* 1.18: [7d98ab060be82fe3684d15501b9eb83373303d18|https://github.com/apache/flink/commit/7d98ab060be82fe3684d15501b9eb83373303d18];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session window tvf failed when using named params,FLINK-34323,13566893,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,31/Jan/24 13:06,05/Feb/24 02:07,04/Jun/24 20:40,05/Feb/24 02:07,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34300,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 02:07:12 UTC 2024,,,,,,,,,,"0|z1n4m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 13:06;xuyangzhong;I'll try to fix it.;;;","05/Feb/24 02:07;qingyue;Fixed in master 6be30b167990c22765c244a703ab0424e7c3b4d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make secrets work in GitHub Action workflows,FLINK-34322,13566891,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,31/Jan/24 12:52,21/Feb/24 11:06,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,Build System / CI,,,,0,,,,,The secrets need to be handed over to Apache Infra to make them accessible in the nightly runs. We might have to do adaptations to the workflows as well because it wasn't tested in the previous stages of FLIP-396.,,,,,,,,,,,,,FLINK-34326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 11:06:02 UTC 2024,,,,,,,,,,"0|z1n4lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 11:06;mapohl;[~jingge] The secrets are currently listed in the Azure Pipelines project which is owned by Ververica. Someone needs to be figure out what secrets are available and whether it's legit to move these secrets to Apache Infra (because they are the ones maintaining the secrets in the Apache foundation). Can someone on the Ververica side pick this Jira issue up?

just as an fyi: Pushing for GitHub Actions will hopefully free Ververica from the CI/Azure Pipelines maintenance. So, there is an incentive to prioritize these tasks and support the migration.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make nightly trigger select the release branch automatically,FLINK-34321,13566888,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,31/Jan/24 12:33,31/Jan/24 12:33,04/Jun/24 20:40,,,,,,,,,,,,,Build System / CI,,,,0,,,,,"Currently, GHA CI only works with master (i.e. 1.19) and {{{}release-1.18{}}}. After the release of 1.19, we could switch to automatically selecting the release branches analogously to what is done in the [flink-ci/git-repo-sync|https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh#L28]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-31 12:33:35.0,,,,,,,,,,"0|z1n4kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kafka connector tests time out,FLINK-34320,13566883,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,martijnvisser,martijnvisser,31/Jan/24 11:46,16/Apr/24 04:13,04/Jun/24 20:40,,kafka-3.1.0,,,,,,,,,,,Connectors / Kafka,,,,0,test-stability,,,,"https://github.com/apache/flink-connector-kafka/actions/runs/7700171105/job/20987805277?pr=83#step:14:61746

{code:java}
2024-01-29T19:45:07.4412975Z 19:45:07,094 [                main] INFO  org.apache.kafka.common.utils.AppInfoParser                  [] - App info kafka.producer for producer-client-id unregistered
2024-01-29T19:45:07.4413978Z 19:45:07,097 [                main] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-io-3306202c-1639-4b7b-a54c-381826e3682e
2024-01-29T19:45:07.4414533Z 19:45:07,440 [                main] INFO  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationTest [] - 
2024-01-29T19:45:07.4414785Z --------------------------------------------------------------------------------
2024-01-29T19:45:07.4415494Z Test testRestoreProducer[Migration Savepoint: 1.16](org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationTest) successfully run.
2024-01-29T19:45:07.4415646Z ================================================================================
2024-01-29T19:45:07.4698277Z [WARNING] Tests run: 18, Failures: 0, Errors: 0, Skipped: 9, Time elapsed: 206.197 s - in org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationTest
2024-01-29T20:30:32.8459835Z ##[error]The action has timed out.
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-31 11:46:23.0,,,,,,,,,,"0|z1n4js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump okhttp version to 4.12.0,FLINK-34319,13566859,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ConradJam,ConradJam,ConradJam,31/Jan/24 09:50,01/Feb/24 15:26,04/Jun/24 20:40,01/Feb/24 15:26,1.7.2,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,Bump okhttp version to 4.12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 15:26:24 UTC 2024,,,,,,,,,,"0|z1n4eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 15:26;gyfora;merged to main ca3a746e42beb4816c4f3fb7d5b9aea6fc757b32;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveScheduler resource stabilisation should happen before the job is cancelled,FLINK-34318,13566858,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gyfora,gyfora,31/Jan/24 09:48,31/Jan/24 10:06,04/Jun/24 20:40,31/Jan/24 10:06,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"When a new resource requirement is submitted to the AdaptiveScheduler which increases the resource upper bound (max taskmanagers), when the first TaskManager comes up the job is immediately cancelled. 

Once the job is cancelled the scheduler waits for the entire stabilisation period to pass if it cannot acquire all resources before starting with the lower-than-requested parallelism.

The problem here is that waiting for resource stabilisation happens after the job is cancelled, introducing unnecessary downtime for the job if the stabilisation period is large.

We should change logic here to wait for the stabilisation period first to acquire all possible resources before cancelling the job.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33092,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 10:06:48 UTC 2024,,,,,,,,,,"0|z1n4e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 09:49;gyfora;cc [~dmvk] [~chesnay] [~mxm] 

what do you guys think?;;;","31/Jan/24 10:04;fanrui;Hi [~gyfora] , this proposal makes sense to me. I proposed it in FLINK-33092 before a series of months.;;;","31/Jan/24 10:06;gyfora;Closing this as duplicate thanks [~fanrui] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup pending Flink CDC issues on GitHub,FLINK-34317,13566856,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,renqs,renqs,renqs,31/Jan/24 09:45,29/Apr/24 08:27,04/Jun/24 20:40,29/Apr/24 08:27,,,,,,,,,,,,Flink CDC,,,,0,,,,,"Until 01/30/2024, there are 744 open and 865 closed issues on GitHub. Outdated and invalid opening issues should be closed as much as possible before the migration to avoid flushing Jira. 
h1. Working Procedure

1. Close issues [created before 2022-11-10|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3C2022-11-10]

Can be done automatically (392): [https://github.com/marketplace/actions/close-stale-issues] with comment:
{code:java}
Closing this issue because it was created before version 2.3.0 (2022-11-10). Please try the latest version of Flink CDC to see if the issue has been resolved. If the issue is still valid, kindly report it on [Apache Jira](https://issues.apache.org/jira) under project `Flink` with component tag `Flink CDC`. Thank you!{code}
2. Manually review by module (352):
 * [Db2|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3E%3D2022-11-10+db2]: 3
 * [MongoDB|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3E%3D2022-11-10+mongo]: 14
 * [MySQL|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3E%3D2022-11-10+mysql]: 148
 * [OceanBase|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3E%3D2023-01-01+oceanbase]: 4
 * [Oracle|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3E%3D2022-11-10+oracle]: 80
 * [Postgres|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3E%3D2022-11-10+postgres+]: 25
 * [SQL Server|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3E%3D2022-11-10+sqlserver+]: 21
 * [TiDB|https://github.com/ververica/flink-cdc-connectors/issues?q=is%3Aissue+is%3Aopen+created%3A%3E%3D2022-11-10+tidb]: 20
 * Others: 37

If the issue is not valid or too vague to judge, please close them with comment:
{code:java}
Closing this issue as it is no longer valid or requires additional information. If the issue still exists, please report it on [Apache Jira](https://issues.apache.org/jira) under the project `Flink`, using the component tag `Flink CDC`. Thank you!{code}
Issues in Chinese should be closed with comment:
{code:java}
Considering collaboration with developers around the world, please re-create your issue in English on [Apache Jira](https://issues.apache.org/jira) under project `Flink` with component tag `Flink CDC`. Thank you! {code}
If the issue does exist, just leave it as is and we'll migrate all opening issues to Jira by script finally. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 08:27:23 UTC 2024,,,,,,,,,,"0|z1n4ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 08:27;renqs;All issues have been imported to Jira: https://issues.apache.org/jira/browse/FLINK-34890?jql=project%20%3D%20FLINK%20AND%20reporter%20in%20(flink-cdc-import);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce instantiation of ScanRuntimeProvider in streaming mode,FLINK-34316,13566848,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,31/Jan/24 07:50,31/Jan/24 13:05,04/Jun/24 20:40,31/Jan/24 13:05,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,This is pure performance optimization by avoiding an additional call to \{{ScanTableSource#getScanRuntimeProvider}} in \{{org.apache.flink.table.planner.connectors.DynamicSourceUtils#validateScanSource}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 13:05:15 UTC 2024,,,,,,,,,,"0|z1n4c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 13:05;twalthr;Fixed in master: 503580834e06455bbcaf619a51f8d18812bbe83f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Temporarily force disabling window join, window rank and window deduplicate optimization when using session window tvf",FLINK-34315,13566847,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,31/Jan/24 07:44,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,"Currently session window tvf is first introduced after https://issues.apache.org/jira/browse/FLINK-24024 . However after https://issues.apache.org/jira/browse/FLINK-34100 the session window tvf node can exist independently of window aggregation, but it is not ready for window join, window rank and window deduplicate. So we need to temporarily disable window join, window rank and window deduplicate optimization when using session window tvf.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34100,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-31 07:44:40.0,,,,,,,,,,"0|z1n4bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update CI Node Actions from NodeJS 16 to NodeJS 20,FLINK-34314,13566845,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,31/Jan/24 07:34,20/Feb/24 09:13,04/Jun/24 20:40,20/Feb/24 09:13,,,,,,,,,,,,Build System / CI,,,,0,pull-request-available,,,,"{code:java}
Node.js 16 actions are deprecated. Please update the following actions to use Node.js 20: actions/checkout@v3, actions/setup-java@v3, stCarolas/setup-maven@v4.5, actions/cache/restore@v3, actions/cache/save@v3. 
{code}

For more information see: https://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20/.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 09:13:01 UTC 2024,,,,,,,,,,"0|z1n4bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 09:13;martijnvisser;Fixed in apache/flink-connector-shared-utils:ci_utils ec546068089dac4c4192875b57703989fc3bb009;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update doc about session window tvf,FLINK-34313,13566843,13335153,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,31/Jan/24 07:28,04/Feb/24 15:26,04/Jun/24 20:40,04/Feb/24 15:26,1.19.0,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23582,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 04 15:25:53 UTC 2024,,,,,,,,,,"0|z1n4aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 15:25;qingyue;Fixed in master eff073ff1199acf0f26a0f04ede7d692837301c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the handling of default node types for named parameters.,FLINK-34312,13566760,13564274,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,30/Jan/24 16:43,26/Feb/24 11:28,04/Jun/24 20:40,26/Feb/24 11:28,,,,,,,,1.20.0,,,,,,,,0,pull-request-available,,,,"Currently, we have supported the use of named parameters with optional arguments. 

By adapting the interface of Calcite, we can fill in the default operator when a parameter is missing. Whether it is during the validation phase or when converting to SqlToRel phase, we need to handle it specially by modifying the return type of DEFAULT operator based on the argument type of the operator.  
We have multiple places that need to handle the type of DEFAULT operator, including SqlCallBinding, SqlOperatorBinding, and SqlToRelConverter.


The improved solution is as follows: 

Before SqlToRel, we can construct a DEFAULT node with a return type that matches the argument type. This way, during the SqlToRel phase, there is no need for special handling of the DEFAULT node's type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 11:27:54 UTC 2024,,,,,,,,,,"0|z1n3sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 11:27;fsk119;Merged into master: 1070c6e9e0f9f00991bdeb34f0757e4f0597931e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not change min resource requirements when rescaling for adaptive scheduler,FLINK-34311,13566715,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,30/Jan/24 13:43,05/Feb/24 14:33,04/Jun/24 20:40,05/Feb/24 14:33,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"when applying the rescale api to change parallelism we should not change the min parallelism.
The problem currently is that if we cannot aquire the new resources within {{jobmanager.adaptive-scheduler.resource-wait-timeout}} the job will completely fail

The {{jobmanager.adaptive-scheduler.resource-stabilization-timeout}} still allows us to wait for quite long if necessary to get the target parallelism but failing completely because of the wait timeout seems very unfortunate

It's best to keep the min resources unchanged and let the adaptive scheduler take care of the parallelism changes together with the timeout settings.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 14:33:11 UTC 2024,,,,,,,,,,"0|z1n3ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:33;gyfora;merged to main 4342636cdb2c3439389e83cb4fe4366156edfbd7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33325 Built-in cross-platform powerful java profiler,FLINK-34310,13566714,13566678,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,lincoln.86xy,lincoln.86xy,30/Jan/24 13:43,07/Feb/24 03:21,04/Jun/24 20:40,06/Feb/24 11:25,1.19.0,,,,,,,1.19.0,,,,Runtime / REST,Runtime / Web Frontend,,,0,,,,,"Instructions:
1. For the default case, it will print the hint to tell users how to enable this feature.
 !screenshot-2.png! 

2. After we add {{rest.profiling.enabled: true}} in the configurations, we can use this feature now, and the default mode should be {{ITIMER}}
 !screenshot-3.png! 

3. We cannot create another profiling while one is running
 !screenshot-4.png! 

4. We can get at most 10 profilling snapshots by default, and the older one will be deleted automaticially.
 !screenshot-5.png! 
",,,,,,,,,,,,,,,,,,,,FLINK-34367,,FLINK-34309,,,,,,,,,,,,,,,,,,,,"06/Feb/24 06:09;fanrui;image-2024-02-06-14-09-39-874.png;https://issues.apache.org/jira/secure/attachment/13066489/image-2024-02-06-14-09-39-874.png","05/Feb/24 14:45;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066469/screenshot-1.png","06/Feb/24 11:16;yunta;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13066504/screenshot-2.png","06/Feb/24 11:20;yunta;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13066505/screenshot-3.png","06/Feb/24 11:20;yunta;screenshot-4.png;https://issues.apache.org/jira/secure/attachment/13066506/screenshot-4.png","06/Feb/24 11:24;yunta;screenshot-5.png;https://issues.apache.org/jira/secure/attachment/13066507/screenshot-5.png",,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 03:21:30 UTC 2024,,,,,,,,,,"0|z1n3i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:46;lincoln.86xy;[~Yu Chen] Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","06/Feb/24 06:10;fanrui;When I test other features in my Local, I found Profiler page throw some exceptions. I'm not sure whether it's expected.

 

Note: I didn't enable it.

 

!image-2024-02-06-14-09-39-874.png!;;;","06/Feb/24 06:46;lincoln.86xy;[~fanrui] Thanks for reporting! Would you like to take this testing work?;;;","06/Feb/24 06:48;lincoln.86xy;[~Yu Chen] Could you estimate when the user doc (https://issues.apache.org/jira/browse/FLINK-33436) can be finished？;;;","06/Feb/24 06:56;fanrui;{quote}[~fanrui] Thanks for reporting! Would you like to take this testing work?
{quote}
Sure, I can test it after [~Yu Chen]  provided the testing instruction.

BTW, as we know, the Chinese new year is coming. I may be unavailable during the vocation.;;;","06/Feb/24 07:09;lincoln.86xy;[~fanrui] Great! Let's just wait for the testing instructions ready. 
Happy Chinese New Year! :);;;","06/Feb/24 08:09;yunta;I could help to provide the testing instructions, and I will assign it to [~fanrui] later.;;;","07/Feb/24 03:02;Yu Chen;Sorry for the late response.

Thanks [~yunta] for creating the Testing Instructions.  
{quote}When I test other features in my Local, I found Profiler page throw some exceptions. I'm not sure whether it's expected.  
{quote}
Hi [~fanrui], so far, the determination of whether the profiler is enabled or not is achieved by checking if the interface is registered with
`WebMonitorEndpoint`. Therefore, this behavior is by design.
But I think we can implement this check more elegantly in a later version by registering an interface to check the enabled status of the profiler.
 
{quote}[~Yu Chen] Could you estimate when the user doc (https://issues.apache.org/jira/browse/FLINK-33436) can be finished？{quote}
Hi [~lincoln.86xy] , really sorry for the late response. I was quite busy recently, is that OK for me to finish working on the documentation within the next week (before 02.18)?;;;","07/Feb/24 03:21;lincoln.86xy;[~Yu Chen]  Thanks for the updates! The time is ok, as it falls on the vocation, and Happy Chinese New Year! :);;;",,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33315 Optimize memory usage of large StreamOperator,FLINK-34309,13566713,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,lincoln.86xy,lincoln.86xy,30/Jan/24 13:42,06/Feb/24 09:28,04/Jun/24 20:40,06/Feb/24 06:19,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34310,,FLINK-34308,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 06:19:01 UTC 2024,,,,,,,,,,"0|z1n3i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 06:19;fanrui;This improvement doesn't need cross team testing. Closing it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33625 Support System out and err to be redirected to LOG or discarded,FLINK-34308,13566712,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,lincoln.86xy,lincoln.86xy,30/Jan/24 13:41,06/Feb/24 09:28,04/Jun/24 20:40,06/Feb/24 06:51,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34309,FLINK-34382,FLINK-34307,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 06:51:32 UTC 2024,,,,,,,,,,"0|z1n3hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 06:24;fanrui;Test suggestion:
 # Prepare a Flink SQL job and a flink datastream job
 ** they can use the print sink or call System.out.println inside of the UDF
 # Add this config to the config.yaml
 ** taskmanager.system-out.mode : LOG
 # Run the job
 # Check whether the print log is redirected to log file

 

SQL demo:
{code:java}
./bin/sql-client.sh

CREATE TABLE orders (
  id           INT,
  app          INT,
  channel      INT,
  user_id      STRING,
  ts           TIMESTAMP(3),
  WATERMARK FOR ts AS ts
) WITH (
   'connector' = 'datagen',
   'rows-per-second'='20',
   'fields.app.min'='1',
   'fields.app.max'='10',
   'fields.channel.min'='21',
   'fields.channel.max'='30',
   'fields.user_id.length'='10'
);

create table print_sink ( 
  id           INT,
  app          INT,
  channel      INT,
  user_id      STRING,
  ts           TIMESTAMP(3)
) with ('connector' = 'print' );

insert into print_sink
select id       ,app       ,channel       ,user_id       ,ts   from orders {code};;;","06/Feb/24 06:51;lincoln.86xy;Testing tracked by https://issues.apache.org/jira/browse/FLINK-34382;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33972 Enhance and synchronize Sink API to match the Source API,FLINK-34307,13566711,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,pvary,lincoln.86xy,lincoln.86xy,30/Jan/24 13:40,06/Feb/24 12:14,04/Jun/24 20:40,06/Feb/24 11:20,1.19.0,,,,,,,1.19.0,,,,API / DataStream,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34308,,FLINK-34306,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 11:20:51 UTC 2024,,,,,,,,,,"0|z1n3hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:20;pvary;[~lincoln.86xy]: Thanks for managing the release.
For now, the unit tests are sufficient.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-25857 Add committer metrics to track the status of committables ,FLINK-34306,13566709,13566678,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,pvary,lincoln.86xy,lincoln.86xy,30/Jan/24 13:39,22/Feb/24 16:22,04/Jun/24 20:40,06/Feb/24 11:20,1.19.0,,,,,,,1.19.0,,,,Connectors / Common,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34307,,,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:43;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066468/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 12:12:27 UTC 2024,,,,,,,,,,"0|z1n3h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:43;lincoln.86xy;[~pvary] Can you help confirm if this feature(also another one https://issues.apache.org/jira/browse/FLINK-34308) needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
!screenshot-1.png! ;;;","06/Feb/24 11:20;pvary;[~lincoln.86xy]: Thanks for managing the release.
For now, the unit tests are sufficient.;;;","06/Feb/24 12:12;lincoln.86xy;[~pvary] Thank you for confirming!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33261 Support Setting Parallelism for Table/SQL Sources ,FLINK-34305,13566708,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,sudewei.sdw,lincoln.86xy,lincoln.86xy,30/Jan/24 13:38,23/Feb/24 01:43,04/Jun/24 20:40,23/Feb/24 01:43,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34500,FLINK-33261,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 23 01:43:16 UTC 2024,,,,,,,,,,"0|z1n3gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 03:27;Zhanghao Chen;[~sudewei.sdw] Could you help prepare a testing instruction?;;;","08/Feb/24 15:33;jingge;Hi [~sudewei.sdw] I have assigned this ticket to you;;;","20/Feb/24 07:21;lincoln.86xy;[~sudewei.sdw][~Zhanghao Chen] Can you help estimate when the docs will be ready?;;;","20/Feb/24 12:53;sudewei.sdw;[~lincoln.86xy] I will complete this document before February 22nd UTC + 8, and the earliest is the 21st. If this exceeds the deadline, please let me know.;;;","20/Feb/24 13:00;lincoln.86xy;[~sudewei.sdw] Sounds good! Actually we don't have a specific deadline, but at the moment there is only this testing Instruction left, so the completion of this issue could help get 1.19 out sooner, thanks for your work! ;;;","22/Feb/24 16:28;sudewei.sdw;[~lincoln.86xy] [~Zhanghao Chen] Hi, the docs is ready in FLINK-34500 and you can close this issue. If there is any problem, please let me know.;;;","23/Feb/24 01:43;lincoln.86xy;[~sudewei.sdw] Thanks for your work on this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-34219 Introduce a new join operator to support minibatch,FLINK-34304,13566707,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xu_shuai_,lincoln.86xy,lincoln.86xy,30/Jan/24 13:37,22/Feb/24 16:22,04/Jun/24 20:40,05/Feb/24 14:32,1.19.0,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34349,,FLINK-34303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 14:32:04 UTC 2024,,,,,,,,,,"0|z1n3go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 07:17;xu_shuai_;This test is opened in another issue FLINK-34349 and this issue would be closed.;;;","05/Feb/24 14:32;lincoln.86xy;[~xu_shuai_] Thanks for your updates!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-34054 Support named parameters for functions and procedures,FLINK-34303,13566705,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hackergin,lincoln.86xy,lincoln.86xy,30/Jan/24 13:36,07/Feb/24 10:01,04/Jun/24 20:40,05/Feb/24 05:24,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34304,FLINK-34355,FLINK-34302,,,,,,,,,,,,,,FLINK-34054,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 05:24:15 UTC 2024,,,,,,,,,,"0|z1n3g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 09:35;hackergin;Test suggestion:

1. Implement a test UDF or Procedure and support Named Parameters.

2. When calling a function or procedure, use named parameters to verify if the results are as expected.

You can test the following scenarios:
1. Normal usage of named parameters, fully specifying each parameter.
2. Omitting unnecessary parameters.
3. Omitting necessary parameters to confirm if an error is reported.;;;","05/Feb/24 05:24;lincoln.86xy;[~hackergin]Thanks for updating the test instructions!
I've created a new jira to track the testing result following the crossteam testing steps.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33644 Make QueryOperations SQL serializable,FLINK-34302,13566703,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,lincoln.86xy,lincoln.86xy,30/Jan/24 13:31,07/Feb/24 06:16,04/Jun/24 20:40,07/Feb/24 06:16,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34303,,FLINK-34301,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:42;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066467/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 06:16:45 UTC 2024,,,,,,,,,,"0|z1n3fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:42;lincoln.86xy;[~dwysakowicz] Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","06/Feb/24 16:48;dwysakowicz;Test suggestions:
1. Write a few Table API programs.
2. Call {{Table.getQueryOperation#asSerializableString}}, manually verify the produced SQL query
3. Check the produced SQL query is runnable and produces the same results as the Table API program:

{code}

Table table = tEnv.from(""a"") ...

String sqlQuery = table.getQueryOperation().asSerializableString();

//verify the sqlQuery is runnable
tEnv.sqlQuery(sqlQuery).execute().collect()
{code};;;","07/Feb/24 06:16;lincoln.86xy;[~dwysakowicz] Thanks for the updates!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-20281 Window aggregation supports changelog stream input,FLINK-34301,13566702,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xuyangzhong,lincoln.86xy,lincoln.86xy,30/Jan/24 13:30,06/Feb/24 09:27,04/Jun/24 20:40,05/Feb/24 05:09,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34302,FLINK-34348,FLINK-34300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 09:40:38 UTC 2024,,,,,,,,,,"0|z1n3fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 09:40;xuyangzhong;Window TVF aggregation supports changelog stream  is ready for testing. User can add a window tvf aggregation as a down stream after CDC source or some nodes that will produce cdc records.

Someone can verify this feature with:
 # Prepare a mysql table, and insert some data at first.
 # Start sql-client and prepare ddl for this mysql table as a cdc source.
 # You can verify the plan by `EXPLAIN PLAN_ADVICE` to check if there is a window aggregate node and the changelog contains ""UA"" or ""UB"" or ""D"" in its upstream. 
 # Use different kinds of window tvf to test window tvf aggregation while updating the source data to check the data correctness.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-24024 Support session Window TVF,FLINK-34300,13566701,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xuyangzhong,lincoln.86xy,lincoln.86xy,30/Jan/24 13:29,06/Feb/24 09:26,04/Jun/24 20:40,05/Feb/24 05:08,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34301,FLINK-34346,FLINK-34298,,,,,,,,,,,,FLINK-34323,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 04 02:40:09 UTC 2024,,,,,,,,,,"0|z1n3fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 09:27;xuyangzhong;Session window TVF is ready. Users can use Session window TVF aggregation instead of using legacy session group window aggregation.

Someone can verify this feature by following the [doc]([https://github.com/apache/flink/pull/24250]) although it is still being reviewed. 

Further more,  although session window join, session window rank and session window deduplicate are in experimental state, If someone finds some bugs about them, you could also open a Jira linked this one to report them.;;;","04/Feb/24 02:40;xu_shuai_;Hi, [~xuyangzhong]. I'd like to take this verification. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33203 Adding a separate configuration for specifying Java Options of the SQL Gateway ,FLINK-34299,13566700,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,lincoln.86xy,lincoln.86xy,30/Jan/24 13:28,06/Feb/24 09:26,04/Jun/24 20:40,02/Feb/24 02:37,1.19.0,,,,,,,1.19.0,,,,Table SQL / Gateway,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 14:50:30 UTC 2024,,,,,,,,,,"0|z1n3f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 02:37;guoyangze;This change doesn't need crossteam testing.;;;","02/Feb/24 14:50;lincoln.86xy;[~guoyangze] Thanks for confirming!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33397 Support Configuring Different State TTLs using SQL Hint,FLINK-34298,13566699,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,qingyue,lincoln.86xy,lincoln.86xy,30/Jan/24 13:27,21/Feb/24 02:05,04/Jun/24 20:40,04/Feb/24 05:46,1.19.0,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,"This ticket describes how to verify FLINK-33397: Support Configuring Different State TTLs using SQL Hint.

 

The verification steps are as follows.
1. Start the standalone session cluster and sql client.
2. Execute the following DDL statements.
{code:sql}
CREATE TABLE `default_catalog`.`default_database`.`Orders` (
  `order_id` INT,
  `line_order_id` INT
) WITH (
  'connector' = 'datagen', 
  'rows-per-second' = '5'

); 

CREATE TABLE `default_catalog`.`default_database`.`LineOrders` (
  `line_order_id` INT,
  `ship_mode` STRING
) WITH (
  'connector' = 'datagen',
  'rows-per-second' = '5'
);

CREATE TABLE `default_catalog`.`default_database`.`OrdersShipInfo` (
  `order_id` INT,
  `line_order_id` INT,
  `ship_mode` STRING ) WITH (
  'connector' = 'print'
); {code}
3. Compile and verify the INSERT INTO statement with the STATE_TTL hint applied to join
{code:sql}
-- SET the pipeline level state TTL to 24h
SET 'table.exec.state.ttl' = '24h';

-- Configure different state TTL for join operator
COMPILE PLAN '/path/to/join-plan.json' FOR
INSERT INTO OrdersShipInfo 
SELECT /*+STATE_TTL('a' = '2d', 'b' = '12h')*/ a.order_id, a.line_order_id, b.ship_mode 
FROM Orders a JOIN LineOrders b ON a.line_order_id = b.line_order_id;

{code}
The generated JSON file *should* contain the following ""state"" JSON array for StreamJoin ExecNode.
{code:json}
{
    ""id"" : 5,
    ""type"" : ""stream-exec-join_1"",
    ""joinSpec"" : {
      ...
    },
    ""state"" : [ {
      ""index"" : 0,
      ""ttl"" : ""2 d"",
      ""name"" : ""leftState""
    }, {
      ""index"" : 1,
      ""ttl"" : ""12 h"",
      ""name"" : ""rightState""
    } ],
    ""inputProperties"": [...],
    ""outputType"": ...,
    ""description"": ...
}
{code}
4. Compile and verify the INSERT INTO statement with the STATE_TTL hint applied to group aggregate
{code:sql}
CREARE TABLE source_t (
    a INT,
    b BIGINT,
    c STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second' = '5'
);

CREARE TABLE sink_t (
    b BIGINT PRIMARY KEY NOT ENFORCED,
    cnt BIGINT,
    avg_a DOUBLE,
    min_c STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second' = '5'
);
COMPILE PLAN '/path/to/agg-plan.json' FOR
INSERT INTO sink_t SELECT /*+ STATE_TTL('source_t' = '1s') */
b, 
COUNT(*) AS cnt,
AVG(a) FILTER (WHERE a > 1) AS avg_a
MIN(c) AS min_c 
FROM source_t GROUP BY b

{code}
 
The generated JSON file *should* contain the following ""state"" JSON array for StreamExecGroupAggregate ExecNode.
{code:json}
""state"" : [ {
  ""index"" : 0,
  ""ttl"" : ""1 s"",
  ""name"" : ""groupAggregateState""
} ]

{code}",,,,,,,,,,,,,,,,,,,,FLINK-34299,FLINK-34300,FLINK-34297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 02:05:14 UTC 2024,,,,,,,,,,"0|z1n3ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 03:39;liyubin117;[~qingyue] I would like to take this, please assign this to me, thanks!;;;","04/Feb/24 03:45;qingyue;Hi,  [~liyubin117], Thanks for volunteering. Assigned to you.;;;","04/Feb/24 05:45;qingyue;Hi, [~liyubin117]; according to the new workflow, this ticket is an instruction ticket that should be assigned to me. I've opened a new subtask FLINK-34351 and assigned to you.;;;","04/Feb/24 05:46;qingyue;I've added the instructions on how to verify the feature.;;;","21/Feb/24 02:05;liyubin117;[~qingyue] Thanks for your detailed instructions;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-34079 Migrate string configuration key to ConfigOption,FLINK-34297,13566698,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xuannan,lincoln.86xy,lincoln.86xy,30/Jan/24 13:26,06/Feb/24 09:26,04/Jun/24 20:40,31/Jan/24 10:31,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34298,,FLINK-34296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 10:31:32 UTC 2024,,,,,,,,,,"0|z1n3eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 10:28;xuannan;[~lincoln.86xy] This change doesn't need crossteam testing. I think we can close the ticket.;;;","31/Jan/24 10:31;lincoln.86xy;[~xuannan] Thanks for your reply!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33581 Deprecate configuration getters/setters that return/set complex Java objects,FLINK-34296,13566696,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,JunRuiLi,lincoln.86xy,lincoln.86xy,30/Jan/24 13:23,06/Feb/24 09:26,04/Jun/24 20:40,02/Feb/24 14:51,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34297,,FLINK-34295,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 14:51:15 UTC 2024,,,,,,,,,,"0|z1n3e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 02:40;JunRuiLi;[~lincoln.86xy] This change doesn't need cross testing. I think we could close this ticket.;;;","02/Feb/24 14:51;lincoln.86xy;[~JunRuiLi] Thanks for confirming, will close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33712 Deprecate RuntimeContext#getExecutionConfig,FLINK-34295,13566695,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,JunRuiLi,lincoln.86xy,lincoln.86xy,30/Jan/24 13:22,06/Feb/24 09:26,04/Jun/24 20:40,02/Feb/24 14:51,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34296,,FLINK-34294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 14:51:48 UTC 2024,,,,,,,,,,"0|z1n3e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 02:40;JunRuiLi;[~lincoln.86xy] This change doesn't need cross testing. I think we could close this ticket.;;;","02/Feb/24 14:51;lincoln.86xy;[~JunRuiLi] Thanks for confirming!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33297 Support standard YAML for FLINK configuration,FLINK-34294,13566693,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,JunRuiLi,lincoln.86xy,lincoln.86xy,30/Jan/24 13:21,06/Feb/24 09:26,04/Jun/24 20:40,06/Feb/24 05:42,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34295,FLINK-34377,FLINK-34293,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:41;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066466/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 05:42:42 UTC 2024,,,,,,,,,,"0|z1n3dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:41;lincoln.86xy;[~JunRuiLi] Can you help confirm if this feature needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket 
!screenshot-1.png! ;;;","06/Feb/24 02:16;JunRuiLi;[~lincoln.86xy] Yes, this feature does require cross-team testing. I will prepare the test instructions and create a new ticket follow the steps.;;;","06/Feb/24 02:31;lincoln.86xy;[~JunRuiLi]Thanks for your updates!;;;","06/Feb/24 05:42;lincoln.86xy;Testing tracked by https://issues.apache.org/jira/browse/FLINK-34377;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-34190 Deprecate RestoreMode#LEGACY,FLINK-34293,13566691,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zakelly,lincoln.86xy,lincoln.86xy,30/Jan/24 13:18,06/Feb/24 09:25,04/Jun/24 20:40,31/Jan/24 07:10,1.19.0,,,,,,,1.19.0,,,,Runtime / Checkpointing,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34294,,FLINK-34292,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-30 13:18:14.0,,,,,,,,,,"0|z1n3d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-30613 Improve resolving schema compatibility ,FLINK-34292,13566688,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,masteryhx,lincoln.86xy,lincoln.86xy,30/Jan/24 13:08,06/Feb/24 09:25,04/Jun/24 20:40,05/Feb/24 07:35,1.19.0,,,,,,,1.19.0,,,,Runtime / Checkpointing,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34293,,FLINK-34291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 07:35:50 UTC 2024,,,,,,,,,,"0|z1n3cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 07:35;masteryhx;Closed Since it doesn't beak the change of public API.

This introduces a new default interface and deprecate the old one so that all jobs could work without any changes.

The new and old paths have been tested innerally.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33697 Support adding custom metrics in Recovery Spans,FLINK-34291,13566687,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,lincoln.86xy,lincoln.86xy,30/Jan/24 13:07,06/Feb/24 09:25,04/Jun/24 20:40,06/Feb/24 01:26,1.19.0,,,,,,,1.19.0,,,,Runtime / Metrics,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34292,,FLINK-34290,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 01:26:04 UTC 2024,,,,,,,,,,"0|z1n3c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 01:26;lincoln.86xy;The testing will tracked by https://issues.apache.org/jira/browse/FLINK-34289;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33696 Add OpenTelemetryTraceReporter and OpenTelemetryMetricReporter,FLINK-34290,13566686,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Do,pnowojski,lincoln.86xy,lincoln.86xy,30/Jan/24 13:06,06/Feb/24 09:25,04/Jun/24 20:40,05/Feb/24 17:11,1.19.0,,,,,,,1.19.0,,,,Runtime / Metrics,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34291,,FLINK-34289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 17:11:46 UTC 2024,,,,,,,,,,"0|z1n3c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 17:11;pnowojski;Covered by https://issues.apache.org/jira/browse/FLINK-34289;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33695 Introduce TraceReporter and use it to create checkpointing and recovery traces ,FLINK-34289,13566685,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pnowojski,lincoln.86xy,lincoln.86xy,30/Jan/24 13:05,08/Feb/24 17:00,04/Jun/24 20:40,06/Feb/24 08:58,1.19.0,,,,,,,1.19.0,,,,Runtime / Metrics,,,,0,,,,,"This ticket covers testing three related features: FLINK-33695, FLINK-33735 and FLINK-33696.

Instructions:
#  Configure Flink to use [Slf4jTraceReporter|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/trace_reporters/#slf4j] and with enabled *INFO* level logging (can be to console or to a file, doesn't matter).
# Start a streaming job with enabled checkpointing.
# Let it run for a couple of checkpoints.
# Verify presence of a single *JobInitialization* [1] trace logged just after job start up.
# Verify presence of a couple of *Checkpoint* [1] traces logged after each successful or failed checkpoint.

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/ops/traces/#checkpointing-and-initialization",,,,,,,,,,,,,,,,,,,,FLINK-34290,FLINK-34387,FLINK-34288,,,,,,,,,,,,,,,,,,,,"08/Feb/24 06:22;pnowojski;Screenshot 2024-02-08 at 07.22.19.png;https://issues.apache.org/jira/secure/attachment/13066595/Screenshot+2024-02-08+at+07.22.19.png","05/Feb/24 14:40;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066465/screenshot-1.png","08/Feb/24 15:39;lincoln.86xy;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13066606/screenshot-2.png",,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 08 17:00:53 UTC 2024,,,,,,,,,,"0|z1n3bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:40;lincoln.86xy;[~pnowojski] Can you help confirm if this feature(also include other two related flips) needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","05/Feb/24 16:59;pnowojski;Thanks! I will create a single ticket for release testing instructions to cover all 3 FLIPs.;;;","06/Feb/24 08:58;lincoln.86xy;[~pnowojski] I restored the title of this Instructions ticket and closed it(following the new steps in this release and trying not to confuse others).
Testing will'be tracked by https://issues.apache.org/jira/browse/FLINK-34387

;;;","06/Feb/24 12:55;pnowojski;Thanks for cleaning this up. Indeed I was confused what to do and decided to follow an example from someone else.

Anyway, after giving it a second thought, I think there is no need to cross team test this feature. I've closed FLINK-34387.;;;","06/Feb/24 13:17;lincoln.86xy;Thanks [~pnowojski] :)
The adjustment to this release testing process was intended to improve the fact that when using a single ticket, 
non-committers don't have permission to change the status to 'unassigned', and other volunteers may not be able to determine whether the ticket in the current 'assigned' status still needs a volunteer.
Another consideration is that the release wiki page did not get good feedback on collecting testing requirements in advance, so we decided to try it out after discussion, and will continue to improve the process later. ;;;","06/Feb/24 13:29;pnowojski;I wasn't complaining, just explaining myself :) Sorry for misunderstanding what I should have done!;;;","06/Feb/24 14:27;lincoln.86xy;[~pnowojski] Don't get me wrong :) I'm trying to add what this adjustment is trying to address, because if an old flinker will confuse as well, then the process really should still be improved in later releases, so as a 1.19 release manager,  I'm wondering what part of the description could be clearer? (I should have asked this directly in my previous reply);;;","08/Feb/24 06:32;pnowojski; !Screenshot 2024-02-08 at 07.22.19.png|width=600! 

This part was confusing to me. My thought process was:
# Ok first box says, I should create a *new* ticket.
# But following the arrow, I should *remove* ""Instructions"". That's slightly suggested to rename the ticket.
# The truly confusing part. The box on the right, shows an arrow from ""X Instructions: Verify"" to ""X: Verify"". I've interpreted it as a subtask issue first, supporting ""create new ticket"" interpretation. But I couldn't create sub-task. So maybe that arrow is supposed to represent action of renaming thet ticket?
# I've checked that in FLINK-34285 someone else has already renamed ticket by removing ""Instructions"" so that further supported my interpretation that I should rename it as well.

Rephrasing the instruction to clarify the meaning of {{remove ""Instructions""}} and labeling the arrow between ""Instructions"" and non ""Instructions"" tickets, to state that for example it represents a linked ticket would help me avoid this mistake.;;;","08/Feb/24 15:54;lincoln.86xy;[~pnowojski] Really appreciate for your valuable inputs! I see now where the description is confusing, the problem occurred after I changed the 'Clone the ticket' to 'Create a new ticket'(because I found non-committer can not modify the 'Assignee' of the new cloned ticket).
And as you mentioned, we can't create a sub-task from the current ticket(should create from the parent umbrella ticket).

Since the rules have been finalized this time and changing them now could bring more confusion(we will discuss it after the release completed, to find a more convenient and clear way to do it), I've revised the process description according to the points you mentioned, can you help check if some place still not clearly? Thanks again

 !screenshot-2.png! 
;;;","08/Feb/24 17:00;pnowojski;That definitely would prevent my confusion :) ;;;",,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33735 Improve the exponential-delay restart-strategy ,FLINK-34288,13566684,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,lincoln.86xy,lincoln.86xy,30/Jan/24 13:03,06/Feb/24 09:25,04/Jun/24 20:40,06/Feb/24 06:57,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34289,FLINK-34384,FLINK-34287,,,,,,,,,,,,,,,,,,,,"06/Feb/24 06:51;fanrui;image-2024-02-06-14-51-11-256.png;https://issues.apache.org/jira/secure/attachment/13066490/image-2024-02-06-14-51-11-256.png","05/Feb/24 14:33;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066464/screenshot-1.png",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 06:57:57 UTC 2024,,,,,,,,,,"0|z1n3bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 14:39;lincoln.86xy;[~fanrui] Can you help confirm if this feature(and another two https://issues.apache.org/jira/browse/FLINK-34308 & https://issues.apache.org/jira/browse/FLINK-34309) needs cross-team testing?
If it doesn't, you can just close the ticket, otherwise follow the steps to prepare the test instructions and create a new ticket
 !screenshot-1.png! ;;;","06/Feb/24 06:52;fanrui;Thanks [~lincoln.86xy]  for the reminder.

 

Test suggestion:
 # Prepare a datastream job that all tasks throw exception directly.
 ## Set the parallelism to 5 or above
 # Prepare some configuration options:
 ** restart-strategy.type : exponential-delay
 ** restart-strategy.exponential-delay.attempts-before-reset-backoff : 7
 # Start the cluster: ./bin/start-cluster.sh
 # Run the job: ./bin/flink run -c className jarName
 # Check the result
 ** Check whether job will be retried 7 times
 ** Check the exception history, the list has 7 exceptions
 ** Each retries except the last one can see the 5 subtasks(They are concurrent exceptions).

!image-2024-02-06-14-51-11-256.png!

 

Note: Set these options mentioned at step2 at 2 level separately
 * Cluster level: set them in the config.yaml
 * Job level: Set them in the code

 

Job level demo:
{code:java}
public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();

    conf.setString(""restart-strategy"", ""exponential-delay"");
    conf.setString(""restart-strategy.exponential-delay.attempts-before-reset-backoff"", ""6"");
    StreamExecutionEnvironment env =  StreamExecutionEnvironment.getExecutionEnvironment(conf);
    env.setParallelism(5);

    DataGeneratorSource<Long> generatorSource =
            new DataGeneratorSource<>(
                    value -> value,
                    300,
                    RateLimiterStrategy.perSecond(10),
                    Types.LONG);

    env.fromSource(generatorSource, WatermarkStrategy.noWatermarks(), ""Data Generator"")
            .map(new RichMapFunction<Long, Long>() {
                @Override
                public Long map(Long value) {
                    throw new RuntimeException(
                            ""Excepted testing exception, subtaskIndex: "" + getRuntimeContext().getIndexOfThisSubtask());
                }
            })
            .print();

    env.execute();
} {code};;;","06/Feb/24 06:57;lincoln.86xy;[~fanrui] Thanks for updating the instructions!
Testing tracked by https://issues.apache.org/jira/browse/FLINK-34384;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing Instructions: Verify FLINK-33768  Support dynamic source parallelism inference for batch jobs ,FLINK-34287,13566683,13566678,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xiasun,lincoln.86xy,lincoln.86xy,30/Jan/24 13:02,06/Feb/24 09:25,04/Jun/24 20:40,05/Feb/24 05:13,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34288,FLINK-34356,,,,,,,,,,,,,,,,,,,,,"05/Feb/24 05:05;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066460/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 05:07:04 UTC 2024,,,,,,,,,,"0|z1n3bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 04:19;xiasun;This issue aims to verify [FLIP-379|https://cwiki.apache.org/confluence/display/FLINK/FLIP-379%3A+Dynamic+source+parallelism+inference+for+batch+jobs].

New Source can implement the interface DynamicParallelismInference to enable dynamic parallelism inference. For detailed information, please refer to the [documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/#automatically-decide-parallelisms-for-operators].

We may need to cover the following two types of test cases:
Test 1: FileSource has implemented the dynamic source parallelism inference. Test the automatic parallelism inference of FileSource.
Test 2: Test the dynamic source parallelism inference of a custom Source.;;;","05/Feb/24 05:07;lincoln.86xy;[~xiasun] Thanks for updating the test instructions!
I've created a new jira to track the testing result following the crossteam testing steps
 !screenshot-1.png! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attach cluster config map labels at creation time,FLINK-34286,13566681,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,30/Jan/24 12:25,31/Jan/24 10:23,04/Jun/24 20:40,31/Jan/24 10:23,,,,,,,,1.19.0,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,,"We attach a set of labels to config maps that we create to ease the manual cleanup by users in case Flink fails unrecoverably.

For cluster config maps (that are used for leader election), these labels are not set at creation time, but when leadership is acquired, in contrast to job config maps.

This means there's a gap where we create a CM without any labels being attached, and should Flink fail before leadership can be acquired it will continue to lack labels indefinitely.

AFAICT it should be straight-forward, at least API-wise, to set these labels at creation time. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 10:23:41 UTC 2024,,,,,,,,,,"0|z1n3aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 10:23;chesnay;master: b5a2ee4e9889f248840681544fe55e22ea097305;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Test Flink Release 1.19,FLINK-34285,13566678,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jan/24 12:14,28/Feb/24 01:27,04/Jun/24 20:40,28/Feb/24 01:27,1.19.0,,,,,,,1.19.0,,,,Tests,,,,0,release-testing,,,,"This is an umbrella ticket for the Flink 1.19 testing efforts. 
There're two kinds of ticket: 'Release Testing Instructions' and 'Release Testing' ones.

h2. Release Testing Instructions
For the Release Testing Instructions ticket (title starts with 'Release Testing Instructions:', 
please follow the steps:

 !screenshot-3.png! 

1. Whether the feature needs a crossteam testing, if no, authors just close the ticket

2. If testing is required, the author should prepare user document[must have] and additional instructions (if exists, which are thought necessary for testers, e.g., some limitations that are outside the scope of the design)

3. After No.2 is done, the author should close the jira and clone/create a new jira for tracking the testing result(keep unassigned or assign to a  volunteer)

h3. Other features need cross-team testing
Also contributors are encouraged to create tickets if there are other ones that need to be cross-team tested (Just create new ticket for testing using title 'Release Testing: Verify ...' without 'Instructions' keyword). 

h2. Release Testing
Note:  All the testing sub-tasks should be opened with:
Priority: Blocker
Fix Version: 1.19.0
Label: release-testing",,,,,,,,,,,,,,,,,,,FLINK-34279,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/24 03:21;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066449/screenshot-1.png","04/Feb/24 05:59;lincoln.86xy;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13066456/screenshot-2.png","08/Feb/24 15:56;lincoln.86xy;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13066607/screenshot-3.png",,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 01:27:00 UTC 2024,,,,,,,,,,"0|z1n3a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 01:27;lincoln.86xy;Close it as all sub-tasks finished. Thanks to all contributors!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Submit Software License Grant to ASF,FLINK-34284,13566654,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,leonard,leonard,leonard,30/Jan/24 10:02,27/Feb/24 03:53,04/Jun/24 20:40,27/Feb/24 03:53,,,,,,,,,,,,Flink CDC,,,,0,,,,,"As ASF software license grant[1] required, we need submit the Software Grant Agreement.

[1] https://www.apache.org/licenses/contributor-agreements.html#grants",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 03:53:52 UTC 2024,,,,,,,,,,"0|z1n34w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/24 03:53;leonard;Ververica has signed the Software License Grant and submitted to  secretary@apache.org.

The  Apache Software Foundation officer Craig L Russell  has filed it in the Apache Software Foundation records: Software Grant from Ververica GmbH
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify that no exclusions were erroneously added to the japicmp plugin,FLINK-34283,13566651,13566643,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,lincoln.86xy,lincoln.86xy,30/Jan/24 09:49,18/Mar/24 07:58,04/Jun/24 20:40,18/Mar/24 07:58,,,,,,,,,,,,,,,,0,,,,,"Verify that no exclusions were erroneously added to the japicmp plugin that break compatibility guarantees. Check the exclusions for the japicmp-maven-plugin in the root pom (see [apache/flink:pom.xml:2175ff|https://github.com/apache/flink/blob/3856c49af77601cf7943a5072d8c932279ce46b4/pom.xml#L2175] for exclusions that:
* For minor releases: break source compatibility for {{@Public}} APIs
* For patch releases: break source/binary compatibility for {{@Public}}/{{@PublicEvolving}}  APIs
Any such exclusion must be properly justified, in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 07:58:04 UTC 2024,,,,,,,,,,"0|z1n348:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 07:58;lincoln.86xy;One expected modification within 1.19: https://issues.apache.org/jira/browse/FLINK-28050

Another modification for 2.0(also affect 1.20): https://issues.apache.org/jira/browse/FLINK-33905;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a release branch,FLINK-34282,13566650,13566643,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jan/24 09:49,21/May/24 13:40,04/Jun/24 20:40,06/Mar/24 13:59,1.19.0,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,"If you are doing a new minor release, you need to update Flink version in the following repositories and the [AzureCI project configuration|https://dev.azure.com/apache-flink/apache-flink/]:
 * [apache/flink|https://github.com/apache/flink]
 * [apache/flink-docker|https://github.com/apache/flink-docker]
 * [apache/flink-benchmarks|https://github.com/apache/flink-benchmarks]

Patch releases don't require the these repositories to be touched. Simply checkout the already existing branch for that version:
{code:java}
$ git checkout release-$SHORT_RELEASE_VERSION
{code}
h4. Flink repository

Create a branch for the new version that we want to release before updating the master branch to the next development version:
{code:bash}
$ cd ./tools
tools $ releasing/create_snapshot_branch.sh
tools $ git checkout master
tools $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$NEXT_SNAPSHOT_VERSION releasing/update_branch_version.sh
{code}
In the {{master}} branch, add a new value (e.g. {{v1_16(""1.16"")}}) to [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] as the last entry:
{code:java}
// ...
v1_12(""1.12""),
v1_13(""1.13""),
v1_14(""1.14""),
v1_15(""1.15""),
v1_16(""1.16"");
{code}

Additionally in master, update the branch list of the GitHub Actions nightly workflow (see [apache/flink:.github/workflows/nightly-trigger.yml#L31ff|https://github.com/apache/flink/blob/master/.github/workflows/nightly-trigger.yml#L31]): The two most-recent releases and master should be covered.

The newly created branch and updated {{master}} branch need to be pushed to the official repository.
h4. Flink Docker Repository

Afterwards fork off from {{dev-master}} a {{dev-x.y}} branch in the [apache/flink-docker|https://github.com/apache/flink-docker] repository. Make sure that [apache/flink-docker:.github/workflows/ci.yml|https://github.com/apache/flink-docker/blob/dev-master/.github/workflows/ci.yml] points to the correct snapshot version; for {{dev-x.y}} it should point to {{{}x.y-SNAPSHOT{}}}, while for {{dev-master}} it should point to the most recent snapshot version (\{[$NEXT_SNAPSHOT_VERSION}}).

After pushing the new minor release branch, as the last step you should also update the documentation workflow to also build the documentation for the new release branch. Check [Managing Documentation|https://cwiki.apache.org/confluence/display/FLINK/Managing+Documentation] on details on how to do that. You may also want to manually trigger a build to make the changes visible as soon as possible.

h4. Flink Benchmark Repository
First of all, checkout the {{master}} branch to {{dev-x.y}} branch in [apache/flink-benchmarks|https://github.com/apache/flink-benchmarks], so that we can have a branch named {{dev-x.y}} which could be built on top of (${{CURRENT_SNAPSHOT_VERSION}}).

Then, inside the repository you need to manually update the {{flink.version}} property inside the parent *pom.xml* file. It should be pointing to the most recent snapshot version ($NEXT_SNAPSHOT_VERSION). For example:
{code:xml}
<flink.version>1.18-SNAPSHOT</flink.version>
{code}

h4. AzureCI Project Configuration
The new release branch needs to be configured within AzureCI to make azure aware of the new release branch. This matter can only be handled by Ververica employees since they are owning the AzureCI setup.
 
----
h3. Expectations (Minor Version only if not stated otherwise)
 * Release branch has been created and pushed
 * Changes on the new release branch are picked up by [Azure CI|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary]
 * {{master}} branch has the version information updated to the new version (check pom.xml files and 
 * [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] enum)
 *  [apache/flink:.github/workflows/nightly-trigger.yml#L31ff|https://github.com/apache/flink/blob/master/.github/workflows/nightly-trigger.yml#L31] should have the new release branch included
 * New version is added to the [apache-flink:flink-annotations/src/main/java/org/apache/flink/FlinkVersion|https://github.com/apache/flink/blob/master/flink-annotations/src/main/java/org/apache/flink/FlinkVersion.java] enum.
 * Make sure [flink-docker|https://github.com/apache/flink-docker/] has {{dev-x.y}} branch and docker e2e tests run against this branch in the corresponding Apache Flink release branch (see [apache/flink:flink-end-to-end-tests/test-scripts/common_docker.sh:51|https://github.com/apache/flink/blob/master/flink-end-to-end-tests/test-scripts/common_docker.sh#L51])
 * [apache-flink:docs/config.toml|https://github.com/apache/flink/blob/release-1.17/docs/config.toml] has been updated appropriately in the new Apache Flink release branch.
 * The {{flink.version}} property (see [apache/flink-benchmarks:pom.xml|https://github.com/apache/flink-benchmarks/blob/master/pom.xml#L48] of Flink Benchmark repo has been updated to the latest snapshot version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34411,,,,,,,,,"07/Feb/24 07:26;lincoln.86xy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13066539/screenshot-1.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 21 13:40:27 UTC 2024,,,,,,,,,,"0|z1n340:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 07:36;lincoln.86xy;(Record this for validating the release steps)

After pushing the new release branch release-1.19,  according to the Managing Documentation details manually trigger a  release-1.19 doc build, but the build action doesn't contain release-1.19 
 https://github.com/apache/flink/actions/runs/7811065494
 !screenshot-1.png! 

So I was wondering the similar update like https://github.com/apache/flink/pull/23258 is also necessary for this step? (but seems there was some discussions on when to merge the pr) 
cc [~jingge] [~renqs] [~snuyanzin] since you're the author/reviewers.

;;;","07/Feb/24 10:01;lincoln.86xy;Martijn has helped to confirm that this is a necessary step, and already updated master with commit: eb1f7c8f998c4befa0bdee0743f81cc8dbac71d8;;;","07/Feb/24 10:03;lincoln.86xy;Note: the AzureCI project configuration part is outdated, should follow the lastest version: https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release;;;","07/Feb/24 12:18;mapohl;fyi: The GHA nightly workflow branch configuration was not properly modified. I created [PR #24282|https://github.com/apache/flink/pull/24282] to cover this;;;","07/Feb/24 13:47;lincoln.86xy;[~mapohl] Thanks for fixing it!;;;","07/Feb/24 14:46;lincoln.86xy;Record the related commits that without prs:

master branch version update:  b8b2596a1e4cb726674949b031f577079832f42c
add 1.19 to docs.yml for doc build:  eb1f7c8f998c4befa0bdee0743f81cc8dbac71d8
 ;;;","09/Feb/24 10:35;mapohl;The {{dev-1.19}} branch in [apache/flink-docker|https://github.com/apache/flink-docker/tree/dev-1.19] is based on {{master}} rather than {{{}dev-master{}}}. This causes issues like FLINK-34411.

[~lincoln.86xy] [~martijnvisser] [~jingge] [~yunta]  Are there any objections against force-pushing a rebase? ...since was only created 2 days ago.;;;","09/Feb/24 11:00;martijnvisser;[~mapohl] You mean force-pushing `dev-1.19` based on `dev-master` on flink/docker? I don't think that's an issue at all. ;;;","09/Feb/24 11:02;mapohl;correct, I will go ahead and do that then.;;;","09/Feb/24 11:16;jingge;[~mapohl] please go ahead, thanks!;;;","09/Feb/24 11:24;mapohl;I rebased {{dev-1.19}} to {{dev-master~1}} ([44f05828|https://github.com/apache/flink-docker/commit/44f058287cc956a620b12b6f8ed214e44dc3db77])

There's another open [PR #180|https://github.com/apache/flink-docker/pull/180] to ""fix"" the {{master}} branch.;;;","13/Feb/24 08:55;jingge;[AzureCI project configuration|https://dev.azure.com/apache-flink/apache-flink/]: 
 # go to flink-ci.flink-master-mirror
 # click Edit
 # click More Actions(three dots)
 # click Triggers
 # click 0:15 under scheduled
 # add refs/heads/release-1.19 into the branch filters
 # remove refs/heads/release-1.16;;;","13/Feb/24 09:56;martijnvisser;Nice, thanks [~jingge]!;;;","21/May/24 13:40;lincoln.86xy;One more item needed: Update upgrade compatibility table (docs/ops/upgrading.md). https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release

;;;",,,,,,,,,,,,,,,,,,,
Select executing Release Manager,FLINK-34281,13566649,13566643,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jan/24 09:49,12/Mar/24 05:57,04/Jun/24 20:40,12/Mar/24 05:57,1.19.0,,,,,,,1.19.0,,,,Release System,,,,0,,,,,"h4. GPG Key

You need to have a GPG key to sign the release artifacts. Please be aware of the ASF-wide [release signing guidelines|https://www.apache.org/dev/release-signing.html]. If you don’t have a GPG key associated with your Apache account, please create one according to the guidelines.

Determine your Apache GPG Key and Key ID, as follows:
{code:java}
$ gpg --list-keys
{code}
This will list your GPG keys. One of these should reflect your Apache account, for example:
{code:java}
--------------------------------------------------
pub   2048R/845E6689 2016-02-23
uid                  Nomen Nescio <anonymous@apache.org>
sub   2048R/BA4D50BE 2016-02-23
{code}
In the example above, the key ID is the 8-digit hex string in the {{pub}} line: {{{}845E6689{}}}.

Now, add your Apache GPG key to the Flink’s {{KEYS}} file in the [Apache Flink release KEYS file|https://dist.apache.org/repos/dist/release/flink/KEYS] repository at [dist.apache.org|http://dist.apache.org/]. Follow the instructions listed at the top of these files. (Note: Only PMC members have write access to the release repository. If you end up getting 403 errors ask on the mailing list for assistance.)

Configure {{git}} to use this key when signing code by giving it your key ID, as follows:
{code:java}
$ git config --global user.signingkey 845E6689
{code}
You may drop the {{--global}} option if you’d prefer to use this key for the current repository only.

You may wish to start {{gpg-agent}} to unlock your GPG key only once using your passphrase. Otherwise, you may need to enter this passphrase hundreds of times. The setup for {{gpg-agent}} varies based on operating system, but may be something like this:
{code:bash}
$ eval $(gpg-agent --daemon --no-grab --write-env-file $HOME/.gpg-agent-info)
$ export GPG_TTY=$(tty)
$ export GPG_AGENT_INFO
{code}
h4. Access to Apache Nexus repository

Configure access to the [Apache Nexus repository|https://repository.apache.org/], which enables final deployment of releases to the Maven Central Repository.
 # You log in with your Apache account.
 # Confirm you have appropriate access by finding {{org.apache.flink}} under {{{}Staging Profiles{}}}.
 # Navigate to your {{Profile}} (top right drop-down menu of the page).
 # Choose {{User Token}} from the dropdown, then click {{{}Access User Token{}}}. Copy a snippet of the Maven XML configuration block.
 # Insert this snippet twice into your global Maven {{settings.xml}} file, typically {{{}${HOME}/.m2/settings.xml{}}}. The end result should look like this, where {{TOKEN_NAME}} and {{TOKEN_PASSWORD}} are your secret tokens:
{code:xml}
<settings>
   <servers>
     <server>
       <id>apache.releases.https</id>
       <username>TOKEN_NAME</username>
       <password>TOKEN_PASSWORD</password>
     </server>
     <server>
       <id>apache.snapshots.https</id>
       <username>TOKEN_NAME</username>
       <password>TOKEN_PASSWORD</password>
     </server>
   </servers>
 </settings>
{code}

h4. Website development setup

Get ready for updating the Flink website by following the [website development instructions|https://flink.apache.org/contributing/improve-website.html].
h4. GNU Tar Setup for Mac (Skip this step if you are not using a Mac)

The default tar application on Mac does not support GNU archive format and defaults to Pax. This bloats the archive with unnecessary metadata that can result in additional files when decompressing (see [1.15.2-RC2 vote thread|https://lists.apache.org/thread/mzbgsb7y9vdp9bs00gsgscsjv2ygy58q]). Install gnu-tar and create a symbolic link to use in preference of the default tar program.
{code:bash}
$ brew install gnu-tar
$ ln -s /usr/local/bin/gtar /usr/local/bin/tar
$ which tar
{code}
 
----
h3. Expectations
 * Release Manager’s GPG key is published to [dist.apache.org|http://dist.apache.org/]
 * Release Manager’s GPG key is configured in git configuration
 * Release Manager's GPG key is configured as the default gpg key.
 * Release Manager has {{org.apache.flink}} listed under Staging Profiles in Nexus
 * Release Manager’s Nexus User Token is configured in settings.xml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-30 09:49:00.0,,,,,,,,,,"0|z1n33s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review Release Notes in JIRA,FLINK-34280,13566648,13566643,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jan/24 09:49,18/Mar/24 07:27,04/Jun/24 20:40,18/Mar/24 07:27,,,,,,,,,,,,,,,,0,,,,,"JIRA automatically generates Release Notes based on the {{Fix Version}} field applied to issues. Release Notes are intended for Flink users (not Flink committers/contributors). You should ensure that Release Notes are informative and useful.

Open the release notes from the version status page by choosing the release underway and clicking Release Notes.

You should verify that the issues listed automatically by JIRA are appropriate to appear in the Release Notes. Specifically, issues should:
 * Be appropriately classified as {{{}Bug{}}}, {{{}New Feature{}}}, {{{}Improvement{}}}, etc.
 * Represent noteworthy user-facing changes, such as new functionality, backward-incompatible API changes, or performance improvements.
 * Have occurred since the previous release; an issue that was introduced and fixed between releases should not appear in the Release Notes.
 * Have an issue title that makes sense when read on its own.

Adjust any of the above properties to the improve clarity and presentation of the Release Notes.

Ensure that the JIRA release notes are also included in the release notes of the documentation (see section ""Review and update documentation"").
h4. Content of Release Notes field from JIRA tickets 

To get the list of ""release notes"" field from JIRA, you can ran the following script using JIRA REST API (notes the maxResults limits the number of entries):
{code:bash}
curl -s https://issues.apache.org/jira//rest/api/2/search?maxResults=200&jql=project%20%3D%20FLINK%20AND%20%22Release%20Note%22%20is%20not%20EMPTY%20and%20fixVersion%20%3D%20${RELEASE_VERSION} | jq '.issues[]|.key,.fields.summary,.fields.customfield_12310192' | paste - - -
{code}
{{jq}}  is present in most Linux distributions and on MacOS can be installed via brew.

 
----
h3. Expectations
 * Release Notes in JIRA have been audited and adjusted",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 14:48:30 UTC 2024,,,,,,,,,,"0|z1n33k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 14:48;lincoln.86xy;Tried {{jq}}'s expression, which generates a draft release note in markdown format (but still needs to be sorted manually)
{code:bash}
curl -s  'https://issues.apache.org/jira/rest/api/2/search?maxResults=200&jql=project=FLINK%20AND%20%27Release%20Note%27%20is%20not%20EMPTY%20&fields=summary,components,customfield_12310192&fixVersion=${RELEASE_VERSION}' | jq -r '.issues[]|""### "" + (if (.fields.components | length) == 0 then ""TODO_ADD_COMPONENT"" else ((.fields.components | map(.name) | sort | reverse) | join("","")) end), ""#### "" + .fields.summary, ""##### ["" + .key + ""](https://issues.apache.org/jira/browse/"" + .key + "")"", .fields.customfield_12310192 + ""\n""' >  /tmp/rn-draft.md
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross team testing,FLINK-34279,13566647,13566643,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jan/24 09:48,18/Mar/24 07:27,04/Jun/24 20:40,18/Mar/24 07:27,,,,,,,,,,,,,,,,0,,,,,"For user facing features that go into the release we'd like to ensure they can actually _be used_ by Flink users. To achieve this the release managers ensure that an issue for cross team testing is created in the Apache Flink Jira. This can and should be picked up by other community members to verify the functionality and usability of the feature.
The issue should contain some entry points which enables other community members to test it. It should not contain documentation on how to use the feature as this should be part of the actual documentation. The cross team tests are performed after the feature freeze. Documentation should be in place before that. Those tests are manual tests, so do not confuse them with automated tests.
To sum that up:
 * User facing features should be tested by other contributors
 * The scope is usability and sanity of the feature
 * The feature needs to be already documented
 * The contributor creates an issue containing some pointers on how to get started (e.g. link to the documentation, suggested targets of verification)
 * Other community members pick those issues up and provide feedback
 * Cross team testing happens right after the feature freeze

 
----
h3. Expectations
 * Jira issues for each expected release task according to the release plan is created and labeled as {{{}release-testing{}}}.
 * All the created release-testing-related Jira issues are resolved and the corresponding blocker issues are fixed.",,,,,,,,,,,,,,,,,,FLINK-34285,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-30 09:48:59.0,,,,,,,,,,"0|z1n33c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review and update documentation,FLINK-34278,13566646,13566643,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lincoln.86xy,lincoln.86xy,30/Jan/24 09:48,18/Mar/24 07:26,04/Jun/24 20:40,18/Mar/24 07:26,1.19.0,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,"There are a few pages in the documentation that need to be reviewed and updated for each release.
 * Ensure that there exists a release notes page for each non-bugfix release (e.g., 1.5.0) in {{{}./docs/release-notes/{}}}, that it is up-to-date, and linked from the start page of the documentation.
 * Upgrading Applications and Flink Versions: [https://ci.apache.org/projects/flink/flink-docs-master/ops/upgrading.html]
 * ...

 
----
h3. Expectations
 * Update upgrade compatibility table ([apache-flink:./docs/content/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content/docs/ops/upgrading.md#compatibility-table] and [apache-flink:./docs/content.zh/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content.zh/docs/ops/upgrading.md#compatibility-table]).
 * Update [Release Overview in Confluence|https://cwiki.apache.org/confluence/display/FLINK/Release+Management+and+Feature+Plan]
 * (minor only) The documentation for the new major release is visible under [https://nightlies.apache.org/flink/flink-docs-release-$SHORT_RELEASE_VERSION] (after at least one [doc build|https://github.com/apache/flink/actions/workflows/docs.yml] succeeded).
 * (minor only) The documentation for the new major release does not contain ""-SNAPSHOT"" in its version title, and all links refer to the corresponding version docs instead of {{{}master{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-30 09:48:59.0,,,,,,,,,,"0|z1n334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Triage release-blocking issues in JIRA,FLINK-34277,13566645,13566643,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lincoln.86xy,lincoln.86xy,30/Jan/24 09:48,12/Mar/24 05:56,04/Jun/24 20:40,12/Mar/24 05:56,,,,,,,,,,,,,,,,0,,,,,"There could be outstanding release-blocking issues, which should be triaged before proceeding to build a release candidate. We track them by assigning a specific Fix version field even before the issue resolved.

The list of release-blocking issues is available at the version status page. Triage each unresolved issue with one of the following resolutions:
 * If the issue has been resolved and JIRA was not updated, resolve it accordingly.
 * If the issue has not been resolved and it is acceptable to defer this until the next release, update the Fix Version field to the new version you just created. Please consider discussing this with stakeholders and the dev@ mailing list, as appropriate.
 ** When using ""Bulk Change"" functionality of Jira
 *** First, add the newly created version to Fix Version for all unresolved tickets that have old the old version among its Fix Versions.
 *** Afterwards, remove the old version from the Fix Version.
 * If the issue has not been resolved and it is not acceptable to release until it is fixed, the release cannot proceed. Instead, work with the Flink community to resolve the issue.

 
----
h3. Expectations
 * There are no release blocking JIRA issues",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-30 09:48:59.0,,,,,,,,,,"0|z1n32w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a new version in JIRA,FLINK-34276,13566644,13566643,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,lincoln.86xy,lincoln.86xy,30/Jan/24 09:48,07/Feb/24 08:46,04/Jun/24 20:40,07/Feb/24 08:46,,,,,,,,,,,,,,,,0,,,,,"When contributors resolve an issue in JIRA, they are tagging it with a release that will contain their changes. With the release currently underway, new issues should be resolved against a subsequent future release. Therefore, you should create a release item for this subsequent release, as follows:
 # In JIRA, navigate to the [Flink > Administration > Versions|https://issues.apache.org/jira/plugins/servlet/project-config/FLINK/versions].
 # Add a new release: choose the next minor version number compared to the one currently underway, select today’s date as the Start Date, and choose Add.
(Note: Only PMC members have access to the project administration. If you do not have access, ask on the mailing list for assistance.)

 
----
h3. Expectations
 * The new version should be listed in the dropdown menu of {{fixVersion}} or {{affectedVersion}} under ""unreleased versions"" when creating a new Jira issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 08:46:27 UTC 2024,,,,,,,,,,"0|z1n32o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 08:33;mapohl;It appears that the Jira version {{1.20}} was already created (and used by FLINK-33611 and FLINK-34345 as fixed version). I modified the version's name to {{1.20.0}} to be on par with our Jira version scheme. We can close this issue here, I guess, after it was confirmed by the release managers.;;;","07/Feb/24 08:46;lincoln.86xy;[~mapohl] Thanks for the help! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepare Flink 1.19 Release,FLINK-34275,13566643,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jan/24 09:48,18/Mar/24 08:58,04/Jun/24 20:40,18/Mar/24 08:58,1.19.0,,,,,,,1.19.0,,,,Release System,,,,0,,,,,"This umbrella issue is meant as a test balloon for moving the [release documentation|https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release] into Jira.
h3. Prerequisites
h4. Environment Variables

Commands in the subtasks might expect some of the following enviroment variables to be set accordingly to the version that is about to be released:
{code:bash}
RELEASE_VERSION=""1.5.0""
SHORT_RELEASE_VERSION=""1.5""
CURRENT_SNAPSHOT_VERSION=""$SHORT_RELEASE_VERSION-SNAPSHOT""
NEXT_SNAPSHOT_VERSION=""1.6-SNAPSHOT""
SHORT_NEXT_SNAPSHOT_VERSION=""1.6""
{code}
h4. Build Tools

All of the following steps require to use Maven 3.8.6 and Java 8. Modify your PATH environment variable accordingly if needed.
h4. Flink Source
 * Create a new directory for this release and clone the Flink repository from Github to ensure you have a clean workspace (this step is optional).
 * Run {{mvn -Prelease clean install}} to ensure that the build processes that are specific to that profile are in good shape (this step is optional).

The rest of this instructions assumes that commands are run in the root (or {{./tools}} directory) of a repository on the branch of the release version with the above environment variables set.",,,,,,,,,,,,,,,,,,,,,,FLINK-31146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-30 09:48:59.0,,,,,,,,,,"0|z1n32g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerTest.testRequirementLowerBoundDecreaseAfterResourceScarcityBelowAvailableSlots times out,FLINK-34274,13566640,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,mapohl,mapohl,30/Jan/24 09:19,28/Feb/24 14:24,04/Jun/24 20:40,28/Feb/24 14:24,1.19.0,,,,,,,1.18.2,1.19.0,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"{code:java}
Jan 30 03:15:46 ""ForkJoinPool-420-worker-25"" #9746 daemon prio=5 os_prio=0 tid=0x00007fdfbb635800 nid=0x2dbd waiting on condition [0x00007fdf39528000]
Jan 30 03:15:46    java.lang.Thread.State: WAITING (parking)
Jan 30 03:15:46 	at sun.misc.Unsafe.park(Native Method)
Jan 30 03:15:46 	- parking to wait for  <0x00000000fe642548> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
Jan 30 03:15:46 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 30 03:15:46 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
Jan 30 03:15:46 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
Jan 30 03:15:46 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest$SubmissionBufferingTaskManagerGateway.waitForSubmissions(AdaptiveSchedulerTest.java:2225)
Jan 30 03:15:46 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.awaitJobReachingParallelism(AdaptiveSchedulerTest.java:1333)
Jan 30 03:15:46 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerTest.testRequirementLowerBoundDecreaseAfterResourceScarcityBelowAvailableSlots(AdaptiveSchedulerTest.java:1273)
Jan 30 03:15:46 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...] {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57086&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9893",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34272,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 14:24:30 UTC 2024,,,,,,,,,,"0|z1n31s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 21:24;dmvk;It's possible to hit the resource wait timeout now. We should disable the resourceWait whatsoever, unless it's what we want to test explicitly.;;;","19/Feb/24 07:41;lincoln.86xy;1.19: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57598&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef;;;","20/Feb/24 07:54;mapohl;master (1.20): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57627&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9762;;;","20/Feb/24 14:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57671&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9054;;;","23/Feb/24 07:13;mapohl;https://github.com/apache/flink/actions/runs/8013740652/job/21891466410#step:10:8915;;;","23/Feb/24 11:19;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57806&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=27247;;;","26/Feb/24 07:21;mapohl;https://github.com/apache/flink/actions/runs/8017223924/job/21901781678#step:10:8913;;;","28/Feb/24 11:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57918&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=26181;;;","28/Feb/24 14:24;mapohl;master: [bfaa75a418432162700996053319d1a5c1e72927|https://github.com/apache/flink/commit/bfaa75a418432162700996053319d1a5c1e72927]
1.19: [3c04316da8b2f1e2e2f602c17146fbf5220fe390|https://github.com/apache/flink/commit/3c04316da8b2f1e2e2f602c17146fbf5220fe390]
1.18: [9802d240b341633f460fe15a89a263f12e540fe4|https://github.com/apache/flink/commit/9802d240b341633f460fe15a89a263f12e540fe4];;;",,,,,,,,,,,,,,,,,,,,,,,,
git fetch fails,FLINK-34273,13566639,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,30/Jan/24 09:12,03/Jun/24 01:47,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,,Build System / CI,Test Infrastructure,,,0,test-stability,,,,"We've seen multiple {{git fetch}} failures. I assume this to be an infrastructure issue. This Jira issue is for documentation purposes.
{code:java}
error: RPC failed; curl 18 transfer closed with outstanding read data remaining
error: 5211 bytes of body are still expected
fetch-pack: unexpected disconnect while reading sideband packet
fatal: early EOF
fatal: fetch-pack: invalid index-pack output {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57080&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=5d6dc3d3-393d-5111-3a40-c6a5a36202e6&l=667",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 01:47:59 UTC 2024,,,,,,,,,,"0|z1n31k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 09:13;mapohl;* [https://dev.azure.com/apache-flink/web/build.aspx?pcguid=2d3c0ac8-fecf-45be-8407-6d87302181a9&builduri=vstfs%3a%2f%2f%2fBuild%2fBuild%2f57036&tracking_data=ew0KICAic291cmNlIjogIlNsYWNrUGlwZWxpbmVzQXBwIiwNCiAgInNvdXJjZV9ldmVudF9uYW1lIjogImJ1aWxkLmNvbXBsZXRlIg0KfQ%3d%3d]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57022&view=results] ;;;","31/Jan/24 15:06;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57137&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=62b722db-f9c1-511e-3103-7d995ee907ba&l=320;;;","01/Feb/24 07:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57172&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=62b722db-f9c1-511e-3103-7d995ee907ba&l=486;;;","05/Feb/24 07:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57270&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=e710e26d-e8c7-50a5-a888-d2762c5aa000&l=361;;;","08/Feb/24 12:01;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57389&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=61c73713-1b77-5132-1d22-4d746b4b06d8&l=396;;;","09/Feb/24 08:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57417&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=6d51823d-b341-5f58-cf42-40e574735727&l=359;;;","12/Feb/24 07:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57428&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=d4971026-aeb2-565b-1ecc-8469bdc7d605&l=343;;;","13/Feb/24 07:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57492&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=a47dd1b5-aa0a-596a-799b-05a053059d14;;;","14/Feb/24 11:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57521&view=logs&j=60960eae-6f09-579e-371e-29814bdd1adc&t=1fe608a4-e773-5ca0-5336-1c37a61b9f8d;;;","27/Feb/24 07:33;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57876&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=0da23115-68bb-5dcd-192c-bd4c8adebde1
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57876&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=61c73713-1b77-5132-1d22-4d746b4b06d8;;;","01/Mar/24 10:09;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57993&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=a47dd1b5-aa0a-596a-799b-05a053059d14&l=422;;;","01/Mar/24 10:10;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57994&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f000bd7b-5496-59d2-7808-b9415a705703&l=35;;;","05/Mar/24 08:59;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58058&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=0f26682b-f67a-50cf-830a-156eb5498577;;;","11/Mar/24 14:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58192&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=aacbbeee-ae57-548b-905c-a77a5153b2fb&l=349;;;","25/Mar/24 16:18;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58519&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=bc77b88f-20e6-5fb3-ac3b-0b6efcca48c5&l=406;;;","26/Mar/24 05:49;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58548&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=e710e26d-e8c7-50a5-a888-d2762c5aa000&l=316

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58548&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=4f435a4b-7107-5b76-bdff-206974a490f2&l=457;;;","07/Apr/24 02:23;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58754&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=61c73713-1b77-5132-1d22-4d746b4b06d8&l=277;;;","07/Apr/24 02:25;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58755&view=logs&j=26b84117-e436-5720-913e-3e280ce55cae&t=114e1a26-135e-5832-d3ea-5b589cffc172&l=387;;;","08/Apr/24 05:33;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58782&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c5b19363-f3ba-59e0-bfbf-73c4b9bde45c&l=264;;;","09/Apr/24 03:27;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58798&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=0f26682b-f67a-50cf-830a-156eb5498577&l=385;;;","15/Apr/24 02:15;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58895&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=2cec4644-024f-529b-9379-b711878ccf41&l=262

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58895&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=c853d405-a51b-5dcf-f438-2de530b016d4&l=269;;;","28/Apr/24 05:20;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59207&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=e46af1f3-6e88-5f8c-8976-a244d665959a&l=365;;;","02/May/24 16:33;rskraba;1.20 test_cron_adaptive_scheduler tests https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59303&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=bc77b88f-20e6-5fb3-ac3b-0b6efcca48c5&l=1068;;;","10/May/24 09:40;rskraba;* 1.20 test_cron_hadoop313 connect https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59424&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=a2aa31b1-3076-5dd3-ea01-4a81e1467181&l=384
* 1.20 test_ci misc https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59405&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=10163a1a-ea71-5414-a832-7701bff37ba3&l=380
;;;","17/May/24 12:24;rskraba;* 1.20 test_cron_adaptive_scheduler table https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59617&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=6d51823d-b341-5f58-cf42-40e574735727&l=980;;;","22/May/24 07:10;Weijie Guo;1.20 test_cron_adaptive_scheduler python
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59713&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=62b722db-f9c1-511e-3103-7d995ee907ba&l=263;;;","23/May/24 07:18;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59751&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=5d6dc3d3-393d-5111-3a40-c6a5a36202e6&l=295;;;","31/May/24 13:58;rskraba;* 1.18 test_ci connect https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59986&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=e46af1f3-6e88-5f8c-8976-a244d665959a&l=1200;;;","03/Jun/24 01:47;Weijie Guo;test_cron_hadoop313

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=60011&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=10163a1a-ea71-5414-a832-7701bff37ba3&l=971;;;",,,,
AdaptiveSchedulerClusterITCase failure due to MiniCluster not running,FLINK-34272,13566634,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dmvk,mapohl,mapohl,30/Jan/24 07:54,06/Feb/24 10:18,04/Jun/24 20:40,01/Feb/24 02:09,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57073&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9543]
{code:java}
 Jan 29 17:21:29 17:21:29.465 [ERROR] Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 12.48 s <<< FAILURE! -- in org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase
Jan 29 17:21:29 17:21:29.465 [ERROR] org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp -- Time elapsed: 8.599 s <<< ERROR!
Jan 29 17:21:29 java.lang.IllegalStateException: MiniCluster is not yet running or has already been shut down.
Jan 29 17:21:29 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Jan 29 17:21:29 	at org.apache.flink.runtime.minicluster.MiniCluster.getDispatcherGatewayFuture(MiniCluster.java:1118)
Jan 29 17:21:29 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:991)
Jan 29 17:21:29 	at org.apache.flink.runtime.minicluster.MiniCluster.getArchivedExecutionGraph(MiniCluster.java:840)
Jan 29 17:21:29 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.lambda$waitUntilParallelismForVertexReached$3(AdaptiveSchedulerClusterITCase.java:270)
Jan 29 17:21:29 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
Jan 29 17:21:29 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Jan 29 17:21:29 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.waitUntilParallelismForVertexReached(AdaptiveSchedulerClusterITCase.java:265)
Jan 29 17:21:29 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp(AdaptiveSchedulerClusterITCase.java:146)
Jan 29 17:21:29 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 29 17:21:29 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jan 29 17:21:29 
Jan 29 17:21:29 17:21:29.466 [ERROR] org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testCheckpointStatsPersistedAcrossRescale -- Time elapsed: 2.036 s <<< ERROR!
Jan 29 17:21:29 java.lang.IllegalStateException: MiniCluster is not yet running or has already been shut down.
Jan 29 17:21:29 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Jan 29 17:21:29 	at org.apache.flink.runtime.minicluster.MiniCluster.getDispatcherGatewayFuture(MiniCluster.java:1118)
Jan 29 17:21:29 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:991)
Jan 29 17:21:29 	at org.apache.flink.runtime.minicluster.MiniCluster.getExecutionGraph(MiniCluster.java:969)
Jan 29 17:21:29 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.lambda$testCheckpointStatsPersistedAcrossRescale$1(AdaptiveSchedulerClusterITCase.java:183)
Jan 29 17:21:29 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
Jan 29 17:21:29 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Jan 29 17:21:29 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testCheckpointStatsPersistedAcrossRescale(AdaptiveSchedulerClusterITCase.java:180)
Jan 29 17:21:29 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 29 17:21:29 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21883,,,,FLINK-34274,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 07:36:32 UTC 2024,,,,,,,,,,"0|z1n30g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 07:55;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57079&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9517]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57079&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9323]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57079&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9604;;;","30/Jan/24 08:01;mapohl;[~dmvk] can you have a look. It might be that your changes related to FLINK-33976 caused the instabilities.;;;","30/Jan/24 09:19;mapohl;FLINK-34274 is most-likely due to the same cause?;;;","30/Jan/24 15:39;mapohl;I checked the logs for the build in the Jira description. It sounds like a more serious issue where concurrent state transitions are happening. I'm gonna increase priority to blocker.
{code:java}
17:21:28,764 [flink-pekko.actor.default-dispatcher-11] WARN  org.apache.flink.runtime.minicluster.MiniCluster             [] - Error in MiniCluster. Shutting the MiniCluster down.
java.lang.IllegalStateException: State transitions must not be triggered while another state transition is in progress.
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-core-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1376) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToRestarting(AdaptiveScheduler.java:1068) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.Executing.maybeRescale(Executing.java:178) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.Executing.rescaleWhenCooldownPeriodIsOver(Executing.java:207) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.Executing.<init>(Executing.java:91) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.Executing$Factory.getState(Executing.java:331) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.Executing$Factory.getState(Executing.java:283) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1394) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToExecuting(AdaptiveScheduler.java:1019) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.handleExecutionGraphCreation(CreatingExecutionGraph.java:140) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.CreatingExecutionGraph.lambda$null$0(CreatingExecutionGraph.java:81) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:1345) ~[classes/:?]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$30(AdaptiveScheduler.java:1360) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451) ~[flink-rpc-akkabde5885c-cf2c-492a-ad84-375c89ad3c43.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-core-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451) ~[flink-rpc-akkabde5885c-cf2c-492a-ad84-375c89ad3c43.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218) ~[flink-rpc-akkabde5885c-cf2c-492a-ad84-375c89ad3c43.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85) ~[flink-rpc-akkabde5885c-cf2c-492a-ad84-375c89ad3c43.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168) ~[flink-rpc-akkabde5885c-cf2c-492a-ad84-375c89ad3c43.jar:1.19-SNAPSHOT]
        [...]{code};;;","30/Jan/24 17:25;mapohl;Ok, after having gone through the changes, I'd suspect the two hotfix commits for speeding up the test execution being the cause of the instabilities:

* FLINK-34272: [23377711|https://github.com/apache/flink/commit/23377711]
* FLINK-34274: [9699cf43|https://github.com/apache/flink/commit/9699cf43]

I created [a PR|https://github.com/apache/flink/pull/24236] where the two commits got reverted with AdaptiveScheduler being enabled for CI. I'm lowering the priority to Critical again because it's most likely a test issue.;;;","30/Jan/24 20:48;dmvk;The bug is quite critical and can take down production jobs. Moving back to blocker.;;;","30/Jan/24 21:15;dmvk;The root cause of FLINK-34274 is different, will address it separately.;;;","31/Jan/24 09:19;xuyangzhong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57144&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","31/Jan/24 15:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57136&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=9491;;;","31/Jan/24 15:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57136&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=9462;;;","31/Jan/24 15:32;mapohl;https://github.com/apache/flink/actions/runs/7725587638/job/21061725688#step:10:9171
https://github.com/apache/flink/actions/runs/7725621504/job/21061484491#step:10:9172
https://github.com/apache/flink/actions/runs/7725500052/job/21061413719#step:10:9172;;;","01/Feb/24 01:56;xuyangzhong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57161&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","01/Feb/24 02:09;dmvk;[~xuyangzhong] the CI run doesn't seem to be up to date with master where the fix landed, can you please rebase and try again?;;;","01/Feb/24 02:09;dmvk;master: 8e11b6005c954791879f8cfdc5d08daaaba95945;;;","01/Feb/24 07:36;mapohl;The following build does not contain the fix [8e11b600|https://github.com/apache/flink/commit/8e11b6005c954791879f8cfdc5d08daaaba95945], yet. The build is mentioned for documentation purposes, anyway:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57160&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9483;;;",,,,,,,,,,,,,,,,,,
Fix the potential failure test about GroupAggregateRestoreTest#AGG_WITH_STATE_TTL_HINT,FLINK-34271,13566625,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,30/Jan/24 05:58,31/Jan/24 13:33,04/Jun/24 20:40,31/Jan/24 13:33,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"The underlying reason is that a previous PR introduced a test with state TTL as follows in the SQL: 
{code:java}
.runSql(
        ""INSERT INTO sink_t SELECT /*+ STATE_TTL('source_t' = '4d') */""
                + ""b, ""
                + ""COUNT(*) AS cnt, ""
                + ""AVG(a) FILTER (WHERE a > 1) AS avg_a, ""
                + ""MIN(c) AS min_c ""
                + ""FROM source_t GROUP BY b""){code}
When the savepoint metadata was generated for the first time, the metadata recorded the time when a certain key was accessed. If the test is rerun after the TTL has expired, the state of this key in the metadata will be cleared, resulting in an incorrect test outcome.

To rectify this issue, I think the current tests in RestoreTestBase could be modified to regenerate a new savepoint metadata as needed every time. However, this seems to deviate from the original design purpose of RestoreTestBase.

For my test, I will work around this by removing the data ""consumedBeforeRestore"", as I am only interested in testing the generation of an expected JSON plan.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34053,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 13:32:59 UTC 2024,,,,,,,,,,"0|z1n2yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 08:43;xuyangzhong;BTW, I've noticed that the old tests for json plan changes (diff the json plan on a PR) are about to be completely removed due to FLINK-33421, but it seems that testing to check if the json plan has been affected by modifications from a PR is still necessary. This is because the new RestoreTestBase testing framework does not assess the risk of json plan modifications.

In the meantime, if some of the tests in RestoreTestBase fail due to explicit json plan incompatibility changes, is it possible to directly modify the failing tests in RestoreTestBase (by regenerating json plans and recreating savepoint metadata)?;;;","30/Jan/24 08:44;xuyangzhong;cc [~qingyue] [~dwysakowicz] [~bvarghese] ;;;","30/Jan/24 18:34;bvarghese;[~xuyangzhong] I think we may need to manually update the plan and savepoint in such cases.;;;","31/Jan/24 05:33;xuyangzhong;[~bvarghese] Okay. But we don’t have tests to cover potential changes about json plan , right?;;;","31/Jan/24 09:58;dwysakowicz;> But we don’t have tests to cover potential changes about json plan , right?

I'll need to double check if we need that check. After all we don't necessarily need to maintain exactly the same plan, but we want to make sure the job can be restored.

> I think the current tests in RestoreTestBase could be modified to regenerate a new savepoint metadata as needed every time

That's not an option. We must test with a savepoint from an older version. That's the entire idea of backwards compatibility.;;;","31/Jan/24 10:58;xuyangzhong;[~dwysakowicz] I understand and agree with you. That's why I just work around my test.;;;","31/Jan/24 13:32;qingyue;Fixed in master cb9e220c2291088459f0281aa8e8e8584436a9b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update connector developer-facing doc for FLIP-367 & 146,FLINK-34270,13566624,13553946,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,30/Jan/24 05:50,01/Feb/24 02:37,04/Jun/24 20:40,01/Feb/24 02:37,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,"Update the 'extension points' on User-defined Sources & Sinks page ([https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#extension-points]) about how to integrate with source parallelism setting with their custom connectors.

Since no doc exist for the new interfaces introduced in FLIP-146, which is closely related here, we'll update doc to cover them as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19719,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 02:37:58 UTC 2024,,,,,,,,,,"0|z1n2y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 02:37;libenchao;Fixed via 98e1d079f6e0689652b418b3ce2e271fc84bb55e (master);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync maven-shade-plugin across modules in flink-shaded ,FLINK-34269,13566602,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,30/Jan/24 00:21,30/Jan/24 00:21,04/Jun/24 20:40,,shaded-18.0,,,,,,,,,,,BuildSystem / Shaded,,,,0,,,,,Currently every module uses it's own version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-30 00:21:30.0,,,,,,,,,,"0|z1n2tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a test to verify if restore test exists for ExecNode,FLINK-34268,13566598,13415855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,30/Jan/24 00:05,07/Mar/24 16:45,04/Jun/24 20:40,07/Mar/24 09:21,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33421,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 07 09:21:05 UTC 2024,,,,,,,,,,"0|z1n2sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/24 09:21;dwysakowicz;Implemented in https://issues.apache.org/jira/browse/FLINK-34268;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python connector test fails when running on MacBook with m1 processor,FLINK-34267,13566589,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,29/Jan/24 21:25,09/Feb/24 10:01,04/Jun/24 20:40,09/Feb/24 10:00,,,,,,,,,,,,API / Python,Build System / CI,Connectors / Common,,0,pull-request-available,,,,"Attempt to execute lint_python.sh on m1 macbook fails while trying to install miniconda environment
{code}
=============installing environment=============
installing wget...
install wget... [SUCCESS]
installing miniconda...
download miniconda...
download miniconda... [SUCCESS]
installing conda...
tail: illegal offset -- +000000000000018838: Invalid argument
tail: illegal offset -- +000000000000018838: Invalid argument
/Users/apilipenko/Dev/flink-connector-aws/flink-python/dev/download/miniconda.sh: line 353: /Users/apilipenko/Dev/flink-connector-aws/flink-python/dev/.conda/preconda.tar.bz2: No such file or directory
upgrade pip...
./dev/lint-python.sh: line 215: /Users/apilipenko/Dev/flink-connector-aws/flink-python/dev/.conda/bin/python: No such file or directory
upgrade pip... [SUCCESS]
install conda ... [SUCCESS]
install miniconda... [SUCCESS]
installing python environment...
installing python3.7...
./dev/lint-python.sh: line 247: /Users/apilipenko/Dev/flink-connector-aws/flink-python/dev/.conda/bin/conda: No such file or directory
conda install 3.7 retrying 1/3
./dev/lint-python.sh: line 254: /Users/apilipenko/Dev/flink-connector-aws/flink-python/dev/.conda/bin/conda: No such file or directory
conda install 3.7 retrying 2/3
./dev/lint-python.sh: line 254: /Users/apilipenko/Dev/flink-connector-aws/flink-python/dev/.conda/bin/conda: No such file or directory
conda install 3.7 retrying 3/3
./dev/lint-python.sh: line 254: /Users/apilipenko/Dev/flink-connector-aws/flink-python/dev/.conda/bin/conda: No such file or directory
conda install 3.7 failed after retrying 3 times.            You can retry to execute the script again.
{code}","m1 MacBook Pro

MacOS 14.2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 09 10:00:57 UTC 2024,,,,,,,,,,"0|z1n2qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/24 10:00;gaborgsomogyi;[{{e6e1426}}|https://github.com/apache/flink-connector-shared-utils/commit/e6e14268b8316352031b25f4b67ed64dc142b683] on ci_utils;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Output ratios should be computed over the whole metric window instead of averaged,FLINK-34266,13566532,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,29/Jan/24 14:34,20/Feb/24 11:20,04/Jun/24 20:40,20/Feb/24 11:20,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,,"Currently Output ratios are computed during metric collection based on the current in/out metrics an stored as part of the collected metrics.

During evaluation the output ratios previously computed are then averaged together in the metric window. This however leads to incorrect computation due to the nature of the computation and averaging.

Example:
Let's look at a window operator that simply sorts and re-emits events in windows. During the window collection phase, output ratio will be computed and stored as 0. During the window computation the output ratio will be last_input_rate / window_size.  Depending on the last input rate observation this can be off when averaged into any direction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34213,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 11:20:54 UTC 2024,,,,,,,,,,"0|z1n2ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 14:34;gyfora;cc [~mxm] ;;;","29/Jan/24 14:48;mxm;The way I understand the code is that for every observation, we will store the total output rate of every vertex. During metric window evaluation, we will average all of those. That is in line with how all the code works.

I agree 100% that all metrics should be observed over the entire metric window. So rates should be computed by measuring the number of records produced at the start and at the end up the window, then subtracting them from each other.

This request seems analogue to FLINK-34213 but for rates instead of busy time. Is that fair to say?;;;","29/Jan/24 15:47;gyfora;Minor correction, I think we need to store the total sum of records out and in (not the rate) at every collection time. From this we can compute total input / total output over the window.

I agree this is the same issue as with the busy time but from different angles / metrics;;;","29/Jan/24 17:05;mxm;Yes, using the total sum and substracting the start from the end observation is the way to go for maximum precision. ;;;","20/Feb/24 11:20;gyfora;merged to main 0a7588c084e7197f73e659e763f4d82d2d94ae76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the doc of named parameters,FLINK-34265,13566523,13564274,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,29/Jan/24 13:55,26/Feb/24 11:26,04/Jun/24 20:40,26/Feb/24 11:26,,,,,,,,1.19.0,1.20.0,,,Documentation,Table SQL / Planner,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 05:23:12 UTC 2024,,,,,,,,,,"0|z1n2bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 05:23;fsk119;Merged into master: 26b1d1bbff590589c72af892fc22f80fa4ee1261
Merged into release-1.19: 0af2540dc30340742506b3c61850f1e2d25f4d72;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheuspushgateway's hosturl can use basicAuth certification login,FLINK-34264,13566521,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zackyoung,zackyoung,29/Jan/24 13:40,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,1.20.0,,,,Runtime / Metrics,,,,0,pull-request-available,,,,"The scene is as follows:

Pushgateway uses Basicauth to verify, while the current code does not implement permissions on Basicauth.

hostUrl eg: [https://username:password@localhost:9091|https://username:password@localhost:9091/]

If this proposal is approved, I will propose a PR improvement.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-29 13:40:47.0,,,,,,,,,,"0|z1n2bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Converting double to decimal may fail,FLINK-34263,13566512,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,caicancai,caicancai,29/Jan/24 12:44,20/May/24 07:13,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,API / Core,,,,0,pull-request-available,,,,"Converting double to decimal fails. When the value is infinity, an error will be reported when converting double to decimal.
{code:java}
package org.apache.flink.table.examples.java.basics;

import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.TableEnvironment;

/** The famous word count example that shows a minimal Flink SQL job in batch execution mode. */
public final class WordCountSQLExample {

    public static void main(String[] args) throws Exception {

        // set up the Table API
        final EnvironmentSettings settings =
                EnvironmentSettings.newInstance().inBatchMode().build();
        final TableEnvironment tableEnv = TableEnvironment.create(settings);

        // execute a Flink SQL job and print the result locally
        tableEnv.executeSql(
                        // define the aggregation
                        ""SELECT CAST(power(0,-3) AS DECIMAL)"")
                .print();
    }
}
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-29 12:44:39.0,,,,,,,,,,"0|z1n29c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to set user specified labels for Rest Service Object,FLINK-34262,13566494,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,prabhujoseph,prabhujoseph,29/Jan/24 09:57,29/Jan/24 09:57,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"Flink allows users to label JM and TM pods; the rest service object also requires labeling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-29 09:57:29.0,,,,,,,,,,"0|z1n25c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When slotmanager.redundant-taskmanager-num is set to a value greater than 1, redundant task managers may be repeatedly released and requested",FLINK-34261,13566490,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,easonqin,easonqin,easonqin,29/Jan/24 09:38,19/Feb/24 06:37,04/Jun/24 20:40,19/Feb/24 06:37,,,,,,,,,,,,,,,,0,,,,,"Redundant task managers are extra task managers started by Flink, to speed up job recovery in case of failures due to task manager lost. But when we configured 
{code:java}
slotmanager.redundant-taskmanager-num: 2 // any value greater than 1{code}
Flink will release and request redundant TM repeatedly.
h2. Root cause
 # When `slotmanager.redundant-taskmanager-num` is set to a value greater than 1 and one redundant Task Manager is registered, the `checkResourceRequirementsWithDelay()` function is called.
 # Within `checkResourceRequirementsWithDelay()`, if the `missingResources` list is empty, the `taskExecutorManager` will invoke `clearPendingTaskManagerSlots()` to clear the pending slots.
 # This may result in the release of other redundant Task Managers. In such cases, the `TaskExecutorManager` will request new redundant Task Managers when the existing ones are insufficient.

h2. Reproduce

We can reproduce this situation by using [Flink Kubernetes Operator (using minikube here)|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/], here is an example yaml file:

 
{code:java}
// redundant-tm.yaml
################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  ""License""); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: redundant-tm
spec:
  image: flink:1.18
  flinkVersion: v1_18
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: ""2""
    slotmanager.redundant-taskmanager-num: ""2""
    cluster.fine-grained-resource-management.enabled: ""false""
  serviceAccount: flink
  jobManager:
    resource:
      memory: ""1024m""
      cpu: 1
  taskManager:
    resource:
      memory: ""1024m""
      cpu: 1
  job:
    jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar
    parallelism: 3
    upgradeMode: stateless
  logConfiguration:
    log4j-console.properties: |+
      rootLogger.level = DEBUG
      rootLogger.appenderRef.file.ref = LogFile
      rootLogger.appenderRef.console.ref = LogConsole
      appender.file.name = LogFile
      appender.file.type = File
      appender.file.append = false
      appender.file.fileName = ${sys:log.file}
      appender.file.layout.type = PatternLayout
      appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n{code}
After executing:
{code:java}
kubectl create -f redundant-tm.yaml
kubectl port-forward svc/redundant-tm 8081{code}
We can find repeatedly release and request redundant TM in JM's log:
{code:java}
// release
2024-01-29 09:26:25,033 INFO org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Registering TaskManager with ResourceID redundant-tm-taskmanager-1-4 (pekko.tcp://flink@10.244.1.196:6122/user/rpc/taskmanager_0) at ResourceManager
2024-01-29 09:26:25,060 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Registering task executor redundant-tm-taskmanager-1-4 under 44c649b2d84e87cdd5e6c53971f8b877 at the slot manager.
2024-01-29 09:26:25,061 INFO org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Worker redundant-tm-taskmanager-1-4 is registered.
2024-01-29 09:26:25,061 INFO org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Worker redundant-tm-taskmanager-1-4 with resource spec WorkerResourceSpec {cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=2} was requested in current attempt. Current pending count after registering: 1.
2024-01-29 09:26:25,196 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Update resource declarations to [ResourceDeclaration{spec=WorkerResourceSpec {cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=2}, numNeeded=3, unwantedWorkers=[]}].
2024-01-29 09:26:25,196 INFO org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - need release 1 workers, current worker number 4, declared worker number 3
2024-01-29 09:26:25,199 INFO org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Stopping worker redundant-tm-taskmanager-1-3.
2024-01-29 09:26:25,199 INFO org.apache.flink.kubernetes.KubernetesResourceManagerDriver [] - Stopping TaskManager pod redundant-tm-taskmanager-1-3.
2024-01-29 09:26:25,203 INFO org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Worker redundant-tm-taskmanager-1-3 with resource spec WorkerResourceSpec {cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=2} was requested in current attempt and has not registered. Current pending count after removing: 0. 

......

// request
2024-01-29 09:26:31,622 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 12 as completed for source Source: Events Generator Source.2024-01-29 09:26:32,615 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.TaskExecutorManager [] - Allocating 1 task executors for redundancy.2024-01-29 09:26:32,685 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Update resource declarations to [ResourceDeclaration{spec=WorkerResourceSpec {cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=2}, numNeeded=4, unwantedWorkers=[]}].2024-01-29 09:26:32,685 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - need request 1 new workers, current worker number 3, declared worker number 42024-01-29 09:26:32,685 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Requesting new worker with resource spec WorkerResourceSpec {cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=2}, current pending count: 1. {code}
!image-2024-01-29-17-29-15-453.png!

The job has a parallelism of 3, which requires 4 Task Managers (2 for task execution and 2 for redundancy). However, we observed that the Task Manager ""redundant-tm-taskmanager-1-3"" was initially requested but later released.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/24 09:29;easonqin;image-2024-01-29-17-29-15-453.png;https://issues.apache.org/jira/secure/attachment/13066311/image-2024-01-29-17-29-15-453.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 06:37:31 UTC 2024,,,,,,,,,,"0|z1n24g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 06:37;easonqin;Close this issue because TaskExecutorManager has been removed from the master branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update flink-connector-aws to be compatible with updated SinkV2 interfaces,FLINK-34260,13566480,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,a.pilipenko,martijnvisser,martijnvisser,29/Jan/24 08:29,10/Feb/24 14:10,04/Jun/24 20:40,10/Feb/24 14:10,aws-connector-4.3.0,,,,,,,,,,,Connectors / AWS,,,,0,pull-request-available,,,,"https://github.com/apache/flink-connector-aws/actions/runs/7689300085/job/20951547366#step:9:798

{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-dynamodb: Compilation failure
Error:  /home/runner/work/flink-connector-aws/flink-connector-aws/flink-connector-aws/flink-connector-dynamodb/src/test/java/org/apache/flink/connector/dynamodb/sink/DynamoDbSinkWriterTest.java:[357,40] incompatible types: org.apache.flink.connector.base.sink.writer.TestSinkInitContext cannot be converted to org.apache.flink.api.connector.sink2.Sink.InitContext
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 10 14:10:17 UTC 2024,,,,,,,,,,"0|z1n228:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 10:22;jiabao.sun;Hi [~martijnvisser], can I take this ticket?;;;","29/Jan/24 14:07;dannycranmer;Hey [~jiabao.sun] , thanks for offering, but [~a.pilipenko] has already started on this. ;;;","29/Jan/24 15:24;jiabao.sun;Sorry [~dannycranmer], I just noticed this comment.
I made some attempts in this [PR-126|https://github.com/apache/flink-connector-aws/pull/126] , and it should work. 
However, since Aleksandr is also working on it, I closed it.
This change may not be the best solution, but I hope it can be helpful.;;;","01/Feb/24 09:32;a.pilipenko;Hey [~jiabao.sun], thank you for info, it was helpful.;;;","10/Feb/24 14:10;hong; merged commit [{{5b6f087}}|https://github.com/apache/flink-connector-aws/commit/5b6f087815bcf18cf62ba39b2ac1f84f5e72f951] into   apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-jdbc fails to compile with NPE on hasGenericTypesDisabled,FLINK-34259,13566477,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jiabao.sun,martijnvisser,martijnvisser,29/Jan/24 08:27,31/Jan/24 09:01,04/Jun/24 20:40,31/Jan/24 08:28,,,,,,,,jdbc-3.1.2,,,,Connectors / JDBC,,,,0,pull-request-available,,,,"https://github.com/apache/flink-connector-jdbc/actions/runs/7682035724/job/20935884874#step:14:150

{code:java}
Error:  Tests run: 10, Failures: 5, Errors: 4, Skipped: 0, Time elapsed: 7.909 s <<< FAILURE! - in org.apache.flink.connector.jdbc.JdbcRowOutputFormatTest
Error:  org.apache.flink.connector.jdbc.JdbcRowOutputFormatTest.testInvalidConnectionInJdbcOutputFormat  Time elapsed: 3.254 s  <<< ERROR!
java.lang.NullPointerException: Cannot invoke ""org.apache.flink.api.common.serialization.SerializerConfig.hasGenericTypesDisabled()"" because ""config"" is null
	at org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer(GenericTypeInfo.java:85)
	at org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer(GenericTypeInfo.java:99)
	at org.apache.flink.connector.jdbc.JdbcTestBase.getSerializer(JdbcTestBase.java:70)
	at org.apache.flink.connector.jdbc.JdbcRowOutputFormatTest.testInvalidConnectionInJdbcOutputFormat(JdbcRowOutputFormatTest.java:336)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
{code}

Seems to be caused by FLINK-34122 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 09:01:20 UTC 2024,,,,,,,,,,"0|z1n21k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 08:28;martijnvisser;[~Zhanghao Chen] Since you worked on FLINK-34122 can you please take a look at this ticket?;;;","30/Jan/24 12:44;jiabao.sun;[~martijnvisser]
It seems introduced by [FLINK-34090]
It doesn't seem to break public interfaces, and I think we only need to make some adjustments in the testing of the JDBC connector.;;;","30/Jan/24 12:47;martijnvisser;[~jiabao.sun] Something has changed on the Flink interfaces side, since the JDBC connector uses old interfaces that have been Public for a long time. It's a blocker for 1.19;;;","30/Jan/24 12:50;jiabao.sun;Thanks [~martijnvisser] , I will close the PR for the JDBC connector.;;;","30/Jan/24 13:05;jiabao.sun;[~martijnvisser] But I still have a question. In the previous changes by [FLINK-34090], there was no change in the compatibility of the public interface. Normally, when creating an ExecutionConfig object through the constructor of ExecutionConfig, a SerializerConfig object is also created, so the issue of NPE being thrown by hasGenericTypesDisabled should not occur. The NPE exception thrown in the JDBC connector test is mainly because the ExecutionConfig is mocked using Mockito, so serializerConfig.hasGenericTypesDisabled() will throw NPE. I'm not sure if this qualifies as breaking the public interface.;;;","30/Jan/24 13:23;Weijie Guo;I think [~jiabao.sun] has a point. It's not normally possible to get a null SerializerConfig from ExecutionConfig, which is sort of the expected invariant. But this mocks break that assumption, and it seems like the problem is only related to the JDBC Tests.;;;","30/Jan/24 13:28;jiabao.sun;The PR was reopened, PTAL.;;;","31/Jan/24 03:30;Weijie Guo;I think this is only related to testing, rather than a breaking changes. I will downgrade the priority, feel free to upgrade it if there are other problems.;;;","31/Jan/24 07:57;martijnvisser;If we've had a look and are confident we didn't break any interfaces, then I'm all good. Thanks for looking into it;;;","31/Jan/24 08:28;Weijie Guo;main: 611b45cfa6e30c6ee1f7f752971a19bff69e23c1

I am not sure how to choose the fix version about this one, especially the jdbc-connector version.;;;","31/Jan/24 09:01;martijnvisser;[~Weijie Guo] I've fixed it for you;;;",,,,,,,,,,,,,,,,,,,,,,
Incorrect example of accumulator usage within emitUpdateWithRetract for TableAggregateFunction,FLINK-34258,13566471,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,29/Jan/24 07:30,02/Feb/24 02:12,04/Jun/24 20:40,02/Feb/24 02:11,1.18.1,1.19.0,,,,,,1.19.0,,,,Documentation,Table SQL / API,,,0,pull-request-available,,,,"The [documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/udfs/#retraction-example] provides an example of using `emitUpdateWithRetract`. However, the example is misleading as it incorrectly suggests that the accumulator can be updated within the `emitUpdateWithRetract method`. In reality, the order of invocation is to first call `getAccumulator` and then `emitUpdateWithRetract`, which means that updating the accumulator within `emitUpdateWithRetract` will not take effect. Please see [GroupTableAggFunction#L141|https://github.com/apache/flink/blob/20450485b20cb213b96318b0c3275e42c0300e15/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupTableAggFunction.java#L141] ~ [GroupTableAggFunction#L146|https://github.com/apache/flink/blob/20450485b20cb213b96318b0c3275e42c0300e15/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupTableAggFunction.java#L146] for more details.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31788,,,"29/Jan/24 08:53;qingyue;GroupTableAggHandler$10.java;https://issues.apache.org/jira/secure/attachment/13066304/GroupTableAggHandler%2410.java",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 02:11:43 UTC 2024,,,,,,,,,,"0|z1n208:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 09:00;qingyue;The generated AggHandler can be found in the attachment.

To highlight the problem, I quoted the `getAccumulators` and `emitValue` methods here.

Even if `agg0_acc_external` got updated within `emitValue`, without `getAccumulators` converting `agg0_acc_external` to `agg0_acc_internal`, the `GroupTableAggFunction` would not get the updated accumulator.
{code:java}
    @Override
    public org.apache.flink.table.data.RowData getAccumulators() throws Exception {

        acc$3 = new org.apache.flink.table.data.GenericRowData(1);

        agg0_acc_internal = (org.apache.flink.table.data.RowData) converter$0.toInternalOrNull(
                (org.apache.flink.table.planner.runtime.utils.JavaUserDefinedTableAggFunctions.Top2Accumulator) agg0_acc_external);
        if (false) {
            acc$3.setField(0, null);
        } else {
            acc$3.setField(0, agg0_acc_internal);
        }

        return acc$3;

    }

    @Override
    public void emitValue(
            org.apache.flink.util.Collector<org.apache.flink.table.data.RowData> out,
            org.apache.flink.table.data.RowData key, boolean isRetract)
            throws Exception {

        convertCollector.reset(key, isRetract, out);
        function_org$apache$flink$table$planner$runtime$utils$JavaUserDefinedTableAggFunctions$IncrementalTop2
                .emitUpdateWithRetract(agg0_acc_external, convertCollector);
    }
    
 {code};;;","02/Feb/24 02:11;qingyue;Fixed in master 0779c91e581dc16c4aef61d6cc27774f11495907;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Update Flink YAML Parser to Support YAML 1.2 Specification,FLINK-34257,13566465,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,29/Jan/24 06:18,30/Jan/24 01:57,04/Jun/24 20:40,30/Jan/24 01:57,1.19.0,,,,,,,1.19.0,,,,API / Core,Runtime / Configuration,,,0,pull-request-available,,,,"FLINK-33364 and FLINK-33577 added snakeyaml and pyyaml dependencies to support a standard YAML parser. However, these parsers support the YAML 1.1 specification rather than the YAML 1.2 specification, which is the version referenced by [FLINK official website|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#configuration]. Therefore, we need to update these dependencies that support YAML 1.2.

The updated dependencies are as follows:

1. For Java: change from snakeyaml to snakeyaml-engine
2. For Python: change from pyyaml to ruamel.yaml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33364,FLINK-33577,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 01:57:12 UTC 2024,,,,,,,,,,"0|z1n1yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 01:57;zhuzh;Fixed via 081051a2cacaddf6dfe613da061f15f28a015a41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a documentation section for minibatch join,FLINK-34256,13566463,13565884,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xu_shuai_,xu_shuai_,xu_shuai_,29/Jan/24 06:11,06/Feb/24 08:09,04/Jun/24 20:40,06/Feb/24 08:09,1.19.0,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,We should add a minibatch join section in Performance Tuning to explain the usage and principle of minibatch-join.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 08:09:21 UTC 2024,,,,,,,,,,"0|z1n1yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 08:09;lsy;Merged in master branch: 1ebcaf5b34860026bfcf0ad78eaaf2847cce275c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-406: Reorganize State & Checkpointing & Recovery Configuration,FLINK-34255,13566461,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,29/Jan/24 05:47,22/Feb/24 02:54,04/Jun/24 20:40,,,,,,,,,1.20.0,2.0.0,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,"The FLIP: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=284789560

 

Currently, the configuration options pertaining to checkpointing, recovery, and state management are primarily grouped under the following prefixes:
 * *state.backend.** : configurations related to state accessing and checkpointing, as well as specific options for individual state backends
 * *execution.checkpointing.** : configurations associated with checkpoint execution and recovery
 * {*}execution.savepoint.*{*}: configurations for recovery from savepoint

In addition, there are several individual options such as _{{state.checkpoint-storage}}_ and _{{state.checkpoints.dir}}_ that fall outside of these prefixes. The current arrangement of these options, which span multiple modules, is somewhat haphazard and lacks a systematic structure. For example, the options under the {{_CheckpointingOptions_ }}and {{_ExecutionCheckpointingOptions_ }}are related and have no clear boundaries from the user's perspective, but there is no unified prefix for them. With the upcoming release of Flink 2.0, we have an excellent opportunity to overhaul and restructure the configurations related to checkpointing, recovery, and state management. This FLIP proposes to reorganize these settings, making it more coherent by module, which would significantly lower the barriers for understanding and reduce the development costs moving forward.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-29 05:47:40.0,,,,,,,,,,"0|z1n1y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`DESCRIBE` syntaxes like `DESCRIBE CATALOG xxx` throws strange exceptions,FLINK-34254,13566450,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xuyangzhong,xuyangzhong,29/Jan/24 03:17,06/Feb/24 01:43,04/Jun/24 20:40,06/Feb/24 01:43,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"Add the test following to CalcITCase to re-produce this bug.
{code:java}
@Test
  def test(): Unit = {
    tEnv.executeSql(s""""""
                       |create catalog `c_new` with (
                       |  'type' = 'generic_in_memory',
                       |  'default-database' = 'my_d'
                       |)
                       |"""""".stripMargin)

    tEnv
      .executeSql(s""""""
                     |show catalogs
                     |"""""".stripMargin)
      .print

    tEnv
      .executeSql(s""""""
                     | describe catalog default_catalog
                     |"""""".stripMargin)
      .print

  } {code}
Result:
{code:java}
+-----------------+
|    catalog name |
+-----------------+
|           c_new |
| default_catalog |
+-----------------+
2 rows in set
 org.apache.flink.table.api.ValidationException: SQL validation failed. From line 2, column 19 to line 2, column 33: Column 'default_catalog' not found in any table
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:200)	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:117)	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:259)	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)	at org.apache.flink.table.planner.runtime.stream.sql.CalcITCase.test(CalcITCase.scala:453)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)Caused by: org.apache.calcite.runtime.CalciteContextException: From line 2, column 19 to line 2, column 33: Column 'default_catalog' not found in any table	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:505)	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:932)	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:917)	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5276)	at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:273)	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateIdentifier(SqlValidatorImpl.java:3150)	at org.apache.calcite.sql.SqlIdentifier.validateExpr(SqlIdentifier.java:304)	at org.apache.calcite.sql.SqlOperator.validateCall(SqlOperator.java:474)	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateCall(SqlValidatorImpl.java:6005)	at org.apache.calcite.sql.SqlCall.validate(SqlCall.java:138)	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1009)	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:758)	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:196)	... 11 moreCaused by: org.apache.calcite.sql.validate.SqlValidatorException: Column 'default_catalog' not found in any table	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:505)	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:599)	... 23 more
{code}
The syntax `DESCRIBE DATABASE xxx` has the same bug.

 

It seems that this flip is not finished yet. https://issues.apache.org/jira/browse/FLINK-14686",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34370,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 01:42:57 UTC 2024,,,,,,,,,,"0|z1n1vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 01:42;xuyangzhong;I have created a Jira (https://issues.apache.org/jira/browse/FLINK-34370) linked this one to do the improvement. So close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offsets out of range with no configured reset policy for partitions,FLINK-34253,13566448,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,1028344078@qq.com,1028344078@qq.com,29/Jan/24 02:58,29/Jan/24 09:00,04/Jun/24 20:40,,1.14.4,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-table_2.12-1.14.4.jar:1.14.4] at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) [flink-table_2.12-1.14.4.jar:1.14.4] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_241] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_241] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_241] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_241] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_241] Caused by: org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {dp-oracle-sllv-0=12734616,"flink 1.14.4

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 09:00:11 UTC 2024,,,,,,,,,,"0|z1n1v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 09:00;martijnvisser;[~1028344078@qq.com] Have you configured an offset policy like documented at https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/#starting-offset ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WatermarkAssignerOperator should not emit WatermarkStatus.IDLE under continuous data flow,FLINK-34252,13566442,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dchristle,dchristle,dchristle,28/Jan/24 20:53,11/Mar/24 12:44,04/Jun/24 20:40,,1.16.3,1.17.2,1.18.1,,,,,1.18.2,1.20.0,,,Table SQL / Runtime,,,,0,pull-request-available,,,,"The WatermarkAssignerOperator in the table runtime incorrectly transitions to an IDLE state even when data is continuously flowing. This behavior, observed under normal operating conditions where the interval between data elements is shorter than the configured idleTimeout, leads to regular transitions between ACTIVE and IDLE states, which are unnecessary.

_Detail:_
In the current implementation, the lastRecordTime variable, which tracks the time of the last received data element, is updated only when the WatermarkStatus transitions from IDLE to ACTIVE. However, it is not updated when WatermarkStatus is ACTIVE, which means even under continuous data flow, the condition `(currentTime - lastRecordTime > idleTimeout)` will eventually always become true, and the WatermarkStatus will erroneously be marked IDLE. 

It is unclear to me if this bug produces any incorrectness downstream, since when the WatermarkStatus is in in the IDLE state, the next processElement will cause a WatermarkStatus.ACTIVE to be emitted. Nevertheless, we should eliminate this flip-flop behavior between states.

The test I wrote fails without the fix and illustrates the flip-flops:

{noformat}
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.030 s <<< FAILURE! -- in org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTest
[ERROR] org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorTest.testIdleStateAvoidanceWithConsistentDataFlow -- Time elapsed: 0.013 s <<< FAILURE!
java.lang.AssertionError:

Expecting
  [WatermarkStatus(IDLE),
    WatermarkStatus(ACTIVE),
    WatermarkStatus(IDLE),
    WatermarkStatus(ACTIVE),
    WatermarkStatus(IDLE),
    WatermarkStatus(ACTIVE),
    WatermarkStatus(IDLE),
    WatermarkStatus(ACTIVE),
    WatermarkStatus(IDLE)]
not to contain
  [WatermarkStatus(IDLE)]
but found
  [WatermarkStatus(IDLE)]
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22881,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 18 10:12:14 UTC 2024,,,,,,,,,,"0|z1n1ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 08:58;martijnvisser;[~dchristle] If I look at https://cwiki.apache.org/confluence/display/FLINK/FLIP-180%3A+Adjust+StreamStatus+and+Idleness+definition - Which of the situations are you referring to in your ticket? (Static assignment + temporary no data or Dynamic assignment + temporary no split);;;","29/Jan/24 22:54;dchristle;[~martijnvisser] The swapping of watermark between active/idle/active/idle appears to happen when the data stream is _not_ truly idle. The static vs temporary no data vs dynamic assignment cases are all times where a stream should be signaled as ""idle"" in some way. But here, a stream that is active is erroneously marked idle.

The `WatermarkAssignerOperator` isn't a Source, so it does not know which of the three cases is causing idleness. To detect idleness, the Operator takes `idleTimeout` as an argument, and compares it against the processing timestamp of the last record it received. The way it appears the Operator _should_ work is that if no record is incoming for longer than `idleTimeout`, it infers the stream is idle, and emits `WatermarkStatus.IDLE`. This logic is like how `withIdleness` works.

This makes sense: whether the reason for the idleness is a static assignment that made it so no records are received, a stream doesn't produce records for a while, or whether a split happens to not be assigned for a while due to dynamic assignment/not enough splits, all of these cases translate to the Operator observing no more records for too long of a time. When it hasn't seen any records for longer than `idleTimeout`, it emits `WatermarkStatus.IDLE`. I believe this signals to all downstream operators that this sub-stream is idle & they should not wait for anymore watermarks from it.

The problem is that the code doesn't work like this because the tracking of `lastRecordTime` is broken, causing the check against idleTimeout to fail, even when the stream has records coming in below idleTimeout. The Operator will flip back and forth between emitting WatermarkStatus.IDLE/WatermarkStatus.ACTIVE, when it should just stay ACTIVE.

This breaks the guarantees around watermarks/event time & could cause incorrect results. If downstream operators receive an IDLE status from this Operator when they shouldn't, if I understand correctly, they will advance their watermarks too early (IDLE signals they should ignore this sub-stream in their watermark update logic). 

Here is a PR I submitted: https://github.com/apache/flink/pull/24211 - that might make the issue/fix clearer.;;;","08/Feb/24 15:24;martijnvisser;[~fanrui] I'm curious on your thoughts on this one, given your work you've put in watermark alignment etc.;;;","18/Feb/24 10:12;fanrui;Sorry for the late response as I was on vacation for 10 days.

 

Thanks [~dchristle] for the report and fix, and thanks [~martijnvisser]  for the ping.

After my detailed analysis, I think it's indeed a bug, and this bug was introduced by FLINK-22881 [1][2]. The fix PR LGTM, and I have cced [~dwysakowicz]  help review as well.

BTW, IIUC this bug is related to watermark instead of watermark alignment. And it only happens for Table api or SQL.

[1] [https://github.com/apache/flink/pull/16221]
[2] [https://github.com/apache/flink/commit/5382cf5d5c143cce8a925a9063c74c51297d6a93];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClosureCleaner to include reference classes for non-serialization exception,FLINK-34251,13566388,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,liuml07,liuml07,27/Jan/24 05:25,29/Jan/24 19:03,04/Jun/24 20:40,,1.18.2,,,,,,,,,,,API / Core,,,,0,pull-request-available,,,,"Currently the ClosureCleaner throws exception if {{checkSerializable} is enabled while some object is non-serializable. It includes the non-serializable (nested) object in the exception in the exception message.

However, when the user job program gets more complex pulling multiple operators each of which pulls multiple 3rd party libraries, it is unclear how the non-serializable object is referenced as some of those objects could be nested in multiple levels. For example, following exception is not straightforward where to check:
{code}
org.apache.flink.api.common.InvalidProgramException: java.lang.Object@528c868 is not serializable. 
{code}

It would be nice to include the reference stack in the exception message, as following:
{code}
org.apache.flink.api.common.InvalidProgramException: java.lang.Object@72437d8d is not serializable. Referenced via [class com.mycompany.myapp.ComplexMap, class com.mycompany.myapp.LocalMap, class com.yourcompany.yourapp.YourPojo, class com.hercompany.herapp.Random, class java.lang.Object] ...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-27 05:25:13.0,,,,,,,,,,"0|z1n1hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option 'timestamp_mapping.legacy' to docs for Avro format,FLINK-34250,13566385,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,27/Jan/24 03:35,11/Apr/24 08:47,04/Jun/24 20:40,11/Apr/24 08:47,1.19.0,,,,,,,,,,,Documentation,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,pull-request-available,,,,We have options defined for AVRO and CVS formats. But they are not included in docs. It is better to show in a common section.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33198,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 08:44:19 UTC 2024,,,,,,,,,,"0|z1n1h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 07:32;renqs;[~ZhenqiuHuang] There are already pages for formats and their options:

https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/overview/

Not sure if these pages have covered what you want to add
;;;","11/Apr/24 08:44;renqs;master: 722fb9b405229c0e1ea0f8330f1c0063a80b0144;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove DefaultSlotTracker related logic.,FLINK-34249,13566383,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,RocMarshal,RocMarshal,27/Jan/24 03:02,29/Jan/24 08:38,04/Jun/24 20:40,29/Jan/24 02:34,,,,,,,,1.19.0,,,,Runtime / Task,,,,0,pull-request-available,,,,"pre step: https://issues.apache.org/jira/browse/FLINK-34174

The main reason for initiating this ticket is 
https://issues.apache.org/jira/browse/FLINK-31449  &  https://issues.apache.org/jira/browse/FLINK-34174
(IIUC) as the current related logic is no longer being used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 02:33:44 UTC 2024,,,,,,,,,,"0|z1n1go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/24 03:12;RocMarshal;pre step: https://issues.apache.org/jira/browse/FLINK-34174

The main reason for initiating this ticket is 
https://issues.apache.org/jira/browse/FLINK-31449  &  https://issues.apache.org/jira/browse/FLINK-34174
(IIUC) as the current related logic is no longer being used.;;;","29/Jan/24 02:33;fanrui;Merged to master(1.19.0) via : a24f7717847ce4e4c511070257e99d7c3f948d2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for ChangelogNormalize node,FLINK-34248,13566375,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,26/Jan/24 23:04,15/Feb/24 11:23,04/Jun/24 20:40,15/Feb/24 11:23,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 15 11:23:04 UTC 2024,,,,,,,,,,"0|z1n1ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/24 11:23;dwysakowicz;Implemented in e76ccdc1fd8b15a5aac4968fd89643b0b17e1a48..6e93394b4f2c22e5c50858242c17bcbd8fcf45c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document FLIP-366: Support standard YAML for FLINK configuration,FLINK-34247,13566337,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,26/Jan/24 15:58,05/Feb/24 10:42,04/Jun/24 20:40,05/Feb/24 02:14,,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 10:42:16 UTC 2024,,,,,,,,,,"0|z1n16g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 02:14;zhuzh;master/release-1.19:
5b61baadd02ccdfa702834e2e63aeb8d1d9e1250
04dd91f2b6c830b9ac0e445f72938e3d6f479edd
e9bea09510e18c6143e6e14ca17a894abfaf92bf;;;","05/Feb/24 10:42;fanrui;Extra commit(1.19): d44e940e9d28518115cf6e8d5ba4f963faf30f52;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow only archive failed job to history server,FLINK-34246,13566336,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,qingwei91,qingwei91,26/Jan/24 15:55,27/Jan/24 08:43,04/Jun/24 20:40,,,,,,,,,,,,,Client / Job Submission,,,,0,,,,,"Hi, I wonder if we can support only archiving Failed job to History Server.

History server is a great tool to allow us to check on previous job, we are using FLink batch which can run many times throughout the week, we only need to check job on History Server when it has failed.

It would be more efficient if we can choose to only store a subset of the data.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 27 08:43:37 UTC 2024,,,,,,,,,,"0|z1n168:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/24 04:25;Wencong Liu;Thanks [~qingwei91], for suggesting this. Are you suggesting that we should offer an option that allows the HistoryServer to archive only the failed batch jobs? This requirement seems quite specific. For instance, we would also need to consider archiving the logs of failed streaming jobs.;;;","27/Jan/24 08:43;qingwei91;Hi [~Wencong Liu] , thanks for replying.

Sorry I wasnt clear, I meant we should have an option to only archive failed job, not specific to batch.

Is this something sensible to add?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraSinkTest.test_cassandra_sink fails under JDK17 and JDK21 due to InaccessibleObjectException,FLINK-34245,13566333,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,JunRuiLi,mapohl,mapohl,26/Jan/24 15:37,29/Jan/24 14:37,04/Jun/24 20:40,28/Jan/24 09:42,1.19.0,,,,,,,1.19.0,,,,API / Python,Connectors / Cassandra,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56942&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=b68c9f5c-04c9-5c75-3862-a3a27aabbce3]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56942&view=logs&j=60960eae-6f09-579e-371e-29814bdd1adc&t=7a70c083-6a74-5348-5106-30a76c29d8fa&l=63680]
{code:java}
Jan 26 01:29:27 E                   py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.python.util.PythonConfigUtil.configPythonOperator.
Jan 26 01:29:27 E                   : java.lang.reflect.InaccessibleObjectException: Unable to make field final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not ""opens java.util"" to unnamed module @17695df3
Jan 26 01:29:27 E                   	at java.base/java.lang.reflect.AccessibleObject.throwInaccessibleObjectException(AccessibleObject.java:391)
Jan 26 01:29:27 E                   	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:367)
Jan 26 01:29:27 E                   	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:315)
Jan 26 01:29:27 E                   	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:183)
Jan 26 01:29:27 E                   	at java.base/java.lang.reflect.Field.setAccessible(Field.java:177)
Jan 26 01:29:27 E                   	at org.apache.flink.python.util.PythonConfigUtil.registerPythonBroadcastTransformationTranslator(PythonConfigUtil.java:357)
Jan 26 01:29:27 E                   	at org.apache.flink.python.util.PythonConfigUtil.configPythonOperator(PythonConfigUtil.java:101)
Jan 26 01:29:27 E                   	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
Jan 26 01:29:27 E                   	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
Jan 26 01:29:27 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
Jan 26 01:29:27 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
Jan 26 01:29:27 E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
Jan 26 01:29:27 E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
Jan 26 01:29:27 E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
Jan 26 01:29:27 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
Jan 26 01:29:27 E                   	at java.base/java.lang.Thread.run(Thread.java:1583) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 14:28:31 UTC 2024,,,,,,,,,,"0|z1n15k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/24 13:44;Sergey Nuyanzin;Bisect shows that the error is related to FLINK-33577, FLINK-34223, FLINK-34232
[~JunRuiLi] could you please have a look here?;;;","27/Jan/24 17:11;JunRuiLi;[~Sergey Nuyanzin] Thanks for kindly reminder, it's a bug that handle standard yaml config file in Pyflink gateway, I'll prepare a fix as soon as possible.;;;","28/Jan/24 09:42;zhuzh;Fixed via ddbf87f2a7aeeeb20a8590578c6d037b239d5593;;;","29/Jan/24 14:28;mapohl;The following builds didn't contain the fix above, yet:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57007&view=logs&j=60960eae-6f09-579e-371e-29814bdd1adc&t=7a70c083-6a74-5348-5106-30a76c29d8fa&l=24236]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57022&view=logs&j=60960eae-6f09-579e-371e-29814bdd1adc&t=7a70c083-6a74-5348-5106-30a76c29d8fa&l=24602] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Confluent Platform to latest compatible version,FLINK-34244,13566331,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,26/Jan/24 15:06,09/Feb/24 07:53,04/Jun/24 20:40,09/Feb/24 07:53,1.19.0,kafka-3.1.0,,,,,,1.20.0,kafka-3.2.0,,,Connectors / Kafka,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,pull-request-available,,,,"Flink uses Confluent Platform for its Confluent Avro Schema Registry implementation, and we can update that to the latest version.

It's also used by the Flink Kafka connector, and we should upgrade it to the latest compatible version of the used Kafka Client (in this case, 7.4.x)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 09 07:53:56 UTC 2024,,,,,,,,,,"0|z1n154:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/24 15:25;martijnvisser;Fixed in apache/flink:master 6bfc53c1edf48b4a452d3181944ec7685cf39760;;;","09/Feb/24 07:53;martijnvisser;Fixed in apache/flink-connector-kafka

main: cfb275b478ff97e9105c5ffaf20224f59a89ebd7
v3.1: 927b2e71bf6cd5e5b8db90c58b91bf145510da58;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update fabric8io:kubernetes-client to v6.9.0+ to allow the usage of ReleaseOnCall,FLINK-34243,13566311,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,,mapohl,mapohl,26/Jan/24 11:39,01/Feb/24 09:46,04/Jun/24 20:40,01/Feb/24 09:46,1.18.1,1.19.0,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"FLINK-34007 revealed that we could use the {{ReleaseOnCall}} configuration parameter in Flink's KubernetesLeaderElector. Currently, we have to call `release` explicitly. This is due to a ""bug"" in v6.6.2 which only calls release or calls the notLeader callback. This was fixed in [0f6c6965|https://github.com/fabric8io/kubernetes-client/commit/0f6c6965] which ended up in v6.9.0.

This issue is about the following things:
* Upgrading the client
* Enabling {{ReleaseOnCall}}
* Remove explict {{#release}} call {{KubernetesLeaderElector#stopLeaderElectionCycle}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 09:46:10 UTC 2024,,,,,,,,,,"0|z1n10o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 09:46;mapohl;Implemented by FLINK-34007.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
External connector should bump its flink version timely after a new flink release  ,FLINK-34242,13566268,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leonard,leonard,26/Jan/24 09:20,26/Jan/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,,Documentation,Release System,,,0,,,,,"As we discussed in FLINK-34241,  we should make sure that when a new Flink release（eg: 1.19.0） has been created, that the externalized connectors also get updated so that :
a) they don't test against 1.19-SNAPSHOT, but actually 1.19.0 
b) that the test is also run against 1.20-SNAPSHOT.  

This should be documented in flink release wiki as part of flink release guidance. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-26 09:20:21.0,,,,,,,,,,"0|z1n0r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transfer external connectors's CI result to Flink Slack #builds channel,FLINK-34241,13566267,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gongzhongqiang,leonard,leonard,26/Jan/24 09:12,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.1,,,,,,,1.20.0,,,,Build System / CI,,,,0,,,,,"As we discussed in FLINK-34237, we could timely found some blocker issue by monitoring
external connectors' daily CI, we can transfer the CI failure to Flink’s slack channel #builds
which release managers will check these failures and create JIRA tickets timely.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 10:02:53 UTC 2024,,,,,,,,,,"0|z1n0qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 09:37;gongzhongqiang;[~leonard] I'm willing to take this issue , Please assign to me.;;;","26/Jan/24 10:02;gongzhongqiang;Hi [~leonard]  ,

This should add bot or use app in slack and maintain `SLACK_BOT_TOKEN`  or `SLACK_WEBHOOK_URL` variable in every connector github repo.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The example of sliding windows with offset in documentation is not correct,FLINK-34240,13566239,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,Weijie Guo,Weijie Guo,Weijie Guo,26/Jan/24 06:08,26/Jan/24 06:08,04/Jun/24 20:40,,,,,,,,,,,,,Documentation,,,,0,,,,,"In documentation of windows, we have the following example code:
{code:java}
// sliding processing-time windows offset by -8 hours
input
    .keyBy(<key selector>)
    .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8)))
    .<windowed transformation>(<window function>);
{code}

Unfortunately, it will raise as the absolute value of offset must be less than the slide.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-26 06:08:56.0,,,,,,,,,,"0|z1n0ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a deep copy method of SerializerConfig for merging with Table configs in org.apache.flink.table.catalog.DataTypeFactoryImpl ,FLINK-34239,13566226,13564005,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mallikarjuna,Zhanghao Chen,Zhanghao Chen,26/Jan/24 03:30,30/May/24 10:38,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,API / Core,,,,1,pull-request-available,,,,"*Problem*

Currently, org.apache.flink.table.catalog.DataTypeFactoryImpl#createSerializerExecutionConfig will create a deep-copy of the SerializerConfig and merge Table config into it. However, the deep copy is done by manully calling the getter and setter methods of SerializerConfig, and is prone to human errors, e.g. missing copying a newly added field in SerializerConfig.

*Proposal*

Introduce a deep copy method for SerializerConfig and replace the curr impl in org.apache.flink.table.catalog.DataTypeFactoryImpl#createSerializerExecutionConfig.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 02:45:33 UTC 2024,,,,,,,,,,"0|z1n0hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/24 07:13;mallikarjuna;Hi, I'm new to the community. This seems like a good change! If we want to implement this, may I pick this up?;;;","28/Jan/24 11:07;Zhanghao Chen;[~mallikarjuna] Welcome to the community! Go ahead~ [~zjureel] Could you help assign it to Kumar?;;;","07/Feb/24 05:56;mallikarjuna;Thanks [~Zhanghao Chen] ! Hey [~zjureel] , would really appreciate if you could assign the task! TIA :);;;","08/Feb/24 02:48;zjureel;[~mallikarjuna] DONE;;;","09/Feb/24 14:13;mallikarjuna;Thank you, [~zjureel] !;;;","21/Mar/24 16:51;mallikarjuna;Hello [~Zhanghao Chen] , [~zjureel], I've raised a PR for the change. Could you please take a look! Thanks! :);;;","25/Mar/24 02:21;Zhanghao Chen;[~mallikarjuna] Thanks for the PR, I'll take a look;;;","30/Mar/24 14:01;mallikarjuna;Thanks for the review, [~Zhanghao Chen] ! I've updated the PR as per your comments.;;;","27/May/24 02:45;Zhanghao Chen;[~mallikarjuna] Hi could you help rebase the code on the latest master branch? We can merge it after CI passes after the rebase.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"In streaming mode, redundant exchange nodes can be optimally deleted in some cases",FLINK-34238,13566219,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xuyangzhong,xuyangzhong,26/Jan/24 01:27,29/Jan/24 06:49,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Take the following plan as an example:
{code:java}
Calc(select=[window_start, window_end, a, EXPR$3, EXPR$4, EXPR$5, wAvg, uv])
+- WindowAggregate(groupBy=[a], window=[SESSION(win_start=[window_start], win_end=[window_end], gap=[5 min], partition keys=[a])], select=[a, COUNT(*) AS EXPR$3, SUM(d) AS EXPR$4, MAX(d) FILTER $f4 AS EXPR$5, weightedAvg(b, e) AS wAvg, COUNT(DISTINCT c) AS uv, start('w$) AS window_start, end('w$) AS window_end])
   +- Exchange(distribution=[hash[a]])
      +- Calc(select=[a, window_start, window_end, d, IS TRUE(>(b, 1000)) AS $f4, b, e, c], where=[>=(window_start, 2021-01-01 10:10:00)])
         +- WindowTableFunction(window=[SESSION(time_col=[rowtime], gap=[5 min], partition keys=[a])])
            +- Exchange(distribution=[hash[a]])
               +- WatermarkAssigner(rowtime=[rowtime], watermark=[-(rowtime, 1000:INTERVAL SECOND)])
                  +- TableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, d, e, rowtime]) {code}
If the node `WindowTableFunction`, `Calc` and `WindowAggregate` can be chained finally, the  `Exchange` between `Calc` and `WindowAggregate` can be removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 06:49:20 UTC 2024,,,,,,,,,,"0|z1n0g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 09:14;libenchao;Make sense to me. Another general way is to let {{FlinkExpandConversionRule}} to remove shuffle via distribution trait, which is now used only in batch. (I know it needs a lot of efforts, since many rules in streaming need to adapt, we've done this work internally, hopefully someone could contribute this back);;;","29/Jan/24 06:49;lsy;+1, [~libenchao] Looks forward to your team contribution if possible.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MongoDB connector compile failed with Flink 1.19-SNAPSHOT,FLINK-34237,13566166,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Wencong Liu,leonard,leonard,25/Jan/24 16:48,29/Jan/24 01:50,04/Jun/24 20:40,26/Jan/24 09:40,,,,,,,,1.19.0,,,,API / Core,Connectors / MongoDB,,,0,pull-request-available,,,,"{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project flink-connector-mongodb: Compilation failure
134Error:  /home/runner/work/flink-connector-mongodb/flink-connector-mongodb/flink-connector-mongodb/src/main/java/org/apache/flink/connector/mongodb/source/reader/MongoSourceReaderContext.java:[35,8] org.apache.flink.connector.mongodb.source.reader.MongoSourceReaderContext is not abstract and does not override abstract method getTaskInfo() in org.apache.flink.api.connector.source.SourceReaderContext
135{code}
[https://github.com/apache/flink-connector-mongodb/actions/runs/7657281844/job/20867604084]

This is related to FLINK-33905

One point: As [FLIP-382|https://cwiki.apache.org/confluence/display/FLINK/FLIP-382%3A+Unify+the+Provision+of+Diverse+Metadata+for+Context-like+APIs] is accepted,  all connectors who implement SourceReaderContext (i.e MongoSourceReaderContext) should implement new introduced methods ` getTaskInfo()` if they want to compile/work with Flink 1.19.

Another point: The FLIP-382 didn't mentioned the connector backward compatibility well, maybe we need to rethink the section. As I just have a rough look at the FLIP, maybe [~xtsong] and [~Wencong Liu] could comment under this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 09:40:36 UTC 2024,,,,,,,,,,"0|z1n04g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 22:02;martijnvisser;[~leonard] This looks like a blocker for Flink 1.19, not for MongoDB right? Since the FLIP and the discussion thread didn't mention breaking anything in Flink 1.19, only for Flink 2.0?;;;","25/Jan/24 22:33;martijnvisser;For context, we had a similar issue with FLINK-34149;;;","26/Jan/24 01:51;xtsong;Thanks for reporting. This is indeed an unintended breaking change and a blocker for Flink 1.19.

We thought `SourceReaderContext` is only called by various connectors and were not aware of that it is also implemented by connectors. FLIP-382 is for clean-up purposes and does not introduce any new feature. Even in Flink 2.0, I think we should not require all connectors to change their codes only for such clean-up purposes. So let's simply revert changes for this interface.

[~Wencong Liu], could you please help fix this?;;;","26/Jan/24 03:22;Wencong Liu;Thanks for the reminder. I'll fix it as soon as possible.;;;","26/Jan/24 03:24;leonard;[~martijnvisser] ,this issue is different with  FLINK-34149, this is introduced by  FLIP-382 and we should fix it as [~xtsong] commented;;;","26/Jan/24 08:15;martijnvisser;[~leonard] I know, I just wanted to highlight that it's a similar/comparable issue, where we also had to fix 1.19 :);;;","26/Jan/24 08:58;leonard;[~martijnvisser] Got you. Btw, I think external connectors' daily CI should have captured this blocker earlier[1],  but we obviously missed this. I have an idea to report all external connector daily CI failure to Slack #builds channel，WDYT?

[1] https://github.com/apache/flink-connector-mongodb/actions/runs/7515733681/job/20460002228;;;","26/Jan/24 09:02;martijnvisser;[~leonard] I think that makes sense. In order to get there, we should make sure that when a new Flink release has been created, that the externalized connectors also get updated so that a) they don't test against 1.19-SNAPSHOT, but actually 1.19.0 and b) that the test is also run against 1.20-SNAPSHOT. We should probably update the release documentation for this. ;;;","26/Jan/24 09:08;leonard;+1 to record this to release wiki docs, I'll create two tickets to track the CI builds and release wiki as well.;;;","26/Jan/24 09:40;xtsong;master (1.19): d0829ba3b162c24f2655b35258c9a8dc61cdba67;;;",,,,,,,,,,,,,,,,,,,,,,,
Evaluate strange unstable build after cleaning up CI machines,FLINK-34236,13566135,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,jeyhunkarimov,jingge,jingge,25/Jan/24 13:13,25/Jan/24 13:15,04/Jun/24 20:40,,,,,,,,,,,,,Test Infrastructure,,,,0,,,,,"To check if it is one time issue because infra change or not.

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56601&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=5d91035e-8022-55f2-2d4f-ab121508bf7e",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34135,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-25 13:13:36.0,,,,,,,,,,"0|z1mzxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not using Optional as input arguments in QueryHintsResolver,FLINK-34235,13566126,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,xuyangzhong,xuyangzhong,xuyangzhong,25/Jan/24 12:14,26/Jan/24 03:52,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,It also seems that we always can't get an empty left or right name from input.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-25 12:14:36.0,,,,,,,,,,"0|z1mzvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apply ShadeOptionalChecker for flink-shaded,FLINK-34234,13566125,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,25/Jan/24 12:13,07/Feb/24 17:26,04/Jun/24 20:40,,,,,,,,,,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,"As it was found within FLINK-34148
 that newer version of shade plugin breaks previous behavior and non shaded artifacts are started being added to flink-shaded deps.

 

The tasks is to apply same check for flink-shaded with help of {{ShadeOptionalChecker}} which is already applied for Flink",,,,,,,,,,,,,FLINK-34148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 17:26:33 UTC 2024,,,,,,,,,,"0|z1mzvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/24 15:19;mapohl;[~Sergey Nuyanzin] what's the state of this issue now? Shall we close it as {{Not A Problem}}? Or is it still worth adding the check?;;;","07/Feb/24 17:26;Sergey Nuyanzin;This is a good question

From one side just to fix perf regression on flink-shaded side we don't need to add this check

From another side it is still not clear whether we need to do it in order to be able to release with maven 3.8.6 or not...
I asked this question in PR however still no answer.

I think we can continue with fix of pref regression (on flink-shaded side) first and have another task for movement towards maven 3.8.6
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase.testHybridSelectiveExchangesRestart failed due to a IllegalStateException,FLINK-34233,13566109,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunfengzhou,mapohl,mapohl,25/Jan/24 10:12,30/Jan/24 09:08,04/Jun/24 20:40,30/Jan/24 07:51,1.19.0,,,,,,,1.19.0,,,,Runtime / Network,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56791&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8357

{code}
Jan 24 02:10:03 02:10:03.582 [ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 34.74 s <<< FAILURE! -- in org.apache.flink.test.runtime.HybridShuffleITCase
Jan 24 02:10:03 02:10:03.582 [ERROR] org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart -- Time elapsed: 3.347 s <<< FAILURE!
Jan 24 02:10:03 java.lang.AssertionError: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=0)
Jan 24 02:10:03 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:59)
Jan 24 02:10:03 	at org.apache.flink.test.runtime.BatchShuffleITCaseBase.executeJob(BatchShuffleITCaseBase.java:137)
Jan 24 02:10:03 	at org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart(HybridShuffleITCase.java:91)
Jan 24 02:10:03 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Jan 24 02:10:03 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Jan 24 02:10:03 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Jan 24 02:10:03 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
Jan 24 02:10:03 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Jan 24 02:10:03 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Jan 24 02:10:03 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Jan 24 02:10:03 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Jan 24 02:10:03 	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:992)
Jan 24 02:10:03 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
Jan 24 02:10:03 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
Jan 24 02:10:03 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Jan 24 02:10:03 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Jan 24 02:10:03 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Jan 24 02:10:03 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
Jan 24 02:10:03 	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276)
Jan 24 02:10:03 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625)
Jan 24 02:10:03 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
Jan 24 02:10:03 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
Jan 24 02:10:03 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Jan 24 02:10:03 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Jan 24 02:10:03 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Jan 24 02:10:03 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
Jan 24 02:10:03 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Jan 24 02:10:03 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
Jan 24 02:10:03 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
Jan 24 02:10:03 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
Jan 24 02:10:03 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
Jan 24 02:10:03 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
Jan 24 02:10:03 Caused by: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=0)
Jan 24 02:10:03 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:180)
Jan 24 02:10:03 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
Jan 24 02:10:03 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:277)
Jan 24 02:10:03 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:268)
Jan 24 02:10:03 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:261)
Jan 24 02:10:03 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:787)
Jan 24 02:10:03 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:764)
Jan 24 02:10:03 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
Jan 24 02:10:03 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)
Jan 24 02:10:03 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Jan 24 02:10:03 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
Jan 24 02:10:03 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
Jan 24 02:10:03 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
Jan 24 02:10:03 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
Jan 24 02:10:03 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
Jan 24 02:10:03 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
Jan 24 02:10:03 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
Jan 24 02:10:03 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
Jan 24 02:10:03 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
Jan 24 02:10:03 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
Jan 24 02:10:03 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
Jan 24 02:10:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
Jan 24 02:10:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Jan 24 02:10:03 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Jan 24 02:10:03 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
Jan 24 02:10:03 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
Jan 24 02:10:03 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
Jan 24 02:10:03 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
Jan 24 02:10:03 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
Jan 24 02:10:03 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
Jan 24 02:10:03 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
Jan 24 02:10:03 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
Jan 24 02:10:03 	... 5 more
Jan 24 02:10:03 Caused by: java.lang.IllegalStateException: java.lang.IllegalStateException: Queried for a buffer before requesting the subpartition.
Jan 24 02:10:03 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.checkAndWaitForSubpartitionView(LocalInputChannel.java:308)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.getNextBuffer(LocalInputChannel.java:248)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.hybrid.tiered.netty.NettyConnectionReaderImpl.readBuffer(NettyConnectionReaderImpl.java:69)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.memory.MemoryTierConsumerAgent.getNextBuffer(MemoryTierConsumerAgent.java:103)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageConsumerClient.getNextBuffer(TieredStorageConsumerClient.java:102)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.readBufferFromTieredStore(SingleInputGate.java:964)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.readRecoveredOrNormalBuffer(SingleInputGate.java:899)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.waitAndGetNextData(SingleInputGate.java:842)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:813)
Jan 24 02:10:03 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.pollNext(SingleInputGate.java:801)
Jan 24 02:10:03 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.pollNext(InputGateWithMetrics.java:130)
Jan 24 02:10:03 	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:150)
Jan 24 02:10:03 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:122)
Jan 24 02:10:03 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
Jan 24 02:10:03 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:579)
Jan 24 02:10:03 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
Jan 24 02:10:03 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:909)
Jan 24 02:10:03 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:858)
Jan 24 02:10:03 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
Jan 24 02:10:03 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
Jan 24 02:10:03 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:751)
Jan 24 02:10:03 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
Jan 24 02:10:03 	at java.base/java.lang.Thread.run(Thread.java:833)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33743,,,,FLINK-34225,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 09:08:32 UTC 2024,,,,,,,,,,"0|z1mzrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 10:12;mapohl;Linking FLINK-34225 because it's the same test and might be due to the same issue. [~yunfengzhou] can you pick that one as well? Feel free to object and to alter the issue if you think it's unrelated.;;;","25/Jan/24 10:31;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56796&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8396;;;","26/Jan/24 01:21;yunfengzhou;Hi [~mapohl], truly this is also a bug caused by FLINK-33743. I'll look into this.;;;","29/Jan/24 14:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57035&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8077;;;","30/Jan/24 07:51;Weijie Guo;master(1.19): 973190e8ca5b7225f18b5c176726ef8680faffca;;;","30/Jan/24 09:08;mapohl;The following build failure didn't contain the fix mentioned above, yet, and is only added here for documentation purposes:

* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57079&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8355;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Config file unexpectedly lacks support for env.java.home,FLINK-34232,13566103,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,25/Jan/24 09:50,26/Jan/24 10:51,04/Jun/24 20:40,26/Jan/24 10:51,,,,,,,,1.19.0,,,,API / Core,,,,0,pull-request-available,,,,"We removed the option to set JAVA_HOME in the config file with commit [24091|https://github.com/apache/flink/pull/24091] to improve how we handle standard YAML with BashJavaUtils. But since setting JAVA_HOME is a publicly documented feature, we need to keep it available for users. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33721,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 10:51:25 UTC 2024,,,,,,,,,,"0|z1mzqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 10:51;zhuzh;Fixed in master/release-1.19:
e623c07f4e56fdef1bd8514ccd02df347af5b122;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in doc,FLINK-34231,13566064,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,gyang94,gyang94,25/Jan/24 04:03,25/Jan/24 12:47,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Documentation,,,,0,documentation,pull-request-available,,,"Fix a typo in flink document ""Hive Dialect"" page, ""Show Partitions"" Section.



 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/24 03:52;gyang94;Screen Shot 2024-01-25 at 11.48.29.png;https://issues.apache.org/jira/secure/attachment/13066240/Screen+Shot+2024-01-25+at+11.48.29.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink/pull/24192,,,,,,,,,,9223372036854775807,,,,2024-01-25 04:03:54.0,,,,,,,,,,"0|z1mzhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update flink-docs README: add -Pskip-webui-build to the config doc generation command,FLINK-34230,13565992,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,24/Jan/24 16:07,29/Jan/24 11:29,04/Jun/24 20:40,29/Jan/24 11:29,,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,"I used to generate Flink config docs with the command listed in flink-docs README: 
""mvn package -Dgenerate-config-docs -pl flink-docs -am -nsu DskipTests"". The command will compile Flink web frontend, which usu. takes a few minutes and is in fact unnecessary. We can add ""-Pskip-webui-build"" to the config doc generation command to save us a few minutes each time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 11:29:13 UTC 2024,,,,,,,,,,"0|z1mz1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 03:24;Zhanghao Chen;[~Weijie Guo] Could you help take a look when you are free? A simple doc improvement, discovered when I was modifying config options frequently.;;;","29/Jan/24 11:29;huweihua;resolved in master: ce67b7d7c380f8639ef2b13ed7b9af34bb032726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate entry in InnerClasses attribute in class file FusionStreamOperator,FLINK-34229,13565974,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zoudan,xiasun,xiasun,24/Jan/24 13:29,05/Feb/24 02:22,04/Jun/24 20:40,05/Feb/24 02:05,1.19.0,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,"I noticed a runtime error happens in 10TB TPC-DS (q35.sql) benchmarks in 1.19, the problem did not happen in 1.18.0. This issue may have been newly introduced recently. !image-2024-01-24-17-05-47-883.png|width=589,height=279!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/24 09:05;xiasun;image-2024-01-24-17-05-47-883.png;https://issues.apache.org/jira/secure/attachment/13066223/image-2024-01-24-17-05-47-883.png","26/Jan/24 15:15;xiasun;taskmanager_log.txt;https://issues.apache.org/jira/secure/attachment/13066286/taskmanager_log.txt",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 02:22:19 UTC 2024,,,,,,,,,,"0|z1myxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 15:00;lincoln.86xy;[~lsy] Can you have a look at this error?;;;","25/Jan/24 01:48;lsy;Hi, [~xiasun] Can you revert this [PR|https://github.com/apache/flink/pull/23984] and try again, I suspect this it's causing the problem;;;","25/Jan/24 01:54;lsy;cc [~zoudan] , can you also help to check it?;;;","25/Jan/24 03:13;FrankZou;[~lsy] It seem to be related to this [PR|https://github.com/apache/flink/pull/23984], I will check it.;;;","25/Jan/24 08:13;xiasun;[~lsy] I checked that after reverting the [PR|https://github.com/apache/flink/pull/23984], TDC-DS (q35.sql) can execute to completion normally. Sorry for my late response.;;;","25/Jan/24 11:35;lsy;[~xiasun]Thanks for double-checking. [~FrankZou] Can you help to find the root cause? Regarding your PR change, I think maybe we have ignored some points for OFCG.;;;","25/Jan/24 11:47;FrankZou;[~lsy] I have run the q35 in `flink-tpcds-test` on master, and it worked as expected. [~xiasun] May I ask if it is convenient for you to try on master branch.;;;","25/Jan/24 12:01;zhuzh;[~FrankZou] the query plan can be different between q35 of `flink-tpcds-test` and q35 of a 10TB TPC-DS benchmark. e.g., DPP or runtime filters may not be created in `flink-tpcds-test` due to the data size is very small.;;;","26/Jan/24 03:48;FrankZou;[~xiasun], I could not reproduce in my environment, could you please turn on the debug log and obtain the generated code in the log in CompileUtils#
doCompile.
{code:java}
    private static <T> Class<T> doCompile(ClassLoader cl, String name, String code) {
        checkNotNull(cl, ""Classloader must not be null."");
        CODE_LOG.debug(""Compiling: {} \n\n Code:\n{}"", name, code)
        ...
    }
{code};;;","26/Jan/24 15:17;xiasun;[~FrankZou] Sure, I have uploaded all the debug logs from CompileUtils for the failed task. Hope they can help you locate the issue.;;;","30/Jan/24 03:50;lincoln.86xy;[~FrankZou] Any updates on this issue? ;;;","30/Jan/24 09:56;FrankZou;[~lincoln.86xy] I am working on it and expect to submit a CR before tomorrow.;;;","30/Jan/24 12:55;lincoln.86xy;[~FrankZou] Thanks for the udpate!;;;","01/Feb/24 11:19;FrankZou;[~xiasun] Could you please check if this [PR|https://github.com/apache/flink/pull/24228] could solve you problem.;;;","02/Feb/24 10:47;xiasun;[~FrankZou] Thanks for resolving this. I have checked that after cherry-picked the commit, q35.sql is now able to run to completion without exceptions. It looks good to me.;;;","05/Feb/24 02:21;lsy;[~xiasun] This fix has been merged in master branch, can you help run the all tpc-ds queries to double-check the fix?;;;","05/Feb/24 02:22;lsy;Merged in master branch: a603f2bb8536fa75453694a77c66a76f2e33b941;;;",,,,,,,,,,,,,,,,
Add long UTF serializer/deserializer,FLINK-34228,13565969,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,pvary,pvary,24/Jan/24 12:43,19/Mar/24 08:36,04/Jun/24 20:40,19/Mar/24 08:36,,,,,,,,1.20.0,,,,,,,,0,pull-request-available,,,,"DataOutputSerializer.writeUTF has a hard limit on the length of the string (64k). This is inherited from the DataOutput.writeUTF method, where the JDK specifically defines this limit [1].

For our use-case we need to enable the possibility to serialize longer UTF strings, so we will need to define a writeLongUTF method with a similar specification than the writeUTF, but without the length limit.

Based on the discussion on the mailing list, this is a good additional serialization utility to Flink [2]

[1] - https://docs.oracle.com/javase/8/docs/api/java/io/DataOutput.html#writeUTF-java.lang.String-
[2] - https://lists.apache.org/thread/ocm6cj0h8o3wbwo7fz2l1b4odss750rk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 08:36:27 UTC 2024,,,,,,,,,,"0|z1mywo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 08:36;gyfora;merged to master f75935245799471ddf025d2bab0d0d212e79088e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job doesn't disconnect from ResourceManager,FLINK-34227,13565966,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,mapohl,mapohl,mapohl,24/Jan/24 12:08,27/May/24 15:15,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,Runtime / Coordination,,,,0,github-actions,pull-request-available,test-stability,,"https://github.com/XComp/flink/actions/runs/7634987973/job/20800205972#step:10:14557

{code}
[...]
""main"" #1 prio=5 os_prio=0 tid=0x00007fcccc4b7000 nid=0x24ec0 waiting on condition [0x00007fccce1eb000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000bdd52618> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2131)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2099)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2077)
	at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:876)
	at org.apache.flink.table.planner.runtime.stream.sql.WindowDistinctAggregateITCase.testHopWindow_Cube(WindowDistinctAggregateITCase.scala:550)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-34560,FLINK-34570,FLINK-33900,,,,,,,,,,,,,,,,"24/Jan/24 14:53;mapohl;FLINK-34227.7e7d69daebb438b8d03b7392c9c55115.log;https://issues.apache.org/jira/secure/attachment/13066227/FLINK-34227.7e7d69daebb438b8d03b7392c9c55115.log","24/Jan/24 14:53;mapohl;FLINK-34227.log;https://issues.apache.org/jira/secure/attachment/13066228/FLINK-34227.log",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 15:15:39 UTC 2024,,,,,,,,,,"0|z1myw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 12:28;mapohl;There are several test failures in {{WindowDistinctAggregateITCase}} (not only for {{testHopWindow_Cube}}) where the test fails due to a {{NoResourceAvailableException}}. It's also weird that the {{WindowDistinctAggregateITCase.testHopWindow_Cube}} times out even though all four test runs for this test method started and finished (1 successfully and 3 failed) according to the logs.;;;","24/Jan/24 12:47;mapohl;:facepalm: The test run was cancelled because we reached the timeout of 4h;;;","24/Jan/24 12:57;mapohl;Ok, {{WindowDistinctAggregateITCase}} seems to throttle the overall stage execution with 21 succeeding but 35 failing:
{code:bash}
➜  logs-test-nightly-adaptive-scheduler-64-table-test-1706074714 grep ""Test test.*WindowDistinctAggregateITCase"" mvn-2.log | grep -v ""is running"" | grep -c success
21
➜  logs-test-nightly-adaptive-scheduler-64-table-test-1706074714 grep ""Test test.*WindowDistinctAggregateITCase"" mvn-2.log | grep -v ""is running"" | grep -c fail   
35
{code}
There are no other test failures in that stage:
{code:bash}
➜  logs-test-nightly-adaptive-scheduler-64-table-test-1706074714 grep -c ""failed with"" mvn-2.log
35
{code}
All of them seem to have been caused by {{ResourceNotAvailableException}}s:
{code:bash}
➜  logs-test-nightly-adaptive-scheduler-64-table-test-1706074714 grep -c ""Failed to go from CreatingExecutionGraph to Executing because the ExecutionGraph creation failed."" mvn-2.log 
35
➜  logs-test-nightly-adaptive-scheduler-64-table-test-1706074714 grep -c ""NoResourceAvailableException"" mvn-2.log
140 # 140 / 35 = 4
{code}
A [successful run with AdaptiveScheduler enabled|https://github.com/XComp/flink/actions/runs/7632434711] runs 64 dedicated tests of {{WindowDistinctAggregateITCase}} (runtime: 2min);;;","24/Jan/24 14:00;mapohl;The difference seems to be that we're running multiple jobs with a parallelism of 4 on the same cluster with 4 task slots, e.g.:
{code}
Missing resources:
         Job 7e7d69daebb438b8d03b7392c9c55115
                ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}
         Job 0ed4bdf3f9a589b16f846fc11a28f127
                ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}
Current resources:
        TaskManager 7d5dbc6f-47ad-4043-ba88-b8cadefe0ca5
                Available: ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=80.000mb (83886080 bytes), networkMemory=64.000mb (67108864 bytes)}
                Total:     ResourceProfile{taskHeapMemory=1024.000gb (1099511627776 bytes), taskOffHeapMemory=1024.000gb (1099511627776 bytes), managedMemory=80.000mb (83886080 bytes), networkMemory=64.000mb (67108864 bytes)}
{code}

The job {{7e7d69daebb438b8d03b7392c9c55115}} doesn't seem to be cleaned up properly on the ResourceManager's side.;;;","24/Jan/24 15:22;mapohl;I attached the logs of the {{WindowDistinctAggregateITCase}} test runs ([FLINK-34227.log|https://issues.apache.org/jira/secure/attachment/13066228/FLINK-34227.log]) and an extract only focusing on the test run that created job 7e7d69daebb438b8d03b7392c9c55115 and the subsequent test run up to the point where the missing resources for two jobs are logged for the first time ([FLINK-34227.7e7d69daebb438b8d03b7392c9c55115.log|https://issues.apache.org/jira/secure/attachment/13066227/FLINK-34227.7e7d69daebb438b8d03b7392c9c55115.log]).

There's something odd going on when shutting down the JobMaster/deregistering the JobMaster from the ResourceManager:
{code}
[...]
06:01:26,703 [flink-pekko.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor 7d5dbc6f-47ad-4043-ba88-b8cadefe0ca5 because: TaskExecutor pekko://flink/user/rpc/taskmanager_0 has no more allocated slots for job
 7e7d69daebb438b8d03b7392c9c55115.
06:01:26,703 [flink-pekko.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection bb1ce04485fc5291a97cd5d488709ff9: Stopping JobMaster for job 'Flink Streaming Job' (7e7d69daebb438b8d03b7392c9c55115).
06:01:26,703 [flink-pekko.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager pekko://flink/user/rpc/resourcemanager_2(891f46bbb398d49ad91e1dde0bee410c)
[...]
{code}

The disconnect is not finalized but a connection is re-established.;;;","25/Jan/24 08:06;mapohl;*Call Hierarchy*
{code:bash}
=> ""INFO: Connecting to ResourceManager [...]"" printed
JobMaster.connectToResourceManager() (org.apache.flink.runtime.jobmaster)
    JobMaster.tryConnectToResourceManager()  (org.apache.flink.runtime.jobmaster)
        JobMaster.reconnectToResourceManager(Exception)  (org.apache.flink.runtime.jobmaster)
            JobMaster.disconnectResourceManager(ResourceManagerId, Exception)  (org.apache.flink.runtime.jobmaster)
                => ""INFO: Disconnect job manager [...]"" NOT printed 
                ResourceManager.closeJobManagerConnection(JobID, ResourceRequirementHandling, Exception)  (org.apache.flink.runtime.resourcemanager)
                    ResourceManager.removeJob(JobID, Exception)  (org.apache.flink.runtime.resourcemanager)
                        # [...]
                        ResourceManager.disconnectJobManager(JobID, JobStatus, Exception)  (org.apache.flink.runtime.resourcemanager)
                            => ""INFO: Close ResourceManager connection"" printed
                            JobMaster.dissolveResourceManagerConnection(EstablishedResourceManagerConnection, Exception)  (org.apache.flink.runtime.jobmaster)
                                JobMaster.closeResourceManagerConnection(Exception)  (org.apache.flink.runtime.jobmaster)
                                    JobMaster.reconnectToResourceManager(Exception)  (org.apache.flink.runtime.jobmaster)
                                    JobMaster.disconnectTaskManagerResourceManagerConnections(Exception)  (org.apache.flink.runtime.jobmaster)
                                        JobMaster.stopJobExecution(Exception)  (org.apache.flink.runtime.jobmaster)
                                            JobMaster.onStop()  (org.apache.flink.runtime.jobmaster)
                                                RpcEndpoint.internalCallOnStop()  (org.apache.flink.runtime.rpc)
                                                    terminate(PekkoRpcActor<?>, ClassLoader) in StartedState in PekkoRpcActor  (org.apache.flink.runtime.rpc.pekko)
                                                        PekkoRpcActor.handleControlMessage(ControlMessages)  (org.apache.flink.runtime.rpc.pekko)
                                                            PekkoRpcActor.createReceive()  (org.apache.flink.runtime.rpc.pekko)
                    # [...]
            => ""heartbeat handling""
            handleResourceManagerConnectionLoss(ResourceID, Exception) in ResourceManagerHeartbeatListener in JobMaster  (org.apache.flink.runtime.jobmaster)
                notifyHeartbeatTimeout(ResourceID) in ResourceManagerHeartbeatListener in JobMaster  (org.apache.flink.runtime.jobmaster)
                    DefaultHeartbeatMonitor.run()  (org.apache.flink.runtime.heartbeat)
                notifyTargetUnreachable(ResourceID) in ResourceManagerHeartbeatListener in JobMaster  (org.apache.flink.runtime.jobmaster)
                    DefaultHeartbeatMonitor.reportHeartbeatRpcFailure()  (org.apache.flink.runtime.heartbeat)
            => ""new RM leader elected""
            JobMaster.notifyOfNewResourceManagerLeader(String, ResourceManagerId)  (org.apache.flink.runtime.jobmaster)
                notifyLeaderAddress(String, UUID) in ResourceManagerLeaderListener in JobMaster  (org.apache.flink.runtime.jobmaster)

{code}
*Findings*
Reasons that could prevent the disconnect log message (and as a consequence traversing the call tree further)
 * {{ResourceManager#jobManagerRegistrations}} does not include job anymore ([ResourceManager:1082|https://github.com/apache/flink/blob/ab9445aca56e7d139e8fd9bcc23e5f7e06288e66/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java#L1082])
 ** ResourceManager#jobManagerRegistrations is updated in three different locations:
 *** When registering a JobMaster with the ResourceManager ([ResourceManager#registerJobMasterInternal|#registerJobMasterInternal])
 **** This would be revealed through a log message [INFO: Registering job manager|https://github.com/apache/flink/blob/ab9445aca56e7d139e8fd9bcc23e5f7e06288e66/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java#L388]
 *** When clearing the internal state due to a [ResourceManager#stopResourceManagerServices|#stopResourceManagerServices]
 **** This only happens if the ResourceManager gets restarted or stopped
 *** As part of the JobManagerConnection closing ([ResourceManager#closeJobManagerConnection|#closeJobManagerConnection])
 **** This is the case we're in and which should have removed the job actually but didn't

Reasons for why the connect could happen again:
 * [handleResourceManagerConnectionLoss|https://github.com/apache/flink/blob/7b9b4e53a59ab8f4f2a99a6e162a794d264f7daf/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L1523] triggers a reconnect and is triggered by the following two callers
 ** [JobMaster#ResourceManagerHeartbeatListener#handleResourceConnectionLoss|https://github.com/apache/flink/blob/7b9b4e53a59ab8f4f2a99a6e162a794d264f7daf/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L1514] which would produce a log message ""INFO: The heartbeat of ResourceManager ...""
 ** [JobMaster#ResourceManagerHeartbeatListener#notifyTargetUnreachable|https://github.com/apache/flink/blob/7b9b4e53a59ab8f4f2a99a6e162a794d264f7daf/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L1535] which would produce a log message ""INFO: ResourceManager with id ? is not longer reachable.""
 * [JobMaster#notifyOfNewResourceManagerLeader|https://github.com/apache/flink/blob/7b9b4e53a59ab8f4f2a99a6e162a794d264f7daf/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L1145]
 ** Leader change for the ResourceManager cannot be the reason:
 *** no logs available related to leader election
 *** the ResourceManager session ID {{891f46bbb398d49ad91e1dde0bee410c}} stays the same between both test runs;;;","01/Mar/24 11:36;mapohl;We've seen this behavior also for the JoinITCase in https://github.com/apache/flink/actions/runs/8105495458/job/22154140154#step:10:11906 (see further details in the now closed FLINK-34560).;;;","04/Mar/24 07:54;mapohl;https://github.com/apache/flink/actions/runs/8134966351/job/22228823914;;;","11/Mar/24 08:14;mapohl;Same issue was observed for {{GroupWindowTableAggregateITCase}}:
https://github.com/apache/flink/actions/runs/8181761395/job/22372319416#step:10:12561;;;","11/Mar/24 13:53;mapohl;Transferred over from FLINK-34570 (JoinITCase):
* https://github.com/apache/flink/actions/runs/8127069912/job/22211928085#step:10:14479
* https://github.com/apache/flink/actions/runs/8165884599/job/22324024789#step:10:14479;;;","11/Mar/24 14:02;mapohl;Interestingly, the behavior is only observed on Github Actions with the AdaptiveScheduler and on the 1.18 release branch, so far.;;;","11/Mar/24 14:34;mapohl;This one is a 1.19 workflow run:
https://github.com/apache/flink/actions/runs/8197679758/job/22420248800#step:10:14967;;;","11/Mar/24 14:54;mapohl;AggregateITCase:
https://github.com/apache/flink/actions/runs/8211401561/job/22460442229#step:10:17161;;;","11/Mar/24 14:56;mapohl;https://github.com/apache/flink/actions/runs/8218856667/job/22476299334#step:10:15130

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f177c2b7000 nid=0x21f26 waiting on condition [0x00007f177dfd6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000a804e648> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2131)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2099)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2077)
	at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:876)
	at org.apache.flink.table.planner.runtime.stream.sql.WindowJoinITCase.testRightJoin(WindowJoinITCase.scala:983)
{code};;;","12/Mar/24 07:39;mapohl;https://github.com/apache/flink/actions/runs/8242516176/job/22541877232#step:10:12172;;;","12/Mar/24 11:23;mapohl;My suspicion is that there's a bug in the {{AdaptiveScheduler}} returning the wrong JobStatus in [JobMaster:1274|https://github.com/apache/flink/blob/d6a4eb966fbc47277e07b79e7c64939a62eb1d54/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L1274] resulting in the data being retained on the ResourceManager's side (see [line 566|https://github.com/apache/flink/blob/d6a4eb966fbc47277e07b79e7c64939a62eb1d54/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java#L566]). That would explain why we're only seeing the issue in the AdaptiveScheduler CI profile.;;;","12/Mar/24 12:08;mapohl;The [findings of my initial analysis|https://issues.apache.org/jira/browse/FLINK-34227?focusedCommentId=17810745&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17810745] are not correct. The missing log message does exist. It's just that the ""{{Close ResourceManager connection [...]}}"" log message appears twice (once triggered from the JobMaster's IO thread and once from the Dispatcher's main thread). The latter one seems to retrigger the reconnection.

{code}
[...]
02:51:28,193 [flink-pekko.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job e7cb13faaae707768a1a4db28427af80 from job leader monitoring.
02:51:28,193 [flink-pekko.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job e7cb13faaae707768a1a4db28427af80.
02:51:28,193 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer [] - Freeing slot 98a0c702ce550d2fd7dd3710ec7b76e0.
02:51:28,194 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Disconnect TaskExecutor d71ee9b8-f278-48ee-bb1c-f05fd568947f because: TaskExecutor pekko://flink/user/rpc/taskmanager_0 has no more allocated slots for job e7cb13faaae707768a1a4db28427af80.
02:51:28,194 [jobmanager-io-thread-3] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 3c08958c5ef3906fae847097373b047a: Stopping JobMaster for job 'Flink Streaming Job' (e7cb13faaae707768a1a4db28427af80).
02:51:28,194 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager a38b8b4ba6c4894c7cfca5f1c0fe4f68@pekko://flink/user/rpc/jobmanager_70 for job e7cb13faaae707768a1a4db28427af80 from the resource manager.
02:51:28,194 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 3c08958c5ef3906fae847097373b047a: Stopping JobMaster for job 'Flink Streaming Job' (e7cb13faaae707768a1a4db28427af80).
02:51:28,194 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager pekko://flink/user/rpc/resourcemanager_2(86dfd2ebd79836698df3e4a5de474282)
02:51:28,194 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
02:51:28,194 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager a38b8b4ba6c4894c7cfca5f1c0fe4f68@pekko://flink/user/rpc/jobmanager_70 for job e7cb13faaae707768a1a4db28427af80.
02:51:28,195 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager a38b8b4ba6c4894c7cfca5f1c0fe4f68@pekko://flink/user/rpc/jobmanager_70 for job e7cb13faaae707768a1a4db28427af80.
02:51:28,195 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 86dfd2ebd79836698df3e4a5de474282.
[...]
{code};;;","12/Mar/24 14:33;mapohl;[~chesnay] supported the investigation. His findings are based around the question why the close call was actually triggered from within an IO thread:
# The {{JobMaster#onStop}} call triggers [JobMaster#stopJobExecution|https://github.com/apache/flink/blob/d6a4eb966fbc47277e07b79e7c64939a62eb1d54/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L1048] which calls {{JobMaster#stopScheduling}} from within the JobMaster's main thread which calls {{AdaptiveScheduler#closeAsync}} in [JobMaster:1088|https://github.com/apache/flink/blob/d6a4eb966fbc47277e07b79e7c64939a62eb1d54/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L1088].
# {{AdaptiveScheduler#closeAsync}} composes the resultFuture of the {{CheckpointsCleaner}} in [AdaptiveScheduler:581|https://github.com/apache/flink/blob/d4e0084649c019c536ee1e44bab15c8eca01bf13/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/AdaptiveScheduler.java#L581]. The future is completed from within the {{ioExecutor}} that is used in the {{CheckpointsCleaner}}.
# The future is forwarded to {{JobMaster#stopJobExecution}} where the disconnect is triggered after the cleanup future is completed causing the disconnect to be executed in an IO thread rather than the JobMaster's main thread.

update: But that doesn't explain, yet, why this is only happening in the AdaptiveScheduler profile, so far. We have the same issue in [SchedulerBase#closeAsync|https://github.com/apache/flink/blob/d4e0084649c019c536ee1e44bab15c8eca01bf13/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/SchedulerBase.java#L674].;;;","13/Mar/24 12:27;mapohl;So, we should fix the {{runAfterwards}} issue Chesnay raised. But I continued investigating it because I found it strange that it only appears in the {{AdaptiveScheduler}} test profile even though we're using the same logic in [SchedulerBase#close|https://github.com/apache/flink/blob/d4e0084649c019c536ee1e44bab15c8eca01bf13/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/SchedulerBase.java#L674] as mentioned in the previous comment.

The problem seems to be that the {{JobMaster}} disconnecting from the {{ResourceManager}} happens twice. The second disconnect triggers a reconnect and re-registration of the {{JobMaster}} in the {{ResourceManager}}. This can theoretically happen because the first disconnect will trigger a {{JobMaster#disconnectResourceManager}} in [ResourceManager#closeJobManagerConnection in line 1150|https://github.com/apache/flink/blob/d6a4eb966fbc47277e07b79e7c64939a62eb1d54/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java#L1150]. If this RPC call makes it to the {{JobMaster}} before its {{RPCEndpoint}} is shutdown (which can happen due to the concurrency introduced by the {{runAfterwards}} issue), it will get processed once more leading to the reconnect because the {{JobMaster#resourceManagerAddress}} is still set (which is the condition for {{JobMaster#isConnectingToResourceManager}} which is called in [JobMaster#disconnectResourceManager|https://github.com/apache/flink/blob/7d0111dfab640f2f590dd710d76de927c86cf83e/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L847]). With the {{resourceManagerAddress}} being set and pointing to the correct {{ResourceManager}}, reconnection would be triggered.

But this still doesn't answer the question why it only happens with the {{AdaptiveScheduler}} test profile?;;;","13/Mar/24 15:42;mapohl;https://github.com/apache/flink/actions/runs/8258416104/job/22590931762#step:10:17193;;;","21/Mar/24 10:39;mapohl;https://github.com/apache/flink/actions/runs/8290287446/job/22688312326#step:10:14956;;;","21/Mar/24 11:11;mapohl;SetOperatorsITCase: https://github.com/apache/flink/actions/runs/8352823891/job/22863768994#step:10:12399;;;","25/Mar/24 16:33;mapohl;https://github.com/apache/flink/actions/runs/8414062328/job/23037443503#step:10:12562;;;","02/Apr/24 16:00;rskraba;1.18, adaptive scheduler: [https://github.com/apache/flink/actions/runs/8502821617/job/23287705033#step:10:11601] 

This time on SemiAntiJoinStreamITCase.testGenericAntiJoin  (but with the same symptoms);;;","08/Apr/24 14:52;rskraba;1.18 adaptive scheduler https://github.com/apache/flink/actions/runs/8577964007/job/23511401057#step:10:11866
;;;","12/Apr/24 12:12;rskraba;1.18 AdaptiveScheduler: Test (module: table) https://github.com/apache/flink/actions/runs/8655936112/job/23735927701#step:10:12070 
This time on WindowDeduplicateITCase;;;","18/Apr/24 09:05;rskraba;1.19 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/8731358221/job/23956907827#step:10:12482;;;","19/Apr/24 13:34;rskraba;1.18 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/8747381390/job/24006045095#step:10:12211;;;","22/Apr/24 11:55;rskraba;1.18 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/8769422951/job/24065034854#step:10:14503
1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/8777471561/job/24082689462#step:10:13087;;;","03/May/24 15:54;rskraba;* 1.18 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/8904361381/job/24453748069#step:10:14980
* 1.18 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/8809948818/job/24181785187#step:10:17166
;;;","06/May/24 08:09;rskraba;* 1.18 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/8947210915/job/24579026057#step:10:14494
;;;","13/May/24 14:00;rskraba;* 1.19 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9056197345/job/24878932830#step:10:14273
* 1.18 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9056197329/job/24879136968#step:10:11976
;;;","27/May/24 15:15;rskraba;* 1.18 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9248172203/job/25438330034#step:10:15163
* 1.18 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9239908314/job/25419753266#step:10:12055;;;"
Add the backoff-multiplier configuration in ExponentialBackoffRetryStrategy,FLINK-34226,13565914,13559834,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiangyu0xf,xiangyu0xf,24/Jan/24 09:09,24/Jan/24 09:09,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-24 09:09:46.0,,,,,,,,,,"0|z1mykg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase.testHybridSelectiveExchangesRestart failed due to NullPointerException,FLINK-34225,13565901,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunfengzhou,mapohl,mapohl,24/Jan/24 08:00,30/Jan/24 08:07,04/Jun/24 20:40,30/Jan/24 08:04,1.19.0,,,,,,,1.19.0,,,,Runtime / Network,,,,0,github-actions,pull-request-available,test-stability,,"This test failed in a master nightly workflow run in GitHub Actions ([FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink's+current+Azure+CI+infrastructure]) which is based on master@[fd673a2f4|https://github.com/apache/flink/commit/fd673a2f46206ff65978f05fcb96b525696aead2]
https://github.com/XComp/flink/actions/runs/7632434859/job/20793612930#step:10:8625

{code}
Error: 01:07:53 01:07:53.367 [ERROR] Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 16.85 s <<< FAILURE! -- in org.apache.flink.test.runtime.HybridShuffleITCase
Error: 01:07:53 01:07:53.367 [ERROR] org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart -- Time elapsed: 1.164 s <<< FAILURE!
Jan 24 01:07:53 java.lang.AssertionError: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=0)
Jan 24 01:07:53 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:59)
Jan 24 01:07:53 	at org.apache.flink.test.runtime.BatchShuffleITCaseBase.executeJob(BatchShuffleITCaseBase.java:137)
Jan 24 01:07:53 	at org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart(HybridShuffleITCase.java:91)
Jan 24 01:07:53 	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
Jan 24 01:07:53 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
Jan 24 01:07:53 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Jan 24 01:07:53 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
Jan 24 01:07:53 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Jan 24 01:07:53 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
Jan 24 01:07:53 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Jan 24 01:07:53 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Jan 24 01:07:53 	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024)
Jan 24 01:07:53 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
Jan 24 01:07:53 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
Jan 24 01:07:53 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
Jan 24 01:07:53 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
Jan 24 01:07:53 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Jan 24 01:07:53 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
Jan 24 01:07:53 	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276)
Jan 24 01:07:53 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708)
Jan 24 01:07:53 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
Jan 24 01:07:53 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
Jan 24 01:07:53 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
Jan 24 01:07:53 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
Jan 24 01:07:53 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Jan 24 01:07:53 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
Jan 24 01:07:53 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Jan 24 01:07:53 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
Jan 24 01:07:53 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
Jan 24 01:07:53 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
Jan 24 01:07:53 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
Jan 24 01:07:53 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Jan 24 01:07:53 Caused by: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=0)
Jan 24 01:07:53 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
Jan 24 01:07:53 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
Jan 24 01:07:53 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:277)
Jan 24 01:07:53 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:268)
Jan 24 01:07:53 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:261)
Jan 24 01:07:53 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:770)
Jan 24 01:07:53 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:747)
Jan 24 01:07:53 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
Jan 24 01:07:53 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)
Jan 24 01:07:53 	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
Jan 24 01:07:53 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
Jan 24 01:07:53 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
Jan 24 01:07:53 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
Jan 24 01:07:53 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
Jan 24 01:07:53 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
Jan 24 01:07:53 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
Jan 24 01:07:53 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
Jan 24 01:07:53 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
Jan 24 01:07:53 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
Jan 24 01:07:53 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
Jan 24 01:07:53 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
Jan 24 01:07:53 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
Jan 24 01:07:53 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Jan 24 01:07:53 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Jan 24 01:07:53 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
Jan 24 01:07:53 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
Jan 24 01:07:53 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
Jan 24 01:07:53 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
Jan 24 01:07:53 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
Jan 24 01:07:53 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
Jan 24 01:07:53 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
Jan 24 01:07:53 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
Jan 24 01:07:53 	... 5 more
Jan 24 01:07:53 Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Fatal error at remote task manager 'localhost/127.0.0.1:42319 [ e9ae6545-d016-40a8-934c-3e46e5605fec ] '.
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.decodeMsg(CreditBasedPartitionRequestClientHandler.java:308)
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelRead(CreditBasedPartitionRequestClientHandler.java:191)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelRead(NettyMessageClientDecoderDelegate.java:112)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:800)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
Jan 24 01:07:53 	at java.base/java.lang.Thread.run(Thread.java:1583)
Jan 24 01:07:53 Caused by: java.io.IOException: java.io.IOException: Cannot invoke ""java.lang.Integer.intValue()"" because ""java.util.Queue.peek().f1"" is null
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:357)
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.enqueueAvailableReader(PartitionRequestQueue.java:125)
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.userEventTriggered(PartitionRequestQueue.java:253)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:400)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:376)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:368)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:387)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:400)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:376)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:368)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.userEventTriggered(DefaultChannelPipeline.java:1428)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:396)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:376)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireUserEventTriggered(DefaultChannelPipeline.java:913)
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.lambda$notifyReaderNonEmpty$0(PartitionRequestQueue.java:93)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
Jan 24 01:07:53 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:413)
Jan 24 01:07:53 	... 3 more
Jan 24 01:07:53 Caused by: java.lang.NullPointerException: java.lang.NullPointerException
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.partition.UnionResultSubpartitionView.peekNextBufferSubpartitionId(UnionResultSubpartitionView.java:92)
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.CreditBasedSequenceNumberingViewReader.peekNextBufferSubpartitionId(CreditBasedSequenceNumberingViewReader.java:250)
Jan 24 01:07:53 	at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:317)
Jan 24 01:07:53 	... 22 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33743,,,,,,FLINK-34233,,,,,,"24/Jan/24 08:08;mapohl;FLINK-34225.log;https://issues.apache.org/jira/secure/attachment/13066217/FLINK-34225.log",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 08:04:44 UTC 2024,,,,,,,,,,"0|z1myhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 08:13;mapohl;I attached the logs of the test failure to this issue. The {{NullPointerException}} originated in [PartitionRequestQueue#writeAndFlushNextMessageIfPossible|https://github.com/apache/flink/blob/439d1091daa12803268bb8ff7c0642bcc5f9127c/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestQueue.java#L317] which was added with FLINK-33743 recently. 

[~yunfengzhou] can you have a look at this?;;;","24/Jan/24 08:14;mapohl;I'm raising this as a blocker because it's seems to be related to a feature that was added in 1.19. Feel free to alter the priority.

Additionally, let me know if you need help with the GitHub Actions workflow (since it's not part of the Apache Flink main repo, yet.;;;","24/Jan/24 09:01;yunfengzhou;Hi [~mapohl] thanks for raising this bug. I am trying to fix this problem and the progress can be tracked in the following PR (for now it's still a draft). 

[https://github.com/apache/flink/pull/24151]

 ;;;","25/Jan/24 10:59;mapohl;https://github.com/XComp/flink/actions/runs/7646851691/job/20837497723#step:10:8885;;;","30/Jan/24 08:04;Weijie Guo;This also fixed via 973190e8ca5b7225f18b5c176726ef8680faffca.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogStorageMetricsTest.testAttemptsPerUpload(ChangelogStorageMetricsTest timed out,FLINK-34224,13565900,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,24/Jan/24 07:58,27/May/24 15:15,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,Runtime / State Backends,,,,0,github-actions,test-stability,,,"The timeout appeared in the GitHub Actions workflow (currently in test phase; [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure]):
https://github.com/XComp/flink/actions/runs/7632434859/job/20793613726#step:10:11040

{code}
Jan 24 01:38:36 ""ForkJoinPool-1-worker-1"" #16 daemon prio=5 os_prio=0 tid=0x00007f3b200ae800 nid=0x406e3 waiting on condition [0x00007f3b1ba0e000]
Jan 24 01:38:36    java.lang.Thread.State: WAITING (parking)
Jan 24 01:38:36 	at sun.misc.Unsafe.park(Native Method)
Jan 24 01:38:36 	- parking to wait for  <0x00000000dfbbb358> (a java.util.concurrent.CompletableFuture$Signaller)
Jan 24 01:38:36 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 24 01:38:36 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Jan 24 01:38:36 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Jan 24 01:38:36 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Jan 24 01:38:36 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 24 01:38:36 	at org.apache.flink.changelog.fs.ChangelogStorageMetricsTest.testAttemptsPerUpload(ChangelogStorageMetricsTest.java:251)
Jan 24 01:38:36 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30023,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 15:15:26 UTC 2024,,,,,,,,,,"0|z1myhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/24 10:34;mapohl;https://github.com/apache/flink/actions/runs/7925038590/job/21637838951#step:10:11756;;;","27/Feb/24 07:38;mapohl;https://github.com/apache/flink/actions/runs/8058552355/job/22011781420#step:10:10549;;;","01/Mar/24 10:14;mapohl;Both workflow runs appeared in close timely proximity:
* https://github.com/apache/flink/actions/runs/8105495458/job/22154216852#step:10:11823
* https://github.com/apache/flink/actions/runs/8105495552/job/22154215442#step:10:10934;;;","08/Mar/24 16:27;rskraba;1.18: [https://github.com/apache/flink/actions/runs/8181761395/job/22372333985#step:10:10556]

1.19: https://github.com/apache/flink/actions/runs/8197437597/job/22419673611#step:10:10874;;;","02/Apr/24 16:00;rskraba;1.20, hadoop-3.1.3: https://github.com/apache/flink/actions/runs/8495048430/job/23271290787#step:10:10970;;;","02/Apr/24 16:10;rskraba;1.19, JDK8: [https://github.com/apache/flink/actions/runs/8516411871/job/23325656205#step:10:12135]

 ;;;","11/Apr/24 14:37;rskraba;1.20 Java 8: Test (module: core) https://github.com/apache/flink/actions/runs/8643097154/job/23696501028#step:10:11318;;;","18/Apr/24 09:05;rskraba;1.20 Hadoop 3.1.3 / Test (module: core) https://github.com/apache/flink/actions/runs/8731358306/job/23956935029#step:10:12643;;;","10/May/24 09:39;rskraba;* 1.18 Hadoop 3.1.3 / Test (module: core) https://github.com/apache/flink/actions/runs/9011311755/job/24759083100#step:10:10641;;;","27/May/24 15:15;rskraba;* 1.20 Hadoop 3.1.3 / Test (module: core) https://github.com/apache/flink/actions/runs/9239908683/job/25419763061#step:10:12699;;;",,,,,,,,,,,,,,,,,,,,,,,
Introduce a migration tool to transfer legacy config file to new config file,FLINK-34223,13565890,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,24/Jan/24 06:35,26/Jan/24 10:51,04/Jun/24 20:40,26/Jan/24 10:51,,,,,,,,1.19.0,,,,Deployment / Scripts,,,,0,pull-request-available,,,,"As we transition to new configuration files that adhere to the standard YAML format, users are expected to manually migrate their existing config files. However, this process can be error-prone and time-consuming.

To simplify the migration, we're introducing an automated script. This script leverages BashJavaUtils to efficiently convert old flink-conf.yaml files into the new config file config.yaml, thereby reducing the effort required for migration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 10:51:49 UTC 2024,,,,,,,,,,"0|z1myf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 10:51;zhuzh;master/release-1.19:
8fceb101e7e45f3fdc9357019757230bd8c16aa7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports mini-batch for streaming regular join,FLINK-34222,13565887,13565884,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xu_shuai_,xu_shuai_,xu_shuai_,24/Jan/24 06:28,29/Jan/24 06:39,04/Jun/24 20:40,29/Jan/24 06:38,,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,Implement minibatch join in E2E which includes both plan and runtime parts.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 06:39:50 UTC 2024,,,,,,,,,,"0|z1myeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 06:39;lsy;Merged in master: 20450485b20cb213b96318b0c3275e42c0300e15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce operator for minibatch join,FLINK-34221,13565886,13565884,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xu_shuai_,xu_shuai_,xu_shuai_,24/Jan/24 06:27,26/Jan/24 01:49,04/Jun/24 20:40,26/Jan/24 01:49,,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,Introduce operator that implements minibatch join,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 01:49:29 UTC 2024,,,,,,,,,,"0|z1mye8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 01:49;lsy;Merged in master: 8170c457cb70bb8fd88b98baf3acc612eaab8ec5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
introduce buffer bundle for minibatch join,FLINK-34220,13565885,13565884,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xu_shuai_,xu_shuai_,xu_shuai_,24/Jan/24 06:26,25/Jan/24 09:38,04/Jun/24 20:40,25/Jan/24 09:38,,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,introduce buffer bundle for storing records to implement minibatch join,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 09:38:12 UTC 2024,,,,,,,,,,"0|z1mye0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 09:38;lsy;Merged in master: 1bf27c6362413709226cd5d6f3dc5ae9223cc37c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a new join operator to support minibatch,FLINK-34219,13565884,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xu_shuai_,xu_shuai_,xu_shuai_,24/Jan/24 06:23,21/Feb/24 05:52,04/Jun/24 20:40,06/Feb/24 08:10,,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,,,,,This is the parent task of FLIP-415.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 01:36:14 UTC 2024,,,,,,,,,,"0|z1myds:",9223372036854775807,Support minibatch regular join to reduce intermediate result and resolve record amplification in cascading join scenarios.,,,,,,,,,,,,,,,,,,,"29/Jan/24 05:45;lincoln.86xy;[~xu_shuai_] For this new optimization, can we add some user documentation, for example, a new section on this page: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tuning/#minibatch-aggregation;;;","21/Feb/24 01:36;lincoln.86xy;[~xu_shuai_] Should we also add release notes for this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutoRescalingITCase#testCheckpointRescalingInKeyedState fails,FLINK-34218,13565875,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,fanrui,fanrui,24/Jan/24 04:00,25/Jan/24 10:45,04/Jun/24 20:40,25/Jan/24 10:45,,,,,,,,,,,,Tests,,,,0,,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56740&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&s=ae4f8708-9994-57d3-c2d7-b892156e7812,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 10:45:11 UTC 2024,,,,,,,,,,"0|z1mybs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 04:03;fanrui;Hi [~srichter], AutoRescalingITCase#testCheckpointRescalingInKeyedState fails occasionally, would you mind helping take a look? Thanks

I'm not sure whether current flink core logic has some bugs or this test has some bugs.;;;","24/Jan/24 04:50;fanrui;I run it in my local more than 30 times, all of them are successful.;;;","24/Jan/24 10:11;srichter;Hi, I also cannot reproduce the problem locally, but I'm rather confident that it's a test problem.;;;","25/Jan/24 10:45;mapohl;I'm gonna close this issue in favor of FLINK-34200. There are two other CI failures documented already. I will move the CI failure, [~fanrui] shared to FLINK-34200.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Serialization-related doc with the new way of configuration,FLINK-34217,13565836,13564005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,23/Jan/24 16:49,04/Feb/24 08:41,04/Jun/24 20:40,04/Feb/24 08:41,1.19.0,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 04 08:41:39 UTC 2024,,,,,,,,,,"0|z1my34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 08:41;Weijie Guo;1.19 via 60795b78dce7b50a0e3326adb277d6644ea38437.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support fine-grained configuration to control filter push down for MongoDB Connector,FLINK-34216,13565821,13565819,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jiabao.sun,jiabao.sun,23/Jan/24 15:25,17/Apr/24 09:32,04/Jun/24 20:40,,mongodb-1.0.2,,,,,,,mongodb-1.3.0,,,,Connectors / MongoDB,,,,0,pull-request-available,,,,Support fine-grained configuration to control filter push down for MongoDB Connector.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-23 15:25:35.0,,,,,,,,,,"0|z1mxzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support fine-grained configuration to control filter push down for JDBC Connector,FLINK-34215,13565820,13565819,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jiabao.sun,jiabao.sun,23/Jan/24 15:24,25/Jan/24 03:50,04/Jun/24 20:40,,jdbc-3.1.2,,,,,,,jdbc-3.1.3,,,,Connectors / JDBC,,,,0,pull-request-available,,,,Support fine-grained configuration to control filter push down for JDBC Connector.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-23 15:24:41.0,,,,,,,,,,"0|z1mxzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-377: Support fine-grained configuration to control filter push down for Table/SQL Sources,FLINK-34214,13565819,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jiabao.sun,jiabao.sun,jiabao.sun,23/Jan/24 15:20,17/Apr/24 09:32,04/Jun/24 20:40,,jdbc-3.1.2,mongodb-1.0.2,,,,,,jdbc-3.1.3,mongodb-1.3.0,,,Connectors / JDBC,Connectors / MongoDB,,,0,,,,,"This improvement implements [FLIP-377 Support fine-grained configuration to control filter push down for Table/SQL Sources|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=276105768]

This FLIP has 2 goals:
 * Introduces a new configuration filter.handling.policy to the JDBC and MongoDB connector.
 * Suggests a convention option name if other connectors are going to add an option for the same purpose.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 11 08:24:50 UTC 2024,,,,,,,,,,"0|z1mxzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 06:42;stayrascal;Hi, may i consulting one question that if we disable the push down for JDBC/MongoDB source, does it mean the source will scan whole data?

The performance of scan whole data is worse than filter data via unindexed column?;;;","11/Feb/24 08:24;jiabaosun;Hi [~stayrascal]. 
When pushing to the database without filters, we can iterate through the data in batches using primary key indexing or natural order and filter the data using external computing resources. This greatly reduces the computational overhead on the database for filters that do not hit the indexes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider using accumulated busy time instead of busyMsPerSecond,FLINK-34213,13565799,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gyfora,mxm,mxm,23/Jan/24 11:36,20/Feb/24 11:20,04/Jun/24 20:40,20/Feb/24 11:20,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,Kubernetes Operator,,,0,,,,,"We might achieve much better accuracy if we used the accumulated busy time metrics from Flink, instead of the momentarily collected ones.

We would use the diff between the last accumulated and the current accumulated busy time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34266,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 11:20:37 UTC 2024,,,,,,,,,,"0|z1mxuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 12:10;gyfora;The problem here is that we are using aggregated metrics only currently. To track diff you would need to query individual vertex metrics and track them overhead which would be a huge overhead / cost for jobs with many vertices;;;","23/Jan/24 15:20;mxm;If we had to query metrics per vertex, that would be too expensive, but it seems like that is not necessary. Here is an exemplary REST API response to the {{/jobs/<jobId>}} endpoint:

{noformat}
{
    ""jid"": ""b4f918c2a0312de9fe7369a7db093e96"",
    ""name"": ""-----"",
    ""isStoppable"": false,
    ""state"": ""RUNNING"",
    ""start-time"": 1705094021727,
    ""end-time"": -1,
    ""duration"": 928985186,
    ""maxParallelism"": 10000,
    ""now"": 1706023006913,
    ""timestamps"": {
        ""SUSPENDED"": 0,
        ""RUNNING"": 1705094036134,
        ""FAILING"": 0,
        ""CANCELED"": 0,
        ""CANCELLING"": 0,
        ""CREATED"": 1705094035034,
        ""INITIALIZING"": 1705094021727,
        ""FAILED"": 0,
        ""RESTARTING"": 0,
        ""RECONCILING"": 0,
        ""FINISHED"": 0
    },
    ""vertices"": [
        {
            ""id"": ""db1f263dc155338dc2a9622a2e06d115"",
            ""name"": ""----"",
            ""maxParallelism"": 10000,
            ""parallelism"": 18,
            ""status"": ""RUNNING"",
            ""start-time"": 1705094037437,
            ""end-time"": -1,
            ""duration"": 928969476,
            ""tasks"": {
                ""CANCELED"": 0,
                ""DEPLOYING"": 0,
                ""CANCELING"": 0,
                ""RECONCILING"": 0,
                ""FINISHED"": 0,
                ""SCHEDULED"": 0,
                ""CREATED"": 0,
                ""INITIALIZING"": 0,
                ""FAILED"": 0,
                ""RUNNING"": 18
            },
            ""metrics"": {
                ""read-bytes"": 0,
                ""read-bytes-complete"": true,
                ""write-bytes"": 2907138853415272,
                ""write-bytes-complete"": true,
                ""read-records"": 0,
                ""read-records-complete"": true,
                ""write-records"": 229589536334,
                ""write-records-complete"": true,
                ""accumulated-backpressured-time"": 1533744940,
                ""accumulated-idle-time"": 10026044858,
                ""accumulated-busy-time"": 5161601268
            }
        },
   ...........
    ]
}
{noformat}

Note the accumulated backpressure/idle time.;;;","20/Feb/24 11:20;gyfora;merged to main 0a7588c084e7197f73e659e763f4d82d2d94ae76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler Standalone should clean up stopped jobs to prevent memory leaks,FLINK-34212,13565788,13556703,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,23/Jan/24 10:25,09/May/24 08:01,04/Jun/24 20:40,09/May/24 08:01,,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,,,,0,pull-request-available,,,,"Autoscaler Standalone will fetch job list, and scale them. In general, autoscaler has some cache in memory. 

Autoscaler Standalone should clean up them if some jobs cannot be fetched anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 09 08:00:56 UTC 2024,,,,,,,,,,"0|z1mxsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 08:00;fanrui;merged to main(1.9.0) via 04829bf1fadb45d8f76849f4263596c01b94fee3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filtering on Column names with ?s fails for JDBC lookup join. ,FLINK-34211,13565778,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,davidradl,davidradl,23/Jan/24 09:50,27/Feb/24 12:53,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / JDBC,Table SQL / JDBC,,,0,,,,,"There is a check for ? character in 

[https://github.com/apache/flink-connector-jdbc/blob/e3dd84160cd665ae17672da8b6e742e61a72a32d/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatementImpl.java#L186 |FieldNamedPreparedStatementImpl.java]

Removing this check allows column names containing _?_ ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 12:53:35 UTC 2024,,,,,,,,,,"0|z1mxq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 09:55;davidradl;fyi JDBC lookup Filters are ignored until the fix for [https://issues.apache.org/jira/projects/FLINK/issues/FLINK-33365] is merged;;;","07/Feb/24 06:10;xuyangzhong;Hi, [~davidradl] this Jira looks like an improvement, not a bug, right?;;;","27/Feb/24 12:53;davidradl;Hi [~xuyangzhong] , I have changed this to an improvement. Are you ok to assign this to me and merge the pr (with unit tests)? As the fix seems reasonably obvious; or do you think there are other considerations around this that warrant deeper discussion? As part of this I can add a tests for ? for scan queries.    

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultExecutionGraphBuilder#isCheckpointingEnabled may return Wrong Value when checkpoint disabled,FLINK-34210,13565777,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mayuehappy,mayuehappy,mayuehappy,23/Jan/24 09:49,25/Jan/24 13:44,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"The *DefaultExecutionGraphBuilder* will call 
_{*}isCheckpointingEnabled{*}(JobGraph jobGraph)_
to determine whether the job has enabled Checkpoint and whether to initialize CheckpointCoordinator related components such as CheckpointCoordinator, CheckpointIDCounter , etc.

 
{code:java}
// DefaultExecutionGraphBuilder#isCheckpointingEnabled

public static boolean isCheckpointingEnabled(JobGraph jobGraph) {
    return jobGraph.isCheckpointingEnabled();
}{code}
 

The problem is that the logic for determining isCheckpointingEnable here is inaccurate, as *jobGraph. getCheckpointingSettings()* will not be NULL when checkpoint is not enabled, but with CheckpointCoordinatorConfiguration.DISABLED_CHECKPOINT_INTERVAL Interval

 
{code:java}
// JobGraph#isCheckpointingEnabled

public boolean isCheckpointingEnabled() {

    if (snapshotSettings == null) {
        return false;
    }

    return snapshotSettings.getCheckpointCoordinatorConfiguration().isCheckpointingEnabled();
} {code}
 

 

The method to fix this problem is also quite clear. We need to directly reuse the result of jobGraph.isCheckpointingEnable() here

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-23 09:49:23.0,,,,,,,,,,"0|z1mxq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate FileSink to the new SinkV2 API,FLINK-34209,13565771,13563444,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,23/Jan/24 08:31,26/Jan/24 07:04,04/Jun/24 20:40,25/Jan/24 13:15,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,Currently `FileSink` uses `TwoPhaseCommittingSink` and `StatefulSink` from the SinkV2 API. We should migrate it to use the new SinkV2 API,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 13:15:57 UTC 2024,,,,,,,,,,"0|z1mxoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 13:15;gyfora;merged to master 70136905ad16c83c55d0bc403d8b060ac759e6a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate SinkV1Adapter to the new SinkV2 API,FLINK-34208,13565769,13563444,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,23/Jan/24 08:29,24/Jan/24 09:37,04/Jun/24 20:40,24/Jan/24 09:37,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,"Currently SinkV1Adapter still using `TwoPhaseCommittingSink` and `StatefulSink`.
We should migrate it to use the new API",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 09:37:52 UTC 2024,,,,,,,,,,"0|z1mxo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 09:37;gyfora;merged to master 2cf213ec9e4db81815a9b37013904aa47a94aa01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink:flink-shaded-jackson:jar:2.15.3-18.0 couldn't be downloaded,FLINK-34207,13565765,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,mapohl,mapohl,23/Jan/24 07:38,27/Feb/24 06:35,04/Jun/24 20:40,31/Jan/24 15:02,1.19.0,,,,,,,,,,,Build System / CI,BuildSystem / Shaded,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56728&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=1368

{code}
Jan 23 00:55:44 00:55:44.078 [ERROR] Failed to execute goal on project flink-core: Could not resolve dependencies for project org.apache.flink:flink-core:jar:1.19-SNAPSHOT: Could not transfer artifact org.apache.flink:flink-shaded-jackson:jar:2.15.3-18.0 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: org/apache/flink/flink-shaded-jackson/2.15.3-18.0/flink-shaded-jackson-2.15.3-18.0.jar from google-maven-central failed: Connection reset -> [Help 1]
Jan 23 00:55:44 00:55:44.078 [ERROR] 
Jan 23 00:55:44 00:55:44.078 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 23 00:55:44 00:55:44.078 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 23 00:55:44 00:55:44.078 [ERROR] 
Jan 23 00:55:44 00:55:44.078 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 23 00:55:44 00:55:44.078 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
Jan 23 00:55:44 00:55:44.078 [ERROR] 
Jan 23 00:55:44 00:55:44.078 [ERROR] After correcting the problems, you can resume the build with the command
Jan 23 00:55:44 00:55:44.078 [ERROR]   mvn <args> -rf :flink-core
{code}",,,,,,,,,,,,,,,,,,,,FLINK-34523,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-23 07:38:43.0,,,,,,,,,,"0|z1mxnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CacheITCase.testRetryOnCorruptedClusterDataset(Path) failed,FLINK-34206,13565764,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xiasun,mapohl,mapohl,23/Jan/24 07:36,01/Feb/24 07:56,04/Jun/24 20:40,01/Feb/24 07:52,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56728&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=8763
{code}
Jan 23 01:39:48 01:39:48.152 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.24 s <<< FAILURE! -- in org.apache.flink.test.streaming.runtime.CacheITCase
Jan 23 01:39:48 01:39:48.152 [ERROR] org.apache.flink.test.streaming.runtime.CacheITCase.testRetryOnCorruptedClusterDataset(Path) -- Time elapsed: 4.755 s <<< ERROR!
Jan 23 01:39:48 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Jan 23 01:39:48 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
Jan 23 01:39:48 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:646)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2179)
Jan 23 01:39:48 	at org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2179)
Jan 23 01:39:48 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1287)
Jan 23 01:39:48 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93)
Jan 23 01:39:48 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Jan 23 01:39:48 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2179)
Jan 23 01:39:48 	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)
Jan 23 01:39:48 	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)
Jan 23 01:39:48 	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)
Jan 23 01:39:48 	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)
Jan 23 01:39:48 	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)
Jan 23 01:39:48 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
Jan 23 01:39:48 	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)
Jan 23 01:39:48 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
Jan 23 01:39:48 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
Jan 23 01:39:48 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
Jan 23 01:39:48 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
Jan 23 01:39:48 	at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
Jan 23 01:39:48 	at org.apache.pekko.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:128)
Jan 23 01:39:48 	at org.apache.pekko.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:1154)
Jan 23 01:39:48 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
Jan 23 01:39:48 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
Jan 23 01:39:48 	at org.apache.pekko.remote.EndpointActor.aroundReceive(Endpoint.scala:550)
Jan 23 01:39:48 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
Jan 23 01:39:48 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
Jan 23 01:39:48 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
Jan 23 01:39:48 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
Jan 23 01:39:48 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
Jan 23 01:39:48 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
Jan 23 01:39:48 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
Jan 23 01:39:48 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
Jan 23 01:39:48 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
Jan 23 01:39:48 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Jan 23 01:39:48 Caused by: org.apache.flink.runtime.execution.SuppressRestartsException: Unrecoverable failure. This suppresses job restarts. Please check the stack trace for the root cause.
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.lambda$tryComputeSourceParallelismThenRunAsync$7(AdaptiveBatchScheduler.java:285)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:990)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:974)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:614)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:844)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:482)
Jan 23 01:39:48 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
Jan 23 01:39:48 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Jan 23 01:39:48 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
Jan 23 01:39:48 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
Jan 23 01:39:48 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
Jan 23 01:39:48 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
Jan 23 01:39:48 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
Jan 23 01:39:48 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
Jan 23 01:39:48 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
Jan 23 01:39:48 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
Jan 23 01:39:48 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
Jan 23 01:39:48 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
Jan 23 01:39:48 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Jan 23 01:39:48 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Jan 23 01:39:48 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
Jan 23 01:39:48 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
Jan 23 01:39:48 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
Jan 23 01:39:48 	... 10 more
Jan 23 01:39:48 Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Expected execution c2dc985dca4e7acdbcba039b20654a06_306d8342cb5b2ad8b53f1be57f65bee8_1_0 to be in CREATED state, was: CANCELED
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:332)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:347)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:874)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)
Jan 23 01:39:48 	... 28 more
Jan 23 01:39:48 Caused by: java.lang.IllegalStateException: Expected execution c2dc985dca4e7acdbcba039b20654a06_306d8342cb5b2ad8b53f1be57f65bee8_1_0 to be in CREATED state, was: CANCELED
Jan 23 01:39:48 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$validateExecutionStates$0(DefaultExecutionDeployer.java:110)
Jan 23 01:39:48 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.validateExecutionStates(DefaultExecutionDeployer.java:108)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.allocateSlotsAndDeploy(DefaultExecutionDeployer.java:93)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlotsAndDeploy(DefaultScheduler.java:475)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.allocateSlotsAndDeploy(AdaptiveBatchScheduler.java:237)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.lambda$scheduleVerticesOneByOne$3(VertexwiseSchedulingStrategy.java:207)
Jan 23 01:39:48 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.scheduleVerticesOneByOne(VertexwiseSchedulingStrategy.java:206)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.maybeScheduleVertices(VertexwiseSchedulingStrategy.java:145)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.strategy.VertexwiseSchedulingStrategy.onExecutionStateChange(VertexwiseSchedulingStrategy.java:114)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFinished(DefaultScheduler.java:251)
Jan 23 01:39:48 	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.lambda$onTaskFinished$1(AdaptiveBatchScheduler.java:202)
Jan 23 01:39:48 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)
Jan 23 01:39:48 	... 29 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 07:52:13 UTC 2024,,,,,,,,,,"0|z1mxn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 10:09;mapohl;Same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56791&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=9107
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56791&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=9269;;;","25/Jan/24 10:30;mapohl;I'm not able to pinpoint the cause of this test failure. The test itself wasn't touched for a while. The test is based on the AdaptiveBatchScheduler which hasn't been touched for some time either. I'm raising the priority to blocker for now until the cause of this issue is identified. [~lincoln] [~yunta] [~jingge] [~martijnvisser] Any idea who we could ping?;;;","25/Jan/24 10:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56808&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9150;;;","25/Jan/24 10:42;mapohl;Same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56859&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=8777
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56859&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9342;;;","25/Jan/24 10:48;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56865&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9151;;;","25/Jan/24 10:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56869&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9150;;;","25/Jan/24 12:15;lincoln.86xy;[~xiasun] Can you take a look at this? The stack trace seems to be related to the new dynamic source parallelism
{code}
at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.lambda$tryComputeSourceParallelismThenRunAsync$7(AdaptiveBatchScheduler.java:285)
{code}
cc [~zhuzh];;;","25/Jan/24 15:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56893&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8828;;;","26/Jan/24 01:57;zhuzh;We will take a look.;;;","26/Jan/24 11:29;xiasun; [~mapohl] [~lincoln.86xy] [~zhuzh] We have located the issue, however, the fix will take some time. This problem is only likely to be triggered when a global failure and task finish occur simultaneously. In order not to block the CI, we plan to temporarily ignore this case and will enable it again after the issue is fixed.;;;","26/Jan/24 11:39;zhuzh;Disabled {{CacheITCase.testRetryOnCorruptedClusterDataset}} temporarily via 05ee359ebd564af3dd8ab31975cd479e92ba1785;;;","26/Jan/24 15:45;mapohl;The following builds don't have the test disabled, yet. I'm adding them for documentation purposes:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56942&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9364]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56942&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=9216]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56948&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8829];;;","01/Feb/24 07:52;zhuzh;Fixed via b737b71859672e8020881ce2abf998735ee98abb;;;",,,,,,,,,,,,,,,,,,,,
Update flink-docker's Dockerfile and docker-entrypoint.sh to use BashJavaUtils for Flink configuration management,FLINK-34205,13565745,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,23/Jan/24 03:45,24/Jan/24 06:19,04/Jun/24 20:40,24/Jan/24 06:19,,,,,,,,1.19.0,,,,flink-docker,,,,0,pull-request-available,,,,"The flink-docker's Dockerfile and docker-entrypoint.sh currently use shell scripting techniques with grep and sed for configuration reading and modification. This method is not suitable for the standard YAML configuration format.

Following the changes introduced in FLINK-33721, we should update flink-docker's Dockerfile and docker-entrypoint.sh to use BashJavaUtils for Flink configuration reading and writing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 06:19:08 UTC 2024,,,,,,,,,,"0|z1mxiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 06:19;zhuzh;dev-master:
44f058287cc956a620b12b6f8ed214e44dc3db77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dispatch legacy serializer related methods in ExecutionConfig to SerializerConfig,FLINK-34204,13565743,13564005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,23/Jan/24 03:17,29/Jan/24 15:20,04/Jun/24 20:40,26/Jan/24 08:45,,,,,,,,1.19.0,,,,API / Type Serialization System,,,,0,pull-request-available,,,,"Since we already support set serializer related things via config options, we need to dispatch the relevant methods to the new implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 15:20:21 UTC 2024,,,,,,,,,,"0|z1mxig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 08:45;Weijie Guo;master(1.19) via 84444d5071984a8543a81574dc69c0c9849a8a4a;;;","29/Jan/24 15:20;mapohl;master (1.19): eb50b4eddab8ad8f79e99be38c42d8f451075c74 (compilation error fixed);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink `get_field_data_types()` supports only `precision = 3` for `TimestampType`,FLINK-34203,13565700,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mfatihaktas,mfatihaktas,22/Jan/24 18:51,22/Jan/24 18:51,04/Jun/24 20:40,,,,,,,,,,,,,API / Python,,,,0,,,,,"PyFlink schema's `get_field_data_types()` set `precision` to 3 for `TIMESTAMP` regardless of the input precision – copied the relevant pieces of code below. There does not seem to be any reason for this implicit assumption. It led to an error in our project, forcing us to implement a wrapper around `get_field_data_types()`.

 

pyflink/table/table_schema.py: get_field_data_types():
{code:java}
    def get_field_data_types(self) -> List[DataType]:
        """"""
        Returns all field data types as a list.        :return: A list of all field data types.
        """"""
        return [_from_java_data_type(item) for item in self._j_table_schema.getFieldDataTypes()] {code}
pyflink/table/types.py: _from_java_data_type():
{code:java}
def _from_java_data_type(j_data_type):
    """"""
    Converts Java DataType to Python DataType.
    """"""
    ...
    if is_instance_of(j_data_type, gateway.jvm.AtomicDataType):
        logical_type = j_data_type.getLogicalType()     
    ...

        elif is_instance_of(logical_type, gateway.jvm.TimestampType):
            data_type = DataTypes.TIMESTAMP(precision=3, nullable=logical_type.isNullable())
    ...{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-22 18:51:42.0,,,,,,,,,,"0|z1mx8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python tests take suspiciously long in some of the cases,FLINK-34202,13565680,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxb,mapohl,mapohl,22/Jan/24 16:02,21/Feb/24 16:18,04/Jun/24 20:40,21/Feb/24 14:30,1.17.2,1.18.1,1.19.0,,,,,1.17.3,1.18.2,1.19.0,,API / Python,,,,0,pull-request-available,test-stability,,,"[This release-1.18 build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56603&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a] has the python stage running into a timeout without any obvious reason. The [python stage run for JDK17|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56603&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06] was also getting close to the 4h timeout.

I'm creating this issue for documentation purposes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/24 10:36;lorenzo.affetti;Screenshot 2024-02-21 at 09.45.18.png;https://issues.apache.org/jira/secure/attachment/13066926/Screenshot+2024-02-21+at+09.45.18.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 16:18:58 UTC 2024,,,,,,,,,,"0|z1mx4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 16:03;mapohl;FYI: The build run 1 day after we had infrastructure issues with CI runners (FLINK-34135, FLINK-34098).;;;","23/Jan/24 07:33;mapohl;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56728&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=26728;;;","23/Jan/24 07:40;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56729&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=27046;;;","25/Jan/24 10:41;mapohl;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56859&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a;;;","25/Jan/24 10:47;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56860&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf;;;","30/Jan/24 09:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57080&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a;;;","30/Jan/24 09:16;mapohl;[~lincoln.86xy] do we have someone who can look into this?;;;","30/Jan/24 09:30;lincoln.86xy;[~dianfu] Sorry for the ping, but consider that you're the expert of this area, could you help take a look at this issue?;;;","02/Feb/24 07:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57203&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=27977;;;","02/Feb/24 08:19;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57205&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=27518;;;","05/Feb/24 07:37;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57271&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=27032;;;","06/Feb/24 07:33;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57324&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=27713;;;","06/Feb/24 08:36;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57325&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=27032;;;","08/Feb/24 08:39;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57388&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3;;;","16/Feb/24 07:50;mapohl;1.20 (master): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57550&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf;;;","18/Feb/24 06:16;dianfu;[~hxbks2ks] Could you help to take a look at this issue?;;;","18/Feb/24 10:40;hxb;I reviewed all the stages where timeouts occurred and found that these stages all ran on AlibabaCI001. Simultaneously, the runtime of all other successful stages is consistently around 2 hours and 40 minutes. In the logs, I didn't notice any tests being stuck or having an overly long runtime, so I think the timeout is largely due to AlibabaCI001's performance not being sufficient to complete 4 Python version tests within 4 hours. I submitted a PR to have the nightly CI randomly select a Python version for testing rather than running all 4 Python versions. By only running one Python version test, even if the machine's performance is poor, it should not exceed 2 hours (the lab triggered by the PR only runs the latest Python version, which takes about 40 minutes).;;;","19/Feb/24 01:28;lincoln.86xy;[~hxb] Thanks for investigating this issue! Regarding the solution, is it necessary to test all 4 python verisons in nightly ci build？[~dianfu] ;;;","19/Feb/24 02:45;dianfu;[~lincoln.86xy] It will randomly select one Python version to test and all the Python versions will be tested finally, however, maybe not in one nightly CI. Personally I think this should be acceptable as the case that some test only failed in one specific Python version rarely happens. Even it does, it will still be found, just with one day or two day delay and this should be acceptable.;;;","19/Feb/24 05:37;lincoln.86xy;[~dianfu] Yes, the difference is between testing 4 versions at a time and testing a random one but eventually covering 4 versions. Though in extreme cases the detection of a failure case can be delayed for several days, it will never be missed.
+1 for [~hxb]'s solution, it may be a reasonable one for now.  [~mapohl] WDYT?;;;","19/Feb/24 08:20;mapohl;Thanks for looking into it, everyone. We use random feature selection in other places as well (e.g. with [buffer debloating|https://github.com/apache/flink/blob/c8f27c25e8726360bd09fd21fa8e908c40376881/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/MiniClusterResource.java#L256]). So, generally, I'm not against it but leave the judgement to you who are more familiar with the Python module. On the other hand, [~hxb] concludes that there's something wrong with the Alibaba001 VM. Therefore, we might want to check that VM fix the actual issue there rather than working around it in the Python tests. The actual cause might lead to problems in other tests as well. WDYT?;;;","19/Feb/24 10:14;lincoln.86xy;1.20(master): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57573&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a;;;","19/Feb/24 10:20;lincoln.86xy;[~mapohl] Agree with you that we need to look at these two issues separately.
[~jingge] Can you help check if the Alibaba001 VM is normal?;;;","20/Feb/24 07:58;jingge;Thanks for the info, [~lorenzo.affetti] could you please help check the Alibaba001 VM?;;;","20/Feb/24 08:14;lorenzo.affetti;Sure thing (y);;;","20/Feb/24 14:30;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57649&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27389;;;","21/Feb/24 10:44;lorenzo.affetti;Update, CI001 looks just a bit overcommitted:

!Screenshot 2024-02-21 at 09.45.18.png|width=731,height=191!

All these Java processes are happening at the moment:
{code:java}
agent05   6537  0.0  0.0  17004  1412 ?        S    18:35   0:00 /bin/sh -c cd '/__w/2/s/flink-table/flink-table-planner' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.lang=ALL-UNNAMED' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED' '--add-opens=java.base/java.time=ALL-UNNAMED' '--add-opens=java.base/java.math=ALL-UNNAMED' '--add-opens=java.base/java.nio=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/2/s/flink-table/flink-table-planner/target/surefire/surefirebooter-20240221103036252_109.jar' '/__w/2/s/flink-table/flink-table-planner/target/surefire' '2024-02-21T10-23-54_036-jvmRun1' 'surefire-20240221103036252_105tmp' 'surefire_34-20240221103036252_107tmp'
agent05   6538  0.0  0.0  17004  1424 ?        S    18:35   0:00 /bin/sh -c cd '/__w/2/s/flink-table/flink-table-planner' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.lang=ALL-UNNAMED' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED' '--add-opens=java.base/java.time=ALL-UNNAMED' '--add-opens=java.base/java.math=ALL-UNNAMED' '--add-opens=java.base/java.nio=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/2/s/flink-table/flink-table-planner/target/surefire/surefirebooter-20240221103036252_110.jar' '/__w/2/s/flink-table/flink-table-planner/target/surefire' '2024-02-21T10-23-54_036-jvmRun2' 'surefire-20240221103036252_106tmp' 'surefire_35-20240221103036252_108tmp'
agent05   6542  231  1.9 4738124 1289100 ?     Sl   18:35   3:37 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.math=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED -Xmx1536m -jar /__w/2/s/flink-table/flink-table-planner/target/surefire/surefirebooter-20240221103036252_110.jar /__w/2/s/flink-table/flink-table-planner/target/surefire 2024-02-21T10-23-54_036-jvmRun2 surefire-20240221103036252_106tmp surefire_35-20240221103036252_108tmp
agent05   6547  168  2.3 4631284 1545452 ?     Sl   18:35   2:38 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.base/java.math=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED -Xmx1536m -jar /__w/2/s/flink-table/flink-table-planner/target/surefire/surefirebooter-20240221103036252_109.jar /__w/2/s/flink-table/flink-table-planner/target/surefire 2024-02-21T10-23-54_036-jvmRun1 surefire-20240221103036252_105tmp surefire_34-20240221103036252_107tmp
agent03   8139  5.4  2.9 19693224 1963424 ?    Sl   17:50   2:35 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -XX:+IgnoreUnrecognizedVMOptions --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED -classpath /__w/1/s/.mvn/wrapper/maven-wrapper.jar -Dmaven.multiModuleProjectDirectory=/__w/1/s org.apache.maven.wrapper.MavenWrapperMain -Dmaven.repo.local=/__w/1/.m2/repository -Dmaven.wagon.http.pool=false -Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat=HH:mm:ss.SSS -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn --no-snapshot-updates -B -Dflink.hadoop.version=2.10.2 --settings /__w/1/s/tools/ci/google-mirror-settings.xml -Dfast -Pskip-webui-build -Dlog.dir=/__w/_temp/debug_files -Dlog4j.configurationFile=file:///__w/1/s/tools/ci/log4j.properties -Dflink.tests.with-openssl -Dflink.tests.check-segment-multiple-free -Darchunit.freeze.store.default.allowStoreUpdate=false -Dpekko.rpc.force-invocation-serialization -Dflink.hadoop.version=2.10.2 -pl flink-tests, verify
agent04  12833  1.7  0.0      0     0 ?        Z    18:13   0:25 [java] <defunct>
agent04  15877  1.3  0.0      0     0 ?        Z    18:13   0:19 [java] <defunct>
agent04  16126  1.4  0.0      0     0 ?        Z    18:13   0:20 [java] <defunct>
200      18382 2629  5.4 15012168 3609384 ?    Ssl  Jan18 1291718:08 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.392.b08-4.el8.x86_64/jre/bin/java -server -Dinstall4j.jvmDir=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.392.b08-4.el8.x86_64/jre -Dexe4j.moduleName=/opt/sonatype/nexus/bin/nexus -XX:+UnlockDiagnosticVMOptions -Dinstall4j.launcherId=245 -Dinstall4j.swt=false -Di4jv=0 -Di4jv=0 -Di4jv=0 -Di4jv=0 -Di4jv=0 -Xms2703m -Xmx2703m -XX:MaxDirectMemorySize=2703m -Djava.util.prefs.userRoot=/nexus-data/javaprefs -XX:+UnlockDiagnosticVMOptions -XX:+LogVMOutput -XX:LogFile=../sonatype-work/nexus3/log/jvm.log -XX:-OmitStackTraceInFastThrow -Djava.net.preferIPv4Stack=true -Dkaraf.home=. -Dkaraf.base=. -Dkaraf.etc=etc/karaf -java.util.logging.config.file=etc/karaf/java.util.logging.properties -Dkaraf.data=../sonatype-work/nexus3 -Dkaraf.log=../sonatype-work/nexus3/log -Djava.io.tmpdir=../sonatype-work/nexus3/tmp -Dkaraf.startLocalConsole=false -Djdk.tls.ephemeralDHKeySize=2048 -Djava.endorsed.dirs=lib/endorsed -Di4j.vpt=true -classpath /opt/sonatype/nexus/.install4j/i4jruntime.jar:/opt/sonatype/nexus/lib/boot/nexus-main.jar:/opt/sonatype/nexus/lib/boot/activation-1.1.1.jar:/opt/sonatype/nexus/lib/boot/jakarta.xml.bind-api-2.3.3.jar:/opt/sonatype/nexus/lib/boot/jaxb-runtime-2.3.3.jar:/opt/sonatype/nexus/lib/boot/txw2-2.3.3.jar:/opt/sonatype/nexus/lib/boot/istack-commons-runtime-3.0.10.jar:/opt/sonatype/nexus/lib/boot/org.apache.karaf.main-4.3.9.jar:/opt/sonatype/nexus/lib/boot/osgi.core-7.0.0.jar:/opt/sonatype/nexus/lib/boot/org.apache.karaf.specs.activator-4.3.9.jar:/opt/sonatype/nexus/lib/boot/org.apache.karaf.diagnostic.boot-4.3.9.jar:/opt/sonatype/nexus/lib/boot/org.apache.karaf.jaas.boot-4.3.9.jar com.install4j.runtime.launcher.UnixLauncher run 9d17dc87 0 0 org.sonatype.nexus.karaf.NexusMain
agent03  22044  1.2  0.0      0     0 ?        Z    18:07   0:22 [java] <defunct>
agent05  23299 34.2  4.5 19662424 3020476 ?    Sl   18:23   4:43 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -XX:+IgnoreUnrecognizedVMOptions --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED -classpath /__w/2/s/.mvn/wrapper/maven-wrapper.jar -Dmaven.multiModuleProjectDirectory=/__w/2/s org.apache.maven.wrapper.MavenWrapperMain -Dmaven.repo.local=/__w/2/.m2/repository -Dmaven.wagon.http.pool=false -Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat=HH:mm:ss.SSS -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn --no-snapshot-updates -B -Dflink.hadoop.version=2.10.2 --settings /__w/2/s/tools/ci/google-mirror-settings.xml -Dfast -Pskip-webui-build -Dlog.dir=/__w/_temp/debug_files -Dlog4j.configurationFile=file:///__w/2/s/tools/ci/log4j.properties -Dflink.tests.with-openssl -Dflink.tests.check-segment-multiple-free -Darchunit.freeze.store.default.allowStoreUpdate=false -Dpekko.rpc.force-invocation-serialization -Dflink.hadoop.version=2.10.2 -pl flink-table,flink-table/flink-sql-parser,flink-table/flink-table-common,flink-table/flink-table-api-java,flink-table/flink-table-api-scala,flink-table/flink-table-api-bridge-base,flink-table/flink-table-api-java-bridge,flink-table/flink-table-api-scala-bridge,flink-table/flink-table-api-java-uber,flink-table/flink-sql-client,flink-table/flink-sql-gateway-api,flink-table/flink-sql-gateway,flink-table/flink-table-planner,flink-table/flink-table-planner-loader,flink-table/flink-table-planner-loader-bundle,flink-table/flink-table-runtime,flink-table/flink-table-code-splitter,flink-table/flink-table-test-utils, verify
agent03  24292  1.1  0.0      0     0 ?        Z    18:07   0:20 [java] <defunct>
agent03  24475  1.1  0.0      0     0 ?        Z    18:07   0:19 [java] <defunct>
agent04  25593  0.0  0.0  17004  5572 ?        S    18:36   0:00 /bin/sh -c cd /__w/2/s/flink-tests && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED -Xmx1536m -jar /__w/2/s/flink-tests/target/surefire/surefirebooter1389210316768519672.jar /__w/2/s/flink-tests/target/surefire 2024-02-21T09-57-00_677-jvmRun4 surefire7018523912167266799tmp surefire_1897583157620381530347tmp
agent04  25605  131  2.7 4783948 1796980 ?     Sl   18:36   1:14 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED -Xmx1536m -jar /__w/2/s/flink-tests/target/surefire/surefirebooter1389210316768519672.jar /__w/2/s/flink-tests/target/surefire 2024-02-21T09-57-00_677-jvmRun4 surefire7018523912167266799tmp surefire_1897583157620381530347tmp
root     26976  0.0  0.0 112792  1992 pts/0    R+   18:37   0:00 grep --color=auto java
agent03  27978  0.0  0.0  17004  5432 ?        S    18:31   0:00 /bin/sh -c cd '/__w/1/s/flink-tests' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/1/s/flink-tests/target/surefire/surefirebooter-20240221095043142_572.jar' '/__w/1/s/flink-tests/target/surefire' '2024-02-21T09-50-36_899-jvmRun1' 'surefire-20240221095043142_570tmp' 'surefire_189-20240221095043142_571tmp'
agent03  27981  125  4.0 5963448 2689132 ?     Sl   18:31   8:06 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED -Xmx1536m -jar /__w/1/s/flink-tests/target/surefire/surefirebooter-20240221095043142_572.jar /__w/1/s/flink-tests/target/surefire 2024-02-21T09-50-36_899-jvmRun1 surefire-20240221095043142_570tmp surefire_189-20240221095043142_571tmp
agent03  28372  0.0  0.0  17004  5612 ?        S    18:36   0:00 /bin/sh -c cd '/__w/1/s/flink-tests' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/1/s/flink-tests/target/surefire/surefirebooter-20240221095043142_596.jar' '/__w/1/s/flink-tests/target/surefire' '2024-02-21T09-50-36_899-jvmRun4' 'surefire-20240221095043142_594tmp' 'surefire_190-20240221095043142_595tmp'
agent03  28379  143  1.6 4642024 1059432 ?     Sl   18:36   1:14 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED -Xmx1536m -jar /__w/1/s/flink-tests/target/surefire/surefirebooter-20240221095043142_596.jar /__w/1/s/flink-tests/target/surefire 2024-02-21T09-50-36_899-jvmRun4 surefire-20240221095043142_594tmp surefire_190-20240221095043142_595tmp
agent04  28408  5.8  4.0 19652536 2679676 ?    Sl   17:56   2:22 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -XX:+IgnoreUnrecognizedVMOptions --add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED --add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED -classpath /__w/2/s/.mvn/wrapper/maven-wrapper.jar -Dmaven.home=/__w/2 -Dmaven.multiModuleProjectDirectory=/__w/2/s org.apache.maven.wrapper.MavenWrapperMain -Dmaven.repo.local=/__w/2/.m2/repository -Dmaven.wagon.http.pool=false -Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat=HH:mm:ss.SSS -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn --no-snapshot-updates -B -Dflink.hadoop.version=2.10.2 -Dscala-2.12 --settings /__w/2/s/tools/ci/google-mirror-settings.xml -Dfast -Pskip-webui-build -Dlog.dir=/__w/_temp/debug_files -Dlog4j.configurationFile=file:///__w/2/s/tools/ci/log4j.properties -Dflink.tests.with-openssl -Dflink.tests.check-segment-multiple-free -Darchunit.freeze.store.default.allowStoreUpdate=false -Dpekko.rpc.force-invocation-serialization -Dflink.hadoop.version=2.10.2 -Dscala-2.12 -pl flink-tests, verify
agent04  31966  0.0  0.0  17004  1476 ?        S    18:33   0:00 /bin/sh -c cd /__w/2/s/flink-tests && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED -Xmx1536m -jar /__w/2/s/flink-tests/target/surefire/surefirebooter7921915626739830970.jar /__w/2/s/flink-tests/target/surefire 2024-02-21T09-57-00_677-jvmRun2 surefire2539620150285362830tmp surefire_1868293788085014629717tmp
agent04  31971  157  4.5 5721044 3029500 ?     Sl   18:33   6:20 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED -Xmx1536m -jar /__w/2/s/flink-tests/target/surefire/surefirebooter7921915626739830970.jar /__w/2/s/flink-tests/target/surefire 2024-02-21T09-57-00_677-jvmRun2 surefire2539620150285362830tmp surefire_1868293788085014629717tmp {code}
Output of
{code:java}
ps -aux | grep java | awk '{print $1}' | grep agent | sort | uniq{code}
is:
{code:java}
agent03
agent04
agent05{code}
So, 3 agent are running in parallel.

For further investigation, I need to better understand Ci architecture, for example how the CI workers are configured and their parallelism degree.

Just throwing around an idea, maybe it is just a matter of reducing the parallelism accepted by the worker: CI jobs will stay pending, but won't fail at runtime for timeouts.

Will get back to you once I clarify the situation.;;;","21/Feb/24 10:56;mapohl;That still leaves the question open why we don't see this in other Alibaba VMs. 

About the runner configuration: As far as I remember, there are 5 agents set up per VM right now. This can be configured in the Azure Pipeline pool configuration where you can enable/disable each agent individually.;;;","21/Feb/24 14:17;lorenzo.affetti;[~mapohl] thanks for pointing that out.

Initially I did not understand this was happening only on CI001, my bad.

I found out that the Nexus service was consuming an enormous amount of CPU on CI001.

I restarted and updated the service right now, the issue should disappear.

Please don't hesitate to contact me if something similar happens again(y);;;","21/Feb/24 14:29;mapohl;Thanks Lorenzo for looking into it. I guess, we won't be able to verify anymore whether this resolved the issue because of optimization [~hxb] already provided for the Python stage. I'm gonna close this issue. Thanks everyone for participating.

master: [95163e89c84edbbad8477715739d3e5f2d80615e|https://github.com/apache/flink/commit/95163e89c84edbbad8477715739d3e5f2d80615e]
1.19: [a25fca9cc9342c59f8e2c5e5b5a17e5f875e9732|https://github.com/apache/flink/commit/a25fca9cc9342c59f8e2c5e5b5a17e5f875e9732]
1.18: [5c16321b8064859d68004462410c8fcfd3ca0b2f|https://github.com/apache/flink/commit/5c16321b8064859d68004462410c8fcfd3ca0b2f]
1.17: [dfc85f6544cf196c6e6e09c3f9e71c936b89d45c|https://github.com/apache/flink/commit/dfc85f6544cf196c6e6e09c3f9e71c936b89d45c];;;","21/Feb/24 16:18;mapohl;The following CI failures didn't include the fix, yet, and are just added for documentation purposes:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57700&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=27458;;;",,
Datadog name resolution fails and do not retry causing metrics to not get exported,FLINK-34201,13565678,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,pedromazala,pedromazala,22/Jan/24 15:33,22/Jan/24 15:35,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,Runtime / Metrics,,,,0,,,,,"When node restarts happens on k8s, some deployments fail to report metrics to datadog.

At first, I thought it could be related to some timeout and added a cap of 500 metrics. But then I got to this exception:


{code:java}
java.lang.IllegalStateException: Failed contacting Datadog to validate API key
	at org.apache.flink.metrics.datadog.DatadogHttpClient.validateApiKey(DatadogHttpClient.java:106) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at org.apache.flink.metrics.datadog.DatadogHttpClient.<init>(DatadogHttpClient.java:86) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at org.apache.flink.metrics.datadog.DatadogHttpReporter.<init>(DatadogHttpReporter.java:75) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at org.apache.flink.metrics.datadog.DatadogHttpReporterFactory.createMetricReporter(DatadogHttpReporterFactory.java:59) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.metrics.ReporterSetup.loadViaFactory(ReporterSetup.java:418) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.metrics.ReporterSetup.loadViaFactory(ReporterSetup.java:408) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.metrics.ReporterSetup.loadReporter(ReporterSetup.java:372) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.metrics.ReporterSetup.setupReporters(ReporterSetup.java:326) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.metrics.ReporterSetup.fromConfiguration(ReporterSetup.java:207) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManagerRunnerServices(TaskManagerRunner.java:224) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.start(TaskManagerRunner.java:293) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:486) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.lambda$runTaskManagerProcessSecurely$5(TaskManagerRunner.java:530) ~[flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) [flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:530) [flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:510) [flink-dist-1.17.2.jar:1.17.2]
	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.main(TaskManagerRunner.java:468) [flink-dist-1.17.2.jar:1.17.2]
Caused by: java.net.UnknownHostException: app.datadoghq.com: Temporary failure in name resolution
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method) ~[?:?]
	at java.net.InetAddress$PlatformNameService.lookupAllHostAddr(Unknown Source) ~[?:?]
	at java.net.InetAddress.getAddressesFromNameService(Unknown Source) ~[?:?]
	at java.net.InetAddress$NameServiceAddresses.get(Unknown Source) ~[?:?]
	at java.net.InetAddress.getAllByName0(Unknown Source) ~[?:?]
	at java.net.InetAddress.getAllByName(Unknown Source) ~[?:?]
	at java.net.InetAddress.getAllByName(Unknown Source) ~[?:?]
	at okhttp3.Dns.lambda$static$0(Dns.java:39) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.connection.RouteSelector.resetNextInetSocketAddress(RouteSelector.java:171) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.connection.RouteSelector.nextProxy(RouteSelector.java:135) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.connection.RouteSelector.next(RouteSelector.java:84) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.java:187) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.java:108) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.java:88) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.connection.Transmitter.newExchange(Transmitter.java:169) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:41) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:94) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:88) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:229) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at okhttp3.RealCall.execute(RealCall.java:81) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
	at org.apache.flink.metrics.datadog.DatadogHttpClient.validateApiKey(DatadogHttpClient.java:101) ~[flink-metrics-datadog-1.17.2.jar:1.17.2]
{code}


There is no retry mechanism [here|https://github.com/apache/flink/blob/f9f9299f6e25080c6f869b46ec0bdc5e3e19e00d/flink-metrics/flink-metrics-datadog/src/main/java/org/apache/flink/metrics/datadog/DatadogHttpClient.java#L98-L108]


{code:java}
    private void validateApiKey() {
        Request r = new Request.Builder().url(validateUrl).get().build();

        try (Response response = client.newCall(r).execute()) {
            if (!response.isSuccessful()) {
                throw new IllegalArgumentException(String.format(""API key: %s is invalid"", apiKey));
            }
        } catch (IOException e) {
            throw new IllegalStateException(""Failed contacting Datadog to validate API key"", e);
        }
    }
{code}
","
{code:java}
    metrics.reporters: dghttp
    metrics.reporter.dghttp.factory.class: org.apache.flink.metrics.datadog.DatadogHttpReporterFactory
    metrics.reporter.dghttp.apikey: {{ required ""A valid .Values.ddApiKey entry required!"" .Values.ddApiKey }}
    metrics.reporter.dghttp.dataCenter: US
    metrics.reporter.dghttp.maxMetricsPerRequest: ""500""
    metrics.reporter.dghttp.useLogicalIdentifier: ""true""
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-22 15:33:59.0,,,,,,,,,,"0|z1mx40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutoRescalingITCase#testCheckpointRescalingInKeyedState fails,FLINK-34200,13565677,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fanrui,mapohl,mapohl,22/Jan/24 15:21,08/Feb/24 16:30,04/Jun/24 20:40,08/Feb/24 16:30,1.19.0,,,,,,,1.19.0,1.20.0,,,Runtime / Checkpointing,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56601&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8200]
{code:java}
Jan 19 02:31:53 02:31:53.954 [ERROR] Tests run: 32, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1050 s <<< FAILURE! -- in org.apache.flink.test.checkpointing.AutoRescalingITCase
Jan 19 02:31:53 02:31:53.954 [ERROR] org.apache.flink.test.checkpointing.AutoRescalingITCase.testCheckpointRescalingInKeyedState[backend = rocksdb, buffersPerChannel = 2] -- Time elapsed: 59.10 s <<< FAILURE!
Jan 19 02:31:53 java.lang.AssertionError: expected:<[(0,8000), (0,32000), (0,48000), (0,72000), (1,78000), (1,30000), (1,54000), (0,2000), (0,10000), (0,50000), (0,66000), (0,74000), (0,82000), (1,80000), (1,0), (1,16000), (1,24000), (1,40000), (1,56000), (1,64000), (0,12000), (0,28000), (0,52000), (0,60000), (0,68000), (0,76000), (1,18000), (1,26000), (1,34000), (1,42000), (1,58000), (0,6000), (0,14000), (0,22000), (0,38000), (0,46000), (0,62000), (0,70000), (1,4000), (1,20000), (1,36000), (1,44000)]> but was:<[(0,8000), (0,32000), (0,48000), (0,72000), (1,78000), (1,30000), (1,54000), (0,2000), (0,10000), (0,50000), (0,66000), (0,74000), (0,82000), (1,80000), (1,0), (1,16000), (1,24000), (1,40000), (1,56000), (1,64000), (0,12000), (0,28000), (0,52000), (0,60000), (0,68000), (0,76000), (0,1000), (0,25000), (0,33000), (0,41000), (1,18000), (1,26000), (1,34000), (1,42000), (1,58000), (0,6000), (0,14000), (0,22000), (0,38000), (0,46000), (0,62000), (0,70000), (1,4000), (1,20000), (1,36000), (1,44000)]>
Jan 19 02:31:53 	at org.junit.Assert.fail(Assert.java:89)
Jan 19 02:31:53 	at org.junit.Assert.failNotEquals(Assert.java:835)
Jan 19 02:31:53 	at org.junit.Assert.assertEquals(Assert.java:120)
Jan 19 02:31:53 	at org.junit.Assert.assertEquals(Assert.java:146)
Jan 19 02:31:53 	at org.apache.flink.test.checkpointing.AutoRescalingITCase.testCheckpointRescalingKeyedState(AutoRescalingITCase.java:296)
Jan 19 02:31:53 	at org.apache.flink.test.checkpointing.AutoRescalingITCase.testCheckpointRescalingInKeyedState(AutoRescalingITCase.java:196)
Jan 19 02:31:53 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 19 02:31:53 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) {code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-34218,,,,,,FLINK-33246,,,,FLINK-33919,,,,,,,,"26/Jan/24 14:16;mapohl;FLINK-34200.failure.log.gz;https://issues.apache.org/jira/secure/attachment/13066285/FLINK-34200.failure.log.gz","01/Feb/24 12:49;fanrui;debug-34200.log;https://issues.apache.org/jira/secure/attachment/13066395/debug-34200.log",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 08 16:30:20 UTC 2024,,,,,,,,,,"0|z1mx3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 15:22;mapohl;[~srichter] Can you have a look at this one and FLINK-33919?;;;","22/Jan/24 15:25;mapohl;Update: I don't know whether the build above was still affected by the CI worker issues we've seen last week (FLINK-34098);;;","22/Jan/24 15:35;mapohl;{quote}Update: I don't know whether the build above was still affected by the CI worker issues we've seen last week (FLINK-34098)
{quote}
Ok, after looking into the other job failures, I'm not that confident anymore whether the build was actually executed in a stable environment. Even though, some of the build failures happened on Azure machines rather than Alibaba VMs;;;","25/Jan/24 10:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56859&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8298;;;","25/Jan/24 10:45;mapohl;Copied over from FLINK-34218: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56740&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&s=ae4f8708-9994-57d3-c2d7-b892156e7812;;;","26/Jan/24 03:32;fanrui;Thanks [~mapohl] for adding more failed cases.

As [~srichter] feedback in FLINK-34218, the reason of test fails isn't test code. So I guess current master branch might has some potential bugs related to correctness. I'm not sure should we mark this bug as the Blocker or Critical?;;;","26/Jan/24 10:00;srichter;[~fanrui] I think you misunderstood my comment. I believe the problem is with the test code/test utils, not in Flink.;;;","26/Jan/24 10:55;fanrui;Sorry for the bother. If so, let us follow it in this JIRA.;;;","26/Jan/24 13:20;mapohl;Not sure whether that helps, but from the three builds that we have documented here, it looks like we have unexpected lines being processed:
{code}
➜  FLINK-34200 diff <(sed 's/), (/\n/g' 56601.expected) <(sed s'/), (/\n/g' 56601.actual) 
26a27,30
> 0,1000
> 0,25000
> 0,33000
> 0,41000
➜  FLINK-34200 diff <(sed 's/), (/\n/g' 56859.expected) <(sed s'/), (/\n/g' 56859.actual)
13a14,15
> 0,23000
> 0,31000
38a41,42
> 0,19000
> 0,35000
➜  FLINK-34200 diff <(sed 's/), (/\n/g' 56740.expected) <(sed s'/), (/\n/g' 56740.actual)
26a27,30
> 0,1000
> 0,25000
> 0,33000
> 0,41000
{code};;;","26/Jan/24 13:23;mapohl;btw. I was able to reproduce the test instability locally after the third run (keep in mind that it appears to only fail for backend=rocksdb, buffersPerChannel=2):

{code}
java.lang.AssertionError: 
Expected :[(0,8000), (0,32000), (0,48000), (0,72000), (1,78000), (1,30000), (1,54000), (0,2000), (0,10000), (0,50000), (0,66000), (0,74000), (0,82000), (1,80000), (1,0), (1,16000), (1,24000), (1,40000), (1,56000), (1,64000), (0,12000), (0,28000), (0,52000), (0 ...

Actual   :[(0,8000), (0,32000), (0,48000), (0,72000), (1,78000), (1,30000), (1,54000), (0,2000), (0,10000), (0,50000), (0,66000), (0,74000), (0,82000), (1,80000), (1,0), (1,16000), (1,24000), (1,40000), (1,56000), (1,64000), (0,12000), (0,28000), (0,52000), (0 ...
{code}
The actual diff is again 4 events more than expected: {{(0,1000), (0,25000), (0,33000), (0,41000),}}
;;;","26/Jan/24 13:30;mapohl;Another local repeated test run for (rocksdb,buffersPerChannel=2) failed in the 2nd test run:
The diff was {{(0,23000), (0,31000), [...] (0,19000), (0,35000)}} this time.

I hope that helps. Let me know if you need help with the test setup.;;;","26/Jan/24 14:17;mapohl;I attached the [debug logs|https://issues.apache.org/jira/secure/attachment/13066285/FLINK-34200.failure.log.gz] for a failed local test run.;;;","28/Jan/24 06:38;zhuzh;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57024&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","30/Jan/24 09:24;mapohl;I did a local test run with the following diff to check whether the failure is still reproducible:
{code:java}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PrioritizedOperatorSubtaskState.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PrioritizedOperatorSubtaskState.java
index e41bcfe7338..676e738ff45 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PrioritizedOperatorSubtaskState.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/PrioritizedOperatorSubtaskState.java
@@ -290,14 +290,14 @@ public class PrioritizedOperatorSubtaskState {
             }
 
             return new PrioritizedOperatorSubtaskState(
-                    computePrioritizedAlternatives(
+                    resolvePrioritizedAlternatives(
                             jobManagerState.getManagedKeyedState(),
                             managedKeyedAlternatives,
-                            KeyedStateHandle::getKeyGroupRange),
-                    computePrioritizedAlternatives(
+                            eqStateApprover(KeyedStateHandle::getKeyGroupRange)),
+                    resolvePrioritizedAlternatives(
                             jobManagerState.getRawKeyedState(),
                             rawKeyedAlternatives,
-                            KeyedStateHandle::getKeyGroupRange),
+                            eqStateApprover(KeyedStateHandle::getKeyGroupRange)),
                     resolvePrioritizedAlternatives(
                             jobManagerState.getManagedOperatorState(),
                             managedOperatorAlternatives, {code}
Even with the above change, the error appeared in the 2nd repetition. According to [~srichter] , that reveals that it must be either a test setup issue or a hidden issue that was just revealed by introducing the {{{}AutoRescalingITCase{}}}.

[~srichter] do we have someone who can look into it in more detail? I don't have the capacity right now.;;;","31/Jan/24 15:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57136&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=8361;;;","01/Feb/24 07:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57172&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8596;;;","01/Feb/24 12:58;fanrui;After re-run hundreds of times and adding a series of LOG to analysis this test. I find the root cause.

In biref, it's a test issue, the root cause is that the counter and sum of SubtaskIndexFlatMapper in the Checkpoint before rescale are incomplete.

Here is a test commit, I added some logs to analyze it.
 * [https://github.com/1996fanrui/flink/commit/420fdf3cfec4e2ec6d1f600fa0c87a3fc131b5fe]

 

I have added the detailed reason in this comment: [https://github.com/1996fanrui/flink/commit/420fdf3cfec4e2ec6d1f600fa0c87a3fc131b5fe#r138161972]

 ;;;","01/Feb/24 13:12;fanrui;I have attached the detailed log in this JIRA.;;;","06/Feb/24 07:14;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57323&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8318;;;","08/Feb/24 02:16;fanrui;Merged to:

master(1.20) via: 1b95b191922829fd8e7a76e5c9d8de68bb57ae7d and 9d7de680f4cde3e97368a18d58c2b70646a6f242;;;","08/Feb/24 16:30;mapohl;master: [1b95b191922829fd8e7a76e5c9d8de68bb57ae7d|https://github.com/apache/flink/commit/1b95b191922829fd8e7a76e5c9d8de68bb57ae7d]
1.19: [8b289000f65aef39f48d6cc6ddb817208a62fdee|https://github.com/apache/flink/commit/8b289000f65aef39f48d6cc6ddb817208a62fdee];;;",,,,,,,,,,,,
Add tracing for durations of rescaling/restoring RocksDB incremental checkpoints from downloaded and local state,FLINK-34199,13565672,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,srichter,srichter,srichter,22/Jan/24 14:36,22/Mar/24 08:27,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,,Adds tracing for durations of rescaling/restoring from local state. This enables more fine grained monitoring of restore operations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34913,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-22 14:36:42.0,,,,,,,,,,"0|z1mx2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove e2e test operator log error check,FLINK-34198,13565662,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,22/Jan/24 13:56,22/Jan/24 16:36,04/Jun/24 20:40,22/Jan/24 16:36,1.8.0,,,,,,,1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"There are too many false positives because of negative test cases and its not realistic to be so strict that error typed messages can't appear in the operator log.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 16:36:15 UTC 2024,,,,,,,,,,"0|z1mx0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 16:36;gaborgsomogyi;[{{31d01f2}}|https://github.com/apache/flink-kubernetes-operator/commit/31d01f246d8a344b560aab1653b7aba561baea26] on main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How can i recover job by savepoint with multi-job run by executeAsync in application mode,FLINK-34197,13565650,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,mazhen,mazhen,22/Jan/24 12:11,22/Jan/24 12:31,04/Jun/24 20:40,22/Jan/24 12:31,1.18.1,,,,,,,,,,,API / Core,,,,0,applicationmode,java,recovery,savepoints,"Hello guys, i'm working on flink java with 1.18 version, and want to use Application-mode to run 2 jobs in one pod(k8s docker deployment).

In java code, i use a _for_ statement to create 2 or more jobs with env.executeAsync, creating a new env in loop clause. Thus we can run multi parallel job in one docker pod, to reduce resource cost.

In application-mode, i think i cannot take over recovery with checkpoint, because we cannot enable HA in this mode, thus we cannot store the previous job id in Zookeeper to recover from checkpoint. Ref: https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/overview/#application-mode

So i want to recover by savepoint, when the docker pod is down or need to restart. My problems are:
 * how can i trigger savepoint for each job (now i run 2 jobs in one pod) every hour?
 * how can i recover from savepoint for each job when the docker pod restart?

with java code or REST api.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 12:31:03 UTC 2024,,,,,,,,,,"0|z1mwxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 12:31;martijnvisser;Please reach out to the User mailing list, Stackoverflow or Slack for user questions. Jira is not meant for these type of tickets;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-389 Annotate SingleThreadFetcherManager as PublicEvolving.,FLINK-34196,13565648,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,loserwang1024,loserwang1024,loserwang1024,22/Jan/24 12:01,26/Jan/24 03:27,04/Jun/24 20:40,26/Jan/24 03:22,1.18.1,,,,,,,1.19.0,,,,Connectors / Common,,,,0,pull-request-available,,,,"This improvement implements [FLIP-389](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=278465498)

This flip has 2 goals:
 * To expose the SplitFetcherManager / SingleThreadFetcheManager as Public, allowing connector developers to easily create their own threading models in the SourceReaderBase.
 * To hide the element queue from the connector developers and make SplitFetcherManager the only owner class of the queue",,,,,,,,,,,,,,,,,,,,,,,,FLINK-33465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 03:23:11 UTC 2024,,,,,,,,,,"0|z1mwxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 12:03;loserwang1024;[~renqs] , [~Leonard] , CC, WDYT?;;;","26/Jan/24 03:23;renqs;Merged to master: 6afee1de6585074e0df6205f1f52bb239dcf4a77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonEnvUtils creates python environment instead of python3,FLINK-34195,13565621,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,22/Jan/24 09:06,24/Jan/24 07:33,04/Jun/24 20:40,,,,,,,,,,,,,API / Python,Build System / CI,,,0,starter,,,,"I was looking into the Python installation of the Flink test suite because I working on updating the CI Docker image from 16.04 (Xenial) to 22.04 (FLINK-34194). I noticed that there is test code still relying on the {{python}} command instead of {{{}python3{}}}. For Ubuntu 16.04 that meant relying on Python 2. Therefore, we have tests still relying on Python 2 as far as I understand.

I couldn't find any documentation or mailing list discussion on major Python version support. But AFAIU, we're relying on Python3 (based on the e2e tests) which makes these tests out-dated.

Additionally, [python.client.executable|https://github.com/apache/flink/blob/50cb4ee8c545cd38d0efee014939df91c2c9c65f/flink-python/src/main/java/org/apache/flink/python/PythonOptions.java#L170] relies on {{{}python{}}}.

Should we make it more explicit in our test code that we're actually expecting python3? Additionally, should that be mentioned somewhere in the docs? Or if it's already mentioned, could you point me to it? (As someone looking into PyFlink for the ""first"" time) I would have expected something like that being mentioned on the [PyFlink overview|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/overview/]. Or is it the default to assume nowadays that {{python}} refers to {{python3?}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34194,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 09:10:14 UTC 2024,,,,,,,,,,"0|z1mwrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 09:07;mapohl;[~hxbks2ks] I'm curious what's your take on that?;;;","23/Jan/24 09:07;hxb;[~mapohl] Thanks for the great work. Since the second version of PyFlink's inception, 1.10, it has only supported Python3. I believe that using `python3` rather than `python` is indeed a more standard practice. For information on which Python version PyFlink supports, one method is to check the Flink documentationhttps://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/installation/, and another method is to view the Python package meta on PyPI https://pypi.org/project/apache-flink/.;;;","23/Jan/24 09:10;mapohl;I see - thanks for the clarification. :) Having the supported version documented in the installation section also seems to be reasonable (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Flink CI Docker container to Ubuntu 22.04,FLINK-34194,13565616,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,22/Jan/24 08:22,19/Mar/24 07:41,04/Jun/24 20:40,,1.17.2,1.18.1,1.19.0,,,,,1.20.0,,,,Build System / CI,,,,0,github-actions,pull-request-available,,,"The current CI Docker image is based on Ubuntu 16.04. We already use 20.04 for the e2e tests. We can update the Docker image to a newer version to be on par with what we need in GitHub Actions (FLINK-33923).

This issue can cover the following topics:
 * Update to 22.04
 ** OpenSSL 1.0.0 dependency should be added for netty-tcnative support
 ** Use Python3 instead of Python 2.7 (python symlink needs to be added due to FLINK-34195) 
 * Removal of Maven (FLINK-33501 makes us rely on the Maven wrapper)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34720,FLINK-33501,FLINK-34195,FLINK-34695,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 07:41:48 UTC 2024,,,,,,,,,,"0|z1mwq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 13:28;mapohl;master: [2b94a57f59ee5a4fa09831236764712d1f5affc6|https://github.com/apache/flink/commit/2b94a57f59ee5a4fa09831236764712d1f5affc6];;;","19/Mar/24 07:41;mapohl;There are issues with the artifact deployment and docs build where the workflows are still relying on {{mvn}}.

The previous commit was reverted in 676a1995c2b20e2ffc20c0b9b99d14656878d3bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove usage of Flink-Shaded Jackson and Snakeyaml in flink-connector-kafka,FLINK-34193,13565614,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jiabao.sun,martijnvisser,martijnvisser,22/Jan/24 08:17,13/Feb/24 08:46,04/Jun/24 20:40,13/Feb/24 08:45,,,,,,,,kafka-3.2.0,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"The Flink Kafka connector doesn't have a direct dependency in the POM on flink-shaded, but it still uses the shaded versions of Jackson and SnakeYAML in {{YamlFileMetaDataService.java}} and {{KafkaRecordDeserializationSchemaTest}} 

Those cause problems when trying to compile the Flink Kafka connector for Flink 1.19, since these dependencies have been updated in there. Since connectors shouldn't rely on Flink-Shaded, we should refactor these implementations ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 13 08:45:53 UTC 2024,,,,,,,,,,"0|z1mwps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 13:41;jinsuichen;I think it's useful to specify Jackson and SnakeYAML versions directly in the pom.xml file to address conflicts. Specifically, look for and replace references to org.apache.flink.shaded.* with standard Jackson and SnakeYAML libraries. If you think this solution suitable, feel free to assign the issue to me.;;;","24/Jan/24 14:46;martijnvisser;That's one way to deal with it yes, but one thing that might happen is that you will have clashes with the Jackson or SnakeYAML version that a user might provide. In case there's an alternative for Jackson/SnakeYAML in the code base (like a plain Java implementation), that could be desirable over your proposed solution. ;;;","26/Jan/24 14:49;jinsuichen;Hi, [~martijnvisser]. Thank you for you response. I have one more question. JSONKeyValueDeserializationSchema which depends on a specific version of Jackson is annotated by '@PublicEvolving'. Is it required to remove this class for this refactoring?;;;","29/Jan/24 09:54;martijnvisser;[~jinsuichen] That depends on the proposed changes. We shouldn't need to break the interface?;;;","13/Feb/24 08:45;martijnvisser;Fixed in apache/flink-connector-kafka

main: 2606a8256d0e25da19ebce4f92cd426b5bf63f7c
v3.1: 906b6c05db1a06c5684b2826ae79a2e346736aae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update flink-connector-kafka to be compatible with updated SinkV2 interfaces,FLINK-34192,13565613,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jiabao.sun,martijnvisser,martijnvisser,22/Jan/24 08:14,13/Feb/24 08:45,04/Jun/24 20:40,13/Feb/24 08:45,,,,,,,,kafka-3.2.0,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"{code:java}
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableTestUtils.java:[101,76] incompatible types: java.util.List<org.apache.flink.types.Row> cannot be converted to java.util.List<java.lang.String>
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java:[136,46] cannot find symbol
Error:    symbol:   method mock(org.apache.flink.metrics.MetricGroup,org.apache.flink.metrics.groups.OperatorIOMetricGroup)
Error:    location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java:[171,46] cannot find symbol
Error:    symbol:   method mock(org.apache.flink.metrics.MetricGroup)
Error:    location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java:[204,54] cannot find symbol
Error:    symbol:   method mock(org.apache.flink.metrics.MetricGroup)
Error:    location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java:[233,54] cannot find symbol
Error:    symbol:   method mock(org.apache.flink.metrics.MetricGroup)
Error:    location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java:[263,54] cannot find symbol
Error:    symbol:   method mock(org.apache.flink.metrics.MetricGroup)
Error:    location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java:[294,54] cannot find symbol
Error:    symbol:   method mock(org.apache.flink.metrics.MetricGroup)
Error:    location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java:[337,54] cannot find symbol
Error:    symbol:   method mock(org.apache.flink.metrics.MetricGroup)
Error:    location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaWriterITCase.java:[525,46] cannot find symbol
Error:    symbol:   method mock(org.apache.flink.metrics.MetricGroup)
Error:    location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  -> [Help 1]
{code}

https://github.com/apache/flink-connector-kafka/actions/runs/7597711401/job/20692858078#step:14:221",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34337,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 13 08:45:46 UTC 2024,,,,,,,,,,"0|z1mwpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/24 08:45;martijnvisser;Fixed in apache/flink-connector-kafka

main: b8328ab55e2bcf026ef82e35cebbb1d867cfb18f
v3.1 186e72a2a71cda9ca1bd9ae45420b64611c10900;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Remove RestoreMode#LEGACY ,FLINK-34191,13565611,13565609,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zakelly,zakelly,22/Jan/24 07:58,22/Jan/24 07:58,04/Jun/24 20:40,,,,,,,,,2.0.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-22 07:58:22.0,,,,,,,,,,"0|z1mwp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate RestoreMode#LEGACY,FLINK-34190,13565610,13565609,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,22/Jan/24 07:55,31/Jan/24 07:23,04/Jun/24 20:40,24/Jan/24 13:56,,,,,,,,1.19.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 10:24:38 UTC 2024,,,,,,,,,,"0|z1mwow:",9223372036854775807,RestoreMode#LEGACY is deprecated. Please use RestoreMode#CLAIM or RestoreMode#NO_CLAIM mode instead to get a clear state file ownership when restoring.,,,,,,,,,,,,,,,,,,,"24/Jan/24 13:56;masteryhx;merged f7a2819f...bed79d33 into master;;;","30/Jan/24 08:43;martijnvisser;[~Zakelly] [~masteryhx] Can you please include in the release notes information on what's deprecated, and what users should be using?;;;","30/Jan/24 10:24;zakelly;[~martijnvisser] Thanks for the reminder, will do~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-416: Deprecate and remove the RestoreMode#LEGACY,FLINK-34189,13565609,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,22/Jan/24 07:54,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,,,1.20.0,2.0.0,,,Runtime / Checkpointing,,,,0,,,,,"[https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=287607202]

 

The [FLIP-193|https://cwiki.apache.org/confluence/x/bIyqCw] introduced two modes of state file ownership during checkpoint restoration: RestoreMode#CLAIM and RestoreMode#NO_CLAIM. The LEGACY mode, which was how Flink worked until 1.15, has been superseded by NO_CLAIM as the default mode. The main drawback of LEGACY mode is that the new job relies on artifacts from the old job without cleaning them up, leaving users uncertain about when it is safe to delete the old checkpoint directories. This leads to the accumulation of unnecessary checkpoint files that are never cleaned up. Considering cluster availability and job maintenance, it is not recommended to use LEGACY mode. Users could choose the other two modes to get a clear semantic for the state file ownership.

This FLIP proposes to deprecate the LEGACY mode and remove it completely in the upcoming Flink 2.0. This will make the semantic clear as well as eliminate many bugs caused by mode transitions involving LEGACY mode (e.g. !https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype! FLINK-27114 - On JM restart, the information about the initial checkpoints can be lost OPEN ) and enhance code maintainability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-22 07:54:01.0,,,,,,,,,,"0|z1mwoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup release infrastructure for Flink CDC project,FLINK-34188,13565608,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kunni,leonard,leonard,22/Jan/24 07:45,24/Apr/24 08:08,04/Jun/24 20:40,24/Apr/24 08:08,,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 08:08:30 UTC 2024,,,,,,,,,,"0|z1mwog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/24 08:08;renqs;flink-cdc master: 65d670a58f6e11566699840a8ce76dd1b56561f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup CI for Flink CDC project,FLINK-34187,13565607,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,gongzhongqiang,leonard,leonard,22/Jan/24 07:44,05/Mar/24 05:59,04/Jun/24 20:40,05/Mar/24 05:59,,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 05:59:26 UTC 2024,,,,,,,,,,"0|z1mwo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 03:20;gongzhongqiang;[~leonard] I'm willing to take this, Please assign to me. Thank you~;;;","23/Jan/24 03:46;gongzhongqiang;[~leonard] Here is the pr https://github.com/ververica/flink-cdc-connectors/pull/3022;;;","31/Jan/24 09:54;leonard;Thanks [~gongzhongqiang]for taking this ticket, assigned to you!;;;","05/Mar/24 05:59;leonard;Resolved in flink cdc (master): f7fa3175809b4b912b19ad91c89599bf1c2b9ea3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate issue management from Github to JIRA,FLINK-34186,13565606,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,renqs,leonard,leonard,22/Jan/24 07:44,25/Mar/24 08:05,04/Jun/24 20:40,25/Mar/24 08:05,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 08:05:31 UTC 2024,,,,,,,,,,"0|z1mwo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/24 08:05;renqs;136 opening issues on GitHub has been migrated to Jira:

https://issues.apache.org/jira/browse/FLINK-34890?jql=project%20%3D%20FLINK%20AND%20component%20%3D%20%22Flink%20CDC%22%20AND%20labels%20%3D%20github-import;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unwanted bundle dependencies,FLINK-34185,13565605,13565600,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,leonard,leonard,22/Jan/24 07:44,29/Apr/24 08:28,04/Jun/24 20:40,29/Apr/24 08:28,,,,,,,,cdc-3.1.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35055,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-22 07:44:26.0,,,,,,,,,,"0|z1mwns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update copyright and license file,FLINK-34184,13565604,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,leonard,leonard,22/Jan/24 07:44,06/Mar/24 03:13,04/Jun/24 20:40,06/Mar/24 03:13,,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 03:13:36 UTC 2024,,,,,,,,,,"0|z1mwnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 03:13;leonard;Fixed in flink-cdc(master) via: a6c1b06e11004918cb6e5714ade3699e052e1aad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add NOTICE files for Flink CDC project,FLINK-34183,13565603,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,ruanhang1993,leonard,leonard,22/Jan/24 07:43,07/Mar/24 03:39,04/Jun/24 20:40,07/Mar/24 03:39,,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 07 03:39:22 UTC 2024,,,,,,,,,,"0|z1mwnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 04:00;gongzhongqiang;[~leonard]  I'm willing to take this.;;;","07/Mar/24 03:39;jiabaosun;Implement via flink-cdc master: 86272bf1029022adbf6d34132f4b34df14f2ad89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate doc website from ververica to flink	,FLINK-34182,13565602,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,gongzhongqiang,leonard,leonard,22/Jan/24 07:43,25/Mar/24 08:03,04/Jun/24 20:40,25/Mar/24 08:03,,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,0,,,,,,,,,,,,,,,,,INFRA-25578,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 08:03:41 UTC 2024,,,,,,,,,,"0|z1mwn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 03:20;gongzhongqiang;[~leonard] I'm willing to take this, Please assign to me. Thank you~;;;","23/Jan/24 03:44;gongzhongqiang;[~leonard] Here is the pr https://github.com/ververica/flink-cdc-connectors/pull/3028;;;","31/Jan/24 09:53;leonard;Thanks [~gongzhongqiang]for taking this ticket, assigned to you!;;;","14/Mar/24 07:40;liguang;https://github.com/apache/flink-cdc/pull/3144;;;","25/Mar/24 08:03;renqs;flink-cdc master: 1dc201f9b397028f50e493bb21bff34ddfb21d99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate repo from ververica to apache,FLINK-34181,13565601,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,renqs,leonard,leonard,22/Jan/24 07:43,05/Mar/24 09:31,04/Jun/24 20:40,05/Mar/24 09:31,,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INFRA-25534,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 09:31:22 UTC 2024,,,,,,,,,,"0|z1mwmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/24 08:55;renqs;Opened ticket INFRA-25534 and waiting for response from Apache Infra team;;;","05/Mar/24 09:31;leonard;Closed as target repo  https://github.com/apache/flink-cdc looks well so far;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accept Flink CDC project as part of Apache Flink,FLINK-34180,13565600,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,leonard,leonard,22/Jan/24 07:40,29/Apr/24 08:28,04/Jun/24 20:40,29/Apr/24 08:28,,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,0,pull-request-available,,,,"As discussed in  Flink dev  mailing list[1][2], we have accepted the Flink CDC project contribution, we should finish the repo and doc migration as soon as possible.

[1] https://lists.apache.org/thread/sq5w21tcomrmb025tl820cxty9l0z26w
[2] https://lists.apache.org/thread/cw29fhsp99243yfo95xrkw82s5s418ob",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-22 07:40:32.0,,,,,,,,,,"0|z1mwmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More than one taskmanager is not coming up in flink (session mode) in aws eks cluster,FLINK-34179,13565596,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Information Provided,,nikhil_d,nikhil_d,22/Jan/24 06:39,22/Jan/24 11:16,04/Jun/24 20:40,22/Jan/24 11:16,1.18.0,,,,,,,,,,,Deployment / Kubernetes,,,,0,flink-k8s,,,,"Deployed flink in aws eks cluster using bitnami helm chart (bitnami/flink 1.18). After deployment, noticing that out of 5 taskmanager pods only one taskmanager pod is able to connect to resourcemanager which is present in jobmanager.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/24 06:38;nikhil_d;flink-uxxx-taskmanager-8b69df9d7-xxnbn.log;https://issues.apache.org/jira/secure/attachment/13066179/flink-uxxx-taskmanager-8b69df9d7-xxnbn.log","22/Jan/24 06:38;nikhil_d;flink-values.yaml;https://issues.apache.org/jira/secure/attachment/13066178/flink-values.yaml",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 07:49:26 UTC 2024,,,,,,,,,,"0|z1mwls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 07:49;gyfora;I am not aware of the bitnami helm chart, and I definitely don't think that it is supported by the community. Did you try the Flink Kubernetes operator?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The ScalingTracking of autoscaler is wrong,FLINK-34178,13565593,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,22/Jan/24 06:15,02/Feb/24 05:55,04/Jun/24 20:40,02/Feb/24 05:55,kubernetes-operator-1.8.0,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,,"The ScalingTracking of autoscaler is wrong, it's always greater than AutoScalerOptions#STABILIZATION_INTERVAL.
h2. Reason:

When flink job isStabilizing, ScalingMetricCollector#updateMetrics will return a empty metric history. In the JobAutoScalerImpl#runScalingLogic method, if `collectedMetrics.getMetricHistory().isEmpty()` , we don't update the ScalingTracking.

 

The default value of AutoScalerOptions#STABILIZATION_INTERVAL is 5 min, so the restartTime is always greater than 5 min.

However, it's quick when we use rescale api.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 05:55:26 UTC 2024,,,,,,,,,,"0|z1mwl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 05:55;fanrui;Merged to main(1.8.0) via:
 * 2bd6b1b38171ce821509d46d687291b876f72a2e
 * 746a996732e56e573f7882764c82e2faa2cba71d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not able to create FlinkSessionJob in different namespace than flink deployment,FLINK-34177,13565589,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,pburnwal,pburnwal,22/Jan/24 04:10,22/Jan/24 07:51,04/Jun/24 20:40,22/Jan/24 07:51,kubernetes-operator-1.7.0,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Here is my use case:

1) I created a namespace ""flink"" in aws cluster with k8 version 1.25 

2) Deployed flink kubernetes operator in ""flink"" namespace using helm chart

3) deployed FlinkDeployment similar to [flink-kubernetes-operator/examples/basic-session-deployment-only.yaml at main · apache/flink-kubernetes-operator · GitHub|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/basic-session-deployment-only.yaml] in flink namespace

4) Then deployed FlinkSession job similar to [flink-kubernetes-operator/examples/basic-session-job-only.yaml at main · apache/flink-kubernetes-operator · GitHub|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/basic-session-job-only.yaml] but in different namespace ""tss""

5) Already added both the namespaces to watchednamespaces 
watchNamespaces: [""tss"", ""flink""]
 
Expected:
 FlinkSessionJob will start in tss namespace
 
Actual:
 Job is not starting and throwing the error ""*Flink version null is not supported by this operator version*""
 
 
 
 
My suspect is it seems FlinkDeployment and FlinkSessionJob should be in the namespace. However i am not sure. So i am raising this bug.
 
{color:#FF0000}*Can someone confirm if Flink kubernetes operator supports Flink cluster(FlinkDeployment) to be in one namespace and then FlinkSessionJob in another namespace.??*{color}
Supporting multiple namespaces make my life a lot easier.
 
Note: Deploying FlinkSessionJob in the same namespace ""Flink"" works fine.
 
 ","*AWS EKS K8 version 1.25*

{*}Kubernetes Flink Operator 1.7.0{*}{*}{*}

*Flink 1.17/1.18*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 07:51:41 UTC 2024,,,,,,,,,,"0|z1mwk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 07:50;gyfora;Flink session jobs need to be created in the same namespace as the corresponding FlinkDeployment. This is an intentional limitation at the moment.;;;","22/Jan/24 07:51;gyfora;Closing this for now as the behaviour is intended. If you would like to change the behaviour, please consider creating a FLIP / mailing list discussion about it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary failure-rate and fixed-delay restart-strategies ,FLINK-34176,13565588,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,22/Jan/24 04:01,22/Jan/24 04:45,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"h2. Could we deprecate the  failure-rate and fixed-delay restart-strategies directly?

After FLINK-33735 (FLIP-364), the exponential-delay restart strategy is already very feature-rich. It can replace fix-delay and failure rate restart strategies directly.
h2. How to replace fix-delay restart strategy?
 * Set backoffMultiplier = 1 and jitterFactor = 0
 * resetBackoffThresholdMS = Interget.Max
 * initialBackoffMS and maxBackoffMS are the backoffTimeMS of FixedDelayRestartBackoffTimeStrategy
 * attemptsBeforeResetBackoff is the maxNumberRestartAttempts of FixedDelayRestartBackoffTimeStrategy

h2. How to replace failure-rate restart strategy?
 * Set backoffMultiplier = 1 and jitterFactor = 0
 * resetBackoffThresholdMS is the failuresIntervalMS of FailureRateRestartBackoffTimeStrategy
 * initialBackoffMS and maxBackoffMS are the backoffTimeMS of FailureRateRestartBackoffTimeStrategy
 * attemptsBeforeResetBackoff is the maxFailuresPerInterval of FailureRateRestartBackoffTimeStrategy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-22 04:01:36.0,,,,,,,,,,"0|z1mwk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When meeting WindowedSliceAssigner, slice window agg registers an wrong timestamp timer ",FLINK-34175,13565586,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,22/Jan/24 03:16,04/Feb/24 19:57,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"The following test added to SlicingWindowAggOperatorTest can re-produce this problem.
{code:java}
private static final RowType INPUT_ROW_TYPE_FROM_WINDOW_TVF =
        new RowType(
                Arrays.asList(
                        new RowType.RowField(""f0"", new VarCharType(Integer.MAX_VALUE)),
                        new RowType.RowField(""f1"", new IntType()),
                        new RowType.RowField(""f2"", new TimestampType()),
                        new RowType.RowField(""f3"", new TimestampType()),
                        new RowType.RowField(
                                ""f4"", new TimestampType(false, TimestampKind.ROWTIME, 3))));

protected static final RowDataSerializer INPUT_ROW_SER_FROM_WINDOW_TVF =
        new RowDataSerializer(INPUT_ROW_TYPE_FROM_WINDOW_TVF); 

@Test
public void test() throws Exception {
    final SliceAssigner innerAssigner =
            SliceAssigners.tumbling(2, shiftTimeZone, Duration.ofSeconds(3));
    final SliceAssigner assigner = SliceAssigners.windowed(3, innerAssigner);
    final SlicingSumAndCountAggsFunction aggsFunction =
            new SlicingSumAndCountAggsFunction(assigner);
    SlicingWindowOperator<RowData, ?> operator =
            (SlicingWindowOperator<RowData, ?>)
                    WindowAggOperatorBuilder.builder()
                            .inputSerializer(INPUT_ROW_SER_FROM_WINDOW_TVF)
                            .shiftTimeZone(shiftTimeZone)
                            .keySerializer(KEY_SER)
                            .assigner(assigner)
                            .aggregate(wrapGenerated(aggsFunction), ACC_SER)
                            .build();


    OneInputStreamOperatorTestHarness<RowData, RowData> testHarness =
            createTestHarness(operator);


    testHarness.setup(OUT_SERIALIZER);
    testHarness.open();


    // process elements
    ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();


    // add elements out-of-order
    testHarness.processElement(
            insertRecord(
                    ""key2"",
                    1,
                    fromEpochMillis(999L),
                    fromEpochMillis(3999L),
                    fromEpochMillis(3998L)));


    testHarness.processWatermark(new Watermark(999));
    expectedOutput.add(new Watermark(999));
    ASSERTER.assertOutputEqualsSorted(
            ""Output was not correct."", expectedOutput, testHarness.getOutput());


    testHarness.close();
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 04 19:57:27 UTC 2024,,,,,,,,,,"0|z1mwjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 19:57;walls.flink.m;Hi [~xuyangzhong] 

Are you working on this? Can I take this up ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SlotMatchingStrategy related logic,FLINK-34174,13565520,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,RocMarshal,RocMarshal,20/Jan/24 14:10,24/Jan/24 04:58,04/Jun/24 20:40,24/Jan/24 04:58,,,,,,,,1.19.0,,,,Runtime / Task,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 04:58:25 UTC 2024,,,,,,,,,,"0|z1mw4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 03:29;RocMarshal;The main reason for initiating this ticket is 
https://issues.apache.org/jira/browse/FLINK-31449  
(IIUC) as the current related logic is no longer being used.;;;","24/Jan/24 04:58;fanrui;Merged to master(1.19.0) via : 551bb9ebdc484a41951bb3aa9b88430cdca1c0d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement CatalogTable.Builder,FLINK-34173,13565485,13557293,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,jhughes,jhughes,jhughes,19/Jan/24 19:33,19/Feb/24 12:41,04/Jun/24 20:40,19/Feb/24 12:41,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 12:41:36 UTC 2024,,,,,,,,,,"0|z1mvx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 12:41;twalthr;Fixed as part of FLINK-33495.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for altering a distribution via ALTER TABLE ,FLINK-34172,13565484,13557293,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jhughes,jhughes,jhughes,19/Jan/24 19:33,25/Mar/24 08:23,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-19 19:33:26.0,,,,,,,,,,"0|z1mvww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot restore from savepoint when increasing parallelism of operator using reinterpretAsKeyedStream and RichAsyncFunction,FLINK-34171,13565479,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,burfordk,burfordk,19/Jan/24 18:46,23/Jan/24 10:42,04/Jun/24 20:40,,1.17.0,,,,,,,,,,,API / DataStream,Runtime / Checkpointing,Runtime / State Backends,,0,,,,,"We recently upgraded from Flink 1.14.2 to 1.17.0. Our job has not materially changed beyond a few feature changes (enabling snapshot compression, unaligned checkpoints), but we're seeing the following exception when attempting to adjust the parallelism of our job up or down:
{code:java}
java.lang.RuntimeException: Exception occurred while setting the current key context.
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.setCurrentKey(StreamOperatorStateHandler.java:373)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setCurrentKey(AbstractStreamOperator.java:508)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setKeyContextElement(AbstractStreamOperator.java:503)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.setKeyContextElement1(AbstractStreamOperator.java:478)
    at org.apache.flink.streaming.api.operators.OneInputStreamOperator.setKeyContextElement(OneInputStreamOperator.java:36)
    at org.apache.flink.streaming.runtime.io.RecordProcessorUtils.lambda$getRecordProcessor$0(RecordProcessorUtils.java:59)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:94)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:75)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:51)
    at org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:64)
    at org.apache.flink.streaming.api.operators.async.queue.UnorderedStreamElementQueue$Segment.emitCompleted(UnorderedStreamElementQueue.java:272)
    at org.apache.flink.streaming.api.operators.async.queue.UnorderedStreamElementQueue.emitCompletedElement(UnorderedStreamElementQueue.java:159)
    at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:393)
    at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$1800(AsyncWaitOperator.java:92)
    at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:621)
    at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:602)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:383)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:345)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:712)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.IllegalArgumentException: Key group 30655 is not in KeyGroupRange{startKeyGroup=19346, endKeyGroup=19360}. Unless you're directly using low level state access APIs, this is most likely caused by non-deterministic shuffle key (hashCode and equals implementation).
    at org.apache.flink.runtime.state.KeyGroupRangeOffsets.newIllegalKeyGroupException(KeyGroupRangeOffsets.java:37)
    at org.apache.flink.runtime.state.heap.InternalKeyContextImpl.setCurrentKeyGroupIndex(InternalKeyContextImpl.java:77)
    at org.apache.flink.runtime.state.AbstractKeyedStateBackend.setCurrentKey(AbstractKeyedStateBackend.java:250)
    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.setCurrentKey(RocksDBKeyedStateBackend.java:430)
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.setCurrentKey(StreamOperatorStateHandler.java:371)
    ... 29 more{code}
 
We're seeing this in an operator where we make use of DataStreamUtils::reinterpretAsKeyedStream for collecting multiple tasks into a single operator chain. However, each task takes the same data structure as input with an immutable key (represented as a string) which all use the same exact KeySelector instance.

However, one pattern we're using here is a chain of:
KeyedProcessFunction --> RichAsyncFunction --> reinterpretAsKeyedStream(KeyedProcessFunction)

...and I suspect that this might have something to do the way that the buffered in-flight data from the RichAsyncFunction is redistributed during re-scaling. We've observed that this failure is seemingly non-deterministic during re-scaling, but the probability of encountering it (from our admittedly anecdotal and limited testing) is reduced, but not eliminated, when we disable unaligned checkpoints. (Note that we first take a savepoint, restore with unaligned checkpoints disabled, then take another savepoint which we then use to adjust the parallelism to keep ""persisted in-flight data"" out the savepoint.)

We've never had any issues in the past with this under 1.14, so we're wondering if this is due to unaligned checkpointing, or possibly a regression/change in behavior since then. And if it is due to unaligned checkpointing, any thoughts on why disabling it hasn't seemed to address the problem?

Update: I took a savepoint and confirmed that it could not be restored under a new parallelism. I then removed the RichAsyncFunction and restarted the job (using --allowNonRestoredState) successfully. I was then able to take another savepoint and restart the job with the RichAsyncFunction re-inserted into the DS.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Tue Jan 23 10:42:08 UTC 2024,,,,,,,,,,"0|z1mvvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 16:40;mapohl;Thanks for reporting this, [~burfordk]. Are you able to reproduce it? That might help identifying the version where it appeared.;;;","22/Jan/24 17:42;burfordk;It's intermittently reproducible in 1.17.0 with both aligned and unaligned checkpoints, but I haven't attempted to see if it's present in a previous version. We had made multiple parallelism changes under 1.14.2 with this job without issue, so either we were very likely for over a year, or the issue appeared in a future version.

I've found that we're more likely to reproduce it by letting our input fall behind, restarting the job, and then immediately taking a savepoint. Essentially, if we can increase the size of the ""processed in-flight data"", we increase the probability that a savepoint will exhibit this failure on restart when the parallelism has been changed.

As a workaround, I'm concatenating the parallelism onto the RichAsyncFunction's uid, essentially forcing the state to clear when we make a parallelism change. That would seem to confirm my suspicion that this has to do with in-flight messages in that operator not being redistributed in the same way as the upstream KeyedProcessFunction.

Were there any changes made to how RichAsyncFunction state is persisted/redistributed between 1.14 and 1.17?;;;","23/Jan/24 10:42;mapohl;that's where I hoped bisecting the Flink versions to pin down the version where this change was introduced would help. [~afedulov] do you someone we could reach out to in this part of the codebase?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include the look up join conditions in the optimised plan.,FLINK-34170,13565473,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,davidradl,davidradl,19/Jan/24 17:13,29/Feb/24 11:30,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"As per [https://github.com/apache/flink-connector-jdbc/pull/79#discussion_r1458664773]

[~libenchao] asked that I raise this issue to Include the look up join conditions in the optimised plan; in lime with the scan conditions. The JDBC and other lookup sources could then be updated to pick up the conditions from the plan. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33365,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-19 17:13:45.0,,,,,,,,,,"0|z1mvug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[benchmark] CI fails during test running,FLINK-34169,13565392,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,zakelly,zakelly,19/Jan/24 08:42,02/Feb/24 15:35,04/Jun/24 20:40,02/Feb/24 15:35,,,,,,,,,,,,Benchmarks,,,,0,,,,,"[CI link: https://github.com/apache/flink-benchmarks/actions/runs/7580834955/job/20647663157?pr=85|https://github.com/apache/flink-benchmarks/actions/runs/7580834955/job/20647663157?pr=85]

which says:
{code:java}
// omit some stack traces
	Caused by: java.util.concurrent.ExecutionException: Boxed Error
1115		at scala.concurrent.impl.Promise$.resolver(Promise.scala:87)
1116		at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:79)
1117		at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
1118		at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
1119		at org.apache.pekko.actor.ActorRef.tell(ActorRef.scala:141)
1120		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:317)
1121		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
1122		... 22 more
1123	Caused by: java.lang.NoClassDefFoundError: javax/activation/UnsupportedDataTypeException
1124		at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.createKnownInputChannel(SingleInputGateFactory.java:387)
1125		at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.lambda$createInputChannel$2(SingleInputGateFactory.java:353)
1126		at org.apache.flink.runtime.shuffle.ShuffleUtils.applyWithShuffleTypeCheck(ShuffleUtils.java:51)
1127		at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.createInputChannel(SingleInputGateFactory.java:333)
1128		at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.createInputChannelsAndTieredStorageService(SingleInputGateFactory.java:284)
1129		at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.create(SingleInputGateFactory.java:204)
1130		at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.createInputGates(NettyShuffleEnvironment.java:265)
1131		at org.apache.flink.runtime.taskmanager.Task.<init>(Task.java:418)
1132		at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:821)
1133		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
1134		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
1135		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
1136		at java.base/java.lang.reflect.Method.invoke(Method.java:566)
1137		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
1138		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
1139		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
1140		... 23 more
1141	Caused by: java.lang.ClassNotFoundException: javax.activation.UnsupportedDataTypeException
1142		at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
1143		at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
1144		at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
1145		... 39 more {code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 15:34:57 UTC 2024,,,,,,,,,,"0|z1mvcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 08:43;zakelly;After offline discussion with [~yunfengzhou] , he has some ideas that might be able to fix it.;;;","02/Feb/24 02:43;yunfengzhou;Fixed in 8dd28a2e8bf10ec278ff90c99563468e294a135a.

Hi [~Zakelly] Could you please help close this issue?;;;","02/Feb/24 15:34;zakelly;Thanks, closing now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Refactor all callers that using the public Xxx getXxx(String key) and public void setXxx(String key, Xxx value) ",FLINK-34168,13565382,13564656,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuannan,fanrui,fanrui,19/Jan/24 07:45,19/Jan/24 07:45,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / Configuration,,,,0,,,,,"Refactor all callers that using the public Xxx getXxx(String key) and public void setXxx(String key, Xxx value)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-19 07:45:21.0,,,,,,,,,,"0|z1mva8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add dependence to fit jdk21,FLINK-34167,13565378,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,blackpighe,blackpighe,19/Jan/24 07:23,19/Jan/24 07:45,04/Jun/24 20:40,,jdbc-3.1.1,,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,,"add dependence to fit jdk21

When pipelining jdk 21+flink 1.19, an error occurred with the message
{code:java}
javax.activation.UnsupportedDataTypeException {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 07:36:54 UTC 2024,,,,,,,,,,"0|z1mv9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 07:36;blackpighe;{code:java}
Caused by: java.util.concurrent.ExecutionException: Boxed Error
266	at scala.concurrent.impl.Promise$.resolver(Promise.scala:87)
267	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:79)
268	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
269	at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
270	at org.apache.pekko.actor.ActorRef.tell(ActorRef.scala:141)
271	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:317)
272	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
273	... 22 more
274Caused by: java.lang.NoClassDefFoundError: javax/activation/UnsupportedDataTypeException
275	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.createKnownInputChannel(SingleInputGateFactory.java:387)
276	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.lambda$createInputChannel$2(SingleInputGateFactory.java:353)
277	at org.apache.flink.runtime.shuffle.ShuffleUtils.applyWithShuffleTypeCheck(ShuffleUtils.java:51)
278	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.createInputChannel(SingleInputGateFactory.java:333)
279	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.createInputChannelsAndTieredStorageService(SingleInputGateFactory.java:284)
280	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.create(SingleInputGateFactory.java:204)
281	at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.createInputGates(NettyShuffleEnvironment.java:265)
282	at org.apache.flink.runtime.taskmanager.Task.<init>(Task.java:418)
283	at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:815)
284	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
285	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
286	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
287	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
288	... 23 more
289Caused by: java.lang.ClassNotFoundException: javax.activation.UnsupportedDataTypeException
290	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
291	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
292	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
293	... 36 more {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyedLookupJoinWrapper incorrectly process delete message for inner join when previous lookup result is empty,FLINK-34166,13565374,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,19/Jan/24 07:04,25/Jan/24 01:58,04/Jun/24 20:40,22/Jan/24 14:04,1.17.2,1.18.1,,,,,,1.17.3,1.18.2,1.19.0,,Table SQL / Runtime,,,,0,pull-request-available,,,,"KeyedLookupJoinWrapper(when 'table.optimizer.non-deterministic-update.strategy
' is set to 'TRY_RESOLVE' and the lookup join exists NDU problemns) incorrectly process delete message for inner join when previous lookup result is empty

The intermediate delete result 
{code}
        expectedOutput.add(deleteRecord(3, ""c"", null, null));
{code}
in current case KeyedLookupJoinHarnessTest#testTemporalInnerJoinWithFilterLookupKeyContainsPk is incorrect:
{code}
    @Test
    public void testTemporalInnerJoinWithFilterLookupKeyContainsPk() throws Exception {
        OneInputStreamOperatorTestHarness<RowData, RowData> testHarness =
                createHarness(JoinType.INNER_JOIN, FilterOnTable.WITH_FILTER, true);

        testHarness.open();

        testHarness.processElement(insertRecord(1, ""a""));
        testHarness.processElement(insertRecord(2, ""b""));
        testHarness.processElement(insertRecord(3, ""c""));
        testHarness.processElement(insertRecord(4, ""d""));
        testHarness.processElement(insertRecord(5, ""e""));
        testHarness.processElement(updateBeforeRecord(3, ""c""));
        testHarness.processElement(updateAfterRecord(3, ""c2""));
        testHarness.processElement(deleteRecord(3, ""c2""));
        testHarness.processElement(insertRecord(3, ""c3""));

        List<Object> expectedOutput = new ArrayList<>();
        expectedOutput.add(insertRecord(1, ""a"", 1, ""Julian""));
        expectedOutput.add(insertRecord(4, ""d"", 4, ""Fabian""));
        expectedOutput.add(deleteRecord(3, ""c"", null, null));
        expectedOutput.add(insertRecord(3, ""c2"", 6, ""Jark-2""));
        expectedOutput.add(deleteRecord(3, ""c2"", 6, ""Jark-2""));
        expectedOutput.add(insertRecord(3, ""c3"", 9, ""Jark-3""));

        assertor.assertOutputEquals(""output wrong."", expectedOutput, testHarness.getOutput());
        testHarness.close();
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 01:58:45 UTC 2024,,,,,,,,,,"0|z1mv8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 06:29;lincoln.86xy;fixed in master: 
27e6ac836171c5c5539ceeb234a806be661cc30a
b79d3f01a4e1160f097bc6a0273b8da65d626483;;;","23/Jan/24 11:42;lincoln.86xy;fixed in release-1.17: 4e795b5f01a42659367a55d9913e60b599247a92;;;","25/Jan/24 01:58;lincoln.86xy;fixed in release-1.18: 78064d93cbb762372d5567a0211c0e8e55e40a7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
It seems that Apache download link has been changed,FLINK-34165,13565372,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,19/Jan/24 06:59,23/Jan/24 09:39,04/Jun/24 20:40,23/Jan/24 09:37,1.15.4,1.16.3,1.17.2,1.18.1,,,,,,,,flink-docker,,,,0,pull-request-available,,,,"For example, the link [https://www.apache.org/dist/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz.asc][1] worked previously now redirect to a list page which leads to a wrong flink.tgz.asc with HTML instead of expected signature.

!image-2024-01-19-07-55-07-775.png!

The link should be replace with [https://downloads.apache.org/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz.asc]

 

[1] [https://github.com/apache/flink-docker/blob/627987997ca7ec86bcc3d80b26df58aa595b91af/1.17/scala_2.12-java11-ubuntu/Dockerfile#L48C19-L48C101]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33842,,,,,,"19/Jan/24 06:55;jingge;image-2024-01-19-07-55-07-775.png;https://issues.apache.org/jira/secure/attachment/13066143/image-2024-01-19-07-55-07-775.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-19 06:59:51.0,,,,,,,,,,"0|z1mv80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Benchmark] Compilation error since Jan. 16th,FLINK-34164,13565371,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,zakelly,zakelly,19/Jan/24 06:38,19/Jan/24 08:50,04/Jun/24 20:40,19/Jan/24 08:50,,,,,,,,1.19.0,,,,Benchmarks,,,,0,pull-request-available,,,,"An error occured during the benchmark compile:
{code:java}
13:17:40 [ERROR] /mnt/jenkins/workspace/flink-main-benchmarks/flink-benchmarks/warning:[options] bootstrap class path not set in conjunction with -source 8
13:17:40 /mnt/jenkins/workspace/flink-main-benchmarks/flink-benchmarks/src/main/java/org/apache/flink/benchmark/StreamGraphUtils.java:38:19: error: cannot find symbol {code}
It seems related with the FLINK-33980",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 08:50:01 UTC 2024,,,,,,,,,,"0|z1mv7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 06:42;JunRuiLi;Thanks [~Zakelly] for pointing out, I'll prepare a pr to fix this issue.;;;","19/Jan/24 08:50;masteryhx;merged fc87459 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate SimplifyJoinConditionRule,FLINK-34163,13565347,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Jan/24 23:09,18/Jan/24 23:25,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-18 23:09:37.0,,,,,,,,,,"0|z1mv2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate LogicalUnnestRule,FLINK-34162,13565346,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Jan/24 23:06,08/Apr/24 09:08,04/Jun/24 20:40,08/Apr/24 09:08,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 09:08:26 UTC 2024,,,,,,,,,,"0|z1mv28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 09:08;Sergey Nuyanzin;Merged as [4d762c2bdc0720e1bf2615e3d30a5741c4688212|https://github.com/apache/flink/commit/4d762c2bdc0720e1bf2615e3d30a5741c4688212];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate RewriteMinusAllRule,FLINK-34161,13565345,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Jan/24 23:03,08/Apr/24 10:12,04/Jun/24 20:40,08/Apr/24 10:12,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 10:12:32 UTC 2024,,,,,,,,,,"0|z1mv20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 10:12;Sergey Nuyanzin;Merged as [97a67277c1d7878f320ab1c67589e05fdd8b153a|https://github.com/apache/flink/commit/97a67277c1d7878f320ab1c67589e05fdd8b153a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate FlinkCalcMergeRule,FLINK-34160,13565344,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Jan/24 22:59,19/Feb/24 22:31,04/Jun/24 20:40,19/Feb/24 22:31,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 22:31:02 UTC 2024,,,,,,,,,,"0|z1mv1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 22:31;Sergey Nuyanzin;Merged as [c2eac7ec85bef93fe2b61c028984e704c5a9d126|https://github.com/apache/flink/commit/c2eac7ec85bef93fe2b61c028984e704c5a9d126];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate ConstantRankNumberColumnRemoveRule,FLINK-34159,13565343,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Jan/24 22:52,08/Apr/24 09:12,04/Jun/24 20:40,08/Apr/24 09:12,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 09:12:26 UTC 2024,,,,,,,,,,"0|z1mv1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 09:12;Sergey Nuyanzin;Merged as [3ec9bfb5ba5e43c001fdd876148ff75722e6e4f9|https://github.com/apache/flink/commit/3ec9bfb5ba5e43c001fdd876148ff75722e6e4f9];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate WindowAggregateReduceFunctionsRule,FLINK-34158,13565342,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Jan/24 22:45,15/Apr/24 06:29,04/Jun/24 20:40,15/Apr/24 06:29,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 06:29:18 UTC 2024,,,,,,,,,,"0|z1mv1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 06:29;Sergey Nuyanzin;Merged as  [f74dc57561a058696bd2bd42593f862a9b490474|https://github.com/apache/flink/commit/f74dc57561a058696bd2bd42593f862a9b490474];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate FlinkLimit0RemoveRule,FLINK-34157,13565340,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Jan/24 22:39,12/Feb/24 22:19,04/Jun/24 20:40,12/Feb/24 22:19,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 12 22:19:01 UTC 2024,,,,,,,,,,"0|z1mv0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/24 22:19;Sergey Nuyanzin;Merged to master as [6f74889cbb52a2e7c11ad6cd86db70826000004c|https://github.com/apache/flink/commit/6f74889cbb52a2e7c11ad6cd86db70826000004c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Flink Calcite rules from Scala to Java,FLINK-34156,13565339,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Jan/24 22:38,11/Mar/24 00:36,04/Jun/24 20:40,,,,,,,,,2.0.0,,,,Table SQL / Planner,,,,0,,,,,"This is an umbrella task for migration of Calcite rules from Scala to Java mentioned at [https://cwiki.apache.org/confluence/display/FLINK/2.0+Release]

The reason is that since 1.28.0 ( CALCITE-4787 - Move core to use Immutables instead of ImmutableBeans ) Calcite started to use Immutables ([https://immutables.github.io/]) and since 1.29.0 removed ImmutableBeans ( CALCITE-4839 - Remove remnants of ImmutableBeans post 1.28 release ). All rule configuration related api which is not Immutables based is marked as deprecated. Since Immutables implies code generation while java compilation it is seems impossible to use for rules in Scala code.

We could follow steps from javadocs of {{org.apache.calcite.plan.RelRule}} written for migration from deprecated java api to Immutables. 
It would work for scala to java migration as well.
Please keep in mind that there is +*no need*+ to migrate rules extending +ConverterRule+ since these rules do not have such problem.",,,,,,,,,,,,,,,,,,,FLINK-34473,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 00:36:00 UTC 2024,,,,,,,,,,"0|z1mv0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 01:56;jiabao.sun;Hi [~Sergey Nuyanzin], can I help with this task?;;;","19/Jan/24 02:11;jackylau;hi [~Sergey Nuyanzin] can I also help with this task?;;;","19/Jan/24 06:45;Sergey Nuyanzin;Thanks for volunteering
currently this is only MVP activity meaning that the main part is aiming for later (2.0 )
as also mentioned at confluence page;;;","19/Jan/24 12:42;jiabao.sun;Thanks [~Sergey Nuyanzin].
If there is a need for assistance, please feel free to ping me at any time.;;;","20/Feb/24 01:59;337361684@qq.com;Hi, [~Sergey Nuyanzin] . Since I have been continuously involved in the development related to table-planner and calcite, I am quite familiar with this area. Could I possibly join this work to help you to deal with some subtasks? Looking forward your reply, Thanks.;;;","06/Mar/24 20:18;Sergey Nuyanzin;Thanks everyone for volunteering

Yes I think we could have something like collaboration here
It would be great if everyone here will not only submit PRs but also help to review it

About  the procedure to migrate: I would suggest to follow the steps already described at javadocs of {{org.apache.calcite.plan.RelRule}}
In fact the steps are written for migration from deprecated java api to Immutables. It would work for scala to java migration as well.
Also it would make sense to mention that there is *NO* need to migrate rules extending {{ConverterRule}} since such rules do not have the problem described in issue description.;;;","07/Mar/24 01:14;337361684@qq.com;Thanks, [~Sergey Nuyanzin] . I will try to review these PRs.;;;","07/Mar/24 06:10;jackylau;hi [~Sergey Nuyanzin] Thanks for your summary.

Although the current ConverterRule does not have the issue described, and the current Flink planner module has achieved Scala-free status, I still suggest migrate to Java to maintain consistency in the rules. What do you think?

And i am familiar with planner model and calcite, I am very happy to participate in the review work.
 ;;;","07/Mar/24 06:25;Sergey Nuyanzin;I do not see the real benefit from such migration ({{{}ConverterRules{}}}), besides that there is more code than just {{{}ConverterRules{}}}) and then we could come up with everything.
Rules written both in Scala and Java are working fine together, so it was not an issue before, I don't think it is an issue now.
The real reason of this migration is inability to use non deprecated code to configure these rules in Scala

We could think about full migration to java in future, however i guess it should be discussed in ML as well

UPD: Also I updated description;;;","07/Mar/24 11:55;jackylau;[~Sergey Nuyanzin] 
I've noticed that some Scala code is being converted to Java in other PRs. Given that migrating these few doesn't pose any problems, maybe we should leave the rest as is. Since the issues for these have already been created and the PRs have been submitted, let's just continue with them. What do you think?
 
 ;;;","07/Mar/24 12:19;Sergey Nuyanzin;I would suggest put it on hold with the lowest minor priority
We shouldn't priorities working on it when the real issue is not fixed and these changes do not change the situation

We could come back to these converter rules once the main issue is fixed;;;","08/Mar/24 01:58;jackylau;[~Sergey Nuyanzin] I agree that these can be set to a low priority, but since these PRs have already been submitted, I personally think there is no need to close them. Besides, in the long run, to maintain uniformity in rule coding, these will eventually need to be migrated, and the community is also trying to avoid writing new code in Scala as much as possible. Moreover, Calcite itself is written in Java. So, I believe there is no need to close the PRs for the converter rules. What do you think?;;;","08/Mar/24 01:59;jackylau;[~Sergey Nuyanzin] 
If the community is short on manpower, I can also get my colleagues involved in submitting PRs and reviewing code.;;;","11/Mar/24 00:36;Sergey Nuyanzin;IMHO the more people the better for the community

may be however it would also make sense to double check with them what they are going to do, especially if they are new to Flink community;;;",,,,,,,,,,,,,,,,,,,
Recurring SqlExecutionException,FLINK-34155,13565283,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jeyhunkarimov,jeyhunkarimov,18/Jan/24 16:02,24/Jan/24 07:30,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Tests,,,,0,test-stability,,,,"When analyzing very big maven log file in our CI system, I found out that there is a recurring {{{}SqlException (subset of the log file is attached){}}}:

 
{{org.apache.flink.table.gateway.service.utils.SqlExecutionException: Only 'INSERT/CREATE TABLE AS' statement is allowed in Statement Set or use 'END' statement to submit Statement Set.}}
 
 

which leads to:

 
{{06:31:41,155 [flink-rest-server-netty-worker-thread-22] ERROR org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler [] - Unhandled exception.}}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34098,,,,,,"18/Jan/24 16:01;jeyhunkarimov;disk-full.log;https://issues.apache.org/jira/secure/attachment/13066133/disk-full.log",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 07:30:42 UTC 2024,,,,,,,,,,"0|z1muo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 01:38;lincoln.86xy;[~jeyhunkarimov] thanks for the investigation! Could you offer more information of the test cases related to above exceptions? 
I saw there were several cases expected these error msg, e.g., statement_set.q and begin_statement_set.q;;;","19/Jan/24 08:20;jeyhunkarimov;Hi [~lincoln.86xy] for example {{org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase is the one failing.}};;;","23/Jan/24 02:30;fsk119;Hi [~jeyhunkarimov]. 

The exception mentioned in the text are due to the addition of exception cases in our testing. The same exceptions occurring in multiple places:

1. During the process of submitting SQL, an illegal query is detected. The SQL gateway currently logs this exception;
2. When users directly fetch the results of the exceptional SQL, the logging system will also record the same exception. 

I see that there is some redundancy in the exception information here, but I believe it does not affect the stability of the tests, at most it may affect future troubleshooting. This test hasn't been modified for a long time, if there were any issues, the CI tests would have become unstable a while ago.
 
 ;;;","24/Jan/24 07:30;mapohl;How big is the mvn.log? I'm wondering whether it would be possible to share the entire log for investigation purposes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.apache.zookeeper:zookeeper from 3.5.9 to 3.7.2 for Kafka connector,FLINK-34154,13565279,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,18/Jan/24 15:14,19/Jan/24 08:15,04/Jun/24 20:40,19/Jan/24 08:15,,,,,,,,kafka-3.1.0,,,,Connectors / Kafka,,,,0,,,,,"The Flink Kafka connector still uses Zookeeper but only for tests. Version 3.5.9 has a CVE, we should bump this to avoid getting falsely flagged for this vulnerability ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 08:15:00 UTC 2024,,,,,,,,,,"0|z1munc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 08:15;martijnvisser;Fixed in apache/flink-connector-kafka:main 6f06f158e75fc557523eca25724ef026b911af61;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set ALWAYS ChainingStrategy in TemporalSort,FLINK-34153,13565276,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,18/Jan/24 15:08,19/Jan/24 08:07,04/Jun/24 20:40,19/Jan/24 08:07,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,Similarly to FLINK-27992 we should ALWAYS chaining strategy in TemporalSort operator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 08:07:09 UTC 2024,,,,,,,,,,"0|z1mumo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 08:07;dwysakowicz;Implemented in 3d088f6e154d1c380b8c2273ede69f76e12df598;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tune TaskManager memory,FLINK-34152,13565267,13570189,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,18/Jan/24 13:39,14/Mar/24 14:31,04/Jun/24 20:40,14/Mar/24 14:31,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,Kubernetes Operator,,,2,pull-request-available,,,,"The current autoscaling algorithm adjusts the parallelism of the job task vertices according to the processing needs. By adjusting the parallelism, we systematically scale the amount of CPU for a task. At the same time, we also indirectly change the amount of memory tasks have at their dispense. However, there are some problems with this.
 # Memory is overprovisioned: On scale up we may add more memory than we actually need. Even on scale down, the memory / cpu ratio can still be off and too much memory is used.
 # Memory is underprovisioned: For stateful jobs, we risk running into OutOfMemoryErrors on scale down. Even before running out of memory, too little memory can have a negative impact on the effectiveness of the scaling.

We lack the capability to tune memory proportionally to the processing needs. In the same way that we measure CPU usage and size the tasks accordingly, we need to evaluate memory usage and adjust the heap memory size.

https://docs.google.com/document/d/19GXHGL_FvN6WBgFvLeXpDABog2H_qqkw1_wrpamkFSc/edit

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 09:07:23 UTC 2024,,,,,,,,,,"0|z1muko:",9223372036854775807,"TaskManager memory (heap, network, metaspace, managed) is optimized together with autoscaling decisions.",,,,,,,,,,,,,,,,,,,"02/Feb/24 17:27;ema_pp;Thanks Maximilian for reporting this. It's indeed an issue, especially the `OutOfMemoryErrors` on scale in.;;;","01/Mar/24 10:58;yang;Hello [~mxm] , I am very interested in this memory tuning feature and  I have reviewed this PR  [https://github.com/apache/flink-kubernetes-operator/pull/762|https://github.com/apache/flink-kubernetes-operator/pull/762.]

Here is my feedback about this auto-tuning feature
 * We may need to dynamically adjust the Kubernetes CPU and memory limits for both the job manager and task manager eventually, to align with the automatically tuned memory and CPU parameters and prevent unnecessary resource allocation.


 * In our specific use-case, our Flink cluster is deployed on a dedicated node group with predefined CPU and memory settings, unlike a typical Kubernetes cluster. Consequently, this auto-tuning feature might not aid in reducing infrastructure costs, as billing is based on the allocated nodes behind the scenes.;;;","01/Mar/24 11:18;mxm;Hi [~yang]! Thanks for taking a look at the recent changes. There has been two more follow-up PRs since the initial PR you linked. I'm very curious to hear your feedback.
{quote}We may need to dynamically adjust the Kubernetes CPU and memory limits for both the job manager and task manager eventually, to align with the automatically tuned memory and CPU parameters and prevent unnecessary resource allocation.
{quote}
Tuning JobManager memory is still pending, but I agree that tuning only TaskManagers is not enough. As for tuning CPU, I think we eventually want to tune the number of task slots to fit them to the CPUs assigned. As for scaling CPU itself, that is already taken care of by the autoscaler which essentially scales based on the CPU usage of TaskManagers.
{quote}In our specific use-case, our Flink cluster is deployed on a dedicated node group with predefined CPU and memory settings, unlike a typical Kubernetes cluster. Consequently, this auto-tuning feature might not aid in reducing infrastructure costs, as billing is based on the allocated nodes behind the scenes.
{quote}
Autoscaling assumes some sort of Kubernetes Cluster Autoscaling to be active. When fewer resources are allocated, that should result in fewer nodes, but in practice it isn't quite that easy. It requires a bit of extra work for nodes to get released when fewer resources are used. The default Kubernetes scheduler doesn't bin-pack, but it can be reconfigured to do bin-packing as opposed to its default behavior to evenly spread out pods.;;;","01/Mar/24 15:56;yang;Hi [~mxm] after reviewing those 2 pr and especially this one [https://github.com/apache/flink-kubernetes-operator/pull/786] , I have understood better how memory tuning is supposed to handle the flink cluster's scaling down. 
To answer your comment :

{*}“{*}We may need to dynamically adjust the Kubernetes CPU and memory limits for both the job manager and task manager eventually, to align with the automatically tuned memory and CPU parameters and prevent unnecessary resource allocation.{*}”{*}

What I'm trying to say is that I didn't notice any updated configuration for the memory request and limit at the pod template level for the taskmanager. Therefore, I assume that the pod's memory allocation won't automatically adjust to reflect changes in the taskmanager's heap size, unless I've missed something.


{*}""{*}Autoscaling assumes some sort of Kubernetes Cluster Autoscaling to be active. When fewer resources are allocated, that should result in fewer nodes, but in practice it isn't quite that easy. It requires a bit of extra work for nodes to get released when fewer resources are used. The default Kubernetes scheduler doesn't bin-pack, but it can be reconfigured to do bin-packing as opposed to its default behavior to evenly spread out pods.{*}""{*}

Indeed, by implementing bin-packing, we can optimize resource utilization, which is now clearer to me. However, its management becomes more complex (K8s upgrade, daily node restart/eviction) for sure, especially when there are other application components in the same Kubernetes cluster IMO.  But I am very curious now to test this feature when it's fully released, it looks indeed another important step to push flink to cloud native :);;;","01/Mar/24 17:33;yang;Hi [~mxm] , Here's some feedback that might be slightly outside the scope: I've conducted tests with a few custom enhancements aimed at:



 # Protecting memory during scale-down operations to prevent the Flink cluster from experiencing Out of Memory (OOM) issues.
 # Implementing a minor improvement to refine the scaling decisions following each operator's rescaling, ensuring there are no unused slots post-rescaling.



After reviewing the memory tuning feature, I believe my memory protection enhancement may no longer be necessary. However, I've still pushed the minor improvement to eliminate any unused slots.


Can you take a look on it https://issues.apache.org/jira/browse/FLINK-34563 and [https://github.com/apache/flink-kubernetes-operator/pull/787] ? And tell me if you think it's making sense, thanks :);;;","02/Mar/24 08:58;mxm;{quote}What I'm trying to say is that I didn't notice any updated configuration for the memory request and limit at the pod template level for the taskmanager. Therefore, I assume that the pod's memory allocation won't automatically adjust to reflect changes in the taskmanager's heap size, unless I've missed something.
{quote}
I would ask you to check again. When the tuning is applied, the pod resource requests/limits of the TaskManager pods will be adjusted. So changes will be directly reflected in terms of resource usage in Kubernetes.
{quote}Indeed, by implementing bin-packing, we can optimize resource utilization, which is now clearer to me. However, its management becomes more complex (K8s upgrade, daily node restart/eviction) for sure, especially when there are other application components in the same Kubernetes cluster IMO
{quote}
You are right, there is more complexity to realize Flink Autoscaling benefits end to end. However, there is also a great amount of resource savings and convenience for the user that come out of it. We have seen 60% fewer nodes after enabling Flink Autoscaling while maintaing the same amount of service and drastically decreasing the maintaince for our users who would have to adjust parallelism constantly to run cost-efficient. They usually did not want to do that and thus all jobs ran very over-provisioned.
{quote}Can you take a look on it https://issues.apache.org/jira/browse/FLINK-34563 and [https://github.com/apache/flink-kubernetes-operator/pull/787] ? And tell me if you think it's making sense, thanks :)
{quote}
Thank you, I'll review in the next days!;;;","05/Mar/24 17:32;yang;Hello [~mxm] , 

About the integration of Flink autoscaling with memory tuning into our Kubernetes (k8s) cluster. We believe that incorporating the Flink cluster into a standard k8s cluster will enhance our flexibility in resource allocation. However, a significant concern arises from the frequent node upgrades within our company's k8s cluster. These upgrades mandate rolling updates across all deployed applications, entailing a sequential restart of each pod.


Such operations are notoriously time-consuming. Moreover, they pose a unique challenge for Flink jobs, which are forced to restart with each task manager's stop-and-start cycle, leading to substantial downtime in comparison to other applications in our environment.

Our current flink cluster is less impacted by this because we are in a dedicated node group with always the same type instance inside, so we can do it in a fashion destroy and re-create everything once.

Given this backdrop, I am curious to know your thoughts on the potential development of Flink features designed to better accommodate the rolling update process. While elastic scaling represents a step in the right direction for mitigating such issues, but still I fear is not enough

Do you believe there are, or will be, enhancements to Flink that could ease the impact of rolling updates on Flink jobs? 

 ;;;","11/Mar/24 09:07;fanrui;Merged to main(1.8.0) via:
* 48692b7243086a2ccf2cb4c64a6b00306fa1d65f
* 3ead906f33a4b3790fa5c12f2018e09db9443a09
* d526174d9ab4c3ccf92548c821d9f44acbd3f247;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Integrate Karpenter resource limits into cluster capacity check,FLINK-34151,13565260,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mxm,mxm,mxm,18/Jan/24 12:48,14/Mar/24 14:33,04/Jun/24 20:40,,,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,Kubernetes Operator,,,0,,,,,"FLINK-33771 added cluster capacity checking for Flink Autoscaling decisions. The checks respect the scaling limits of the Kubernetes Cluster Autoscaler. 

We should also support Karpenter-based resource checks, as Karpenter is the preferred method of expanding the cluster size in some environments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-18 12:48:20.0,,,,,,,,,,"0|z1muj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
file sink e2e tests with local setup are not executed if s3 credentials are not provided,FLINK-34150,13565253,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,18/Jan/24 12:33,31/Jan/24 13:38,04/Jun/24 20:40,19/Jan/24 11:15,1.17.2,1.18.1,1.19.0,,,,,1.17.3,1.18.2,1.19.0,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"While looking into test parity between Azure Pipelines and GitHub Actions, I noticed that the only OpenSSL-based test is {{flink-end-to-end-tests/test-scripts/test_file_sink.sh}} which comes with a local and a s3 setting.

S3 requires S3 credential and the bucket information to be available through environment variables. That's handled in [flink-end-to-end-tests/test-scripts/common_s3.sh#L25|https://github.com/apache/flink/blob/8a9a08bf408aae8a33438a38614199efeb8f1c63/flink-end-to-end-tests/test-scripts/common_s3.sh#L25]. The problem is that this shell script is also source'd when running the test with local setup (see [flink-end-to-end-tests/test-scripts/test_file_sink.sh#L27|https://github.com/apache/flink/blob/a6bea224ed012e5594ee755526f54ae7f3b0d22f/flink-end-to-end-tests/test-scripts/test_file_sink.sh#L27]).

This means that also the local test is only running in the main repository which is not necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34324,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 11:15:48 UTC 2024,,,,,,,,,,"0|z1muhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 12:35;mapohl;This issue is came up in FLINK-33923 because we noticed that the OpenSSL dependency is installed for the e2e tests in the Azure Pipelines but we missed it in the GitHub Actions migration. But that doesn't cause any issues because the only OpenSSL-based e2e test is actually disabled. Merging FLINK-33914 would have revealed the issue on {{master}} most likely after the corresponding S3 secrets are set.;;;","19/Jan/24 11:15;mapohl;master: [3a3b724136e91f9b8ac792e98517a335963c929a|https://github.com/apache/flink/commit/3a3b724136e91f9b8ac792e98517a335963c929a]
1.18: [6e96772f8d50d21b64ff2b5b601253b4c307a624|https://github.com/apache/flink/commit/6e96772f8d50d21b64ff2b5b601253b4c307a624]
1.17: [a775f2e1ca2f53ec88ff5d2d20acf414840c7c1a|https://github.com/apache/flink/commit/a775f2e1ca2f53ec88ff5d2d20acf414840c7c1a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kafka connector can't compile against 1.19-SNAPSHOT,FLINK-34149,13565246,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunfengzhou,martijnvisser,martijnvisser,18/Jan/24 11:58,19/Jan/24 12:34,04/Jun/24 20:40,19/Jan/24 12:15,1.19.0,,,,,,,1.19.0,,,,Connectors / Kafka,Runtime / Checkpointing,,,0,pull-request-available,,,,"The Flink Kafka connector for {{main}} fails for 1.19-SNAPSHOT, see https://github.com/apache/flink-connector-kafka/actions/runs/7569481434/job/20612876543#step:14:134

{code:java}
Error:  COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/dynamic/source/enumerator/StoppableKafkaEnumContextProxy.java:[65,8] org.apache.flink.connector.kafka.dynamic.source.enumerator.StoppableKafkaEnumContextProxy is not abstract and does not override abstract method setIsProcessingBacklog(boolean) in org.apache.flink.api.connector.source.SplitEnumeratorContext
{code}

This interface seems to be added as part of https://issues.apache.org/jira/browse/FLINK-32514 / https://cwiki.apache.org/confluence/display/FLINK/FLIP-309%3A+Support+using+larger+checkpointing+interval+when+source+is+processing+backlog
The FLIP indicates that the changes should be backward compatible, but that appears to have not been the case",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32514,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 12:34:53 UTC 2024,,,,,,,,,,"0|z1mug0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 11:59;martijnvisser;[~yunfengzhou] [~lindong] I've filed this ticket, after I've left the comment at https://issues.apache.org/jira/browse/FLINK-32514;;;","19/Jan/24 12:15;lindong;Merged to master branch b309ceb8de005fd65d979ffce928d08606f36e89;;;","19/Jan/24 12:34;martijnvisser;Thank you [~lindong] and [~yunfengzhou] - I'll check the Kafka connector :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential regression (Jan. 13): stringWrite with Java8,FLINK-34148,13565235,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Resolved,snuyanzin,zakelly,zakelly,18/Jan/24 10:58,21/Feb/24 18:41,04/Jun/24 20:40,04/Feb/24 15:17,,,,,,,,1.19.0,,,,API / Type Serialization System,,,,0,,,,,"Significant drop of performance in stringWrite with Java8 from commit [881062f352|https://github.com/apache/flink/commit/881062f352f8bf8c21ab7cbea95e111fd82fdf20] to [5d9d8748b6|https://github.com/apache/flink/commit/5d9d8748b64ff1a75964a5cd2857ab5061312b51] . It only involves strings not so long (128 or 4).

stringWrite.128.ascii(Java8) baseline=1089.107756 current_value=754.52452
stringWrite.128.chinese(Java8) baseline=504.244575 current_value=295.358989
stringWrite.128.russian(Java8) baseline=655.582639 current_value=421.030188
stringWrite.4.chinese(Java8) baseline=9598.791964 current_value=6627.929927
stringWrite.4.russian(Java8) baseline=11070.666415 current_value=8289.95767",,,,,,,,,,,,FLINK-34234,,,,,,,,,,,,,,,,,,,,,,FLINK-28016,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 18:41:37 UTC 2024,,,,,,,,,,"0|z1mudk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 12:04;zakelly;After some investigation, I found it is this commit [5d9d8748b6|https://github.com/apache/flink/commit/5d9d8748b64ff1a75964a5cd2857ab5061312b51] that introduce the regression.

The result without this commit:
{code:java}
19:10:36  Benchmark                                 (lengthStr)   (type)   Mode  Cnt      Score    Error   Units
19:10:36  StringSerializationBenchmark.stringWrite            4    ascii  thrpt   30  13269.950 ± 32.092  ops/ms
19:10:36  StringSerializationBenchmark.stringWrite            4  russian  thrpt   30  11040.363 ± 47.226  ops/ms
19:10:36  StringSerializationBenchmark.stringWrite            4  chinese  thrpt   30   9554.081 ± 38.286  ops/ms
19:10:36  StringSerializationBenchmark.stringWrite          128    ascii  thrpt   30   1086.989 ±  3.005  ops/ms
19:10:36  StringSerializationBenchmark.stringWrite          128  russian  thrpt   30    654.435 ±  1.718  ops/ms
19:10:36  StringSerializationBenchmark.stringWrite          128  chinese  thrpt   30    502.264 ±  2.158  ops/ms
19:10:36  StringSerializationBenchmark.stringWrite        16384    ascii  thrpt   30      6.041 ±  0.013  ops/ms
19:10:36  StringSerializationBenchmark.stringWrite        16384  russian  thrpt   30      3.203 ±  0.008  ops/ms
19:10:36  StringSerializationBenchmark.stringWrite        16384  chinese  thrpt   30      2.266 ±  0.007  ops/ms {code}
with this commit:
{code:java}
19:27:30  Benchmark                                 (lengthStr)   (type)   Mode  Cnt      Score    Error   Units
19:27:30  StringSerializationBenchmark.stringWrite            4    ascii  thrpt   30  13453.751 ± 31.950  ops/ms
19:27:30  StringSerializationBenchmark.stringWrite            4  russian  thrpt   30   8266.997 ± 15.012  ops/ms
19:27:30  StringSerializationBenchmark.stringWrite            4  chinese  thrpt   30   6618.916 ± 11.011  ops/ms
19:27:30  StringSerializationBenchmark.stringWrite          128    ascii  thrpt   30    754.952 ±  1.549  ops/ms
19:27:30  StringSerializationBenchmark.stringWrite          128  russian  thrpt   30    420.060 ±  0.806  ops/ms
19:27:30  StringSerializationBenchmark.stringWrite          128  chinese  thrpt   30    295.121 ±  0.604  ops/ms
19:27:30  StringSerializationBenchmark.stringWrite        16384    ascii  thrpt   30      6.052 ±  0.014  ops/ms
19:27:30  StringSerializationBenchmark.stringWrite        16384  russian  thrpt   30      3.201 ±  0.009  ops/ms
19:27:30  StringSerializationBenchmark.stringWrite        16384  chinese  thrpt   30      2.268 ±  0.004  ops/ms {code}
So [~Sergey Nuyanzin] could you please spare some time to investigate on this?;;;","19/Jan/24 12:08;zakelly;Also ping release managers of 1.19 [~lincoln.86xy] [~yunta] [~jingge] [~martijnvisser]  as you may have concerns on this.;;;","19/Jan/24 12:18;Sergey Nuyanzin;interesting, my first guess is jackson upgrade since they introduced some breaking changes
however need to check it
thanks for letting know;;;","19/Jan/24 12:35;martijnvisser;Let's mark it as a blocker; worst case we'll downgrade;;;","23/Jan/24 08:15;Sergey Nuyanzin;An interesting finding:
1. jackson 2.15.x requires maven-shade-plugin 3.4.1+ because of multirelease jar
2. Starting 3.4.1 maven-shade-plugin stopped modifying the dependency tree at runtime when bundling dependencies https://issues.apache.org/jira/browse/MSHADE-413 
as a result now these dependencies could be seen via {{mvn dependency:tree}} and there is no such for prev flink-shaded

I was able to reproduce the perf issue mentioned in description. And as a local test just added exclusions for bundle dependencies in flink-benchmarks. After that perf issue disappeared. It looks like it could be the root cause.

There are several ways to fix it 
1) Add such exclusions everywhere where it is required (however it looks hacky and might be error-prone in future)
2) Revert it to flink-shaded 1.17.0 (however need to keep in mind that it brings netty 4.1.91 which is lower than required by jdk21, even though current ci passes for jdk21 there could be a hidden issue)
3) Add fix to flink-shaded with making bundled deps optional like it was done within FLINK-28016 and create a new release of flink-shaded (1.18.1 e.g.) (The issue here is that it could take a couple of months to get enough binding votes before the release happens);;;","23/Jan/24 10:34;mapohl;Thanks for looking into it, [~Sergey Nuyanzin]. It sounds it's the same issue that was fixed for Flink's main repo in FLINK-28016 where we also introduced the {{<flink.markBundledAsOptional/>}} to cover this topic with the upgrade of the {{maven-shade-plugin}}. I'm wondering whether we should make the shade plugin being done for flink-shaded, entirely, instead of having a dedicated {{maven-shade-plugin}} upgrade for the jackson dependencies. Or was there a reason why you only wanted to apply this plugin upgrade to the jackson dependencies? 

If I understand FLINK-28016 correctly, running the release with Maven 3.2.5 (i.e. something lower than 3.3) might also work around this issue.

Keep in mind that it might be useful to also add the {{ShadeOptionalChecker}} (see [.tools/ci/verify_bundled_optional.sh|https://github.com/apache/flink/blob/5bf5003f5c7baf19b0164f78558e495d8bb62b04/tools/ci/verify_bundled_optional.sh]) to {{flink-shaded}} analogously to what we're doing with the {{LicenseChecker}} if we implement the {{<flink.markBundledAsOptional/>}} approach. Anything to add from your side, [~chesnay]?

So, from my side, trying to fix the issue in flink-shaded and doing another release 18.1 sounds reasonable. We can unblock the Apache Flink 1.19 release if we want to by just reverting to flink-shaded 17.0. The JDK21 issue isn't necessarily a blocking issue because JDK21 is still meant to be a beta feature.;;;","30/Jan/24 09:21;chesnay;Just chiming in to point out that the shade-plugin version shouldn't be relevant; we used 3.4.1 in Flink already in 1.17 and didn't run into issues. We only ever had issue due to more recent maven, so I'm questioning the conclusions in this ticket a bit.

We should be able to just bump the shade-plugin and call it a day.;;;","30/Jan/24 10:01;chesnay;So it's not really maven or the shade-plugin itself, but some interplay with the flatten plugin. The dependency reduced pom is written correctly, but then the flatten-plugin comes along and works on the original pom.

Upgrading the flatten-plugin to 1.2.7 resolved the issue for me locally.;;;","30/Jan/24 11:39;Sergey Nuyanzin;Thanks a lot [~chesnay]  for highlighting this

I confirm that upgrading of flatten plugin to 1.2.7 also resolves the issue and it looks like a much simpler way.

I was misleaded by the fact that in Flink we have optional tags and no issues with shading and in flink-shaded not and some issues appeared 
I should have paid more attention to other components as flatten plugin as well;;;","01/Feb/24 15:40;Sergey Nuyanzin;As it was decided in release meeting
flink-shaded upgrade was reverted as d6c7eee8243b4fe3e593698f250643534dc79cb5

[~Zakelly], [~martijnvisser] could you please check perf report?;;;","02/Feb/24 08:23;martijnvisser;[~Sergey Nuyanzin] It's looking good, copying a couple of links for easy checking:

stringWrite.128.ascii(Java8) http://flink-speed.xyz/timeline/#/?exe=1&ben=stringWrite.128.ascii&extr=on&quarts=on&equid=off&env=3&revs=200
stringWrite.128.chinese(Java8) http://flink-speed.xyz/timeline/#/?exe=1&ben=stringWrite.128.chinese&extr=on&quarts=on&equid=off&env=3&revs=200
stringWrite.128.russian(Java8) http://flink-speed.xyz/timeline/#/?exe=1&ben=stringWrite.128.russian&extr=on&quarts=on&equid=off&env=3&revs=200
stringWrite.4.chinese(Java8) http://flink-speed.xyz/timeline/#/?exe=1&ben=stringWrite.4.chinese&extr=on&quarts=on&equid=off&env=3&revs=200
stringWrite.4.russian(Java8) http://flink-speed.xyz/timeline/#/?exe=1&ben=stringWrite.4.russian&extr=on&quarts=on&equid=off&env=3&revs=200

;;;","04/Feb/24 15:15;zakelly;[~snuyanzin] I triggered a test on commit [0779c91e|https://github.com/apache/flink/commit/0779c91e581dc16c4aef61d6cc27774f11495907], it seems the regression is gone:

 
{code:java}
01:08:52  Benchmark                                 (lengthStr)   (type)   Mode  Cnt      Score    Error   Units
01:08:52  StringSerializationBenchmark.stringWrite            4    ascii  thrpt   30  13289.044 ± 55.119  ops/ms
01:08:52  StringSerializationBenchmark.stringWrite            4  russian  thrpt   30  11056.146 ± 37.383  ops/ms
01:08:52  StringSerializationBenchmark.stringWrite            4  chinese  thrpt   30   9580.176 ± 41.299  ops/ms
01:08:52  StringSerializationBenchmark.stringWrite          128    ascii  thrpt   30   1090.814 ±  1.763  ops/ms
01:08:52  StringSerializationBenchmark.stringWrite          128  russian  thrpt   30    656.249 ±  2.392  ops/ms
01:08:52  StringSerializationBenchmark.stringWrite          128  chinese  thrpt   30    504.180 ±  1.091  ops/ms
01:08:52  StringSerializationBenchmark.stringWrite        16384    ascii  thrpt   30      6.041 ±  0.029  ops/ms
01:08:52  StringSerializationBenchmark.stringWrite        16384  russian  thrpt   30      3.197 ±  0.010  ops/ms
01:08:52  StringSerializationBenchmark.stringWrite        16384  chinese  thrpt   30      2.272 ±  0.003  ops/ms {code}
 

Thanks for the fix!

I'm closing this.;;;","21/Feb/24 13:51;rmetzger;Thanks for addressing this issue.
Maybe I'm overlooking something in the discussion or related tickets, but have we understood what caused the performance regression in flink-shaded 1.18? E.g. do we know what we need to fix flink-shaded to move to flink-shaded 1.18.1 or 1.19?;;;","21/Feb/24 13:56;Sergey Nuyanzin;[~rmetzger] yes, there is FLINK-34234 under which there is a R addressing this issue on flink-shaded side;;;","21/Feb/24 14:03;rmetzger;Thanks, but how does the broken shading behavior cause a performance regression in the {{StringSerializationBenchmark}} ?;;;","21/Feb/24 18:41;Sergey Nuyanzin;Seems I didn't get your initial question, sorry

yes that's a good question. 
To be honest I don't know the exact answer.
However what we have so far: 
1. I checked that it's reproducible only for jdk8 among LTS (didn't check 9, 10) , e.g. for 11, 17 it is not reproducible, looks like it was fixed at some point on jdk level
2. I was not able to find any noticeable thing in profiler
3. Bisect shows that this perf regression comes with update of shading plugin (and changed behavior as mentioned above)
I tend to think that it could relate to something like classloading or SPI since without the fix amount of classes is bigger however as I mentioned this is only a hypotheses 
 ;;;",,,,,,,,,,,,,,,,,
TimestampData to/from LocalDateTime is ambiguous,FLINK-34147,13565220,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,337361684@qq.com,lirui,lirui,18/Jan/24 09:41,21/Feb/24 01:30,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,"It seems TimestampData is essentially an {{Instant}}. Therefore an implicit time zone is used in the {{fromLocalDateTime}} and {{toLocalDateTime}} methods. However neither the method name nor the API doc indicates which time zone is used. So from caller's perspective, the results of these two methods are ambiguous.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 03:17:52 UTC 2024,,,,,,,,,,"0|z1mua8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 09:53;337361684@qq.com;Hi, [~lirui] . I also think it ambiguous. IMO, `TimestampData` may be possible to split into two classes, such as `TimestampLtz` and `TimestampNtz`. In `TimestampLtz`,  it may contain methods  `fromInstant()/toInstant()`, for `TimestampNtz`, it may contain methods `fromLocalDateTime()/toLocalDateTime()`.  So, the logical type `LocalZonedTimestampType` need to represent by `TimestampLtz`, and the logical type `TimestampType` is represented by `TimestampNtz`.  

I'm not sure how significant the impact of such a change would be on the flink dataType system and whether it would be compatible.  [~jark] WDYT?;;;","24/Jan/24 12:03;lirui;I think having two separate classes is fine, but it might be an overkill. Perhaps we can just add methods that take a time zone parameter? Or we can at least improve the doc to clarify all conversions are performed in UTC.;;;","25/Jan/24 07:17;jark;I agree it is ambiguous. Currently, {{TimestampData}} represents both {{Instant}} and {{LocalDateTime}}. 
* When it represents as an {{Instant}}, you can use conversion methods {{fromInstant()/toInstant()}} to convert an {{Instant}} to {{TimestampData}}, but do not use {{fromLocalDateTime()/toLocalDateTime()}}. 
* When it represents as an {{LocalDateTime}}, you can use conversion methods {{fromLocalDateTime()/toLocalDateTime()}} to convert a {{LocalDateTime}} to {{TimestampData}}, but do not use {{fromInstant()/toInstant()}}. 

Therefore, if you have a Java {{LocalDateTime}} and want to convert it into an Instant {{TimestampData}}, you need to convert the {{LocalDateTime}} into Java {{Instant}} with a specific time zone first. And then use the {{Instant}} to create the {{TimestampData}}.

This is not very clear from the javadoc of {{TimestampData}} and I agree we need to improve the doc. ;;;","06/Feb/24 01:50;337361684@qq.com;Hi, [~lirui]  and [~jark] . Can you assign this issue to me, I can help to improve the doc. Thanks!;;;","06/Feb/24 03:17;lirui;Thanks [~337361684@qq.com], just assigned to you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC lookup joins fail with RDB column names containing colons,FLINK-34146,13565219,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,davidradl,davidradl,18/Jan/24 09:34,18/Jan/24 10:36,04/Jun/24 20:40,,1.18.1,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,"[https://github.com/apache/flink-connector-jdbc/pull/79] adds filter support for lookup joins. This was implemented using FieldNamedPreparedStatements in line with the way that the join key was implemented.   The [FieldNamedPreparedStatementImpl logic|https://github.com/apache/flink-connector-jdbc/blob/e3dd84160cd665ae17672da8b6e742e61a72a32d/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/statement/FieldNamedPreparedStatementImpl.java#L221] explicitly tests for the colon key and can incorrectly pickup column names.  So JDBC lookup joins fail with RDB column names containing colons when used in filters and lookup keys.

It looks like we have used the approach from [https://stackoverflow.com/questions/2309970/named-parameters-in-jdbc]. It says {{Please note that the above simple example does not handle using named parameter twice. Nor does it handle using the : sign inside quotes.}} It looks like we could play with some Regex Patterns to see if we can get one that works well for us.

 

A junit that shows the issue can be added to
FieldNamedPreparedStatementImplTest
 
...
private final String[] fieldNames2 =
new String[] \{""id?:"", ""name:?"", ""email"", ""ts"", ""field1"", ""field_2"", ""__field_3__""};
private final String[] keyFields2 = new String[] \{""id?:"", ""__field_3__""};
...
@Test
void testSelectStatementWithWeirdCharacters() {
String selectStmt = dialect.getSelectFromStatement(tableName, fieldNames2, keyFields2);
assertThat(selectStmt)
.isEqualTo(
""SELECT `id?:`, `name:?`, `email`, `ts`, `field1`, `field_2`, `__field_3__` FROM `tbl` ""
+ ""WHERE `id?:` = :id?: AND `__field_3__` = :__field_3__"");
NamedStatementMatcher.parsedSql(
""SELECT `id?:`, `name:?`, `email`, `ts`, `field1`, `field_2`, `__field_3__` FROM `tbl` ""
+ ""WHERE `id?:` = ? AND `__field_3__` = ?"")
.parameter(""id"", singletonList(1))
.parameter(""__field_3__"", singletonList(2))
.matches(selectStmt);
}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-18 09:34:16.0,,,,,,,,,,"0|z1mua0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File source connector support dynamic source parallelism inference in batch jobs,FLINK-34145,13565217,13560921,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,xiasun,xiasun,18/Jan/24 09:26,01/Feb/24 08:26,04/Jun/24 20:40,26/Jan/24 13:44,1.19.0,,,,,,,1.19.0,,,,Connectors / FileSystem,,,,0,pull-request-available,,,,"[FLIP-379|https://cwiki.apache.org/confluence/display/FLINK/FLIP-379%3A+Dynamic+source+parallelism+inference+for+batch+jobs] has introduced support for dynamic source parallelism inference in batch jobs, and we plan to give priority to enabling this feature for the file source connector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 13:44:32 UTC 2024,,,,,,,,,,"0|z1mu9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 13:44;zhuzh;master/release-1.19:
11631cb59568df60d40933fb13c8433062ed9290;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the documentation and configuration description about dynamic source parallelism inference,FLINK-34144,13565215,13560921,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,xiasun,xiasun,18/Jan/24 09:13,01/Feb/24 08:25,04/Jun/24 20:40,25/Jan/24 15:31,1.19.0,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,"[FLIP-379|https://cwiki.apache.org/confluence/display/FLINK/FLIP-379%3A+Dynamic+source+parallelism+inference+for+batch+jobs#FLIP379:Dynamicsourceparallelisminferenceforbatchjobs-IntroduceDynamicParallelismInferenceinterfaceforSource] introduces the new feature of dynamic source parallelism inference, and we plan to update the documentation and configuration items accordingly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 15:31:35 UTC 2024,,,,,,,,,,"0|z1mu94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 09:15;xiasun;[~zhuzh] please assign this ticket to me, thanks!;;;","19/Jan/24 01:54;zhuzh;[~xiasun] Assigned. Feel free to open a PR for it.;;;","25/Jan/24 15:31;zhuzh;master/release-1.19:
78e31f0dcc4da65d88e18560f3374a47cc0a7c9b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify the effective strategy of `execution.batch.adaptive.auto-parallelism.default-source-parallelism`,FLINK-34143,13565212,13560921,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,xiasun,xiasun,18/Jan/24 09:08,01/Feb/24 08:25,04/Jun/24 20:40,24/Jan/24 02:31,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"Currently, if users do not set the `{{{}execution.batch.adaptive.auto-parallelism.default-source-parallelism`{}}} configuration option, the AdaptiveBatchScheduler defaults to a parallelism of 1 for source vertices. In [FLIP-379|https://cwiki.apache.org/confluence/display/FLINK/FLIP-379%3A+Dynamic+source+parallelism+inference+for+batch+jobs#FLIP379:Dynamicsourceparallelisminferenceforbatchjobs-IntroduceDynamicParallelismInferenceinterfaceforSource], the value of `{{{}execution.batch.adaptive.auto-parallelism.default-source-parallelism`{}}} will act as the upper bound for inferring dynamic source parallelism, and continuing with the current policy is no longer appropriate.

We plan to change the effectiveness strategy of `{{{}execution.batch.adaptive.auto-parallelism.default-source-parallelism`{}}}; when the user does not set this config option, we will use the value of `{{{}execution.batch.adaptive.auto-parallelism.max-parallelism`{}}} as the upper bound for source parallelism inference. If {{`execution.batch.adaptive.auto-parallelism.max-parallelism`}} is also not configured, the value of `{{{}parallelism.default`{}}} will be used as a fallback.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 02:31:37 UTC 2024,,,,,,,,,,"0|z1mu8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 09:15;xiasun;[~zhuzh] please assign this ticket to me, thanks!;;;","19/Jan/24 01:54;zhuzh;[~xiasun] Assigned. Feel free to open a PR for it.;;;","24/Jan/24 02:31;zhuzh;master/release-1.19:
97caa3c251e416640a6f54ea103912839c346f70;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManager WorkingDirectory is not removed during shutdown ,FLINK-34142,13565211,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,prabhujoseph,prabhujoseph,18/Jan/24 08:52,18/Jan/24 08:52,04/Jun/24 20:40,,1.16.0,1.17.1,,,,,,,,,,Deployment / YARN,,,,0,,,,,"TaskManager WorkingDirectory is not removed during shutdown. 

*Repro*

 
{code:java}
1. Execute a Flink batch job within a Flink on YARN Session

flink-yarn-session -d

flink run -d /usr/lib/flink/examples/batch/WordCount.jar --input s3://prabhuflinks3/INPUT --output s3://prabhuflinks3/OUT

{code}
The batch job completes successfully, but the taskmanager working directory is not being removed.
{code:java}
[root@ip-1-2-3-4 container_1705470896818_0017_01_000002]# ls -R -lrt /mnt2/yarn/usercache/hadoop/appcache/application_1705470896818_0017/tm_container_1705470896818_0017_01_000002
/mnt2/yarn/usercache/hadoop/appcache/application_1705470896818_0017/tm_container_1705470896818_0017_01_000002:
total 0
drwxr-xr-x 2 yarn yarn  6 Jan 18 08:34 tmp
drwxr-xr-x 4 yarn yarn 66 Jan 18 08:34 blobStorage
drwxr-xr-x 2 yarn yarn  6 Jan 18 08:34 slotAllocationSnapshots
drwxr-xr-x 2 yarn yarn  6 Jan 18 08:34 localState

/mnt2/yarn/usercache/hadoop/appcache/application_1705470896818_0017/tm_container_1705470896818_0017_01_000002/tmp:
total 0

/mnt2/yarn/usercache/hadoop/appcache/application_1705470896818_0017/tm_container_1705470896818_0017_01_000002/blobStorage:
total 0
drwxr-xr-x 2 yarn yarn 94 Jan 18 08:34 job_d11f7085314ef1fb04c4e12fe292185a
drwxr-xr-x 2 yarn yarn  6 Jan 18 08:34 incoming

/mnt2/yarn/usercache/hadoop/appcache/application_1705470896818_0017/tm_container_1705470896818_0017_01_000002/blobStorage/job_d11f7085314ef1fb04c4e12fe292185a:
total 12
-rw-r--r-- 1 yarn yarn 10323 Jan 18 08:34 blob_p-cdd441a64b3ea6eed0058df02c6c10fd208c94a8-86d84864273dad1e8084d8ef0f5aad52

/mnt2/yarn/usercache/hadoop/appcache/application_1705470896818_0017/tm_container_1705470896818_0017_01_000002/blobStorage/incoming:
total 0

/mnt2/yarn/usercache/hadoop/appcache/application_1705470896818_0017/tm_container_1705470896818_0017_01_000002/slotAllocationSnapshots:
total 0

/mnt2/yarn/usercache/hadoop/appcache/application_1705470896818_0017/tm_container_1705470896818_0017_01_000002/localState:
total 0


{code}
*Analysis*

1. The TaskManagerRunner removes the working directory only when its 'close' method is called, which never happens.
{code:java}
    public void close() throws Exception {
        try {
            closeAsync().get();
        } catch (ExecutionException e) {
            ExceptionUtils.rethrowException(ExceptionUtils.stripExecutionException(e));
        }
    }

    public CompletableFuture<Result> closeAsync() {
        return closeAsync(Result.SUCCESS);
    }
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-18 08:52:51.0,,,,,,,,,,"0|z1mu88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bash exited with code '143',FLINK-34141,13565196,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,xuyangzhong,xuyangzhong,18/Jan/24 06:01,23/Jan/24 10:50,04/Jun/24 20:40,23/Jan/24 07:52,,,,,,,,,,,,Build System / CI,,,,0,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56544&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56503&view=logs&j=66645748-20ed-5f80-dbdf-bb5906c15462&t=e32bdfab-58bb-53ea-d411-d67a54d2939f

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34098,FLINK-34135,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 10:50:45 UTC 2024,,,,,,,,,,"0|z1mu4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 07:52;mapohl;Thanks for reporting the issues [~xuyangzhong]. We experienced CI infrastructure last week around Wednesday and Thursday. It's quite likely that you saw the instability due to these issues. They were resolved by Friday. Sorry for the inconvenience.;;;","23/Jan/24 10:50;xuyangzhong;Thanks a lot for your fix [~mapohl] :D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename WindowContext and TriggerContext in window,FLINK-34140,13565185,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,xuyangzhong,xuyangzhong,18/Jan/24 03:46,19/Jan/24 03:54,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"Currently, WindowContext and TriggerContext not only contains a series of get methods to obtain context information, but also includes behaviors such as clear.

Maybe it's better to rename them as WindowDelegator and TriggerDelegator or WindowHandler and TriggerHandler.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 11:37:15 UTC 2024,,,,,,,,,,"0|z1mu2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 08:12;martijnvisser;[~xuyangzhong] Are these internal renames, or changes to Public/PublicInternal/Experimental interfaces?;;;","18/Jan/24 11:37;xuyangzhong;Hi, [~martijnvisser] . All these classes are inner class, and no changes about Public/PublicInternal/Experimental interfaces.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The slice assigner should not reveal its event time or process time at the interface level.,FLINK-34139,13565182,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,xuyangzhong,xuyangzhong,18/Jan/24 03:08,18/Jan/24 03:47,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"Currently, there is a function `boolean isEventTime()` to tell other that it is by event time or process time. However, as an assigner, it should not expose this information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-18 03:08:53.0,,,,,,,,,,"0|z1mu1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the interface about MergeCallback in window,FLINK-34138,13565181,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,xuyangzhong,xuyangzhong,18/Jan/24 03:02,18/Jan/24 03:46,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"As a merge method, the return value type is `void`, that is confusing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-18 03:02:52.0,,,,,,,,,,"0|z1mu1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update CI to test archunit configuration,FLINK-34137,13565114,13565110,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,17/Jan/24 16:03,01/Mar/24 13:30,04/Jun/24 20:40,01/Mar/24 13:30,,,,,,,,,,,,Build System / CI,Connectors / Parent,,,0,pull-request-available,,,,"Update CI to test skiping archunit tests on non-main Flink versions. Test on submodules with archunit tests and without

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-17 16:03:38.0,,,,,,,,,,"0|z1mtmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execute archunit tests only with Flink version that connectors were built against,FLINK-34136,13565110,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,17/Jan/24 15:43,01/Mar/24 13:31,04/Jun/24 20:40,01/Mar/24 13:31,,,,,,,,,,,,Build System / CI,Connectors / Parent,,,0,,,,,"As part of [this discussion|https://lists.apache.org/thread/pr0g812olzpgz21d9oodhc46db9jpxo3] , the need for connectors to specify the main flink version that a connector supports has arisen. 

This CI variable will allow to configure the build and tests differently depending on this version. This parameter would be optional.

The first use case is to run archunit tests only on the main supported version as discussed in the above thread.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-17 15:43:12.0,,,,,,,,,,"0|z1mtls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A number of ci failures with Access to the path '.../_work/_temp/containerHandlerInvoker.js' is denied.,FLINK-34135,13565103,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jeyhunkarimov,Sergey Nuyanzin,Sergey Nuyanzin,17/Jan/24 14:34,25/Jan/24 13:14,04/Jun/24 20:40,23/Jan/24 20:55,,,,,,,,,,,,Build System / CI,,,,0,test-stability,,,,"There is a number of builds failing with something like 
{noformat}

##[error]Access to the path '/home/agent03/myagent/_work/_temp/containerHandlerInvoker.js' is denied.

{noformat}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56490&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=fb588352-ef18-568d-b447-699986250ccb
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56481&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=554d7c3f-d38e-55f4-96b4-ada3a9cb7d6f&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56481&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=1798d435-832b-51fe-a9ad-efb9abf4ab04&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56481&view=logs&j=a1ac4ce4-9a4f-5fdb-3290-7e163fba19dc&t=e4c57254-ec06-5788-3f8e-5ad5dffb418e&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56481&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=56881383-f398-5091-6b3b-22a7eeb7cfa8&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56481&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=2d9c27d0-8dbb-5be9-7271-453f74f48ab3&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56481&view=logs&j=162f98f7-8967-5f47-2782-a1e178ec2ad3&t=c9934c56-710d-5f85-d2b8-28ec1fd700ed&l=9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34098,,FLINK-34141,FLINK-34236,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 20:56:08 UTC 2024,,,,,,,,,,"0|z1mtk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 14:35;Sergey Nuyanzin;[~jingge]  since it seems to be related to Alibaba machines could you please help here or help with identifying a person who could help?;;;","17/Jan/24 14:49;mapohl;Could this be related to FLINK-34098 [~jeyhunkarimov] ?;;;","17/Jan/24 14:49;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56498&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=554d7c3f-d38e-55f4-96b4-ada3a9cb7d6f&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56500&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=645570fb-c588-5675-4122-f66ed42fb72f&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56500&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=554d7c3f-d38e-55f4-96b4-ada3a9cb7d6f&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56500&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=27d1d645-cbce-54e2-51c4-d8b45fe24607&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56500&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=645570fb-c588-5675-4122-f66ed42fb72f&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56500&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=554d7c3f-d38e-55f4-96b4-ada3a9cb7d6f&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56500&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=27d1d645-cbce-54e2-51c4-d8b45fe24607&l=9;;;","17/Jan/24 15:01;mapohl;Just as a fyi for the other release managers: [~lincoln] [~yunta]  [~martijnvisser] ;;;","17/Jan/24 15:07;jingge;[~Sergey Nuyanzin] [~mapohl] FYI: [~jeyhunkarimov] is working on it.;;;","17/Jan/24 16:32;jingge;[~Sergey Nuyanzin] it looks like most failed builds were running on AlibabaCI001, correct?;;;","17/Jan/24 16:42;mapohl;Yes, actually all the builds [~snuyanzin] shared were running on Alibaba001;;;","17/Jan/24 16:46;Sergey Nuyanzin;most of them yes
there is a couple of reproductions with *AlibabaCI006*
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56486&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602]
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56486&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=6e6da9c7-2448-523d-ca43-b6f326469c3d&l=10

Also *AlibabaCI002* and *AlibabaCI003* are failing with 
{noformat}
[error]Could not find a part of the path '/home/agent01/myagent/_work/_temp/containerHandlerInvoker.js'
{noformat}
*AlibabaCI002*
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56482&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=36f187fe-972b-5dcd-fbe7-74e193d0fc1f&l=9

*AlibabaCI003*
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56482&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=1f6c33d4-eb81-529f-4844-d14d67a2c6f7&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56482&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=048b100e-e87d-5e10-7bc7-0dd1c9aa5dd0&l=9
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56482&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=bc4c9170-c121-5244-cb07-eb2bb41ef63d&l=9;;;","18/Jan/24 12:56;mapohl;The runners are restarted according to [~jingge]. I'm gonna keep this one open until we have nightly builds again;;;","19/Jan/24 01:44;wangyang0918;The CI should work since the {{containerHandlerInvoker.js}} and other files are recreated after restarted the AZure agents.;;;","19/Jan/24 06:50;jingge;CI works again, I will close this ticket. [~mapohl] [~Sergey Nuyanzin] could you please double confirm?;;;","19/Jan/24 06:54;Sergey Nuyanzin;[~jingge] I'm not sure that this is working...
it looks working for PRs build
however I still see lots of failures for currently running nightlies
e.g. https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56601&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5;;;","19/Jan/24 11:39;jingge;[~Sergey Nuyanzin] thanks for letting us know. After checking the example build, it was e2e test failures which should have less to do with the CI machines. WDYT?;;;","19/Jan/24 12:07;Sergey Nuyanzin;The point is that there are several strange failures for nightlies

e.g. this fails to connect to github.com
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56603&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=7bed0b30-293b-5ef7-5cba-4b650e80eeea&l=33]
don't know whether it sis related or not

yeah from one side others seems to be failing/hanging on some tests...
however the point is that if we have a look at the latest nightly for master
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56601&view=results]

there are 18(!!) jobs are failed. It is not clear to me whether it is related to the infrastructure issue or something else...
However IMHO it should be considered as a blocker for the release at least need to see whether the amount of failed ci jobs will decrease or not with coming nightlies;;;","19/Jan/24 12:10;jingge;gotcha, thanks! Let's keep this ticket open for a few more days.;;;","22/Jan/24 08:53;martijnvisser;It looks to be stable again;;;","23/Jan/24 15:27;Sergey Nuyanzin;As it was discussed during sync meeting the priority  decreased;;;","23/Jan/24 17:05;mapohl;[~jeyhunkarimov] can't we close this issue now? Or is there still some work being done?;;;","23/Jan/24 20:56;jeyhunkarimov;Hi [~mapohl] yes CI seems stable again, we can close the issue;;;",,,,,,,,,,,,,,
Add tracing for restored state size and locations,FLINK-34134,13565094,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,srichter,srichter,srichter,17/Jan/24 13:36,14/Mar/24 07:36,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,Runtime / Task,,,,0,pull-request-available,,,,We can add tracing during the restore that reports the state size that was restored by location(s). This is particularly interesting for a mixed recovery with some local and some remote state.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33775,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-17 13:36:11.0,,,,,,,,,,"0|z1mti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge MiniBatchInterval when propagate traits to child block,FLINK-34133,13565081,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Leo Zhou,Leo Zhou,17/Jan/24 12:33,18/Jan/24 02:09,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,," 

we should merge MiniBatchInterval when propagate traits to child block, otherwise we may get wrong MiniBatchInterval in MinibatchAssigner. For example:

!image-2024-01-17-20-31-30-975.png!

!image-2024-01-17-20-34-30-039.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/24 02:07;Leo Zhou;StreamWindowSQLExample.java;https://issues.apache.org/jira/secure/attachment/13066118/StreamWindowSQLExample.java","17/Jan/24 12:31;Leo Zhou;image-2024-01-17-20-31-30-975.png;https://issues.apache.org/jira/secure/attachment/13066088/image-2024-01-17-20-31-30-975.png","17/Jan/24 12:34;Leo Zhou;image-2024-01-17-20-34-30-039.png;https://issues.apache.org/jira/secure/attachment/13066089/image-2024-01-17-20-34-30-039.png","18/Jan/24 02:08;Leo Zhou;plan.txt;https://issues.apache.org/jira/secure/attachment/13066119/plan.txt",,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 02:09:15 UTC 2024,,,,,,,,,,"0|z1mtfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 01:38;xuyangzhong;Hi, [~Leo Zhou] can you attach the code and plan as text in the `code` block? I'll try to re-produce this bug in my local environment.;;;","18/Jan/24 02:09;Leo Zhou;Hi [~xuyangzhong] , I have put the code and plan in the attachments area.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch WordCount job fails when run with AdaptiveBatch scheduler,FLINK-34132,13565058,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,prabhujoseph,prabhujoseph,17/Jan/24 09:59,01/Feb/24 15:36,04/Jun/24 20:40,01/Feb/24 08:03,1.17.1,1.18.1,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,"Batch WordCount job fails when run with AdaptiveBatch scheduler.

*Repro Steps*
{code:java}
flink-yarn-session -Djobmanager.scheduler=adaptive -d

 flink run -d /usr/lib/flink/examples/batch/WordCount.jar --input s3://prabhuflinks3/INPUT --output s3://prabhuflinks3/OUT
{code}
*Error logs*
{code:java}
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
	at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1067)
	at org.apache.flink.client.program.ContextEnvironment.executeAsync(ContextEnvironment.java:144)
	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:73)
	at org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:106)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
	... 12 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.api.java.ExecutionEnvironment.executeAsync(ExecutionEnvironment.java:1062)
	... 20 more
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:75)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: At the moment, adaptive batch scheduler requires batch workloads to be executed with types of all edges being BLOCKING or HYBRID_FULL/HYBRID_SELECTIVE. To do that, you need to configure 'execution.batch-shuffle-mode' to 'ALL_EXCHANGES_BLOCKING' or 'ALL_EXCHANGES_HYBRID_FULL/ALL_EXCHANGES_HYBRID_SELECTIVE'.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
	... 3 more
Caused by: java.lang.IllegalStateException: At the moment, adaptive batch scheduler requires batch workloads to be executed with types of all edges being BLOCKING or HYBRID_FULL/HYBRID_SELECTIVE. To do that, you need to configure 'execution.batch-shuffle-mode' to 'ALL_EXCHANGES_BLOCKING' or 'ALL_EXCHANGES_HYBRID_FULL/ALL_EXCHANGES_HYBRID_SELECTIVE'.
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.checkAllExchangesAreSupported(AdaptiveBatchSchedulerFactory.java:324)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchSchedulerFactory.createInstance(AdaptiveBatchSchedulerFactory.java:127)
	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:124)
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:393)
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:362)
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:128)
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:100)
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	... 3 more

{code}
*Analysis*

I have configured the execution.batch-shuffle-mode to use either ALL_EXCHANGES_BLOCKING, ALL_EXCHANGES_HYBRID_FULL, or ALL_EXCHANGES_HYBRID_SELECTIVE, but all attempts resulted in the same error message.

The Wordcount program runs fine when setting below in the code
{code:java}
env.getConfig().setExecutionMode(ExecutionMode.BATCH_FORCED);
{code}
Need to investigate why the execution.batch-shuffle-mode is not being recognized and, if this behavior is intentional, correct the reported misleading error message. Additionally, we need to address the Wordcount job to ensure it runs seamlessly with both batch and adaptive scheduler.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 15:36:17 UTC 2024,,,,,,,,,,"0|z1mta8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 11:25;wanglijie;[~prabhujoseph] AFAIK, adaptive batch scheduler does not support dataset jobs, you can see details in [FLIP-283|https://cwiki.apache.org/confluence/display/FLINK/FLIP-283%3A+Use+adaptive+batch+scheduler+as+default+scheduler+for+batch+jobs].

But maybe we should make the exception message more friendly. cc [~JunRuiLi];;;","17/Jan/24 11:38;JunRuiLi;[~prabhujoseph] The [limitation|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/#limitations-2] of adaptive batch scheduler requires that only supports jobs whose shuffle mode is {{{}ALL_EXCHANGES_BLOCKING / ALL_EXCHANGES_HYBRID_FULL / ALL_EXCHANGES_HYBRID_SELECTIVE{}}}.

However, this config option execution.batch-shuffle-mode is applicable to DataStream jobs but not to DataSet jobs. For DataSet jobs, ExecutionMode.BATCH_FORCED could be considered a workaround (not very certain). 

But I think the doc on the website is not very clear on this limitation and could potentially confuse users. It's better to update the documentation to clarify this point. 

[~wanglijie] , WDYT?
 ;;;","17/Jan/24 12:18;JunRuiLi;After discuss with [~wanglijie] offline, I will prepare a fix to update the error message and documentation on the website.;;;","01/Feb/24 08:03;zhuzh;The documentation is updated via dd3e60a4b1e473c167837b7c3bc4fb90c0a1f51a;;;","01/Feb/24 08:12;zhuzh;We should also migrate the existing batch examples from DataSet to DataStream so that it can directly work with AdaptiveBatchScheduler. 
This work needs to be done before removing the DataSet API in Flink 2.0.
cc [~Wencong Liu];;;","01/Feb/24 15:36;Wencong Liu;Thanks for the reminding. [~zhuzh] I will address these issues when I have some free time. 😄;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint check window should take in account checkpoint job configuration,FLINK-34131,13565057,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,17/Jan/24 09:59,21/Jan/24 19:59,04/Jun/24 20:40,21/Jan/24 19:59,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"When enabling checkpoint progress check (kubernetes.operator.cluster.health-check.checkpoint-progress.enabled) to define cluster health the operator rely detect if a checkpoint has been performed during the kubernetes.operator.cluster.health-check.checkpoint-progress.window

As indicated in the doc it must be bigger to checkpointing interval.

But this is a manual configuration which can leads to misconfiguration and unwanted restart of the flink cluster if the checkpointing interval is bigger than the window one.

The operator must check that the config is healthy before to rely on this check. If it is not well set it should not execute the check (return true on [evaluateCheckpoints|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/observer/ClusterHealthEvaluator.java#L197C1-L199C50]) and log a WARN message.

Also flink jobs have other checkpointing parameters that should be taken in account for this window configuration which are execution.checkpointing.timeout and execution.checkpointing.tolerable-failed-checkpoints

The idea would be to check that kubernetes.operator.cluster.health-check.checkpoint-progress.window >= max(execution.checkpointing.interval * execution.checkpointing.tolerable-failed-checkpoints, execution.checkpointing.timeout * execution.checkpointing.tolerable-failed-checkpoints)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 21 19:59:43 UTC 2024,,,,,,,,,,"0|z1mta0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/24 19:59;gyfora;merged to main 775bc5f41df09accefca6590112509c6023b2fb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Mark setBytes and getBytes of Configuration as @Internal in Flink 2.0,FLINK-34130,13565045,13564656,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,17/Jan/24 09:04,19/Jan/24 10:09,04/Jun/24 20:40,,,,,,,,,2.0.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-17 09:04:26.0,,,,,,,,,,"0|z1mt7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniBatchGlobalGroupAggFunction will make -D as +I then make +I as -U when state expired ,FLINK-34129,13565041,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,loserwang1024,loserwang1024,17/Jan/24 08:50,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.1,,,,,,,1.20.0,,,,Table SQL / Runtime,,,,0,,,,,"Take sum for example:
When state is expired, then an update operation from source happens. MiniBatchGlobalGroupAggFunction take -U[1, 20] and +U[1, 20] as input, but will emit +I[1, -20] and -D[1, -20]. The sink will detele the data from external database.

Let's see why this will happens:
 * when state is expired and -U[1, 20] arrive, MiniBatchGlobalGroupAggFunction will create a new sum accumulator and set firstRow as true.

{code:java}
if (stateAcc == null) { 
    stateAcc = globalAgg.createAccumulators(); 
    firstRow = true; 
}   {code}
 * then sum accumulator will retract sum value as -20
 * As the first row, MiniBatchGlobalGroupAggFunction will change -U as +I, then emit to downstream.

{code:java}
if (!recordCounter.recordCountIsZero(acc)) {
   // if this was not the first row and we have to emit retractions
    if (!firstRow) {
       // ignore
    } else {
    // update acc to state
    accState.update(acc);
 
   // this is the first, output new result
   // prepare INSERT message for new row
   resultRow.replace(currentKey, newAggValue).setRowKind(RowKind.INSERT);
   out.collect(resultRow);
}  {code}
 * when next +U[1, 20] arrives, sum accumulator will retract sum value as 0, so RetractionRecordCounter#recordCountIsZero will return true. Because firstRow = false now, will change the +U as -D, then emit to downtream.

{code:java}
if (!recordCounter.recordCountIsZero(acc)) {
    // ignode
}else{
   // we retracted the last record for this key
   // if this is not first row sent out a DELETE message
   if (!firstRow) {
   // prepare DELETE message for previous row
   resultRow.replace(currentKey, prevAggValue).setRowKind(RowKind.DELETE);
   out.collect(resultRow);
} {code}
 
So the sink will receiver +I and -D after a source update operation, the data will be delete.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 09 23:38:00 UTC 2024,,,,,,,,,,"0|z1mt6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 05:48;loserwang1024;[~fsk119] ,[~lsy] , [~andrewlincong@gmail.com] , CC;;;","26/Jan/24 08:49;xuyangzhong;If the upstream CDC data is incomplete, such as receiving UB before UA, and there is no I data before UB, taking SUM as an example, the global group aggregation will first emit (+I, key, -1), then emit (-D, key, -1). This leads to the final data being deleted.

This bug can be reproduced by adding the code in `TableSinkITCase` below.
{code:java}
@TestTemplate
def test(): Unit = {
  val tableConfig = tEnv.getConfig
  tableConfig.set(TABLE_EXEC_MINIBATCH_ENABLED, Boolean.box(true))
  tableConfig.set(TABLE_EXEC_MINIBATCH_ALLOW_LATENCY, Duration.ofSeconds(1))
  tableConfig.set(TABLE_EXEC_MINIBATCH_SIZE, Long.box(1L))

  val userDataId = TestValuesTableFactory.registerData(
    Seq(
      changelogRow(""-U"", ""k1"", new JLong(1L)),
      changelogRow(""+U"", ""k1"", new JLong(1L))
    ))
  tEnv.executeSql(s""""""
                     |CREATE TABLE TT (
                     |  a STRING,
                     |  b BIGINT
                     |) WITH (
                     |  'connector' = 'values',
                     |  'bounded' = 'false',
                     |  'changelog-mode' = 'I,UA,UB,D',
                     |  'data-id' = '$userDataId'
                     |)
                     |"""""".stripMargin)

  val sql =
    """"""
      |SELECT a, SUM(b)
      |FROM TT GROUP BY a
    """""".stripMargin
  tEnv.executeSql(sql).print
} {code}
The return values are:
{code:java}
+----+--------------------------------+----------------------+
| op |                              a |               EXPR$1 |
+----+--------------------------------+----------------------+
| +I |                             k1 |                   -1 |
| -D |                             k1 |                   -1 |
+----+--------------------------------+----------------------+
2 rows in set {code}
 ;;;","09/Feb/24 23:38;jeyhunkarimov;Hi [~loserwang1024],  [~xuyangzhong] I am not sure if it a bug or expected behaviour in local-global aggregation. 

Partitioned aggregates (see {{GroupAggFunction::processElement}}) solve the above-mentioned issue by tracking the {{firstRow}} and avoid sending the first row to {{retract}} function. In this case, since the state partitioned and there is only one operator instance responsible for the partition, we can avoid the above mentioned behaviour. 

In the presence of local-global aggregates,  however:
- it is difficult to prevent the above-mentioned behaviour in {{LocalGroupAggFunction}} instances, since there can be multiple of {{LocalGroupAggFunction}} instances, and there is no ordering among them ( to track {{firstRow}} and to avoid it being retracted)
- it is difficult to prefent the above-mentioned behaviour in {{GlobalGroupAggFunction}} instances, since it already receives pre-aggregated data. 

Currently, the only way to avoid this behavior is to either

- Use the {{firstRow}} tracking (similar to {{GroupAggFunction::processElement}}) in {{LocalGroupAggFunction}} AND use parallelism 1
- Use the partitioned aggregates;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"oracle jdbc connector bug. When the oracle table field is of type float, the type obtained by jdbc is bigdecimal. error is  'java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.Float'",FLINK-34128,13565020,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,blackpighe,blackpighe,17/Jan/24 06:40,19/Jan/24 10:46,04/Jun/24 20:40,19/Jan/24 10:46,jdbc-3.0.0,jdbc-3.1.0,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,,"create oracle table contain float field and execute sql occurred error.

java.lang.ClassCastException: java.math.BigDecimal cannot be cast to java.lang.Float

 

Locate the cause of the error:

org/apache/flink/connector/jdbc/converter/AbstractJdbcRowConverter.java

!image-2024-01-17-14-33-05-713.png!

Object field = resultSet.getObject(pos + 1);

this method for oracle jdbc produce bug. 

expect：float value

actual： bigdecimal value

 

Suggest this modification：

Object field =
resultSet.getObject(pos + 1, rowType.getTypeAt(pos).getDefaultConversion());

Specify the type explicitly according to the schema.but mock test case is error.Let's talk about it and see what we can do about it.

 

 

 ",All current versions of flinks-jdbc-connector,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/24 06:33;blackpighe;image-2024-01-17-14-33-05-713.png;https://issues.apache.org/jira/secure/attachment/13066077/image-2024-01-17-14-33-05-713.png","17/Jan/24 14:18;blackpighe;image-2024-01-17-22-18-27-781.png;https://issues.apache.org/jira/secure/attachment/13066095/image-2024-01-17-22-18-27-781.png",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 14:19:55 UTC 2024,,,,,,,,,,"0|z1mt1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 07:47;blackpighe;https://github.com/apache/flink-connector-jdbc/pull/91;;;","17/Jan/24 11:11;blackpighe;[~martijnvisser] this test case is not pass，This change caused flink's other test cases to fail, which was really hard;;;","17/Jan/24 14:19;blackpighe;!image-2024-01-17-22-18-27-781.png!

update this commit，will reduce the degree of change;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka connector repo runs a duplicate of `IntegrationTests` framework tests,FLINK-34127,13565011,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masc,mason6345,mason6345,17/Jan/24 05:58,26/Apr/24 02:16,04/Jun/24 20:40,,kafka-3.0.2,,,,,,,,,,,Build System / CI,Connectors / Kafka,,,0,pull-request-available,,,,"I found out this behavior when troubleshooting CI flakiness. These integration tests make heavy use of the CI since they require Kafka, Zookeeper, and Docker containers. We can further stablize CI by not redundantly running these set of tests.


`grep -E "".*testIdleReader\[TestEnvironment.*"" 14_Compile\ and\ test.txt` returns:

```

2024-01-17T00:51:05.2943150Z Test org.apache.flink.connector.kafka.dynamic.source.DynamicKafkaSourceITTest.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [org.apache.flink.connector.kafka.testutils.DynamicKafkaSourceExternalContext@43e9a8a2], Semantic: [EXACTLY_ONCE]] is running.
2024-01-17T00:51:07.6922535Z Test org.apache.flink.connector.kafka.dynamic.source.DynamicKafkaSourceITTest.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [org.apache.flink.connector.kafka.testutils.DynamicKafkaSourceExternalContext@43e9a8a2], Semantic: [EXACTLY_ONCE]] successfully run.
2024-01-17T00:56:27.1326332Z Test org.apache.flink.connector.kafka.dynamic.source.DynamicKafkaSourceITTest.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [org.apache.flink.connector.kafka.testutils.DynamicKafkaSourceExternalContext@2db4a84a], Semantic: [EXACTLY_ONCE]] is running.
2024-01-17T00:56:28.4000830Z Test org.apache.flink.connector.kafka.dynamic.source.DynamicKafkaSourceITTest.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [org.apache.flink.connector.kafka.testutils.DynamicKafkaSourceExternalContext@2db4a84a], Semantic: [EXACTLY_ONCE]] successfully run.
2024-01-17T00:56:58.7830792Z Test org.apache.flink.connector.kafka.source.KafkaSourceITCase.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [KafkaSource-PARTITION], Semantic: [EXACTLY_ONCE]] is running.
2024-01-17T00:56:59.0544092Z Test org.apache.flink.connector.kafka.source.KafkaSourceITCase.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [KafkaSource-PARTITION], Semantic: [EXACTLY_ONCE]] successfully run.
2024-01-17T00:56:59.3910987Z Test org.apache.flink.connector.kafka.source.KafkaSourceITCase.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [KafkaSource-TOPIC], Semantic: [EXACTLY_ONCE]] is running.
2024-01-17T00:56:59.6025298Z Test org.apache.flink.connector.kafka.source.KafkaSourceITCase.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [KafkaSource-TOPIC], Semantic: [EXACTLY_ONCE]] successfully run.
2024-01-17T00:57:37.8378640Z Test org.apache.flink.connector.kafka.source.KafkaSourceITCase.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [KafkaSource-PARTITION], Semantic: [EXACTLY_ONCE]] is running.
2024-01-17T00:57:38.0144732Z Test org.apache.flink.connector.kafka.source.KafkaSourceITCase.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [KafkaSource-PARTITION], Semantic: [EXACTLY_ONCE]] successfully run.
2024-01-17T00:57:38.2004796Z Test org.apache.flink.connector.kafka.source.KafkaSourceITCase.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [KafkaSource-TOPIC], Semantic: [EXACTLY_ONCE]] is running.
2024-01-17T00:57:38.4072815Z Test org.apache.flink.connector.kafka.source.KafkaSourceITCase.IntegrationTests.testIdleReader[TestEnvironment: [MiniCluster], ExternalContext: [KafkaSource-TOPIC], Semantic: [EXACTLY_ONCE]] successfully run.
2024-01-17T01:06:11.2933375Z Test org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.testIdleReader[TestEnvironment: [FlinkContainers], ExternalContext: [KafkaSource-PARTITION], Semantic: [EXACTLY_ONCE]] is running.
2024-01-17T01:06:12.1790031Z Test org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.testIdleReader[TestEnvironment: [FlinkContainers], ExternalContext: [KafkaSource-PARTITION], Semantic: [EXACTLY_ONCE]] successfully run.
2024-01-17T01:06:12.5703927Z Test org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.testIdleReader[TestEnvironment: [FlinkContainers], ExternalContext: [KafkaSource-TOPIC], Semantic: [EXACTLY_ONCE]] is running.
2024-01-17T01:06:13.3369574Z Test org.apache.flink.tests.util.kafka.KafkaSourceE2ECase.testIdleReader[TestEnvironment: [FlinkContainers], ExternalContext: [KafkaSource-TOPIC], Semantic: [EXACTLY_ONCE]] successfully run.

```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 06:41:46 UTC 2024,,,,,,,,,,"0|z1mszs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 06:00;mason6345;I found this is an instance of a 50 minute, timed out CI run. This causes network instability which causes a KafkaConsumer to infinitely reset the offset:

 

```

2024-01-17T00:56:08.7826779Z 00:56:08,659 [Source Data Fetcher for Source: Tested Source (3/5)#0] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState [] - [Consumer clientId=DynamicKafkaSourceExternalContext-cluster0-2, groupId=DynamicKafkaSourceExternalContext] Seeking to earliest offset of partition topic1-3301243828175368455-0

```

 

This log is repeated over 10 times through the log and the thread dump confirms that this test's thread was stuck waiting for the test to complete.;;;","11/Apr/24 12:25;martijnvisser;[~mason6345] Is this something that you can check? I think I also see some flakyness here, like with https://github.com/apache/flink-connector-kafka/actions/runs/8646160813/job/23704885473;;;","12/Apr/24 00:50;mason6345;Sure. Can I assign myself now with committer permissions? :D;;;","12/Apr/24 09:25;martijnvisser;You should :);;;","17/Apr/24 06:41;masc;It seems to be a bug in junit. https://github.com/junit-team/junit5/issues/3782;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the description of jobmanager.scheduler,FLINK-34126,13565009,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,17/Jan/24 05:50,01/Feb/24 07:57,04/Jun/24 20:40,01/Feb/24 07:57,,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,"Now the config option jobmanager.scheduler has description: 

_Determines which scheduler implementation is used to schedule tasks. Accepted values are:_
 * _'Default': Default scheduler_
 * _'Adaptive': Adaptive scheduler. More details can be found [here|https://nightlies.apache.org/flink/flink-docs-master/zh/docs/deployment/elastic_scaling#adaptive-scheduler]._
 * _'AdaptiveBatch': Adaptive batch scheduler. More details can be found [here|https://nightlies.apache.org/flink/flink-docs-master/zh/docs/deployment/elastic_scaling#adaptive-batch-scheduler]._

_Possible values:_
 * _""Default""_
 * _""Adaptive""_
 * _""AdaptiveBatch""_

 

However, after FLIP-283 we changed the default scheduler for batch job to AdaptiveBatchScheduler. This config option description will mislead users that the 'DefaultScheduler' is the universal fallback for both batch and streaming jobs.

We should update this description.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 07:57:06 UTC 2024,,,,,,,,,,"0|z1mszc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 07:57;zhuzh;Fixed via 2be1ea801cf616d0d0a82729829245c205caaad8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 2.0: Remove deprecated serialization config methods and options,FLINK-34125,13565001,13564005,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,17/Jan/24 03:56,17/Jan/24 05:28,04/Jun/24 20:40,,,,,,,,,,,,,API / Type Serialization System,Runtime / Configuration,,,0,2.0-related,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-17 03:56:10.0,,,,,,,,,,"0|z1msxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 2.0: Disable Kyro by default,FLINK-34124,13565000,13564005,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,17/Jan/24 03:55,17/Jan/24 05:27,04/Jun/24 20:40,,,,,,,,,,,,,API / Type Serialization System,Runtime / Configuration,,,0,2.0-related,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-17 03:55:28.0,,,,,,,,,,"0|z1msxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce built-in serialization support for Map and List,FLINK-34123,13564999,13564005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,17/Jan/24 03:50,27/May/24 02:02,04/Jun/24 20:40,27/May/24 02:02,1.20.0,,,,,,,1.20.0,,,,API / Type Serialization System,,,,0,pull-request-available,,,,"Introduce built-in serialization support for Map and List, two common collection types for which Flink already have custom serializers implemented.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 02:02:56 UTC 2024,,,,,,,,,,"0|z1msx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/24 11:10;Zhanghao Chen;[~zjureel] May I take this? I'll start working on it after code freeze ends.;;;","27/May/24 02:02;Weijie Guo;master(1.20) via f860631c523c1d446c0d01046f0fbe6055174dc6.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate old serialization config methods and options,FLINK-34122,13564993,13564005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,17/Jan/24 03:38,02/Feb/24 15:04,04/Jun/24 20:40,27/Jan/24 12:32,1.19.0,,,,,,,1.19.0,,,,API / Type Serialization System,Runtime / Configuration,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 15:04:10 UTC 2024,,,,,,,,,,"0|z1msvs:",9223372036854775807,"Configuring serialization behavior through hard codes is deprecated, because you need to modify the codes when upgrading job version. You should configure this via options pipeline.serialization-config, pipeline.force-avro, pipeline.force-kryo, and pipeline.generic-types. Registration of instance-level serializer is deprecated, using class-level serializer instead.",,,,,,,,,,,,,,,,,,,"27/Jan/24 12:32;Weijie Guo;master via 2e56caf11c0ebe4461e6389b7d5c1a9ea2ff621f;;;","30/Jan/24 08:43;martijnvisser;[~Zhanghao Chen] Can you please include in the release notes information on what's deprecated, and what users should be using?;;;","30/Jan/24 12:55;Zhanghao Chen;[~martijnvisser] Sure, shall I put the release notes info in this subtask or in the parent task?;;;","02/Feb/24 15:04;martijnvisser;Whatever works for you! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce pipeline.force-kryo-avro to control whether to force registration of Avro serializer with Kryo,FLINK-34121,13564992,13564005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,Zhanghao Chen,Zhanghao Chen,17/Jan/24 03:36,19/Mar/24 05:21,04/Jun/24 20:40,19/Mar/24 05:21,1.19.0,,,,,,,1.20.0,,,,API / Type Serialization System,Runtime / Configuration,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 05:21:21 UTC 2024,,,,,,,,,,"0|z1msvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 05:21;Weijie Guo;master: dd382894b4da5b2d153913c92b6679fbd877b18b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Introduce unified serialization config option for all Kryo, POJO and customized serializers",FLINK-34120,13564990,13564005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,17/Jan/24 03:23,25/Jan/24 02:53,04/Jun/24 20:40,25/Jan/24 02:53,1.19.0,,,,,,,1.19.0,,,,API / Type Serialization System,Runtime / Configuration,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 02:53:15 UTC 2024,,,,,,,,,,"0|z1msv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 02:53;Weijie Guo;master(1.19) via 806970c4b4712f9bf1e938c53040f19023d37ad2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve description about changelog in document,FLINK-34119,13564989,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,17/Jan/24 02:31,18/Jan/24 02:24,04/Jun/24 20:40,18/Jan/24 02:24,,,,,,,,1.19.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"Since we have resolved some issues and marked as prodution-ready in [release note|https://flink.apache.org/2022/10/28/announcing-the-release-of-apache-flink-1.16/#generalized-incremental-checkpoint],

we could update some description about it in doc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 02:24:27 UTC 2024,,,,,,,,,,"0|z1msuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 02:24;masteryhx;merged 2ec8f81 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Sort node,FLINK-34118,13564987,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,17/Jan/24 01:57,23/Feb/24 13:00,04/Jun/24 20:40,23/Feb/24 13:00,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 23 13:00:45 UTC 2024,,,,,,,,,,"0|z1msug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/24 13:00;dwysakowicz;Implemented in fe3d9a42995cfee0dfd90e8031768cb130543189..faacf7e28bd9a43723303d0bd4a6ee9adebcb5bb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactCoordinator for table file sink loses data upon job termination,FLINK-34117,13564950,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,16/Jan/24 20:34,26/Jan/24 09:40,04/Jun/24 20:40,26/Jan/24 09:40,1.18.1,,,,,,,1.18.2,1.19.0,,,Connectors / FileSystem,,,,0,pull-request-available,,,,"CompactCoordinator accumulates data in currentInputFiles and only rolls them into inputFiles in snapshotState(). At the same time it relies on separately receiving checkpoint indications from the upstream operator via processElement() (EndCheckpoint). If the job terminates, the final EndCheckpoint can arrive before the snapshotState() gets called. This leads to data loss (all events in currentInputFiles get discarded).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 09:40:19 UTC 2024,,,,,,,,,,"0|z1msm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 09:09;leonard;master: 1d0f34e5a4a9127724bd09ecd0d9c0c0dba431f3
1.18: TODO [~afedulov]  would you like to backport this fix to 1.18 branch ?;;;","25/Jan/24 11:37;afedulov;[~leonard] thanks! Here is the backport PR https://github.com/apache/flink/pull/24195;;;","26/Jan/24 09:40;leonard;Fixed via 
master: 1d0f34e5a4a9127724bd09ecd0d9c0c0dba431f3
1.18: f287fb8f3c89cad1db028d8119c748b1c7b2194b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlobalConfigurationTest.testInvalidStandardYamlFile fails,FLINK-34116,13564876,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,mapohl,mapohl,16/Jan/24 14:33,17/Jan/24 03:20,04/Jun/24 20:40,17/Jan/24 03:20,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,test-stability,,,"All build failures in the same build:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56416&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=6274]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56416&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=6256]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56416&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=6176]

{code:java}
Jan 16 01:38:10 01:38:10.780 [ERROR] Failures: 
Jan 16 01:38:10 01:38:10.781 [ERROR]   GlobalConfigurationTest.testInvalidStandardYamlFile:200 
Jan 16 01:38:10 Multiple Failures (1 failure)
Jan 16 01:38:10 -- failure 1 --
Jan 16 01:38:10 Expecting actual:
Jan 16 01:38:10   ""java.lang.RuntimeException: Error parsing YAML configuration.
Jan 16 01:38:10 	at org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource(GlobalConfiguration.java:351)
Jan 16 01:38:10 	at org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(GlobalConfiguration.java:162)
Jan 16 01:38:10 	at org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(GlobalConfiguration.java:115)
Jan 16 01:38:10 	at org.apache.flink.configuration.GlobalConfigurationTest.lambda$testInvalidStandardYamlFile$3(GlobalConfigurationTest.java:198)
Jan 16 01:38:10 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
Jan 16 01:38:10 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
Jan 16 01:38:10 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
Jan 16 01:38:10 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
Jan 16 01:38:10 	at org.apache.flink.configuration.GlobalConfigurationTest.testInvalidStandardYamlFile(GlobalConfigurationTest.java:198)
Jan 16 01:38:10 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 16 01:38:10 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 16 01:38:10 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 16 01:38:10 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 03:20:48 UTC 2024,,,,,,,,,,"0|z1ms5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 14:40;mapohl;[~JunRuiLi] can you have a look. It looks like you committed config-related things here related to FLINK-33297;;;","16/Jan/24 16:40;JunRuiLi;[~mapohl] Thank you for the kind reminder, I will take a look.;;;","17/Jan/24 02:08;JunRuiLi;This bug arose due to discrepancies in {{ClassCastException}} error messages between different JDK versions, leading to test failures when assertions depended on specific error text.

I will fix it soon.;;;","17/Jan/24 03:20;zhuzh;Fixed via
cdf314d30b59994283e0bbf70f350618de02118c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableAggregateITCase.testFlagAggregateWithOrWithoutIncrementalUpdate fails,FLINK-34115,13564874,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,mapohl,mapohl,16/Jan/24 14:19,07/Feb/24 12:04,04/Jun/24 20:40,07/Feb/24 05:58,1.18.0,1.19.0,,,,,,1.18.2,1.19.0,,,Table SQL / Runtime,,,,0,pull-request-available,test-stability,,,"It failed twice in the same pipeline run:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56348&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=11613]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56348&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11963]

{code:java}
 Jan 14 01:20:01 01:20:01.949 [ERROR] Tests run: 18, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 29.07 s <<< FAILURE! -- in org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase
Jan 14 01:20:01 01:20:01.949 [ERROR] org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase.testFlagAggregateWithOrWithoutIncrementalUpdate -- Time elapsed: 0.518 s <<< FAILURE!
Jan 14 01:20:01 org.opentest4j.AssertionFailedError: 
Jan 14 01:20:01 
Jan 14 01:20:01 expected: List((true,6,1), (false,6,1), (true,6,1), (true,3,2), (false,6,1), (false,3,2), (true,6,1), (true,5,2), (false,6,1), (false,5,2), (true,8,1), (true,6,2), (false,8,1), (false,6,2), (true,8,1), (true,6,2))
Jan 14 01:20:01  but was: List((true,3,1), (false,3,1), (true,5,1), (true,3,2), (false,5,1), (false,3,2), (true,8,1), (true,5,2), (false,8,1), (false,5,2), (true,8,1), (true,5,2), (false,8,1), (false,5,2), (true,8,1), (true,6,2))
Jan 14 01:20:01 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Jan 14 01:20:01 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Jan 14 01:20:01 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Jan 14 01:20:01 	at org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase.checkRank$1(TableAggregateITCase.scala:122)
Jan 14 01:20:01 	at org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase.testFlagAggregateWithOrWithoutIncrementalUpdate(TableAggregateITCase.scala:69)
Jan 14 01:20:01 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 14 01:20:01 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Jan 14 01:20:01 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Jan 14 01:20:01 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
Jan 14 01:20:01 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Jan 14 01:20:01 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Jan 14 01:20:01 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Jan 14 01:20:01 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Jan 14 01:20:01 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Jan 14 01:20:01 	at scala.collection.convert.Wrappers$IteratorWrapper.forEachRemaining(Wrappers.scala:26)
Jan 14 01:20:01 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Jan 14 01:20:01 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Jan 14 01:20:01 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Jan 14 01:20:01 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Jan 14 01:20:01 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Jan 14 01:20:01 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Jan 14 01:20:01 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Jan 14 01:20:01 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
Jan 14 01:20:01 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
Jan 14 01:20:01 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Jan 14 01:20:01 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Jan 14 01:20:01 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Jan 14 01:20:01 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Jan 14 01:20:01 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Jan 14 01:20:01 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Jan 14 01:20:01 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Jan 14 01:20:01 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Jan 14 01:20:01 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Jan 14 01:20:01 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Jan 14 01:20:01 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 12:04:12 UTC 2024,,,,,,,,,,"0|z1ms5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 14:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56359&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12065;;;","16/Jan/24 14:26;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56360&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11744;;;","16/Jan/24 14:52;mapohl;The test was introduced with FLINK-31788. [~qingyue] can you have a look into it?;;;","17/Jan/24 01:58;qingyue;Hi [~mapohl], sorry for the late reply.

The case seems unstable, and it failed on the second run.  I'll take a look right now.
{code:java}
2024-01-14T04:08:09.8492919Z Jan 14 04:08:09 04:08:09.848 [ERROR] Failures: 
2024-01-14T04:08:09.8518261Z Jan 14 04:08:09 04:08:09.851 [ERROR] org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase.testFlagAggregateWithOrWithoutIncrementalUpdate
2024-01-14T04:08:09.8537141Z Jan 14 04:08:09 04:08:09.851 [INFO]   Run 1: PASS
2024-01-14T04:08:09.8538525Z Jan 14 04:08:09 04:08:09.851 [ERROR]   Run 2: TableAggregateITCase.testFlagAggregateWithOrWithoutIncrementalUpdate:95->checkRank$1:122 
2024-01-14T04:08:09.8539189Z Jan 14 04:08:09 expected: List((true,6,1), (true,3,2), (false,3,2), (true,5,2), (false,6,1), (true,8,1), (false,5,2), (true,6,2))
2024-01-14T04:08:09.8539723Z Jan 14 04:08:09  but was: List((true,4,1), (false,4,1), (true,6,1), (true,4,2), (false,4,2), (true,5,2), (false,6,1), (true,8,1), (false,5,2), (true,6,2)) {code}
 

 ;;;","17/Jan/24 07:49;mapohl;I wouldn't consider it a late reply. :D Thanks for checking it. (y);;;","17/Jan/24 07:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56435&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11573;;;","22/Jan/24 15:36;mapohl;Same 1.18 build:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56680&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11620]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56680&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11748];;;","22/Jan/24 15:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56679&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11962;;;","22/Jan/24 15:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56659&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11746;;;","22/Jan/24 15:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56657&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11962;;;","23/Jan/24 07:18;qingyue;Sorry for the late update.

I attempted to reproduce the failure, but I have been unsuccessful in replicating the issue in the local environment. However, upon inspecting the table's maven log, I did identify some common attributes shared by the failed cases. It is worth noting that all the failures occurred when the state backend type was set to {{{}HEAP{}}}. 

I suspect that the issue may have arisen because I performed two checks within a single test case. This is likely problematic because the {{HEAP}} uses a shared {{statebackend}} directory within the test, whereas {{RocksDB}} generates a distinct one each time. Modifying the test should be able to resolve this issue.
{code:java}
# heap first run
18:30:10,490 [jobmanager-io-thread-21] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'file:/tmp/org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase2122011191309948972', savepoints: 'null, maxStateSize: 5242880)

# heap second run
18:30:10,894 [jobmanager-io-thread-29] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'file:/tmp/org.apache.flink.table.planner.runtime.stream.table.TableAggregateITCase2122011191309948972', savepoints: 'null, maxStateSize: 5242880)

# rocksdb first run
07:25:56,198 [GroupTableAggregate[130] -> Calc[131] -> SinkConversion[132] -> Sink: Unnamed (1/1)#0] INFO  org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Finished building RocksDB keyed state-backend at /tmp/junit7454986721637689418/junit5007728163575453684/minicluster_6563313f5ee6acd20722570d8ee3f603/tm_0/tmp/job_4533b42d0c81a9802d737351ccde6227_op_KeyedProcessOperator_fd6cc938aebe8552b8ddacecdb53eb63__1_1__uuid_5ae4637b-a86c-4aec-b9fc-de22680251ca. 

# rocksdb second run
07:25:56,734 [GroupTableAggregate[141] -> Calc[142] -> SinkConversion[143] -> Sink: Unnamed (1/1)#0] INFO  org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Finished building RocksDB keyed state-backend at /tmp/junit7454986721637689418/junit5007728163575453684/minicluster_6563313f5ee6acd20722570d8ee3f603/tm_0/tmp/job_df30e81186a25ba0221aaf4a6e4aa56d_op_KeyedProcessOperator_fd6cc938aebe8552b8ddacecdb53eb63__1_1__uuid_1c2cea2b-ec6a-47de-957e-a1ffe4d1ea66.{code};;;","23/Jan/24 07:41;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56729&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11618;;;","23/Jan/24 10:54;qingyue;Fixed in master ecd7efc2522b7640ec8d561ee8607c971cd714e7

Fixed in release-1.18 0a1d671d0c7c5912de2116dbf1d1d641cff72b95;;;","25/Jan/24 10:17;mapohl;[~qingyue] 
Unfortunately, the following build is based on [efbd8c40|https://github.com/apache/flink/commit/efbd8c40e6ce74a268a7d6d1fadde8be4b4141a1] incorperates the fix [ecd7efc2|https://github.com/apache/flink/commit/ecd7efc2522b7640ec8d561ee8607c971cd714e7] but includes two test failures related to this issue:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56791&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11870
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56791&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11964

I'm reopening the Jira issue.;;;","25/Jan/24 10:33;mapohl;1.18 build that didn't include the fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56793&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11745;;;","26/Jan/24 03:39;qingyue;Hi [~mapohl], Thank you for pointing out that the bug fix did not work as expected. I apologize for any inconvenience. I'll revisit the bug report to reproduce the issue before submitting any further bug fixes. I appreciate your patience in resolving this issue.;;;","26/Jan/24 03:51;qingyue;{quote}1.18 build that didn't include the fix, yet:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56793&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11745]
{quote}
I think this build does not include 0a1d671d0c7c5912de2116dbf1d1d641cff72b95.;;;","26/Jan/24 13:27;mapohl;{quote}
I think this build does not include 0a1d671d0c7c5912de2116dbf1d1d641cff72b95.
{quote}
Correct. I mentioned that above the link. :)

I added it for the sake of completeness when going through CI failures that happened before the merge.;;;","26/Jan/24 15:42;mapohl;1.18 (this one included the fix from above): [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56944&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11746];;;","29/Jan/24 04:01;qingyue;The root cause of the unstable case is the source used. `tableEnv.fromValues` will interpret multiple records by Values->Calc and union them all, which uses a global ship strategy, and the input order is not guaranteed.
{code:sql}
== Optimized Physical Plan ==
Calc(select=[f0 AS top_price, f1 AS rank], changelogMode=[I,UA,D])
+- GroupTableAggregate(select=[incrementalTop2(price) AS (f0, f1)], changelogMode=[I,UA,D])
   +- Exchange(distribution=[single], changelogMode=[I])
      +- Union(all=[true], union=[id, name, price], changelogMode=[I])
         :- Calc(select=[CAST(1 AS INTEGER) AS id, CAST(_UTF-16LE'Latte':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS name, CAST(6 AS INTEGER) AS price], changelogMode=[I])
         :  +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]], changelogMode=[I])
         :- Calc(select=[CAST(2 AS INTEGER) AS id, CAST(_UTF-16LE'Milk':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS name, CAST(3 AS INTEGER) AS price], changelogMode=[I])
         :  +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]], changelogMode=[I])
         :- Calc(select=[CAST(3 AS INTEGER) AS id, CAST(_UTF-16LE'Breve':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS name, CAST(5 AS INTEGER) AS price], changelogMode=[I])
         :  +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]], changelogMode=[I])
         :- Calc(select=[CAST(4 AS INTEGER) AS id, CAST(_UTF-16LE'Mocha':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS name, CAST(8 AS INTEGER) AS price], changelogMode=[I])
         :  +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]], changelogMode=[I])
         +- Calc(select=[CAST(5 AS INTEGER) AS id, CAST(_UTF-16LE'Tea':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS name, CAST(4 AS INTEGER) AS price], changelogMode=[I])
            +- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]], changelogMode=[I]) {code};;;","29/Jan/24 14:32;mapohl;1.18:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57008&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=11652]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57008&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11633];;;","30/Jan/24 07:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57079&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11995;;;","31/Jan/24 06:52;qingyue;Master and release-1.18 branches are fixed with commit bb01c3070ea476e0410a867e930bda7518e1b4aa and d903990e1ede423e92b6d6ec93876500519aab14, respectively.

I'll monitor the CI status for one week, and if no more failure happens, I will close the issue.;;;","07/Feb/24 05:58;qingyue;I think this issue can be closed now.;;;","07/Feb/24 12:04;mapohl;I agree. It didn't appear anymore in any of the builds I checked. Thanks for your persistence.;;;",,,,,,,,
Parse error while $internal.application.program-args contains '#' in Yarn/K8s Application Mode,FLINK-34114,13564845,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,ChunJi,ChunJi,16/Jan/24 11:06,17/Jan/24 02:44,04/Jun/24 20:40,17/Jan/24 02:44,1.18.0,,,,,,,,,,,Deployment / Kubernetes,Deployment / YARN,,,0,,,,,"When run job by K8s or Yarn Application Mode use org.apache.flink.configuration.GlobalConfiguration#loadYAMLResource method parse config。

 
{code:java}
private static Configuration loadYAMLResource(File file) {
    final Configuration config = new Configuration();

    try (BufferedReader reader =
            new BufferedReader(new InputStreamReader(new FileInputStream(file)))) {

        String line;
        int lineNo = 0;
        while ((line = reader.readLine()) != null) {
            lineNo++;
            // 1. check for comments
            String[] comments = line.split(""#"", 2);
            String conf = comments[0].trim();

            // 2. get key and value
            if (conf.length() > 0) {
                String[] kv = conf.split("": "", 2);

                // skip line with no valid key-value pair
                if (kv.length == 1) {
                    LOG.warn(
                            ""Error while trying to split key and value in configuration file ""
                                    + file
                                    + "":""
                                    + lineNo
                                    + "": Line is not a key-value pair (missing space after ':'?)"");
                    continue;
                }

                String key = kv[0].trim();
                String value = kv[1].trim();

                // sanity check
                if (key.length() == 0 || value.length() == 0) {
                    LOG.warn(
                            ""Error after splitting key and value in configuration file ""
                                    + file
                                    + "":""
                                    + lineNo
                                    + "": Key or value was empty"");
                    continue;
                }

                config.setString(key, value);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException(""Error parsing YAML configuration."", e);
    }

    return config;
} {code}
if config value contains '#' like

 
{code:java}
$internal.application.program-args: '{test:test#jsonstring}'{code}
 

 

the following code is not correct
{code:java}
line.split(""#"", 2) {code}
To fix this,i think we should import snakeyaml to fix complex situation",,,,,,,,,,FLINK-33297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 02:43:32 UTC 2024,,,,,,,,,,"0|z1mryw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 02:08;ChunJi;[~MartijnVisser] Hi,i have already fix this problem and valided in my company,if the plan has no problem ,assign to me please.;;;","17/Jan/24 02:31;JunRuiLi;[~ChunJi] This issue will be addressed by [FLIP-366|https://cwiki.apache.org/confluence/display/FLINK/FLIP-366%3A+Support+standard+YAML+for+FLINK+configuration] and is planned for release with Flink 1.19.

For further details, please follow the progress: https://issues.apache.org/jira/browse/FLINK-33297. ;;;","17/Jan/24 02:43;ChunJi;[~JunRuiLi]  Ok, thank you for your good job, i will close the issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update flink-connector-elasticsearch to be compatible with updated SinkV2 interfaces,FLINK-34113,13564837,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jiabao.sun,martijnvisser,martijnvisser,16/Jan/24 09:46,31/Jan/24 07:57,04/Jun/24 20:40,31/Jan/24 07:57,,,,,,,,elasticsearch-3.2.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,"Make sure that the connector is updated to deal with the new changes introduced in FLINK-33973
See also https://github.com/apache/flink-connector-elasticsearch/actions/runs/7539688654/job/20522689108#step:14:159 for details on the current failure

This means that the new Elasticsearch connector will be compatible only with Flink 1.19, with (the upcoming) v3.1.0 being compatible with only Flink 1.18",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 07:57:14 UTC 2024,,,,,,,,,,"0|z1mrx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 07:36;jiabao.sun;[~martijnvisser] We can use TestSinkInitContext instead of MockInitContext to resolve this compilation problem.
It is possible to make the elasticsearch connector still compatible with version 1.18.;;;","31/Jan/24 07:57;leonard;Fixed in flink-connector-elasticsearch(main): 7bda67eccd773034c48fdb3f7527d8fea129c8cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaCodeSplitter OOM,FLINK-34112,13564834,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,Akihito Liang,Akihito Liang,16/Jan/24 09:22,17/Jan/24 02:46,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,"I writed a sql that has many case when syntax in FLINK 1.17 release version. But even if the client provides 8GB of memory, there is still an OOM exception:
{code:java}
16:38:51,975 ERROR org.apache.flink.client.didi.job.FlinkKubernetesJobClient    [] - Failed to execute flink job.org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: JavaCodeSplitter failed. This is a bug. Please file an issue.    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.client.program.PackagedProgramUtils.getPipelineFromProgram(PackagedProgramUtils.java:158) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.client.didi.job.FlinkKubernetesJobClient.internalSubmitJob(FlinkKubernetesJobClient.java:474) ~[flink-client-executor-core-1.17.0-011_pre.jar:?]    at org.apache.flink.client.didi.job.FlinkKubernetesJobClient.submitStreamSQL(FlinkKubernetesJobClient.java:184) [flink-client-executor-core-1.17.0-011_pre.jar:?]    at com.didichuxing.bigdata.flink.client.executor.k8s.core.component.FlinkJobK8sClientComponent.startStreamSql(FlinkJobK8sClientComponent.java:107) [flink-client-executor-core-1.17.0-011_pre.jar:?]    at com.didichuxing.bigdata.flink.client.executor.k8s.core.FlinkK8sClientExecutor.handleStartFlinkJob(FlinkK8sClientExecutor.java:230) [flink-client-executor-core-1.17.0-011_pre.jar:?]    at com.didichuxing.bigdata.flink.client.executor.k8s.core.FlinkK8sClientExecutor.handleStartJob(FlinkK8sClientExecutor.java:130) [flink-client-executor-core-1.17.0-011_pre.jar:?]    at com.didichuxing.bigdata.flink.client.executor.k8s.core.FlinkK8sClientExecutor.main(FlinkK8sClientExecutor.java:55) [flink-client-executor-core-1.17.0-011_pre.jar:?]Caused by: java.lang.RuntimeException: JavaCodeSplitter failed. This is a bug. Please file an issue.    at org.apache.flink.table.codesplit.JavaCodeSplitter.split(JavaCodeSplitter.java:37) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.runtime.generated.GeneratedClass.<init>(GeneratedClass.java:58) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.runtime.generated.GeneratedOperator.<init>(GeneratedOperator.java:43) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.codegen.OperatorCodeGenerator$.generateOneInputStreamOperator(OperatorCodeGenerator.scala:130) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:60) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:100) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecUnion.translateToPlanInternal(CommonExecUnion.java:61) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:145) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1873) ~[flink-table-api-java-uber-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:891) ~[flink-table-api-java-uber-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:2053) ~[flink-table-api-java-uber-1.17.0-018.jar:1.17.0-018]    at com.didichuxing.flink.executor.CommunitySqlExecutor.execute(CommunitySqlExecutor.java:34) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at com.didichuxing.flink.executor.FlinkSqlExecutor.execute(FlinkSqlExecutor.java:51) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at com.didichuxing.flink.FlinkSqlApp.main(FlinkSqlApp.java:35) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    ... 8 moreCaused by: java.lang.OutOfMemoryError: Java heap space    at org.apache.flink.table.shaded.org.antlr.v4.runtime.ParserRuleContext.addAnyChild(ParserRuleContext.java:133) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.shaded.org.antlr.v4.runtime.ParserRuleContext.addChild(ParserRuleContext.java:139) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.shaded.org.antlr.v4.runtime.Parser.addContextToParseTree(Parser.java:617) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.shaded.org.antlr.v4.runtime.Parser.enterRule(Parser.java:629) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.primary(JavaParser.java:7760) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.expression(JavaParser.java:6954) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.expression(JavaParser.java:7020) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.parExpression(JavaParser.java:6653) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5521) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]16:38:51,997 ERROR com.didichuxing.bigdata.flink.client.executor.k8s.core.FlinkK8sClientExecutor [] - start job fail!org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: JavaCodeSplitter failed. This is a bug. Please file an issue.    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.client.program.PackagedProgramUtils.getPipelineFromProgram(PackagedProgramUtils.java:158) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.client.didi.job.FlinkKubernetesJobClient.internalSubmitJob(FlinkKubernetesJobClient.java:474) ~[flink-client-executor-core-1.17.0-011_pre.jar:?]    at org.apache.flink.client.didi.job.FlinkKubernetesJobClient.submitStreamSQL(FlinkKubernetesJobClient.java:184) ~[flink-client-executor-core-1.17.0-011_pre.jar:?]    at com.didichuxing.bigdata.flink.client.executor.k8s.core.component.FlinkJobK8sClientComponent.startStreamSql(FlinkJobK8sClientComponent.java:107) ~[flink-client-executor-core-1.17.0-011_pre.jar:?]    at com.didichuxing.bigdata.flink.client.executor.k8s.core.FlinkK8sClientExecutor.handleStartFlinkJob(FlinkK8sClientExecutor.java:230) ~[flink-client-executor-core-1.17.0-011_pre.jar:?]    at com.didichuxing.bigdata.flink.client.executor.k8s.core.FlinkK8sClientExecutor.handleStartJob(FlinkK8sClientExecutor.java:130) [flink-client-executor-core-1.17.0-011_pre.jar:?]    at com.didichuxing.bigdata.flink.client.executor.k8s.core.FlinkK8sClientExecutor.main(FlinkK8sClientExecutor.java:55) [flink-client-executor-core-1.17.0-011_pre.jar:?]Caused by: java.lang.RuntimeException: JavaCodeSplitter failed. This is a bug. Please file an issue.    at org.apache.flink.table.codesplit.JavaCodeSplitter.split(JavaCodeSplitter.java:37) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.runtime.generated.GeneratedClass.<init>(GeneratedClass.java:58) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.runtime.generated.GeneratedOperator.<init>(GeneratedOperator.java:43) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.codegen.OperatorCodeGenerator$.generateOneInputStreamOperator(OperatorCodeGenerator.scala:130) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.codegen.CalcCodeGenerator$.generateCalcOperator(CalcCodeGenerator.scala:60) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.codegen.CalcCodeGenerator.generateCalcOperator(CalcCodeGenerator.scala) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:100) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecUnion.translateToPlanInternal(CommonExecUnion.java:61) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:145) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:161) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197) ~[flink-table-planner-loader-bundle-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1873) ~[flink-table-api-java-uber-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:891) ~[flink-table-api-java-uber-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:2053) ~[flink-table-api-java-uber-1.17.0-018.jar:1.17.0-018]    at com.didichuxing.flink.executor.CommunitySqlExecutor.execute(CommunitySqlExecutor.java:34) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at com.didichuxing.flink.executor.FlinkSqlExecutor.execute(FlinkSqlExecutor.java:51) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at com.didichuxing.flink.FlinkSqlApp.main(FlinkSqlApp.java:35) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist_2.12-1.17.0-018.jar:1.17.0-018]    ... 8 moreCaused by: java.lang.OutOfMemoryError: Java heap space    at org.apache.flink.table.shaded.org.antlr.v4.runtime.ParserRuleContext.addAnyChild(ParserRuleContext.java:133) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.shaded.org.antlr.v4.runtime.ParserRuleContext.addChild(ParserRuleContext.java:139) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.shaded.org.antlr.v4.runtime.Parser.addContextToParseTree(Parser.java:617) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.shaded.org.antlr.v4.runtime.Parser.enterRule(Parser.java:629) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.primary(JavaParser.java:7760) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.expression(JavaParser.java:6954) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.expression(JavaParser.java:7020) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.parExpression(JavaParser.java:6653) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5521) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5489) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.statement(JavaParser.java:5532) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.blockStatement(JavaParser.java:5190) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018]    at org.apache.flink.table.codesplit.JavaParser.block(JavaParser.java:5119) ~[flink-table-runtime-1.17.0-018.jar:1.17.0-018] {code}
the sql is follow:
{code:java}
add jar 'hdfs://xxxx/realtime-netcar-udf-lastest-v1.1.3-SNAPSHOT.jar';
CREATE FUNCTION RemoveMapKeySort as 'org.didichuxing.flink.udf.RemoveMapKey2ArraySort';set `table.exec.resource.default-parallelism` = `1`;
set `table.generated-code.max-length`=`7000`;
set `table.generated-code.allow` = `true`;-- Source schema
CREATE TABLE source_table (
binlog_table                  string,
binlog_time                   string,
binlog_event                  string,
order_id                      string,
driver_id                     string,
passenger_id                  string,
channel                       string,
product_id                    string,
area                          string,
to_area                       string,
county                        string,
order_status                  string,
_status                       string,
t_order_status                string,
f_order_status                string,
_birth_time                   string,
cancelled_time                string,
new_time                      string,
assigned_time                 string,
prepared_time                 string,
completed_time                string,
departure_time                string,
begun_time                    string,
consult_time                  string,
finished_time                 string,
estimate_time                 string,
_modify_time                  string,
start_dest_distance           string,
driver_start_distance         string,
distance                      string,
pre_total_fee                 string,
`type`                        string,
combo_type                    string,
complete_type                 string,
airport_type                  string,
carpool_type                  string,
compound_type                 string,
source_type                   string,
driver_type                   string,
capacity_level                string,
level_type                    string,
require_level                 string,
route_type                    string,
strive_car_level              string,
extra_type                    string,
extend_feature                string,
is_special_price              string,
carpool_price_type            string,
estimate_id                   string, 
new_lng                       string, 
new_lat                       string,
assigned_lng                  string, 
assigned_lat                  string,
prepared_lng                  string, 
prepared_lat                  string,
begun_lng                     string, 
begun_lat                     string,
finished_lng                  string,
finished_lat                  string,
dynamic_price                 string,
travel_id                     string,
passenger_count               string,
combo_id                      string,
starting_lng                  string,
starting_lat                  string,
dest_lng                      string,
dest_lat                      string,
cap_price                     string,
spacious_car_alliance         string,
json_extend_1                 string,
p_access_key_id               string
)
WITH
(
    'connector.topic' = 'xxx',
    'connector.startup-mode' = 'group-offsets',
    'connector.type' = 'kafka',
    'connector.properties.bootstrap.servers' = 'xxxx',
    'connector.properties.group.id' = 'rxxx',
    'connector.parallelism' = '1',
    'format.type' = 'json'
);-- Sink schema
CREATE TABLE realtime_dwd_trip_trd_order_base (
table_filter                    string,
table_time                        string,
table_event                        string,
order_id                        bigint,
driver_id                       bigint,
passenger_id                    bigint,
channel_id                        bigint,
channel_name                     string,
channel_level1                  string,
channel_level2                  string,
product_id                        bigint,
sub_product_line                bigint,
level_1_product                    bigint,
level_2_product                    bigint,
level_3_product                    bigint,
call_city_id                    bigint,
to_city_id                        bigint,
call_city_name                    string,
call_county_id                    bigint,
call_county_name                string,
region_name                     string,
order_status                    bigint,
call_time                        string,
cancel_time                        string,
answer_time                     string,
arrive_time                     string,
depart_time                        string,
charge_time                        string,
finish_time                        string,
call_time_minute                string,
cancel_time_minute              string,
answer_time_minute              string,
arrive_time_minute              string,
depart_time_minute              string,
charge_time_minute              string,
finish_time_minute              string,
est_dis                            bigint,
est_arrive_dis                    bigint,
est_price_amt                    double,
combo_type                        bigint,
complete_type                    bigint,
airport_type                    bigint,
carpool_type                    bigint,
compound_type                    bigint,
source_type                        bigint,
driver_type                        bigint,
capacity_level                    bigint,
level_type                        bigint,
require_level                    bigint,
route_type                        bigint,
strive_car_level                bigint,
extra_type                        bigint,
answer_dur                        bigint,
est_dur                            bigint,
arrive_dur                        bigint,
cancel_dur                        bigint,
driver_wait_dur                    bigint,
charge_dur                        bigint,
carpool_price_type              bigint,
is_appt_flag                    bigint,
is_agency_call                    bigint,
is_anycar                        bigint,
is_cross_city_flag                bigint,
is_call_flag                    bigint,
cancel_type                     bigint,
is_cancel_flag                    bigint,
is_grab_before_cannel_flag        bigint,
is_grab_after_pas_cancel_flag    bigint,
is_grab_after_dri_cancel_flag    bigint,
is_grab_after_srvc_cancel_flag    bigint,
is_reassigned_flag                bigint,
is_reassign_flag                bigint,
is_answer_flag                    bigint,
is_arrive_flag                    bigint,
is_begin_charge_flag            bigint,
is_finish_flag                    bigint,
is_openapi                        bigint,
is_nirvana                        bigint,
is_ontheway                        bigint,
is_update_dest                    bigint,
is_service_agency_call            bigint,
is_serial_assign                bigint,
is_prepay                        bigint,
estimate_id                     string,
estimate_id_anycar              string,
rank_index                            bigint,
is_anycar_sumtag                bigint, 
call_lng                         string,
call_lat                         string,
answer_lng                    string,
answer_lat                    string,
arrive_lng                    string,
arrive_lat                    string,
begin_charge_lng                       string,
begin_charge_lat                       string,
finish_lng                    string,
finish_lat                    string,
event_lng                       string,
event_lat                       string,
actual_distance                 bigint,
event_distance                  bigint,
extend_feature_value             string,
distance_category               int,
is_special_price                int,
is_guide_scene                  int,
extend_feature                  string,
dynamic_price                   int,
travel_id                     bigint,
passenger_num               bigint,
combo_id                       bigint, 
dest_lng                        string, 
dest_lat                        string, 
cap_price_amt                   string,
spacious_car_alliance           int,
pro_id                          int,
pro_name                        string,
json_extend_1                   string,
access_key_id                   int              
)
WITH
(
    'connector' = 'print'
);-- 非anycar部分
create view source_non_anycar_table
as
select  binlog_table
        ,binlog_time
        ,binlog_event
        ,order_id
        ,driver_id
        ,passenger_id
        ,channel
        ,source_table.product_id
        ,area
        ,to_area
        ,county
        ,order_status as order_status
        ,_status
        ,t_order_status
        ,f_order_status
        ,_birth_time
        ,coalesce(cancelled_time,'1971-01-01 00:00:00') as cancelled_time
        ,new_time
        ,assigned_time
        ,prepared_time
        ,completed_time
        ,departure_time
        ,begun_time
        ,consult_time
        ,finished_time
        ,estimate_time
        ,_modify_time
        ,cast(start_dest_distance as bigint) as start_dest_distance
        ,cast(driver_start_distance as bigint) as driver_start_distance
        ,cast(cast(distance as double)*1000 as bigint)  as distance
        ,source_table.pre_total_fee
        ,`type`
        ,source_table.combo_type
        ,complete_type
        ,airport_type
        ,carpool_type
        ,compound_type
        ,source_type
        ,driver_type
        ,capacity_level
        ,level_type
        ,source_table.require_level
        ,route_type
        ,strive_car_level
        ,source_table.extra_type
        ,RemoveMapKeySort(get_json_object(extend_feature, '$.multi_require_product')) as extend_feature_value
        ,is_special_price
        ,carpool_price_type
        ,estimate_id 
        ,'0' as estimate_id_anycar  
        ,9999 as rank_index
        ,new_lng         
        ,new_lat         
        ,assigned_lng    
        ,assigned_lat    
        ,prepared_lng    
        ,prepared_lat    
        ,begun_lng       
        ,begun_lat       
        ,finished_lng    
        ,finished_lat 
        ,extend_feature   
        ,dynamic_price
        ,travel_id
        ,passenger_count 
        ,combo_id
        ,starting_lng
        ,starting_lat
        ,dest_lng
        ,dest_lat
        ,cap_price
        ,spacious_car_alliance
        ,json_extend_1
        ,p_access_key_id
        ,case
                when ((coalesce(product_id,'-1') ='1' and coalesce(combo_type,'-1') <> '308') or coalesce(product_id,'-1') ='2') and not (coalesce(require_level,'-1')='200' and coalesce(area,'-1') in ('1','4','2','3','5','10','17','7','6','37')) then '1'
                when coalesce(product_id,'-1')='1' and coalesce(combo_type,'-1') = '308' then '308' 
                when coalesce(product_id,'-1') in ('3','4') and coalesce(require_level,'-1')<>'900' and coalesce(combo_type,'-1')<>'314' or coalesce(product_id,'-1') in ('287') or coalesce(cast(product_id as bigint),-1) between 3000 and 3999 then '3' 
                when coalesce(product_id,'-1') in ('6') then '6' 
                when coalesce(product_id,'-1') in ('7','20') then product_id
                when coalesce(product_id,'-1') in ('3','4') and coalesce(require_level,'-1')='900' then '99' 
                when (((coalesce(product_id,'-1') ='1' and coalesce(combo_type,'-1') <> '308') or coalesce(product_id,'-1') ='2') and (coalesce(require_level,'-1')='200' and coalesce(area,'-1') in ('1','4','2','3','5','10','17','7','6','37'))) or coalesce(product_id,'-1') in ('9','22') then '9' 
                when coalesce(product_id,'-1') in ('11','19','12','21') then '11' 
                when coalesce(product_id,'-1') in ('26') then '999' 
                when (coalesce(product_id,'-1') in ('3','4') and coalesce(combo_type,'-1')='314') or (coalesce(product_id,'-1') in ('800','801')) then '314' 
                when product_id in ('38') then '38'
                when coalesce(cast(channel as bigint),-1) between 1000001 and 2000000 and (coalesce(cast(product_id as bigint),-1) between 50 and 59 or coalesce(cast(product_id as bigint),-1) between 70 and 79 or coalesce(cast(product_id as bigint),-1) between 200 and 599) then '50' 
                when coalesce(cast(channel as bigint),-1) not between 1000001 and 2000000 and coalesce(product_id,'-1') in ('51','52','53','54','59','71','73','74','75','76','79','77') then '50' 
                when coalesce(cast(product_id as bigint),-1) between 50 and 59 or coalesce(cast(product_id as bigint),-1) between 70 and 79 or coalesce(cast(product_id as bigint),-1) between 200 and 599 then '51' 
                when coalesce(product_id,'-1') in ('700') then '700' 
                when coalesce(product_id,'-1') in ('81') then '81' 
                when coalesce(product_id,'-1') in ('83') then '327' 
                when coalesce(product_id,'-1') in ('15') then '15' 
        end as category_id
from    source_table
where   not (order_status in ('0','6')
        and   extend_feature is not null
        and   extend_feature like '%multi_require_product%')
        and   not (binlog_table='d_order_status' and order_status='5' and f_order_status is null and completed_time>'2020-01-01 00:00:00') 
;-- anycar部分
create view source_anycar_table
as
select  binlog_table
        ,binlog_time
        ,binlog_event
        ,order_id
        ,driver_id
        ,passenger_id
        ,channel
        ,cast(GET_INT_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'product_id', -1) as string) as product_id
        ,area
        ,to_area
        ,county
        ,order_status as order_status
        ,_status
        ,t_order_status
        ,f_order_status
        ,_birth_time
        ,coalesce(cancelled_time,'1971-01-01 00:00:00') as cancelled_time
        ,new_time
        ,assigned_time
        ,prepared_time
        ,completed_time
        ,departure_time
        ,begun_time
        ,consult_time
        ,finished_time
        ,estimate_time
        ,_modify_time
        ,cast(start_dest_distance as bigint) as start_dest_distance
        ,cast(driver_start_distance as bigint) as driver_start_distance
        ,cast(cast(distance as double)*1000 as bigint) as distance
        ,cast(GET_INT_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'pre_total_fee', 0) as string) as pre_total_fee
        ,`type`
        ,cast(GET_INT_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'combo_type', -1) as string) as combo_type
        ,complete_type
        ,airport_type
        ,cast(GET_INT_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'carpool_type', -1) as string) as carpool_type
        ,compound_type
        ,source_type
        ,driver_type
        ,capacity_level
        ,cast(GET_INT_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'level_type', -1) as string) as level_type
        ,GET_OBJ_STR_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'require_level', '-1') as require_level
        ,GET_OBJ_STR_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'route_type', '-1') as route_type
        ,strive_car_level
        ,cast(GET_LONG_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'extra_type', -1) as string) as extra_type
        ,RemoveMapKeySort(get_json_object(extend_feature, '$.multi_require_product')) as extend_feature_value
        ,cast(GET_INT_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'is_special_price', -1) as string) as is_special_price 
        ,carpool_price_type
        ,estimate_id  
        ,GET_OBJ_STR_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'estimate_id', '1') as estimate_id_anycar 
        ,GET_INT_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'rank', -1) as rank_index
        ,new_lng         
        ,new_lat         
        ,assigned_lng    
        ,assigned_lat    
        ,prepared_lng    
        ,prepared_lat    
        ,begun_lng       
        ,begun_lat       
        ,finished_lng    
        ,finished_lat 
        ,extend_feature   
        ,dynamic_price
        ,travel_id
        ,passenger_count
        ,combo_id
        ,starting_lng
        ,starting_lat
        ,dest_lng
        ,dest_lat
        ,cap_price
        ,cast(GET_INT_FROM_MAP(JSON_STR_TO_MAP(T.any_call_info), 'spacious_car_alliance', -1) as string) as spacious_car_alliance 
        ,json_extend_1
        ,p_access_key_id
        ,case
                when ((coalesce(product_id,'-1') ='1' and coalesce(combo_type,'-1') <> '308') or coalesce(product_id,'-1') ='2') and not (coalesce(require_level,'-1')='200' and coalesce(area,'-1') in ('1','4','2','3','5','10','17','7','6','37')) then '1' 
                when coalesce(product_id,'-1')='1' and coalesce(combo_type,'-1') = '308' then '308' 
                when coalesce(product_id,'-1') in ('3','4') and coalesce(require_level,'-1')<>'900' and coalesce(combo_type,'-1')<>'314' or coalesce(product_id,'-1') in ('287') or coalesce(cast(product_id as bigint),-1) between 3000 and 3999 then '3' 
                when coalesce(product_id,'-1') in ('6') then '6' 
                when coalesce(product_id,'-1') in ('7','20') then product_id 
                when coalesce(product_id,'-1') in ('3','4') and coalesce(require_level,'-1')='900' then '99' 
                when (((coalesce(product_id,'-1') ='1' and coalesce(combo_type,'-1') <> '308') or coalesce(product_id,'-1') ='2') and (coalesce(require_level,'-1')='200' and coalesce(area,'-1') in ('1','4','2','3','5','10','17','7','6','37'))) or coalesce(product_id,'-1') in ('9','22') then '9' 
                when coalesce(product_id,'-1') in ('11','19','12','21') then '11' 
                when coalesce(product_id,'-1') in ('26') then '999' 
                when (coalesce(product_id,'-1') in ('3','4') and coalesce(combo_type,'-1')='314') or (coalesce(product_id,'-1') in ('800','801')) then '314' 
                when product_id in ('38') then '38' 
                when coalesce(cast(channel as bigint),-1) between 1000001 and 2000000 and (coalesce(cast(product_id as bigint),-1) between 50 and 59 or coalesce(cast(product_id as bigint),-1) between 70 and 79 or coalesce(cast(product_id as bigint),-1) between 200 and 599) then '50' 
                when coalesce(cast(channel as bigint),-1) not between 1000001 and 2000000 and coalesce(product_id,'-1') in ('51','52','53','54','59','71','73','74','75','76','79','77') then '50' 
                when coalesce(cast(product_id as bigint),-1) between 50 and 59 or coalesce(cast(product_id as bigint),-1) between 70 and 79 or coalesce(cast(product_id as bigint),-1) between 200 and 599 then '51' 
                when coalesce(product_id,'-1') in ('700') then '700' 
                when coalesce(product_id,'-1') in ('81') then '81' 
                when coalesce(product_id,'-1') in ('83') then '327' 
                when coalesce(product_id,'-1') in ('15') then '15' 
        end as category_id
from    source_table
        ,LATERAL TABLE(JSON_ARRAY_TO_STR(RemoveMapKeySort(get_json_object(extend_feature, '$.multi_require_product')))) as T(any_call_info)
where   order_status in ('0','6')
  and   extend_feature is not null
  and   extend_feature like '%multi_require_product%'
  and   not (binlog_table='d_order_status' and order_status='5' and f_order_status is null and completed_time>'2020-01-01 00:00:00') 
;
-- Union all
create view view_merge
as
select  *
from    source_non_anycar_table
union all   
select  *
from    source_anycar_table
;
-- View
create view view_01
as
select  binlog_table
        ,binlog_time
        ,binlog_event
        ,order_id
        ,driver_id
        ,passenger_id
        ,channel        ,'未知' as channel_name
        ,'未知' as channel_level1
        ,'未知' as channel_level2
        ,product_id
        ,case   when level_type = '1' then '30'    
                when product_id = '3' and capacity_level = '11000' then '81'    
                when spacious_car_alliance = '1' then '90' 
                else category_id
         end as sub_product_line    
        ,area
        ,to_area
        ,'未知' as city_name
        ,county
        ,'未知' as county_name
        ,'未知' as region
        ,order_status
        ,_birth_time
        ,case   when order_status = '6' and source_type <> '2' then _modify_time    
                when (source_type = '2' and order_status = '6') then _modify_time    
                when order_status = '7' and cancelled_time = '1971-01-01 00:00:00' then _modify_time    
                when ((complete_type not in ('7', '13') and order_status = '12') or (source_type = '2' and order_status = '0') or order_status = '9') and (order_status = '12' and complete_type = '14') then _modify_time    
                when order_status = '11' then _modify_time    
                when complete_type = '13' and order_status = '12' then _modify_time    
                else cancelled_time
         end cancel_time
        ,assigned_time
        ,prepared_time
        ,departure_time
        ,begun_time
        ,finished_time
        ,start_dest_distance
        ,driver_start_distance
        ,pre_total_fee
        ,combo_type
        ,complete_type
        ,airport_type
        ,carpool_type
        ,compound_type
        ,source_type
        ,driver_type
        ,capacity_level
        ,level_type
        ,require_level
        ,route_type
        ,strive_car_level
        ,extra_type
        ,case when assigned_time is not null and _birth_time is not null and assigned_time > _birth_time then cast(DATE_DIFF(_birth_time,assigned_time) as string) else '0' end as answer_dur
        ,cast(estimate_time as bigint)*60 as est_dur
        ,case when assigned_time is not null and prepared_time is not null and prepared_time > assigned_time then cast(DATE_DIFF(assigned_time,prepared_time) as string) else '0' end as arrive_dur
        ,case   when cancelled_time is not null and assigned_time is not null and cancelled_time > assigned_time then cast(DATE_DIFF(assigned_time,cancelled_time) as string)
                when cancelled_time is not null and _birth_time is not null and cancelled_time > _birth_time then cast(DATE_DIFF(_birth_time ,cancelled_time) as string)
                when f_order_status is not null and assigned_time is not null and order_status = '7' and _modify_time > assigned_time then cast(DATE_DIFF(assigned_time,_modify_time) as string)
                when f_order_status is not null and _birth_time is not null and order_status = '7' and _modify_time > _birth_time then cast(DATE_DIFF(_birth_time,_modify_time) as string)
                else '0'
         end as cancel_dur
        ,case when begun_time is not null and prepared_time is not null and begun_time > prepared_time then cast(DATE_DIFF(prepared_time,begun_time) as string) else '0' end as driver_wait_dur
        ,case when finished_time is not null and begun_time is not null and finished_time > begun_time then cast(DATE_DIFF(begun_time,finished_time) as string) else '0' end as charge_dur
        ,cast(`type` as int) as is_appt_flag
        ,case when extra_type is not null and BITWISE_AND(cast(extra_type as bigint), 131072) > 0 then  '1' else '0' end as is_agency_call
        ,case when compound_type is not null and BITWISE_AND(cast(compound_type as bigint),8192) > 0 then '1' else '0' end as is_anycar
        ,case when area is not null and to_area is not null and area <> to_area then '1' else '0' end as is_cross_city_flag
        -- ,case when _birth_time is not null and substring(_birth_time,1,10) > '2020-01-01 00:00:00' and order_status='0' and binlog_table='d_order_base' and (driver_id = '0' or driver_id is null) and substring(_birth_time,1,16)=substring(_modify_time,1,16) and binlog_event='i' then '1' else '0' end as is_call_flag
        ,case when _birth_time is not null and substring(_birth_time,1,10) > '2020-01-01 00:00:00' and order_status='0' and (driver_id = '0' or driver_id is null) and substring(_birth_time,1,10)=substring(_modify_time,1,10) then '1' else '0' end as is_call_flag
        ,case   when f_order_status is not null and source_type is not null and cancelled_time is not null and _modify_time is not null and order_status is not null and substring((case when order_status = '7' then cancelled_time else _modify_time end),1,10) > '2020-01-01 00:00:00' and order_status = '6' and source_type <> '2' and binlog_table='d_order_status' then '1'
                when f_order_status is not null and source_type is not null and cancelled_time is not null and _modify_time is not null and order_status is not null and substring((case when order_status = '7' then cancelled_time else _modify_time end),1,10) > '2020-01-01 00:00:00' and ((source_type = '2' and order_status in ('6', '7')) or (source_type <> '2' and order_status = '7')) and binlog_table='d_order_status' then '2'
                when f_order_status is not null and source_type is not null and cancelled_time is not null and complete_type is not null and _modify_time is not null and order_status is not null and substring((case when order_status = '7' then cancelled_time else _modify_time end),1,10) > '2020-01-01 00:00:00' and ((complete_type not in ('7', '13') and order_status = '12') or (source_type = '2' and order_status = '0') or order_status = '9') and (order_status <> '12' or complete_type <> '14') and binlog_table='d_order_status' then '3'
                when f_order_status is not null and source_type is not null and cancelled_time is not null and _modify_time is not null and order_status is not null and substring((case when order_status = '7' then cancelled_time else _modify_time end),1,10) > '2020-01-01 00:00:00' and order_status = '11' and binlog_table='d_order_status' then '4'
                when f_order_status is not null and source_type is not null and cancelled_time is not null and complete_type is not null and _modify_time is not null and order_status is not null and substring((case when order_status = '7' then cancelled_time else _modify_time end),1,10) > '2020-01-01 00:00:00' and complete_type = '13' and order_status = '12' and binlog_table='d_order_status' then '5'
                else '0'
         end as cancel_type
        ,case when f_order_status is not null and complete_type is not null and substring((case when order_status = '7' then cancelled_time else _modify_time end),1,10) > '2020-01-01 00:00:00' and complete_type = '7' and binlog_table='d_order_status' then '1' else '0' end as is_reassigned_flag
        ,case when source_type is not null and source_type = '2' and binlog_table='d_order_status' then '1' else '0' end as is_reassign_flag
        ,case when assigned_time is not null and substring(assigned_time,1,10) > '2020-01-01 00:00:00' and order_status='1' and f_order_status is not null and binlog_table='d_order_status' and driver_id is not null and cast(driver_id as bigint) > 0 then '1' else '0' end as is_answer_flag
        ,case when prepared_time is not null and substring(prepared_time,1,10) > '2020-01-01 00:00:00' and order_status='2' and f_order_status is not null and binlog_table='d_order_status' and driver_id is not null and cast(driver_id as bigint) > 0 then '1' else '0' end as is_arrive_flag
        ,case when begun_time is not null and substring(begun_time,1,10) > '2020-01-01 00:00:00' and order_status='4' and binlog_table='d_order_status' and f_order_status is not null and driver_id is not null and cast(driver_id as bigint) > 0 then '1' else '0' end as is_begin_charge_flag
        ,case when finished_time is not null and substring(finished_time,1,10) > '2020-01-01 00:00:00' and order_status = '5' and f_order_status is not null and binlog_table='d_order_status' then '1' else '0' end as is_finish_flag
        ,case when passenger_id is not null and passenger_id = '2624535533694' or (channel is not null and cast(channel as bigint) between 27010 and 27019) then '1' else '0' end as is_openapi
        ,case when product_id is not null and capacity_level is not null and product_id in ('3', '4') and capacity_level = '2000' then '1' else '0' end as is_nirvana
        ,case when extra_type is not null and BITWISE_AND(cast(extra_type as bigint), 33554432) > 0 then '1' else '0' end as is_ontheway
        ,case when extra_type is not null and BITWISE_AND(cast(extra_type as bigint), 8796093022208) > 0 then '1' else '0' end as is_update_dest
        ,case when extra_type is not null and BITWISE_AND(cast(extra_type as bigint), 137438953472) > 0 then '1' else '0' end as is_service_agency_call
        ,case when extra_type is not null and BITWISE_AND(cast(extra_type as bigint), 8388608) > 0 then '1' else '0' end as is_serial_assign
        ,case when extra_type is not null and BITWISE_AND(cast(extra_type as bigint), 8589934592) > 0 then '1' else '0' end as is_prepay
        ,is_special_price
        ,carpool_price_type
        ,estimate_id
        ,estimate_id_anycar
        ,rank_index
        ,starting_lng
        ,starting_lat
        ,assigned_lng 
        ,assigned_lat 
        ,prepared_lng 
        ,prepared_lat 
        ,begun_lng    
        ,begun_lat    
        ,finished_lng 
        ,finished_lat 
        ,distance
        ,extend_feature_value
        ,case 
                when distance<3*1000 then 0 
                when distance>=3*1000 and distance<6*1000 then 3
                when distance>=6*1000 and distance<10*1000 then 6
                when distance>=10*1000 and distance<15*1000 then 10
                when distance>=15*1000 and distance<20*1000 then 15
                when distance>=20*1000 then 20 
        end as  real_distance_category
        ,case 
                when start_dest_distance<3*1000 then 0 
                when start_dest_distance>=3*1000 and start_dest_distance<6*1000 then 3
                when start_dest_distance>=6*1000 and start_dest_distance<10*1000 then 6
                when start_dest_distance>=10*1000 and start_dest_distance<15*1000 then 10
                when start_dest_distance>=15*1000 and start_dest_distance<20*1000 then 15
                when start_dest_distance>=20*1000 then 20 
        end as  estimate_distance_category
        ,extend_feature
        ,case when extra_type is not null and BITWISE_AND(cast(extra_type as bigint), 268435456) > 0 then '1' else '0' end as is_guide_scene
        ,dynamic_price
        ,travel_id
        ,passenger_count
        ,combo_id
        ,dest_lng
        ,dest_lat
        ,cap_price
        ,spacious_car_alliance
        ,'1' as pro_id
        ,'未知' as pro_name
        ,json_extend_1
        ,p_access_key_id as access_key_id
from    view_merge
;
-- insert into order_base_test
-- select 
-- order_id,
-- driver_id,
-- passenger_id,
-- product_id,
-- sub_product_line
-- from view_01;--Sink
insert into realtime_dwd_trip_trd_order_base
select  cast(binlog_table as string) as table_filter
        ,cast(binlog_time as string) as table_time
        ,cast(binlog_event as string) as table_event
        ,cast(order_id as bigint) as order_id
        ,cast(driver_id as bigint) as driver_id
        ,cast(passenger_id as bigint) as passenger_id
        ,cast(channel as bigint) as channel_id
        ,cast(channel_name as string) as channel_name
        ,cast(channel_level1 as string) as channel_level1
        ,cast(channel_level2 as string) as channel_level2
        ,cast(product_id as bigint) as product_id
        ,cast(sub_product_line as bigint) as sub_product_line
        ,case
                when sub_product_line in ('1', '3', '6', '7', '9', '20', '99', '314', '30', '81', '700', '90','327') or (sub_product_line in ('11','12') and require_level='2200') then 110000
                when sub_product_line in ('11','12') and require_level<>'2200' then 120000
                when sub_product_line in ('308') then 130000
                when sub_product_line in ('38') then 140000
                when sub_product_line in ('50','51') then 190000
                when sub_product_line = '15' then 150000 
                else 990000
        end as level_1_product
        ,case
                when sub_product_line in ('3','7','30','81','90')  and combo_type not in ('4','302')     then 110100
                when sub_product_line in ('3','7') and combo_type = '4'                    then 110200
                when sub_product_line in ('3','7') and combo_type = '302'                  then 110300
                when sub_product_line  in ('20','99')                                    then 110400
                when sub_product_line  in ('1','6')                                      then 110500
                when sub_product_line  in ('9')                                        then 110600    
                when sub_product_line in ('11','12') and require_level<>'2200'             then 120100 
                when sub_product_line  in ('308')                                      then 130100
                when sub_product_line  in ('38')                                       then 140200
                when sub_product_line  in ('700')                                      then 110800
                when sub_product_line  in ('314')                                      then 110900 
                when sub_product_line in ('11','12') and require_level='2200'              then 111000 
                when (cast(channel as bigint)>=1000001 and cast(channel as bigint)<=2000000 and sub_product_line = '50') or ((cast(channel as bigint)<1000001 or cast(channel as bigint) >2000000) and sub_product_line in ('50') and coalesce(product_id,'-1') <> '77') or sub_product_line = '51'  then 190200 
                when sub_product_line in ('327') then 111200 
                when sub_product_line = '15' then 150100 
                when (cast(channel as bigint)<1000001 or cast(channel as bigint) >2000000) and sub_product_line = '50' and coalesce(product_id,'-1') = '77' then 190100 
                else 999900
        end as  level_2_product
        ,case   when sub_product_line = '11' and require_level = '2000' and level_type = '5' then 120105 
                when sub_product_line = '11' and require_level = '1100' and level_type = '5' then 120106 
                when sub_product_line = '11' and require_level = '2200' and level_type = '7' then 111002 
         else 
                case
                        when sub_product_line in ('3','7') and combo_type not in ('4','302') then 110101 
                        when sub_product_line='30' then 110102 
                        when sub_product_line='314' then 110103 
                        when sub_product_line in ('3','7') and combo_type='4' and carpool_type='4' then 110201 
                        when sub_product_line in ('3','7') and combo_type='4' and carpool_type='5' then 110202 
                        when sub_product_line in ('3','7') and require_level='600' and combo_type='4' and carpool_type in ('1','2') then 110203 
                        when sub_product_line in ('3','7') and require_level='610' and combo_type='4' and carpool_type='2' then 110204 
                        when sub_product_line in ('3','7') and require_level='600' and combo_type='4' and carpool_type in ('10') then 110205 
                        when sub_product_line in ('3','7') and require_level = '4' then 110299 
                        when sub_product_line in ('3','7') and combo_type = '302' and carpool_type = '6' then 110303 
                        when sub_product_line in ('3','7') and combo_type = '302' and carpool_type = '3' and not (coalesce(product_id,'-1') in ('287') or (coalesce(product_id,'-1') not in ('3013') and cast(coalesce(product_id,'-1') as bigint) between 3000 and 3999)) then 110302 
                        when sub_product_line in ('3','7') and combo_type = '302' and carpool_type not in ('3','6','8') then 110304 
                        when sub_product_line in ('3','7') and combo_type = '302' and carpool_type = '3' then 110305 
                        when sub_product_line in ('3','7') and combo_type = '302' and carpool_type = '8' then 110306 
                        when sub_product_line in ('20','99') then 110401 
                        when sub_product_line in ('1','6') and require_level='100' then 110501 
                        when sub_product_line in ('1','6') and require_level='400' then 110502 
                        when sub_product_line in ('1','6') and require_level='3300' then 110503 
                        when sub_product_line in ('1','6') and require_level='200' then 110504 
                        when sub_product_line in ('1','6') then 110599 
                        when sub_product_line in ('9') then 110601 
                        when (cast(channel as bigint)<1000001 or cast(channel as bigint) >2000000) and sub_product_line in ('50') and coalesce(product_id,'-1') in ('52','53','54','59') then 110721  
                        when sub_product_line= '51' or ((cast(channel as bigint)<1000001 or cast(channel as bigint) >2000000) and sub_product_line in ('50') and coalesce(product_id,'-1') not in ('52','53','54','59','77')) then 110722 
                        when sub_product_line = '11' and require_level='2000' then 120101  
                        when sub_product_line = '11' and require_level = '1100' and combo_type = '4' and carpool_type = '2' then 120102 
                        when sub_product_line = '11' and coalesce(cast(is_special_price as bigint), -1) = 1 then 120103 
                        when sub_product_line = '11' and require_level ='1100' then 120104 
                        when sub_product_line = '12' then 120201 
                        when sub_product_line in ('308') then 130101 
                        when sub_product_line in ('38') then 140201 
                        when sub_product_line in ('81') then 110104 
                        when sub_product_line in ('90') then 110105 
                        when sub_product_line in ('700') then 110801 
                        when (cast(channel as bigint)>=1000001 and cast(channel as bigint)<=2000000 and sub_product_line = '50') then 190201 
                        when sub_product_line in ('11','12') and require_level='2200' and level_type='0' then 111001 
                        when sub_product_line in ('11','12') and require_level='2200' and level_type='7' then 111002 
                        when sub_product_line in ('327') then 111201 
                        when sub_product_line = '15' then 150101 
                        when (cast(channel as bigint)<1000001 or cast(channel as bigint) >2000000) and sub_product_line = '50' and coalesce(product_id,'-1')='77' then 190101 
                        else 999999 
                end
         end as level_3_product
        ,cast(area as bigint) as call_city_id
        ,cast(to_area as bigint) as to_city_id
        ,cast(city_name as string) as call_city_name
        ,cast(county as bigint) as call_county_id
        ,cast(county_name as string) as call_county_name
        ,cast(region as string) as region_name
        ,cast(order_status as bigint) as order_status
        ,cast(_birth_time as string) as call_time
        ,cast(cancel_time as string) as cancel_time
        ,cast(assigned_time as string) as answer_time
        ,cast(prepared_time as string) as arrive_time
        ,cast(departure_time as string) as depart_time
        ,cast(begun_time as string) as charge_time
        ,cast(finished_time as string) as finish_time
        ,cast(cast(substring(cast(TIMESTAMP_TO_MS(_birth_time, 'yyyy-MM-dd HH:mm:ss')/60 as string),0,8) as bigint)*60 as string) as call_time_minute
        ,cast(cast(substring(cast(TIMESTAMP_TO_MS(cancel_time, 'yyyy-MM-dd HH:mm:ss')/60 as string),0,8) as bigint)*60 as string) as cancel_time_minute
        ,cast(cast(substring(cast(TIMESTAMP_TO_MS(assigned_time, 'yyyy-MM-dd HH:mm:ss')/60 as string),0,8) as bigint)*60 as string) as answer_time_minute
        ,cast(cast(substring(cast(TIMESTAMP_TO_MS(prepared_time, 'yyyy-MM-dd HH:mm:ss')/60 as string),0,8) as bigint)*60 as string) as arrive_time_minute
        ,cast(cast(substring(cast(TIMESTAMP_TO_MS(departure_time, 'yyyy-MM-dd HH:mm:ss')/60 as string),0,8) as bigint)*60 as string) as depart_time_minute
        ,cast(cast(substring(cast(TIMESTAMP_TO_MS(begun_time, 'yyyy-MM-dd HH:mm:ss')/60 as string),0,8) as bigint)*60 as string) as charge_time_minute
        ,cast(cast(substring(cast(TIMESTAMP_TO_MS(finished_time, 'yyyy-MM-dd HH:mm:ss')/60 as string),0,8) as bigint)*60 as string) as finish_time_minute
        ,cast(start_dest_distance as bigint) as est_dis
        ,cast(driver_start_distance as bigint) as est_arrive_dis
        ,cast(pre_total_fee as double) as est_price_amt
        ,cast(combo_type as bigint) as combo_type
        ,cast(complete_type as bigint) as complete_type
        ,cast(airport_type as bigint) as airport_type
        ,cast(carpool_type as bigint) as carpool_type
        ,cast(compound_type as bigint) as compound_type
        ,cast(source_type as bigint) as source_type
        ,cast(driver_type as bigint) as driver_type
        ,cast(capacity_level as bigint) as capacity_level
        ,cast(level_type as bigint) as level_type
        ,cast(require_level as bigint) as require_level
        ,cast(route_type as bigint) as route_type
        ,cast(strive_car_level as bigint) as strive_car_level
        ,cast(extra_type as bigint) as extra_type
        ,cast(answer_dur as bigint) as answer_dur
        ,cast(est_dur as bigint) as est_dur
        ,cast(arrive_dur as bigint) as arrive_dur
        ,cast(cancel_dur as bigint) as cancel_dur
        ,cast(driver_wait_dur as bigint) as driver_wait_dur
        ,cast(charge_dur as bigint) as charge_dur
        ,cast(carpool_price_type as bigint) as carpool_price_type
        ,cast(is_appt_flag as bigint) as is_appt_flag    
        ,cast(is_agency_call as bigint) as is_agency_call
        ,cast(is_anycar as bigint) as is_anycar
        ,cast(is_cross_city_flag as bigint) as is_cross_city_flag
        ,cast(is_call_flag as bigint) as is_call_flag
        ,cast(cancel_type as bigint) as cancel_type
        ,cast(case when cancel_type <> '0' then 1 else 0 end as bigint) as is_cancel_flag    
        ,cast(case when cancel_type = '1' then 1 else 0 end as bigint) as is_grab_before_cannel_flag
        ,cast(case when order_status<>'6' and cancel_type = '2' then 1 else 0 end as bigint) as is_grab_after_pas_cancel_flag    
        ,cast(case when order_status<>'6' and cancel_type = '3' then 1 else 0 end as bigint) as is_grab_after_dri_cancel_flag    
        ,cast(case when order_status<>'6' and cancel_type = '4' then 1 else 0 end as bigint) as is_grab_after_srvc_cancel_flag    
        ,cast(is_reassigned_flag as bigint) as is_reassigned_flag
        ,cast(is_reassign_flag as bigint) as is_reassign_flag
        ,cast(is_answer_flag as bigint) as is_answer_flag
        ,cast(is_arrive_flag as bigint) as is_arrive_flag
        ,cast(is_begin_charge_flag as bigint) as is_begin_charge_flag
        ,cast(is_finish_flag as bigint) as is_finish_flag
        ,cast(is_openapi as bigint) as is_openapi
        ,cast(is_nirvana as bigint) as is_nirvana
        ,cast(is_ontheway as bigint) as is_ontheway
        ,cast(is_update_dest as bigint) as is_update_dest
        ,cast(is_service_agency_call as bigint) as is_service_agency_call
        ,cast(is_serial_assign as bigint) as is_serial_assign
        ,cast(is_prepay as bigint) as is_prepay
        ,estimate_id
        ,estimate_id_anycar
        ,rank_index
        ,case when (rank_index = 0 or rank_index = 9999) then 1 else 0 end is_anycar_sumtag
        ,starting_lng as call_lng
        ,starting_lat as call_lat
        ,assigned_lng as answer_lng
        ,assigned_lat as answer_lat
        ,prepared_lng as arrive_lng
        ,prepared_lat as arrive_lat
        ,begun_lng as begin_charge_lng
        ,begun_lat as begin_charge_lat
        ,finished_lng as finish_lng
        ,finished_lat as finish_lat
        ,case when is_call_flag='1' then starting_lng
              when is_answer_flag='1' then assigned_lng
              when is_arrive_flag='1' then prepared_lng
              when is_begin_charge_flag='1' then begun_lng
              when is_finish_flag='1' then finished_lng
              end as event_lng
        ,case when is_call_flag='1' then starting_lat
              when is_answer_flag='1' then assigned_lat
              when is_arrive_flag='1' then prepared_lat
              when is_begin_charge_flag='1' then begun_lat
              when is_finish_flag='1' then finished_lat
              end as event_lat
        ,distance as actual_distance
        ,case when is_call_flag='1' then start_dest_distance
              when is_answer_flag='1' then driver_start_distance
              when is_finish_flag='1' then distance
              end as event_distance 
        ,'' as extend_feature_value   
        ,case when is_finish_flag = '1' then real_distance_category
              else  estimate_distance_category 
        end distance_category
        ,cast(is_special_price as int) as is_special_price
        ,cast(is_guide_scene as int) as is_guide_scene
        ,extend_feature
        ,cast(dynamic_price as int) as dynamic_price
        ,cast(travel_id as bigint) as travel_id
        ,cast(passenger_count as bigint) as passenger_num
        ,cast(combo_id as bigint) as combo_id
        ,dest_lng
        ,dest_lat
        ,cap_price as cap_price_amt
        ,cast(spacious_car_alliance as int) as spacious_car_alliance
        ,cast(pro_id as int) as pro_id
        ,pro_name
        ,json_extend_1
        ,cast(access_key_id as int) as access_key_id
from    view_01
where channel not in ('1', '20001','1500001','1510001', '1010000001')
;{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 02:46:39 UTC 2024,,,,,,,,,,"0|z1mrwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 09:34;martijnvisser;[~Akihito Liang] Can you please include the affected versions? It appears to be 1.17.0, so please validate this with both the latest version of 1.17 and with 1.18 as well. ;;;","17/Jan/24 02:46;Akihito Liang;[~martijnvisser] The last commit of the version I am using is 839b18ea9271b9828ab7890da98b9ff2a18f371f. I have checked the latest version 1.17 and there have been no significant changes to the code split. 1.18 Maybe there's a way, I'll try it out

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JSON_QUOTE and JSON_UNQUOTE function,FLINK-34111,13564832,13076759,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jeyhunkarimov,martijnvisser,martijnvisser,16/Jan/24 09:06,03/Mar/24 23:55,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,"Escapes or unescapes a JSON string removing traces of offending characters that could prevent parsing.

Proposal:
- JSON_QUOTE: Quotes a string by wrapping it with double quote characters and escaping interior quote and other characters, then returning the result as a utf8mb4 string. Returns NULL if the argument is NULL.
- JSON_UNQUOTE: Unquotes value and returns the result as a string. Returns NULL if the argument is NULL. An error occurs if the value starts and ends with double quotes but is not a valid JSON string literal.

The following characters are reserved in JSON and must be properly escaped to be used in strings:

Backspace is replaced with \b
Form feed is replaced with \f
Newline is replaced with \n
Carriage return is replaced with \r
Tab is replaced with \t
Double quote is replaced with \""
Backslash is replaced with \\

This function exists in MySQL: 
- https://dev.mysql.com/doc/refman/8.0/en/json-creation-functions.html#function_json-quote
- https://dev.mysql.com/doc/refman/8.0/en/json-modification-functions.html#function_json-unquote

It's still open in Calcite CALCITE-3130",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 12 14:34:18 UTC 2024,,,,,,,,,,"0|z1mrw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/24 14:34;Sergey Nuyanzin;A couple of findings
1. MySQL seems not following JSON spec [1], [2] regarding escaping 
{quote}
escape
    '""'
    '\'
    '/'
    'b'
    'f'
    'n'
    'r'
    't'
    'u' hex hex hex hex
{quote}
Probably it's better to follow json spec rules
2. I would expect the {{INPUT_STRING}} as output for the query like 
{code:sql}
SELECT json_unquote(json_quote(<INPUT_STRING>));
{code}
by the way MySQL seems following this rule (it is not stated explicitly in docs however tests show that it does)


[1] https://www.json.org/json-en.html
[2]  https://www.crockford.com/mckeeman.html
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump janino to 3.1.11,FLINK-34110,13564831,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,16/Jan/24 09:04,16/Jan/24 09:22,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,Among others it fixes {{ArrayIndexOutOfBoundsException}} for unconditional loops which cannot complete normally https://github.com/janino-compiler/janino/issues/208 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-16 09:04:31.0,,,,,,,,,,"0|z1mrvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystem sink connector restore job from historical checkpoint failure,FLINK-34109,13564827,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,paryshevsergey,paryshevsergey,16/Jan/24 08:56,15/May/24 04:43,04/Jun/24 20:40,,1.12.7,1.13.6,1.14.6,1.15.4,1.16.3,1.17.2,1.18.0,,,,,Connectors / FileSystem,,,,0,pull-request-available,,,,"FileSystem connector sink with compaction setting is enabled can't restore job from historical checkpoint (when MAX_RETAINED_CHECKPOINTS > 1 and restroing checkpoint is not last)
{code:java}
java.io.UncheckedIOException: java.io.FileNotFoundException: File file:/tmp/parquet-test/output/.uncompacted-part-81340e1d-9004-4ce2-a45c-628d17919bbf-0-1 does not exist or the user running Flink ('user') has insufficient permissions to access it.
    at org.apache.flink.connector.file.table.stream.compact.CompactCoordinator.lambda$coordinate$1(CompactCoordinator.java:165) ~[classes/:?]
    at org.apache.flink.connector.file.table.BinPacking.pack(BinPacking.java:40) ~[classes/:?]
    at org.apache.flink.connector.file.table.stream.compact.CompactCoordinator.lambda$coordinate$2(CompactCoordinator.java:175) ~[classes/:?]
    at java.util.HashMap.forEach(HashMap.java:1290) ~[?:1.8.0_312]
    at org.apache.flink.connector.file.table.stream.compact.CompactCoordinator.coordinate(CompactCoordinator.java:171) ~[classes/:?]
    at org.apache.flink.connector.file.table.stream.compact.CompactCoordinator.commitUpToCheckpoint(CompactCoordinator.java:153) ~[classes/:?]
    at org.apache.flink.connector.file.table.stream.compact.CompactCoordinator.processElement(CompactCoordinator.java:143) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:262) ~[classes/:?]
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:155) ~[classes/:?]
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:114) ~[classes/:?]
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:554) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:245) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:848) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:797) ~[classes/:?]
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:954) ~[classes/:?]
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:933) ~[classes/:?]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:747) ~[classes/:?]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) ~[classes/:?]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_312]
Caused by: java.io.FileNotFoundException: File file:/tmp/parquet-test/output/.uncompacted-part-81340e1d-9004-4ce2-a45c-628d17919bbf-0-1 does not exist or the user running Flink ('user') has insufficient permissions to access it.
    at org.apache.flink.core.fs.local.LocalFileSystem.getFileStatus(LocalFileSystem.java:113) ~[classes/:?]
    at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.getFileStatus(SafetyNetWrapperFileSystem.java:65) ~[classes/:?]
    at org.apache.flink.connector.file.table.stream.compact.CompactCoordinator.lambda$coordinate$1(CompactCoordinator.java:163) ~[classes/:?]
    ... 19 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 15 04:43:23 UTC 2024,,,,,,,,,,"0|z1mruw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 08:59;paryshevsergey;[~martijnvisser] [~trohrmann] 

Hi, please, assign me to this task;;;","29/Jan/24 05:01;paryshevsergey;I guess this is same: https://lists.apache.org/thread/2c17wy7nz3jj10mh1jzgwxjhodzv4bwg;;;","15/May/24 04:43;paryshevsergey;bump;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add URL_ENCODE and URL_DECODE function,FLINK-34108,13564826,13076759,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,superdiao,martijnvisser,martijnvisser,16/Jan/24 08:54,01/Jun/24 10:34,04/Jun/24 20:40,,,,,,,,,,,,,Table SQL / API,,,,1,pull-request-available,,,,"Add URL_ENCODE and URL_DECODE function

URL_ENCODE(str) - Translates a string into 'application/x-www-form-urlencoded' format using a specific encoding scheme. 
URL_DECODE(str) - Decodes a string in 'application/x-www-form-urlencoded' format using a specific encoding scheme. 

Related ticket from Calcite: CALCITE-5825",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 12 02:33:04 UTC 2024,,,,,,,,,,"0|z1mruo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/24 02:33;superdiao;Hey, there~
If no one was assigned, please check this PR: [https://github.com/apache/flink/pull/24773];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clearer separation of unit test and integration tests,FLINK-34107,13564817,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,16/Jan/24 07:38,16/Jan/24 07:41,04/Jun/24 20:40,,1.17.2,1.18.0,1.19.0,,,,,,,,,Build System,Build System / CI,,,0,,,,,"FLINK-33907 revealed an issue in the current Maven setup where the surefire-plugin is in charge of executing both, the unit and the integration tests in two separate executions: The {{-Dtest}} system parameter is overwriting the {{<includes/>}} and {{<excludes/>}} configuration of the two executions. That results in a test being executed twice if specifically selected for execution (i.e. {{mvn [...] verify -Dtest=???}}).

That's especially problematic in cases where the ITCase relies on artifacts that are build during the Maven run (see the discussion in FLINK-33907 where {{DefaultPackagedProgramRetrieverITCase}} relies on {{flink-client-test-utils}} to provide test jars). The artifacts should be moved in the {{pre-integration-test}} phase from a conceptual point of view.

A workaround for this issue is to directly call the execution {{surefire:test@integration-tests}}:
{code}
mvn -pl flink-clients surefire:test@integration-tests -Dtest=DefaultPackagedProgramRetrieverITCase -Dfast
{code}
This approach has the flow that the Maven lifecycle is ignored. You would have to do a {{mvn -pl flink-clients clean verify}} run upfront to build all the artifacts the test relies on. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 07:41:19 UTC 2024,,,,,,,,,,"0|z1mrso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 07:41;mapohl;One approach here could be to use the [failsafe plugin|https://maven.apache.org/surefire/maven-failsafe-plugin/] for the integration test execution. This wasn't done so far to keep the number of plugins in the Flink build low (it was conveniently enough to have the test executions being covered by the surefire plugin).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
user defined source parallelism in ddl can't work if connector ScanRuntimeProvider is SourceFunctionProvider,FLINK-34106,13564804,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,Akihito Liang,Akihito Liang,16/Jan/24 06:39,16/Jan/24 08:36,04/Jun/24 20:40,16/Jan/24 08:36,,,,,,,,,,,,,,,,0,,,,,I implemented a connector using the SourceFunctionProvider myself and found that setting scan parallelism is not effective.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Tue Jan 16 07:54:13 UTC 2024,,,,,,,,,,"0|z1mrps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 06:45;Akihito Liang;I have fixed the issue, please assign it to me~;;;","16/Jan/24 07:54;Zhanghao Chen;[~Akihito Liang] Are you refering to the new API introduced in [FLIP-367|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=263429150]? If so, it is still undergoing, but we'll make it in Flink 1.19. Please subscribe  FLINK-33261 for the latest progress;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Akka timeout happens in TPC-DS benchmarks,FLINK-34105,13564801,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsdy,zhuzh,zhuzh,16/Jan/24 06:00,06/Mar/24 13:35,04/Jun/24 20:40,06/Mar/24 13:35,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,,,,,"We noticed akka timeout happens in 10TB TPC-DS benchmarks in 1.19. The problem did not happen in 1.18.0.
After bisecting, we find the problem was introduced in FLINK-33532.

 !image-2024-01-16-13-59-45-556.png|width=800! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33532,,,,,,,,,,,,"16/Jan/24 05:59;zhuzh;image-2024-01-16-13-59-45-556.png;https://issues.apache.org/jira/secure/attachment/13066010/image-2024-01-16-13-59-45-556.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 13:35:47 UTC 2024,,,,,,,,,,"0|z1mrp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 06:01;zhuzh;[~lsdy] [~guoyangze] would you help to take a look?

This indicates a problem that existing large Flink batch jobs may become unstable in 1.19.;;;","16/Jan/24 10:13;guoyangze;Thanks for the pointer, [~zhuzh]. How does the pekko.framesize configure in this cluster?;;;","16/Jan/24 12:05;zhuzh;[~guoyangze] pekko.framesize is not configured and the default value should have been used.
Here are all the configuration:
```
parallelism.default: 1500
slotmanager.number-of-slots.max: 1500
taskmanager.numberOfTaskSlots: 10
jobmanager.memory.process.size: 24000m
taskmanager.memory.process.size: 24000m
resourcemanager.taskmanager-timeout: 900000
taskmanager.memory.network.fraction: 0.2
cluster.evenly-spread-out-slots: true

table.optimizer.join-reorder-enabled: true
table.optimizer.join.broadcast-threshold: 10485760
table.exec.operator-fusion-codegen.enabled: true
```;;;","16/Jan/24 13:25;guoyangze;[~zhuzh] My gut feeling is that the change leads to payload overhead. Could you adjust the pekko.framesize to 1g and rerun the benchmark?

BTW, the default value of pekko.framesize might be too small. AFAIK, many giants like Alibaba and Bytedance would increase it in their production environments. WDYT?;;;","16/Jan/24 13:32;guoyangze;We'll try to reproduce this issue in our environment.;;;","17/Jan/24 02:11;zhuzh;We tried increasing {{pekko.ask.timeout}} to {{1min}}(from the default {{10s}}), and the problem did not happen again.
So I guess it's not related to the framesize.;;;","17/Jan/24 03:17;guoyangze;[~zhuzh] I encountered the same Akka timeout issue in my testing with a two-level join job using the same concurrency configuration. Adjusting pekko.ask.timeout indeed resolved this problem.

I believe the root cause of this issue is that we moved the serialization and compression of ShuffleDescriptorGroup from the RPC main thread to Akka's serialization thread. The time spent on this operation is included in the process monitored by pekko.ask.timeout. Personally speaking, I consider this as an optimization rather than a problem for users because the serialization thread is pooled, allowing parallelization of the serialization and compression process. Otherwise, each compression are executed sequentially in the main thread. This change will speed up job deployment, although for very large jobs, users may need to manually adjust the configuration. WDYT?;;;","17/Jan/24 05:42;zhuzh;I think it is a regression because existing jobs can become unstable.
Is it possible that we use a thread pool to do parallelized serialization before conducting task submission, so that it will not be counted as part of pekko RPC process?;;;","18/Jan/24 05:23;lsdy;[~zhuzh] Great suggestion. We've tried to move the serialization of shuffleDescriptor logic on the JobMaster side to the futureExecutor for asynchronous serialization. Moving the deserialization to a separate thread pool on the TM side would disrupt the original synchronous logic of the submission interface, potentially introducing additional risks. Not implementing serialization modifications on the TM side results in about a 30% performance degradation in our tests under OLAP scenarios. Furthermore, we plan to advance batch submission optimizations for Task submission stages in OLAP scenarios. We intend to test the asynchronous serialization optimization internally, as its performance is roughly consistent with placing it in the Akka remote thread pool. Therefore, for this fix, we plan to move the serialization operation on the Jobmaster side to an asynchronous thread pool, while keeping the deserialization on the TM side back on the main thread. WDYT？;;;","19/Jan/24 04:50;zhuzh;[~lsdy] Sounds good to me. Feel free to open a PR for it.;;;","29/Jan/24 13:00;zhuzh;Hi [~lsdy], what's the status of the fix?;;;","30/Jan/24 02:46;guoyangze;[~lsdy] has some personal affairs and will back this Thursday.;;;","30/Jan/24 02:54;zhuzh;Thanks for the updates! [~guoyangze];;;","06/Mar/24 09:10;guoyangze;master: 7a709bf;;;","06/Mar/24 13:35;zhuzh;1.19: 837f8e584850bdcbc586a54f58e3fe953a816be8;;;",,,,,,,,,,,,,,,,,,
Improve the ScalingReport format of autoscaling,FLINK-34104,13564800,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,16/Jan/24 05:51,14/Mar/24 13:56,04/Jun/24 20:40,14/Mar/24 13:56,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,,"Currently, the scaling report format is 

{color:#6a8759} Vertex ID %s | Parallelism %d -> %d | Processing capacity %.2f -> %.2f | Target data rate %.2f{color}

{color:#172b4d}It has 2 disadvantages:{color}
 # {color:#172b4d}When one job has multiple vertices, the report of all vertices are mixed together without any separator{color}{color:#172b4d}, here is an example:{color}
 ** {color:#172b4d}Scaling execution enabled, begin scaling vertices: Vertex ID ea632d67b7d595e5b851708ae9ad79d6 | Parallelism 2 -> 1 | Processing capacity 800466.67 -> 320186.67 | Target data rate 715.10 Vertex ID bc764cd8ddf7a0cff126f51c16239658 | Parallelism 2 -> 1 | Processing capacity 79252.08 -> 31700.83 | Target data rate 895.93 Vertex ID 0a448493b4782967b150582570326227 | Parallelism 8 -> 16 | Processing capacity 716.05 -> 1141.00 | Target data rate 715.54{color}
 ** {color:#172b4d}We can see the Vertex ID is the beginning of each vertex report, it doesn't have any {color}{color:#172b4d}separator with the last vertex.{color}
 # {color:#172b4d}This format is non-standard{color}{color:#172b4d}, it's hard to deserialize.{color}
 ** {color:#172b4d}When job enables the autoscaler and disable the scaling.{color}
 ** {color:#172b4d}Flink platform maintainer wants to show the scaling report in WebUI, it's helpful to using the report result for flink users.{color}
 ** {color:#172b4d}So easy to deserialize is useful for these flink platform.{color}

h2. {color:#172b4d}Solution:{color}
 * {color:#172b4d}Adding the {{{}} and {{}}} as the separator between multiple vertices. {color}
 * {color:#172b4d}Adding the {{AutoscalerEventUtils}} to easy deserialize the {{ScalingReport}} message.{color}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 02:28:38 UTC 2024,,,,,,,,,,"0|z1mrow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 02:28;fanrui;Merged to main(1.8.0) via : 66808eb9f2d7d26dcd755fec7fdab5ccaf75f8bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncIO example failed to run as DataGen Connector is not bundled,FLINK-34103,13564799,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,16/Jan/24 05:43,30/Jan/24 01:43,04/Jun/24 20:40,30/Jan/24 01:43,1.18.0,,,,,,,1.19.0,,,,Examples,,,,0,pull-request-available,,,,"From the comments of FLINK-32821:


root@73186f600374:/opt/flink# bin/flink run /volume/flink-examples-streaming-1.18.0-AsyncIO.jar
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
java.lang.NoClassDefFoundError: org/apache/flink/connector/datagen/source/DataGeneratorSource
at org.apache.flink.streaming.examples.async.AsyncIOExample.main(AsyncIOExample.java:82)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.base/java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105)
at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.connector.datagen.source.DataGeneratorSource
at java.base/java.net.URLClassLoader.findClass(Unknown Source)
at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:67)
at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:65)
at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:51)
at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
... 15 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32821,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 01:43:53 UTC 2024,,,,,,,,,,"0|z1mroo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 01:43;Weijie Guo;master(1.19): 3b4fa6c424b713398fbce21f5dfbfd0053231a87.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid configuration when using 'env.log.max' on yarn application mode,FLINK-34102,13564798,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,RocMarshal,RocMarshal,16/Jan/24 04:59,17/Jan/24 02:15,04/Jun/24 20:40,17/Jan/24 02:15,1.19.0,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 02:15:36 UTC 2024,,,,,,,,,,"0|z1mrog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 02:15;fanrui;Merged to master(1.19) via: a20d57b18bbab93e943d0b64ffb993382697d242;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"use flink ui upload jar error, message is 'Exactly 1 file must be sent, received 0'",FLINK-34101,13564791,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,blackpighe,blackpighe,16/Jan/24 03:35,21/Feb/24 12:06,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,Runtime / REST,Runtime / Web Frontend,,,0,,,,,"use flink ui upload jar, Something went wrong.

 

!image-2024-01-16-11-32-50-015.png!

message is :
{code}
Server Response:org.apache.flink.runtime.rest.handler.RestHandlerException: Exactly 1 file must be sent, received 0. at org.apache.flink.runtime.webmonitor.handlers.JarUploadHandler.handleRequest(JarUploadHandler.java:80) 
    at org.apache.flink.runtime.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:83) at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196) 
    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83) at java.util.Optional.ifPresent(Optional.java:159) at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45) 
    at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80) at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49) 
    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) 
    at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) 
    at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) 
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) 
    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:203) at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) 
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) 
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) 
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:336) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:308) 
    at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) 
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) 
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) 
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) 
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) 
    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) 
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at java.lang.Thread.run(Thread.java:750)
{code} 

I am very confused as to why this error is reported here and why this check error is.

 triggered !image-2024-01-16-11-35-21-582.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/24 03:32;blackpighe;image-2024-01-16-11-32-50-015.png;https://issues.apache.org/jira/secure/attachment/13066008/image-2024-01-16-11-32-50-015.png","16/Jan/24 03:35;blackpighe;image-2024-01-16-11-35-21-582.png;https://issues.apache.org/jira/secure/attachment/13066007/image-2024-01-16-11-35-21-582.png","16/Jan/24 03:37;blackpighe;image-2024-01-16-11-37-26-343.png;https://issues.apache.org/jira/secure/attachment/13066009/image-2024-01-16-11-37-26-343.png",,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 12:06:43 UTC 2024,,,,,,,,,,"0|z1mrmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 03:39;blackpighe; 

!image-2024-01-16-11-37-26-343.png!

this is copy curl request by web console：

 

curl ""http://localhost:8083/proxy/ingress/ui/1/224/jars/upload"" ^
  -H ""Accept: application/json, text/plain, */*"" ^
  -H ""Accept-Language: zh-CN,zh;q=0.9"" ^
  -H ""Cache-Control: no-cache"" ^
  -H ""Connection: keep-alive"" ^
  -H ""Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryl6QGLTdn1N5xv5zc"" ^
  -H ""Origin: http://localhost:8083"" ^
  -H ""Pragma: no-cache"" ^
  -H ""Referer: http://localhost:8083/proxy/ingress/ui/1/224/"" ^
  -H ""Sec-Fetch-Dest: empty"" ^
  -H ""Sec-Fetch-Mode: cors"" ^
  -H ""Sec-Fetch-Site: same-origin"" ^
  -H ""User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"" ^
  -H ""sec-ch-ua: ^\^""Not_A Brand^\^"";v=^\^""8^\^"", ^\^""Chromium^\^"";v=^\^""120^\^"", ^\^""Google Chrome^\^"";v=^\^""120^\^"""" ^
  -H ""sec-ch-ua-mobile: ?0"" ^
  -H ""sec-ch-ua-platform: ^\^""Windows^\^"""" ^
  --data-raw ^""------WebKitFormBoundaryl6QGLTdn1N5xv5zc^

Content-Disposition: form-data; name=^\^""jarfile^\^""; filename=^\^""faas-flink-1.14.3_2.12-20481024-SNAPSHOT-session.jar^\^""^

Content-Type: application/octet-stream^

^

^

------WebKitFormBoundaryl6QGLTdn1N5xv5zc--^

^"" ^
  --compressed;;;","21/Feb/24 12:06;mapohl;Hi [~blackpighe], thanks for reporting this issue. Is this reproducible or could it be an issue due to some networking problems (even though I would expect an IOException somewhere in that case)?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support session window table function without pulling up with window agg ,FLINK-34100,13564788,13335153,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,16/Jan/24 02:56,31/Jan/24 07:45,04/Jun/24 20:40,25/Jan/24 15:39,,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,This subtask resolves the session support in ExecWindowTableFunction. And then the session window can support window agg with WindowAttachedWindowingStrategy.,,,,,,,,,,,,FLINK-34048,,,,,,,,,,,,,,,,,,,,,,FLINK-34315,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 15:38:44 UTC 2024,,,,,,,,,,"0|z1mrm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 15:38;lsy;Merged in master: 65f022853151c79a67c387147633fecfe8e838e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointIntervalDuringBacklogITCase.testNoCheckpointDuringBacklog is unstable on AZP,FLINK-34099,13564786,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Yu Chen,Yu Chen,16/Jan/24 02:21,09/Apr/24 03:25,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,Build System / CI,,,,0,,,,,"This build [Pipelines - Run 20240115.30 logs (azure.com)|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56403&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba] fails as 
{code:java}
Jan 15 18:29:51 18:29:51.938 [ERROR] org.apache.flink.test.checkpointing.CheckpointIntervalDuringBacklogITCase.testNoCheckpointDuringBacklog -- Time elapsed: 2.022 s <<< FAILURE!
Jan 15 18:29:51 org.opentest4j.AssertionFailedError: 
Jan 15 18:29:51 
Jan 15 18:29:51 expected: 0
Jan 15 18:29:51  but was: 1
Jan 15 18:29:51 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Jan 15 18:29:51 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Jan 15 18:29:51 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Jan 15 18:29:51 	at org.apache.flink.test.checkpointing.CheckpointIntervalDuringBacklogITCase.testNoCheckpointDuringBacklog(CheckpointIntervalDuringBacklogITCase.java:141)
Jan 15 18:29:51 	at java.lang.reflect.Method.invoke(Method.java:498)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 03:24:43 UTC 2024,,,,,,,,,,"0|z1mrls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 07:48;mapohl;https://github.com/apache/flink/actions/runs/7763815403/job/21176583259#step:10:7652;;;","09/Apr/24 03:24;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58794&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9404;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not enough Azure Pipeline CI runners available?,FLINK-34098,13564758,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jeyhunkarimov,mapohl,mapohl,15/Jan/24 16:41,23/Jan/24 20:54,04/Jun/24 20:40,23/Jan/24 20:54,1.17.2,1.18.0,1.19.0,,,,,,,,,Build System / CI,,,,0,,,,,CI takes longer than usual. There might be an issue with the runner pool (on the Alibaba VMs)?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34155,,FLINK-34135,FLINK-34141,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 20:54:25 UTC 2024,,,,,,,,,,"0|z1mrfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 16:43;mapohl;[~lincoln] [~jingge] [~yunta] Can one of you check whether there's something wrong with the Alibaba machines? It feels like there are not enough runners available that can pick up jobs.;;;","16/Jan/24 01:59;lincoln.86xy;[~jingge] Can you help check it?;;;","16/Jan/24 22:38;jingge;checking;;;","18/Jan/24 16:10;jeyhunkarimov;Hi [~mapohl] [~jingge] it was a disk-full issue. I reported the recurring error (that leads to the big maven log file) in [FLINK-34155|https://issues.apache.org/jira/browse/FLINK-34155];;;","23/Jan/24 17:06;mapohl;Same here: We could close this issue if you think that the runner queue is up-to-speed again. it's easier to check for you because you have access to the Azure Pipeline management console.;;;","23/Jan/24 20:53;jeyhunkarimov;Hi [~mapohl] yes, we can close.;;;","23/Jan/24 20:54;jeyhunkarimov;It was a disk-full issue. I reported the recurring error (that leads to the big maven log file) in FLINK-34155;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused JobMasterGateway#requestJobDetails,FLINK-34097,13564747,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Jan/24 15:20,17/Jan/24 09:20,04/Jun/24 20:40,17/Jan/24 09:20,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"This method is wired all the way to the scheduler; remove it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 09:20:13 UTC 2024,,,,,,,,,,"0|z1mrd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 09:20;chesnay;master: 1ffb481111f658b699702357921a48e914d13caf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upscale of kafka source operator leads to some splits getting lost,FLINK-34096,13564746,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,yang,yang,15/Jan/24 14:58,16/Jan/24 19:05,04/Jun/24 20:40,16/Jan/24 19:05,1.18.0,,,,,,,,,,,,,,,0,,,,,"Hello,

We've been conducting experiments with Autoscaling in Apache Flink version 1.18.0 and encountered a bug associated with the Kafka source split.

The issue manifested in our system as follows: upon experiencing a sudden spike in traffic, the autoscaler opted to upscale the Kafka source vertex. However, the Kafka source fetcher failed to retrieve all available Kafka partitions. Additionally, we observed duplication in source splits. For example, taskmanager-1 and taskmanager-4 both fetched the same Kafka partition.

!image-2024-01-15-15-46-47-104.png|width=423,height=381!

!image-2024-01-15-15-48-07-871.png|width=719,height=329!
{noformat}
taskmanager-9 2024-01-09 17:59:37 [Source: kafka_source_input_with_kt -> Flat Map -> session_valid (5/18)#6] INFO  o.a.f.c.b.s.r.SourceReaderBase - Adding split(s) to reader: [[Partition: sf-enriched-4, StartingOffset: 26084169, StoppingOffset: -9223372036854775808], [Partition: sf-anonymized-8, StartingOffset: 46477069, StoppingOffset: -9223372036854775808], [Partition: sf-anonymized-9, StartingOffset: 46121324, StoppingOffset: -9223372036854775808], [Partition: sf-enriched-5, StartingOffset: 26221751, StoppingOffset: -9223372036854775808]] 

taskmanager-6 2024-01-09 17:59:37 [Source: kafka_source_input_with_kt -> Flat Map -> session_valid (4/18)#6] INFO  o.a.f.c.b.s.r.SourceReaderBase - Adding split(s) to reader: [[Partition: sf-enriched-4, StartingOffset: 26084169, StoppingOffset: -9223372036854775808], [Partition: sf-anonymized-8, StartingOffset: 46477069, StoppingOffset: -9223372036854775808], [Partition: sf-anonymized-20, StartingOffset: 46211745, StoppingOffset: -9223372036854775808], [Partition: sf-anonymized-32, StartingOffset: 46340878, StoppingOffset: -9223372036854775808]] {noformat}

You can see in these logs, taskmanager-9 and taskmanager-6 has both fetched partition sf-enriched-4 and sf-anonymized-8







Additional Questions
 * During some other experiments which also lead to kafka partition issues, we noticed that the autoscaler attempted to increase the parallelism of the source vertex to a value that is not a divisor of the Kafka topic's partition count. For example, it recommended a parallelism of 48 when the total partition count was 72. In such scenarios: 

 * 
 ** Does kafka source connector vertex still suppose to works well when its parallelism is not divisor of topic's partition count?
 ** If this configuration is not ideal, should there be a mechanism within the autoscaler to ensure that the parallelism of the source vertex always matches the topic's partition count?

 

 

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34063,,,,,,,,,,,,,,,"16/Jan/24 10:56;yang;global-configuration-log.txt;https://issues.apache.org/jira/secure/attachment/13066022/global-configuration-log.txt","15/Jan/24 14:46;yang;image-2024-01-15-15-46-47-104.png;https://issues.apache.org/jira/secure/attachment/13065992/image-2024-01-15-15-46-47-104.png","15/Jan/24 14:47;yang;image-2024-01-15-15-47-36-509.png;https://issues.apache.org/jira/secure/attachment/13065991/image-2024-01-15-15-47-36-509.png","15/Jan/24 14:48;yang;image-2024-01-15-15-48-07-871.png;https://issues.apache.org/jira/secure/attachment/13065990/image-2024-01-15-15-48-07-871.png","16/Jan/24 10:56;yang;source-split-log.txt;https://issues.apache.org/jira/secure/attachment/13066023/source-split-log.txt","16/Jan/24 10:56;yang;substate-log.txt;https://issues.apache.org/jira/secure/attachment/13066024/substate-log.txt",,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 18:43:27 UTC 2024,,,,,,,,,,"0|z1mrcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 15:02;martijnvisser;Is https://issues.apache.org/jira/browse/FLINK-34063 applicable to your situation?;;;","15/Jan/24 15:03;martijnvisser;I see that this is a follow-up ticket already; I've not marked it as a Blocker, that requires some more insights first. ;;;","15/Jan/24 15:06;yang;Hello [~martijnvisser] , https://issues.apache.org/jira/browse/FLINK-34063 is similar to my situation, but the difference is that I didn't use snapshot compression feature, for me it's more a bug of kafka source connector. ;;;","15/Jan/24 15:09;isburmistrov;> Does kafka source connector vertex still suppose to works well when its parallelism is not divisor of topic's partition count?

Yes. [This|https://github.com/apache/flink/blob/b666de974b5dbc6e0d6710baececb1d6ede9b769/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/RoundRobinOperatorStateRepartitioner.java#L291] is the logic responsible for redistributing splits to source tasks, and it does take into account that number of tasks may not be the divisor of total partitions.;;;","15/Jan/24 15:12;isburmistrov;[~yang] can you enable debug logging as was done in https://issues.apache.org/jira/browse/FLINK-34063 and search for the logs from {{org.apache.flink.runtime.state.TaskStateManagerImpl }}starting from ""Operator xxxxx has remote state SubtaskState..."" (you can see an example in the ticket). Curious to see how split offsets looked like in this case.;;;","15/Jan/24 16:37;yang;OK [~isburmistrov] , I'll do it;;;","15/Jan/24 18:16;dmvk;If you can share the whole log (it includes the cluster configuration), that would be great.;;;","16/Jan/24 11:02;yang;Hello [~dmvk] [~isburmistrov] I have uploaded following logs after a succeed re-production of kafka partition issues manuelly this time

1. logs of souce split : [^source-split-log.txt]
2. logs of remote state SubtaskState [^substate-log.txt]
3. global configuration [^global-configuration-log.txt]

Env conf:
flink 18 + autoscaler
source vertex parallelism 12
following vertex parallelism 18
kafka partition 72

Action to produce this scenario 
    1. scaling source vertex parallelism from 12 to 48 manuelly via scale button on flink UI
    2. scaled from 5 taskmanagers to 8 taskmanagers

You can see we can split for partition sf-anonymized-13 duplicated for taskmanager 9, 11, 10, 6;;;","16/Jan/24 11:41;isburmistrov;[~yang] 

See those lines



 
{code:java}
taskmanager-1-9 2024-01-16 09:15:11 [Source: kafka_source_input_with_kt -> Flat Map -> session_valid (22/48)#1] DEBUG o.a.f.r.s.TaskStateManagerImpl - Operator 4f63a01ea9a1672597497583a99872cb has remote state SubtaskState{operatorStateFromBackend=StateObjectCollection{[OperatorStateHandle{stateNameToPartitionOffsets={SourceReaderState=StateMetaInfo{offsets=[244, 244], distributionMode=SPLIT_DISTRIBUTE}}, delegateStateHandle=File State: s3p://flink-storage-eu-west-1/dev/flink/project/Zg/44f3388604c7fc6280856a1d18499d92/chk-80/b592aca5-6960-4348-8f92-22986390e3c1 [388 bytes]}]}, operatorStateFromStream=StateObjectCollection{[]}, keyedStateFromBackend=StateObjectCollection{[]}, keyedStateFromStream=StateObjectCollection{[]}, inputChannelState=StateObjectCollection{[]}, resultSubpartitionState=StateObjectCollection{[]}, stateSize=388, checkpointedSize=388} from job manager and local state alternatives [] from local state store org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@426bbeb5.{code}

We see here the very same issue as described in https://issues.apache.org/jira/browse/FLINK-34063 : offsets are wrong: [244, 244] (they must be different). It feels to me that snapshot compression is used somehow. Can it be configured in the code of the job, not in the flink-conf? 
Not sure what kind of logs would help to confirm / deny this here, maybe [~dmvk] knows.

 ;;;","16/Jan/24 17:32;dmvk;What version of the Kafka connector are you using?;;;","16/Jan/24 18:17;dmvk;> It feels to me that snapshot compression is used somehow. Can it be configured in the code of the job, not in the flink-conf?

It's part of the execution config, so it's configured on per job basis.;;;","16/Jan/24 18:25;isburmistrov;> It's part of the execution config, so it's configured on per job basis.

 

Yes I know, sorry for the confusion. My question was to [~yang] could it be that for this particular job it was configured explicitly in the execution config. Not sure if it's clear from the logs provided ;;;","16/Jan/24 18:43;yang;Hello [~dmvk] and [~isburmistrov] and [~martijnvisser] , sorry for the late answer. after re-check and re-check into my application code, Indeed I have activated snapshot compression, 



{code:java}
//  env.getConfig.setUseSnapshotCompression(true) {code}
Sorry for the noise, we can says this one is the duplicate of https://issues.apache.org/jira/browse/FLINK-34063 ;;;",,,,,,,,,,,,,,,,,,,,
Add a restore test for StreamExecAsyncCalc,FLINK-34095,13564745,13563506,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,asheinberg,twalthr,twalthr,15/Jan/24 14:48,06/Feb/24 17:55,04/Jun/24 20:40,06/Feb/24 17:55,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,StreamExecAsyncCalc needs at least one restore test to check whether the node can be restored from a CompiledPlan and whether restore from a savepoint works.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 17:55:11 UTC 2024,,,,,,,,,,"0|z1mrco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 17:55;twalthr;Fixed in master: 4264c30d367c1847d702ea5438da64876559a95d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document new AsyncScalarFunction,FLINK-34094,13564744,13563506,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,asheinberg,twalthr,twalthr,15/Jan/24 14:46,11/Mar/24 12:43,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,Documentation,,,,0,pull-request-available,,,,Write documentation in the user-defined functions page. Maybe summarizing the behavior of async calls in general. We could also think about add a REST API example in flink-table-examples?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-15 14:46:48.0,,,,,,,,,,"0|z1mrcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RetryingRegistrationTest.testRetryConnectOnFailure failed,FLINK-34093,13564735,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,15/Jan/24 14:01,15/Jan/24 14:01,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Runtime / Coordination,,,,0,github-actions,test-stability,,,"https://github.com/XComp/flink/actions/runs/7509676054/job/20447250709#step:10:9131

{code}
Error: 03:46:44 03:46:44.483 [ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.965 s <<< FAILURE! -- in org.apache.flink.runtime.registration.RetryingRegistrationTest
Error: 03:46:44 03:46:44.484 [ERROR] org.apache.flink.runtime.registration.RetryingRegistrationTest.testRetryConnectOnFailure -- Time elapsed: 0.213 s <<< FAILURE!
Jan 13 03:46:44 java.lang.AssertionError: The registration should have failed the first time. Thus the duration should be longer than at least a single error delay.
Jan 13 03:46:44 	at org.apache.flink.runtime.registration.RetryingRegistrationTest.testRetryConnectOnFailure(RetryingRegistrationTest.java:185)
Jan 13 03:46:44 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Jan 13 03:46:44 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Jan 13 03:46:44 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
Jan 13 03:46:44 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
Jan 13 03:46:44 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
Jan 13 03:46:44 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
Jan 13 03:46:44 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
Jan 13 03:46:44 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-15 14:01:44.0,,,,,,,,,,"0|z1mrag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyEpollITCase#testNettyEpoll on arrch64 running fail,FLINK-34092,13564724,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mr_baixs,mr_baixs,15/Jan/24 12:35,15/Jan/24 12:55,04/Jun/24 20:40,15/Jan/24 12:38,1.16.2,,,,,,,,,,,Tests,,,,0,,,,,"run flink-test[1] on aarch64 machine in the flink root dir using command {{mvn test -pl flink-tests -am -Dtest=org.apache.flink.test.runtime.NettyEpollITCase#testNettyEpoll -DfailIfNoTests=false - Dcheckstyle.skip -dtestNettyEpoll -DfailIfNoTests=false -Dcheckstyle.skip Dcheckstyle.skip}}
{code:java}
ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.071 s <<< FAILURE! - in org.apache.flink.test.runtime.NettyEpollITCase
[ERROR] org.apache.flink.test.runtime.NettyEpollITCase.testNettyEpoll  Time elapsed: 2.048 s  <<< ERROR!
java.lang.UnsatisfiedLinkError: failed to load the required native library
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Epoll.ensureAvailability(Epoll.java:81)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.<clinit>(EpollEventLoop.java:51)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:185)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:36)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:84)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:60)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:49)
    at org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:59)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:113)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:100)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:77)
    at org.apache.flink.runtime.io.network.netty.NettyClient.initEpollBootstrap(NettyClient.java:164)
    at org.apache.flink.runtime.io.network.netty.NettyClient.init(NettyClient.java:79)
    at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.start(NettyConnectionManager.java:87)
    at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.start(NettyShuffleEnvironment.java:357)
    at org.apache.flink.runtime.taskexecutor.TaskManagerServices.fromConfiguration(TaskManagerServices.java:293)
    at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManager(TaskManagerRunner.java:626)
    at org.apache.flink.runtime.minicluster.MiniCluster.startTaskManager(MiniCluster.java:753)
    at org.apache.flink.runtime.minicluster.MiniCluster.startTaskManagers(MiniCluster.java:734)
    at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:452)
    at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:234)
    at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:109)
    at org.apache.flink.test.util.MiniClusterWithClientResource.before(MiniClusterWithClientResource.java:64)
    at org.apache.flink.test.runtime.NettyEpollITCase.trySetUpCluster(NettyEpollITCase.java:83)
    at org.apache.flink.test.runtime.NettyEpollITCase.testNettyEpoll(NettyEpollITCase.java:50)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
    at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
    at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
    at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
    at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
    at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Caused by: java.lang.UnsatisfiedLinkError: could not load a native library: org_apache_flink_shaded_netty4_netty_transport_native_epoll_aarch_64
    at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:223)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:306)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.<clinit>(Native.java:85)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:40)
    ... 67 more
    Suppressed: java.lang.UnsatisfiedLinkError: could not load a native library: org_apache_flink_shaded_netty4_netty_transport_native_epoll
        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:223)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:309)
        ... 69 more
    Caused by: java.io.FileNotFoundException: META-INF/native/liborg_apache_flink_shaded_netty4_netty_transport_native_epoll.so
        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:170)
        ... 70 more
        Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_transport_native_epoll in java.library.path
            at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
            at java.lang.Runtime.loadLibrary0(Runtime.java:843)
            at java.lang.System.loadLibrary(System.java:1136)
            at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
            at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:301)
            at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136)
            ... 70 more
            Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_transport_native_epoll in java.library.path
                at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
                at java.lang.Runtime.loadLibrary0(Runtime.java:843)
                at java.lang.System.loadLibrary(System.java:1136)
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:498)
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:335)
                at java.security.AccessController.doPrivileged(Native Method)
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:327)
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:293)
                ... 71 more
Caused by: java.io.FileNotFoundException: META-INF/native/liborg_apache_flink_shaded_netty4_netty_transport_native_epoll_aarch_64.so
    at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:170)
    ... 70 more
    Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_transport_native_epoll_aarch_64 in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
        at java.lang.Runtime.loadLibrary0(Runtime.java:843)
        at java.lang.System.loadLibrary(System.java:1136)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:301)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136)
        ... 70 more
        Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_transport_native_epoll_aarch_64 in java.library.path
            at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
            at java.lang.Runtime.loadLibrary0(Runtime.java:843)
            at java.lang.System.loadLibrary(System.java:1136)
            at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
            at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:335)
            at java.security.AccessController.doPrivileged(Native Method)
            at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:327)
            at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:293)
            ... 71 more {code}
In addition to this, I've also tested myself with demos that show that netty's epoll doesn't work on arrch64 machines
{code:java}
import io.netty.channel.epoll.Epoll;

public class test {
    public static void main(String[] args) {
        System.out.println(""Epoll is available:"" + Epoll.isAvailable());
    }
}
/* <dependency>
            <groupId>io.netty</groupId>
            <artifactId>netty-all</artifactId>
            <version>4.1.104.Final</version>
        </dependency>
*/
//netty version is 4.1.104.Final{code}
!image-2024-01-15-20-36-25-511.png!

So I found that the reason the org.apache.flink.test.runtime.NettyEpollITCase#testNettyEpoll test method doesn't pass is that version 4.1.70 of netty doesn't work with epoll on arrch64
h3. solution

I think there are two solutions right now, the first one is to update the version of netty, but I found that in that demo example above where I introduced the latest version of netty, the result of Epoll.isAvailable() on arsch64 is also false.
The second option is to modify org.apache.flink.test.runtime.NettyEpollITCase#testNettyEpoll test method, add a new assertion for determining whether netty's epoll is available or not, if it is available then run the test, if it is unavailable then skip the test method, so that even if in the If it is not available, the test method will be skipped, so that even if the netty version is upgraded later, the test method will not be affected.

 

If this is an issue, I hope that it would be helpful to improve Flink and if I have a chance, I want to fix it!

 
{code:java}
public void testNettyEpoll() throws Exception {
    // my add code
    Assume.assumeTrue(Epoll.isAvailable());
    MiniClusterWithClientResource cluster = trySetUpCluster();
    try {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(NUM_TASK_MANAGERS);

        DataStream<Integer> input = env.fromElements(1, 2, 3, 4, 1, 2, 3, 42);
        input.keyBy(
                        new KeySelector<Integer, Integer>() {
                            @Override
                            public Integer getKey(Integer value) throws Exception {
                                return value;
                            }
                        })
                .sum(0)
                .print();

        env.execute();
    } finally {
        cluster.after();
    }
} {code}
 

 

 

 ","flink 1.16.2
arrch64",,,,,,,,,,,,,,,,,,,,,,FLINK-33417,,,,,,,,,,,,,,,,,,,"15/Jan/24 12:36;mr_baixs;image-2024-01-15-20-36-25-511.png;https://issues.apache.org/jira/secure/attachment/13065988/image-2024-01-15-20-36-25-511.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 12:38:30 UTC 2024,,,,,,,,,,"0|z1mr80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 12:38;martijnvisser;This has already been addressed in newer Flink versions, please upgrade. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink pulsar connect add automatic failover capability,FLINK-34091,13564723,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,1012293987,1012293987,15/Jan/24 12:32,15/Apr/24 09:55,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,,"For some business scenarios with high SLA support capabilities, we need to support automated cross cluster disaster recovery capabilities. However, the current flick pulsar connect does not have this disaster tolerance capability.

We hope to support the following scenarios:
1. master slave hot standby;
2. double live (It can support double write or a cluster sharing half of the traffic.);

3.Multi activity disaster recovery;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 09:55:05 UTC 2024,,,,,,,,,,"0|z1mr7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 09:55;syhily;Since this is a requirement from Pulsar side. I don't think connector should care about such huge requirements. The connector only wraps the pulsar client and expose all its available configuration options.

BTW, pr is welcome for adding such big feature. The pulsar sink has support the geo replication. https://github.com/apache/flink-connector-pulsar/blob/9f4b902c2a478d0105eec1e32bac3ea40f318d00/flink-connector-pulsar/src/main/java/org/apache/flink/connector/pulsar/sink/writer/message/PulsarMessageBuilder.java#L116 Maybe you can check it and add the support in the source side.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce SerializerConfig for serialization,FLINK-34090,13564711,13564005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,zjureel,zjureel,15/Jan/24 11:03,23/Jan/24 01:56,04/Jun/24 20:40,23/Jan/24 01:56,1.19.0,,,,,,,1.19.0,,,,API / Type Serialization System,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 01:56:47 UTC 2024,,,,,,,,,,"0|z1mr54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 15:30;Zhanghao Chen;Hi [~zjureel], I'd be willing to take it, could you assign it to me?;;;","23/Jan/24 01:56;Weijie Guo;master(1.19) via 7336788a9c3495745cb4b3eda7bc00d396a14b18.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing check of subscribed and assigned topics at job startup,FLINK-34089,13564706,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,tanjialiang,tanjialiang,15/Jan/24 10:35,19/Jan/24 10:32,04/Jun/24 20:40,19/Jan/24 08:13,kafka-3.0.2,,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"When we unsubscribe a topic and still restore from the old state, the job will still read data from the unsubscribed topic. I think we should check if that the subscribed topic partitions match the assigned partitions, and throw an error so the user can handle it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 08:13:59 UTC 2024,,,,,,,,,,"0|z1mr40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 08:13;martijnvisser;If users want to remove a topic from their job, they should change their savepoint file. Just removing an input topic has implications on your pipeline, and users shouldn't just do that. 

If users have a requirement to (dynamically) add or remove Kafka topics, they should use the solution that has been brought forward with https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217389320;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix the problems with special table name characters of postgres and oracle  and sqlserver.,FLINK-34088,13564689,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,blackpighe,blackpighe,15/Jan/24 09:15,01/Feb/24 07:29,04/Jun/24 20:40,,jdbc-3.1.1,,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,," 

!image-2024-01-15-17-11-11-136.png!  

[https://github.com/apache/flink-connector-jdbc/]

The new version of flink jdbc is stand-alone and does not have an open issue entry.

 

The FLink-JDBC-connector does not handle special table names for oracle and postgres and sqlserver, when the table name is a special character such as default. jdbc will report an error. hopefully giving me a chance to fix the problem, I can provide a commit

 

 

!image-2024-01-15-17-13-16-448.png!

 ",,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/24 09:11;blackpighe;image-2024-01-15-17-10-57-586.png;https://issues.apache.org/jira/secure/attachment/13065960/image-2024-01-15-17-10-57-586.png","15/Jan/24 09:11;blackpighe;image-2024-01-15-17-11-11-136.png;https://issues.apache.org/jira/secure/attachment/13065959/image-2024-01-15-17-11-11-136.png","15/Jan/24 09:13;blackpighe;image-2024-01-15-17-13-16-448.png;https://issues.apache.org/jira/secure/attachment/13065958/image-2024-01-15-17-13-16-448.png",,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Jan 18 22:30:00 UTC 2024,,,,,,,,,,"0|z1mr08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 12:54;martijnvisser;Please provide the input in English;;;","16/Jan/24 01:25;blackpighe;i edit and update the issue, by use english;;;","16/Jan/24 01:27;blackpighe;This problem has persisted in other releases as well, but take the new project's 3.1.1 as an example;;;","16/Jan/24 06:23;blackpighe;https://github.com/apache/flink-connector-jdbc/pull/89;;;","17/Jan/24 03:51;blackpighe;add the commit to update test case;;;","18/Jan/24 22:30;Sergey Nuyanzin;Doesn't it work if during creation of table we specify something like 
{code:sql}
...
'table-name' = '`DEFAULT`',
...
{code}

or 


{code:sql}
...
'table-name' = '""DEFAULT""',
...
{code}
?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-dist,FLINK-34087,13564685,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,jiabao.sun,jiabao.sun,15/Jan/24 09:04,26/Jan/24 08:10,04/Jun/24 20:40,26/Jan/24 08:06,,,,,,,,,,,,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33577,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 08:06:18 UTC 2024,,,,,,,,,,"0|z1mqzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 08:06;jiabao.sun;Resolved via bcd448b2f1efecc701079c1a0f7f565a59817f22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native k8s session cannot specify port in nodeport mode,FLINK-34086,13564676,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,waywtdcc,waywtdcc,15/Jan/24 08:02,15/Jan/24 08:02,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-15 08:02:48.0,,,,,,,,,,"0|z1mqxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated string configuration keys in Flink 2.0,FLINK-34085,13564665,13564656,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuannan,xuannan,xuannan,15/Jan/24 07:07,15/Jan/24 07:17,04/Jun/24 20:40,,,,,,,,,2.0.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-15 07:07:34.0,,,,,,,,,,"0|z1mqv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate unused configuration in BinaryInput/OutputFormat and FileInput/OutputFormat,FLINK-34084,13564664,13564656,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xuannan,xuannan,xuannan,15/Jan/24 07:06,30/Jan/24 08:44,04/Jun/24 20:40,17/Jan/24 08:51,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,,"Update FileInputFormat.java, FileOutputFormat.java, BinaryInputFormat.java, and BinaryOutputFormat.java to deprecate unused string configuration keys.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 08:44:38 UTC 2024,,,,,,,,,,"0|z1mquw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 08:51;xtsong;master (1.19): 4559b851b22d6ffa197aa311adbea15b21a43c66;;;","30/Jan/24 08:44;martijnvisser;[~xuannan] [~xtsong] Can you please include in the release notes information on what's deprecated, and what users should be using?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate string configuration keys and unused constants in ConfigConstants,FLINK-34083,13564663,13564656,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuannan,xuannan,xuannan,15/Jan/24 06:57,30/Jan/24 08:44,04/Jun/24 20:40,19/Jan/24 06:09,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,,"* Update ConfigConstants.java to deprecate and replace string configuration keys
 * Mark unused constants in ConfigConstants.java as deprecated",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 08:44:13 UTC 2024,,,,,,,,,,"0|z1mquo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 06:09;fanrui;Merged to master(1.19) via:

38f7b51d0cc45293dc71ad31607ecc685b11498f

752d3a79a918b9300dd2b89e96f3915ba6a2dfa6;;;","30/Jan/24 08:44;martijnvisser;[~xuannan] [~fanrui] Can you please include in the release notes information on what's deprecated, and what users should be using?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated methods of Configuration in 2.0,FLINK-34082,13564659,13564656,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,15/Jan/24 06:11,15/Jan/24 06:11,04/Jun/24 20:40,,,,,,,,,2.0.0,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-15 06:11:19.0,,,,,,,,,,"0|z1mqts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Refactor all callers that using the public Xxx getXxx(ConfigOption<Xxx> configOption)  and public void setXxx(ConfigOption<Xxx> key, Xxx value) ",FLINK-34081,13564658,13564656,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,15/Jan/24 06:10,22/Jan/24 13:29,04/Jun/24 20:40,22/Jan/24 13:29,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,,"FLINK-34080 deprecate some methods of Configuration, we should refactor all callers of deprecated methods to use the recommended method.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 13:28:49 UTC 2024,,,,,,,,,,"0|z1mqtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 13:28;fanrui;Merged to master(1.19.0) via: c8f27c25e8726360bd09fd21fa8e908c40376881;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify the Configuration,FLINK-34080,13564657,13564656,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,fanrui,fanrui,15/Jan/24 06:09,19/Jan/24 06:07,04/Jun/24 20:40,19/Jan/24 06:07,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,,"This Jira is 2.2 part of FLIP-405:
 * 2.2.1 Update Configuration to encourage the usage of ConfigOption over string configuration key
 * 2.2.2 Introduce public <T> T get(ConfigOption<T> configOption, T overrideDefault)
 * 2.2.3 Deprecate some unnecessary setXxx and getXxx methods in Configuration",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 06:07:21 UTC 2024,,,,,,,,,,"0|z1mqtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 06:07;fanrui;Merged to master(1.19) via :

725b3edc05a1f3f186626038f8a7e60c1d8dd4fb

a2cc47a71e17cb22a86fb19ddadcbf1fb4308274

79abfaab3460834887df9e8284dff51569b63ecc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-405: Migrate string configuration key to ConfigOption,FLINK-34079,13564656,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuannan,fanrui,fanrui,15/Jan/24 06:07,11/Mar/24 12:43,04/Jun/24 20:40,,,,,,,,,1.20.0,,,,Runtime / Configuration,,,,0,,,,,"This is an umbrella Jira of [FLIP-405|https://cwiki.apache.org/confluence/x/6Yr5E]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 03:52:05 UTC 2024,,,,,,,,,,"0|z1mqt4:",9223372036854775807,"We have deprecated all `setXxx` and `getXxx` methods except getString(String key, String defaultValue) and setString(String key, String value), such as: setInteger, setLong, getInteger and getLong etc. We strongly recommend users and developers use get and set methods directly.

In addition, we recommend users to use ConfigOption instead of string as key.",,,,,,,,,,,,,,,,,,,"31/Jan/24 03:52;fanrui;Hi [~martijnvisser] , thanks for your reminder in the subtask JIRA.

I have updated the Release note in the umbrella JIRA. cc [~xuannan] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move InternalKeyContext classes from o.a.f.runtime.state.heap to o.a.f.runtime.state package,FLINK-34078,13564652,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lijinzhong,lijinzhong,lijinzhong,15/Jan/24 05:30,16/Jan/24 05:50,04/Jun/24 20:40,16/Jan/24 02:28,,,,,,,,1.19.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"h3. Motication:

When Rocksdb statebackend throws a keyGroup check illegal exception, 
the exception stack contains the heap stateBackend scoped class, which looks so strange to user.

!image-2024-01-15-12-57-12-667.png|width=555,height=68!
h3. Proposed changes:

InternalKeyContext and InternalKeyContextImpl are commonly used by all state backends (heap/rocksdb/changelog), they should be moved from org.apache.flink.runtime.state.heap package to org.apache.flink.runtime.state package.
h3. Compatibility:

InternalKeyContext is annotated with @Internal, so this change has no compatibility issues.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/24 04:57;lijinzhong;image-2024-01-15-12-57-12-667.png;https://issues.apache.org/jira/secure/attachment/13065953/image-2024-01-15-12-57-12-667.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 02:28:54 UTC 2024,,,,,,,,,,"0|z1mqs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 07:18;masteryhx;Thanks for reporting this.

It makes sense to move it to the outer package.

Already assigned to you, please go ahead.;;;","16/Jan/24 02:28;masteryhx;merged 6bdb4f75 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Sphinx version error,FLINK-34077,13564642,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxb,yunfengzhou,yunfengzhou,15/Jan/24 01:22,17/Jan/24 12:55,04/Jun/24 20:40,15/Jan/24 06:24,1.19.0,,,,,,,1.19.0,,,,API / Python,,,,0,pull-request-available,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56357&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901]
 
{code:java}
Jan 14 15:49:17 /__w/2/s/flink-python/dev/.conda/bin/sphinx-build -b html -d _build/doctrees -a -W . _build/html    
Jan 14 15:49:17 Running Sphinx v4.5.0    
Jan 14 15:49:17    
Jan 14 15:49:17 Sphinx version error:    
Jan 14 15:49:17 The sphinxcontrib.applehelp extension used by this project needs at least Sphinx v5.0; it therefore cannot be built with this version.    
Jan 14 15:49:17 Makefile:76: recipe for target 'html' failed    
Jan 14 15:49:17 make: *** [html] Error 2    
Jan 14 15:49:18 ==========sphinx checks... [FAILED]=========== {code}
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 14:31:03 UTC 2024,,,,,,,,,,"0|z1mqq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 01:32;xuyangzhong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56351&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901;;;","15/Jan/24 02:27;hxb;Some sphinxcontrib(sphinxcontrib-applehelp, sphinxcontrib.devhelp sphinxcontrib.htmlhelp and so on) packages have released new versions, but they have not done compatibility, so the document build fails. I will hofix to limit the versions of these packages. Regarding upgrading the sphix version, some current conf configurations need to be changed, which are incompatible with the current conf. I think it can be done as a new feature.;;;","15/Jan/24 06:24;hxb;Merged into master via d2fbe464b1a353a7eb35926299d5c048647a3073;;;","16/Jan/24 14:31;mapohl;The following build failures didn't include the fix, yet, and are only added for documentation purposes:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56340&view=results]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56348&view=results]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56359&view=results]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56363&view=results]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56364&view=results]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56365&view=results] 
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56370&view=results]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56368&view=results];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-base missing fails kinesis table sink to create,FLINK-34076,13564626,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,khanhvu,khanhvu,14/Jan/24 15:36,15/Jan/24 12:52,04/Jun/24 20:40,,aws-connector-4.2.0,,,,,,,,,,,Connectors / Kinesis,,,,0,,,,,"The following issue encounters with flink-kinesis-connector v4.2.0, Flink 1.17, it's working properly with kinesis connector v4.1.0 (I have not tested version pre v4.1.0).

The [commit|https://github.com/apache/flink-connector-aws/commit/01f112bd5a69f95cd5d2a4bc7e08d1ba9a81d56a] which stops bundling `flink-connector-base` with `flink-connector-kinesis` has caused kinesis sink failing to create when using Table API as required classes from `flink-connector-base` are not loaded in runtime.

E.g. with following depenency only in pom.xml
{code:java}
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kinesis</artifactId>
            <version>${flink.connector.kinesis.version}</version>
        </dependency>
{code}

and a minimal job definition:

{code:java}
	public static void main(String[] args) throws Exception {
		// create data stream environment
		StreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();
		sEnv.setRuntimeMode(RuntimeExecutionMode.STREAMING);
		StreamTableEnvironment tEnv = StreamTableEnvironment.create(sEnv);

		Schema a = Schema.newBuilder().column(""a"", DataTypes.STRING()).build();
		TableDescriptor descriptor =
				TableDescriptor.forConnector(""kinesis"")
						.schema(a)
						.format(""json"")
						.build();
		tEnv.createTemporaryTable(""sinkTable"", descriptor);

		tEnv.executeSql(""CREATE TABLE sinkTable "" + descriptor.toString()).print();
	}
{code}

following exception will be thrown:
{code:java}
Caused by: java.lang.ClassNotFoundException: org.apache.flink.connector.base.table.AsyncDynamicTableSinkFactory
	at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581) ~[?:?]
	at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) ~[?:?]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:527) ~[?:?]
	... 28 more
{code}

The fix is to explicitly specify `flink-connector-base` as dependency of the project:

{code:java}
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-connector-kinesis</artifactId>
			<version>${flink.connector.kinesis.version}</version>
		</dependency>
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-connector-base</artifactId>
			<version>${flink.version}</version>
			<scope>provided</scope>
		</dependency>
{code}

In general, `flink-connector-base` should be pulled in by default when pulling in the kinesis connector, the current separation adds unnecessary hassle to use the connector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/24 02:59;jiabao.sun;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13065948/screenshot-1.png","15/Jan/24 08:04;jiabao.sun;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13065956/screenshot-2.png","15/Jan/24 09:58;jiabao.sun;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13065981/screenshot-3.png","15/Jan/24 12:34;jiabao.sun;screenshot-4.png;https://issues.apache.org/jira/secure/attachment/13065987/screenshot-4.png",,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 12:52:42 UTC 2024,,,,,,,,,,"0|z1mqmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 03:04;jiabao.sun;Hi [~khanhvu], do you add dependencies with ""provided"" scope to classpath?
I can run correctly locally.


{code:xml}
<dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kinesis</artifactId>
            <version>${flink.connector.kinesis.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-base</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-loader</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-runtime</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
    </dependencies>
{code}
;;;","15/Jan/24 07:50;khanhvu;Hi [~jiabao.sun],
Yes, if I have `flink-connector-base` in the dependency list, it will run properly (it's the fix I mentioned), but if I leave the base out, it's failing. Before the aforementioned commit, I just need to have `flink-connector-kinesis` in the list (not along with the `flink-connector-base`). 

It's actually not correct when the connector depends on `flink-connector-base` to execute but does not have it as (transitive) dependency.;;;","15/Jan/24 08:04;jiabao.sun;Hi [~khanhvu],

The flink-connector-base is already included in flink-dist and we will not package it in the externalized connectors[1].
The dependencies of ""provided"" will also be passed on. You just need to check ""add dependencies with 'provided' scope to classpath"" in IDEA.


{code:xml}
    <properties>
        <flink.version>1.17.0</flink.version>
        <flink.connector.kinesis.version>4.2.0-1.17</flink.connector.kinesis.version>
        <scala.binary.version>2.12</scala.binary.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kinesis</artifactId>
            <version>${flink.connector.kinesis.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-loader</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-runtime</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
    </dependencies>
{code}

 !screenshot-2.png! 

[1] https://issues.apache.org/jira/browse/FLINK-30400;;;","15/Jan/24 09:44;khanhvu;I'm aware of the IntelliJ feature, it wont work without the fix I mentioned even I enabled it in IntelliJ.

I'm aware of the FLINK-30400 too, and I raised this one as we're creating the issue for kinesis connector by the change. I was not aware when the decision was made, but IMO it's the issue with the base, and we should not solve the issue by transferring it to concrete connectors. For things which likely breaks compatibility, it should be abstracted away from the base, here the coupling is still there, and we cut the connection.


bq. The flink-connector-base is already included in flink-dist

You're not declaring `flink-dist` in your app's pom.xml, are you? it wont be issue with prod environment, but it's an issue with local development. We have a workaround, but it's a bad user experience to use the connector.

Again, it's programmatically/semantically incorrect as kinesis connector depends on connector base to execute, but it's not in the dependency tree of kinesis connector?! How am I (as a user) suppose to know I need to declare flink-connector-base in my app pom.xml before I execute the app?;;;","15/Jan/24 09:58;jiabao.sun;The purpose of ""Stop bundling connector-base in externalized connectors"" is to prevent external connectors from having compile-time dependencies on specific versions of flink-connector-base, but instead obtain it at runtime from flink-dist to achieve better compatibility.

In fact, users do not need to explicitly declare the flink-connector-base dependency in the pom.xml file. It is already included in the dependency tree of the Kinesis connector and declared with a ""provided"" scope. Users only need to add the provided dependency to the classpath to run the testing.

 !screenshot-3.png! ;;;","15/Jan/24 10:01;khanhvu;hm, I dont see it's there: https://github.com/apache/flink-connector-aws/blob/v4.2/flink-connector-aws/flink-connector-kinesis/pom.xml;;;","15/Jan/24 10:04;khanhvu;> In fact, users do not need to explicitly declare the flink-connector-base dependency in the pom.xml file

Have you verified this? it does not work for me with 4.2.0-1.18 version.;;;","15/Jan/24 10:38;khanhvu;The `flink-connector-base` is in `flink-connector-aws-base` dependency tree ([pom.xml|https://github.com/apache/flink-connector-aws/blob/v4.2/flink-connector-aws-base/pom.xml#L46]). But because `flink-connector-aws-base` is [shaded|https://github.com/apache/flink-connector-aws/blob/v4.2/flink-connector-aws/flink-connector-kinesis/pom.xml#L322] in `flink-connector-kinesis` uber jar, the `flink-connector-base` is not present in `flink-kinesis-connector`'s dependency tree anymore.

I think a simple fix here is to add `flink-connector-base` with `provided` scope as direct dependency of `flink-connector-kinesis`.;;;","15/Jan/24 10:59;jiabao.sun;flink-connector-base has already been included as a parent dependency, and the submodules will also inherit this dependency.

[https://github.com/apache/flink-connector-aws/blob/38aafb44d3a8200e4ff41d87e0780338f40da258/pom.xml#L141-L146]

[https://repo1.maven.org/maven2/org/apache/flink/flink-connector-kinesis/4.2.0-1.18/flink-connector-kinesis-4.2.0-1.18.pom];;;","15/Jan/24 11:06;dannycranmer;> flink-connector-base has already been included as a parent dependency, and the submodules will also inherit this dependency.

This is in {{dependencyManagement}} so it will only take effect if there is a {{dependency}} in the child pom.

It sounds like the problem/request is a convenience change for IntelliJ to add a {{provided}} scoped dependency. [~khanhvu] can you confirm that adding the {{provided}} dependency helps IntelliJ configure the project? If this is the case, given there is no impact on the output jar, I +1 the change. Alternatively we can promote {{flink-connector-base}} from {{dependencyManagement}} to {{dependencies}} in the parent (as provided). ;;;","15/Jan/24 12:34;jiabao.sun;Hey [~dannycranmer], it's in dependencies.

 !screenshot-4.png! 

 

 ;;;","15/Jan/24 12:52;dannycranmer;[~jiabao.sun]  Ah, I missed that. Interesting. ;;;",,,,,,,,,,,,,,,,,,,,,
Mark version as released in Jira (need PMC role),FLINK-34075,13564615,13561858,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,renqs,jingge,jingge,14/Jan/24 13:23,17/Jan/24 07:56,04/Jun/24 20:40,17/Jan/24 07:13,,,,,,,,1.18.1,,,,,,,,0,,,,,"In JIRA, inside [version management|https://issues.apache.org/jira/plugins/servlet/project-config/FLINK/versions], hover over the current release and a settings menu will appear. Click Release, and select today’s date.

(Note: Only PMC members have access to the project administration. If you do not have access, ask on the mailing list for assistance.)

 
----
h3. Expectations
 * Release tagged in the source code repository
 * Release version finalized in JIRA. (Note: Not all committers have administrator access to JIRA. If you end up getting permissions errors ask on the mailing list for assistance)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 07:56:44 UTC 2024,,,,,,,,,,"0|z1mqk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 07:13;renqs;1.18.1 has been marked as released on JIRA;;;","17/Jan/24 07:56;jingge;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify artifacts related expectations,FLINK-34074,13564614,13561858,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Jan/24 13:16,16/Jan/24 22:02,04/Jun/24 20:40,16/Jan/24 22:02,,,,,,,,,,,,,,,,0,,,,,"h3. Expectations
 * Maven artifacts released and indexed in the [Maven Central Repository|https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.flink%22] (usually takes about a day to show up)
 * Source & binary distributions available in the release repository of [https://dist.apache.org/repos/dist/release/flink/]
 * Dev repository [https://dist.apache.org/repos/dist/dev/flink/] is empty
 * Website contains links to new release binaries and sources in download page
 * (for minor version updates) the front page references the correct new major release version and directs to the correct link",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-14 13:16:51.0,,,,,,,,,,"0|z1mqjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove old release candidates from dist.apache.org,FLINK-34073,13564613,13561858,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Jan/24 13:14,16/Jan/24 13:06,04/Jun/24 20:40,16/Jan/24 13:06,,,,,,,,,,,,,,,,0,,,,,"h3. Remove old release candidates from [dist.apache.org|http://dist.apache.org/]

Remove the old release candidates from [https://dist.apache.org/repos/dist/dev/flink] using Subversion.
{code:java}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
$ cd flink
$ svn remove flink-${RELEASE_VERSION}-rc*
$ svn commit -m ""Remove old release candidates for Apache Flink ${RELEASE_VERSION}
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-14 13:14:05.0,,,,,,,,,,"0|z1mqjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use JAVA_RUN in shell scripts,FLINK-34072,13564605,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Yu Chen,yunta,yunta,14/Jan/24 09:33,18/Jan/24 02:43,04/Jun/24 20:40,18/Jan/24 02:43,,,,,,,,1.19.0,,,,Deployment / Scripts,,,,0,pull-request-available,,,,"We should call {{JAVA_RUN}} in all cases when we launch {{java}} command, otherwise we might be able to run the {{java}} if JAVA_HOME is not set.

such as:

{code:java}
flink-1.19-SNAPSHOT-bin/flink-1.19-SNAPSHOT/bin/config.sh: line 339: > 17 : syntax error: operand expected (error token is ""> 17 "")
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 02:43:04 UTC 2024,,,,,,,,,,"0|z1mqhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/24 10:09;Yu Chen;Hi [~yunta] , I'd like to take this ticket if you don't mind.;;;","14/Jan/24 14:07;yunta;[~Yu Chen] Already assigned, please go ahead.;;;","18/Jan/24 02:43;yunta;merged in master: e7c8cd1562ebd45c1f7b48f519a11c6cd4fdf100;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock in AWS Kinesis Data Streams AsyncSink connector,FLINK-34071,13564575,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,a.pilipenko,a.pilipenko,13/Jan/24 10:07,30/Apr/24 07:40,04/Jun/24 20:40,,1.15.4,aws-connector-3.0.0,aws-connector-4.2.0,,,,,,,,,Connectors / AWS,Connectors / Kinesis,,,0,,,,,"Sink operator hangs while flushing records, similarly to FLINK-32230. Error observed even when using AWS SDK version that contains fix for async client error handling [https://github.com/aws/aws-sdk-java-v2/pull/4402]

Thread dump of stuck thread:
{code:java}
""sdk-async-response-1-6236"" Id=11213 RUNNABLE
    at app//org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.lambda$flush$5(AsyncSinkWriter.java:385)
    at app//org.apache.flink.connector.base.sink.writer.AsyncSinkWriter$$Lambda$1778/0x0000000801141040.accept(Unknown Source)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.handleFullyFailedRequest(KinesisStreamsSinkWriter.java:210)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter.lambda$submitRequestEntries$1(KinesisStreamsSinkWriter.java:184)
    at org.apache.flink.connector.kinesis.sink.KinesisStreamsSinkWriter$$Lambda$1965/0x00000008011a0c40.accept(Unknown Source)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:79)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.CompletableFutureUtils$$Lambda$1925/0x0000000801181840.accept(Unknown Source)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncApiCallMetricCollectionStage.lambda$execute$0(AsyncApiCallMetricCollectionStage.java:56)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncApiCallMetricCollectionStage$$Lambda$1961/0x0000000801191c40.accept(Unknown Source)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncApiCallTimeoutTrackingStage.lambda$execute$2(AsyncApiCallTimeoutTrackingStage.java:67)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncApiCallTimeoutTrackingStage$$Lambda$1960/0x0000000801191840.accept(Unknown Source)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:79)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.CompletableFutureUtils$$Lambda$1925/0x0000000801181840.accept(Unknown Source)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeAttemptExecute(AsyncRetryableStage.java:103)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage.java:184)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor.lambda$attemptExecute$1(AsyncRetryableStage.java:170)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryingExecutor$$Lambda$1956/0x0000000801192840.accept(Unknown Source)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.lambda$null$0(MakeAsyncHttpRequestStage.java:105)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage$$Lambda$1954/0x0000000801193040.accept(Unknown Source)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base@11.0.18/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.completeResponseFuture(MakeAsyncHttpRequestStage.java:238)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.lambda$executeHttpRequest$3(MakeAsyncHttpRequestStage.java:163)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage$$Lambda$1952/0x0000000801193840.apply(Unknown Source)
    ... {code}
Alongside this issue following exception can be observed
{code:java}
java.io.IOException: An error occurred on the connection: java.nio.channels.ClosedChannelException, [channel: 159aa20c]. All streams will be closed
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.MultiplexedChannelRecord.decorateConnectionException(MultiplexedChannelRecord.java:213)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.MultiplexedChannelRecord.lambda$closeChildChannels$10(MultiplexedChannelRecord.java:205)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.MultiplexedChannelRecord.lambda$closeAndExecuteOnChildChannels$11(MultiplexedChannelRecord.java:229)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.utils.NettyUtils.doInEventLoop(NettyUtils.java:248)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.MultiplexedChannelRecord.closeAndExecuteOnChildChannels(MultiplexedChannelRecord.java:220)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.MultiplexedChannelRecord.closeChildChannels(MultiplexedChannelRecord.java:205)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2MultiplexedChannelPool.closeAndReleaseParent(Http2MultiplexedChannelPool.java:353)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2MultiplexedChannelPool.closeAndReleaseParent(Http2MultiplexedChannelPool.java:333)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2MultiplexedChannelPool.access$200(Http2MultiplexedChannelPool.java:76)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2MultiplexedChannelPool$ReleaseOnExceptionHandler.closeAndReleaseParent(Http2MultiplexedChannelPool.java:509)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2MultiplexedChannelPool$ReleaseOnExceptionHandler.channelInactive(Http2MultiplexedChannelPool.java:486)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
    at org.apache.flink.kinesis.shaded.io.netty.handler.logging.LoggingHandler.channelInactive(LoggingHandler.java:206)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
    at org.apache.flink.kinesis.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
    at org.apache.flink.kinesis.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
    at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2PingHandler.channelInactive(Http2PingHandler.java:77)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
    at org.apache.flink.kinesis.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:411)
    at org.apache.flink.kinesis.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:376)
    at org.apache.flink.kinesis.shaded.io.netty.handler.codec.http2.Http2ConnectionHandler.channelInactive(Http2ConnectionHandler.java:430)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
    at org.apache.flink.kinesis.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:411)
    at org.apache.flink.kinesis.shaded.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:376)
    at org.apache.flink.kinesis.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1085)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
    at org.apache.flink.kinesis.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
    at org.apache.flink.kinesis.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
    at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)
    at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
    at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
    at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
    at org.apache.flink.kinesis.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
    at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
    at org.apache.flink.kinesis.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.nio.channels.ClosedChannelException
    ... 41 more
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 07:40:22 UTC 2024,,,,,,,,,,"0|z1mqb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 10:32;hong;As discussed offline, wonder if we can add a timeout to handle this. Maybe we can consider following a similar pattern to Async I/O.

 

[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/asyncio/#timeout-handling]

 

This will need a FLIP if we introduce a new configuration. 

 

 ;;;","29/Apr/24 16:07;a.pilipenko;We found that one of the causes of this - continuous retry of records, that are consistently fail on write.

Visibility into such issues is limited due to https://issues.apache.org/jira/browse/FLINK-35269;;;","29/Apr/24 20:37;chalixar;[~a.pilipenko] Could we follow up and update Exception classifiers to propagate failures of malicious records?;;;","29/Apr/24 20:38;chalixar;Additionally the timeout configuration setup is addressed in [FLIP-451|https://cwiki.apache.org/confluence/display/FLINK/FLIP-451%3A+Refactor+Async+Sink+API+and+Introduce+request+timeout+configuration],Let's keep the feedback on this part on the discussion thread.;;;","30/Apr/24 07:40;a.pilipenko;[~chalixar] for exception classification I following up by increasing visibility (FLINK-35269). For malicious records - will check additionally to see which exceptions to add as Fatal.

In terms of timeout - while nice to have, I am not sure it is related to this issue at all.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniClusterITCase.testHandleStreamingJobsWhenNotEnoughSlot fails for the AdaptiveScheduler,FLINK-34070,13564492,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Jan/24 11:46,17/Jan/24 13:10,04/Jun/24 20:40,17/Jan/24 13:10,1.19.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"We experience test failures of {{MiniClusterITCase.testHandleStreamingJobsWhenNotEnoughSlot}} with the {{AdaptiveScheduler}} being enabled after FLINK-33414 was fixed:
{code:java}
Jan 09 02:01:16         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 09 02:01:16 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Jan 09 02:01:16 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Jan 09 02:01:16 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Jan 09 02:01:16 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:1050)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniClusterITCase.runHandleJobsWhenNotEnoughSlots(MiniClusterITCase.java:152)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniClusterITCase.lambda$testHandleStreamingJobsWhenNotEnoughSlot$0(MiniClusterITCase.java:119)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniClusterITCase$$Lambda$1927/1144737794.call(Unknown Source)
Jan 09 02:01:16 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
Jan 09 02:01:16 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
Jan 09 02:01:16 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
Jan 09 02:01:16 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniClusterITCase.testHandleStreamingJobsWhenNotEnoughSlot(MiniClusterITCase.java:119)
Jan 09 02:01:16 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 09 02:01:16 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 09 02:01:16 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56124&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9813]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56166&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=10782]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56226&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=10773]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56285&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=10800]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33414,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 13:10:37 UTC 2024,,,,,,,,,,"0|z1mpso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 13:29;mapohl;This issue can be reproduced locally:
{code:java}
mvn -pl flink-runtime test -Dtest=MiniClusterITCase#testHandleStreamingJobsWhenNotEnoughSlot -Dflink.tests.enable-adaptive-scheduler=true -Dfast {code};;;","12/Jan/24 14:48;mapohl;The {{AdaptiveScheduler}} uses {{DeclarativeSlotPoolServiceFactory}} in contrast to the {{DefaultScheduler}} and the {{AdaptiveBatchScheduler}} which use {{DeclarativeSlotPoolBridgeServiceFactory}} (see [DefaultSlotPoolServiceSchedulerFactory:177ff|https://github.com/apache/flink/blob/a9383fd4d51b1161292628145e2f427f574a07d4/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/DefaultSlotPoolServiceSchedulerFactory.java#L177]). That makes the {{JobMaster}} use different implementations of the {{SlotPoolService}} interface in [JobMaster:333|https://github.com/apache/flink/blob/16bac7802284563c95cfe18fcf153e91dc06216e/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L333]:
 * {{DeclarativeSlotPoolService}} (used by the {{{}AdaptiveScheduler{}}}) doesn't implement {{SlotPoolService#notifyNotEnoughResourcesAvailable}} but relies on the empty default implementation.
 * {{DeclarativeSlotPoolBridge}} (used by the other two scheduler implementations) does implement implement {{notifyNotEnoughResourcesAvailable}} in [DeclarativeSlotPoolBridge:395ff|https://github.com/apache/flink/blob/72bff2a2d0072602e4e625476bf5480dc50dc76c/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/DeclarativeSlotPoolBridge.java#L395] which triggers aborting pending requests.

Conclusion: The {{AdaptiveScheduler}} doesn't participate in the early cancellation of pending slot requests that cannot be fulfilled. ;;;","12/Jan/24 15:56;mapohl;[~chesnay] Do you remember whether there was a specific architectural-driven reason why we don't respond to {{notifyNotEnoughResourcesAvailable}} for the {{{}DeclarativeSlotPoolService{}}}?;;;","12/Jan/24 17:06;Jiang Xin;[~mapohl] Thanks for the clarification of FLINK-33414 and creating the new one.

I think the following comments in the original test may caused some misunderstandings. Actually, `SLOT_REQUEST_TIMEOUT` is the fallback timeout of the default scheduler, while `RESOURCE_WAIT_TIMEOUT` is the delay to trigger the slot resource checking of the adaptive scheduler. 
{code:java}
// this triggers the failure for the default scheduler
configuration.setLong(
    JobManagerOptions.SLOT_REQUEST_TIMEOUT, slotRequestTimeout.toMillis());
// this triggers the failure for the adaptive scheduler
configuration.set(JobManagerOptions.RESOURCE_WAIT_TIMEOUT, slotRequestTimeout); {code}
So we should set the `RESOURCE_WAIT_TIMEOUT` to `Duration.ofMillis(1)` to trigger the checking ASAP instead of `Duration.ofMillis(Long.MAX_VALUE)`, WDYT?;;;","15/Jan/24 13:28;mapohl;I provided a PR for this issue that made the test more explicit and covers all three supported scenarios. WDYT?;;;","16/Jan/24 14:13;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56340&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=10869;;;","16/Jan/24 14:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56348&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=10840;;;","16/Jan/24 14:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56359&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9767;;;","16/Jan/24 14:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56416&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9821;;;","17/Jan/24 13:10;mapohl;master: [024ed963ed8358ae78e728108ac6c95046924f67|https://github.com/apache/flink/commit/024ed963ed8358ae78e728108ac6c95046924f67];;;",,,,,,,,,,,,,,,,,,,,,,,
build_wheels_on_macos times out and gets cancelled,FLINK-34069,13564487,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,12/Jan/24 10:43,22/Jan/24 15:44,04/Jun/24 20:40,,1.17.2,1.18.0,1.19.0,,,,,,,,,Build System,,,,0,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56285&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=146]

It doesn't look like the wheels step halts but rather that the step needs more time in general which increases the likelihood of the build running into the timeout here. We should investigate whether we're close to the timeout for this step in general.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 15:44:13 UTC 2024,,,,,,,,,,"0|z1mprk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 15:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56659&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=4ceca519-1cd0-4c87-9921-3d590208bfdb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
build_wheels_on_macos fails due to connection error,FLINK-34068,13564485,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,12/Jan/24 10:36,12/Jan/24 10:37,04/Jun/24 20:40,,1.17.2,1.18.0,1.19.0,,,,,,,,,Build System,,,,0,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56226&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=187]
{code:java}
 [...]
 ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/a9/3e/6498bcf34a8194a2e3cc6dab0a742f4de9a28363ccd91ca37c38d3fe9cb4/apache_beam-2.48.0-cp310-cp310-macosx_10_9_x86_64.whl (Caused by NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1075c5f90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-12 10:36:59.0,,,,,,,,,,"0|z1mpr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix javacc warnings in flink-sql-parser,FLINK-34067,13564453,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jhughes,jhughes,jhughes,12/Jan/24 04:04,15/Jan/24 08:40,04/Jun/24 20:40,15/Jan/24 08:40,1.19.0,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,"While extending the Flink SQL parser, I noticed these two warnings:
```
[INFO] — javacc:2.4:javacc (javacc) @ flink-sql-parser ---                                                 
Java Compiler Compiler Version 4.0 (Parser Generator)                                                       
(type ""javacc"" with no arguments for help)                                                                                                 
Reading from file .../flink-table/flink-sql-parser/target/generated-sources/javacc/Parser.jj . . .               
Note: UNICODE_INPUT option is specified. Please make sure you create the parser/lexer using a Reader with the correct character encoding.  
Warning: Choice conflict involving two expansions at                                                           
         line 2043, column 13 and line 2052, column 9 respectively.                                        
         A common prefix is: ""IF""                                                                                                           Consider using a lookahead of 2 for earlier expansion.                                                
Warning: Choice conflict involving two expansions at                                                          
         line 2097, column 13 and line 2105, column 8 respectively.                                         
         A common prefix is: ""IF""                                                                                                  
         Consider using a lookahead of 2 for earlier expansion.     
```

As the warning suggestions, adding `LOOKAHEAD(2)` in a few places addresses the warning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 08:40:11 UTC 2024,,,,,,,,,,"0|z1mpk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 08:40;twalthr;Fixed in master: 96eea964d2a75d9d953bb14485ed40c9b339f9a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LagFunction throw NPE when input argument are not null,FLINK-34066,13564451,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,337361684@qq.com,337361684@qq.com,337361684@qq.com,12/Jan/24 03:59,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"This issue is related to https://issues.apache.org/jira/browse/FLINK-31967. In FLINK-31967, the NPE error has not been thoroughly fixed. If the select value  LAG(len, 1, cast(null as int)) and  LAG(len, 1, 1) exists together in test case AggregateITCase.testLagAggFunction() as:
{code:java}
val sql =
  s""""""
     |select
     |  LAG(len, 1, cast(null as int)) OVER w AS nullable_prev_quantity,
     |  LAG(len, 1, 1) OVER w AS prev_quantity,
     |  LAG(len) OVER w AS prev_quantity
     |from src
     |WINDOW w AS (ORDER BY proctime)
     |"""""".stripMargin {code}
before is:
{code:java}
val sql =
  s""""""
     |select
     |  LAG(len, 1, cast(null as int)) OVER w AS prev_quantity,
     |  LAG(len) OVER w AS prev_quantity
     |from src
     |WINDOW w AS (ORDER BY proctime)
     |"""""".stripMargin {code}
NPE will throw.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 12:08:50 UTC 2024,,,,,,,,,,"0|z1mpjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 08:04;martijnvisser;Can you please elaborate a bit more and have a reproducer? What I'm seeing in the PR is that you're just adjusting the test so that we don't test with NULL anymore, but I don't see an explainer why we don't want to test for that anymore. ;;;","12/Jan/24 08:31;337361684@qq.com;hi, [~martijnvisser] . This is a related bug of https://issues.apache.org/jira/browse/FLINK-31967.  In FLINK-31967,  the NPE exception was not been thoroughly fixed. For instance, the case I‘ve added now. (In fact, with the current case, I just add a new field without modifying the original sql pattern, it will still lead to an NPE exception).;;;","12/Jan/24 08:33;martijnvisser;[~337361684@qq.com] OK. For next time, it would be good to immediately provide that background and link to this ticket. Not everyone has the background that you have on this :);;;","12/Jan/24 08:38;337361684@qq.com;Hi, [~martijnvisser] . The new test case has not abandoned the previous tests; instead, it covers both the former cases and the new ones (they work in conjunction). As for the newly added field, if it is not handled with nullable(), it will still throw an NPE error.;;;","12/Jan/24 08:40;337361684@qq.com;[~martijnvisser] ，Sorry for not adding more details, I will add it.  Thanks for your reminder. (y);;;","16/Jan/24 12:08;fsk119;Merged into master: 488d60a1d3912246130b1fb7d238a0e7b336516f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Design AbstractAutoscalerStateStore to support serialize State to String,FLINK-34065,13564448,13556703,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,12/Jan/24 02:36,23/Jan/24 09:50,04/Jun/24 20:40,,,,,,,,,,,,,Autoscaler,,,,0,,,,,"Some logic of {{KubernetesAutoScalerStateStore}} and {{JDBCAutoScalerStateStore}} are similar, we can share some common code.
 * {{ConfigMapStore}} and {{JDBCStore}} can be abstracted to {{StringStateStore}} interface

 ** They support {{{}put{}}}, {{get}} and {{remove}}
 ** The parameters of {{ConfigMapStore}} are the (JobContext, String key, String value).
 ** The parameters of {{JDBCStore}} are the (String jobKey, StateType stateType, String value).
 ** We can define a interface {{{}StringStateStore{}}}, and the parameters are {{{}(JobContext, StateType stateType, String value){}}}.
 * {{KubernetesAutoScalerStateStore}} and {{JDBCAutoScalerStateStore}} can be abstracted to {{AbstractAutoscalerStateStore}}

 ** They support serialize and compress {{Original State}} to String.
 ** {{AbstractAutoscalerStateStore}} can reuse the serialize and compress logic
 ** {{KubernetesAutoScalerStateStore}} support the limitation of stateValue
 ** We can define a parameter for {{{}AbstractAutoscalerStateStore{}}}, the limitation is disabled by default, and {{KubernetesAutoScalerStateStore}} can enable it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-12 02:36:45.0,,,,,,,,,,"0|z1mpiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-471: Expose JobManagerOperatorMetrics via REST API,FLINK-34064,13564417,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masc,mason6345,mason6345,11/Jan/24 20:12,03/Jun/24 17:36,04/Jun/24 20:40,,1.18.0,,,,,,,1.20.0,,,,Runtime / REST,Runtime / Web Frontend,,,0,,,,,"Add a REST API to fetch coordinator metrics.

[https://cwiki.apache.org/confluence/display/FLINK/FLIP-417%3A+Expose+JobManagerOperatorMetrics+via+REST+API]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 19:38:57 UTC 2024,,,,,,,,,,"0|z1mpc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 19:38;mason6345;[~mxm] [~fanrui] [~thw] can you assign this ticket to me? Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When snapshot compression is enabled, rescaling of a source operator leads to some splits getting lost",FLINK-34063,13564374,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dmvk,isburmistrov,isburmistrov,11/Jan/24 17:15,26/Jan/24 10:35,04/Jun/24 20:40,19/Jan/24 14:29,1.18.0,1.18.1,,,,,,1.18.2,,,,Runtime / State Backends,,,,1,pull-request-available,,,,"h2. Backstory

We've been experimenting with Autoscaling on the Flink 1.18 and faced a pretty nasty bug. 

The symptoms on our production system were as following. After a while after deploying a job with autoscaler it started accumulating Kafka lag, and this could only be observed via external lag measurement - from inside Flink (measured by
{{_KafkaSourceReader_KafkaConsumer_records_lag_max_}} metric) the lag was OK:
!image-2024-01-11-16-27-09-066.png|width=887,height=263!

After some digging, it turned out that the job has lost some Kafka partitions - i.e. it stopped consuming from them, “forgot” about their existence. That’s why from the Flink’s perspective everything was fine - the lag was growing on the partitions Flink no longer knew about.

This was visible on a metric called “Assigned partitions” (KafkaSourceReader_KafkaConsumer_assigned_partitions):
!image-2024-01-11-16-30-47-466.png|width=1046,height=254!


We see on the chart that the job used to know about 20 partitions, and then this number got dropped to 16.

This drop has been quickly connected to the job’s scaling events. Or, more precisely, to the scaling of the source operator - with almost 100% probability any scaling of the source operator led to partitions loss.
h2. Investigation

We've conducted the investigation. We use the latest Kubernetes operator and deploy jobs with Native Kubernetes.

The reproducing scenario we used for investigation:
 * Launch a job with source operator parallelism = 4, enable DEBUG logging
 * Wait until it takes the first checkpoint
 * Scale-up the source operator to say 5 (no need to wait for autoscaling, it can be done via Flink UI)
 * Wait until the new checkpoint is taken
 * Scale-down the source operator to 3

These simple actions with almost 100% probability led to some partitions get lost.

After that we've downloaded all the logs and inspected them. Noticed these strange records in logs:


{code:java}
{""timestamp"":1704415753166,""is_logging_enabled"":""false"",""logger_id"":""org.apache.flink.streaming.api.operators.AbstractStreamOperator"",""log_level"":""INFO"",""message"":""Restoring state for 4 split(s) to reader."",""service_name"":""data-beaver""} {""timestamp"":1704415753166,""is_logging_enabled"":""false"",""logger_id"":""org.apache.flink.connector.base.source.reader.SourceReaderBase"",""log_level"":""INFO"",""message"":""Adding split(s) to reader: 
[
[Partition: eventmesh-video-play-v1-6, StartingOffset: 1964306414, StoppingOffset: -9223372036854775808], 
[Partition: eventmesh-video-play-v1-19, StartingOffset: 1963002538, StoppingOffset: -9223372036854775808], 
[Partition: eventmesh-video-play-v1-6, StartingOffset: 1964306414, StoppingOffset: -9223372036854775808], 
[Partition: eventmesh-video-play-v1-19, StartingOffset: 1963002538, StoppingOffset: -9223372036854775808]]"", ""service_name"":""data-beaver""}{code}
We see that some task being restored with 4 splits, however actual splits have duplicates - we see that in reality 2 unique partitions have been added ({_}eventmesh-video-play-v1-6{_} and {_}eventmesh-video-play-v1-19{_}).

Digging into the code and the logs a bit more, log lines like this started looking suspicious:

 
{code:java}
{""timestamp"":1704415753165,""is_logging_enabled"":""false"",""logger_id"":""org.apache.flink.runtime.state.TaskStateManagerImpl"",""log_level"":""DEBUG"", ""message"":""Operator 156a1ebbc1936f7d4558c8070b35ba93 has remote state SubtaskState{operatorStateFromBackend=StateObjectCollection{ [OperatorStateHandle{stateNameToPartitionOffsets={SourceReaderState=StateMetaInfo{offsets=[244, 244], distributionMode=SPLIT_DISTRIBUTE}}, delegateStateHandle=ByteStreamStateHandle{handleName='gs://data-beaver/checkpoints/moj-tj-dummy-partition-loss-debug-v1/6e1ba15b1b5bedda64836ff48ed1c264/chk-3/fadb4f23-85dd-4048-b466-94c1c5329dd3', dataBytes=328}}, OperatorStateHandle{stateNameToPartitionOffsets={SourceReaderState=StateMetaInfo{offsets=[244, 244], distributionMode=SPLIT_DISTRIBUTE}}, delegateStateHandle=ByteStreamStateHandle{handleName='gs://data-beaver/checkpoints/moj-tj-dummy-partition-loss-debug-v1/6e1ba15b1b5bedda64836ff48ed1c264/chk-3/102aa50b-78c2-457e-9a2f-0055f1dbeb98', dataBytes=328}}]}, operatorStateFromStream=StateObjectCollection{[]}, keyedStateFromBackend=StateObjectCollection{[]}, keyedStateFromStream=StateObjectCollection{[]}, inputChannelState=StateObjectCollection{[]}, resultSubpartitionState=StateObjectCollection{[]}, stateSize=656, checkpointedSize=656} from job manager and local state alternatives [] from local state store org.apache.flink.runtime.state.NoOpTaskLocalStateStoreImpl@1f89f054."", ""service_name"":""data-beaver""}{code}
 

We see these strange offsets *offsets=[244, 244]* that look weird.

And this is a clearly wrong. Because when restoring from snapshot, [this code|https://github.com/apache/flink/blob/881062f352f8bf8c21ab7cbea95e111fd82fdf20/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/RoundRobinOperatorStateRepartitioner.java#L350] will redistribute offsets to different batches - and they will read the same value.

These offsets are produced by [this|https://github.com/apache/flink/blob/263f3283724a5081e41f679659fa6a5819350739/flink-runtime/src/main/java/org/apache/flink/runtime/state/PartitionableListState.java#L110] code:

{code:java}
public long[] write(FSDataOutputStream out) throws IOException {

      long[] partitionOffsets = new long[internalList.size()];

      DataOutputView dov = new DataOutputViewStreamWrapper(out);

      for (int i = 0; i < internalList.size(); ++i) {
          S element = internalList.get(i);
          partitionOffsets[i] = out.getPos();
          getStateMetaInfo().getPartitionStateSerializer().serialize(element, dov);
      }

      return partitionOffsets;
} {code}
The actual implementation that’s being used in this piece of code is [CompressibleFSDataOutputStream|https://github.com/apache/flink/blob/263f3283724a5081e41f679659fa6a5819350739/flink-runtime/src/main/java/org/apache/flink/runtime/state/CompressibleFSDataOutputStream.java#L30].

At this moment we realised that we have snapshot compression enabled (execution.checkpointing.snapshot-compression = true).

If we take a look into how getPos() is implemented in CompressibleFSDataOutputStream, we'd see that getPos() is delegated to the actual output stream, while writing is happening through compressing delegate:

 
{code:java}
public CompressibleFSDataOutputStream(
            CheckpointStateOutputStream delegate, StreamCompressionDecorator compressionDecorator)
            throws IOException {
        this.delegate = delegate;
        this.compressingDelegate = compressionDecorator.decorateWithCompression(delegate);
    }

    @Override
    public long getPos() throws IOException {
        return delegate.getPos();
    }

    @Override
    public void write(int b) throws IOException {
        compressingDelegate.write(b);
    } {code}
This is incorrect when compression is enabled, because compressing delegate doesn't flush data into the actual output stream immediately ([link|https://github.com/xerial/snappy-java/blob/ebfbdead182937463735729bd8fe5f4cd69235e4/src/main/java/org/xerial/snappy/SnappyFramedOutputStream.java#L279]):
{code:java}
@Override
public void write(int b)
        throws IOException
{
   if (closed) {
       throw new IOException(""Stream is closed"");
   }
   if (buffer.remaining() <= 0) {
     flushBuffer();
   }
   buffer.put((byte) b);
} {code}
Hence, the position in the _delegate_ doesn't get updated, and all offsets end up being the same.
h2. Simplest reproducing scenario

Now as we know the culprit, a simple reproducing scenario (verified) is the following, that can be checked locally eassily:
 * Create a Kafka topic with say 20 partitions
 * Launch a job reading from this topic with some parallelism, say 5. *Important: snapshot compression should be enabled in this job*
 * Stop the job with savepoint
 * Restore the job from this savepoint and pick a different parallelism, say 3.
 * Result: some Kafka partitions will not be consumed anymore.

 ",Can be reproduced in any environment. The most important thing is to enable snapshot compression.,,,,,,,,,,,,,,,,,,,,,,,FLINK-34096,,,,,,FLINK-30113,FLINK-33863,,,,,,,,,,,"11/Jan/24 16:27;isburmistrov;image-2024-01-11-16-27-09-066.png;https://issues.apache.org/jira/secure/attachment/13065912/image-2024-01-11-16-27-09-066.png","11/Jan/24 16:30;isburmistrov;image-2024-01-11-16-30-47-466.png;https://issues.apache.org/jira/secure/attachment/13065911/image-2024-01-11-16-30-47-466.png",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 11:33:50 UTC 2024,,,,,,,,,,"0|z1mp2g:",9223372036854775807,"Fixes a regression introduced in 1.18.0, where operator state couldn't be properly restored in case snapshot compression is enabled. This could have led to either data loss or disability to restore from checkpoint.

Any snapshots with enabled compression, that include operator state, taken from either Flink 1.18.0 or 1.18.1, need to be discarded, because they're corrupted.",,,,,,,,,,,,,,,,,,,"12/Jan/24 08:32;pnowojski;[~dmvk] could this issue be discovered by our ITCases if snapshot compression had been enabled in them? Regardless of that, it is probably a good idea to enable it using our configuration randomisation framework: {{org.apache.flink.streaming.util.TestStreamEnvironment#randomizeConfiguration}}.;;;","12/Jan/24 16:00;yang;FYI, I have encountered also the issue of losing kafka partition in some slot after rescaling. But I don't use snapshot compression. 

I have used Flink 1.18.0, kafka source connector 3.0.1-1.18. During our observation, even if we don't use autoscaling, this issue could happen after the source parallelism upgrade.

And by experience I don't think we have this issue in flink 1.17;;;","12/Jan/24 17:24;dmvk;Hi [~yang], it would be great if you open a separate issue a provide more details. We need more context to pin down the root cause of what you're describing.;;;","14/Jan/24 19:46;yang;Ok [~dmvk] , I'll do it;;;","15/Jan/24 15:00;yang;Hello [~dmvk] , FYI I have created a seperate ticket to report the issues I have encountered https://issues.apache.org/jira/browse/FLINK-34096 ;;;","19/Jan/24 11:33;dmvk;master:

161378ad3a234f4ff17b6fd4e6f950e232e16a6f

006a0869e32f64dbbe0b6d6142fc36bf7544ac5c

6f76982e26405ee1a8531539e9d2c4b2c5e001c5

d536b5a7d73bb0696291d6ea7c47544f4edd3e77

b7c314b46acdde0936a0e2d329f48972d1ea3e0b

 

release-1.18:

1cd68f954ee7d5360a98a779bd93c3cc7144c5a6

5e40a556011949bd84d401d3161f4007beb56bce

c334cbf24960978461bb3c64eead9907ea10fc99

5136c2513b4601c4e86806ad3c1af47aa9b7670e

e39559895ab694e427df7a840a5e37a3a510969d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate <excludedGroups/> in the surefire-plugin configuration for Java 21,FLINK-34062,13564341,13552124,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,11/Jan/24 11:41,31/Jan/24 11:45,04/Jun/24 20:40,31/Jan/24 11:45,,,,,,,,1.19.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 11:45:43 UTC 2024,,,,,,,,,,"0|z1mov4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 11:45;mapohl;master: [f4c0b716b01200d5e02bb49d9d859934867080a4|https://github.com/apache/flink/commit/f4c0b716b01200d5e02bb49d9d859934867080a4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add explicit exclusion of JDK-related excluded groups in the surefire-plugin config,FLINK-34061,13564340,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,11/Jan/24 11:40,11/Jan/24 11:40,04/Jun/24 20:40,,1.18.0,1.19.0,,,,,,,,,,Build System,,,,0,,,,,"The <excludedGroups/> property of the surefire-plugin doesn't support merging groups, i.e. if -Pjava11 and -Pjava17 are set only the @FailsWithJava17 groups will be considered. This hasn't been detected because we don't have Java11-specific test exclusions anymore. Still, we should fix this in the parent pom.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-11 11:40:31.0,,,,,,,,,,"0|z1mouw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate UserDefinedTableAggFunctions to JavaUserDefinedTableAggFunctions,FLINK-34060,13564325,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,xuyangzhong,qingyue,qingyue,11/Jan/24 09:30,08/Feb/24 03:47,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,"The issue is discovered when testing FLINK-31788.

The Top3 function emits a tuple of (entry.getKey, entry.getKey) see [UserDefinedTableAggFunctions.scala#L127|https://github.com/apache/flink/blob/907d0f32126b9f8acfc80f3f4098e71cb37f0e37/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/utils/UserDefinedTableAggFunctions.scala#L127], which is peculiar.

Meanwhile, consider getting the scala-free goal; it's time to migrate this class to the `JavaUserDefinedTableAggFunctions`, and revisit the implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31788,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 09:33:27 UTC 2024,,,,,,,,,,"0|z1mork:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 09:33;xuyangzhong;Hi, [~qingyue] . I'd like to take this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation on how to use state TTL hint,FLINK-34059,13564285,13556057,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,11/Jan/24 03:51,30/Jan/24 01:37,04/Jun/24 20:40,30/Jan/24 01:37,1.19.0,,,,,,,1.19.0,,,,Documentation,Table SQL / API,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 01:37:08 UTC 2024,,,,,,,,,,"0|z1moio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 01:37;qingyue;Fixed in master a1200ff6ad78146076a5d3b304abd383f1677c29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support optional parameters for named parameters,FLINK-34058,13564278,13564274,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hackergin,hackergin,hackergin,11/Jan/24 02:31,29/Jan/24 15:41,04/Jun/24 20:40,29/Jan/24 15:41,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 15:41:21 UTC 2024,,,,,,,,,,"0|z1moh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 15:41;fsk119;Merged into master: 4cd43fc09bd6c2e4806792fa2cce71f54ec1a9dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support named parameters for functions,FLINK-34057,13564277,13564274,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,hackergin,hackergin,11/Jan/24 02:30,29/Jan/24 15:42,04/Jun/24 20:40,29/Jan/24 15:42,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-11 02:30:30.0,,,,,,,,,,"0|z1mogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support named parameters for procedures,FLINK-34056,13564276,13564274,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,hackergin,hackergin,11/Jan/24 02:29,29/Jan/24 15:42,04/Jun/24 20:40,29/Jan/24 15:42,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-11 02:29:52.0,,,,,,,,,,"0|z1mogo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a new annotation for named parameters,FLINK-34055,13564275,13564274,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hackergin,hackergin,hackergin,11/Jan/24 02:22,25/Jan/24 12:20,04/Jun/24 20:40,25/Jan/24 12:20,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Introduce a new annotation to specify the parameter name, indicate if it is optional, and potentially support specifying default values in the future.

Deprecate the argumentNames method in FunctionHints as it is not user-friendly for specifying argument names with optional configuration.

 
{code:java}
public @interface ArgumentHint {
    /**
     * The name of the parameter, default is an empty string.
     */
    String name() default """";
 
    /**
     * Whether the parameter is optional, default is false.
     */
    boolean isOptional() default false;
 
    /**
     * The data type hint for the parameter.
     */
    DataTypeHint type() default @DataTypeHint();
}
{code}



{code:java}
public @interface FunctionHint {
  
    /**
     * Deprecated attribute for specifying the names of the arguments.
     * It is no longer recommended to use this attribute.
     */
    @Deprecated
    String[] argumentNames() default {""""};
  
    /**
     * Attribute for specifying the hints and additional information for function arguments.
     */
    ArgumentHint[] arguments() default {};
}
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 12:19:39 UTC 2024,,,,,,,,,,"0|z1mogg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 12:19;fsk119;Merged into master:

e1ebcaff52f423fdb54e3cb1bf8d5b3ccafc0a2f

9171194ef2647af1b55e58b98daeebabb6c84ad7

eb848dc0676d2c06adb3d16b3c6e51b378d4c57b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-387: Support named parameters for functions and procedures,FLINK-34054,13564274,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,11/Jan/24 02:18,26/Feb/24 11:28,04/Jun/24 20:40,26/Feb/24 11:28,,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,,Umbrella issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-387%3A+Support+named+parameters+for+functions+and+call+procedures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34303,FLINK-34355,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 06:49:01 UTC 2024,,,,,,,,,,"0|z1mog8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 06:49;hackergin;[~fsk119] I am willing to complete this task, please help assign it to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support state TTL hint for group aggregate,FLINK-34053,13564273,13556057,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,qingyue,qingyue,11/Jan/24 02:09,30/Jan/24 08:25,04/Jun/24 20:40,30/Jan/24 01:36,1.19.0,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34271,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 01:36:25 UTC 2024,,,,,,,,,,"0|z1mog0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 02:11;xuyangzhong;Hi, [~qingyue] Can I take this jira?;;;","30/Jan/24 01:36;qingyue;Fixed in master f32b2a9e347d7539819a88252e4e32deba247515;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing TopSpeedWindowing and SessionWindowing JARs in Flink Maven Repository,FLINK-34052,13564207,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,JunRuiLi,JunRuiLi,10/Jan/24 14:43,29/Jan/24 07:36,04/Jun/24 20:40,29/Jan/24 07:36,1.18.0,,,,,,,1.19.0,,,,Build System,Examples,,,0,pull-request-available,,,,"As a result of the changes implemented in FLINK-32821, the build process no longer produces artifacts with the names flink-examples-streaming-1.x-TopSpeedWindowing.jar and flink-examples-streaming-1.x-SessionWindowing.jar. This has led to the absence of these specific JAR files in the Maven repository (https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming/1.18.0/).

These artifacts were previously available and may still be expected by users as part of their application dependencies. Their removal could potentially break existing build pipelines and applications that depend on these example JARs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32821,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 07:36:51 UTC 2024,,,,,,,,,,"0|z1mo1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/24 14:50;JunRuiLi;Hi [~Zhanghao Chen] , Could you have a look?;;;","16/Jan/24 05:38;Zhanghao Chen;Thanks for reporting this, I'll take a look. [~huweihua] could you assign this to me? ;;;","27/Jan/24 16:25;Zhanghao Chen;After invertigation, it turned out that the issue was resulted as follows:
 * The maven shade plugin will by default shade replaces with original jar with the result of shading. So, when a {{pom.xml}} includes two shades, the second shade execution will (by default) start from the result of the first shade execution ([ref|https://maven.apache.org/plugins/maven-shade-plugin/faq.html]).
 *  We set the [finalName|https://maven.apache.org/plugins/maven-shade-plugin/shade-mojo.html#finalName] param to prevent the file replacement behavior as we'd like to build multiple independent example jars.
 * However, maven by default will only install the default project artifact into the repo, omitting those shaded artifacts with finalName specified. This leads to the problem as reported in this issue.

To fix it, we can set the [shadedArtifactAttached|https://maven.apache.org/plugins/maven-shade-plugin/examples/attached-artifact.html] param, which tells maven to install the shaded artifact aside from the default project artifact. I've attached a fix PR on it.;;;","29/Jan/24 07:36;Weijie Guo;master(1.19) via 324a5e45c80464335a95cad7fbccfd531d0b098b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix equals/hashCode/toString for SavepointRestoreSettings,FLINK-34051,13564205,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,masteryhx,masteryhx,masteryhx,10/Jan/24 14:33,12/Jan/24 06:00,04/Jun/24 20:40,12/Jan/24 06:00,1.19.0,,,,,,,1.19.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,SavepointRestoreSettings#equals/hashCode/toString missed restoreMode property,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 06:00:45 UTC 2024,,,,,,,,,,"0|z1mo0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 06:00;masteryhx;merged 7745ef83 and ecb68704 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rocksdb state has space amplification after rescaling with DeleteRange,FLINK-34050,13564196,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,lijinzhong,lijinzhong,10/Jan/24 13:35,28/Feb/24 11:52,04/Jun/24 20:40,28/Feb/24 11:52,,,,,,,,1.20.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"FLINK-21321 use deleteRange to speed up rocksdb rescaling, however it will cause space amplification in some case.

We can reproduce this problem using wordCount job:

1) before rescaling, state operator in wordCount job has 2 parallelism and 4G+ full checkpoint size;

!image-2024-01-10-21-24-10-983.png|width=266,height=130!

2) then restart job with 4 parallelism (for state operator),  the full checkpoint size of new job will be 8G+ ;

3) after many successful checkpoints, the full checkpoint size is still 8G+;

!image-2024-01-10-21-28-24-312.png|width=454,height=111!

 

The root cause of this issue is that the deleted keyGroupRange does not overlap with current DB keyGroupRange, so new data written into rocksdb after rescaling almost never do LSM compaction with the deleted data (belonging to other keyGroupRange.)

 

And the space amplification may affect Rocksdb read performance and disk space usage after rescaling. It looks like a regression due to the introduction of deleteRange for rescaling optimization.

 

To slove this problem, I think maybe we can invoke Rocksdb.deleteFilesInRanges after deleteRange?
{code:java}
public static void clipDBWithKeyGroupRange() {
  //.......
  List<byte[]> ranges = new ArrayList<>();
  //.......
  deleteRange(db, columnFamilyHandles, beginKeyGroupBytes, endKeyGroupBytes);
  ranges.add(beginKeyGroupBytes);
  ranges.add(endKeyGroupBytes);
  //....

  for (ColumnFamilyHandle columnFamilyHandle : columnFamilyHandles) {
     db.deleteFilesInRanges(columnFamilyHandle, ranges, false);
  }
}


{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/24 13:23;lijinzhong;image-2024-01-10-21-23-48-134.png;https://issues.apache.org/jira/secure/attachment/13065873/image-2024-01-10-21-23-48-134.png","10/Jan/24 13:24;lijinzhong;image-2024-01-10-21-24-10-983.png;https://issues.apache.org/jira/secure/attachment/13065872/image-2024-01-10-21-24-10-983.png","10/Jan/24 13:28;lijinzhong;image-2024-01-10-21-28-24-312.png;https://issues.apache.org/jira/secure/attachment/13065871/image-2024-01-10-21-28-24-312.png",,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 11:51:39 UTC 2024,,,,,,,,,,"0|z1mnyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 06:42;mayuehappy;Hi [~lijinzhong]  

Thanks for reporting this issue. We have also encountered it before. I think this is a great suggestion.

Overall, this is still a trade off of time and space

If recovery time is the most important, then we can use deleteRange

If we want to achieve good recovery time and space amplification, then we can use deleteRange+deleteFilesInRanges

If space enlargement is very important, then we can consider deleteRange+deleteFilesInRanges+CompactRanges

(Of course, perhaps we can see if there are other ways to change space reclamation to an asynchronous process);;;","12/Jan/24 07:26;masteryhx;Thanks [~lijinzhong] and [~mayuehappy] providing this information.

IMO, maybe deleteRange+deleteFilesInRanges could be a good default behavious:
 # Only delteRange may cause some extra space usage which even last forever.
 # adding deleteFilesInRanges should not cost too much time than before because checking then deleting files should be quick.
 # Remaining files should still contain ranges which current keys and compaction will finally access. 

Of course, we could provide some performace check about deleteRange+deleteFilesInRanges vs deleteRange.;;;","15/Jan/24 02:49;lijinzhong;Thanks for your reply. [~mayuehappy]   [~masteryhx] 

IMO, it is unreasonable that redundant data can't be cleaned up for a long time after rescaling. Especially in scenarios where disk space is very tight, this behavior is a major drawback.

I agree with that deleteRange+deleteFilesInRanges could be a good default behaviors. 

As for the  performance check about deleteRange+deleteFilesInRanges vs deleteRange, i think the rescaling-state-benchmark should  satisfy this [1].

WDYT？ 

 

[1] https://github.com/apache/flink-benchmarks/blob/master/src/main/java/org/apache/flink/state/benchmark/RescalingBenchmarkBase.java;;;","15/Jan/24 07:17;masteryhx;[~lijinzhong] Yeah, I think this benchmark result should be enough.;;;","05/Feb/24 10:40;srichter;Just one idea: since the current proposal is making the rescaling times worse, it can have significant drawback. How about we call deleteFiles async before the next checkpoint after a rescaling, thus making sure that the space amplification never makes it into the checkpoint and doing it outside of a critical path for restoring. Wdyt?;;;","21/Feb/24 13:41;lijinzhong;[~srichter]   Thanks for your comments. I think: 
1. The current solution utilizes the deleteFilesInRanges api to bulk remove useless files. This process does not involve file data read and write, so it is expected to be very fast, with no significant impact on rescaling times. (I will validate this leveraging the rescaling benchmark)
2. In scenarios with large states, the space amplification problem mentioned in this issue may cause no space left on local disk during rescaling, resulting in a rescaling failure. Clearly, the async deletion can't solve this problem.

In addition, considering that the implementation of async deletion is more complex, so I think that current proposal (sync deletion) is the better way to solve this problem ?;;;","22/Feb/24 10:23;lijinzhong;Here are the benchmark results before and after applying this proposal.

 
||rescaleType ||Base||Applying this proposal||Diff||
|RESCALE_IN|42536.540 ms|42219.484 ms|-0.3s|
|RESCALE_OUT|595.798 ms|619.703 ms|+0.02s|

From this result, it can be seen that this change has no significant impact on rescaling performance.

 

More detailed data for reference:
||rescaleType||Base||Applying this proposal||
|RESCALE_IN|Fork 1 of 3 
Iteration 1: 44214.024 ms/op
Iteration 2: 44473.591 ms/op
Iteration 3: 41143.378 ms/op
Iteration 4: 45364.796 ms/op
Iteration 5: 45955.292 ms/op
Iteration 6: 41078.509 ms/op
Iteration 7: 45984.066 ms/op
Iteration 8: 41000.731 ms/op
Iteration 9: 45595.620 ms/op
Iteration 10: 46044.924 ms/op
Fork 2 of 3
Iteration 1: 34444.761 ms/op
Iteration 2: 43152.346 ms/op
Iteration 3: 43060.378 ms/op
Iteration 4: 44337.494 ms/op
Iteration 5: 34670.528 ms/op
Iteration 6: 42514.179 ms/op
Iteration 7: 34496.979 ms/op
Iteration 8: 41989.620 ms/op
Iteration 9: 44067.735 ms/op
Iteration 10: 44704.516 ms/op
Fork 3 of 3
Iteration 1: 43385.168 ms/op
Iteration 2: 43096.595 ms/op
Iteration 3: 43370.825 ms/op
Iteration 4: 45175.983 ms/op
Iteration 5: 34956.635 ms/op
Iteration 6: 43147.011 ms/op
Iteration 7: 42810.926 ms/op
Iteration 8: 44908.913 ms/op
Iteration 9: 44195.383 ms/op
Iteration 10: 42755.294 ms/op
 
Result:  {color:#ff0000}42536.540 ms{color}|Fork 1 of 3 
Iteration 1: 44987.651 ms/op
Iteration 2: 45913.319 ms/op
Iteration 3: 34740.433 ms/op
Iteration 4: 43833.981 ms/op
Iteration 5: 44912.708 ms/op
Iteration 6: 45030.893 ms/op
Iteration 7: 44079.639 ms/op
Iteration 8: 34754.194 ms/op
Iteration 9: 42423.861 ms/op
Iteration 10: 42765.109 ms/op
Fork 2 of 3 
Iteration 1: 44712.705 ms/op
Iteration 2: 44599.266 ms/op
Iteration 3: 45105.132 ms/op
Iteration 4: 42825.562 ms/op
Iteration 5: 45664.281 ms/op
Iteration 6: 34835.676 ms/op
Iteration 7: 43294.868 ms/op
Iteration 8: 43319.576 ms/op
Iteration 9: 44627.813 ms/op
Iteration 10: 41309.822 ms/op
Fork 3 of 3 
Iteration 1: 41423.187 ms/op
Iteration 2: 42499.661 ms/op
Iteration 3: 42638.880 ms/op
Iteration 4: 43574.138 ms/op
Iteration 5: 34969.848 ms/op
Iteration 6: 43349.239 ms/op
Iteration 7: 41596.289 ms/op
Iteration 8: 42500.620 ms/op
Iteration 9: 40192.633 ms/op
Iteration 10: 40103.528 ms/op
 
Result: {color:#ff0000}42219.484 ms{color}|
|RESCALE_OUT|Fork 1 of 3 
Iteration 1: 648.341 ms/op
Iteration 2: 588.388 ms/op
Iteration 3: 598.590 ms/op
Iteration 4: 585.059 ms/op
Iteration 5: 585.281 ms/op
Iteration 6: 585.623 ms/op
Iteration 7: 587.027 ms/op
Iteration 8: 584.607 ms/op
Iteration 9: 586.894 ms/op
Iteration 10: 588.937 ms/op
Fork 2 of 3 
Iteration 1: 572.042 ms/op
Iteration 2: 684.044 ms/op
Iteration 3: 578.065 ms/op
Iteration 4: 569.977 ms/op
Iteration 5: 684.787 ms/op
Iteration 6: 628.206 ms/op
Iteration 7: 636.114 ms/op
Iteration 8: 581.125 ms/op
Iteration 9: 565.372 ms/op
Iteration 10: 573.666 ms/op
Fork 3 of 3 
Iteration 1: 579.150 ms/op
Iteration 2: 570.446 ms/op
Iteration 3: 572.630 ms/op
Iteration 4: 637.539 ms/op
Iteration 5: 578.346 ms/op
Iteration 6: 572.454 ms/op
Iteration 7: 570.402 ms/op
Iteration 8: 645.658 ms/op
Iteration 9: 566.496 ms/op
Iteration 10: 568.685 ms/op
 
Result: {color:#ff0000}595.798 ms{color}|Fork 1 of 3 
Iteration 1: 628.622 ms/op
Iteration 2: 665.121 ms/op
Iteration 3: 572.333 ms/op
Iteration 4: 569.295 ms/op
Iteration 5: 576.826 ms/op
Iteration 6: 578.466 ms/op
Iteration 7: 604.477 ms/op
Iteration 8: 628.138 ms/op
Iteration 9: 588.552 ms/op
Iteration 10: 667.854 ms/op
Fork 2 of 3 
Iteration 1: 702.258 ms/op
Iteration 2: 625.250 ms/op
Iteration 3: 587.017 ms/op
Iteration 4: 600.382 ms/op
Iteration 5: 701.370 ms/op
Iteration 6: 617.869 ms/op
Iteration 7: 615.285 ms/op
Iteration 8: 657.985 ms/op
Iteration 9: 641.899 ms/op
Iteration 10: 595.750 ms/op
Fork 3 of 3 
Iteration 1: 586.578 ms/op
Iteration 2: 634.318 ms/op
Iteration 3: 717.040 ms/op
Iteration 4: 708.608 ms/op
Iteration 5: 573.883 ms/op
Iteration 6: 589.254 ms/op
Iteration 7: 572.359 ms/op
Iteration 8: 622.834 ms/op
Iteration 9: 590.137 ms/op
Iteration 10: 571.335 ms/op
 
Result: {color:#ff0000}619.703 ms{color}|

 
 
 ;;;","28/Feb/24 11:51;lijinzhong;commit  534ea31 and b048a9c  has been merged into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Refactor classes related to window TVF aggregation to prepare for non-aligned windows,FLINK-34049,13564164,13335153,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,xuyangzhong,xuyangzhong,xuyangzhong,10/Jan/24 08:24,22/Jan/24 02:17,04/Jun/24 20:40,22/Jan/24 02:17,1.19.0,,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,Refactor classes related to window TVF aggregation such as AbstractWindowAggProcessor to prepare for the implementation of non-aligned windows like session window,,,,,,,,,,,,,FLINK-34048,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 02:16:03 UTC 2024,,,,,,,,,,"0|z1mnrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 02:16;fsk119;Merged into master: e345ffb453ac482d0250736b687cf88b85c606b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support session window agg in table runtime instead of using legacy group window agg op,FLINK-34048,13564163,13335153,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,10/Jan/24 08:20,25/Jan/24 15:39,04/Jun/24 20:40,25/Jan/24 09:35,1.19.0,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,"We not only need FLINK-24024  to support session window agg in planner, but also need to support it in runtime.

This subtask only resolves the session window agg with TimeAttributeWindowingStrategy.",,,,,,,,,,,,FLINK-34049,FLINK-34100,,,,,,,,,,,,,,,,,,,,,,,FLINK-24024,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 09:35:03 UTC 2024,,,,,,,,,,"0|z1mnrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 09:35;lsy;Merged in master: de7322112cbf594ae4b74163b5e58892e6cdf4ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inject GitHub Actions/Azure Pipelines env variables in uploading_watchdog.sh,FLINK-34047,13564162,13562450,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,10/Jan/24 08:08,12/Jan/24 07:49,04/Jun/24 20:40,12/Jan/24 07:49,1.18.0,1.19.0,,,,,,1.18.1,1.19.0,,,Build System / CI,,,,0,github-actions,pull-request-available,,,The workflow that's triggered by {{tools/azure-pipelines/uploading_watchdog.sh}} relies on CI specific environment variables. We should make the two different CI backends explicit in this script to improve the code readability.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 07:49:06 UTC 2024,,,,,,,,,,"0|z1mnrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 07:49;mapohl;master: [f29ee07f5e568fa94da75f3879a9aa7016dfcd5a|https://github.com/apache/flink/commit/f29ee07f5e568fa94da75f3879a9aa7016dfcd5a]
1.18: [3090e772192f4f9444e030b199cd95fc6c5afdd5|https://github.com/apache/flink/commit/3090e772192f4f9444e030b199cd95fc6c5afdd5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics to AsyncWaitOperator Retry Flow,FLINK-34046,13564129,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,sdineshkumar1985,sdineshkumar1985,10/Jan/24 00:53,10/Jan/24 02:03,04/Jun/24 20:40,,,,,,,,,,,,,API / DataStream,,,,0,pull-request-available,,,,"AsyncWaitOperator supports retry if retry Strategy is set. But there is no metrics to count the messages retried, message retry succeeded and dropped message count after reaching configured retry count.

To address this we propose to add metrics for Retry Count, Retry Success Count and Dropped after max retry Count.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-10 00:53:07.0,,,,,,,,,,"0|z1mnk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add docs generation to workflow,FLINK-34045,13564080,13562450,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,09/Jan/24 16:02,23/Feb/24 12:45,04/Jun/24 20:40,,1.18.0,1.19.0,,,,,,,,,,Build System / CI,,,,0,github-actions,pull-request-available,,,"The goal of this subtask is to mimick the optional docs generation from the Azure Pipelines workflow. This includes:
* Running the docs check in each PR CI if some artifacts in the ./docs folder were touched
* Do not run the docs build if it's not a PR
* Skip e2e tests in the Flink CI template if the PR is an docs-only change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-09 16:02:34.0,,,,,,,,,,"0|z1mn94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis Sink Cannot be Created via TableDescriptor,FLINK-34044,13564069,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,tilman151,tilman151,09/Jan/24 15:08,22/Mar/24 10:46,04/Jun/24 20:40,22/Mar/24 10:46,aws-connector-4.2.0,,,,,,,aws-connector-4.3.0,,,,Connectors / AWS,,,,1,pull-request-available,,,,"When trying to create a Kinesis Stream Sink in Table API via a TableDescriptor I get an error:
{code:java}
Caused by: java.lang.UnsupportedOperationException
    at java.base/java.util.Collections$UnmodifiableMap.remove(Collections.java:1460)
    at org.apache.flink.connector.kinesis.table.util.KinesisStreamsConnectorOptionsUtils$KinesisProducerOptionsMapper.removeMappedOptions(KinesisStreamsConnectorOptionsUtils.java:249)
    at org.apache.flink.connector.kinesis.table.util.KinesisStreamsConnectorOptionsUtils$KinesisProducerOptionsMapper.mapDeprecatedClientOptions(KinesisStreamsConnectorOptionsUtils.java:158)
    at org.apache.flink.connector.kinesis.table.util.KinesisStreamsConnectorOptionsUtils.<init>(KinesisStreamsConnectorOptionsUtils.java:90)
    at org.apache.flink.connector.kinesis.table.KinesisDynamicTableSinkFactory.createDynamicTableSink(KinesisDynamicTableSinkFactory.java:61)
    at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:267)
    ... 20 more
{code}
Here is a minimum reproducing example with Flink-1.17.2 and flink-connector-kinesis-4.2.0:
{code:java}
public class Job {
  public static void main(String[] args) throws Exception {
    // create data stream environment
    StreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();
    sEnv.setRuntimeMode(RuntimeExecutionMode.STREAMING);
    StreamTableEnvironment tEnv = StreamTableEnvironment.create(sEnv);

    Schema a = Schema.newBuilder().column(""a"", DataTypes.STRING()).build();
    tEnv.createTemporaryTable(
        ""exampleTable"", TableDescriptor.forConnector(""datagen"").schema(a).build());
    TableDescriptor descriptor =
        TableDescriptor.forConnector(""kinesis"")
            .schema(a)
            .format(""json"")
            .option(""stream"", ""abc"")
            .option(""aws.region"", ""eu-central-1"")
            .build();
    tEnv.createTemporaryTable(""sinkTable"", descriptor);

    tEnv.from(""exampleTable"").executeInsert(""sinkTable""); // error occurs here
  }
} {code}
From my investigation, the error is triggered by the `ResolvedCatalogTable` used when re-mapping the deprecated Kinesis options in `KinesisProducerOptionsMapper`. The `getOptions` method of the table returns an `UnmodifiableMap` which is not mutable.

If the sink table is created via SQL, the error does not occur:
{code:java}
tEnv.executeSql(""CREATE TABLE sinkTable "" + descriptor.toString());
{code}
because `ResolvedCatalogTable.getOptions` returns a regular `HashMap`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 10:46:15 UTC 2024,,,,,,,,,,"0|z1mn6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/24 08:48;dannycranmer;Thanks for reporting this, [~tilman151] .;;;","12/Jan/24 09:51;khanhvu;I can take this one, [~dannycranmer];;;","13/Mar/24 09:10;chalixar;[~tilman151] Thanks for raising this and for the thorough bug report, I was able to easily reproduce this.

[~dannycranmer] I have submitted a PR for it, could you please take a look.;;;","13/Mar/24 09:29;danny.cranmer;Thanks [~chalixar] . I am on vacation this week, will take a look next week;;;","22/Mar/24 10:46;dannycranmer;Merged commit [{{35be16b}}|https://github.com/apache/flink-connector-aws/commit/35be16bfb5696f7fc1b261b6ab4f9cd95c403ed0] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated Sink V2 interfaces,FLINK-34043,13564055,13563444,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pvary,pvary,pvary,09/Jan/24 13:27,12/Jan/24 09:16,04/Jun/24 20:40,,,,,,,,,,,,,API / DataStream,,,,0,,,,,In Flink 1.20.0 we should remove the interfaces deprecated by FLINK-33973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-09 13:27:16.0,,,,,,,,,,"0|z1mn3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the documentation for Sink V2,FLINK-34042,13564054,13563444,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pvary,pvary,pvary,09/Jan/24 13:24,06/Feb/24 11:22,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,Check the documentation and update the Sink V2 API usages whenever it is needed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 11:22:55 UTC 2024,,,,,,,,,,"0|z1mn3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:22;pvary;Currently there is no existing documentation for the Sink V2 API.

It would be good to create on, but this is not a blocker;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in docs/content/docs/deployment/ha/overview.md,FLINK-34041,13564033,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,eeap,eeap,09/Jan/24 11:19,09/Jan/24 11:58,04/Jun/24 20:40,09/Jan/24 11:58,,,,,,,,,,,,,,,,0,pull-request-available,,,,"There is typo error in docs/content/docs/deployment/ha/overview.md
Leader election x => Leader selection",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/24 11:17;eeap;image-2024-01-09-20-17-09-097.png;https://issues.apache.org/jira/secure/attachment/13065831/image-2024-01-09-20-17-09-097.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 09 11:58:38 UTC 2024,,,,,,,,,,"0|z1mmyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/24 11:58;martijnvisser;Leader election is correct English grammar in the context of distributed systems. Also, please follow https://flink.apache.org/how-to-contribute/overview/, especially the parts on documentation spelling errors and grammar issues. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalaSerializersMigrationTest.testStableAnonymousClassnameGeneration fails in GHA with JDK 17 and 21,FLINK-34040,13564018,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,09/Jan/24 08:58,12/Jan/24 07:27,04/Jun/24 20:40,12/Jan/24 07:27,1.19.0,,,,,,,,,,,API / Scala,Build System / CI,,,0,github-actions,test-stability,,,"{code}
Error: 13:05:23 13:05:23.538 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.375 s <<< FAILURE! -- in org.apache.flink.api.scala.migration.ScalaSerializersMigrationTest
Error: 13:05:23 13:05:23.538 [ERROR] org.apache.flink.api.scala.migration.ScalaSerializersMigrationTest.testStableAnonymousClassnameGeneration -- Time elapsed: 0.371 s <<< FAILURE!
Jan 07 13:05:23 org.junit.ComparisonFailure: expected:<...MigrationTest$$anon$[8]> but was:<...MigrationTest$$anon$[1]>
Jan 07 13:05:23 	at org.junit.Assert.assertEquals(Assert.java:117)
Jan 07 13:05:23 	at org.junit.Assert.assertEquals(Assert.java:146)
Jan 07 13:05:23 	at org.apache.flink.api.scala.migration.ScalaSerializersMigrationTest.testStableAnonymousClassnameGeneration(ScalaSerializersMigrationTest.scala:60)
Jan 07 13:05:23 	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
{code}

The error only happens in the [master GHA nightly|https://github.com/XComp/flink/actions/workflows/nightly-dev.yml] for JDK 17 and 21.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 07:27:37 UTC 2024,,,,,,,,,,"0|z1mmvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/24 09:18;mapohl;The issue started to appear when rebasing my fork's master branch from [7d8f62f172|https://github.com/apache/flink/commit/7d8f62f17262545dc51a29399927c85215847fda] (last build without the issue) to [793a66b9c6|https://github.com/apache/flink/commit/793a66b9c6a82de9b1b0c5c547cf27d025d70f9c].;;;","09/Jan/24 10:06;martijnvisser;This look like the error that you would get with an upgraded Scala version?;;;","11/Jan/24 07:45;mapohl;Yeah no. [~snuyanzin] also already gave the hint that it might be related with the Scala version that's used in the GitHub workflow profiles. I don't do anything related to changing the Scala version. I have to double-check how we do it in Azure and apply the same for GHA.;;;","11/Jan/24 13:14;mapohl;:facepalm: The test class is annotated with {{@FailsOnjava17}}. master was probably not based on a version of FLINK-33914 where the {{<excludeGroups/>}} configuration property was correctly injects.;;;","12/Jan/24 07:27;mapohl;The error was, indeed caused by a wrong setup of the {{github-actions}} profile which caused the build to ignore the @FailsOnJava17 flag of the test.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The session group window agg operator does not split the session window when processing retrace records.,FLINK-34039,13564013,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,09/Jan/24 08:36,09/Jan/24 08:37,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"Add the test in GroupWindowITCase to reproduce this bug.
{code:java}
@TestTemplate
def test(): Unit = {
  env.setParallelism(1)
  val upsertSourceDataId = registerData(
    List(
      changelogRow(""+I"", ""a"", ""no1"", localDateTime(1L)),
      changelogRow(""+I"", ""a"", ""no1"", localDateTime(2L)),
      changelogRow(""+I"", ""a"", ""no1"", localDateTime(6L)),
      changelogRow(""+I"", ""a"", ""no1"", localDateTime(9L)),
      changelogRow(""-D"", ""a"", ""no1"", localDateTime(6L))
    ))
  tEnv.executeSql(s""""""
                     |CREATE TABLE upsert_currency (
                     |  pk STRING,
                     |  str STRING,
                     |  currency_time TIMESTAMP(3),
                     |  WATERMARK FOR currency_time AS currency_time - interval '5' SECOND
                     |) WITH (
                     |  'connector' = 'values',
                     |  'changelog-mode' = 'I,UB,UA,D',
                     |  'data-id' = '$upsertSourceDataId'
                     |)
                     |"""""".stripMargin)
  val sql =
    """"""
      |SELECT
      |pk,
      |COUNT(*) AS cnt,
      |SESSION_START(currency_time, INTERVAL '5' SECOND) as w_start,
      |SESSION_END(currency_time, INTERVAL '5' SECOND) as w_end
      |FROM upsert_currency
      |GROUP BY pk, SESSION(currency_time, INTERVAL '5' SECOND)
      |"""""".stripMargin
  val sink = new TestingAppendSink
  tEnv.sqlQuery(sql).toDataStream.addSink(sink)
  env.execute()
  println(sink.getAppendResults.sorted)
}{code}
The result is: 
{code:java}
a,3,1970-01-01T00:00:01,1970-01-01T00:00:14{code}
But if we change the source data as below:
{code:java}
val upsertSourceDataId = registerData(
      List(
        changelogRow(""+I"", ""a"", ""no1"", localDateTime(1L)),
        changelogRow(""+I"", ""a"", ""no1"", localDateTime(2L)),             
        // changelogRow(""+I"", ""a"", ""no1"", localDateTime(6L)),
        changelogRow(""+I"", ""a"", ""no1"", localDateTime(9L))
        // changelogRow(""-D"", ""a"", ""no1"", localDateTime(6L))
      )) {code}
The result will be:
{code:java}
a,1,1970-01-01T00:00:09,1970-01-01T00:00:14
a,2,1970-01-01T00:00:01,1970-01-01T00:00:07{code}
When there is a minibatch operator upstream and CDC messages may be folded, the results of the session group window agg node may be different.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-09 08:36:32.0,,,,,,,,,,"0|z1mmu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncrementalGroupAggregateRestoreTest.testRestore fails,FLINK-34038,13564006,13556317,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bvarghese,mapohl,mapohl,09/Jan/24 08:00,16/Feb/24 09:31,04/Jun/24 20:40,15/Feb/24 11:24,1.19.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,test-stability,,,,"{{IncrementalGroupAggregateRestoreTest.testRestore}} fails on {{master}}:
{code}
Jan 08 18:53:18 18:53:18.406 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 8.706 s <<< FAILURE! -- in org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalGroupAggregateRestoreTest
Jan 08 18:53:18 18:53:18.406 [ERROR] org.apache.flink.table.planner.plan.nodes.exec.stream.IncrementalGroupAggregateRestoreTest.testRestore(TableTestProgram, ExecNodeMetadata)[2] -- Time elapsed: 1.368 s <<< FAILURE!
Jan 08 18:53:18 java.lang.AssertionError: 
Jan 08 18:53:18 
Jan 08 18:53:18 Expecting actual:
Jan 08 18:53:18   [""+I[1, 5, 2, 3]"",
Jan 08 18:53:18     ""+I[2, 2, 1, 1]"",
Jan 08 18:53:18     ""-U[1, 5, 2, 3]"",
Jan 08 18:53:18     ""+U[1, 3, 2, 2]"",
Jan 08 18:53:18     ""-U[1, 3, 2, 2]"",
Jan 08 18:53:18     ""+U[1, 9, 3, 4]""]
Jan 08 18:53:18 to contain exactly in any order:
Jan 08 18:53:18   [""+I[1, 5, 2, 3]"", ""+I[2, 2, 1, 1]"", ""-U[1, 5, 2, 3]"", ""+U[1, 9, 3, 4]""]
Jan 08 18:53:18 but the following elements were unexpected:
Jan 08 18:53:18   [""+U[1, 3, 2, 2]"", ""-U[1, 3, 2, 2]""]
Jan 08 18:53:18 
Jan 08 18:53:18 	at org.apache.flink.table.planner.plan.nodes.exec.testutils.RestoreTestBase.testRestore(RestoreTestBase.java:292)
Jan 08 18:53:18 	at java.lang.reflect.Method.invoke(Method.java:498)
[...]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56110&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10822",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 16 09:31:10 UTC 2024,,,,,,,,,,"0|z1mmso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/24 08:02;mapohl;Most likely caused by FLINK-34000 because it introduced this test yesterday. [~dwysakowicz] can you have a look at it?;;;","09/Jan/24 08:51;dwysakowicz;I'll revert the offending committs. [~bvarghese] Could you take a look at the failure? ;;;","16/Feb/24 09:31;mapohl;[~dwysakowicz] can you add the commit message that fixed the issue? ...and/or the revert? Additionally, is this also fixed for 1.19?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-398: Improve Serialization Configuration And Usage In Flink,FLINK-34037,13564005,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Zhanghao Chen,zjureel,zjureel,09/Jan/24 07:52,08/May/24 03:39,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,API / Type Serialization System,Runtime / Configuration,,,0,2.0-related,,,,Improve serialization in https://cwiki.apache.org/confluence/display/FLINK/FLIP-398%3A+Improve+Serialization+Configuration+And+Usage+In+Flink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34328,FLINK-34354,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-09 07:52:35.0,,,,,,,,,,"0|z1mmsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Various HiveDialectQueryITCase tests fail in GitHub Actions workflow with Hadoop 3.1.3 enabled,FLINK-34036,13564003,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,09/Jan/24 07:48,12/Jan/24 07:26,04/Jun/24 20:40,12/Jan/24 07:26,1.18.0,1.19.0,,,,,,,,,,Connectors / Hadoop Compatibility,Connectors / Hive,,,0,github-actions,test-stability,,,"The following {{HiveDialectQueryITCase}} tests fail consistently in the FLINK-27075 GitHub Actions [master nightly workflow|https://github.com/XComp/flink/actions/workflows/nightly-dev.yml] of Flink (and also the [release-1.18 workflow|https://github.com/XComp/flink/actions/workflows/nightly-current.yml]):
* {{testInsertDirectory}}
* {{testCastTimeStampToDecimal}}
* {{testNullLiteralAsArgument}}
{code}
Error: 03:38:45 03:38:45.661 [ERROR] Tests run: 22, Failures: 1, Errors: 2, Skipped: 0, Time elapsed: 379.0 s <<< FAILURE! -- in org.apache.flink.connectors.hive.HiveDialectQueryITCase
Error: 03:38:45 03:38:45.662 [ERROR] org.apache.flink.connectors.hive.HiveDialectQueryITCase.testNullLiteralAsArgument -- Time elapsed: 0.069 s <<< ERROR!
Jan 09 03:38:45 java.lang.NoSuchMethodError: org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTimestamp(Ljava/lang/Object;Lorg/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector;)Ljava/sql/Timestamp;
Jan 09 03:38:45 	at org.apache.flink.connectors.hive.HiveDialectQueryITCase.testNullLiteralAsArgument(HiveDialectQueryITCase.java:959)
Jan 09 03:38:45 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 09 03:38:45 
Error: 03:38:45 03:38:45.662 [ERROR] org.apache.flink.connectors.hive.HiveDialectQueryITCase.testCastTimeStampToDecimal -- Time elapsed: 0.007 s <<< ERROR!
Jan 09 03:38:45 org.apache.flink.table.api.ValidationException: Table with identifier 'test-catalog.default.t1' does not exist.
Jan 09 03:38:45 	at org.apache.flink.table.catalog.CatalogManager.dropTableInternal(CatalogManager.java:1266)
Jan 09 03:38:45 	at org.apache.flink.table.catalog.CatalogManager.dropTable(CatalogManager.java:1206)
Jan 09 03:38:45 	at org.apache.flink.table.operations.ddl.DropTableOperation.execute(DropTableOperation.java:74)
Jan 09 03:38:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1107)
Jan 09 03:38:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:735)
Jan 09 03:38:45 	at org.apache.flink.connectors.hive.HiveDialectQueryITCase.testCastTimeStampToDecimal(HiveDialectQueryITCase.java:835)
Jan 09 03:38:45 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 09 03:38:45 
Error: 03:38:45 03:38:45.663 [ERROR] org.apache.flink.connectors.hive.HiveDialectQueryITCase.testInsertDirectory -- Time elapsed: 7.326 s <<< FAILURE!
Jan 09 03:38:45 org.opentest4j.AssertionFailedError: 
Jan 09 03:38:45 
Jan 09 03:38:45 expected: ""A:english=90#math=100#history=85""
Jan 09 03:38:45  but was: ""A:english=90math=100history=85""
Jan 09 03:38:45 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Jan 09 03:38:45 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Jan 09 03:38:45 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Jan 09 03:38:45 	at org.apache.flink.connectors.hive.HiveDialectQueryITCase.testInsertDirectory(HiveDialectQueryITCase.java:498)
Jan 09 03:38:45 	at java.lang.reflect.Method.invoke(Method.java:498)
{code}

Additionally, the {{HiveITCase}} in the e2e test suite is affected:
{code}
Error: 05:20:20 05:20:20.949 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.106 s <<< FAILURE! -- in org.apache.flink.tests.hive.HiveITCase
Error: 05:20:20 05:20:20.949 [ERROR] org.apache.flink.tests.hive.HiveITCase -- Time elapsed: 0.106 s <<< ERROR!
Jan 07 05:20:20 java.lang.ExceptionInInitializerError
Jan 07 05:20:20 	at sun.misc.Unsafe.ensureClassInitialized(Native Method)
Jan 07 05:20:20 	at java.lang.reflect.Field.acquireFieldAccessor(Field.java:1088)
Jan 07 05:20:20 	at java.lang.reflect.Field.getFieldAccessor(Field.java:1069)
Jan 07 05:20:20 	at java.lang.reflect.Field.get(Field.java:393)
Jan 07 05:20:20 Caused by: java.lang.RuntimeException: java.io.IOException: Multiple resource files were found matching the pattern .*sql-hive-.*.jar. Matches=[/home/runner/work/flink/flink/flink-end-to-end-tests/flink-end-to-end-tests-hive/target/dependencies/sql-hive-2.3.9_2.12.jar, /home/runner/work/flink/flink/flink-end-to-end-tests/flink-end-to-end-tests-hive/target/dependencies/sql-hive-3.1.3_2.12.jar]
Jan 07 05:20:20 	at org.apache.flink.test.resources.ResourceTestUtils.getResource(ResourceTestUtils.java:76)
Jan 07 05:20:20 	at org.apache.flink.tests.hive.HiveITCase.<clinit>(HiveITCase.java:88)
Jan 07 05:20:20 	... 4 more
Jan 07 05:20:20 Caused by: java.io.IOException: Multiple resource files were found matching the pattern .*sql-hive-.*.jar. Matches=[/home/runner/work/flink/flink/flink-end-to-end-tests/flink-end-to-end-tests-hive/target/dependencies/sql-hive-2.3.9_2.12.jar, /home/runner/work/flink/flink/flink-end-to-end-tests/flink-end-to-end-tests-hive/target/dependencies/sql-hive-3.1.3_2.12.jar]
Jan 07 05:20:20 	... 6 more
{code}

The most-recent build failures in GHA workflow failures are:
* https://github.com/XComp/flink/actions/runs/7455836411/job/20285758541#step:12:23332
* https://github.com/XComp/flink/actions/runs/7447254277/job/20259593089#step:12:23378
* https://github.com/XComp/flink/actions/runs/7442459819/job/20246101021#step:12:23332
* https://github.com/XComp/flink/actions/runs/7438111934/job/20236674470#step:12:23375
* https://github.com/XComp/flink/actions/runs/7435499743/job/20231030744#step:12:23367

Interestingly, the failure doesn't appear in the Azure Pipelines nightlies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 07:26:30 UTC 2024,,,,,,,,,,"0|z1mms0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/24 07:49;mapohl;[~yuxia] Do you have an idea about the cause?;;;","09/Jan/24 07:52;mapohl;The error for {{testInsertDirectory}} looks like an encoding issue:
{code}
Jan 09 03:38:45 expected: ""A:english=90#math=100#history=85""
Jan 09 03:38:45  but was: ""A:english=90math=100history=85""
{code};;;","09/Jan/24 07:53;mapohl;{{testNullLiteralAsArgument}} sounds like a classpath issue:
{code}
Jan 09 03:38:45 java.lang.NoSuchMethodError: org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTimestamp(Ljava/lang/Object;Lorg/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector;)Ljava/sql/Timestamp;
{code};;;","09/Jan/24 07:55;mapohl;It could be that it's a configuration issue in the GHA workflows. But I would assume that this would then also affect the 1.18 nightly workflow. Additionally, I haven't touched the hadoop workflow config around the time the issue started to appear. The only cause I could think of is a rebase onto a most-recent version of {{master}}.;;;","09/Jan/24 08:50;mapohl;Ok, I was wrong about the initial claim that it only happens on {{master}}. I see the same issue popping up for the [1.18 nightly workflow|https://github.com/XComp/flink/actions/workflows/nightly-current.yml]. That is a hint that it's most likely caused by a issue in the workflow configuration.

Still, any pointers to what might be the cause are appreciated. :-);;;","10/Jan/24 01:41;luoyuxia;[~mapohl] It also looks weird to me. I'll have a deep look when I'm free.  But considering in [https://github.com/apache/flink-connector-hive/pull/5,] we have moved it to a dedicated repo and the test pass for hadoop3, and we're to remove hive from flink repo, may be we can ignore it?;;;","11/Jan/24 08:51;mapohl;ignoring it is not an option because it also happens in release-1.18. I have to check whether I miss some setting in the GHA workflow.;;;","11/Jan/24 14:52;mapohl;The test failures can be reproduced locally:
{code}
$ ./mvnw -Dflink.hadoop.version=3.2.3 -Phadoop3-tests,hive3 -pl flink-connectors/flink-connector-hive verify -Dtest=org.apache.flink.connectors.hive.HiveDialectQueryITCase
{code}

It could have something to do with the nightly builds only running on Alibaba machines for the Azure Pipeline builds. But we have seen this test already [succeeding on GHA workflows|https://github.com/XComp/flink/actions/runs/7278032529/job/19831862705#step:12:23467].;;;","11/Jan/24 16:54;mapohl;The problem seems to be that we're not building the Flink artifacts with the PROFILE which causes issues when running the tests with the PROFILE. That is why it's possible to reproduce it locally. Because I didn't trigger the clean phase before running the tests with the profiles enabled.

This is also a problem in the GHA workflow configuration. The run_mvn configuration doesn't inject the PROFILE env variable properly.;;;","12/Jan/24 07:26;mapohl;Yup, it was caused by what I described in my comment above. Thanks for the pointer, [~chesnay]. I'm closing this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,
"when flinksql with group by partition field, some errors occured in jobmanager.log",FLINK-34035,13564001,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hansonhe,hansonhe,09/Jan/24 07:25,06/Feb/24 07:23,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,,0,,,,,"flink.version=1.17.1
kyuubi.version=1.8.0
hive.version=3.1.2
when run some hive sql as followings:
CREATE CATALOG bidwhive WITH ('type' = 'hive', 'hive-version' = '3.1.2', 'default-database' = 'test');
（1）select count({_}) from bidwhive.test.dws_test where dt='2024-01-02' ;{_}
_+---------+_
_| EXPR$0 |_
_+---------+_
_| 1317 |_
_+---------+_
_It's OK. There is no errors anywhere._
{_}（2）select dt,count({_}) from bidwhive.test.dws_test where dt='2024-01-02' group by dt;

{+}----------{+}------+
|dt|EXPR$1|

{+}----------{+}------+
|2024-01-02|1317|

{+}----------{+}------+

It can get correct result. But when i check jobmanager.log，I found some errors appeared repeatly as folowings.Sometimes the errors also appeared on the client terminal. I don't known whether these error will affect task runtime or not?. Can somebody help me to have a see?

'''
2024-01-09 14:03:25,979 WARN org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurred when fetching query results
java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: Coordinator of operator e9a3cbdf90f308bdf13b34acd6410e2b does not exist or the job vertex this operator belongs to is not initialized. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_191]
at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_191]
at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:170) ~[flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:129) [flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) [flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) [flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222) [flink-table-planner_b1e58bff-c004-4dba-b7d4-fff4e8145073.jar:1.17.1]
at org.apache.flink.table.gateway.service.result.ResultStore$ResultRetrievalThread.run(ResultStore.java:155) [flink-sql-gateway-1.17.1.jar:1.17.1]Caused by: org.apache.flink.util.FlinkException: Coordinator of operator e9a3cbdf90f308bdf13b34acd6410e2b does not exist or the job vertex this operator belongs to is not initialized. at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverCoordinationRequestToCoordinator(DefaultOperatorCoordinatorHandler.java:135) ~[flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.runtime.scheduler.SchedulerBase.deliverCoordinationRequestToCoordinator(SchedulerBase.java:1048) ~[flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.runtime.jobmaster.JobMaster.sendRequestToCoordinator(JobMaster.java:602) ~[flink-dist-1.17.1.jar:1.17.1]
at org.apache.flink.runtime.jobmaster.JobMaster.deliverCoordinationRequestToCoordinator(JobMaster.java:918) ~[flink-dist-1.17.1.jar:1.17.1]
at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) ~[?:?]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_191]
at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_191]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) ~[?:?]
at akka.actor.ActorCell.invoke(ActorCell.scala:547) ~[?:?]
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_191]
at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_191]
at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_191]
at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_191]
'''",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 07:23:25 UTC 2024,,,,,,,,,,"0|z1mmrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/24 22:11;walls.flink.m;Hi [~hansonhe] .

""{_}select dt,count({_}) from bidwhive.test.dws_test where dt='2024-01-02' group by dt;""

Why are you doing where and groupBy on the same column 'dt' ?;;;","06/Feb/24 07:23;hansonhe;[~walls.flink.m] 
No special reason to the same  column 'dt',just to have a test;
{_}If select dt,count(*{_}) from bidwhive.test.dws_test where dt >='2024-01-02' group by dt, jobmanager.log also have the same error.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When kv hint and list hint handle duplicate query hints, the results are different.",FLINK-34034,13563994,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,337361684@qq.com,xuyangzhong,xuyangzhong,09/Jan/24 06:48,04/Feb/24 09:51,04/Jun/24 20:40,04/Feb/24 09:49,1.18.0,1.18.1,1.19.0,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"When there are duplicate keys in the kv hint, calcite will overwrite the previous value with the later value.
{code:java}
@TestTemplate
def test(): Unit = {
  val sql =
    ""SELECT /*+ LOOKUP('table'='D', 'retry-predicate'='lookup_miss', 'retry-strategy'='fixed_delay', 'fixed-delay'='10s','max-attempts'='3', 'max-attempts'='4') */ * FROM MyTable AS T JOIN LookupTable "" +
      ""FOR SYSTEM_TIME AS OF T.proctime AS D ON T.a = D.id""
  util.verifyExecPlan(sql)
} {code}
{code:java}
Calc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id, name, age]) 
  +- LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], lookup=[id=a], select=[a, b, c, proctime, rowtime, id, name, age], retry=[lookup_miss, FIXED_DELAY, 10000ms, 4]) 
    +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
{code}
But when a list hint is duplicated (such as a join hint), we will choose the first one as the effective hint.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 04 09:49:50 UTC 2024,,,,,,,,,,"0|z1mmq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/24 07:07;qingyue;Hi [~xuyangzhong], thanks for reporting this issue.

According to the implementation, the description of the [hint doc|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/sql/queries/hints/#conflict-cases-in-join-hints] does not match the actual behavior.
{quote}If the Join Hints conflicts occur, Flink will choose the most matching one. Conflict in one same Join Hint strategy, Flink will choose the first matching table for a join. Conflict in different Join Hints strategies, Flink will choose the first matching hint for a join.
{quote}

However, I think having different behavior for the list option and KV options makes sense. For example, Java Map does not allow duplicate KVs, while the List is okay to have duplicated items.;;;","09/Jan/24 07:44;xuyangzhong;Hi, [~qingyue]

Our documentation currently seems not to expose whether table hints and join hints are kv hints or list hints. I believe that further exposing to users whether a certain type of query hint is a list or kv hint is not beneficial, as it would then need to tell users about their different behaviors: in case of a conflict, a list hint will choose the first one, whereas a kv hint, processed by Calcite, will select the last one when their keys are the same one. I prefer to let the framework ensure the consistency of these behaviors. WDYT?;;;","04/Feb/24 09:49;qingyue;Fixed in master 433b025b057b2be7453cafd6f363ecb4eee48642;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink json supports raw type ,FLINK-34033,13563991,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,09/Jan/24 04:20,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,"when user use es type nested/object, user can using complex type like ROW/ARRAY/MAP.

but it will not convenient when the object type is not fixed size like ROW. for example, user my user udf to produce those data and insert to es. we can supports RAW type
{code:java}
CREATE TABLE es_sink (
 `string` VARCHAR, 
  nested RAW('java.lang.Object', 'AEdvcmcuYXBhY2hlLmZsaW5rLmFwaS5qYXZhLnR5cGV1dGlscy5ydW50aW1lLmtyeW8uS3J5b1NlcmlhbGl6ZXJTbmFwc2hvdAAAAAIAEGphdmEubGFuZy5PYmplY3QAAATyxpo9cAAAAAIAEGphdmEubGFuZy5PYmplY3QBAAAAEgAQamF2YS5sYW5nLk9iamVjdAEAAAAWABBqYXZhLmxhbmcuT2JqZWN0AAAAAAApb3JnLmFwYWNoZS5hdnJvLmdlbmVyaWMuR2VuZXJpY0RhdGEkQXJyYXkBAAAAKwApb3JnLmFwYWNoZS5hdnJvLmdlbmVyaWMuR2VuZXJpY0RhdGEkQXJyYXkBAAAAtgBVb3JnLmFwYWNoZS5mbGluay5hcGkuamF2YS50eXBldXRpbHMucnVudGltZS5rcnlvLlNlcmlhbGl6ZXJzJER1bW15QXZyb1JlZ2lzdGVyZWRDbGFzcwAAAAEAWW9yZy5hcGFjaGUuZmxpbmsuYXBpLmphdmEudHlwZXV0aWxzLnJ1bnRpbWUua3J5by5TZXJpYWxpemVycyREdW1teUF2cm9LcnlvU2VyaWFsaXplckNsYXNzAAAE8saaPXAAAAAAAAAE8saaPXAAAAAA'),
  object RAW('java.lang.Object', 'AEdvcmcuYXBhY2hlLmZsaW5rLmFwaS5qYXZhLnR5cGV1dGlscy5ydW50aW1lLmtyeW8uS3J5b1NlcmlhbGl6ZXJTbmFwc2hvdAAAAAIAEGphdmEubGFuZy5PYmplY3QAAATyxpo9cAAAAAIAEGphdmEubGFuZy5PYmplY3QBAAAAEgAQamF2YS5sYW5nLk9iamVjdAEAAAAWABBqYXZhLmxhbmcuT2JqZWN0AAAAAAApb3JnLmFwYWNoZS5hdnJvLmdlbmVyaWMuR2VuZXJpY0RhdGEkQXJyYXkBAAAAKwApb3JnLmFwYWNoZS5hdnJvLmdlbmVyaWMuR2VuZXJpY0RhdGEkQXJyYXkBAAAAtgBVb3JnLmFwYWNoZS5mbGluay5hcGkuamF2YS50eXBldXRpbHMucnVudGltZS5rcnlvLlNlcmlhbGl6ZXJzJER1bW15QXZyb1JlZ2lzdGVyZWRDbGFzcwAAAAEAWW9yZy5hcGFjaGUuZmxpbmsuYXBpLmphdmEudHlwZXV0aWxzLnJ1bnRpbWUua3J5by5TZXJpYWxpemVycyREdW1teUF2cm9LcnlvU2VyaWFsaXplckNsYXNzAAAE8saaPXAAAAAAAAAE8saaPXAAAAAA'),
  PRIMARY KEY (`string`) NOT ENFORCED
) WITH
('connector'='elasticsearch'); {code}
and es is dependent on flink-json currently, so we can make flink-json supports RAW type",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-09 04:20:28.0,,,,,,,,,,"0|z1mmpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup local-recovery dir when switching local-recovery from enabled to disabled,FLINK-34032,13563984,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,masteryhx,masteryhx,09/Jan/24 03:32,09/Jan/24 04:33,04/Jun/24 20:40,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,,"When switching local-recovery from enabled to disabled, the local-recovery dir could not be cleaned.

In particular, for a job that switched multiple times, lots of historical local checkpoints will be retained forever.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 09 04:33:03 UTC 2024,,,,,,,,,,"0|z1mmns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/24 04:33;lijinzhong;Hi [~masteryhx] , would you please clarify the specific scenario of this issue?

IIUC, in most case, when switching local-recovery from enabled to disabled, user usually needs to restart the job. 
And the local-recovery dir will be cleaned up when k8s pod or yarn container are killed during job restart.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive table sources report statistics in various formats,FLINK-34031,13563983,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,baibaiwuchang,baibaiwuchang,09/Jan/24 03:28,09/Jan/24 03:28,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Hive,,,,0,,,,,"Now for hive table source, reporting statistics only support Orc and Parquet formats.

Currently,  we have some text format hive table. Somewhile text hive table as dimension table， task should use broadcast join, but text format table cannot obtain table stats. 

So, hive table sources report statistics in various formats, such  as `text`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-09 03:28:07.0,,,,,,,,,,"0|z1mmnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid using negative value for periodic-materialize.interval,FLINK-34030,13563974,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,masteryhx,masteryhx,masteryhx,09/Jan/24 01:37,10/Jan/24 02:02,04/Jun/24 20:40,10/Jan/24 02:01,1.19.0,,,,,,,1.19.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"Similar to FLINK-32023, a nagative value doesn't work for Duration Type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 10 02:01:36 UTC 2024,,,,,,,,,,"0|z1mmlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/24 02:01;masteryhx;merged a6c1f308 and 96d62017 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support different profiling mode on Flink WEB,FLINK-34029,13563973,13554911,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,Yu Chen,Yu Chen,09/Jan/24 01:16,23/Jan/24 05:49,04/Jun/24 20:40,23/Jan/24 05:49,1.19.0,,,,,,,1.19.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 05:49:21 UTC 2024,,,,,,,,,,"0|z1mmlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 05:49;yunta;merged in master: 4db6e72ed766791d25ee0379c7c29d1b4e2c08df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow multiple parallel async calls in a single operator,FLINK-34028,13563944,13563506,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,asheinberg,asheinberg,08/Jan/24 19:40,08/Jan/24 19:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-08 19:40:49.0,,,,,,,,,,"0|z1mmew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce AsyncScalarFunction as a new UDF type,FLINK-34027,13563943,13563506,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,asheinberg,asheinberg,asheinberg,08/Jan/24 19:40,05/Feb/24 14:35,04/Jun/24 20:40,15/Jan/24 14:44,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,Initial PR for FLIP-400. Excluding documentation and restore tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 14:35:33 UTC 2024,,,,,,,,,,"0|z1mmeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 14:44;twalthr;Fixed in master: b666de974b5dbc6e0d6710baececb1d6ede9b769;;;","05/Feb/24 14:35;martijnvisser;Added missing fixVersion number;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Azure Pipelines not running for master and the release branches since 4a852fe,FLINK-34026,13563927,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,mapohl,mapohl,08/Jan/24 16:49,08/Jan/24 17:07,04/Jun/24 20:40,08/Jan/24 16:56,1.17.2,1.18.0,1.19.0,,,,,,,,,Build System / CI,,,,0,,,,,"It appears that [no Azure CI builds|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary] are triggered for {{master}} (and possibly the release branches) since 4a852fee28f2d87529dc05f5ba2e79202a0e00b6.

The [PR CI workflows|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=2] appear to be not affected. I suspect some problem with the repo-sync process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 17:07:39 UTC 2024,,,,,,,,,,"0|z1mmb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 16:52;mapohl;[~martijnvisser] [~lincoln] [~yunta] [~jingge] Someone with access might need to check the Ververica-owned VM (if my suspicion with the repo-sync is correct).;;;","08/Jan/24 16:55;jingge;[~mapohl] Thanks the reporting, checking now;;;","08/Jan/24 16:56;jingge;disk full issue;;;","08/Jan/24 17:07;mapohl;yup, the most-recent commit got picked up again. Thanks for the quick response. (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show data skew score on Flink Dashboard,FLINK-34025,13563910,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,iemre,iemre,iemre,08/Jan/24 14:38,05/Apr/24 14:38,04/Jun/24 20:40,05/Apr/24 14:38,1.20.0,,,,,,,,,,,Runtime / Web Frontend,,,,0,dashboard,pull-request-available,,,"*Problem:* Currently users have to click on every operator and check how much data each subtask is processing to see if there is data skew. This is particularly cumbersome and error-prone for jobs with big job graphs. Data skew is an important metric that should be more visible.

 

*Proposed solution:*
 * Show a data skew score on each operator (see screenshot below). This would be an improvement, but would not be sufficient. As it would still not be easy to see the data skew score for jobs with very large job graphs (it'd require a lot of zooming in/out).
 * Show data skew score for each operator under a new ""Data Skew"" tab next to the Exceptions tab. See screenshot below  !skew_tab.png|width=1226,height=719! .

 

!skew_proposal.png|width=845,height=253!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/24 14:20;iemre;skew_proposal.png;https://issues.apache.org/jira/secure/attachment/13065822/skew_proposal.png","08/Jan/24 14:37;iemre;skew_tab.png;https://issues.apache.org/jira/secure/attachment/13065823/skew_tab.png",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 05 14:38:03 UTC 2024,,,,,,,,,,"0|z1mm7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 16:31;iemre;A potential metric definition for data skew percentage for a given operator _(maxRecordsReceivedBySubtask - minRecordsReceivedBySubtask) / totalRecordsReceivedByAllSubtasks_;;;","09/Jan/24 13:14;wanglijie;Hi [~iemre], I think your proposal needs to be discussed on the dev mailing list via FLIP (as it involves UI and metrics add/change). You can find more detail in [here|https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals];;;","09/Jan/24 13:15;iemre;On it, thanks Lijie.;;;","10/Jan/24 11:02;iemre;Closing, needs a FLIP first. Will re-open if FLIP is accepted: https://cwiki.apache.org/confluence/display/FLINK/FLIP-418%3A+Show+data+skew+score+on+Flink+Dashboard;;;","15/Feb/24 12:18;iemre;reopening based on the votes I've received for [https://cwiki.apache.org/confluence/display/FLINK/FLIP-418%3A+Show+data+skew+score+on+Flink+Dashboard]

 ;;;","05/Apr/24 13:49;iemre;https://github.com/apache/flink/pull/24598;;;","05/Apr/24 14:38;hong; merged commit [{{080efb9}}|https://github.com/apache/flink/commit/080efb9c410102a5d12d31bb2af5a3faa3391736] into   apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update connector release process for Python connectors,FLINK-34024,13563904,13557618,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dannycranmer,dannycranmer,dannycranmer,08/Jan/24 13:55,08/Jan/24 13:58,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,Work out how to release the Python libs for Flink connectors. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-08 13:55:48.0,,,,,,,,,,"0|z1mm60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose Kinesis client retry config in sink,FLINK-34023,13563896,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,atchesonaws,atchesonaws,08/Jan/24 12:43,30/May/24 19:28,04/Jun/24 20:40,,,,,,,,,,,,,Connectors / Kinesis,,,,0,,,,,"The consumer side exposes client retry configuration like [flink.shard.getrecords.maxretries|https://nightlies.apache.org/flink/flink-docs-stable/api/java/org/apache/flink/streaming/connectors/kinesis/config/ConsumerConfigConstants.html#SHARD_GETRECORDS_RETRIES] but the producer side lacks similar config for PutRecords.

The KinesisStreamsSinkWriter constructor calls 
{code}
this.httpClient = AWSGeneralUtil.createAsyncHttpClient(kinesisClientProperties);
this.kinesisClient = buildClient(kinesisClientProperties, this.httpClient);
{code}
But those methods only refer to these values (aside from endpoint/region/creds) in the kinesisClientProperties:
* aws.http-client.max-concurrency
* aws.http-client.read-timeout
* aws.trust.all.certificates
* aws.http.protocol.version

Without control over retry, users can observe exceptions like {code}Request attempt 2 failure: Unable to execute HTTP request: connection timed out after 2000 ms: kinesis.us-west-2.amazonaws.com{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 19:24:57 UTC 2024,,,,,,,,,,"0|z1mm48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 13:05;dannycranmer;Thanks for raising this [~atchesonaws] . Is this something you would like to contribute?;;;","10/Jan/24 20:03;atchesonaws;Yes. I'll look into it, post a proposed solution approach here, and wait for agreement before starting to implement.;;;","30/May/24 19:24;atchesonaws;[Here's|https://github.com/brada/flink-connector-aws/commit/aa4fce309e94f979aa1f3ef48d5d27fa17f55759] a possible solution that adds 3 new config properties:
 - aws.dynamodb.client.retry-policy.num-retries
 - aws.firehose.client.retry-policy.num-retries
 - aws.kinesis.client.retry-policy.num-retries

That seemed like the simplest possible approach. Exposing the complete AWS SDK retry policy would require many more parameters, including [backoff strategy|https://sdk.amazonaws.com/java/api/2.0.0/software/amazon/awssdk/core/retry/backoff/BackoffStrategy.html], base delay, max backoff time etc. Since num-retries is the most important parameter, would it be acceptable to expose just that one for now?

The `mvn clean verify` and `mvm clean package` commands fail on that branch, but they also fail on the origin main branch for what appears to be issues unrelated to my commit:
{noformat}
[ERROR] Failures:
[ERROR]   RowDataToAttributeValueConverterTest.testFloat:208
Expecting map:
  {""key""=AttributeValue(N=1.2345679E17)}
to contain entries:
  [""key""=AttributeValue(N=1.23456791E17)]
but the following map entries had different values:
  [""key""=AttributeValue(N=1.2345679E17) (expected: AttributeValue(N=1.23456791E17))]{noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Duration that cannot be expressed as long in nanoseconds causes an ArithmeticException in TimeUtils.formatWithHighestUnit,FLINK-34022,13563882,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ra9hav,mapohl,mapohl,08/Jan/24 11:06,08/Jan/24 17:07,04/Jun/24 20:40,,1.17.2,1.18.0,1.19.0,,,,,,,,,Runtime / Configuration,,,,0,starter,,,,"The following code fails with an {{ArithmeticException}} because of the actual timeframe not being able to be converted into a long value:
{code:java}
TimeUtils.formatWithHighestUnit(Duration.ofMillis(Long.MAX_VALUE));
{code}

This method is used to pretty-print Duration values and most often used with configuration values. We might want to have a reasonable fallback instead of throwing a {{RuntimeException}} here.

E.g. it can cause the error in the [ClusterConfigHandler|https://github.com/apache/flink/blob/ebdde651edae8db6b2ac740f07d97124dc01fea4/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/cluster/ClusterConfigHandler.java#L56] if a Duration-based configuration parameter is set too high (e.g. [JobManagerOptions.RESOURCE_WAIT_TIMEOUT|https://github.com/apache/flink/blob/3ff225c5f993282d6dfc7726fc08cc00058d9a7f/flink-core/src/main/java/org/apache/flink/configuration/JobManagerOptions.java#L540]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 17:06:53 UTC 2024,,,,,,,,,,"0|z1mm14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 17:05;ra9hav;[~mapohl] can you please assign this issue to me ? I would like to work on this as my first contribution.;;;","08/Jan/24 17:06;mapohl;sure, thanks for volunteering.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print jobKey in the Autoscaler standalone log,FLINK-34021,13563880,13556703,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,08/Jan/24 10:57,11/Jan/24 04:02,04/Jun/24 20:40,11/Jan/24 04:02,kubernetes-operator-1.8.0,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,,"FLINK-33814 has supported the multiple threads scaling for autoscaler standalone. When a lot of jobs are scaling, autoscaler standalone will print too many logs. Currently, each log doesn't have the job key, it's hard to maintain.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 04:02:08 UTC 2024,,,,,,,,,,"0|z1mm0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 04:02;fanrui;Merged to main(1.8.0) via : f67cba9c8269ddd4e27fcdc40cfd8a293ae665de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump CI flink version on flink-connector-rabbitmq,FLINK-34020,13563873,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,08/Jan/24 10:09,12/Jan/24 09:13,04/Jun/24 20:40,12/Jan/24 09:13,,,,,,,,elasticsearch-3.0.2,,,,Connectors/ RabbitMQ,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 09:13:57 UTC 2024,,,,,,,,,,"0|z1mlz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 09:13;martijnvisser;Fixed in apache/flink-connector-rabbitmq

main: 33d125f591cc5f99e12280fbe2725bd374f7738b
v3.0: 3d717b21db41dba2561f11fcb93423ad0aabccd4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump com.rabbitmq:amqp-client from 5.13.1 to 5.20.0,FLINK-34019,13563871,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,08/Jan/24 10:04,08/Jan/24 10:06,04/Jun/24 20:40,08/Jan/24 10:06,,,,,,,,rabbitmq-3.1.0,,,,Connectors/ RabbitMQ,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 10:06:37 UTC 2024,,,,,,,,,,"0|z1mlyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 10:06;martijnvisser;Fixed in apache/flink-connector-rabbitqmq

main: 427bf4cd9fbf27d338a33250257489491952aeb8
v3.0: 881c83b5daf3a4e48a8027f85dff1f1f99dc6f81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add flink 1.19 verison in ci for flink-connector-aws,FLINK-34018,13563869,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,gongzhongqiang,gongzhongqiang,08/Jan/24 09:48,08/Jan/24 15:00,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,Add flink 1.19 verison in ci for flink-connector-aws,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 15:00:59 UTC 2024,,,,,,,,,,"0|z1mly8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 10:48;dannycranmer;What is the ask here? We already have 1.19 in the nightly build [https://github.com/apache/flink-connector-aws/blob/main/.github/workflows/nightly.yml#L28]

The pull request workflows are reserved for supported versions. Since 1.19 is not yet supported and hence, not built as part of the push/PR.;;;","08/Jan/24 14:35;gongzhongqiang;Hi [~dannycranmer] , Thanks for your kind explain. I think push PR should add *1.19-SNAPSHOT* and modify exist flink with specific version to *SNAPSHOT.* Because pr for uncoming version should be checked when pr was opened. Also we can add flink with specific version to nightly.yml to check full version of flink.;;;","08/Jan/24 15:00;gongzhongqiang;[https://github.com/apache/flink-connector-aws/blob/main/.github/workflows/common.yml] can only test jdk version with *8, 11 .* Should add more jdk version check for ci.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connectors docs with 404 link in dynamodb.md,FLINK-34017,13563865,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gongzhongqiang,gongzhongqiang,gongzhongqiang,08/Jan/24 09:21,08/Jan/24 13:52,04/Jun/24 20:40,08/Jan/24 13:52,,,,,,,,aws-connector-4.3.0,,,,Documentation,,,,0,pull-request-available,,,,"docs Link: [https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/dynamodb/|http://example.com/]

screenshort:

!image-2024-01-08-17-21-16-434.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/24 09:21;gongzhongqiang;image-2024-01-08-17-21-16-434.png;https://issues.apache.org/jira/secure/attachment/13065795/image-2024-01-08-17-21-16-434.png",,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 13:52:15 UTC 2024,,,,,,,,,,"0|z1mlxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 09:23;gongzhongqiang;Hi [~martijnvisser] [~leonard] ,  Please assign to me, I am willing to fix this. And I think add dead link check is very useful to us.;;;","08/Jan/24 13:52;dannycranmer;Merged commit [{{38aafb4}}|https://github.com/apache/flink-connector-aws/commit/38aafb44d3a8200e4ff41d87e0780338f40da258] into apache:main

Merged commit [{{7a7a6a0}}|https://github.com/apache/flink-connector-aws/commit/7a7a6a0f0b8274b3196a82cfb2cd3ef0776d967f] into apache:v4.2

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Janino compile failed when watermark with column by udf,FLINK-34016,13563854,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wczhu,wczhu,08/Jan/24 07:45,06/Mar/24 07:47,04/Jun/24 20:40,,1.15.0,1.18.0,,,,,,,,,,Table SQL / Runtime,,,,1,pull-request-available,,,,"After submit the following flink sql by sql-client.sh will throw an exception:
{code:java}
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'WatermarkGenerator$0'
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
    at org.apache.flink.table.runtime.generated.GeneratedWatermarkGeneratorSupplier.createWatermarkGenerator(GeneratedWatermarkGeneratorSupplier.java:69)
    at org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.createMainOutput(ProgressiveTimestampsAndWatermarks.java:109)
    at org.apache.flink.streaming.api.operators.SourceOperator.initializeMainOutput(SourceOperator.java:462)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNextNotReading(SourceOperator.java:438)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:414)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
    ... 16 more
Caused by: org.apache.flink.shaded.guava31.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2055)
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache.get(LocalCache.java:3966)
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4863)
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
    ... 18 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4868)
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3533)
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2282)
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2159)
    at org.apache.flink.shaded.guava31.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2049)
    ... 21 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 29, Column 123: Line 29, Column 123: Cannot determine simple type name ""org""
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:7007)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6886)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6899)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6899)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6899)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6899)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6899)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6899)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6857)
    at org.codehaus.janino.UnitCompiler.access$14800(UnitCompiler.java:237)
    at org.codehaus.janino.UnitCompiler$24.visitReferenceType(UnitCompiler.java:6755)
    at org.codehaus.janino.UnitCompiler$24.visitReferenceType(UnitCompiler.java:6752)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:4289)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6752)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7532)
    at org.codehaus.janino.UnitCompiler.access$17300(UnitCompiler.java:237)
    at org.codehaus.janino.UnitCompiler$25.visitNewClassInstance(UnitCompiler.java:6799)
    at org.codehaus.janino.UnitCompiler$25.visitNewClassInstance(UnitCompiler.java:6773)
    at org.codehaus.janino.Java$NewClassInstance.accept(Java.java:5587)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6773)
    at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9621)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9506)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9422)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5263)
    at org.codehaus.janino.UnitCompiler.access$9300(UnitCompiler.java:237)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4766)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4742)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4742)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5885)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:4121)
    at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:237)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:4096)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:4071)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:4071)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2524)
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:237)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1581)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1576)
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:3209)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1576)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1662)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3665)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3330)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1448)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1421)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:830)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:443)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:237)
    at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:423)
    at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:419)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1688)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:419)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
    at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:237)
    at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:364)
    at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:362)
    at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:371)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:362)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:273)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:526)
    at org.codehaus.janino.SimpleCompiler.cook2(SimpleCompiler.java:250)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:229)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:219)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:82)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
    ... 27 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 29, Column 123: Cannot determine simple type name ""org""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:13080)
    at org.codehaus.janino.UnitCompiler.getRawReferenceType(UnitCompiler.java:7175)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:7005)
    ... 94 more
 {code}
 

flink sql： 
{code:java}
CREATE TABLE default_catalog.default_database.KafkaTable (
  `block` STRING,
  `de` STRING,
  `http_path` STRING,
  `logType` STRING,
  `mod` STRING,
  `pb_city_id` STRING,
  `pb_nation_id` STRING,
  `timestamp` BIGINT,
  `v_lineNum` STRING,
  `v_timestamp` BIGINT,
   ts AS COALESCE(TO_TIMESTAMP(
        FROM_UNIXTIME(`timestamp` / 1000, 'yyyy-MM-dd HH:mm:ss')
    ),CURRENT_TIMESTAMP),
    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'kafka_topic_01',
  'properties.bootstrap.servers' = '*****',
  'properties.group.id' = 'testGroup',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'json'
);

CREATE TABLE default_catalog.default_database.table_print(
  `block` STRING,
  `de` STRING,
  `http_path` STRING,
  `logType` STRING,
  `mod` STRING,
  `pb_city_id` STRING,
  `pb_nation_id` STRING,
  `timestamp` BIGINT,
  `v_lineNum` STRING,
  `v_timestamp` BIGINT,
   ts TIMESTAMP
) WITH(
    'connector' = 'print',
    'print-identifier' = '===== PrintResult: ====='
);

insert into default_catalog.default_database.table_print select * from default_catalog.default_database.KafkaTable; {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28693,,,,,,"25/Jan/24 03:53;wczhu;image-2024-01-25-11-53-06-158.png;https://issues.apache.org/jira/secure/attachment/13066238/image-2024-01-25-11-53-06-158.png","25/Jan/24 03:54;wczhu;image-2024-01-25-11-54-54-381.png;https://issues.apache.org/jira/secure/attachment/13066239/image-2024-01-25-11-54-54-381.png","25/Jan/24 04:57;wczhu;image-2024-01-25-12-57-21-318.png;https://issues.apache.org/jira/secure/attachment/13066241/image-2024-01-25-12-57-21-318.png","25/Jan/24 04:57;wczhu;image-2024-01-25-12-57-34-632.png;https://issues.apache.org/jira/secure/attachment/13066242/image-2024-01-25-12-57-34-632.png",,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,28693,https://issues.apache.org/jira/browse/FLINK-28693,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 07:47:36 UTC 2024,,,,,,,,,,"0|z1mluw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 11:45;337361684@qq.com;Hi, [~wczhu] , I cann't reproduce this error in my local env with master branch, can you try to reproduce it using master branch? ;;;","25/Jan/24 04:58;wczhu;[~337361684@qq.com] Yes, I can recover stably. I am use official *flink-1.18.0-bin-scala_2.12.tgz* and *flink-sql-connector-kafka-3.0.2-1.18.jar* to reproduce this error.
!image-2024-01-25-11-54-54-381.png|width=664,height=184!!image-2024-01-25-11-53-06-158.png|width=663,height=597!

!image-2024-01-25-12-57-21-318.png|width=865,height=307!!image-2024-01-25-12-57-34-632.png|width=862,height=349!;;;","30/Jan/24 03:37;xuyangzhong;Hi, [~wczhu] . +1 Could not reproduce it on my local env.

Can you try this following steps?

Add the following options when you start the session:

 
{code:java}
env.java.opts.taskmanager: ""-Dorg.codehaus.janino.source_debugging.enable=true -Dorg.codehaus.janino.source_debugging.dir=/flink/log/""{code}
Then you could see the generated files by codegen in the file path, and then attach the java file about WatermarkGenerator to this jira.

You can also use `explain plan_advice insert xxx` to get the plan and paste it here.;;;","07/Feb/24 07:23;xuyangzhong;Hi, [~wczhu] I have found the root bug and will try to fix it. You can temporarily replace flink-table-planner-loader with flink-table-planer in the opt/ folder just like https://issues.apache.org/jira/browse/FLINK-28693 said to work around this bug.;;;","18/Feb/24 07:26;wczhu;OK [~xuyangzhong] , look forward to your PR！;;;","28/Feb/24 16:28;seb-pereira;Hi [~xuyangzhong] I have tested changes from your [PR|https://github.com/apache/flink/pull/24280] with a patch on 1.18.0 and it fixes the issue (I previously reported the issue on 1.18.0 in  [FLINK-28693|https://issues.apache.org/jira/browse/FLINK-28693?focusedCommentId=17815957&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17815957])







 

 ;;;","29/Feb/24 11:08;xuyangzhong;Thanks for your test! [~seb-pereira] (y);;;","06/Mar/24 07:47;seb-pereira;Hi [~xuyangzhong] any chance for the fix to land in [release 1.19|https://cwiki.apache.org/confluence/display/FLINK/1.19+Release]? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Setting `execution.savepoint.ignore-unclaimed-state` does not take effect when passing this parameter by dynamic properties,FLINK-34015,13563844,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,zhourenxiang,zhourenxiang,zhourenxiang,08/Jan/24 06:30,20/Feb/24 15:32,04/Jun/24 20:40,,1.17.0,,,,,,,,,,,Runtime / State Backends,,,,0,ignore-unclaimed-state-invalid,pull-request-available,,,"We set `execution.savepoint.ignore-unclaimed-state` to true and use -D option to submit the job, but unfortunately we found the value is still false in jobmanager log.

Pic 1: we  set `execution.savepoint.ignore-unclaimed-state` to true in submiting job.
!image-2024-01-08-14-22-09-758.png|width=1012,height=222!

Pic 2: The value is still false in jmlog.

!image-2024-01-08-14-24-30-665.png|width=651,height=51!

 

Besides, the parameter `execution.savepoint-restore-mode` has the same problem since when we pass it by -D option.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/24 06:22;zhourenxiang;image-2024-01-08-14-22-09-758.png;https://issues.apache.org/jira/secure/attachment/13065785/image-2024-01-08-14-22-09-758.png","08/Jan/24 06:24;zhourenxiang;image-2024-01-08-14-24-30-665.png;https://issues.apache.org/jira/secure/attachment/13065784/image-2024-01-08-14-24-30-665.png",,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Fri Jan 19 07:06:06 UTC 2024,,,,,,,,,,"0|z1mlso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 06:57;JunRuiLi;If the jobmanager log reflects your actual input and it shows a space after {{{}-D{}}}, then the usage is incorrect. The correct format should be:

 
{code:java}
-Dexecution.savepoint.ignore-unclaimed-state=true{code}
 ;;;","08/Jan/24 07:23;zhourenxiang;[~JunRuiLi] Many thanks for your reply, I have also tested this command without the space, and it still does not take effect. After reading the code, the reason I think is that the setting has been replaced by mistake in client side since we do not use `-s` option to start the job from a checkpoint/savepoint but use -Dexecution.savepoint.path.;;;","10/Jan/24 06:45;zhourenxiang;[~masteryhx] Hello, hangxiang, please have a look at this issue if you have time, many thanks~

And I have implemented a fix for this little bug.
https://github.com/apache/flink/pull/24058;;;","10/Jan/24 06:54;Zhanghao Chen;Hi [~Zakelly], I'm reaching out to you as you've mentioned about refactoring CLI in Flink 2.0. This is another example of confusing behvior in the current CLI design: the short command and -D dynamic properties may interact with each other in a confusing way.;;;","10/Jan/24 14:23;masteryhx;[~zhourenxiang] Thanks for reporting this.

IIUC, it's because SavepointRestoreSettings has been overrided by the none value passed by CLI, right ?;;;","11/Jan/24 02:35;zhourenxiang;[~masteryhx] Many thanks for your reply~
Yes, if we use the dynamic parameters to recover from a savepoint instead of using '-s' option, the CLI will generate the SavepointRestoreSettings.none() and set it to configuration.;;;","11/Jan/24 03:26;masteryhx;[~zhourenxiang] Fine, it's vaild.

I have assigned it to you and commented in your pr, please take a look and go ahead.;;;","11/Jan/24 05:47;zhourenxiang;[~masteryhx] Many thanks to you, I will check it~(y);;;","19/Jan/24 07:06;zakelly;{quote}Hi [~Zakelly], I'm reaching out to you as you've mentioned about refactoring CLI in Flink 2.0. This is another example of confusing behvior in the current CLI design: the short command and -D dynamic properties may interact with each other in a confusing way.
{quote}
[~Zhanghao Chen] Thanks for the reminder! Got it :D;;;",,,,,,,,,,,,,,,,,,,,,,,,
Jdbc connector can avoid send empty insert to database when there's no buffer data,FLINK-34014,13563835,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeyhunkarimov,luoyuxia,luoyuxia,08/Jan/24 03:42,26/Mar/24 14:11,04/Jun/24 20:40,26/Mar/24 14:11,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,0,pull-request-available,,,,"In jdbc connector, we will have a background thread to flush buffered data to database, but when no data is in buffer, we can avoid the flush to database.

we can avoid it in method JdbcOutputFormat#attemptFlush or in
JdbcBatchStatementExecutor like TableBufferedStatementExecutor  which can aovid calling  {{statementExecutor.executeBatch()}} when buffer is empty",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 26 14:10:58 UTC 2024,,,,,,,,,,"0|z1mlqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/24 14:10;Sergey Nuyanzin;Merged as [ffd2bdf887d997a6080f3a3bdba31a92b969e1e0|https://github.com/apache/flink-connector-jdbc/commit/ffd2bdf887d997a6080f3a3bdba31a92b969e1e0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
