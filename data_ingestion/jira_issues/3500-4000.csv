Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Outward issue link (Dependency),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Inward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Moves DefaultLeaderElectionService.startLeaderElectionBackend() into HAServices,FLINK-32013,13535168,13432464,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,05/May/23 13:01,05/May/23 14:52,04/Jun/24 20:41,05/May/23 14:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,{{DefaultLeaderElectionServivce#startLeaderElectionBackend()}} was introduced in FLINK-31733. This enables us to move the ownership of the driver lifecycle from the {{LeaderContender}} into the {{HighAvailabilityServices}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 05 14:48:10 UTC 2023,,,,,,,,,,"0|z1hpy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 14:48;mapohl;Looks like FLINK-31773 and FLINK-32013 cannot be separated due to some tests that utilize {{HighAvailabilityServices}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Operator failed to rollback due to missing HA metadata,FLINK-32012,13535161,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,nfraison.datadog,nfraison.datadog,05/May/23 12:07,27/Jun/23 15:39,04/Jun/24 20:41,27/Jun/23 15:39,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"The operator has well detected that the job was failing and initiate the rollback but this rollback has failed due to `Rollback is not possible due to missing HA metadata`

We are relying on saevpoint upgrade mode and zookeeper HA.

The operator is performing a set of action to also delete this HA data in savepoint upgrade mode:
 * [flink-kubernetes-operator/AbstractFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L346] : Suspend job with savepoint and deleteClusterDeployment

 * [flink-kubernetes-operator/StandaloneFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java#L158] : Remove JM + TM deployment and delete HA data

 * [flink-kubernetes-operator/AbstractFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L1008] : Wait cluster shutdown and delete zookeeper HA data

 * [flink-kubernetes-operator/FlinkUtils.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java#L155] : Remove all child znode

Then when running rollback the operator is looking for HA data even if we rely on sevepoint upgrade mode:
 * [flink-kubernetes-operator/AbstractFlinkResourceReconciler.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L164] Perform reconcile of rollback if it should rollback

 * [flink-kubernetes-operator/AbstractFlinkResourceReconciler.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L387] Rollback failed as HA data is not available

 * [flink-kubernetes-operator/FlinkUtils.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java#L220] Check if some child znodes are available

For both step the pattern looks to be the same for kubernetes HA so it doesn't looks to be linked to a bug with zookeeper.

 

From https://issues.apache.org/jira/browse/FLINK-30305 it looks to be expected that the HA data has been deleted (as it is also performed by flink when relying on savepoint upgrade mode).

Still the use case seems to differ from https://issues.apache.org/jira/browse/FLINK-30305 as the operator is aware of the failure and treat a specific rollback event.

So I'm wondering why we enforce such a check when performing rollback if we rely on savepoint upgrade mode. Would it be fine to not rely on the HA data and rollback from the last savepoint (the one we used in the deployment step)?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 27 15:39:11 UTC 2023,,,,,,,,,,"0|z1hpwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 12:11;nfraison.datadog;[~gyfora] do you think that we can implement this specific case for rollback or do you foresee some potential issues with this?;;;","06/May/23 07:54;gyfora;Hey!
This is an unfortunate limitation of the rollback mechanism currently that it strictly relies on the HA metadata at the time of the rollback. This works in many cases if the upgraded cluster at least starts up (in that case the HA metadata is created with the initial restore savepoint path).

We need to overhaul the rollback mechansim based on how the upgrade works now where we already fixed these issues.;;;","16/May/23 14:55;nfraison.datadog;I'd like to have your feeling on the first approach I'd like to do to merge upgrade/rollback.

 

Currently to do a rollback we
 * First check if spec change
 * If not check if we should rollback
 * If yes initiate rollback where we set status to ROLLING_BACK and wait for next reconcile loop
 * Next reconcile loop will do same checks
 * As it should rollback and state already up to date it will rollback relying on last stable spec but doesn't update the spec

The idea would be to really rollback the spec spec to rely for both mechanism on reconcileSpecChange:
 * Still keep the check if spec change
 * Then check if should rollback
 * In the initiate rollback set status to ROLLING_BACK and restore last stable spec and wait for next reconcile loop
 * Next reconcile loop will check if spec change
 * It will be the case so will run reconcileSpecChange but with status ROLLING_BACK so could then applied specificities of rollback

Would also simplify the operation that need to reapply the working spec like [resubmitJob|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java#L309]

Still I'm wondering if we will then be really able to simplify things as from your [comment|https://github.com/apache/flink-kubernetes-operator/pull/590#discussion_r1189847280] the rollback doesn't seem to be aligned to the upgrade?

For ex for savepoint upgrade mode:
 * upgrade
 ** delete ha metadata
 ** restore from savepoint
 * rollback
 ** keep ha metadata if exist
 ** restore from it if exist
 ** restore from savepoint if not exist and JM pod never started;;;","16/May/23 15:14;gyfora;I agree [~nfraison.datadog] it should be possible to unify upgrades and rollbacks. After checking if we should rollback we could temporarily reset the spec to the latest stable and execute the upgrade as if this was the spec sent by the user, also we should modify the upgradeMode to last-state. 

I think the upgrade logic already covers the necessary HA meta checks so this should be good in theory :) ;;;","23/May/23 13:35;nfraison.datadog;[~gyfora], I started an implementation which do not modify the upgradeMode to last-state as the [restoreJob|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java#L143%5D] for LAST_STATE upgrade mode will enforce the requirement for HA metadata which is not what we want when relying on SAVEPOINT.

Also when restoring last stable spec there is a case where the UpgradeMode is set to STATELESS in this spec even if the chosen mode is SAVEPOINT ([updateStatusBeforeFirstDeployment|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L193])

In order to avoid restoring this bad state leading to rollback not taking in account the savepoint I enforce the upgrade mode of the restored spec to be the one currently set on the job.

But I'm wondering why we have decided to not persist the really used upgrade mode in the last stable spec for first deployment?

Here is the diff of my [WIP|https://github.com/apache/flink-kubernetes-operator/compare/main...ashangit:flink-kubernetes-operator:nfraison/FLINK-32012?expand=1] if approach is not clear (not to review...);;;","23/May/23 14:02;gyfora;[~nfraison.datadog] , I don't think it's possible to use ""SAVEPOINT"" upgrade mode when rolling back. That would imply that we can take a savepoint which means that the job would be running.

Also regarding your comment about what to set in the status for upgradeMode, we have to set what we actually ended up using, there is some logic that relies on this. The first time a job is started from an empty state we record stateless, if you started with initialSavepointPath then it would be savepoint. All these are key pieces of logic that allows us to upgrade safely without accidentally losing state.;;;","23/May/23 15:02;nfraison.datadog;But the whole point of this request was the fact that when JobManager failed to start the HA metadata was not available during RollBack while the savepoint taken for the upgrade is available.

So relying on SAVEPOINT for rollback will ensure that Flink is aware of the availability of a savepoint.

From my understanding of [tryRestoreExecutionGraphFromSavepoint|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultExecutionGraphFactory.java#L198]  Flink will rely on the saveoint if no HA metadata exist otherwise it will load the checkpoint

 

Forgot to mention it but indeed when calling [cancelJob|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L353%5D] it fallback to LAST_STATE in order to ensure no savepoint is created and HA metadata is not deleted

 ;;;","23/May/23 15:06;gyfora;Yes you are right. But this is already handled in the getAvailableUpgradeModes logic during a normal upgrade. If your job failed to start after a savepoint upgrade you can send in a new spec and it will be upgraded using the previous savepoint.;;;","23/May/23 15:26;gyfora;I think you are on the right track. But we should ensure that in the lastReconciledSpec we record the upgradeMode that was used during the last deployment / rollback otherwise subsequent upgrades can be problematic.

If we used a savepoint we should record savepoint if HA metadata was used it should be last-state.;;;","27/Jun/23 15:39;gyfora;merged to main d346ca9c437d20042ed8f4a1954f0f0ed438b3ae;;;",,,,,,,,,,,,,,,,,
flink1.15.2 loaded all the data in the table in mysql5.7,FLINK-32011,13535146,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,xueyongyang,xueyongyang,05/May/23 10:40,06/May/23 05:37,04/Jun/24 20:41,06/May/23 04:51,1.15.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,,,,,,"CREATE TABLE `T_P_FILTER_MERCHANT_DAY_RES2` (
  `MERCHANT_NO` varchar(200) NOT NULL,
  `ACT_ID` varchar(50) NOT NULL,
  `RULE_ID` varchar(50) NOT NULL,
  `SUM_MONEY` decimal(25,5) DEFAULT NULL,
  `SUM_NUM` decimal(25,5) DEFAULT NULL,
  `DATE_DT` int NOT NULL,
  `DATE_TYPE` varchar(50) DEFAULT NULL,
  `BEGIN_DATE` int DEFAULT NULL,
  `END_DATE` int DEFAULT NULL,
  `ID` bigint NOT NULL AUTO_INCREMENT,
  PRIMARY KEY (`ID`),
  UNIQUE KEY `T_P_FILTER_MERCHANT_DAY_RES2_UN` (`MERCHANT_NO`,`ACT_ID`,`RULE_ID`,`DATE_DT`)
) ENGINE=InnoDB AUTO_INCREMENT=88 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

 

CREATE TABLE `T_P_RED_STAND_RESULT` (
  `ACTIVITY_NO` varchar(50)  ,
  `RULE_ID` varchar(50)    ,
  `WEIGHT` int NOT NULL ,
  `DEAL_TYPE` varchar(50)  ,
  `DATA_TYPE` varchar(50)  ,
  `DATA_NO` varchar(50) ,
  `USER_TYPE` varchar(50)  ,
  `USER_NO` varchar(50)  ,
  `SUM_MONEY` decimal(25,5) ,
  `SUM_NUM` decimal(25,5) ,
  `DATE_TYPE` varchar(50)  ,
  `BEGIN_TIME` int NOT NULL ,
  `END_TIME` int NOT NULL ,
  `TRADE_MONEY` decimal(25,5) DEFAULT NULL,
  `EXTEND_DATA_MEAN` varchar(50)  ,
  `DATA1` varchar(50)  ,
  `DATA2` varchar(50)  ,
  `DATA3` varchar(50)  ,
  `DATA4` varchar(50)  ,
  `DATA5` varchar(50)  ,
  `CK_REACH` varchar(10)  ,
  `READ_STATUS` varchar(10)  ,
  `PROCESS_STATUS` varchar(50)  ,
  `PROCESS_STATUS_DESC` varchar(200)  ,
  `RESULT_TYPE` varchar(50) ,
  `MANAGER_NO` varchar(50) ,
  `MANAGER_ORG_NO` varchar(50)  ,
  `CALCULATION_DATE` int NOT NULL ,
  `CALCULATION_TIME` datetime DEFAULT NULL ,
  `CREATE_TIME` datetime DEFAULT NULL ,
  `CREATE_USER_NO` varchar(50)  ,
  `MAINTENANCE_TIME` varchar(50)  ,
  `MAINTENANCE_USER_NO` varchar(50)  ,
  `ID` bigint NOT NULL AUTO_INCREMENT,
  PRIMARY KEY (`ID`),
  UNIQUE KEY `T_P_RED_STAND_RESULT_UN` (`ACTIVITY_NO`,`USER_NO`,`BEGIN_TIME`,`END_TIME`)
) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

 

INSERT INTO
    T_P_RED_STAND_RESULT ( ACTIVITY_NO,
    RULE_ID,
    WEIGHT,
    DEAL_TYPE,
    DATA_TYPE,
    DATA_NO,
    USER_TYPE,
    USER_NO,
    SUM_MONEY,
    SUM_NUM,
    DATE_TYPE,
    BEGIN_TIME,
    END_TIME,
    TRADE_MONEY,
    EXTEND_DATA_MEAN,
    DATA1,
    DATA2,
    DATA3,
    DATA4,
    DATA5,
    CK_REACH,
    READ_STATUS,
    PROCESS_STATUS,
    PROCESS_STATUS_DESC,
    RESULT_TYPE,
    MANAGER_NO,
    MANAGER_ORG_NO,
    CALCULATION_DATE,
    CALCULATION_TIME,
    CREATE_TIME,
    CREATE_USER_NO,
    MAINTENANCE_TIME,
    MAINTENANCE_USER_NO)  
select
    'abc' as ACTIVITY_NO,
    'def' as RULE_ID,
    1 as WEIGHT,
    'red' as DEAL_TYPE,
    'gear' as DATA_TYPE,
    '001010102' as DATA_NO,
    'merchantRed' as USER_TYPE,
    r.MERCHANT_NO as MERCHANT_NO,
    r.SUM_MONEY as SUM_MONEY,
    r.SUM_NUM as SUM_NUM ,
    r.DATE_TYPE as DATE_TYPE ,
    r.BEGIN_DATE as BEGIN_DATE,
    r.END_DATE as END_DATE,
    0 as TRADE_MONEY,
    'other' as EXTEND_DATA_MEAN,
    if(0.1 * r.SUM_MONEY >= 10 and 0.1 * r.SUM_MONEY <= 100, 0.1 * r.SUM_MONEY, if(0.1 * r.SUM_MONEY<10, 10, if(0.1 * r.SUM_MONEY>100, 100, 0.1 * r.SUM_MONEY))) as DATA1,
    '' as DATA2,
    '' as DATA3,
    '' as DATA4,
    '' as DATA5,
    '1' as CK_REACH,
    '0' as READ_STATUS,
    '' as PROCESS_STATUS,
    '' as PROCESS_STATUS_DESC,
    'flink-batch' as RESULT_TYPE,
    '' as MANAGER_NO,
    '' as MANAGER_ORG_NO,
    r.DATE_DT as CALCULATION_DATE,
    LOCALTIMESTAMP as CALCULATION_TIME,
    LOCALTIMESTAMP as CREATE_TIME,
    'system' as CREATE_USER_NO,
    '' as MAINTENANCE_TIME,
    '' as MAINTENANCE_USER_NO
from
    T_P_FILTER_MERCHANT_OTHER_DATE_RES2 r
where
    1 = 1
    and r.ACT_ID = 'abc'
    and r.RULE_ID = 'def'
    and r.DATE_DT = 20221028
    and r.MERCHANT_NO not in ( select u.USER_NO from T_P_RED_STAND_RESULT u where u.ACTIVITY_NO = 'abc' and u.CALCULATION_DATE = 20221028 and u.WEIGHT>=1)
    and r.MERCHANT_NO not in ( select u.USER_NO from T_P_RED_STAND_RESULT u where u.ACTIVITY_NO = 'abc' and u.READ_STATUS = '1')

 

We found that when executing the above flink sql, we feel that flink has loaded all the data in T_P_FILTER_MERCHANT_DAY_RES2 into the memory, and then converted the where statement into a java filter condition, and fetched the filtered data in the memory. We have this judgment for two reasons
1. The error message of flink is oom, memory overflow
2. The feedback from the dba said that we have done a full table query of the T_P_FILTER_MERCHANT_DAY_RES2 table

 

 ","flink1.15.2

mysql5.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Sat May 06 05:37:11 UTC 2023,,,,,,,,,,"0|z1hptc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 10:59;libenchao;JDBC filter pushdown was introduced via FLINK-16024, and it's released in jdbc-3.1 with flink 1.16/1.17 versions. Unfortunately flink 1.15 does not support this.;;;","05/May/23 11:36;xueyongyang;[~libenchao] Is the flink1.5 you mentioned flink1.15?;;;","05/May/23 12:02;libenchao;[~xueyongyang] Yes, sorry for the typo. I changed it.;;;","06/May/23 00:14;xueyongyang;[~libenchao] Ok, thank you, but I still have a question, I just started with flink, I saw that the flink-connector-jdbc referenced in flink1.15.2 is also 1.15.2, so I want to know what is jdbc-3.1?;;;","06/May/23 04:51;libenchao;JDBC connector was split to separate repo and releasing since flink 1.16, you can take a look at the 1.17 doc here : https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/jdbc/

Besides, for asking questions you'd better to go to the dev mailing list, I'm closing this issue now.;;;","06/May/23 05:37;xueyongyang;_Alright, thanks a lot_;;;",,,,,,,,,,,,,,,,,,,,,
KubernetesLeaderRetrievalDriver always waits for lease update to resolve leadership,FLINK-32010,13535133,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,05/May/23 09:50,06/May/23 06:20,04/Jun/24 20:41,06/May/23 06:20,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Deployment / Kubernetes,Runtime / Coordination,,,,,0,pull-request-available,,,,,"The k8s-based leader retrieval is based on ConfigMap watching. The config map lifecycle (from the consumer point of view) is handled as a series of events with the following types:
 * ADDED -> the first time the consumer has seen the CM
 * UPDATED -> any further changes to the CM
 * DELETED -> ... you get the idea

The implementation assumes that ElectionDriver (the one that creates the CM) and ElectionRetriver are started simultaneously and therefore ignore the ADDED events because the CM is always created as empty and is updated with the leadership information later on.

This assumption is incorrect in the following cases (I might be missing some, but that's not important, the goal is to illustrate the problem):
 * TM joining the cluster later when the leaders are established to discover RM / JM
 * RM tries to discover JM when 
MultipleComponentLeaderElectionDriver is used

This, for example, leads to higher job submission latencies that could be unnecessarily held back for up to the lease retry period [1].

[1] Configured by _high-availability.kubernetes.leader-election.retry-period_",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22054,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 05 19:03:13 UTC 2023,,,,,,,,,,"0|z1hpqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 19:03;dmvk;master: 026d7ccfe1d6f4cfa26c9038dd05403c889d2e0d

release-1.16: 6aa84630a39fbe33209487fbeac412dc98439b46

release-1.17: 2799038b964e88129545bf4e6a5128c03e3d2f2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout,FLINK-32009,13535132,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,baibaiwuchang,baibaiwuchang,05/May/23 09:49,09/May/23 07:20,04/Jun/24 20:41,,1.14.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / YARN,,,,,,0,,,,,,"Flink task lock,but yarn resource is full.
{code:java}
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
at  org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86) ~[flink-dist_2.11-1.14.3.jar:1.14.3]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_191]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_191]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455) ~[flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455) ~[flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213) ~[flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_a8b5fc62-4780-45aa-879e-076def164c9f.jar:1.14.3]
at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_191]
at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_191]
at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_191]
at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) [?:1.8.0_191]
Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 300000 ms
... 29 more{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/23 09:49;baibaiwuchang;jobmanager.log;https://issues.apache.org/jira/secure/attachment/13057852/jobmanager.log","09/May/23 05:52;baibaiwuchang;taskmanager.log;https://issues.apache.org/jira/secure/attachment/13057918/taskmanager.log",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 09 07:20:50 UTC 2023,,,,,,,,,,"0|z1hpq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/23 04:12;huwh;[~baibaiwuchang] This log only shows that Job not allocate the required slot in time. 

 

I saw some ""java.lang.OutOfMemoryError: Java heap space"" in jobmanager.log, you need to increase the memory size of jobmanager.

 ;;;","08/May/23 08:53;baibaiwuchang;[~huwh] 

""java.lang.OutOfMemoryError: Java heap space""  caused the task to restart. 

But task restart fail and always report  above errors. The task cannot be restored to the running state.;;;","08/May/23 09:09;wanglijie;Hi [~baibaiwuchang]

Can you check the TM logs and confirm whether the TM started successfully? I found the following log in JM log, which indicates the TM was not successfully registered on the ResourceMamanger.
{code:java}
2023-04-30 12:10:50,643 WARN  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Worker container_1676606213821_28648_01_000020 did not register in 300000 ms, will stop it and request a new one if needed.
{code}
It would be better if you can post the log of {{container_1676606213821_28648_01_000020}};;;","09/May/23 05:53;baibaiwuchang;Hi [~wanglijie]

This is taskmanager log.   [^taskmanager.log];;;","09/May/23 06:37;huwh;[~baibaiwuchang] 

The ""OOM"" Exception was thrown by Zookeeper thread. And then task manager could not retrieve the job manager leader address from zookeeper so that it could not register to job manager in time.

I think this problem is caused by OOM exception, you can increate the memory of JobManager. If this issue occurs again, you need to check which component is occupying memory.

 

And, I would check the Zookeeper HA related code to see if this is the cause, if so, I think we should let JM to exit early;;;","09/May/23 07:20;baibaiwuchang;[~huwh] 

Zookeeper Cluster often trigger ""OOM"".  Now, tasks move to new zk cluster.

I also think task can not lock in any case.;;;",,,,,,,,,,,,,,,,,,,,,
Protobuf format cannot work with FileSystem Connector,FLINK-32008,13535096,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,xuannan,xuannan,05/May/23 06:22,17/Jun/23 14:55,04/Jun/24 20:41,17/Jun/23 14:55,1.17.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,,,"The protobuf format throws exception when working with Map data type. I uploaded a example project to reproduce the problem.

 
{code:java}
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:261)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:131)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:417)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.io.IOException: Failed to deserialize PB object.
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:75)
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:42)
    at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.readRecord(DeserializationSchemaAdapter.java:197)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.nextRecord(DeserializationSchemaAdapter.java:210)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$Reader.readBatch(DeserializationSchemaAdapter.java:124)
    at org.apache.flink.connector.file.src.util.RecordMapperWrapperRecordIterator$1.readBatch(RecordMapperWrapperRecordIterator.java:82)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:162)
    ... 6 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.convertProtoBinaryToRow(ProtoToRowConverter.java:129)
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:70)
    ... 15 more
Caused by: com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.
    at com.google.protobuf.InvalidProtocolBufferException.truncatedMessage(InvalidProtocolBufferException.java:115)
    at com.google.protobuf.CodedInputStream$ArrayDecoder.pushLimit(CodedInputStream.java:1196)
    at com.google.protobuf.CodedInputStream$ArrayDecoder.readMessage(CodedInputStream.java:887)
    at com.example.proto.MapMessage.<init>(MapMessage.java:64)
    at com.example.proto.MapMessage.<init>(MapMessage.java:9)
    at com.example.proto.MapMessage$1.parsePartialFrom(MapMessage.java:756)
    at com.example.proto.MapMessage$1.parsePartialFrom(MapMessage.java:750)
    at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:158)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:191)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:203)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:208)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:48)
    at com.example.proto.MapMessage.parseFrom(MapMessage.java:320)
    ... 21 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31944,,,,,,,,,,,,,,,,,,,,,,,"05/May/23 06:17;xuannan;flink-protobuf-example.zip;https://issues.apache.org/jira/secure/attachment/13057850/flink-protobuf-example.zip",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jun 17 14:55:45 UTC 2023,,,,,,,,,,"0|z1hpi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 07:13;libenchao;CC [~maosuhan] ;;;","23/May/23 04:29;maosuhan;I think the error is related to corrupt data.
 ;;;","25/May/23 16:53;rskraba;Hello, thanks for the example project!  That's so helpful to reproduce and debug.

The *current* file strategy for protobuf in Flink is to write one record serialized as binary per line, adding a *{{0x0a}}*.  Your example message is serialized as:

{code}
12 06 0a 01 61 12 01 62 0a
{code}

The first *{{0a}}* is the protobuf encoding for the key field in the map.  The last *{{0a}}* is a new line (which probably shouldn't be there).

When reading, from a file, splits are calculated and assigned to tasks using the *{{0a}}* as a delimiter, which is very, very likely to fail and a fault in the protobuf file implementation of Flink.

I'm guessing this isn't limited to maps, we can expect this delimiter byte to occur many different ways in the protobuf binary.

If this hasn't been addressed, it's probably because it's pretty rare to store protobuf messages in a file container (as opposed to in a single message packet, or a table cell).  Do you have a good use case that we can use to guide what we expect Flink to do with protobuf files?

For info, nothing in the Protobuf encoding that can be used to [distinguish the start or end|https://protobuf.dev/programming-guides/techniques/#streaming] of a message.  If we want to store multiple messages in the same container (file or any sequence of bytes), we have to manage the indices ourselves  The above link recommends writing the message size followed by the binary (in Java, this is using {{writeDelimitedTo}}/{{parseDelimitedFrom}} instead of {{writeTo}}/{{parseFrom}}, for example).;;;","25/May/23 17:03;rskraba;Oh, just taking a quick look – protobuf isn't supported by the filesystem connector in the [Flink|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/filesystem/] docs.  The real bug here might be that filesystem + protobuf doesn't fail immediately as an unsupported option!;;;","26/May/23 02:29;libenchao;[~rskraba] Thanks for the digging. I agree that currently FileSystem's fallback (de)serializer does not fit for all formats, this should be one of them. It might be worth to add support to implement a {{BulkReaderFormatFactory}} for protobuf format.;;;","26/May/23 09:56;rskraba;It's probably worth checking in with the community to see what we expect of a bulk format, or if it would be an interesting thing to add!  [I'll ask on the mailing list|https://lists.apache.org/thread/z9tqdqrhj12c17wqsdbm5fhzonqq5kp0].

I took a quick look but didn't see any related JIRA (outside of FLINK-12149, which proposes using the protobuf API to interact with parquet files).  Can a committer change the title of this JIRA to better reflect the issue?  Something like ""Protobuf format on filesystem is faulty"";;;","26/May/23 10:55;libenchao;I've changed the title.;;;","16/Jun/23 17:53;rskraba;I've created a PR that prevents the protobuf format from being used to write bulk files.  (I used the error message {{""The 'protobuf' format is not supported for the 'filesystem' connector.""}}, but this could be refined.)

In my experience and with the responses on the mailing list, I don't think that create a custom BulkReaderFormatFactory/BulkWriterFormatFactory would be used.  At the minimum, we should fail quickly (instead of generating unreadable files silently).

If there's a future need to create our own protobuf-oriented file format (for temporary storage?) or if someone makes a protobuf file container in the future, we can probably revisit this strategy!  In the meantime, I'd probably recommend using something like parquet (with the protobuf-message-oriented API) to persist bulk records in a file.  What do you think?;;;","17/Jun/23 14:47;libenchao;[~rskraba] Thanks, I agree with your proposal, I'll review it.;;;","17/Jun/23 14:55;libenchao;Fixed via https://github.com/apache/flink/commit/7d4ee28e85aad4abc8ad126c4d953d0e921ea07e (master);;;",,,,,,,,,,,,,,,,,
Implement Python Wrappers for DynamoDB Connector,FLINK-32007,13535051,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,khanhvu,chalixar,chalixar,04/May/23 20:15,16/Apr/24 10:33,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,aws-connector-4.4.0,,,,,,,,,API / Python,Connectors / DynamoDB,,,,,0,,,,,,Implement Python API Wrappers for DynamoDB Sink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-04 20:15:32.0,,,,,,,,,,"0|z1hp88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncWaitOperatorTest.testProcessingTimeWithTimeoutFunctionOrderedWithRetry times out on Azure,FLINK-32006,13535048,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,dmvk,dmvk,dmvk,04/May/23 19:17,28/Feb/24 11:47,04/Jun/24 20:41,,1.17.2,1.18.0,1.19.0,,,,,,,,,,,,,,,,,,,,,,,,,,,API / DataStream,,,,,,0,pull-request-available,stale-assigned,test-stability,,,"{code:java}
May 04 13:52:18 [ERROR] org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.testProcessingTimeWithTimeoutFunctionOrderedWithRetry  Time elapsed: 100.009 s  <<< ERROR!
May 04 13:52:18 org.junit.runners.model.TestTimedOutException: test timed out after 100 seconds
May 04 13:52:18 	at java.lang.Thread.sleep(Native Method)
May 04 13:52:18 	at org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.testProcessingTimeAlwaysTimeoutFunctionWithRetry(AsyncWaitOperatorTest.java:1313)
May 04 13:52:18 	at org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorTest.testProcessingTimeWithTimeoutFunctionOrderedWithRetry(AsyncWaitOperatorTest.java:1277)
May 04 13:52:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 04 13:52:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 04 13:52:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 04 13:52:18 	at java.lang.reflect.Method.invoke(Method.java:498)
May 04 13:52:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
May 04 13:52:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
May 04 13:52:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
May 04 13:52:18 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
May 04 13:52:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
May 04 13:52:18 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
May 04 13:52:18 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
May 04 13:52:18 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
May 04 13:52:18 	at java.lang.Thread.run(Thread.java:748)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48671&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9288",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 28 11:47:36 UTC 2024,,,,,,,,,,"0|z1hp7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 21:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48788&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9288;;;","11/Jul/23 07:10;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51135&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9673;;;","11/Aug/23 10:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","01/Sep/23 11:32;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52912&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9707;;;","07/Oct/23 07:02;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53583&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9664;;;","07/Oct/23 07:05;Sergey Nuyanzin;set to critical since it is reproduced again;;;","17/Nov/23 08:39;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54638&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","29/Nov/23 07:52;mapohl;FLINK-27075-related: https://github.com/XComp/flink/actions/runs/7019921193/job/19099004364#step:12:9613;;;","16/Dec/23 13:44;Sergey Nuyanzin;1.17: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55582&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9446];;;","11/Jan/24 16:46;JunRuiLi;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56262&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","13/Feb/24 09:39;mapohl;1.18: https://github.com/apache/flink/actions/runs/7880739758/job/21503455883#step:10:9621;;;","19/Feb/24 07:39;lincoln.86xy;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57597&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702;;;","28/Feb/24 11:47;mapohl;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57917&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9734;;;",,,,,,,,,,,,,,
Add a per-deployment error metric to signal about potential issues,FLINK-32005,13535037,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,04/May/23 17:23,11/May/23 11:11,04/Jun/24 20:41,11/May/23 11:11,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,,,"If any of the autoscaled deployment produce errors they are only visible in the logs or in the k8s events. Additionally, it would be good to have metrics to detect any potential issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-04 17:23:33.0,,,,,,,,,,"0|z1hp54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent ingress creation when JobManager is restarted via autoscaler,FLINK-32004,13535034,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanee.kim,tanee.kim,04/May/23 17:09,04/May/23 17:12,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,,,,,,"I set up access to the flink Web UI via the ingress settings in flinkDeployment.

Since this is an AWS environment, the ingress class uses ALB, not NGINX.

When using aws alb with flinkDeployment ingress settings when JobManager is restarted via autoscaler, intermittently ingress creation fails.

I say intermittent because ingress is generated irregularly, regardless of whether scaling is successful or not.

 

This issue occurs when the initial deployment always succeeds in creating INGRESS and accessing the FLINK WEB UI, but when it is scaled up/down via the autoscaler, INGRESS is not created sometimes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-04 17:09:51.0,,,,,,,,,,"0|z1hp4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release 3.0.0-1.16 and 1.16.1 doesn't work with OAuth2,FLINK-32003,13535025,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,nlu90,nlu90,04/May/23 16:25,24/May/23 08:06,04/Jun/24 20:41,24/May/23 08:06,pulsar-3.0.0,,,,,,,,,,,,,,,,,,,,pulsar-3.0.1,,,,,,,,,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,"The release for 3.0.0-1.16 and 1.16.1 depends on Pulsar client version 2.10.1.

There is an issue using OAuth2 with this client version which results in the following error.


{code:java}
Exception in thread ""main"" java.lang.RuntimeException: org.apache.pulsar.client.admin.PulsarAdminException: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at me.nlu.pulsar.PulsarAdminTester.main(PulsarAdminTester.java:56)
Caused by: org.apache.pulsar.client.admin.PulsarAdminException: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.client.admin.internal.BaseResource.getApiException(BaseResource.java:251)
	at org.apache.pulsar.client.admin.internal.TopicsImpl$1.failed(TopicsImpl.java:187)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation$1.failed(JerseyInvocation.java:882)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.processFailure(ClientRuntime.java:247)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.processFailure(ClientRuntime.java:242)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.access$100(ClientRuntime.java:62)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime$2.lambda$failure$1(ClientRuntime.java:178)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.apache.pulsar.shade.org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:288)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime$2.failure(ClientRuntime.java:178)
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$apply$1(AsyncHttpConnector.java:218)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$retryOperation$4(AsyncHttpConnector.java:277)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.NettyResponseFuture.abort(NettyResponseFuture.java:273)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener.onFailure(NettyConnectListener.java:181)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener$1.onFailure(NettyConnectListener.java:151)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.SimpleFutureListener.operationComplete(SimpleFutureListener.java:26)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.setHandshakeFailure(SslHandler.java:1882)
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1067)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
	at org.apache.pulsar.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at org.apache.pulsar.shade.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.runTask$$$capture(AbstractEventExecutor.java:174)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
	at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at org.apache.pulsar.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
	at org.apache.pulsar.shade.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.apache.pulsar.shade.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$OrApply.tryFire(CompletableFuture.java:1503)
	... 37 more
Caused by: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$retryOperation$4(AsyncHttpConnector.java:279)
	... 34 more
Caused by: java.net.ConnectException: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener.onFailure(NettyConnectListener.java:179)
	... 28 more
Caused by: java.nio.channels.ClosedChannelException
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1064)
	... 17 more{code}

We need to upgrade the pulsar-client-all version to at least 2.10.2 (for which I have verified the client auth issue is fixed) and publish new releases for 1.16 and 3.0.0-1.16.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 24 08:06:27 UTC 2023,,,,,,,,,,"0|z1hp2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/23 06:07;tison;Thanks for reporting this issue [~nlu90]! It's reasonable to upgrade patch versions as long as it holds the semantic.;;;","24/May/23 07:55;leonard;[~tison] Could you also open a PR for branch main?;;;","24/May/23 07:58;tison;The main branch already has more later version. It's not affected by this issue.;;;","24/May/23 08:06;leonard;Thanks [~tison]  for the clarification.;;;",,,,,,,,,,,,,,,,,,,,,,,
Revisit autoscaler defaults for next release,FLINK-32002,13534993,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,04/May/23 13:47,30/Apr/24 16:05,04/Jun/24 20:41,11/May/23 11:20,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,,,We haven't put much thought into the defaults. We should revisit and adjust the defaults to fit most use cases without being overly aggressive.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 11 11:20:58 UTC 2023,,,,,,,,,,"0|z1hovc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/23 11:20;gyfora;merged to main 063f22c09519ee53968474d2a4fd2289bb1ae442;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SupportsRowLevelUpdate does not support returning only a part of the columns.,FLINK-32001,13534991,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,Ming Li,Ming Li,04/May/23 13:18,19/May/23 08:09,04/Jun/24 20:41,19/May/23 08:09,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Table SQL / Runtime,,,,,,0,pull-request-available,,,,,"[FLIP-282|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=235838061] introduces the new Delete and Update API in Flink SQL. Although it is described in the documentation that in case of {{partial-update}} we only need to return the primary key columns and the updated columns.

But in fact, the topology of the job  is {{{}source -> cal -> constraintEnforcer -> sink{}}}, and the constraint check will be performed in the operator of {{{}constraintEnforcer{}}}, which is done according to index, not according to column. If only some columns are returned, the constraint check is wrong, and it is easy to generate {{{}ArrayIndexOutOfBoundsException{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 19 08:09:08 UTC 2023,,,,,,,,,,"0|z1houw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 02:30;lzljs3620320;CC [~yuxia] ;;;","05/May/23 12:22;luoyuxia;[~Ming Li] Thanks for reporting. I have submited a pr.;;;","19/May/23 08:09;Weijie Guo;master(1.18) via 6695d84dbb705640eb3b86b1ba50c4b09d4d3156.
release-1.17 via 0a04422f2838a000f36c80b940d9a04c35b46948.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Expose vertex max parallelism in the WebUI,FLINK-32000,13534980,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dmvk,dmvk,dmvk,04/May/23 12:15,11/Mar/24 16:02,04/Jun/24 20:41,05/May/23 09:09,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,,"It would be great to expose max parallelism in the vertex detail drawer for debug purposes  !Screenshot 2023-05-04 at 14.15.34.png|width=533,height=195! .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25068,,,,,,,,,,,,,,,,,,,,,,,"04/May/23 12:15;dmvk;Screenshot 2023-05-04 at 14.15.34.png;https://issues.apache.org/jira/secure/attachment/13057834/Screenshot+2023-05-04+at+14.15.34.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 05 09:09:02 UTC 2023,,,,,,,,,,"0|z1hosg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 09:09;dmvk;master: 43528be1b10fd291d89f73c423dfb3a02a05f5aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CI fails in preparing the e2e test runs due openssl unavailable,FLINK-31999,13534968,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,mapohl,mapohl,mapohl,04/May/23 11:32,12/Jun/23 06:35,04/Jun/24 20:41,04/May/23 11:44,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,,,0,test-stability,,,,,"We experience build failures due to the openssl download URL causing a 404:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48653&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=d6e79740-7cf7-5407-2e69-ca34c9be0efb]

It is indeed due to the URL having changed slightly:
 * old:   [http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.deb]
 * new: [http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.12_amd64.deb]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31962,,,,,,,FLINK-22856,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 12 06:35:17 UTC 2023,,,,,,,,,,"0|z1hops:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 11:39;mapohl;[~rmetzger] what was the reason for us to rely on {{wget}} instead of {{apt}} for installing the openssl library? Is it because we didn't want to update OpenSSL along the Ubuntu update to be on the safe side for netty?;;;","04/May/23 11:46;rmetzger;I think some e2e test was failing with the ubuntu provided openssl version. 
But maybe this problem has been addressed by netty by now?;;;","05/May/23 11:48;mapohl;[~chesnay] do you have an opinion in that matter? I tried to replace the wget call in [XComp/Flink:netty-openssl|https://github.com/apache/flink/compare/master...XComp:flink:netty-openssl]. It didn't cause problems in [CI|https://dev.azure.com/mapohl/flink/_build/results?buildId=1295&view=results] (, anymore?).;;;","12/Jun/23 06:35;mapohl;just as an fyi: This issue is covered in FLINK-32238. The approach which I proposed in my previous comment was tried and failed (see [PR comments|https://github.com/apache/flink/pull/22697#issuecomment-1572310693]).;;;",,,,,,,,,,,,,,,,,,,,,,,
Flink Operator Deadlock on run job Failure,FLINK-31998,13534959,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chalixar,chalixar,04/May/23 10:31,31/Aug/23 06:25,04/Jun/24 20:41,,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"h2. Description

FlinkOperator Reconciler goes into deadlock situation where it never udpates Session job to DEPLOYED/ROLLED_BACK if {{deploy}} fails.
Attached sequence diagram of the issue where FlinkSessionJob is stuck in UPGRADING indefinitely.
h2. proposed fix

Reconciler should roll back changes CR if {{reconciliationStatus.isBeforeFirstDeployment()}} fails to {{{}deploy(){}}}.
[diagram|https://issues.apache.org/7239bb39-60d8-48a0-9052-f3231947edbe]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/23 10:26;chalixar;gleek-m6pLe3Wy--IpCKQavAQwBQ.png;https://issues.apache.org/jira/secure/attachment/13057828/gleek-m6pLe3Wy--IpCKQavAQwBQ.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 31 06:25:22 UTC 2023,,,,,,,,,,"0|z1hons:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 10:35;chalixar;CC [~gyfora];;;","14/May/23 04:07;ZhenqiuHuang;[~gyfora] Technically, if a session job is created, it is actually a session cluster that can run multiple jobs in parallel or sequentially. But from session job CRD, the cluster to job mapping is 1 to 1. We probably need to adjust the CRD to decouple the job status and session cluster status.;;;","31/Aug/23 06:25;gyfora;[~chalixar] is this still an issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,
Update to Fabric8 6.5.1+ in flink-kubernetes,FLINK-31997,13534929,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,04/May/23 08:24,01/Feb/24 10:24,04/Jun/24 20:41,30/May/23 15:03,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Deployment / Kubernetes,,,,,,0,pull-request-available,,,,,"We should update the fabric8 version in flink-kubernetes to at least 6.5.1. 

Flink currently uses a very old fabric8 version. The fabric8 library dependencies have since been revised and greately improved to make them more moduler and allow eliminating securitiy vulnerabilities more easily like: https://issues.apache.org/jira/browse/FLINK-31815

The newer versions especially 6.5.1 + also add some improvement stability fixes for watches and other parts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34333,,,,,FLINK-33408,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 30 15:03:42 UTC 2023,,,,,,,,,,"0|z1hoh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 00:56;thw;[~gyfora] would it be possible to also shade the fabric8 dependency that goes into flink-kubernetes.jar? Otherwise there is the possibility of running into class path conflicts when the application also has a fabric8 dependency as I have seen in at least one case.;;;","08/May/23 06:39;gyfora;[~thw] I think this is done since 1.17, I will check and if not I will open a separate ticket for this so we can fix it for 1.18;;;","30/May/23 15:03;gyfora;Merged to master c5808b04fdce9ca0b705b6cc7a64666ab6426875;;;",,,,,,,,,,,,,,,,,,,,,,,,
Chaining operators with different max parallelism prevents rescaling,FLINK-31996,13534928,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,04/May/23 08:22,16/Oct/23 11:26,04/Jun/24 20:41,06/Jul/23 13:43,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"We might chain operators with different max parallelism together if they are set to have the same parallelism initially.

When we decide to rescale the JobGraph vertices (using AdaptiveScheduler), we're gapped by the lowest maxParallelism of the operator chain. This is especially visible with things like CollectSink, TwoPhaseCommitSink, CDC, and a GlobalCommiter with maxParallelism set to 1.

 

An obvious solution would be to prevent the chaining of operators with different maxParallelism, but we need to double-check this doesn't introduce a breaking change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32022,,,,,,,,FLINK-31316,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 06 13:43:23 UTC 2023,,,,,,,,,,"0|z1hogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 08:28;gyfora;In theory user can also break the operator chain by redeploying the job with a different parallelism setting and that should be compatible.
So I would expect this to not be a breaking change.;;;","04/May/23 08:44;dmvk;The main question is whether we need an opt-out feature flag to allow people to revert to the old behavior.;;;","04/May/23 08:48;chesnay;??So I would expect this to not be a breaking change.??

We need an opt-in flag because this change may break existing chains, which can result in certain types being serialized that previously weren't. This can affect performance  and savepoint compatibility.
E.g., if you have a job that is one big chain then your super special custom type doesn't go through Kryo. Users may also rely on the same object being passed through the job.;;;","04/May/23 08:50;gyfora;Fair point [~chesnay] , I think you are completely right this cannot be the new default.;;;","06/Jul/23 13:43;chesnay;master: 58a849bcc497e17bf575e49946f106030f3e1a1a;;;",,,,,,,,,,,,,,,,,,,,,,
DirectExecutorService doesn't follow the ExecutorService contract throwing a RejectedExecutionException in case it's already shut down,FLINK-31995,13534922,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,04/May/23 07:45,30/May/23 13:31,04/Jun/24 20:41,04/May/23 15:31,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"We experienced an issue where we tested behavior using the {{DirectExecutorService}} with the {{ExecutorService}} being shutdown already. The tests succeeded. The production code failed, though, because we used a singleThreadExecutor for which the task execution failed after the executor was stopped. We should add this behavior to the {{DirectExecutorService}} as well to make the unit tests be closer to the production code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32204,,,,,FLINK-29813,,,,,,,,FLINK-31838,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 04 15:31:46 UTC 2023,,,,,,,,,,"0|z1hofk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 15:31;mapohl;master: 8d430f51d77cf4f0d3291da6a7333f1aa9a87d22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update copyright in NOTICE files of flink-shaded ,FLINK-31994,13534906,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,leonard,leonard,04/May/23 04:34,04/May/23 06:45,04/Jun/24 20:41,04/May/23 06:45,1.17.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,BuildSystem / Shaded,,,,,,0,,,,,,"The copyright of all flink-shaded dependency NOTICE files are still 2021, we can use tools/update_notice_year.sh to update it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 04 06:45:37 UTC 2023,,,,,,,,,,"0|z1hoc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 06:45;leonard;Fixed in https://github.com/apache/flink-shaded/commit/f12239b5d5ec865713c00087fa379c3d8305ac1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Initialize and pass down FailureEnrichers,FLINK-31993,13534893,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,04/May/23 01:18,31/May/23 15:07,04/Jun/24 20:41,31/May/23 10:50,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 31 10:49:51 UTC 2023,,,,,,,,,,"0|z1ho94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 10:49;dmvk;master: 603181da811edb47c0d573492639a381fbbedc28;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"FlinkKafkaConsumer API is suggested to use as part of documentation, when that API is deprecated for flink version 1.14",FLINK-31992,13534854,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Information Provided,,sandeshmendan,sandeshmendan,03/May/23 16:11,14/Apr/24 11:07,04/Jun/24 20:41,19/Aug/23 18:39,1.14.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,auto-deprioritized-major,documentation,documentation-update,good-first-issue,newbie,"In Flink version 1.14, even though the API class FlinkKafkaConsumer had been [deprecated|https://nightlies.apache.org/flink/flink-docs-release-1.14/api/java/], the official [documentation|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/event-time/generating_watermarks/#watermark-strategies-and-the-kafka-connector] suggests that API to use.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 18:40:22 UTC 2023,,,,,,,,,,"0|z1ho0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/May/23 09:29;shreeju;Hi [~sandeshmendan] , I'm new to the Apache Flink community. I'm eager to contribute. May I take this up?;;;","05/Jun/23 13:07;martijnvisser;[~sandeshmendan] [~shreeju] Please check if this occurs for documentation beyond Flink 1.14. I don't think that's the case. For older Flink versions that are no longer supported, I wouldn't fix this issue since users should use a still supported version.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","19/Aug/23 18:39;sandeshmendan;Thanks for your update [~martijnvisser] ;;;","19/Aug/23 18:40;sandeshmendan;Since this issue has been corrected in higher flink version, I am proceeding to close this ticket;;;",,,,,,,,,,,,,,,,,,,,,
Update Autoscaler doc to reflect the changes brought by the new source scaling logic,FLINK-31991,13534825,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,03/May/23 12:19,03/May/23 15:40,04/Jun/24 20:41,03/May/23 15:40,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Autoscaler,,,,,,0,pull-request-available,,,,,"The current statements on job requirements are outdated:

??- All sources must use the new Source API (most common connectors already do)??
??- Source scaling requires sources to expose the standardized connector metrics for accessing backlog information (source scaling can be disabled)??

The Autoscaler doc needs to be updated to reflect the changes brought by the new source scaling logic [FLINK-31326] Disabled source scaling breaks downstream scaling if source busyTimeMsPerSecond is 0 - ASF JIRA (apache.org).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/23 12:16;Zhanghao Chen;image-2023-05-03-20-16-33-704.png;https://issues.apache.org/jira/secure/attachment/13057806/image-2023-05-03-20-16-33-704.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 03 14:55:13 UTC 2023,,,,,,,,,,"0|z1hnu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/23 14:55;mxm;Thanks for raising this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Use Flink Configuration to specify KDS Source configuration object,FLINK-31990,13534800,13404461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,liangtl,liangtl,liangtl,03/May/23 09:08,02/Feb/24 10:10,04/Jun/24 20:41,19/Jun/23 12:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"*What*

Use the Flink Configuration object to standardise the method of specifying configurations for the KDS source. 

 

Also include validations:
 - -Check that region in config matches ARN. ARN should take priority.-

 

*Why*

We want to standardise error messages + source serialization methods implemented by Flink on the Flink Configuration objects.

 ",,,,,,,,,,FLINK-31813,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 19 12:50:19 UTC 2023,,,,,,,,,,"0|z1hnog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 17:18;dannycranmer;Merged commit [{{1006e6a}}|https://github.com/apache/flink-connector-aws/commit/1006e6ae47bcc6185b2782188958dd1e6f45634a] into apache:main ;;;","15/Jun/23 17:19;dannycranmer;This bit is not done [~liangtl] ""Check that region in config matches ARN. ARN should take priority."". Please extract to new Jira or open another PR;;;","15/Jun/23 22:04;liangtl;Thanks for flagging, Danny!

I did some thinking about this, and actually just realise that this is not needed.

A user might want to make a cross-region call to a Kinesis stream. That way the config's Region can be different to the stream ARN's region. Will strike off in the description

 

 ;;;","19/Jun/23 12:50;dannycranmer;ok thanks [~liangtl] 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Update documentation,FLINK-31989,13534798,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,03/May/23 09:05,02/Feb/24 10:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Update Flink documentation to explain the new KDS source. Include
 * Improvements available in new KDS source
 * Incompatible changes made
 * Example implementation
 * Example customisations",,,,,,,,,,FLINK-31813,FLINK-34341,FLINK-31987,FLINK-31980,FLINK-31988,FLINK-32324,FLINK-32218,FLINK-32229,FLINK-33180,FLINK-34342,FLINK-32097,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 02 10:09:48 UTC 2024,,,,,,,,,,"0|z1hno0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/24 10:09;dannycranmer;We could split this task and deliver incrementally with each subtask, rather than all at once;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Python wrapper for new KDS source,FLINK-31988,13534797,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,03/May/23 09:03,02/Feb/24 10:08,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,Connectors / Kinesis,,,,,0,,,,,,"*What?*

- Implement Python wrapper for KDS source

- Write tests for this KDS source",,,,,,,,,,FLINK-31813,,,,,,,,,,,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-03 09:03:58.0,,,,,,,,,,"0|z1hnns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement KDS Table API support,FLINK-31987,13534796,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,03/May/23 09:02,02/Feb/24 10:07,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kinesis,,,,,,0,pull-request-available,,,,,"Implement Table API support for KDS Source.

 

Consider:
 * Configurations to support. Should have customisation parity with DataStream API
 * Testing should include both SQL client + Table API via Java",,,,,,,,,,FLINK-31813,,,,,,,,,,,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-03 09:02:57.0,,,,,,,,,,"0|z1hnnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Integration test for FLIP-27 DataStreams API,FLINK-31986,13534792,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,03/May/23 08:55,02/Feb/24 10:05,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"*What?*

Implement an integration test that uses Kinesalite.

It should spin up FLIP-27 KDS source and test:
 * Reading from single shard KDS stream
 * Reading from multiple shard KDS stream
 * Reading from resharded KDS stream
 * Failure modes (stream doesn't exist)",,,,,,,,,,FLINK-31813,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-03 08:55:50.0,,,,,,,,,,"0|z1hnmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The currentDate() get transformed to TIMESTAMP_LTZ(3) instead of DATE(),FLINK-31985,13534774,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dheerajpanangat,dheerajpanangat,03/May/23 05:31,21/May/23 15:36,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / JDBC,,,,,,0,,,,,,"As per the [doc|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/systemfunctions/], the currentDate() function should match the SQL Function CURRENT_DATE 
and currentTimestamp() function should match the SQL Function CURRENT_TIMESTAMP.

Currently seeing that the currentDate() transforms the column to TIMESTAMP instead of DATE.

Alternate/Temporary Solution which works : 
If instead of using {{}}
{code:java}
table.select(currentDate().as(""xyz_date""){code}
if I write SqlQuery 
{code:java}
select CURRENT_DATE as xyz_date{code}
it work fine and gets the value as a DATE and not timestamp

Can we please fix this for flink version 1.16 ?

Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 21 15:36:41 UTC 2023,,,,,,,,,,"0|z1hnio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/23 15:36;jark;Do you mean the return type of Table API {{currentDate()}} doesn't return ""DATE"" but ""TIMESTAMP""? If that is true, I think this is a bug and welcome contributing the fix. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint on S3 should be relocatable if entropy injection is not effective,FLINK-31984,13534759,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liuml07,liuml07,liuml07,02/May/23 23:33,08/May/23 14:31,04/Jun/24 20:41,08/May/23 14:31,1.16.1,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,FileSystems,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"We have a limitation that if we create savepoints with an injected entropy, they are not relocatable (https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/savepoints/#triggering-savepoints).

FLINK-25952 improves the check by inspecting both the FileSystem extending {{EntropyInjectingFileSystem}} and {{FlinkS3FileSystem#getEntropyInjectionKey}} not returning null. We can improve this further by checking the checkpoint path is indeed using the entropy injection key. Without that, the savepoint is not relocatable even if the {{state.savepoints.dir}} does not contain the entropy.

In our setting, we enable entropy injection by setting {{s3.entropy.key}} to 
 {{\__ENTROPY_KEY\__}} and use the entropy key in the checkpoint path (for e.g. {{s3://mybuket/checkpoints/__ENTROPY_KEY__/myapp}}). However, in the savepoint path, we don't use the entropy key (for e.g. {{s3://mybuket/savepoints/myapp}}) because we want the savepoint to be relocatable. But the current logic still generates non-relocatable savepoint path just because the entropy injection key is non-null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 08 14:31:59 UTC 2023,,,,,,,,,,"0|z1hnfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/23 23:39;liuml07;CC: [~markcho];;;","04/May/23 08:52;xtsong;- master (1.18): 7cdbfce8d1b8f222f42229a533282f1e8a0ed95a
- release-1.17: 5c0a53534edb34eeee4bbff09fc5ea4dcc5983c8
- release-1.16: cccef2da73e0a12e7e19bac182497919c72e46f0;;;","04/May/23 11:20;mapohl;I'm reopening the issue because it seems that the backports break API constraints.

I reverted the backports:
 * 1.17: 8dd0b770936b14dcdc23a392e3a00db5ffa2d50f
 * 1.16: 9e08f2059689109be9b647a1064b56bce286e345;;;","04/May/23 12:45;xtsong;Thanks for fixing it, [~mapohl]. And sorry for causing this trouble. I'm a bit surprised that {{EntropyInjector}} is an API class.

Now I see two options to moving forward: 1) we merge this only for 1.18, or 2) we try to fix this for 1.16 & 1.17 in a compatible way (preserving the old and likely unused public method). Given that this is an improvement, I'm leaning towards 1).

[~liuml07] WDTY?;;;","04/May/23 15:46;liuml07;Thanks for catching this [~mapohl]. Sorry it is my fault to break public API without a closer look into EntropyInjector. I completely agree with [~xtsong] on preserving (and deprecating?) the old public method while introducing the new one. I will provide a patch shortly for 1.16 and 1.17.;;;","05/May/23 06:28;mapohl;No worries - things happen. :);;;","05/May/23 07:33;liuml07;The new [PR #22522|https://github.com/apache/flink/pull/22522] for 1.17 branch can be cherry-picked to release-1.16 branch cleanly. So I didn't attach a separate PR for 1.16.;;;","08/May/23 14:31;xtsong;release-1.17: 9d31efe7e4ec358a64f79ca5e8ff516cabea0cce
release-1.16: 68a79a05155d4a43395ad51589d9a2cae740bea5;;;",,,,,,,,,,,,,,,,,,,
Add yarn acls capability to flink containers,FLINK-31983,13534755,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,argoyal,argoyal,argoyal,02/May/23 22:08,28/Jun/23 02:05,04/Jun/24 20:41,28/Jun/23 02:05,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Deployment / YARN,,,,,,0,pull-request-available,,,,,"Yarn provide application acls mechanism to be able to provide specific rights to other users than the one running the job (view logs through the resourcemanager/job history, kill the application)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 28 02:05:20 UTC 2023,,,,,,,,,,"0|z1hneo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/23 22:19;argoyal;{*}Note{*}: This PR is based on an earlier PR and I have addressed all the comments on the previous PR.
[GitHub Pull Request #8760|https://github.com/apache/flink/pull/8760]

Jira reference ticket: FLINK-12869;;;","28/Jun/23 02:05;wangyang0918;Fixed via:

master: c5acd8dd800dfcd2c8873c569d0028fc7d991b1c

 

Thanks [~argoyal] for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Build image from source Dockerfile error in main,FLINK-31982,13534738,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Workaround,,jbusche,jbusche,02/May/23 19:07,10/May/23 18:16,04/Jun/24 20:41,10/May/23 18:16,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"I'm noticing a problem trying to build the Debian Flink Operator image from the Dockerfile in the main branch.

 

podman build -f Dockerfile -t debian-release:1.5.0-rc1

....

[INFO] Compiling 9 source files to /app/flink-kubernetes-operator-autoscaler/target/test-classes

....

[ERROR] COMPILATION ERROR : 

[INFO] -------------------------------------------------------------

[ERROR] /app/flink-kubernetes-operator-autoscaler/src/test/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingMetricEvaluatorTest.java:[59,8] error while writing org.apache.flink.kubernetes.operator.autoscaler.ScalingMetricEvaluatorTest: /app/flink-kubernetes-operator-autoscaler/target/test-classes/org/apache/flink/kubernetes/operator/autoscaler/ScalingMetricEvaluatorTest.class: Too many open files

[ERROR] /app/flink-kubernetes-operator-autoscaler/src/test/java/org/apache/flink/kubernetes/operator/autoscaler/JobVertexScalerTest.java:[78,29] cannot access org.apache.flink.kubernetes.operator.autoscaler.ScalingSummary

  bad class file: /app/flink-kubernetes-operator-autoscaler/target/classes/org/apache/flink/kubernetes/operator/autoscaler/ScalingSummary.class

    unable to access file: java.nio.file.FileSystemException: /app/flink-kubernetes-operator-autoscaler/target/classes/org/apache/flink/kubernetes/operator/autoscaler/ScalingSummary.class: Too many open files

    Please remove or make sure it appears in the correct subdirectory of the classpath.

[ERROR] /app/flink-kubernetes-operator-autoscaler/src/test/java/org/apache/flink/kubernetes/operator/autoscaler/JobVertexScalerTest.java:[84,29] incompatible types: inferred type does not conform to equality constraint(s)

 

I've tried increasing my nofiles to unlimited, but still see the error.

I tried building the release 1.4.0 and it built fine, so not certain what's recently changed in 1.5.0. Maybe it builds fine in Docker instead of podman?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 10 18:14:16 UTC 2023,,,,,,,,,,"0|z1hnaw:",9223372036854775807,Can successfully build the 1.5.0 container using podman while on Red Hat 9.0 and podman version 4.4.1,,,,,,,,,,,,,,,,,,,"03/May/23 04:52;gyfora;Strange error, haven’t seen this before. We did not add any new modules and these tests were there in similar form in 1.4.0 also.[~mbalassi] do you have any idea what’s going on here?;;;","10/May/23 18:12;jbusche;I'm not sure why it is that the 1.5.x build is failing with podman on my Red Hat 8.x servers, but 1.4.0 worked fine.

The podman version there is a bit older:

_podman --version_

_podman version 4.2.0_

I just tried it on a Red Hat 9.0 server and it built great.  podman there is:

_podman --version_

_podman version 4.4.1_;;;","10/May/23 18:14;jbusche;I'm going to mark this as resolved - that the resolution is to use a later version of Red Hat (Red Hat 9.0) for building the 1.5.0 release of the Flink Operator.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Enabling waiting on the Flink Application to complete before returning to Flink client for batch jobs,FLINK-31981,13534733,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,achang52255,achang52255,02/May/23 17:58,14/Oct/23 03:16,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / YARN,,,,,,0,,,,,,"Currently the Flink Client by default will immediately complete when it hits RUNNING state - we want to make it configurable for batch jobs so that the client only completes when the flink application has fully completed running, rather than just returning upon submission of the job. 

This allows us to have richer information about whether the underlying application has completed successfully or failed. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 10 09:28:18 UTC 2023,,,,,,,,,,"0|z1hn9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 09:28;huwh;Hi, [~achang52255] Does FLINK-25495 meets your requirement?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement support for EFO in new KDS Source,FLINK-31980,13534685,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,02/May/23 12:30,02/Feb/24 10:08,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kinesis,,,,,,0,pull-request-available,,,,,Implement support for reading from Kinesis Stream using Enhanced Fan Out mechanism,,,,,,,,,,FLINK-34339,,,,,,,,,,,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-02 12:30:08.0,,,,,,,,,,"0|z1hmz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constructing Glue Client using AwsGeneralUtils,FLINK-31979,13534676,13485272,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,02/May/23 11:39,02/May/23 11:39,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-02 11:39:09.0,,,,,,,,,,"0|z1hmx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-jdbc v.1.17.0 not published,FLINK-31978,13534606,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,sergiosp,sergiosp,01/May/23 15:53,05/Jun/23 13:08,04/Jun/24 20:41,05/Jun/23 13:08,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,,,,,,"The connector flink-connector-jdbc version 1.17.0 is not being published in public maven repositories:

 

[https://mvnrepository.com/artifact/org.apache.flink/flink-connector-jdbc]

 

Maybe related to FLINK-29642?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-05-01 15:53:50.0,,,,,,,,,,"0|z1hmhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If scaling.effectiveness.detection.enabled is false, the call to the detectIneffectiveScaleUp() function is unnecessary",FLINK-31977,13534554,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,tanee.kim,tanee.kim,30/Apr/23 09:21,11/Jan/24 15:15,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,,,,,,0,,,,,,"The code below is a function to detect inefficient scaleups.
It returns a result if the value of SCALING_EFFECTIVENESS_DETECTION_ENABLED (scaling.effectiveness.detection.enabled) is true after all the necessary computations for detection, but this is an unnecessary computation.
{code:java}
JobVertexScaler.java #175

private boolean detectIneffectiveScaleUp(
        AbstractFlinkResource<?, ?> resource,
        JobVertexID vertex,
        Configuration conf,
        Map<ScalingMetric, EvaluatedScalingMetric> evaluatedMetrics,
        ScalingSummary lastSummary) {

    double lastProcRate = lastSummary.getMetrics().get(TRUE_PROCESSING_RATE).getAverage(); // 22569.315633422066
    double lastExpectedProcRate =
            lastSummary.getMetrics().get(EXPECTED_PROCESSING_RATE).getCurrent(); // 37340.0
    var currentProcRate = evaluatedMetrics.get(TRUE_PROCESSING_RATE).getAverage();

    // To judge the effectiveness of the scale up operation we compute how much of the expected
    // increase actually happened. For example if we expect a 100 increase in proc rate and only
    // got an increase of 10 we only accomplished 10% of the desired increase. If this number is
    // below the threshold, we mark the scaling ineffective.
    double expectedIncrease = lastExpectedProcRate - lastProcRate;
    double actualIncrease = currentProcRate - lastProcRate;

    boolean withinEffectiveThreshold =
            (actualIncrease / expectedIncrease)
                    >= conf.get(AutoScalerOptions.SCALING_EFFECTIVENESS_THRESHOLD);
    if (withinEffectiveThreshold) {
        return false;
    }

    var message = String.format(INNEFFECTIVE_MESSAGE_FORMAT, vertex);

    eventRecorder.triggerEvent(
            resource,
            EventRecorder.Type.Normal,
            EventRecorder.Reason.IneffectiveScaling,
            EventRecorder.Component.Operator,
            message);

    if (conf.get(AutoScalerOptions.SCALING_EFFECTIVENESS_DETECTION_ENABLED)) {
        LOG.info(message);
        return true;
    } else {
        return false;
    }
} {code}
In the call to the detectIneffectiveScaleUp function, I would suggest checking SCALING_EFFECTIVENESS_DETECTION_ENABLED first, as follows.
{code:java}
JobVertexScaler.java #150

if (currentParallelism == lastSummary.getNewParallelism() && lastSummary.isScaledUp()) {
    if (scaledUp) {
        if(conf.get(AutoScalerOptions.SCALING_EFFECTIVENESS_DETECTION_ENABLED)) {
            return detectIneffectiveScaleUp(resource, vertex, conf, evaluatedMetrics, lastSummary);
        } else {
            return true;
        }
    } else {
        return detectImmediateScaleDownAfterScaleUp(vertex, conf, lastScalingTs);
    }
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33993,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 11 15:15:08 UTC 2024,,,,,,,,,,"0|z1hm6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/May/23 10:28;gyfora;The logic here is that we trigger an Event even if the effectiveness detection is disabled. This will not prevent further scale up but would still notify the user.;;;","01/May/23 13:03;tanee.kim;If so, what do you think about pulling out the event trigger as a caller as well?

 
{code:java}
if (currentParallelism == lastSummary.getNewParallelism() && lastSummary.isScaledUp()) {
    if (scaledUp) {
        var message = String.format(INNEFFECTIVE_MESSAGE_FORMAT, vertex);
        eventRecorder.triggerEvent(
                resource,
                EventRecorder.Type.Normal,
                EventRecorder.Reason.IneffectiveScaling,
                EventRecorder.Component.Operator,
                message);
        if(conf.get(AutoScalerOptions.SCALING_EFFECTIVENESS_DETECTION_ENABLED)) {
            return detectIneffectiveScaleUp(resource, vertex, conf, evaluatedMetrics, lastSummary);
        } else {
            return true;
        }
    } else {
        return detectImmediateScaleDownAfterScaleUp(vertex, conf, lastScalingTs);
    }
} {code}
 

 ;;;","04/May/23 08:55;gyfora;The problem here is that you need to call the detection logic before you trigger the event. The event should only be sent if ineffective scaling is detected.

From this perspective I think the current logic is correct;;;","04/May/23 16:55;tanee.kim;I understand what you're saying.
However, there's a part of the code that's a bit hard to understand because it has multiple returns based on conditional statements.
I think I can change it to something more intuitive like this, what do you think?

This is unrelated to the original suggestion in the jira ticket to improve inefficient function calls per SCALING_EFFECTIVENESS_DETECTION_ENABLED, but it does make the code a little easier to understand.
{code:java}
private boolean detectIneffectiveScaleUp(
        AbstractFlinkResource<?, ?> resource,
        JobVertexID vertex,
        Configuration conf,
        Map<ScalingMetric, EvaluatedScalingMetric> evaluatedMetrics,
        ScalingSummary lastSummary) {

    double lastProcRate = lastSummary.getMetrics().get(TRUE_PROCESSING_RATE).getAverage();
    double lastExpectedProcRate =
            lastSummary.getMetrics().get(EXPECTED_PROCESSING_RATE).getCurrent();
    var currentProcRate = evaluatedMetrics.get(TRUE_PROCESSING_RATE).getAverage();

    // To judge the effectiveness of the scale up operation we compute how much of the expected
    // increase actually happened. For example if we expect a 100 increase in proc rate and only
    // got an increase of 10 we only accomplished 10% of the desired increase. If this number is
    // below the threshold, we mark the scaling ineffective.
    double expectedIncrease = lastExpectedProcRate - lastProcRate;
    double actualIncrease = currentProcRate - lastProcRate;

    boolean isInEffectiveScaleUp =
            (actualIncrease / expectedIncrease)
                    < conf.get(AutoScalerOptions.SCALING_EFFECTIVENESS_THRESHOLD);

    if (isInEffectiveScaleUp) {
        var message = String.format(INNEFFECTIVE_MESSAGE_FORMAT, vertex);

        eventRecorder.triggerEvent(
                resource,
                EventRecorder.Type.Normal,
                EventRecorder.Reason.IneffectiveScaling,
                EventRecorder.Component.Operator,
                message);

        if (conf.get(AutoScalerOptions.SCALING_EFFECTIVENESS_DETECTION_ENABLED)) {
            LOG.info(
                    ""Ineffective scaling detected for {}, expected increase {}, actual {}"",
                    vertex,
                    expectedIncrease,
                    actualIncrease);
            return true;
        }
    }
    return false;
} {code};;;","11/Jan/24 15:15;mxm;I think this is related to FLINK-33993. The name of the configuration option is a bit misleading, as effectiveness detection is always on but scalings are only blocked when the option is set to {{true}}.;;;",,,,,,,,,,,,,,,,,,,,,,
"Once marked as an inefficient scale-up, further scaling may not happen forever",FLINK-31976,13534551,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,tanee.kim,tanee.kim,30/Apr/23 08:37,06/May/23 12:02,04/Jun/24 20:41,03/May/23 15:58,1.17.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Autoscaler,,,,,,0,pull-request-available,,,,,"The determination of whether it is an inefficient scale-up is calculated as follows
{code:java}
double lastProcRate = lastSummary.getMetrics().get(TRUE_PROCESSING_RATE).getAverage();
double lastExpectedProcRate =
lastSummary.getMetrics().get(EXPECTED_PROCESSING_RATE).getCurrent();
var currentProcRate = evaluatedMetrics.get(TRUE_PROCESSING_RATE).getAverage();
double expectedIncrease = lastExpectedProcRate - lastProcRate;
double actualIncrease = currentProcRate - lastProcRate;

boolean withinEffectiveThreshold =
(actualIncrease / expectedIncrease)
>= conf.get(AutoScalerOptions.SCALING_EFFECTIVENESS_THRESHOLD);{code}
Because the expectedIncrease value references the last scaling history, it will not change unless there is an additional scale-up, only the actualIncrease value will change.
The actualIncrease value is currentProcRate( avg of TRUE_PROCESSING_RATE),
The calculation of TRUE_PROCESSING_RATE is as follows
trueProcessingRate = busyTimeMultiplier * numRecordsInPerSecond.getSum()

For example, let's say you've been marked as an inefficient scale-up, but the LAG continues to build up.
You need to scale up to eliminate the growing LAG, but because you're marked as an inefficient scale-up, it won't happen.
To unmark a scaleup as inefficient, the following conditions must be met: actualIncrease/expectedIncrease > SCALING_EFFECTIVENESS_THRESHOLD (default 0.1)

Here, expectedIncrease is a constant with lastSummary, so the value of actualIncrease must increase.
However, the actualIncrease value is proportional to busyTimeMultiplier and numRecordsInPerSecond, and these two values will converge to a certain value if no scaling occurs.
Therefore, the value of actualIncrease will also converge.
If this value fails to cross a threshold, no further scaling up is possible, even if the lag continues to build up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/23 13:41;tanee.kim;image-2023-05-01-22-41-57-208.png;https://issues.apache.org/jira/secure/attachment/13057754/image-2023-05-01-22-41-57-208.png","01/May/23 14:54;tanee.kim;image-2023-05-01-23-54-06-383.png;https://issues.apache.org/jira/secure/attachment/13057755/image-2023-05-01-23-54-06-383.png","01/May/23 14:55;tanee.kim;image-2023-05-01-23-55-08-254.png;https://issues.apache.org/jira/secure/attachment/13057756/image-2023-05-01-23-55-08-254.png","01/May/23 17:08;tanee.kim;image-2023-05-02-02-08-25-920.png;https://issues.apache.org/jira/secure/attachment/13057758/image-2023-05-02-02-08-25-920.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 06 12:02:16 UTC 2023,,,,,,,,,,"0|z1hm5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/May/23 10:25;gyfora;We have some logic in the `addToScalingHistory` method to limit the scaling history size but we don't enforce this on get. If we also trim the history on get operations that would solve this right?;;;","01/May/23 13:50;tanee.kim;As you say, this could be improved if we could trim the history from the GET operation.

However, it doesn't seem to provide a fundamental workaround.
Below are some scaling-related metrics for situations where the lag is increasing, but is marked as ineffective, preventing further scaling.

!image-2023-05-01-22-41-57-208.png|width=655,height=327!

Under normal circumstances (roughly between 09:00 and 10:00), the value of TRUE_PROCESSING_RATE_AVG is located between SCALE_UP/DOWN_THRESHOLD. However, at around 10:00, after the PARALLELISM value increases from 1 to 2, the TRUE_PROCESSING_RATE_AVG value remains significantly below the SCALE_UP_THRESHOLD, marking it as ineffective and not continuing to scale.;;;","01/May/23 13:55;gyfora;We have to distinguish 2 things here. 

1. Scaling history that is not within the configured limit should be ignored during scaling decisions (this is the point I made earlier regarding trimming on get)
2. Is there something wrong with the innefective scale up detection

As for 2. , the innefective scaling logic does not have anything to do with scale up/down thresholds or LAG. The question is whether the last scale up operation resulted in increased throughput or not. If we did not get any increased throughput we mark the scaling ineffective for that operator. Even if the lag increases further scaling would not help (in theory) so we don't do it. ;;;","01/May/23 15:03;tanee.kim;Thank you for clarifying the issue.
I guess I was misleading in what I said.
I think it would be good to discuss 2. further.


In the chart above, we need to think about why the TRUE_PROCESSING_RATE_AVG value still remained constant after PARALLELISM went from 1 to 2.
The next chart shows the metrics for the source operator over the same time period.

!image-2023-05-01-23-55-08-254.png|width=703,height=396!
The PARALLELISM value went from 1 to 2 around 10:50, when the lag started to build up a bit.
However, there was still no increase in throughput.
Since the throughput of the source operator didn't increase, it makes sense that the throughput downstream (chart above) didn't change either.


I suspect that this issue might be related to the Kafka source connector.
During our testing, we sometimes noticed that even though the PARALLELISM of the source operator increased, only some subtasks were busy and the rest were idle.

This could be an issue with the topic partition not being evenly distributed across consumers (subtasks) when TM restarts as scaling occurs.
If my guess is correct, this could have been judged as ineffective scaling because increasing the parallelism of the source would not result in any change in throughput, and the same goes for the downstream.;;;","01/May/23 16:27;gyfora;Just because the source true processing rate doesn't change (ineffective scaling) doesn't mean that downstream processing cannot change.
Don't forget that true processing rate is the num records per sec adjusted by the busytime. So even if the num records in doesn't change we would expect a lower busytime with increased parallelism.

So I think based on your metrics it is actually ineffective so the logic is ""working"" right?;;;","01/May/23 17:03;tanee.kim;As you say, even if num records doesn't change, if busytime decreases, true processing should increase.
But in the metric, true processing remains the same.
In fact, if you look at the busytime metric, it doesn't decrease in value as you say when parallelism increases.
How can I interpret this?

!image-2023-05-02-02-08-25-920.png|width=603,height=270!;;;","01/May/23 18:41;gyfora;Based on the metrics: 
 - NumRecords in didn't increase
 - BusyTime didn't dicrease

I would interpret this as a truly ineffecting scaling operation :) 

Imagine what would a human operator do. You scale up your job but literally nothing improves. Just as busy as before, you dont process more records. You would probably also don't want to scale further without understanding whats going on.;;;","03/May/23 15:58;gyfora;merged to main 327388ed986a71b7f6a4baee5ea6f25a5c354099;;;","06/May/23 01:59;tanee.kim;[~gyfora] 

I understand the principle of inefficient scaling as explained to me.
So, what can I do to address this issue as the lag is constantly building up?
Increasing parallelism, even inefficiently, will eliminate the lag.
This makes sense in the ""inefficient"" sense, but in the ""operational"" sense, it also means that it's difficult to reliably manage the lag using an autoscaler.

If you decide that scaling is inefficient and stop scaling, but the lag is building up, increasing the source parallelism may cause additional load downstream and no longer be considered ""inefficient"".
Therefore, when judging inefficient scaling, shouldn't we consider not only the throughput of the last scaling history, but also the parallelism of the upstream at that time?

What do you think?;;;","06/May/23 07:35;gyfora;Sorry I don’t understand the suggestion. The increased parallelism either increases the max throughput of an operator or not.

We are trying to detect it if it isn’t , it’s not about the current throughput, it’s about the theoretical maximum (true processing rate)

In your case it sounds like the true processing rate computation may be off in some cases which causes a problem with the mechanism. You can also disable the inefficient scaling check for this application ;;;","06/May/23 08:05;tanee.kim;Turning off inefficient scaling is easy.
What I'm trying to figure out is if there is an improvement that would solve the above problem without turning off inefficient scaling.

By the way, are you saying that true processing rate computation is turned off in some cases, which could cause problems with the mechanism?
In what cases might that be the case?;;;","06/May/23 09:42;gyfora;What I meant by off is that it may be incorrect and under/overestimated ;;;","06/May/23 12:02;tanee.kim;I see, so I guess I'll have to test and decide whether or not to turn on ineffective scaling based on my application.
Thanks for the answer. :);;;",,,,,,,,,,,,,,
default catalog failed to retrieve partition Spec,FLINK-31975,13534526,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,29/Apr/23 09:57,29/Apr/23 09:57,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,,,0,,,,,,"Here is the attached Repro for the error . 
-  Flink 1.16.0 cluster 



 

 
{code:java}
Flink SQL> show current catalog
> ;
+----------------------+
| current catalog name |
+----------------------+
|      default_catalog |
+----------------------+
1 row in set
Flink SQL> show tables;
+-------------------+
|        table name |
+-------------------+
| country_page_view |
|  page_view_source |
|        part_table |
+-------------------+
3 rows in set
Flink SQL> drop table page_view_source;
[INFO] Execute statement succeed.
Flink SQL> drop table country_page_view;
[INFO] Execute statement succeed.
Flink SQL> CREATE TABLE  page_view_source (`user` STRING, `cnt` INT, `date` STRING, `country` STRING)
> WITH (
>   'connector' = 'datagen',  'number-of-rows' = '10'
> );
[INFO] Execute statement succeed.
Flink SQL> CREATE TABLE country_page_view (`user` STRING, `cnt` INT, `date` STRING, `country` STRING)
> PARTITIONED BY (`date`, `country`)
> WITH (
>
>    'format' = 'csv',
>    'path' = 's3://dbsamrat-emr-dev/glue-catalog/dbsamrat/country_page_view/',
>    'connector' = 'filesystem'
> )
> ;
[INFO] Execute statement succeed.
Flink SQL> INSERT INTO country_page_view PARTITION (`date`='2019-8-30', `country`='China')
>   SELECT `user`, `cnt` FROM page_view_source;
>
[INFO] Submitting SQL update statement to the cluster...
2023-04-29 09:51:36,133 INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider [] - Connecting to ResourceManager at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:8032
2023-04-29 09:51:36,134 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:10200
2023-04-29 09:51:36,135 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2023-04-29 09:51:36,135 WARN  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set.The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.
2023-04-29 09:51:36,149 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface ip-172-31-39-51.us-west-2.compute.internal:36583 of application 'application_1682266531513_0004'.
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 7c39db71be1f1b52e13a72831fed8105

Flink SQL> EXECUTE INSERT INTO country_page_view PARTITION (`date`='2019-8-30', `country`='China')
>   SELECT `user`, `cnt` FROM page_view_source;
[INFO] Submitting SQL update statement to the cluster...
2023-04-29 09:51:41,424 INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider [] - Connecting to ResourceManager at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:8032
2023-04-29 09:51:41,424 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:10200
2023-04-29 09:51:41,424 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2023-04-29 09:51:41,424 WARN  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set.The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.
2023-04-29 09:51:41,427 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface ip-172-31-39-51.us-west-2.compute.internal:36583 of application 'application_1682266531513_0004'.
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 69e18cb23f505528948a6398390ad070

Flink SQL> INSERT INTO country_page_view PARTITION (`date`='2019-8-30')
>   SELECT `user`, `cnt`, `country` FROM page_view_source;
[INFO] Submitting SQL update statement to the cluster...
2023-04-29 09:51:47,509 INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider [] - Connecting to ResourceManager at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:8032
2023-04-29 09:51:47,509 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:10200
2023-04-29 09:51:47,509 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2023-04-29 09:51:47,510 WARN  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set.The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.
2023-04-29 09:51:47,512 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface ip-172-31-39-51.us-west-2.compute.internal:36583 of application 'application_1682266531513_0004'.
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: dc82613e0f2f8a2bafc61dcd35486f4e

Flink SQL> INSERT OVERWRITE country_page_view PARTITION (`date`='2019-8-30', `country`='China')
>   SELECT `user`, `cnt` FROM page_view_source;
[INFO] Submitting SQL update statement to the cluster...
2023-04-29 09:51:53,534 INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider [] - Connecting to ResourceManager at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:8032
2023-04-29 09:51:53,534 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:10200
2023-04-29 09:51:53,535 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2023-04-29 09:51:53,535 WARN  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set.The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.
2023-04-29 09:51:53,542 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface ip-172-31-39-51.us-west-2.compute.internal:36583 of application 'application_1682266531513_0004'.
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 117900654da5a89ce517d85383d4fe4a

Flink SQL> INSERT OVERWRITE country_page_view PARTITION (`date`='2019-8-30')
>   SELECT `user`, `cnt`, `country` FROM page_view_source;
[INFO] Submitting SQL update statement to the cluster...
2023-04-29 09:51:58,834 INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider [] - Connecting to ResourceManager at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:8032
2023-04-29 09:51:58,834 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:10200
2023-04-29 09:51:58,834 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2023-04-29 09:51:58,835 WARN  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set.The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.
2023-04-29 09:51:58,838 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface ip-172-31-39-51.us-west-2.compute.internal:36583 of application 'application_1682266531513_0004'.
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: ca63640e867b9309b8c69d4dba7d94b1

Flink SQL> INSERT INTO country_page_view PARTITION (`date`='2019-8-30', `country`='China') (`user`)
>   SELECT user FROM page_view_source;
[INFO] Submitting SQL update statement to the cluster...
2023-04-29 09:52:04,467 INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider [] - Connecting to ResourceManager at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:8032
2023-04-29 09:52:04,469 INFO  org.apache.hadoop.yarn.client.AHSProxy                       [] - Connecting to Application History server at ip-172-31-38-72.us-west-2.compute.internal/172.31.38.72:10200
2023-04-29 09:52:04,470 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2023-04-29 09:52:04,470 WARN  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set.The Flink YARN Client needs one of these to be set to properly load the Hadoop configuration for accessing YARN.
2023-04-29 09:52:04,474 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface ip-172-31-39-51.us-west-2.compute.internal:36583 of application 'application_1682266531513_0004'.
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 8bca09468a1193f47500ab3eadf04375
{code}
 

Finally while selecting rows from the table , it throws the following error 
{code:java}
Flink SQL> select * from country_page_view;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Partition keys are: [date, country], incomplete partition spec: {}
Flink SQL>
{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-29 09:57:08.0,,,,,,,,,,"0|z1hm08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager crashes after KubernetesClientException exception with FatalExitExceptionHandler,FLINK-31974,13534509,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,sergiosp,sergiosp,28/Apr/23 22:18,01/Jun/23 18:12,04/Jun/24 20:41,01/Jun/23 18:12,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,,,,Deployment / Kubernetes,,,,,,0,pull-request-available,,,,,"When resource quota limit is reached JobManager will throw

 

org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.

 

In {*}1.16.1 , this is handled gracefully{*}:
{code}
2023-04-28 22:07:24,631 WARN  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Failed requesting worker with resource spec WorkerResourceSpec \{cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=4}, current pending count: 0
java.util.concurrent.CompletionException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-138"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
        at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-138"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.16.1.jar:1.16.1]
        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.16.1.jar:1.16.1]
        ... 4 more
{code}

But , {*}in Flink 1.17.0 , Job Manager crashes{*}:
{code}
2023-04-28 20:50:50,534 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
        at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.17.0.jar:1.17.0]
        ... 4 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30908,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 01 13:21:59 UTC 2023,,,,,,,,,,"0|z1hlwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/23 15:04;mapohl;Thanks for reporting. This should be caused by the changes that were introduced with FLINK-30908. Could you share the logs that are printed before the fatal error is reported? There should be additional logs related to the error.;;;","03/May/23 15:17;mapohl;I'm still wondering what the desired behavior in that case is. The k8s cluster doesn't provide the resources so that the Flink cluster would be able to handle the parallelism of the submitted job. In my opinion, it feels like the fatal error is correct. [~xtsong] what's your take on that one?;;;","04/May/23 02:03;xtsong;Thanks [~sergiosp] for reporting, and thanks [~mapohl] for looking into this.

bq. The k8s cluster doesn't provide the resources so that the Flink cluster would be able to handle the parallelism of the submitted job.

This is not always true. For streaming workloads in reactive mode, it is expected that not all requested resources can be obtained, and as long as the minimum resource requirements are fulfilled the job can be executed. Also for batch workloads, ideally a job can be executed with a single slot, because tasks don't have to be executed at the same time.

Moreover, there's a timeout at the JobMaster side that will fail the job if resources cannot be fulfilled within a certain time, with the execution mode and minimum resource requirements taken into consideration.

In most cases, the phenomenon for not obtaining a resource is that Flink can create the meta of desired pod at K8s API Server and will keep waiting for the K8s cluster to schedule and bring up the pod. However, in this case it throws an exception, which was not covered by the current implementation.

I think we may identify the specific error and not treat it as fatal error. Instead, we can pass this information to JobMaster via {{JobMasterGateway#notifyNotEnoughResourcesAvailable}} and rely on JobMaster to decide whether should fail the job.

WDYT?;;;","04/May/23 04:30;Weijie Guo;Thanks Xintong for the analysis and proposal, It makes sense to me for relying JobMaster to decide whether to fail the job or not. IMO, The exception mentioned in the ticket should not arbitrarily cause JM to crash, especially for batch workload. If this is reasonable, I'm willing to fix it.;;;","04/May/23 07:06;mapohl;Sounds good to me, too.

Just for me to understand: With ""we can pass this information to JobMaster"" you mean letting the SlotManager implementations deal with the timeout you mentioned that occurs when we fail to create a new worker. That way, we only need to identify the {{KubernetesClientException}} in {{ResourceManagerDriver#requestResource}} and print a warning to make the user aware. I'm asking because I struggled to find a code path between the JobMaster and the ResourceManager that would enable us to inform the JobMaster about this specific error which is how I understood ""passing the information"" in the first place.;;;","04/May/23 07:30;xtsong;[~mapohl],
There're two paths for JobMaster to handle the situation that resources are not obtained.
- With the timeout
- {{JobMasterGateway#notifyNotEnoughResourcesAvailable}}

The second path is for the job to fail earlier rather than waiting for the timeout, if SlotManager knows that the resource cannot be obtained and it makes no sense to wait, e.g., in a standalone cluster. I think the question is, after we identify the specific error that suggest a quota exceeding, how do we pass this information all the way from {{KubernetesResourceManagerDriver}} to {{ResourceManager}} and to {{SlotManager}}. I think it shouldn't be complex to complete the missing part of the path.;;;","04/May/23 08:02;gyfora;Somewhat of a side comment:
I think in native kubernetes integration case we should basically never give these fatal exceptions. Even if there is a missing serviceaccount/permission/timeout we should keep retrying because more often than not these are actually temporary (even if they need some time to be resolved by the operating platform team).

A job fatal error requires a complete redeployment which is not what most users want. For standalone this may be different but there we will get different errors.;;;","04/May/23 08:35;xtsong;Not sure about never giving fatal exceptions. I personally would lean towards a whitelist approach, where Flink only handles a certain set of errors that are known to be non-fatal, and by default fail for whatever errors that it doesn't recognize and doesn't know how to handle. My concern for keeping retrying by default is that, when there's a large Kubernetes cluster with lot's of applications, this approach would exacerbate the burden on the Kubernetes API server and sometimes make the temporary outage even harder to recovery. I've seen that for many times in production.;;;","04/May/23 08:47;gyfora;[~xtsong] what errors would you consider actually fatal in Kubernetes world?

From my perspective I would like to treat almost every kubernetes error non fatal. At least this should be configurable because as you say some may prefer shuttind down the flink jobs (fatal) and some (we for instance) would like to retry everything based on the restart strategy.;;;","04/May/23 09:22;xtsong;[~gyfora],

IMO, errors that Flink cannot recover from by itself should be considered fatal. E.g., for a permission issue, if not provided the details that it's due to reaching the quota limit, I don't see how Flink can fix that by itself. I would be fine with Flink trying to improve how it handles various errors based on understanding of what the errors mean. However, I'd be hesitate about to simply retry for arbitrary errors.

bq. because more often than not these are actually temporary

TBH, my observations are to the contrary. Might because of differences between our production environments.

bq. At least this should be configurable

Normally, I'd avoid introducing new configuration unless absolutely necessary. In this case, if you believe it worths the complexity not to trigger a re-deployment upon arbitrary errors, I'd be fine with making it configurable.

I'm still trying to understand why re-deployment upon API server outage is a big deal. Is it because the outage happens a lot in your production environment?

bq. retry everything based on the restart strategy

I believe restart strategy only controls behaviors upon job failures. An error thrown from the interactions between the resource manager and the kubernetes cluster would not invoke the restart strategy. Unless you mean waiting for the resource allocation timeout.;;;","04/May/23 10:06;gyfora;Flink treats only very few errors fatal. IO errors, connector (source/sink ) errors etc all cause job restarts and in many cases ""Flink cannot recover from by itself"". You actually expect the error to be temporary and hopefully not get it after the restart. So I think it would be generally inconsistent with the current error handling behaviour if resource manager errors would simply let the job die fatally and not retry in the same way.

So I am mostly looking at this from the user perspective. Flink jobs/clusters should be resilient and keep retrying in case of errors and should not give up especially for streaming workloads. This is how it works now and this what most users expect I think.;;;","04/May/23 10:08;gyfora;cc [~mbalassi] [~mxm] [~thw] ;;;","04/May/23 11:17;mbalassi;In the specific case I much prefer the behaviour exhibited by 1.16.1. Resource quota not being available changes dynamically, if the JobManager kept retrying (ideally with a backoff) it is not unreasonable to expect that eventually it could succeed in most real-world scenarios. Adding some guardrails around this (if a minimum parallelism is not satisfied fail instead, if a max timeout is reached fail etc) to avoid ending up with many small jobs competing for insufficient resources and wasting capacity would be acceptable to me, but outright failing on the first try is more a bug than a feature imho. :);;;","04/May/23 13:43;xtsong;[~gyfora],

bq. Flink treats only very few errors fatal. IO errors, connector (source/sink ) errors etc all cause job restarts and in many cases ""Flink cannot recover from by itself"". You actually expect the error to be temporary and hopefully not get it after the restart. So I think it would be generally inconsistent with the current error handling behaviour if resource manager errors would simply let the job die fatally and not retry in the same way.

I think the difference here is that, for IO errors and connector errors, it affects the job but not the Flink cluster / deployment. Thinking of a session cluster, we should not fail the cluster for an error from a single job. But for resource manager interacting with Kubernetes API server, this is a cluster behavior and conceptually we don't distinguish resources for individual jobs until the slots are allocated. Moreover, it's possible that multiple jobs share the same resource (pod). One could argue that in application mode the cluster / deployment is equivalent to the job. However, the cluster mode (session / application) is transparent to the resource manager.

 bq. Flink jobs/clusters should be resilient and keep retrying in case of errors and should not give up especially for streaming workloads.

This is different from the feedback that I get from our production. But I can understand if that's what some of the users want. So I guess maybe it worth a configuration option as you suggested.

[~mbalassi],

+1 to what you said about the specific case. I think there's a consensus on reaching quota limit should not be treated as fatal errors.;;;","04/May/23 14:24;thw;There are many cases where errors are transient. This specific case is actually quite obvious, the resource availability on a large cluster is changing constantly. A pod may not be scheduled now but few seconds later. Other k8s related issues can also be transient, for example a failed request due to rate limiting will likely succeed soon after and we would actually make things worse by not following a backoff/retry strategy and simply letting the job fail. I'm also leaning more towards retry by default strategy and identify the cases that should be fatal error.;;;","04/May/23 20:48;sergiosp;Hi [~mapohl] - let me setup a new cluster later on to get the full logs. Meanwhile, please find the thread dump from the Flink 1.17.0 crash:

 
{code:java}
2023-04-28 20:50:50,305 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 0a97c80a173b7ebb619c5b030b607520: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
...
2023-04-28 20:50:50,534 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/env-my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
        at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/env-my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.17.0.jar:1.17.0]
        ... 4 more
2023-04-28 20:50:50,602 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - Thread dump: 
""main"" prio=5 Id=1 WAITING on java.util.concurrent.CompletableFuture$Signaller@2897b146
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.CompletableFuture$Signaller@2897b146
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.CompletableFuture$Signaller.block(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.managedBlock(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.CompletableFuture.waitingGet(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.CompletableFuture.get(Unknown Source)
        at app//org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:741)
        at app//org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint.main(KubernetesSessionClusterEntrypoint.java:61)""Reference Handler"" daemon prio=10 Id=2 RUNNABLE
        at java.base@11.0.19/java.lang.ref.Reference.waitForReferencePendingList(Native Method)
        at java.base@11.0.19/java.lang.ref.Reference.processPendingReferences(Unknown Source)
        at java.base@11.0.19/java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)""Finalizer"" daemon prio=8 Id=3 WAITING on java.lang.ref.ReferenceQueue$Lock@2b21a3e4
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on java.lang.ref.ReferenceQueue$Lock@2b21a3e4
        at java.base@11.0.19/java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.base@11.0.19/java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.base@11.0.19/java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)""Signal Dispatcher"" daemon prio=9 Id=4 RUNNABLE""Common-Cleaner"" daemon prio=8 Id=9 TIMED_WAITING on java.lang.ref.ReferenceQueue$Lock@16aab001
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on java.lang.ref.ReferenceQueue$Lock@16aab001
        at java.base@11.0.19/java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.base@11.0.19/jdk.internal.ref.CleanerImpl.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)
        at java.base@11.0.19/jdk.internal.misc.InnocuousThread.run(Unknown Source)""Log4j2-TF-3-Scheduled-1"" daemon prio=5 Id=12 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2ee9ffbd
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2ee9ffbd
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""Log4j2-TF-6-Scheduled-2"" daemon prio=5 Id=18 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3b6166a7
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3b6166a7
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-scheduler-1"" prio=5 Id=19 TIMED_WAITING
        at java.base@11.0.19/java.lang.Thread.sleep(Native Method)
        at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
        at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
        at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-akka.actor.internal-dispatcher-2"" prio=5 Id=20 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@1d79ccac
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@1d79ccac
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-akka.actor.default-dispatcher-4"" prio=5 Id=22 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-akka.remote.default-remote-dispatcher-5"" prio=5 Id=23 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@3fe545c2
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@3fe545c2
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-akka.remote.default-remote-dispatcher-6"" prio=5 Id=24 TIMED_WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@3fe545c2
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@3fe545c2
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkUntil(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""New I/O worker #1"" prio=5 Id=25 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@f0148c7
        -  locked sun.nio.ch.EPollSelectorImpl@5c4ee927
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@708d421""New I/O boss #2"" prio=5 Id=27 RUNNABLE
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@6f9289
        -  locked sun.nio.ch.EPollSelectorImpl@15b10c13
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@409550d1""New I/O worker #3"" prio=5 Id=28 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@10733912
        -  locked sun.nio.ch.EPollSelectorImpl@3bdffdea
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@dbc95df""New I/O server boss #4"" prio=5 Id=29 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@4758dbc2
        -  locked sun.nio.ch.EPollSelectorImpl@5a1496ae
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@85887aa""Timer-0"" daemon prio=5 Id=32 TIMED_WAITING on java.util.TaskQueue@53a69f57
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on java.util.TaskQueue@53a69f57
        at java.base@11.0.19/java.util.TimerThread.mainLoop(Unknown Source)
        at java.base@11.0.19/java.util.TimerThread.run(Unknown Source)""BLOB Server listener at 6124"" daemon prio=5 Id=31 RUNNABLE (in native)
        at java.base@11.0.19/java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.base@11.0.19/java.net.AbstractPlainSocketImpl.accept(Unknown Source)
        at java.base@11.0.19/java.net.ServerSocket.implAccept(Unknown Source)
        at java.base@11.0.19/java.net.ServerSocket.accept(Unknown Source)
        at app//org.apache.flink.util.NetUtils.acceptWithoutTimeout(NetUtils.java:143)
        at app//org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:316)""flink-metrics-scheduler-1"" prio=5 Id=34 TIMED_WAITING
        at java.base@11.0.19/java.lang.Thread.sleep(Native Method)
        at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
        at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
        at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-metrics-akka.actor.internal-dispatcher-3"" prio=5 Id=36 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@17521864
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@17521864
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-metrics-akka.remote.default-remote-dispatcher-5"" prio=5 Id=38 TIMED_WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@16555787
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@16555787
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkUntil(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-metrics-akka.remote.default-remote-dispatcher-6"" prio=5 Id=39 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@16555787
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@16555787
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""New I/O worker #5"" prio=5 Id=40 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@688a0205
        -  locked sun.nio.ch.EPollSelectorImpl@f316464
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@7d652799""New I/O boss #6"" prio=5 Id=42 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@827ccde
        -  locked sun.nio.ch.EPollSelectorImpl@34f25b32
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@73a2c645""New I/O worker #7"" prio=5 Id=43 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@cd0fd83
        -  locked sun.nio.ch.EPollSelectorImpl@846aa39
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@779d4ce9""New I/O server boss #8"" prio=5 Id=44 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@3eb1c6d3
        -  locked sun.nio.ch.EPollSelectorImpl@18bc21fc
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@329fcc7e""flink-rest-server-netty-boss-thread-1"" daemon prio=5 Id=48 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet@130bfb8d
        -  locked sun.nio.ch.EPollSelectorImpl@2ea19cb7
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:879)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:526)
        at app//org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        ...""Flink-DispatcherRestEndpoint-thread-1"" daemon prio=5 Id=49 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""cluster-io-thread-1"" daemon prio=5 Id=50 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""pool-2-thread-1"" prio=5 Id=51 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2bc769e4
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2bc769e4
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""cluster-io-thread-2"" daemon prio=5 Id=52 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-akka.actor.default-dispatcher-13"" prio=5 Id=55 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-akka.actor.default-dispatcher-14"" prio=5 Id=56 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""resourcemanager_1-main-scheduler-thread-1"" daemon prio=5 Id=73 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@9a11c0f
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@9a11c0f
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-kubeclient-io-for-resourcemanager-thread-1"" daemon prio=5 Id=74 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-akka.actor.default-dispatcher-15"" prio=5 Id=75 RUNNABLE
        at java.management@11.0.19/sun.management.ThreadImpl.dumpThreads0(Native Method)
        at java.management@11.0.19/sun.management.ThreadImpl.dumpAllThreads(Unknown Source)
        at java.management@11.0.19/sun.management.ThreadImpl.dumpAllThreads(Unknown Source)
        at app//org.apache.flink.util.concurrent.ThreadUtils.errorLogThreadDump(ThreadUtils.java:33)
        at app//org.apache.flink.util.FatalExitExceptionHandler.uncaughtException(FatalExitExceptionHandler.java:47)
        at app//org.apache.flink.util.concurrent.FutureUtils.lambda$handleUncaughtException$20(FutureUtils.java:1216)
        at app//org.apache.flink.util.concurrent.FutureUtils$$Lambda$623/0x00000001006a9840.accept(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)
        ...""OkHttp https://10.96.0.1/..."" prio=5 Id=76 RUNNABLE (in native)
        at java.base@11.0.19/java.net.SocketInputStream.socketRead0(Native Method)
        at java.base@11.0.19/java.net.SocketInputStream.socketRead(Unknown Source)
        at java.base@11.0.19/java.net.SocketInputStream.read(Unknown Source)
        at java.base@11.0.19/java.net.SocketInputStream.read(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.read(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.readHeader(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketImpl.readApplicationRecord(Unknown Source)
        ...        Number of locked synchronizers = 2
        - java.util.concurrent.locks.ReentrantLock$NonfairSync@234e456e
        - java.util.concurrent.ThreadPoolExecutor$Worker@330607e6""OkHttp ConnectionPool"" daemon prio=5 Id=77 TIMED_WAITING on org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool@335680aa
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool@335680aa
        at java.base@11.0.19/java.lang.Object.wait(Unknown Source)
        at app//org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool.lambda$new$0(RealConnectionPool.java:62)
        at app//org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool$$Lambda$709/0x000000010071d040.run(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@457b7a36""Okio Watchdog"" daemon prio=5 Id=78 TIMED_WAITING on java.lang.Class@711498ff
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on java.lang.Class@711498ff
        at app//org.apache.flink.kubernetes.shaded.okio.AsyncTimeout.awaitTimeout(AsyncTimeout.java:348)
        at app//org.apache.flink.kubernetes.shaded.okio.AsyncTimeout$Watchdog.run(AsyncTimeout.java:313)""flink-kubeclient-io-for-resourcemanager-thread-2"" daemon prio=5 Id=80 RUNNABLE
        at java.base@11.0.19/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base@11.0.19/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at java.base@11.0.19/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.base@11.0.19/java.lang.reflect.Method.invoke(Unknown Source)
        at app//org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:689)
        at app//org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:774)
        at app//org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178)
        at app//org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:119)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@5fdf1d80""cluster-io-thread-3"" daemon prio=5 Id=84 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""Flink-DispatcherRestEndpoint-thread-2"" daemon prio=5 Id=85 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-rest-server-netty-worker-thread-1"" daemon prio=5 Id=86 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet@3296b3ab
        -  locked sun.nio.ch.EPollSelectorImpl@72ad4a98
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:879)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:526)
        at app//org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        ...""Flink-DispatcherRestEndpoint-thread-3"" daemon prio=5 Id=88 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""Flink-DispatcherRestEndpoint-thread-4"" daemon prio=5 Id=89 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""OkHttp WebSocket https://10.96.0.1/..."" prio=5 Id=98 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@23991496
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@23991496
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""jobmanager-io-thread-1"" daemon prio=5 Id=99 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@24cd939f
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@24cd939f
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-akka.actor.supervisor-dispatcher-16"" prio=5 Id=100 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@23417746
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@23417746
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.poll(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-metrics-13"" prio=1 Id=102 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3ba0103c
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3ba0103c
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.poll(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""cluster-io-thread-4"" daemon prio=5 Id=108 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""jobmanager-future-thread-1"" daemon prio=5 Id=109 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@50769e7a
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@50769e7a
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-kubeclient-io-for-resourcemanager-thread-3"" daemon prio=5 Id=112 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""OkHttp 10.96.0.1 Writer"" prio=5 Id=113 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@186fae80
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@186fae80
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""OkHttp 10.96.0.1"" daemon prio=5 Id=114 RUNNABLE (in native)
        at java.base@11.0.19/java.net.SocketInputStream.socketRead0(Native Method)
        at java.base@11.0.19/java.net.SocketInputStream.socketRead(Unknown Source)
        at java.base@11.0.19/java.net.SocketInputStream.read(Unknown Source)
        at java.base@11.0.19/java.net.SocketInputStream.read(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.read(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.readHeader(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketImpl.readApplicationRecord(Unknown Source)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.locks.ReentrantLock$NonfairSync@303c118c""OkHttp Http2Connection"" daemon prio=5 Id=115 TIMED_WAITING on java.util.concurrent.SynchronousQueue$TransferStack@ba6d647
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.SynchronousQueue$TransferStack@ba6d647
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.SynchronousQueue$TransferStack.transfer(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.SynchronousQueue.poll(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""FlinkCompletableFutureDelayScheduler-thread-1"" daemon prio=5 Id=117 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6fc71c07
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6fc71c07
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-kubeclient-io-for-resourcemanager-thread-4"" daemon prio=5 Id=118 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""AkkaRpcService-Supervisor-Termination-Future-Executor-thread-1"" daemon prio=5 Id=119 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@ab9b4d1
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@ab9b4d1
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""jobmanager_3-main-scheduler-thread-1"" daemon prio=5 Id=121 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@5a59647b
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@5a59647b
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""SourceCoordinator-Source: *anonymous_kafka$15*[66]"" prio=5 Id=122 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67708599
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67708599
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""Checkpoint Timer"" daemon prio=5 Id=123 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30809a54
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30809a54
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""kafka-admin-client-thread | KafkaSource--5297459524620964018-enumerator-admin-client"" daemon prio=5 Id=125 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@77d8bfa9
        -  locked sun.nio.ch.EPollSelectorImpl@66b37795
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at app//org.apache.kafka.common.network.Selector.select(Selector.java:873)
        at app//org.apache.kafka.common.network.Selector.poll(Selector.java:465)
        at app//org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
        at app//org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1408)
        ...""SourceCoordinator-Source: *anonymous_kafka$15*[66]-worker-thread-1"" daemon prio=5 Id=127 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@58c6a97
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@58c6a97
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...
2023-04-28 20:50:50,607 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesSessionClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2023-04-28 20:50:50,609 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
[pod@my-namespace-flink-cluster-59cd575d6b-bll6b log]$ {code}
 

 ;;;","05/May/23 01:25;xtsong;cc [~wangyang0918];;;","05/May/23 08:00;mapohl;[~sergiosp] I guess it's not necessary to provide the logs anymore. The problem is understood and the discussion went on, already.

On the discussion about how to handle errors in this part of the code: tbh, initially I leaned towards [~xtsong]'s proposal where he suggested to make the error handling as strict as possible through a whitelist and avoid adding yet another configuration parameter with the idea in mind that Flink's deployment environment should be in a healthy state without any mis-configuration. But as the discussion moved on, I started to acknowledge that it's too strict in quite a few scenarios. I also get [~gyfora]'s point that we're not that restrictive in other places of the code base, either.

One concern I have with the error whitelisting, though, is that the error classification could become ""complex"". The error [~sergiosp] shared was about hitting quota limits. The error type we're seeing is a Forbidden error (unfortunately, without the error code being logged but I would assume 403 analogously to the HTTP error code). I could imagine this error type also being returned in other cases (e.g. wrong service account being used). The former error is something we want to retry in certain scenarios but the latter one (based on my understanding) would be one that could be considered a general infrastructure issue and, as a consequence, could be treated as a fatal error. It looks like it would require error message parsing to identify the type of error. How confident are we about the stability of those error messages? It looks like they are derived from the k8s HTTP responses and, therefore, might be stable among different Kubernetes versions. But generally, relying on error messages for deriving Flink's behavior feels not right. Is this a valid concern? In this sense, I started to favor what was proposed by [~gyfora] in the discussion.

I might be wrong here because I'm not that familiar with the k8s API. I wanted to share this, anyway.;;;","05/May/23 08:44;xtsong;Thanks all for the explanation and patience. It seems there's a commonly tendency towards the retry-by-default approach. 

I also consulted a few colleagues from our Kubernetes team about this. They also share the opinion that there might be more error types that can be resolved by retrying than a whitelist could possibly handle. The only concern they mentioned is that keeping retrying may make the Kubernetes API Server harder to recover from outages in some specific cases, which I believe can be addressed with backoff and guardrails as [~mbalassi] mentioned.

I'd respect the opinion of the majority, withdraw my proposal, and +1 for [~gyfora]'s proposal.;;;","30/May/23 06:55;gyfora;[~Weijie Guo] are you working on this ticket?;;;","30/May/23 08:00;Weijie Guo;[~gyfora] Sorry, I am quite busy recently, feel free to re-assign this ticket if you want to pick up it. :);;;","30/May/23 12:07;gyfora;No worries, I will assign it to myself and will work on this shortly.;;;","01/Jun/23 13:21;Weijie Guo;master(1.18) via 3b9f7cf8ffcd357f252f62dee62d26dbc6a76e91.
release-1.17 via 07f43a5c68301ce119352d40cbec46b2c52a79a2.;;;",,,,
Introduce Managed table support in glue catalog,FLINK-31973,13534494,13485272,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,28/Apr/23 17:00,28/Apr/23 17:00,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,Hive-catalog support managed table. Similarly glue catalog can support managed table backed by glue metasore. This task focuses on introducing managed table support in glue catalog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-28 17:00:43.0,,,,,,,,,,"0|z1hlt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove powermock whitebox usages,FLINK-31972,13534491,13534279,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Apr/23 16:47,22/May/23 08:10,04/Jun/24 20:41,22/May/23 08:10,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 22 08:10:00 UTC 2023,,,,,,,,,,"0|z1hlsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 08:10;chesnay;master: 10f38e85d8d3ebda54e70b6354aec3e5b9806188;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Drop HadoopRecoverableWriterOldHadoopWithNoTruncateSupportTest,FLINK-31971,13534480,13534279,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Apr/23 15:01,18/May/23 13:34,04/Jun/24 20:41,18/May/23 13:34,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Connectors / Hadoop Compatibility,Tests,,,,,0,pull-request-available,,,,,"This test explicitly checks behavior for Hadoop < 2.7, which we no longer support.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 18 13:33:48 UTC 2023,,,,,,,,,,"0|z1hlq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/23 13:33;dmvk;master: 777ae5cb2aaafce9c534565412d6e350846aadc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"""Key group 0 is not in KeyGroupRange"" when using CheckpointedFunction",FLINK-31970,13534467,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,YordanPavlov,YordanPavlov,28/Apr/23 13:39,23/Aug/23 11:31,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,,,"I am experiencing a problem where the following exception would be thrown on Flink stop (stop with savepoint):

 
{code:java}
org.apache.flink.util.SerializedThrowable: java.lang.IllegalArgumentException: Key group 0 is not in KeyGroupRange{startKeyGroup=86, endKeyGroup=127}.{code}
 

I do not have a non deterministic keyBy() operator in fact, I use 
{code:java}
.keyBy(_ => 1){code}
I believe the problem is related to using RocksDB state along with a 
{code:java}
CheckpointedFunction{code}
In my test program I have commented out a reduction of the parallelism which would make the problem go away. I am attaching a standalone program which presents the problem and also a script which generates the input data. For clarity I would paste here the essence of the job:

 

 
{code:scala}
env.fromSource(kafkaSource, watermarkStrategy, ""KafkaSource"")
.setParallelism(3)
.keyBy(_ => 1)
.window(TumblingEventTimeWindows.of(Time.of(1, TimeUnit.MILLISECONDS)))
.apply(new TestWindow())
/* .setParallelism(1) this would prevent the problem */
.uid(""window tester"")
.name(""window tester"")
.print()

class TestWindow() extends WindowFunction[(Long, Int), Long, Int, TimeWindow] with CheckpointedFunction {
  var state: ValueState[Long] = _
  var count = 0

  override def snapshotState(functionSnapshotContext: FunctionSnapshotContext): Unit = {
    state.update(count)
  }

  override def initializeState(context: FunctionInitializationContext): Unit = {
    val storeDescriptor = new ValueStateDescriptor[Long](""state-xrp-dex-pricer"", createTypeInformation[Long])

    state = context.getKeyedStateStore.getState(storeDescriptor)
  }

  override def apply(key: Int, window: TimeWindow, input: Iterable[(Long, Int)], out: Collector[Long]): Unit = {
    count += input.size
    out.collect(count)
  }
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/23 13:34;YordanPavlov;fill-topic.sh;https://issues.apache.org/jira/secure/attachment/13057709/fill-topic.sh","28/Apr/23 13:33;YordanPavlov;main.scala;https://issues.apache.org/jira/secure/attachment/13057710/main.scala",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 23 11:31:06 UTC 2023,,,,,,,,,,"0|z1hln4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 13:59;martijnvisser;[~pnowojski] WDYT?;;;","04/May/23 13:51;pnowojski;[~YordanPavlov] I was not able to reproduce your error, however you are incorrectly working with the state in your {{TestWindow}} function. Please take a look at the [documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/#using-keyed-state]. Especially the following paragraph
{quote}
It is important to keep in mind that these state objects are only used for interfacing with state. The state is not necessarily stored inside but might reside on disk or somewhere else. The second thing to keep in mind is that the value you get from the state depends on the key of the input element. So the value you get in one invocation of your user function can differ from the value in another invocation if the keys involved are different.
{quote}
First problem is that your {{TestWindow#snapshoState}} method is trying to access {{state}} field, in a keyed operator/function, inside a method that doesn't have key context. That's the error I was getting when I tried to run your code. 
Secondly, in your {{TestWindow#apply}} method is invoked within some key context, but you are storing the actual {{count}} as a regular java field. So if you ever had more then one key (that's not the case in your example, as you are using {{.keyBy(_ => 1)}} key selector), all of the keys processed by a single parallel instance of the window operator, would be collected on the same {{count}} field, which doesn't make any sense.

The corrected window function should look like this:

{code:scala}
class CorrectTestWindow() extends WindowFunction[(Long, Int), Long, Int, TimeWindow] with CheckpointedFunction {
  var state: ValueState[Long] = _

  override def snapshotState(functionSnapshotContext: FunctionSnapshotContext): Unit = {
  }

  override def initializeState(context: FunctionInitializationContext): Unit = {
    val storeDescriptor = new ValueStateDescriptor[Long](""state-xrp-dex-pricer"", createTypeInformation[Long])
    state = context.getKeyedStateStore.getState(storeDescriptor)
  }

  override def apply(key: Int, window: TimeWindow, input: lang.Iterable[(Long, Int)], out: Collector[Long]): Unit = {
    val count = state.value() + input.size
    state.update(count)
    out.collect(count)
  }
}
{code}
However even that is kind of strange. You are creating a tumbling window every 1ms, only to aggregate the {{count}} across all past tumbling windows for the given key? If you want to aggregate count only across the given tumbling window, your apply could look like as simple as:
{code:scala}
  override def apply(key: Int, window: TimeWindow, input: lang.Iterable[(Long, Int)], out: Collector[Long]): Unit = {
    out.collect(input.size)
  }
{code}
And you don't need any additional state for that. On the other hand, if you want to aggregate results globally, you can use
{code:scala}
        .keyBy(_ => 1)
        .window(GlobalWindows.create())
        .tigger(/* maybe a custom trigger here */)
        .apply(new TestWindow())
{code};;;","16/May/23 16:47;YordanPavlov;First and foremost thank you for your input [~pnowojski] 

??First problem is that your {{TestWindow#snapshoState}} method is trying to access {{state}} field, in a keyed operator/function, inside a method that doesn't have key context.??

Looking at the comments in the [source code|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/checkpoint/CheckpointedFunction.java], I was left with the impression that CheckpointedFunction can be used with a keyed state:
{quote}While more lightweight interfaces exist as
 * shortcuts for various types of state, this interface offer the greatest flexibility in managing
 * both <i>keyed state</i> and <i>operator state</i>.{quote}
 

??First problem is that your TestWindow#snapshoState method is trying to access state field, in a keyed operator/function, inside a method that doesn't have key context. That's the error I was getting when I tried to run your code.??

This error seems helpful, unfortunately I don't seem to get it. I have a simple build.sbt which builds a jar file, which I then run on a locally running cluster. Let me know if you run it differently.

??However even that is kind of strange. You are creating a tumbling window every 1ms, only to aggregate the count across all past tumbling windows for the given key???

Obviously this is a rather simplified example of a more complex problem I try to solve. I want to construct a very huge state, which is updated as result of tumbling window data. However, I want to access the RocksDB backend as rarely as possible, only when checkpoint is being constructed. Between two checkpoints the state would change drastically, so I do not want to preserve intermediate changes, If I use a {{ValueState}} it can potentially spill to disk on each {{.update}} call in-between checkpoints. Do you see better approach to achieve this?

Again thank you for your time.

;;;","17/May/23 08:43;pnowojski;Yes, {{CheckpointedFunction}} can be used with a keyed state. Just keep in mind that with keyed state, any Flink's state field is a kind of proxy/wrapper, that for different keys is referencing a different value. For example if you have integer elements and {{keyBy(x -> x % 2)}}:

{code:java}
public YourFunction ... { 
  ValueState x = ...;
  (...)
}
{code}
then between calls
{code:java}
yourFunction.processElement(1);
yourFunction.processElement(2);
{code}
{{x}} will be referencing different objects. While between calls:
{code:java}
yourFunction.processElement(1);
yourFunction.processElement(3);
{code}
{{x}} will be referencing the same object.

Now the issue is that for {{CheckpointedFunction#snapshotState}} or {{CheckpointedFunction#initializeState}} calls, we are not processing any element, so we don't have any key, so who knows where {{x}} will be pointing to. Maybe to the last processed key? I don't know.

I was testing your code from IDE after creating an [integration test|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/testing/#junit-rule-miniclusterwithclientresource]

{quote}
Obviously this is a rather simplified example of a more complex problem I try to solve. I want to construct a very huge state, which is updated as result of tumbling window data. However, I want to access the RocksDB backend as rarely as possible, only when checkpoint is being constructed. Between two checkpoints the state would change drastically, so I do not want to preserve intermediate changes, If I use a ValueState it can potentially spill to disk on each .update call in-between checkpoints. Do you see better approach to achieve this?
{quote}
I see. However you have to store the intermediate changes somewhere. If they fit into memory, and your primary goal is to not touch disks while processing elements, then I would suggest to use {{HashMapStateBackend}}. In that case your state would be flushed to disks only on checkpoints. If your intermediate changes might not fit into memory, then you have no choice, you have to use {{RocksDB}} all the time.
;;;","23/Aug/23 02:42;masteryhx;Hi, [~YordanPavlov] 

Just kindly ping. 

I think [~pnowojski] 's analysis is right, does this has been resolved after updateing your code ?;;;","23/Aug/23 11:31;YordanPavlov;Hi, [~masteryhx] 
I do not experience the problem as I do the workaround I described in my original post:


.keyBy(_ => 1)
.window(TumblingEventTimeWindows.of(Time.of(1, TimeUnit.MILLISECONDS)))
.apply(new TestWindow())/* .setParallelism(1) this would prevent the problem */
the .setParallelism(1) would prevent the problem from happening. As for the other suggestion, using `HashMapStateBackend` is not an option for me as this would modify the state backend for the whole Flink job (right?) and I have multiple different states in it.

In a summary I think I have found a good solution for my particular use case (that is .keyBy(1)) and I am able to touch RocksDB only on checkpoint trigger time. However I think the usage of CheckpointedFunction in a Keyed environment is very confusing. 

Regards;;;",,,,,,,,,,,,,,,,,,,,,
Remove all local recovery files immediately when local recovery fail,FLINK-31969,13534439,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Yanfei Lei,Yanfei Lei,28/Apr/23 10:43,28/Apr/23 10:43,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,,1,,,,,,"In BackendRestorerProcedure, when local recovery fail, state backend will try to recover from the remote state, at this time all local recovery files are useless, they needs to wait until the next checkpoint to be cleared, deleting local files immediately can better avoid file leakage, especially when the local space is insufficient, deleting useless local recovery files can free up space to download the remote state file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-28 10:43:09.0,,,,,,,,,,"0|z1hlgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[PyFlink] 1.17.0 version on M1 processor,FLINK-31968,13534420,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,gradysnik,gradysnik,28/Apr/23 08:23,05/May/23 02:27,04/Jun/24 20:41,05/May/23 02:26,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"PyFlink version 1.17.0
NumPy version 1.21.4 and 1.21.6
Python versions 3.8, 3.9, 3.9

Slack thread: [https://apache-flink.slack.com/archives/C03G7LJTS2G/p1682508650487979] 

While running any of [PyFlink Examples|https://github.com/apache/flink/blob/release-1.17/flink-python/pyflink/examples] , getting error:
{code:java}
/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/bin/python /Users/ysmirnov/Documents/Repos/flink-python-jobs/word_count/word_count.py 
Traceback (most recent call last):
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/word_count/word_count.py"", line 76, in <module>
    basic_operations()
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/word_count/word_count.py"", line 53, in basic_operations
    show(ds.map(update_tel), env)
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/word_count/word_count.py"", line 28, in show
    env.execute()
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/pyflink/datastream/stream_execution_environment.py"", line 764, in execute
    return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/py4j/java_gateway.py"", line 1322, in __call__
    return_value = get_return_value(
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o10.execute.
: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
    at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
    at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
    at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
    at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:267)
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
    at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
    at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
    at akka.dispatch.OnComplete.internal(Future.scala:300)
    at akka.dispatch.OnComplete.internal(Future.scala:297)
    at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
    at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
    at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
    at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:622)
    at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
    at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
    at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
    at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
    at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
    at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
    at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242)
    at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:748)
    at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
    at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
    at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
    at akka.actor.ActorCell.invoke(ActorCell.scala:547)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    ... 4 more
Caused by: java.lang.RuntimeException: Failed to create stage bundle factory! Traceback (most recent call last):
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/__init__.py"", line 22, in <module>
    from . import multiarray
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/multiarray.py"", line 12, in <module>
    from . import overrides
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/overrides.py"", line 7, in <module>
    from numpy.core._multiarray_umath import (
ImportError: dlopen(/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))During handling of the above exception, another exception occurred:Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/pyflink/fn_execution/beam/beam_boot.py"", line 36, in <module>
    from apache_beam.portability.api.org.apache.beam.model.fn_execution.v1.beam_fn_api_pb2 import \
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/apache_beam/__init__.py"", line 92, in <module>
    from apache_beam import coders
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>
    from apache_beam.coders.coders import *
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/apache_beam/coders/coders.py"", line 59, in <module>
    from apache_beam.coders import coder_impl
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/apache_beam/coders/coder_impl.py"", line 56, in <module>
    import numpy as np
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/__init__.py"", line 150, in <module>
    from . import core
  File ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/__init__.py"", line 48, in <module>
    raise ImportError(msg)
ImportError: IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.We have compiled some common reasons and troubleshooting tips at:    https://numpy.org/devdocs/user/troubleshooting-importerror.htmlPlease note and check the following:  * The Python version is: Python3.9 from ""/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/bin/python""
  * The NumPy version is: ""1.21.6""and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.Original error was: dlopen(/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Users/ysmirnov/Documents/Repos/flink-python-jobs/venv/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))
    at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.createStageBundleFactory(BeamPythonFunctionRunner.java:639)
    at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:274)
    at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.open(AbstractExternalPythonFunctionOperator.java:57)
    at org.apache.flink.streaming.api.operators.python.process.AbstractExternalDataStreamPythonFunctionOperator.open(AbstractExternalDataStreamPythonFunctionOperator.java:85)
    at org.apache.flink.streaming.api.operators.python.process.AbstractExternalOneInputPythonFunctionOperator.open(AbstractExternalOneInputPythonFunctionOperator.java:117)
    at org.apache.flink.streaming.api.operators.python.process.ExternalPythonProcessOperator.open(ExternalPythonProcessOperator.java:64)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.beam.vendor.guava.v26_0_jre.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Process died with exit code 0
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2050)
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.get(LocalCache.java:3952)
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4964)
    at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:451)
    at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory.<init>(DefaultJobBundleFactory.java:436)
    at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.forStage(DefaultJobBundleFactory.java:303)
    at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.createStageBundleFactory(BeamPythonFunctionRunner.java:637)
    ... 15 more
Caused by: java.lang.IllegalStateException: Process died with exit code 0
    at org.apache.beam.runners.fnexecution.environment.ProcessManager$RunningProcess.isAliveOrThrow(ProcessManager.java:75)
    at org.apache.beam.runners.fnexecution.environment.ProcessEnvironmentFactory.createEnvironment(ProcessEnvironmentFactory.java:110)
    at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:252)
    at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$1.load(DefaultJobBundleFactory.java:231)
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
    at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
    ... 23 more
Process finished with exit code 1
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28786,,,,,,,,,,,,,,,,,,,,,,"28/Apr/23 08:21;gradysnik;1170_output.log;https://issues.apache.org/jira/secure/attachment/13057687/1170_output.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Python3,Fri May 05 02:26:22 UTC 2023,,,,,,,,,,"0|z1hlco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 08:30;martijnvisser;[~gradysnik] I believe this is a duplicate of FLINK-28786, which was re-opened and fixed for Flink 1.17.1 (so for the next release). Can you check?;;;","28/Apr/23 15:05;gradysnik;[~martijnvisser] you are right. This is my report in the Slack mentioned in the comment.
Thank you!;;;","05/May/23 02:26;dianfu;[~gradysnik] Thanks for the confirmation. I'm closing this ticket since it should have been addressed in FLINK-28786. Feel free to reopen it if this issue still happens after 1.17.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,
SQL with LAG function NullPointerException,FLINK-31967,13534418,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pyro,padavan,padavan,28/Apr/23 08:06,31/May/23 03:07,04/Jun/24 20:41,31/May/23 03:07,,,,,,,,,,,,,,,,,,,,,1.16.3,1.17.2,1.18.0,,,,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,"I want to make a query with the LAG function. And got Job Exception without any explanations.

 

*Code:*
{code:java}
private static void t1_LeadLag(DataStream<UserModel> ds, StreamExecutionEnvironment env) {
    StreamTableEnvironment te = StreamTableEnvironment.create(env);
    Table t = te.fromDataStream(ds, Schema.newBuilder().columnByExpression(""proctime"", ""proctime()"").build());

    te.createTemporaryView(""users"", t);

    Table res = te.sqlQuery(""SELECT userId, `count`,\n"" +
            "" LAG(`count`) OVER (PARTITION BY userId ORDER BY proctime) AS prev_quantity\n"" +
            "" FROM users"");

    te.toChangelogStream(res).print();
}{code}
 

*Input:*

{""userId"":3,""count"":0,""dt"":""2023-04-28T07:44:21.551Z""}

 

*Exception:* I remove part about basic JobExecutionException and kept the important(i think)
{code:java}
Caused by: java.lang.NullPointerException
    at org.apache.flink.table.data.GenericRowData.getInt(GenericRowData.java:149)
    at org.apache.flink.table.data.RowData.lambda$createFieldGetter$245ca7d1$6(RowData.java:245)
    at org$apache$flink$table$runtime$functions$aggregate$LagAggFunction$LagAcc$2$Converter.toExternal(Unknown Source)
    at org.apache.flink.table.data.conversion.StructuredObjectConverter.toExternal(StructuredObjectConverter.java:101)
    at UnboundedOverAggregateHelper$15.setAccumulators(Unknown Source)
    at org.apache.flink.table.runtime.operators.over.ProcTimeUnboundedPrecedingFunction.processElement(ProcTimeUnboundedPrecedingFunction.java:92)
    at org.apache.flink.table.runtime.operators.over.ProcTimeUnboundedPrecedingFunction.processElement(ProcTimeUnboundedPrecedingFunction.java:42)
    at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
    at org.apache.flink.streaming.runtime.io.RecordProcessorUtils.lambda$getRecordProcessor$0(RecordProcessorUtils.java:60)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:237)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:146)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Thread.java:829){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/23 11:46;padavan;image-2023-04-28-14-46-19-736.png;https://issues.apache.org/jira/secure/attachment/13057705/image-2023-04-28-14-46-19-736.png","28/Apr/23 12:06;padavan;image-2023-04-28-15-06-48-184.png;https://issues.apache.org/jira/secure/attachment/13057706/image-2023-04-28-15-06-48-184.png","28/Apr/23 12:14;padavan;image-2023-04-28-15-14-58-788.png;https://issues.apache.org/jira/secure/attachment/13057707/image-2023-04-28-15-14-58-788.png","28/Apr/23 12:17;padavan;image-2023-04-28-15-17-49-144.png;https://issues.apache.org/jira/secure/attachment/13057708/image-2023-04-28-15-17-49-144.png","28/Apr/23 14:06;padavan;image-2023-04-28-17-06-20-737.png;https://issues.apache.org/jira/secure/attachment/13057713/image-2023-04-28-17-06-20-737.png","28/Apr/23 14:05;padavan;simpleFlinkKafkaLag.zip;https://issues.apache.org/jira/secure/attachment/13057712/simpleFlinkKafkaLag.zip",,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 31 03:07:34 UTC 2023,,,,,,,,,,"0|z1hlc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 08:55;martijnvisser;I can't reproduce this. I've created the following table:

{code:sql}
CREATE TABLE users (
    userId STRING,
    `count` INT,
    proctime AS PROCTIME()
) WITH (
  'connector' = 'datagen',
  'fields.count.min' = '0',
  'fields.count.max' = '0'
);
{code}

And then run 

{code:sql}
SELECT userId, `count`, LAG(`count`) OVER (PARTITION BY userId ORDER BY proctime) AS prev_quantity FROM users;
{code}

On Flink 1.17.0. There's no NPE for me. Which Flink version did you use?;;;","28/Apr/23 08:58;martijnvisser;Also, when using:

{code:sql}
CREATE TABLE users (
    userId INT,
    `count` INT,
    proctime AS PROCTIME()
) WITH (
  'connector' = 'datagen',
  'fields.userId.min' = '1',
  'fields.userId.max' = '10',
  'fields.count.min' = '0',
  'fields.count.max' = '10'
);
{code}

I get results as I would expect. ;;;","28/Apr/23 11:44;padavan;[~martijnvisser] 

/usr/lib/jvm/java-1.11.0-openjdk-amd64/

!image-2023-04-28-14-46-19-736.png!

 
{code:java}
 public static void main(String[] args) throws Exception {
  
  final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 
  KafkaSource<UserModel> source = KafkaSource.<UserModel>builder()
                .setBootstrapServers(kafka)
                .setTopics(""x1"").setGroupId(""flink_group"")
                .setValueOnlyDeserializer(new JsonConverter()).build();
 
        WatermarkStrategy<UserModel> strategy = WatermarkStrategy.               <UserModel>forBoundedOutOfOrderness(Duration.ofSeconds(10))
                .withTimestampAssigner((i, timestamp) -> {
                  long time = i.dt.toInstant(ZoneOffset.UTC).toEpochMilli();
                    return time;
                });
        DataStream<UserModel> ds = env.fromSource(source, strategy, ""Kafka Source"");
        
        t1_LeadLag(ds, env);
        env.execute(""Flink Java API Skeleton"");
}
{code}
 
{code:java}
private static void t1_LeadLag(DataStream<UserModel> ds, StreamExecutionEnvironment env) {
    StreamTableEnvironment te = StreamTableEnvironment.create(env);
    Table t = te.fromDataStream(ds, Schema.newBuilder().columnByExpression(""proctime"", ""proctime()"").build());

    te.createTemporaryView(""users"", t);

    Table res = te.sqlQuery(""SELECT userId, `count`,\n"" +
            "" LAG(`count`) OVER (PARTITION BY userId ORDER BY proctime) AS prev_quantity\n"" +
            "" FROM users"");

    te.toChangelogStream(res).print();
}{code}
 

 

 ;;;","28/Apr/23 11:54;martijnvisser;[~padavan] You are mixing different versions of Flink (1.17.0, 1.16.1). That won't work. Please make sure that you are all using the same versions. ;;;","28/Apr/23 12:15;padavan;[~martijnvisser]  i sync to latest version 

1.17.0

nothing change , same error

!image-2023-04-28-15-17-49-144.png!

 ;;;","28/Apr/23 12:25;martijnvisser;I can't reproduce it. You're using Kafka Source, so it could be your incoming data. Can you create a reproducer with the datagen connector? ;;;","28/Apr/23 13:18;padavan;[~martijnvisser] 

wow , intresting if use DataGen then all work fine: 
{code:java}
  private static void t1_LeadLag(StreamExecutionEnvironment env) {
        StreamTableEnvironment te = StreamTableEnvironment.create(env);

        te.executeSql(
                ""CREATE TABLE users (\n"" +
                        ""    userId INT,\n"" +
                        ""    `count` INT,\n"" +
                        ""    proctime AS PROCTIME()\n"" +
                        "") WITH (\n"" +
                        ""  'connector' = 'datagen',\n"" +
                        ""  'fields.userId.min' = '1',\n"" +
                        ""  'fields.userId.max' = '10',\n"" +
                        ""  'fields.count.min' = '0',\n"" +
                        ""  'fields.count.max' = '10'\n"" +
                        "");"");

/*      
        Table t = te.fromDataStream(ds, Schema.newBuilder().columnByExpression(""proctime"", ""proctime()"").build());
        t.printSchema();
        te.createTemporaryView(""users"", t);
*/

        Table res = te.sqlQuery(""SELECT userId, `count`, proctime,"" +
                "" LAG(`count`, 1, 1) OVER w AS prev_quantity"" +
                "" FROM users"" +
                "" WINDOW w AS (ORDER BY proctime)"");
 
        te.toChangelogStream(res).print();
    }
{code}
 

I work with kafka and did various ETLs with it, Window Agg all worked fine.

But when I started working with the LAG and above code, it gives an exception. 

What can it be?
If need, Maybe I should make a project archive with my example (but you need kafka (for example in docker))? 

 

 ;;;","28/Apr/23 13:34;martijnvisser;That would be great for debugging, yes. ;;;","28/Apr/23 14:07;padavan;[~martijnvisser] 

Add Project

[^simpleFlinkKafkaLag.zip]

with two functions (work datagen and exception kafka)

!image-2023-04-28-17-06-20-737.png!

For kafta input example: 
{code:java}
{""userId"":1,""count"":5,""dt"":""2023-04-28T14:02:23.113Z""}{code}
 

java-1.11.0-openjdk-amd64;;;","07/May/23 12:27;padavan;[~martijnvisser] [~jark] [~lincoln.86xy] 

If you need anything else, let me know, i'll do it. :);;;","11/May/23 12:45;fsk119;[~padavan] after investigation, the type inference is not correct for the lag function. For a quick fix, you can modify the type of the `count` in the UserModel to `Integer`. ;;;","11/May/23 21:50;padavan;[~fsk119] with Integer work. 

I think it is at least unexpected behavior that int != Integer. Will there be any fixes? Or will you just improve the output error with a description?;;;","12/May/23 01:38;fsk119;[~padavan] Sure. There will be a PR to fix this soon. ;;;","15/May/23 06:25;pyro;[~padavan]  This problem is caused by the basic data type of Java, and I will propose a PR to solve this problem in the future;;;","18/May/23 07:49;padavan;[~fsk119] [~pyro] 

This looks like basic functionality that is often used. It's very strange that it still doesn't work. I hope to get this fixed as soon as possible (y);;;","25/May/23 03:07;fsk119;Merged into master: ac6aedbf0f35ba9734108a3c347e649bbf231c62

Merged into release-1.17: 6cac01d827ddb58bd79b5b42e1af4e05fbc45814

Merged into release-1.16: c07b50170d081978a3ca17b968d183ca9f4a57a2;;;","27/May/23 18:18;padavan;[~fsk119] already in master ? 

How can I check what works? Will there be new versions of java packages ? 1.17.2 ? ;;;","31/May/23 03:06;fsk119;[~padavan] Yes we have already merged into the master, 1.17 and 1.16. It will a bug fix version for 1.17 in the future. But right now you should package the latest 1.17 branch. It's welcome you report the new problems here.;;;","31/May/23 03:07;fsk119;I close this right now. If meet a new problem about this, please reopen this again.;;;",,,,,,,,
Flink Kubernetes operator lacks TLS support ,FLINK-31966,13534403,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tagarr,adrianalexvasiliu,adrianalexvasiliu,28/Apr/23 07:12,16/Apr/24 16:33,04/Jun/24 20:41,07/Dec/23 12:20,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.8.0,,,,,,,,,Kubernetes Operator,,,,,,1,,,,,,"*Summary*

The Flink Kubernetes operator lacks support inside the FlinkDeployment operand for configuring Flink with TLS (both one-way and mutual) for the internal communication between jobmanagers and taskmanagers, and for the external REST endpoint. Although a workaround exists to configure the job and task managers, this breaks the operator and renders it unable to reconcile.

*Additional information*
 * The Apache Flink operator supports passing through custom flink configuration to be applied to job and task managers.
 * If you supply SSL-based properties, the operator can no longer speak to the deployed job manager. The operator is reading the flink conf and using it to create a connection to the job manager REST endpoint, but it uses the truststore file paths within flink-conf.yaml, which are unresolvable from the operator. This leaves the operator hanging in a pending state as it cannot complete a reconcile.

*Proposal*

Our proposal is to make changes to the operator code. A simple change exists that would be enough to enable anonymous SSL at the REST endpoint, but more invasive changes would be required to enable full mTLS throughout.

The simple change to enable anonymous SSL would be for the operator to parse flink-conf and podTemplate to identify the Kubernetes resource that contains the certificate from the job manager keystore and use it inside the operator’s trust store.

In the case of mutual TLS, further changes are required: the operator would need to generate a certificate signed by the same issuing authority as the job manager’s certificates and then use it in a keystore when challenged by that job manager. We propose that the operator becomes responsible for making CertificateSigningRequests to generate certificates for job manager, task manager and operator. The operator can then coordinate deploying the job and task managers with the correct flink-conf and volume mounts. This would also work for anonymous SSL.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 07 12:20:41 UTC 2023,,,,,,,,,,"0|z1hl8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/23 10:37;adrianalexvasiliu;[~MartijnVisser] Referring to your changes of the issue type and priority:
* We opened the issue with type = bug, not ""new feature"", because the operator breaks when configuring Flink with TLS, and there is no indication in the documentation that this is not supported.
* Priority = critical was our take because not being able to secure the Flink deployment is perceived as critical for enterprise deployments.;;;","01/May/23 10:44;gyfora;Thank you [~adrianalexvasiliu] . We have not heard this requirement so far from the (numerous) operator users. I guess since most Kubernetes platforms come with their own network that is not externally accessible, in most cases adding encryption between the operator and resources is not necessary.

In any case I see the value of this feature. Would you be interested in working on it?;;;","08/May/23 12:01;martijnvisser;[~adrianalexvasiliu] I was on holiday so apologies for the late reply. I understand why you choose for a bug, but from a Flink perspective, this feature was never supported or tested, hence the change to new feature. The reason for the priority is to be in accordance with [Flink Jira process|https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process];;;","10/May/23 07:23;adrianalexvasiliu;[~martijnvisser] [~gyfora] Ok, fair enough. I'd just mention security guidelines such as [https://www.ncsc.gov.uk/collection/cloud/the-cloud-security-principles/principle-1-data-in-transit-protection] which refer to the internal communication, not only the external one: ""data is protected in transit as it flows between internal components within the service"".
""Would you be interested in working on it?"": thanks for the proposal, it's possible, but not short term. I'll let you know if/when starting to work on a candidate implementation. By the way, for our knowledge, are there plans for FlinkDeployment CRD changes for some other reasons? Just to know if this would be the occasion to introduce additional config parameters that would make simpler/saner the configuration of operator's TLS.;;;","12/May/23 10:13;gyfora;[~adrianalexvasiliu] new configuration changes do not usually require a CRD change. It could be just an additional value set in the flinkConfiguration.

If CRD changes are required (such as adding new fields) we can always make those as long as they are backward compatible. There is no schedule for that :) ;;;","12/Sep/23 09:55;tagarr;Hi [~gyfora] I think I can take this on, but can I discuss the approach before I start working on it.

Most users will be placing their certificates into kubernetes secrets (either by using cert-manager or creating them manually). We should be able to link the kubernetes.secrets property to the path used in the security.ssl sections to find the secret used for the rest service. The operator will need to create those stores locally onto it's disk (probably into /tmp). Then the relevant config when creating the rest client needs to be modified to point to the location we've created those files. The locations I've identified that will need modifications to the flink config appear to be in AbstractFlinkService's getClusterClient. Though there is another section in submitClusterInternal where it is using the default 
ClusterClientFactory where we would also somehow need to make changes.
 
Do you think this is the right approach and have a missed any other place where the config would need to be changed ?;;;","13/Sep/23 13:29;gyfora;[~tagarr] I think that sounds reasonable. I think this would work but I don't really know the exact expectation of users requiring this feature unfortunately :) ;;;","11/Oct/23 16:19;tagarr;Hi [~gyfora] , [~martijnvisser] I did some initial coding of a solution. I modified the AbstractFlinkService's getClusterClient method so that it would copy the relevent certs from the secret mount and place them into a defined directory of /tmp/\{namespace}/\{clusterId} and override the config of the cluster client to point to these files. This means that all the rest calls the operator makes work well. However, I can't find a way of merging the config the operator has and that of the flinkdeployment so they can interoperate and currently when the operator runs either the submitApplicationCluster or submitSessionCluster the operator emits a stacktrace complaining it can't find the relevant file. However after a short time the application or session cluster starts up fine and the status of the relevant flink cluster corrects itself. 

I currently can't see a way of getting the ssl config to work in the operator and in the cluster unless the certs are placed in the same location. Do you have any ideas ? My only thought would be to mount an emptydir in the operator e.g /flink/certs and then document that the kubernetes.secret mount points be defined on the CR so that they would be placed in a unique directory within that folder

E.g 

{{kubernetes.secrets: my-ssl-cert-secret:/flink/certs/\{clusterid}}}

{{security.ssl.rest.keystore: /flink/certs/\{clusterid}/keystore.jks}}

 This way, the operator would be able to copy the certs from the secret and place them in the same location defined in the config. Do you think this is an acceptable approach ?;;;","17/Oct/23 05:51;gyfora;[~tagarr] we could introduce a new config specific for the operator for the keystore location and only use it during rest interaction (set the original SSL with that). And we would use the other one for submitting the cluster. That way they can be different path.
What do you think?;;;","23/Oct/23 08:02;tagarr;[~gyfora] Can you assign me to this issue. I have a PR ready for review;;;","07/Dec/23 12:20;gyfora;merged to main 30566ed7390a4fce1dc3e239efe3c519c55aa9b1;;;",,,,,,,,,,,,,,,,
Fix ClassNotFoundException in benchmarks,FLINK-31965,13534398,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Paul Lin,Paul Lin,Paul Lin,28/Apr/23 06:29,09/May/23 06:52,04/Jun/24 20:41,09/May/23 06:52,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,,0,pull-request-available,,,,,"The benchmarks rely on the test jar of `flink-streaming-java`. However, the jar is set to test scope, thus not included in the packaged jar. Therefore ClassNotFoundException occurs while running the benchmarks with `java --jar xxx` command.

```
java.lang.NoClassDefFoundError: org/apache/flink/streaming/util/KeyedOneInputStreamOperatorTestHarness
    at org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmark.prepareState(RescalingBenchmark.java:111)
    at org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmark.setUp(RescalingBenchmark.java:78)
    at org.apache.flink.state.benchmark.RocksdbStateBackendRescalingBenchmarkExecutor.setUp(RocksdbStateBackendRescalingBenchmarkExecutor.java:66)
    at org.apache.flink.state.benchmark.generated.RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest._jmh_tryInit_f_rocksdbstatebackendrescalingbenchmarkexecutor0_0(RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.java:370)
    at org.apache.flink.state.benchmark.generated.RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.rescaleRocksDB_AverageTime(RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.java:147)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
    at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 17 more
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 09 06:52:36 UTC 2023,,,,,,,,,,"0|z1hl7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 09:26;Yanfei Lei;Have you tried running it with the exec command?
{code:java}
mvn clean package exec:exec \
 -Dbenchmarks=""org.apache.flink.state.benchmark.*"" {code};;;","28/Apr/23 09:30;Paul Lin;Yes, it runs fine with source codes.;;;","09/May/23 06:52;martijnvisser;Fixed in master: c038bf3f83c56230c6e621373160d0215af0e256;;;",,,,,,,,,,,,,,,,,,,,,,,,
Improve the document of Autoscaler as 1.17.0 is released,FLINK-31964,13534380,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,bgeng777,bgeng777,28/Apr/23 02:22,31/Aug/23 06:21,04/Jun/24 20:41,31/Aug/23 06:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"Since 1.17.0 is released and the official image is [available|https://hub.docker.com/layers/library/flink/1.17.0-scala_2.12-java8/images/sha256-a8bbef97ec3f7ce4fa6541d48dfe16261ee7f93f93b164c0e84644605f9ea0a3?context=explore], we can update the image link in the Autoscaler section.
 !image-2023-04-28-10-21-09-935.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/23 02:21;bgeng777;image-2023-04-28-10-21-09-935.png;https://issues.apache.org/jira/secure/attachment/13057673/image-2023-04-28-10-21-09-935.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 28 11:54:36 UTC 2023,,,,,,,,,,"0|z1hl3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 11:54;gyfora;In addition we should also improve the docs for the upcoming 1.5.0 release as we have resolved a few limitations listed here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException when scaling down with unaligned checkpoints,FLINK-31963,13534378,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,srichter,tanee.kim,tanee.kim,28/Apr/23 02:01,19/May/23 04:55,04/Jun/24 20:41,19/May/23 04:53,1.15.4,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Runtime / Checkpointing,,,,,,0,stability,,,,,"I'm testing Autoscaler through Kubernetes Operator and I'm facing the following issue.

As you know, when a job is scaled down through the autoscaler, the job manager and task manager go down and then back up again.

When this happens, an index out of bounds exception is thrown and the state is not restored from a checkpoint.

[~gyfora] told me via the Flink Slack troubleshooting channel that this is likely an issue with Unaligned Checkpoint and not an issue with the autoscaler, but I'm opening a ticket with Gyula for more clarification.

Please see the attached JM and TM error logs.
Thank you.","Flink: 1.17.0
FKO: 1.4.0
StateBackend: RocksDB(Genetic Incremental Checkpoint & Unaligned Checkpoint enabled)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27031,,,,,,,,,,,,,,,"28/Apr/23 17:49;tanee.kim;image-2023-04-29-02-49-05-607.png;https://issues.apache.org/jira/secure/attachment/13057718/image-2023-04-29-02-49-05-607.png","28/Apr/23 01:51;tanee.kim;jobmanager_error.txt;https://issues.apache.org/jira/secure/attachment/13057672/jobmanager_error.txt","28/Apr/23 01:51;tanee.kim;taskmanager_error.txt;https://issues.apache.org/jira/secure/attachment/13057671/taskmanager_error.txt",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 19 04:55:48 UTC 2023,,,,,,,,,,"0|z1hl3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 03:02;masteryhx;We also saw simliar exception when rescaling down manually with unaligned checkpoint is enabled.
This issue is related to unaligned checkpoint rescaling.
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 54
at org.apache.flink.runtime.io.network.partition.PipelinedResultPartition.getCheckpointedSubpartition(PipelinedResultPartition.java:183)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.getSubpartition(RecoveredChannelStateHandler.java:222)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.lambda$calculateMapping$1(RecoveredChannelStateHandler.java:237)
at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250)
at java.util.Spliterators$IntArraySpliterator.forEachRemaining(Spliterators.java:1032)
at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693)
at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.calculateMapping(RecoveredChannelStateHandler.java:238)
at java.util.HashMap.computeIfAbsent(HashMap.java:1126)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.getMappedChannels(RecoveredChannelStateHandler.java:227)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.getBuffer(RecoveredChannelStateHandler.java:182)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.getBuffer(RecoveredChannelStateHandler.java:157)
at org.apache.flink.runtime.checkpoint.channel.ChannelStateChunkReader.readChunk(SequentialChannelStateReaderImpl.java:198)
at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readSequentially(SequentialChannelStateReaderImpl.java:107)
at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.read(SequentialChannelStateReaderImpl.java:93)
at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readOutputData(SequentialChannelStateReaderImpl.java:79)
at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:704)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:683)
at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:650)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:954)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568)
at java.lang.Thread.run(Thread.java:834) {code};;;","28/Apr/23 04:38;tanee.kim;Thanks, Hangxiang.

I've saw unaligned checkpoint have been stable in production but some issues still remain?

If there is a related ticket about rescaling down with unaligned checkpoint, please let me know.

 ;;;","28/Apr/23 09:50;pnowojski;Yes, I agree it looks like a problem with unaligned checkpoints. [~tanee.kim] could you clarify a couple of things?
* Can you reproduce this issue? Or did it happen only once? Or maybe a couple of times, but not always? If you can reproduce, can you post steps to reproduce?
* Could you share a job graph for which this error happened?
* What are the parallelism values before the rescale and after for all of the tasks?
* From which task/subtask this error is being thrown?;;;","28/Apr/23 17:53;tanee.kim;Thank you for responding, [~pnowojski].
 * It happens a couple of times, but not always.
 * Please check below for job graph
!image-2023-04-29-02-49-05-607.png!
 * It's actually scaled down through the autoscaler, so it depends on the processing speed or whatever the relevant metric is at the time, but it's usually something like 10 -> 5, 5 -> 3.
 * The middle of above graph,  process_stream :- Sink: es_error_sink +- Filter +- Filter

 ;;;","28/Apr/23 18:03;tanee.kim;A question unrelated to this ticket, but if the subtasks that exist in the above jobgraph all appear to be one, why is that?
In order to do source scaling, the outputRecords value needs to be non-zero, but since the downstream after the kafka source stream is not separated on the jobgraph, the outputRecords is getting zero, so we explicitly added a keyBy operator to the kafka source stream so that we can intentionally separate them and then calculate the outputRecords value.
(I don't think this is very good for performance) Is there any other way to ensure that the streams are separated into two at the desired location in the jobgraph?;;;","02/May/23 15:26;pnowojski;Thanks for the answers [~tanee.kim]

{quote}
It happens a couple of times, but not always.
{quote}
But once it happened once during a recovery from an unaligned checkpoint, does it happen always for that same checkpoint? Or even that is indeterministic and retrying recovery from the same checkpoint can sucede? 
{quote}
A question unrelated to this ticket, but if the subtasks that exist in the above jobgraph all appear to be one, why is that?
In order to do source scaling, the outputRecords value needs to be non-zero, but since the downstream after the kafka source stream is not separated on the jobgraph, the outputRecords is getting zero, so we explicitly added a keyBy operator to the kafka source stream so that we can intentionally separate them and then calculate the outputRecords value.
(I don't think this is very good for performance) Is there any other way to ensure that the streams are separated into two at the desired location in the jobgraph?
{quote}
You can brake chains via {{startNewChain}} or {{disableChaining}} https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/#task-chaining-and-resource-groups . However this doesn't seem like the right think to do. What do you mean by {{outputRecords}}? {{numRecordsOut}} metric should be available for all operators, including chained source operators.;;;","06/May/23 01:37;tanee.kim;Q1

If it happens once during recovery from an unaligned checkpoint, it will always happen from the same checkpoint.

Q2

If the numRecordsOut metric applies to all operators, including chaining, then I may have jumped the gun.
Since scaling usually takes time and source & downstream scaling conditions are different, I guess I should have monitored it more closely.

Can you explain the difference between Vertex and Operator?
Scaling is done on a per-Vertex basis in the JobGraph, but if chaining is applied, are multiple Operators considered as one Vertex and therefore not subject to Source & Downstream scaling?

Thanks for answering my question.;;;","08/May/23 16:52;srichter;Hi, just to clarify: when you say a checkpoint that fails once fails always - does this only apply for restore with rescaling or can you also not recover from the CP when the parallelism remains unchanged? If it only happens with rescaling, can you at least recover for some parallelism values or for no change at all?;;;","08/May/23 17:03;pnowojski;So far I was not able to reproduce this :(

Additionally to what [~srichter] asked. [~tanee.kim], would it be possible for you to provide the checkpoint files from when the failure was happening, so that we could reproduce it more easily? 

Secondly, a random guess. Can someone verify if setting {{execution.checkpointing.unaligned.max-subtasks-per-channel-state-file}} to 1 stops this issue from reoccurring? [1]

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#execution-checkpointing-unaligned-max-subtasks-per-channel-state;;;","10/May/23 04:08;masteryhx;Hi, [~pnowojski]. I am a bit sure that it may not be related to unified file mergeing of unaligned checkpoints because I meet above exception in 1.15.
My job is a bit complicated so I tried to simplify it to reproduce it. But I haven't currently. 
I will share more if I can reproduce it by a simple job or an ITCase.;;;","10/May/23 14:45;srichter;[~masteryhx] Did your job also make use of side-outputs? Just fishing among things that are potentially ""unusual"" about the jobs.;;;","11/May/23 02:45;masteryhx;[~srichter] No, I haven't used side-outputs.
The problematic nodes / connection in my job: keyedProcessFunction -> sink (The partitioner type is rebalance).

The exception is thrown while scaling down sink node.;;;","11/May/23 13:36;pnowojski;We have managed to reproduce and find the bug. Thank you for reporting the issue and help with analysing [~tanee.kim] and [~masteryhx]. We are now working on fixing it.;;;","11/May/23 13:59;srichter;I have a local reproducer as well as a fix, will open a PR once I have written the tests.;;;","12/May/23 11:14;srichter;Seems that this is similar to the problem described in FLINK-27031.;;;","16/May/23 08:11;renqs;[~srichter] Is there any updates on this issue? Thanks;;;","16/May/23 08:41;srichter;Yes, PR is currently in review here: https://github.com/apache/flink/pull/22584;;;","16/May/23 12:57;pnowojski;To clarify impact of this bug. This is a rare issue that can happen in every not backpressured job. The problem is that if the input buffers of a downstream subtask are empty AND the output buffers of the upstream subtask are not empty, then in-flight data are incorrectly restored from such checkpoint during a recovery attempt combined with rescaling. This can lead to variety of issues:
* ArrayIndexOutOfBoundException when downscaling (as reported here)
* in-flight records sent to incorrect downstream subtasks during scaling up or down. This for keyed exchanges will cause an immediate failure when trying to match key group on the downstream subtask. For non keyed exchanges the misalignment can remain undetected, causing incorrect results. 

Checkpoint itself is not corrupted, so recovery attempt without rescaling would work without without problems. Also recovery and rescaling from such checkpoint using a Flink version that has this bug fixed will also work correctly.;;;","19/May/23 04:53;Weijie Guo;master(1.18) via 354c0f455b92c083299d8028f161f0dd113ab614.
release-1.17 via 8d8a486aaa8360d6beabacc6980280c96bf900ea.
release-1.16 via 2203bc3bdc9f963e9bddf305714adae379b268bc.;;;","19/May/23 04:55;Weijie Guo;It seems that all pull request has been merged but this ticket not closed. Include this fix in rc1 of 1.16.2 and close it.;;;",,,,,,,
libssl not found when running CI,FLINK-31962,13534357,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,27/Apr/23 19:57,04/May/23 11:44,04/Jun/24 20:41,27/Apr/23 20:03,1.16.2,1.17.1,1.18.0,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Build System,,,,,,0,,,,,,"{code:java}
Installed Maven 3.2.5 to /home/vsts/maven_cache/apache-maven-3.2.5
Installing required software
Reading package lists...
Building dependency tree...
Reading state information...
bc is already the newest version (1.07.1-2build1).
bc set to manually installed.
libapr1 is already the newest version (1.6.5-1ubuntu1).
libapr1 set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.
--2023-04-27 11:42:53--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-04-27 11:42:53 ERROR 404: Not Found.
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31999,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 27 20:03:22 UTC 2023,,,,,,,,,,"0|z1hkyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 20:03;martijnvisser;Fixed in

master: ed7ca22efc68203d4882887a3856434137e6980f
release-1.17: a6e4652b071196c66f525a487baaae076ea9d64f
release-1.16: 4944df6aad2b46a864aa153303b9b4a49fedea36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement ExpandedQuery and OriginalQuery methods for glue in catalog,FLINK-31961,13534344,13485272,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,27/Apr/23 16:46,27/Apr/23 16:46,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / AWS,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-27 16:46:24.0,,,,,,,,,,"0|z1hkw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL OverBy. Error on a code that does not exist ,FLINK-31960,13534342,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,padavan,padavan,27/Apr/23 16:35,21/May/23 15:29,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"Hello. I use latest Flink. And want make query with LEAD, LAG function , but i got error 
{noformat}
SQL validation failed. From line 1, column 138 to line 1, column 142: ROW/RANGE not allowed with RANK, DENSE_RANK or ROW_NUMBER functions{noformat}
But i dont use RANK, DENSE_RANK or ROW_NUMBER functions in my code
{code:java}
        Table win = te.sqlQuery(
                ""SELECT userId, "" +
                ""lead(`count`, 1) over w as ld, "" +
                ""lag(`count`, 1) over w as lg "" +
                ""FROM users "" +
                ""WINDOW w AS ("" +
                ""PARTITION BY userId "" +
                ""ORDER BY proctime "" +
                ""RANGE BETWEEN INTERVAL '1' MINUTE PRECEDING AND CURRENT ROW)""
        );{code}
 

I found what this problem is fix in 2020 but not...

[https://github.com/apache/flink/pull/12868]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 21 15:29:22 UTC 2023,,,,,,,,,,"0|z1hkvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 08:46;martijnvisser;[~jark] [~lincoln.86xy] WDYT?;;;","21/May/23 15:29;jark;[~padavan], ROW/RANGE clause is not allowed for the LEAD/LAG functions. You can remove the last ""{{RANGE BETWEEN INTERVAL '1' MINUTE PRECEDING AND CURRENT ROW}}""  statement and try again 

I think the exception message can be improved but that exception is thrown by Calcite, and we can report this to the Calcite community.


;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Correct the unaligned checkpoint type at checkpoint level,FLINK-31959,13534303,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,27/Apr/23 10:43,10/May/23 08:42,04/Jun/24 20:41,10/May/23 08:41,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,,,,,"FLINK-18851 added the checkpoint type in web UI to distinguish aligned checkpoint, unaligned checkpoint, savepoint and savepoint on cancel in {*}Flink 1.12{*}.

It distinguishes between UC and AC based on whether UC is enabled or disabled.[1]

However, FLINK-19680, FLINK-19681 and FLINK-19682 introduced the alignment-timeout in {*}Flink 1.13{*}, and it has been changed to {{aligned-checkpoint-timeout.}}
{code:java}
When activated, each checkpoint will still begin as an aligned checkpoint, but when the global checkpoint duration exceeds the aligned-checkpoint-timeout, if the aligned checkpoint has not completed, then the checkpoint will proceed as an unaligned checkpoint.{code}
If UC and AC-timeout is enabled and the checkpoint is completed as aligned checkpoint. It should show the aligned checkpoint instead of unaligned checkpoint.

 

[1] [https://github.com/apache/flink/blob/a3368635e3d06f764d144f8c8e2e06e499e79665/flink-runtime-web/web-dashboard/src/app/pages/job/checkpoints/detail/job-checkpoints-detail.component.ts#L118]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 10 08:41:45 UTC 2023,,,,,,,,,,"0|z1hkn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 07:42;pnowojski;merged as 35254cbdc9a^,35254cbdc9a to master;;;","10/May/23 08:41;fanrui;Merged 01ed7dbbde9f9bc8c3dc54e063c81fbe7de7f86b to 1.17-release

Merged 538edbdc11962a436134fc99e9cdbbc821341c2d to 1.16-release;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Table to DataStream allow partial fields,FLINK-31958,13534302,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,padavan,padavan,27/Apr/23 10:38,23/Nov/23 19:22,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / DataStream,Table SQL / API,,,,,0,,,,,,"Hello i have a Model with many many fields, example:
{code:java}
public class UserModel { public int userId; public int count; public int zip; public LocalDateTime dt; public LocalDateTime wStart; public LocalDateTime wEnd; }{code}
I work with Table API, select fields and convert Table to DataStream by Model. But problem what *i should select all fields if I don't even need it* or i will get exception
{quote}Column types of query result and sink for do not match. Cause: Different number of columns.
{quote}
And I just have to substitute fake data for the plugs...

 

I want simple use with only fields wich i have selected like:
{code:java}
.select($(""userId""), $(""count"").sum().as(""count""));
DataStream<UserModel> dataStream = te.toDataStream(win, UserModel.class);{code}
 

*Excepted:* 

Remove rule valdiation ""Different number of columns."". If a column is not selected it is initialized by default(T) / Null",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 23 19:22:22 UTC 2023,,,,,,,,,,"0|z1hkmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/23 20:57;padavan;What do you think?

this is very necessary for those who work with models :)

 ;;;","21/Jun/23 16:31;padavan;plz vote to this issue if you often work with models and hit this problem (y);;;","19/Jul/23 19:38;padavan;Perhaps it should be moved to the bugs category? Or is it by design?;;;","23/Nov/23 19:22;padavan;up;;;",,,,,,,,,,,,,,,,,,,,,,,
Add documentation for the user story,FLINK-31957,13534291,13532429,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,27/Apr/23 09:31,08/Jan/24 02:27,04/Jun/24 20:41,28/Jun/23 01:24,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Documentation,Table SQL / Planner,,,,,0,pull-request-available,,,,,Add documentation on how to use compiled plan to configure operator-level state TTL.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34001,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 28 01:24:30 UTC 2023,,,,,,,,,,"0|z1hkkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 01:24;luoyuxia;master: 118ac2bf454861d0bd741f5b80aff7ed55e9d035

Thanks for the pr!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Extend the CompiledPlan to read from/write to Flink's FileSystem,FLINK-31956,13534289,13532429,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xu_shuai_,qingyue,qingyue,27/Apr/23 09:12,28/Jun/23 07:14,04/Jun/24 20:41,25/Jun/23 09:37,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Client,Table SQL / Planner,,,,,0,pull-request-available,,,,,"At present, COMPILE/EXECUTE PLAN FOR '${plan.json}' only supports writing to/reading from a local file without the scheme. We propose to extend the support for Flink's FileSystem.
{code:sql}
-- before
COMPILE PLAN FOR '/tmp/foo/bar.json' <insert_query>
EXECUTE PLAN FOR '/tmp/foo/bar.json' 

-- after
COMPILE PLAN FOR 'file:///tmp/foo/bar.json' <insert_query>
COMPILE PLAN FOR 'hdfs:///tmp/foo/bar.json' <insert_query>
COMPILE PLAN FOR 's3:///tmp/foo/bar.json' <insert_query>
COMPILE PLAN FOR 'oss:///tmp/foo/bar.json' <insert_query> 
EXECUTE PLAN FOR 'file:///tmp/foo/bar.json'
EXECUTE PLAN FOR 'hdfs:///tmp/foo/bar.json'
EXECUTE PLAN FOR 's3:///tmp/foo/bar.json'
EXECUTE PLAN FOR 'oss:///tmp/foo/bar.json' {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32219,FLINK-32374,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jun 25 09:37:33 UTC 2023,,,,,,,,,,"0|z1hkk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 09:23;xu_shuai_;Hi Jane, I have taken the FLINK-31952 which is similar to the task. I think it could be achieved by modifying the streamplanner and tableEnvironmentImpl.Could I take the task?;;;","26/May/23 01:54;lincoln.86xy;[~xu_shuai_] assigned to you :);;;","19/Jun/23 12:44;qingyue;Hi, [~xu_shuai_] Thanks for your contribution! We have undergone several rounds of code review, but the implementation and testing code still need further refinement. Due to time constraints, I will take over your previous work. I would appreciate it if you could participate in the code review process.;;;","25/Jun/23 09:37;luoyuxia;master:

c83eb5cd5a632e1e25ee5cda897b6acf68b72e53

ed36b2062dd04ccf5cdc7f9bdde239c41cc6cab2;;;",,,,,,,,,,,,,,,,,,,,,,,
Prevent JUnit 4 usage for the new code with Checkstyle,FLINK-31955,13534280,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dmvk,dmvk,27/Apr/23 08:38,27/Apr/23 08:38,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Build System,,,,,,0,,,,,,,"Based on [https://lists.apache.org/thread/xl456044hmxk87mwq02p4m22yp3b04sc] discussion.

 

We'll set up a Checkstyle rule that disallows JUnit 4 usage and create a one-off suppression list for the existing violations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-27 08:38:32.0,,,,,,,,,,"0|z1hki0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent Mockito/Powermock usage for the new code with Checkstyle,FLINK-31954,13534279,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,dmvk,dmvk,27/Apr/23 08:37,24/May/23 14:12,04/Jun/24 20:41,24/May/23 14:12,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Build System,,,,,,0,,,,,,"Based on [https://lists.apache.org/thread/xl456044hmxk87mwq02p4m22yp3b04sc] discussion.

 

We'll set up a Checkstyle rule that disallows Mockito usage and create a one-off suppression list for the existing violations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-27 08:37:49.0,,,,,,,,,,"0|z1hkhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-288: Enable Dynamic Partition Discovery by Default in Kafka Source,FLINK-31953,13534275,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,loserwang1024,loserwang1024,loserwang1024,27/Apr/23 08:18,26/Jan/24 08:32,04/Jun/24 20:41,18/Jul/23 08:41,,,,,,,,,,,,,,,,,,,,,kafka-3.1.0,,,,,,,,,Connectors / Kafka,,,,,,0,Connector,kafka-source,pull-request-available,,,"This improvement implements [FLIP-288|https://cwiki.apache.org/confluence/display/FLINK/FLIP-288%3A+Enable+Dynamic+Partition+Discovery+by+Default+in+Kafka+Source].

 this Flip has there main objectives:
 # Enable partition discovery by default.
 # Provide a *EARLIEST* strategy for later discovered partitions.

 # Organize the code logic of the current built-in OffsetsInitializer, then modify the JavaDoc to let users know.

Each objective corresponds to a sub-task
h4.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://cwiki.apache.org/confluence/display/FLINK/FLIP-288%3A+Enable+Dynamic+Partition+Discovery+by+Default+in+Kafka+Source,,Important,,,,,,,,9223372036854775807,,,Java,,Mon Jul 17 02:38:50 UTC 2023,,,,,,,,,,"0|z1hkgw:",9223372036854775807,"In kafka-3.1.0, we enable partition discovery by default and set EARLIEST offset strategy for later discovered partitions.",,,,,,,,,,,,,,,,,,,"27/Apr/23 09:29;loserwang1024;I'd like to keep working on FLIP-288. Could someone please assign it to me?;;;","06/May/23 08:09;leonard;Assigned to you.;;;","17/Jul/23 02:38;loserwang1024;modify document;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support 'EXPLAIN' statement for CompiledPlan,FLINK-31952,13534270,13532429,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,xu_shuai_,qingyue,qingyue,27/Apr/23 07:27,03/Jul/23 03:33,04/Jun/24 20:41,18/Jun/23 05:46,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,," Support the explain SQL syntax towards serialized CompiledPlan
{code:sql}
EXPLAIN [ <explain_details> | PLAN FOR] <compiled_plan_json> 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jun 18 05:45:34 UTC 2023,,,,,,,,,,"0|z1hkfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 07:30;xu_shuai_;Hi Jane, I have some ideas for the task and this is how I consider it. 
Purpose of the task:
Support explain [plan for | <explain_detail>] '/path-to-json.json'
The steps I think could resolve the task :
1. Modify parserImpl.ftl template to add syntax support, which can be verified by FlinkSqlParserImplTest
2. Translate the sql to Operation.
3. TableEnv#explainPlan is a ready API, we just got the internal plan from the Operation  and pass it to TableEnv#explainPlan.
4. Finally add the test in and SqlOperationConverterTest and TableEnvironmentTest for verification.
Could I take the task?;;;","27/Apr/23 07:33;qingyue;Hi [~xu_shuai_], thanks for your interest in this ticket! The brief sounds good to me. cc [~luoyuxia] ;;;","18/Jun/23 05:45;qingyue;After revisiting `StreamPlanner#explainPlan`, I think the necessity of implementing ""explain"" syntax is not significant because the serialized execGraph cannot be rolled back to relNode. Therefore, the supported ExplainDetail types are limited to JSON_EXECUTION_PLAN only. As it is also in JSON format, its readability is not so good, and users may have confused regarding the relationship between JSON_EXECUTION_PLAN and COMPILED_PLAN.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Mix schema record source creates corrupt record,FLINK-31951,13534254,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,aeolus811tw,aeolus811tw,aeolus811tw,27/Apr/23 04:09,11/Mar/24 13:18,04/Jun/24 20:41,,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,fix-provided,pull-request-available,stale-assigned,,,"This seems to be an unexpected side effect with how AvroDeserializationSchema class was written.

Sometimes we do not have control over what record comes through a Kafka Topic.

In current implementation, if AvroDeserializationSchema encountered a record byte array that does not conform to the specified Schema / SpecificRecord type, it will cause future record to be deserialized incorrectly.

Origin of the issue is with how
{code:java}
AvroDeserializationSchema.deserialize{code}
handles exception, and how
{code:java}
AvroDeserializationSchema.checkAvroInitialized{code}
handles initialization of Decoder object",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 22 22:35:02 UTC 2023,,,,,,,,,,"0|z1hkc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 04:10;aeolus811tw;can a committer assign this bug to me? As this is fairly straightforward and I already have a fix prepared. Thanks!;;;","11/Aug/23 10:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Aug/23 17:26;aeolus811tw;PR has been submitted and approved, can a committer perform final action to close out this issue? thanks;;;","22/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,
Introduce StateMetadata,FLINK-31950,13534253,13532429,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,27/Apr/23 03:59,15/May/23 02:24,04/Jun/24 20:41,15/May/23 02:09,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"According to the FLIP design, we're about to introduce
 * StateMetadata, which describes the TTL attribute of the stateful stream operator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 15 02:09:45 UTC 2023,,,,,,,,,,"0|z1hkc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 02:09;godfrey;Fixed in master: 62b11e2e117f874f073de93756c6b9889c464562;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The state of CountTumblingWindowAssigner of Python DataStream API were never purged,FLINK-31949,13534249,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianfu,dianfu,27/Apr/23 03:20,27/Apr/23 03:20,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"Posted By Urs from user-mailing list:
{code:java}
""In FLINK-26444, a couple of convenience window assigners were added to
the Python Datastream API, including CountTumblingWindowAssigner. This
assigner uses a CountTrigger by default, which produces TriggerResult.FIRE.

As such, using this window assigner on a data stream will always produce
a ""state leak"" since older count windows will always be retained without
any chance to work on the elements again.""
{code}
See [https://lists.apache.org/thread/ql8x283xzgd98z0vsqr9npl5j74hscsm] for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-27 03:20:49.0,,,,,,,,,,"0|z1hkb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports triggering CI manually,FLINK-31948,13534243,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiang Xin,Jiang Xin,27/Apr/23 02:02,21/Jun/23 10:33,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,ml-2.4.0,,,,,,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,,,Supports triggering CI manually.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-27 02:02:12.0,,,,,,,,,,"0|z1hk9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable stdout redirect in flink-console.sh,FLINK-31947,13534194,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,26/Apr/23 17:08,29/Jun/23 12:19,04/Jun/24 20:41,29/Jun/23 12:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,flink-docker,,,,,,0,pull-request-available,,,,," flink-console.sh is used by Flink Kubenates bins to start containers. But there is no stdout redirect as flink-dameon.sh. It will cause the program that when user want to access stdout from web ui, no file is found.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 29 12:18:38 UTC 2023,,,,,,,,,,"0|z1hjyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 12:18;wangyang0918;Already covered by FLINK-31234.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
DynamoDB Sink Allow Multiple Item Writes,FLINK-31946,13534188,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,cjensen,cjensen,26/Apr/23 16:35,17/May/23 16:20,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / DynamoDB,,,,,,0,,,,,,"In some cases, it is desirable to be able to write aggregation data to multiple partition keys.  This supports the case of denormalizing data to facilitate more efficient read operations.

However, the DynamoDBSink allows for only a single DynamoDB item to be generated for each Flink Element.  This appears to be a limitation of the ElementConverter more than DyanmoDBSink.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 17 16:20:25 UTC 2023,,,,,,,,,,"0|z1hjxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/23 16:43;cjensen;I attempted to fork the flink-connector-aws repo and implement a solution that allowed for multiple items to be generated.  I modified the DynamoDbWriteRequest object to support multiple items, but caused the maxBatch size to be exceeded because the AsyncSinkWriter expects one item per element.  So this may cause a bigger change to the connector-base, unless a better way is available.

 ;;;","15/May/23 16:34;liangtl;Hi [~cjensen] ! Thanks for creating this issue. 

Trying to understand the use case better here. If we want 1 record to map to 2 WriteRequests, would doing a FlatMap before the sink operator work?

If the intention is to have more efficient reads, could we instead use a GSI?;;;","16/May/23 17:51;cjensen;Hello [~liangtl] 
Thank you for the reply.

To better describe a use case, I have aggregations data for how many times a user logs in from a specific ip address.  I also have aggregations for how many times any user logs in from that ip address.  These are two separate DynamoDB Items with different partition keys.  For two different accounts logging in I might have DynamoDB items like:

{{partition_key | sort_key   | count}}
{{-------------{-}|{-}----------{-}|{-}-----}}
{{accountid-xxx | ip-1.1.1.1 | 1     }}{{# number of times account xxx logged in from ip 1}}
{{accountid-xxx | ip-1.1.1.2 | 1     }}{{# number of times account xxx logged in from ip 2}}
{{accountid-yyy | ip-1.1.1.1 | 1}}
{{ip-1.1.1.1    |  total     | 2     }}{{# number of times any account logged in from ip 1}}
{{ip-1.1.1.2    |  total     | 1     }}{{# number of times any account logged in from ip 2}}

 

When making a query for counts by an account id, I also need total stats for each ip address they log in from.  So I have to make and additional query for each ip address.  I would like to optimize the query by duplicating the ip total entries for each record with the account partition_key, making a table like:

{{partition_key | sort_key         | count}}
{{-------------{-}|{-}----------------{-}|{-}-----}}
{{accountid-xxx | ip-1.1.1.1       | 1      }}
{{accountid-xxx | ip-1.1.1.2       | 1      }}
{{accountid-xxx | ip-1.1.1.1-total | 2      # duplicate from pk: ip-1.1.1.1}}
{{accountid-xxx | ip-1.1.1.2-total | 1      }}{{# duplicate from pk: ip-1.1.1.2}}
{{accountid-yyy | ip-1.1.1.1       | 1}}
{{{}accountid-yyy | ip-1.1.1.1-total | 2      # duplicate from pk: ip-1.1.1.1{}}}{{{{}}{}}}
{{ip-1.1.1.1    |  total           | 2}}
{{ip-1.1.1.2    |  total           | 1}}

 

This would allow me to get all the aggregation data for the account and ip address with one query (by accountid-xxx) instead of 3 queries (by accountid-xxx, ip-1.1.1.1, and ip-1.1.1.2).

I could accomplish this with a GSI, but that would increase my DynamoDB cost.

I have been able to accomplish this using a FlatMap function.  However, this complicates my code and increases the number of tasks in my Flink Application.

The simplest and most cost effective solution would be to be able to insert multiple DynamoDB items from a single aggregation.

 

 ;;;","16/May/23 20:03;cjensen;I re-evaluated the GSI option. I think the cost may be minimal compared with inserting the extra items.
It may not be worth changing the DynamoDB connector to support my request.;;;","17/May/23 16:20;cjensen;GSI turns out to be problematic to implement as there still need to be multiple inserts.  In the original example, there would need to be the ability to query by multiple account ids, and there would need to be a GSI item for each account id.  So while, the cost may not be a major limiting factor, there is still a functional restriction with the GSI.;;;",,,,,,,,,,,,,,,,,,,,,,
Table of contents in the blogs of the project website is missing some titles,FLINK-31945,13534117,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Thesharing,Thesharing,Thesharing,26/Apr/23 09:08,25/May/23 14:48,04/Jun/24 20:41,25/May/23 14:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Project Website,,,,,,1,pull-request-available,,,,,"The ToC in the blog pages of the project website doesn't have all the section titles. The section titles of the first level is missing. 

Solution: Add the following configuration item in config.toml.
{noformat}
[markup.tableOfContents]
  startLevel = 0{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 25 14:48:02 UTC 2023,,,,,,,,,,"0|z1hjhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/23 06:06;Thesharing;Hi, [~martijnvisser], would you mind assigning this ticket to me? This only requires a minor fix. Also, I'd like to fix FLINK-32051, too. Thank you so much in advance.;;;","25/May/23 14:48;martijnvisser;Fixed in apache/flink-web:asf-site via 6948c400ce9749fdb3a2f7ccc3a96ec76b2e4a47;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Protobuf format throw com.google.protobuf.InvalidProtocolBufferException,FLINK-31944,13534107,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,26/Apr/23 07:37,26/May/23 16:49,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,,,,,,"It seems that protobuf format throws the following exception when the first field of the message is string type. This may also occur for other types. I uploaded the maven project to reproduce the problem.

 
{code:java}
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.io.IOException: Failed to deserialize PB object.
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:75)
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:42)
    at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.readRecord(DeserializationSchemaAdapter.java:197)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.nextRecord(DeserializationSchemaAdapter.java:210)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$Reader.readBatch(DeserializationSchemaAdapter.java:124)
    at org.apache.flink.connector.file.src.util.RecordMapperWrapperRecordIterator$1.readBatch(RecordMapperWrapperRecordIterator.java:82)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:162)
    ... 6 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.convertProtoBinaryToRow(ProtoToRowConverter.java:129)
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:70)
    ... 15 more
Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).
    at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:133)
    at com.google.protobuf.CodedInputStream$ArrayDecoder.readTag(CodedInputStream.java:633)
    at com.example.proto.Message.<init>(Message.java:47)
    at com.example.proto.Message.<init>(Message.java:9)
    at com.example.proto.Message$1.parsePartialFrom(Message.java:540)
    at com.example.proto.Message$1.parsePartialFrom(Message.java:534)
    at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:158)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:191)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:203)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:208)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:48)
    at com.example.proto.Message.parseFrom(Message.java:218)
    ... 21 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32008,,,,,,,,,,,,,,,,,,,,,,"26/Apr/23 07:34;xuannan;flink-protobuf-example.zip;https://issues.apache.org/jira/secure/attachment/13057595/flink-protobuf-example.zip",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 26 16:48:57 UTC 2023,,,,,,,,,,"0|z1hjfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/23 10:53;libenchao;CC [~maosuhan];;;","23/May/23 04:28;maosuhan;I think the error is related to corrupt data.;;;","26/May/23 16:48;rskraba;This is a duplicate of FLINK-32008 – one of the serialized messages is:
{code:java}
0a 03 61 62 63 0a {code}
(Note the first **0a** byte that isn't a newline, and the second **0a** byte that was added to the record and probably shouldn't be there.);;;",,,,,,,,,,,,,,,,,,,,,,,,
Multiple t_env cause class loading problem,FLINK-31943,13534072,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,26/Apr/23 02:18,26/Apr/23 02:18,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"When a PyFlink process creates multiple StreamTableEnvironment with different EnvironmentSettings and sets the ""pipeline.jars"" at the first created t_env, it appears that the jar is not added to the classloader of the first created t_env.

 

After digging a little bit, the reason may be that when creating the second table environment with a new EnvironmentSettings, the context classloader overwrites by a new classloader, see `EnvironmentSettings.Builder.build` method.

 

I uploaded the script to reproduce the problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/23 02:18;xuannan;flink-sql-connector-kafka-1.17.0.jar;https://issues.apache.org/jira/secure/attachment/13057574/flink-sql-connector-kafka-1.17.0.jar","26/Apr/23 02:17;xuannan;pyflink_classloader.py;https://issues.apache.org/jira/secure/attachment/13057575/pyflink_classloader.py",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-26 02:18:57.0,,,,,,,,,,"0|z1hj7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Conditional Writes in DynamoDB connector,FLINK-31942,13534055,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,25/Apr/23 21:31,25/Apr/23 21:31,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / DynamoDB,,,,,,0,,,,,,"Currently, the AWS DynamoDB connector uses the BatchWrite API, which does not support conditional writes. This is not great because there might be some use cases where the Flink user might want to use conditional writes to implement idempotent writes.

 

We propose to implement support for using `PutItem`, `UpdateItem` and `DeleteItem` in the DDB connector",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-25 21:31:02.0,,,,,,,,,,"0|z1hj40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not backwards compatible naming for some kubernetes resources,FLINK-31941,13534053,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,eric.xiao,eric.xiao,25/Apr/23 20:50,31/Aug/23 06:22,04/Jun/24 20:41,31/Aug/23 06:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"We are in the process of migrating all our workloads over to the Kubernetes operator and noticed that some of the Kubernetes resources in the operator are hardcoded and not consistent with how Flink previously defined them. This is leading to us some downstream incompatibilities and some migration toil in our monitoring and dashboards that have queries depending on the previous naming schema.

I couldn't find exact definitions a task-manager or job-manager in the flink repo, but this is what I have noticed, I may be wrong on my interpretations 😅.
h3. Deployment Names

Previously:

 
{code:java}
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
trickle-job-manager    2/2     2            2           13d
trickle-task-manager   10/10   10           10          13d {code}
New (Flink Operator):

 

 
{code:java}
NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
trickle               2/2     2            2           6h25m
trickle-taskmanager   4/4     4            4           6h25m {code}
 

[1] [https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/utils/StandaloneKubernetesUtils.java#L29-L38]
h3. Pod Names

Previously:
{code:java}
NAME                                    READY   STATUS      RESTARTS   AGE
trickle-job-manager-65d95d4854-lgmsm    1/1     Running     0          13d
trickle-job-manager-65d95d4854-vdzl8    1/1     Running     0          5d
trickle-task-manager-86c85cf647-46nxh   1/1     Running     0          5d
trickle-task-manager-86c85cf647-ct6c5   1/1     Running     0          5d
trickle-task-manager-86c85cf647-h894q   1/1     Running     0          5d
trickle-task-manager-86c85cf647-kpr5x   1/1     Running     0          5d{code}
New (Flink Operator):
{code:java}
NAME                                   READY   STATUS    RESTARTS   AGE
trickle-58f895675f-9m5wm               1/1     Running   0          25h
trickle-58f895675f-n4hhv               1/1     Running   0          25h
trickle-taskmanager-6f9f64b9b9-857lv   1/1     Running   0          25h
trickle-taskmanager-6f9f64b9b9-cnsrx   1/1     Running   0          25h{code}
----
The pod names stem from the deployment names, so a fix to update the deployment names may also fix the pod names.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 03 12:17:22 UTC 2023,,,,,,,,,,"0|z1hj3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/May/23 10:47;gyfora;Flink itself doesn't come with standalone deployment mechanism so I don't fully understand what is inconsistent with what :) 

In any case it would probably make sense to attach the ""jobmanager"" suffix to the JM deployment.

[~darenwkt] [~usamj] what do you think?;;;","02/May/23 08:11;usamj;I believe this follows the convention that we use in Native mode, but I do believe that have a ""jobmanager"" suffix would be nice and make it clearer what the pods are.;;;","03/May/23 12:17;darenwkt;Agreed with Usamah. In addition, I think having a pluginable Decorator would help here as well to allow custom renaming of JM/TM pods. 

https://issues.apache.org/jira/browse/FLINK-28831?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel;;;",,,,,,,,,,,,,,,,,,,,,,,,
DataStreamCsvITCase#CityPojo should be public,FLINK-31940,13534036,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,25/Apr/23 17:39,14/Jun/23 09:06,04/Jun/24 20:41,14/Jun/23 09:06,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,,,,0,pull-request-available,,,,,Since the class is package-private it is serialized via Kryo and not the pojo serializer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 14 09:06:28 UTC 2023,,,,,,,,,,"0|z1hizs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 09:06;chesnay;master: 86681f0c36d9d83c67287007078952d5c0f31745;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException on org.apache.beam.runners.flink.translation.wrappers.SourceInputFormat when running Flink 1.14 and up with Beam 2.46.0,FLINK-31939,13534033,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Invalid,,amitdk,amitdk,25/Apr/23 16:30,26/Apr/23 07:30,04/Jun/24 20:41,26/Apr/23 07:30,1.14.2,1.15.4,1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"While running some basic Beam code that is reading a file and transforming it for further processing, I am unable to get past the following error thrown, that prevents the job master from starting up when I try to run/deploy a fat jar on Flink. I'm using Apache Beam 2.46.0, with Java 11 and beam flink runner 1.14 with a local Flink instance that's on 1.14.2.

 

It looks to be some form of class-loading issue, and while I've tried later version of Flink, all produce the same error. Appreciate any help with this.

 

Caused by: java.lang.ClassNotFoundException: org.apache.beam.runners.flink.translation.wrappers.SourceInputFormat
        at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
        at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:64)
        at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:74)
        at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:48)
        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
        at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:172)
        at java.base/java.lang.Class.forName0(Native Method)
        at java.base/java.lang.Class.forName(Class.java:398)
        at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.resolveClass(InstantiationUtil.java:78)

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 07:30:59 UTC 2023,,,,,,,,,,"0|z1hiz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/23 07:30;martijnvisser;I would recommend to first reach out to the Beam community; this most likely is a configuration issue, not a bug and if it's a bug, I doubt it would be in Flink. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Failing Unit Test: FlinkConnectionTest.testCatalogSchema ""Failed to get response for the operation""",FLINK-31938,13534023,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kurto,kurto,25/Apr/23 15:06,25/Apr/23 15:06,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / JDBC,,,,,,0,,,,,,"{noformat}
[ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.885 s <<< FAILURE! - in org.apache.flink.table.jdbc.FlinkConnectionTest
[ERROR] org.apache.flink.table.jdbc.FlinkConnectionTest.testCatalogSchema  Time elapsed: 1.513 s  <<< ERROR!
org.apache.flink.table.client.gateway.SqlExecutionException: Failed to get response for the operation 733f0d91-e9e8-4487-949f-f3abb13384e8.
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:416)
	at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:376)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:242)
	at org.apache.flink.table.jdbc.FlinkConnectionTest.testCatalogSchema(FlinkConnectionTest.java:95){noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-25 15:06:07.0,,,,,,,,,,"0|z1hiww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failing Unit Test: ClientTest.testClientServerIntegration ""Connection leak""",FLINK-31937,13534005,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,kurto,kurto,25/Apr/23 14:09,28/Apr/23 11:09,04/Jun/24 20:41,28/Apr/23 11:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Queryable State,,,,,,0,,,,,,"{code:java}
[ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 34.68 s <<< FAILURE! - in org.apache.flink.queryablestate.network.ClientTest[ERROR] org.apache.flink.queryablestate.network.ClientTest.testClientServerIntegration  Time elapsed: 3.801 s  <<< FAILURE!java.lang.AssertionError: Connection leak (server)     at org.apache.flink.queryablestate.network.ClientTest.testClientServerIntegration(ClientTest.java:719) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 15:28:09 UTC 2023,,,,,,,,,,"0|z1hisw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 14:12;martijnvisser;[~kurto] My initial thinking is that this is a local problem, since we run CI for every PR and every merged commit and this test doesn't fail there... ;;;","25/Apr/23 15:28;kurto;[~martijnvisser] ah, ok, unit tests don't need to run outside of the CI environment. thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Support setting scale up max factor,FLINK-31936,13533985,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,25/Apr/23 11:46,11/May/23 07:41,04/Jun/24 20:41,11/May/23 07:41,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Autoscaler,,,,,,0,pull-request-available,,,,,"Currently, only scale down max factor is supported to be configured. We should also add a config for scale up max factor as well. In many cases, a job's performance won't improve after scaling up due to external bottlenecks. Although we can detect ineffective scaling up and would block further scaling, but it already hurts if we scale too much in a single step which may even burn out external services.

As for the default value, I think 200% would be a good start.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 11 07:41:35 UTC 2023,,,,,,,,,,"0|z1hiog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 11:52;Zhanghao Chen;Hi, [~gyfora], if you think this issue is worthful to do, could you assign it to me? I'm trying to use autoscaling in production, and would like to contribute to autoscaler as well. Hope this could be the starting point of my contribution.;;;","25/Apr/23 13:50;gyfora;Thanks [~Zhanghao Chen] , I assign this to you :) ;;;","11/May/23 07:41;gyfora;merged to main f24668dbe126551caeee570aa0953c689648a335;;;",,,,,,,,,,,,,,,,,,,,,,,,
The new resource requirements REST API is only available for session clusters,FLINK-31935,13533975,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,25/Apr/23 09:45,16/Oct/23 11:38,04/Jun/24 20:41,26/Apr/23 07:43,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / REST,,,,,,0,pull-request-available,,,,,"We need to register both `JobResourceRequirementsHandler` and `
JobResourceRequirementsUpdateHandler` for application / per-job clusters as well.
 
These handlers have been introduced as part of FLINK-31316.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31469,,FLINK-31316,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 07:43:23 UTC 2023,,,,,,,,,,"0|z1him8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 09:55;chesnay;Whoops; just need to move them up into the WebMonitorEndpoint I guess.;;;","26/Apr/23 07:43;chesnay;master: 3acb76dcf450f9136dec150124db901d1a9aa47b;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Remove mocking in RocksDB tests,FLINK-31934,13533959,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,25/Apr/23 08:33,11/May/23 18:58,04/Jun/24 20:41,11/May/23 18:58,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / State Backends,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 11 18:58:03 UTC 2023,,,,,,,,,,"0|z1hiio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/23 18:58;chesnay;master: 2d873629b43ec7339850b685833e3404d5ae824a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Whitebox usage in ExpressionKeysTest,FLINK-31933,13533958,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,25/Apr/23 08:31,28/Apr/23 16:08,04/Jun/24 20:41,28/Apr/23 16:08,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,API / Core,Tests,,,,,0,pull-request-available,,,,,Reduce illegal reflective accesses.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 28 16:08:41 UTC 2023,,,,,,,,,,"0|z1hiig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 16:08;chesnay;master: 1102fc62b8c87d6c01dbfd51e82e689dd1bffaf5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Allow to configure HA on k8s without using service account,FLINK-31932,13533934,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jigga,jigga,25/Apr/23 06:58,25/Apr/23 06:58,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,,,,,,"I have quite uncommon use case where I'd like to configure job manager's high availability on Kubernetes, but without using a service account, but rather a combination of username and password for interacting with the k8s' API. The company's policy only allows read-only service accounts, and if I want to be able to manipulate k8s objects (e.g., ConfigMap creation/modification) I need to have a dedicated account with username/password authentication. I have such an account, but I wasn't yet able to configure Flink's HA with it. Any advise greatly appreciated. Our k8s provider is OpenShift 4.x.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-25 06:58:53.0,,,,,,,,,,"0|z1hid4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception history page should not link to a non-existent TM log page.,FLINK-31931,13533923,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,JunRuiLi,JunRuiLi,JunRuiLi,25/Apr/23 04:08,11/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,stale-assigned,,,,"In FLINK-30358, we supported to show the task manager ID on the exception history page and added a link to the task manager ID to jump to the task manager page. However, if the task manager no longer exists when clicking the link to jump, the page will continue to load and the following error log will be continuously printed in the JM log. This will trouble users, and should be optimized.
{code:java}
2023-04-25 11:40:50,109 [flink-akka.actor.default-dispatcher-35] ERROR org.apache.flink.runtime.rest.handler.taskmanager.TaskManagerDetailsHandler [] - Unhandled exception.
org.apache.flink.runtime.resourcemanager.exceptions.UnknownTaskExecutorException: No TaskExecutor registered under container_01.
  at org.apache.flink.runtime.resourcemanager.ResourceManager.requestTaskManagerDetailsInfo(ResourceManager.java:697) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
  at sun.reflect.GeneratedMethodAccessor106.invoke(Unknown Source) ~[?:?]
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362]
  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362]
  at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
  at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
  at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
  at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
  at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[?:?]
  at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
  at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
  at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
  at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[flink-scala_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
  at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[flink-scala_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
  at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[flink-scala_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
  at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[flink-scala_2.12-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
  at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
  at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
  at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) ~[?:?]
  at akka.actor.ActorCell.invoke(ActorCell.scala:547) ~[?:?]
  at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
  at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
  at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
  at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_362]
  at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_362]
  at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_362]
  at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_362]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30358,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 10:35:00 UTC 2023,,,,,,,,,,"0|z1hiao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
MetricQueryService works not properly in k8s with IPv6 stack,FLINK-31930,13533911,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,caiyi,caiyi,25/Apr/23 02:04,25/Apr/23 09:52,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / RPC,,,,,,0,,,,,,"As attachment below, MetricQueryService works not properly in k8s with IPv6 stack.","1. K8s with ipv6 stack

2. Deploy flink-kubernetes-operator

3. Deploy a standalone cluster with 3 taskmanager using kubernetes high-availability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/23 02:05;caiyi;1.jpg;https://issues.apache.org/jira/secure/attachment/13057531/1.jpg",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 09:52:14 UTC 2023,,,,,,,,,,"0|z1hi80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 09:52;martijnvisser;[~caiyi] Do you want to open a PR to resolve this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
HighAvailabilityServicesUtils.getWebMonitorAddress works not properly in k8s with IPv6 stack,FLINK-31929,13533908,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,caiyi,caiyi,25/Apr/23 01:44,26/Apr/23 03:44,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / REST,,,,,,0,,,,,,"As attachment below, String.format works not properly if address is IPv6, 
new URL(protocol, address, port, """").toString() is correct.",K8s with IPv6 stack,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/23 01:44;caiyi;1.jpg;https://issues.apache.org/jira/secure/attachment/13057528/1.jpg",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 03:44:08 UTC 2023,,,,,,,,,,"0|z1hi7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 04:29;Weijie Guo;Thanks [~caiyi], would you mind submitting a pull request and adding some tests for this?;;;","25/Apr/23 09:50;martijnvisser;It's probably worthwhile to look if you can use a similar approach as FLINK-30478 to resolve this;;;","26/Apr/23 03:44;caiyi;Ok, I'll make a pull request to resolve it.;;;",,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes works not properly in k8s with IPv6 stack,FLINK-31928,13533907,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,caiyi,caiyi,25/Apr/23 01:29,07/Sep/23 09:50,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,auto-deprioritized-major,pull-request-available,,,,"As [https://github.com/square/okhttp/issues/7368|https://github.com/square/okhttp/issues/7368,] ,okhttp3 shaded in flink-kubernetes works not properly in IPv6 stack in k8s, need to upgrade okhttp3 to version 4.10.0 and shade dependency of okhttp3:4.10.0
org.jetbrains.kotlin:kotlin-stdlib in flink-kubernetes or just upgrade
kubernetes-client to latest version, and release a new version of flink-kubernetes-operator.",Kubernetes of IPv6 stack.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 07 09:50:43 UTC 2023,,,,,,,,,,"0|z1hi74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 09:51;martijnvisser;[~caiyi] Do you want to open a PR to update okhttp3? ;;;","26/Apr/23 03:43;caiyi;Ok, I'll upgrade okhttp3 to latest 4.11.0.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","07/Sep/23 09:50;Shixiaolin;Can I merge to version 1.14.4？;;;",,,,,,,,,,,,,,,,,,,,,,
Cassandra source raises an exception on Flink 1.16.0 if the user enables the metrics in the cassandra driver,FLINK-31927,13533886,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,echauchot,echauchot,echauchot,24/Apr/23 18:26,05/May/23 09:23,04/Jun/24 20:41,04/May/23 08:42,1.16.0,cassandra-3.1.0,,,,,,,,,,,,,,,,,,,cassandra-3.1.0,,,,,,,,,Connectors / Cassandra,,,,,,0,pull-request-available,,,,,CassandraSplitEnumerator#prepareSplits() raises  java.lang.NoClassDefFoundError: com/codahale/metrics/Gauge when calling cluster.getMetadata() leading to NPE in CassandraSplitEnumerator#start() async callback. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 04 08:42:52 UTC 2023,,,,,,,,,,"0|z1hi2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/23 12:59;echauchot;This bug has slipped through the ITests because, in the cassandra source tests, the cluster builder is configured using _withoutMetrics()_  . In a real production use case, a user could want to get the Cassandra driver metrics.;;;","02/May/23 13:28;echauchot;Putting the summary of a private discussion with [~chesnay] here: we should only enable these metrics if they are forwarded to the Flink metrics system. So, for now we disable them. We could enable them and implement the metrics forwarding in a follow-up ticket if users ask for this feature.;;;","04/May/23 08:42;echauchot;commits on main:
[275804c445e9c47ab408a108c6583118ea13ec69|https://github.com/apache/flink-connector-cassandra/commit/275804c445e9c47ab408a108c6583118ea13ec69]
[bff833170ae2aa856c409d7a7ab825d088057478|https://github.com/apache/flink-connector-cassandra/commit/bff833170ae2aa856c409d7a7ab825d088057478];;;",,,,,,,,,,,,,,,,,,,,,,,,
Implement rename Table in GlueCatalog,FLINK-31926,13533878,13485272,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,24/Apr/23 17:15,24/Apr/23 17:15,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Glue catalog don't support renaming table. 
Currently marked as unsupported operation . 

This task intend to implement it later.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-24 17:15:44.0,,,,,,,,,,"0|z1hi0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync benchmark dependency versions with the version used in Flink,FLINK-31925,13533859,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,24/Apr/23 14:09,06/May/23 02:20,04/Jun/24 20:41,28/Apr/23 09:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32018,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 28 09:22:37 UTC 2023,,,,,,,,,,"0|z1hhwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 09:22;pnowojski;Merged to flink-benchmarks master as 87c425f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink operator] Flink Autoscale - Limit the max number of scale ups,FLINK-31924,13533850,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Bug,,sriramgr,sriramgr,24/Apr/23 12:34,25/Apr/23 10:30,04/Jun/24 20:41,25/Apr/23 10:30,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,,,,,,"Found that Autoscale keeps happening even after reaching max-parallelism.

{color:#172b4d}Flink version: 1.17{color}

Source: Kafka

Configuration:

 
{code:java}
flinkConfiguration:
    kubernetes.operator.job.autoscaler.enabled: ""true""
    kubernetes.operator.job.autoscaler.scaling.sources.enabled: ""true""
    kubernetes.operator.job.autoscaler.target.utilization: ""0.6""
    kubernetes.operator.job.autoscaler.target.utilization.boundary: ""0.2""
    kubernetes.operator.job.autoscaler.stabilization.interval: ""1m""
    kubernetes.operator.job.autoscaler.metrics.window: ""3m""{code}
Logs:
{code:java}
2023-04-24 12:29:10,738 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] Starting reconciliation2023-04-24 12:29:10,740 o.a.f.k.o.s.FlinkResourceContextFactory [INFO ][my-namespace/my-pod] Getting service for my-job2023-04-24 12:29:10,740 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Observing job status2023-04-24 12:29:10,765 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Job status changed from CREATED to RUNNING2023-04-24 12:29:10,870 o.a.f.k.o.l.AuditUtils         [INFO ][my-namespace/my-pod] >>> Event  | Info    | JOBSTATUSCHANGED | Job status changed from CREATED to RUNNING2023-04-24 12:29:10,938 o.a.f.k.o.l.AuditUtils         [INFO ][my-namespace/my-pod] >>> Status | Info    | STABLE          | The resource deployment is considered to be stable and won’t be rolled back2023-04-24 12:29:10,986 o.a.f.k.o.a.ScalingMetricCollector [INFO ][my-namespace/my-pod] Skipping metric collection during stabilization period until 2023-04-24T12:30:10.765Z2023-04-24 12:29:10,986 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][my-namespace/my-pod] Resource fully reconciled, nothing to do...2023-04-24 12:29:10,986 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] End of reconciliation2023-04-24 12:29:25,991 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] Starting reconciliation2023-04-24 12:29:25,992 o.a.f.k.o.s.FlinkResourceContextFactory [INFO ][my-namespace/my-pod] Getting service for my-job2023-04-24 12:29:25,992 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Observing job status2023-04-24 12:29:26,005 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Job status (RUNNING) unchanged2023-04-24 12:29:26,053 o.a.f.k.o.a.ScalingMetricCollector [INFO ][my-namespace/my-pod] Skipping metric collection during stabilization period until 2023-04-24T12:30:10.765Z2023-04-24 12:29:26,054 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][my-namespace/my-pod] Resource fully reconciled, nothing to do...2023-04-24 12:29:26,054 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] End of reconciliation2023-04-24 12:29:41,059 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] Starting reconciliation2023-04-24 12:29:41,060 o.a.f.k.o.s.FlinkResourceContextFactory [INFO ][my-namespace/my-pod] Getting service for my-job2023-04-24 12:29:41,061 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Observing job status2023-04-24 12:29:41,075 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Job status (RUNNING) unchanged2023-04-24 12:29:41,116 o.a.f.k.o.a.ScalingMetricCollector [INFO ][my-namespace/my-pod] Skipping metric collection during stabilization period until 2023-04-24T12:30:10.765Z2023-04-24 12:29:41,116 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][my-namespace/my-pod] Resource fully reconciled, nothing to do...2023-04-24 12:29:41,116 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] End of reconciliation2023-04-24 12:29:56,121 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] Starting reconciliation2023-04-24 12:29:56,122 o.a.f.k.o.s.FlinkResourceContextFactory [INFO ][my-namespace/my-pod] Getting service for my-job2023-04-24 12:29:56,122 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Observing job status2023-04-24 12:29:56,134 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Job status (RUNNING) unchanged2023-04-24 12:29:56,178 o.a.f.k.o.a.ScalingMetricCollector [INFO ][my-namespace/my-pod] Skipping metric collection during stabilization period until 2023-04-24T12:30:10.765Z2023-04-24 12:29:56,179 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][my-namespace/my-pod] Resource fully reconciled, nothing to do...2023-04-24 12:29:56,179 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] End of reconciliation2023-04-24 12:30:11,183 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] Starting reconciliation2023-04-24 12:30:11,184 o.a.f.k.o.s.FlinkResourceContextFactory [INFO ][my-namespace/my-pod] Getting service for my-job2023-04-24 12:30:11,184 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Observing job status2023-04-24 12:30:11,193 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Job status (RUNNING) unchanged2023-04-24 12:30:11,367 o.a.f.k.o.a.m.ScalingMetrics   [ERROR][my-namespace/my-pod] Cannot compute source target data rate without numRecordsInPerSecond and pendingRecords (lag) metric for e5a72f353fc1e6bbf3bd96a41384998c.2023-04-24 12:30:11,370 o.a.f.k.o.a.ScalingMetricCollector [INFO ][my-namespace/my-pod] Waiting until 2023-04-24T12:33:10.765Z so the initial metric window is full before starting scaling2023-04-24 12:30:11,370 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][my-namespace/my-pod] Resource fully reconciled, nothing to do...2023-04-24 12:30:11,370 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] End of reconciliation2023-04-24 12:30:26,374 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] Starting reconciliation2023-04-24 12:30:26,375 o.a.f.k.o.s.FlinkResourceContextFactory [INFO ][my-namespace/my-pod] Getting service for my-job2023-04-24 12:30:26,376 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Observing job status2023-04-24 12:30:26,385 o.a.f.k.o.o.JobStatusObserver  [INFO ][my-namespace/my-pod] Job status (RUNNING) unchanged2023-04-24 12:30:26,542 o.a.f.k.o.a.m.ScalingMetrics   [ERROR][my-namespace/my-pod] Cannot compute source target data rate without numRecordsInPerSecond and pendingRecords (lag) metric for e5a72f353fc1e6bbf3bd96a41384998c.2023-04-24 12:30:26,543 o.a.f.k.o.a.ScalingMetricCollector [INFO ][my-namespace/my-pod] Waiting until 2023-04-24T12:33:10.765Z so the initial metric window is full before starting scaling2023-04-24 12:30:26,543 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][my-namespace/my-pod] Resource fully reconciled, nothing to do...2023-04-24 12:30:26,544 o.a.f.k.o.c.FlinkDeploymentController [INFO ][my-namespace/my-pod] End of reconciliation{code}
 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 10:30:39 UTC 2023,,,,,,,,,,"0|z1hhug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 12:35;sriramgr;[~gyfora] - Please kindly check. ;;;","24/Apr/23 12:59;gyfora;cc [~mxm] ;;;","24/Apr/23 13:00;gyfora;[~sriramgr] can you please try the autoscaler from the latest main branch and see whether you can reproduce the problem?;;;","24/Apr/23 13:44;sriramgr;Sure. Let me try and come back again by tomorrow.;;;","24/Apr/23 13:56;mxm;Could you clarify what is the issue here? The logs don't indicate an issue. The autoscaler will continue to run even after reaching the max parallelism. The max parallelism is per vertex. There may be other vertices which still get scaled.;;;","25/Apr/23 09:11;sriramgr;The issue still remains the same. I tried from the main branch.;;;","25/Apr/23 10:30;gyfora;Based on offline discussion the problem seems to be related to the job itself not the autoscaler logic;;;",,,,,,,,,,,,,,,,,,,,
Connector weekly runs are only testing main branches instead of all supported branches,FLINK-31923,13533836,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,24/Apr/23 11:47,19/Apr/24 09:26,04/Jun/24 20:41,20/Jun/23 07:47,,,,,,,,,,,,,,,,,,,,,cassandra-3.2.0,elasticsearch-3.1.0,gcp-pubsub-3.0.2,gcp-pubsub-3.1.0,jdbc-3.2.0,mongodb-1.0.2,opensearch-1.1.0,pulsar-4.0.1,rabbitmq-3.1.0,Build System,Connectors / Common,,,,,0,pull-request-available,,,,,"We have a weekly scheduled build for connectors. That's only triggered for the {{main}} branches, because that's how the Github Actions {{schedule}} works, per https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#schedule

We can resolve that by having the Github Action flow checkout multiple branches as a matrix to run these weekly tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32448,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 15 11:11:00 UTC 2023,,,,,,,,,,"0|z1hhrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 08:00;martijnvisser;Fixed in:

apache/flink-connector-shared-utils@ci_utils: 7c237004da01ac1d325d37cec5dd2eea84fbd101;;;","15/Jun/23 11:11;martijnvisser;Fixed in:

apache/flink-connector-gcp-pubsub@main: 01e1b6fa0d830aea734201f63c1ee58874d5fa23
apache/flink-connector-rabbitmq@main: 6b70965d331c5f0f94bf08a7defcb7ecf62dbc5c
apache/flink-connector-jdbc@main: bd371d64be644a36b8000ed06c9afa9928cb8fc4
apache/flink-connector-pulsar@main: f463d3f707c8824bc61cce29c0efcdd4de94257e
apache/flink-connector-mongodb@main: fe65806824ae181831b3440f04cbd13bee9af95d
apache/flink-connector-opensearch@main: aa2d57ee7af212757815378ae43eec8536fcde1a
apache/flink-connector-cassandra@main: ac6cf71fb3ed73f2ca1f6414d6e22abc9c756529
apache/flink-connector-elasticsearch@main: be0f30428fec7871644509cd431e088f1d39f390

AWS will be done later, since they are currently blocked on the Maven upgrade;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Port over Kinesis Client configurations for retry and backoff,FLINK-31922,13533827,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,darenwkt,liangtl,liangtl,24/Apr/23 11:11,16/Apr/24 10:32,04/Jun/24 20:41,,aws-connector-4.2.0,aws-connector-4.3.0,,,,,,,,,,,,,,,,,,,aws-connector-4.4.0,,,,,,,,,Connectors / AWS,,,,,,0,,,,,,"Port over the Kinesis Client configurations for GetRecords, ListShards, DescribeStream",,,,,,,,,,FLINK-34339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 24 11:54:49 UTC 2023,,,,,,,,,,"0|z1hhpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 11:54;darenwkt;[~dannycranmer] Please assign this to me, thank you;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Create a mini cluster based metric collection test,FLINK-31921,13533821,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mxm,mxm,24/Apr/23 10:22,24/Apr/23 10:22,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,,,,,,We would benefit from an e2e test for metric collection which verifies assumptions we have about Flink metrics.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-24 10:22:12.0,,,,,,,,,,"0|z1hho0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Kubernetes operator tests ,FLINK-31920,13533820,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mxm,mxm,24/Apr/23 10:18,24/Apr/23 10:19,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"[ERROR] Errors: 
[ERROR]   FlinkOperatorTest.testConfigurationPassedToJOSDK:63 » NullPointer
[ERROR]   FlinkOperatorTest.testLeaderElectionConfig:108 » NullPointer
[ERROR]   HealthProbeTest.testHealthProbeEndpoint:64 » NullPointer
[INFO] 
[ERROR] Tests run: 323, Failures: 0, Errors: 3, Skipped: 0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-24 10:18:43.0,,,,,,,,,,"0|z1hhns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip ClosureCleaner if object can be serialized,FLINK-31919,13533807,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/Apr/23 09:34,20/Jun/23 13:44,04/Jun/24 20:41,13/Jun/23 21:11,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,API / Core,,,,,,0,pull-request-available,,,,,"Given an object the ClosureCleaner currently recursively digs into every non-static/transient field of the given object. This causes a problem on Java 17 because these reflective accesses all need to be explicitly allowed beforehand.

Instead, we could limit the CC to objects that fail serialization, because if something can be serialized there isn't anything for the CC to do.
This should allow us to avoid a lot of unnecessary reflection accesses to immutable JDK classes, like Strings/BigDecimals etc etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30455,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 13 21:11:21 UTC 2023,,,,,,,,,,"0|z1hhkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 21:11;chesnay;master: 50f069ce5b1203df8acd724d804afc33eda8a9d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Source does not failing build against Flink 1.18 on nightly CI,FLINK-31918,13533804,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dannycranmer,dannycranmer,24/Apr/23 09:05,25/Apr/23 04:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,,0,,,,,,"[https://github.com/apache/flink-connector-pulsar/actions/runs/4783897408/jobs/8504710249]

 
{{Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project flink-connector-pulsar: Compilation failure }}
{{[150|https://github.com/apache/flink-connector-pulsar/actions/runs/4783897408/jobs/8504710249#step:13:151]Error:  /home/runner/work/flink-connector-pulsar/flink-connector-pulsar/flink-connector-pulsar/src/main/java/org/apache/flink/connector/pulsar/source/reader/PulsarSourceFetcherManager.java:[52,8] org.apache.flink.connector.pulsar.source.reader.PulsarSourceFetcherManager is not abstract and does not override abstract method removeSplits(java.util.List<org.apache.flink.connector.pulsar.source.split.PulsarPartitionSplit>) in org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 04:35:41 UTC 2023,,,,,,,,,,"0|z1hhk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 04:35;Weijie Guo;This compile problem should already be fixed in https://github.com/apache/flink-connector-pulsar/pull/43. 

As for the status of CI has not become failed, it needs to be resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Loss of Idempotence in JsonSerDe Round Trip for AggregateCall and RexNode,FLINK-31917,13533800,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,24/Apr/23 08:37,16/May/23 16:31,04/Jun/24 20:41,16/May/23 16:31,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"JsonSerDeTestUtil#testJsonRoundTrip only checks the equality between spec and deserialized object. Some corner cases are detected when serializing the deserialized object again.
{code:java}
static <T> T testJsonRoundTrip(SerdeContext serdeContext, T spec, Class<T> clazz)
            throws IOException {
        String actualJson = toJson(serdeContext, spec);
        T actual = toObject(serdeContext, actualJson, clazz);

        assertThat(actual).isEqualTo(spec);
        assertThat(actualJson).isEqualTo(toJson(serdeContext, actual)); // this will eval some corner cases
        return actual;
    }
{code}
The discovered corner cases are listed as follows.
h5. 1. SerDe for AggregateCall

When deserializing the aggregate call, we should check the JsonNodeType to avoid converting null to ""null"" string.
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/serde/AggregateCallJsonDeserializer.java#L64]
h5. Suggested Fix
{code:java}
JsonNode nameNode = jsonNode.required(FIELD_NAME_NAME);
final String name = JsonNodeType.NULL ? null : nameNode.asText();
{code}
h5. 2. SerDe for RexNode

RexNodeJsonSerdeTest#testSystemFunction should create the temporary system function instead of the temporary catalog function.
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/serde/RexNodeJsonSerdeTest.java#L209]
h5. Suggested Fix

Use functionCatalog#registerTemporarySystemFunction to test.
h5. 3. About RexLiteral type

RexNodeJsonSerdeTest#testRexNodeSerde has a test spec as follows
{code:java}
//This will create the literal with DOUBLE as the literal type, and DECIMAL as the broad type of this literal. You can refer to Calcite for more details
rexBuilder.makeExactLiteral(BigDecimal.valueOf(Double.MAX_VALUE), FACTORY.createSqlType(SqlTypeName.DOUBLE))
{code}
The RexNodeJsonSerializer uses `typeName`(which is DECIMAL) as the literal's type, as a result, the rel data type is serialized as double, but the value is serialized as a string (in case lost the precision)
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/serde/RexNodeJsonSerializer.java#L197]

And then, during the deserialization, according to the JSON, the deserialized literal will assign DOUBLE as the literal type and the broad type of the literal.
This will cause the comparison failure
{code:java}
expected: {""kind"": ""LITERAL"", ""value"": ""1.7976931348623157E+308""}
actual: {""kind"": ""LITERAL"", ""value"": 1.7976931348623157E+308}
{code}
h5. Suggested Fix

SARG is a special case and can be coped first, and for the rest type, we can use literal.getType().getSqlTypeName() instead of literal.getTypeName().
{code:java}
// first cope with SARG type
if (literal.getTypeName() == SARG) {
    serializeSargValue(
        (Sarg<?>) value, literal.getType().getSqlTypeName(), gen, serializerProvider);
} else {
    serializeLiteralValue(
        value,
        literal.getType().getSqlTypeName(),
        gen,
        serializerProvider);
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25217,,,,,,,,,,,,FLINK-31791,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 16 16:31:12 UTC 2023,,,,,,,,,,"0|z1hhjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 10:46;godfrey;good catch [~qingyue] ;;;","04/May/23 02:28;fsk119;Merged into master: 333e023196d90d265c286632f8c01c41b8911ef8;;;","16/May/23 16:31;qingyue;Merged into master: 333e023196d90d265c286632f8c01c41b8911ef8;;;",,,,,,,,,,,,,,,,,,,,,,,,
Python API only respects deprecated env.java.opts key,FLINK-31916,13533797,13281165,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/Apr/23 08:35,14/Jun/23 12:25,04/Jun/24 20:41,14/Jun/23 12:25,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,API / Python,Runtime / Configuration,,,,,0,pull-request-available,,,,,"pyflink_gateway_server.py is only reading the deprecated env.java.opts from the configuration.

This key should only be used as a fallback, with env.java.opts.tm/jm/client being the actual keys to support.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 14 12:25:58 UTC 2023,,,,,,,,,,"0|z1hhio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 12:25;dianfu;Merged to master via 626d70d86d3bee944edf548606b38042f31e09b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Python API incorrectly passes env.java.opts as single argument,FLINK-31915,13533793,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/Apr/23 08:28,25/Apr/23 08:36,04/Jun/24 20:41,25/Apr/23 08:36,1.16.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,API / Python,,,,,,0,pull-request-available,,,,,"The python API passes all java options as a single string argument, which typically means that the JVM will reject them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 08:36:23 UTC 2023,,,,,,,,,,"0|z1hhhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 08:36;chesnay;master: b462b9f2da86715b66dbc4f0701c0574e672ff2c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Failing to close FlinkKafkaInternalProducer created in KafkaWriter with exactly-once semantic results in memory leak,FLINK-31914,13533791,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,datariver,datariver,24/Apr/23 08:24,25/Apr/23 06:19,04/Jun/24 20:41,25/Apr/23 06:19,1.15.0,,,,,,,,,,,,,,,,,,,,1.15.2,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"Hi [~arvid] , If Exactly-Once writing is enabled, Kafka's transactional writing will be used. KafkaWriter will create FlinkKafkaInternalProducer in the initialization and snapshotState methods, but there is no place to close it. As Checkpoints increase, Producers will continue to accumulate. Each Producer maintains a Buffer, which will cause memory leaks and Job OOM.
By dumping an in-memory instance of Task Manager, you can see that there are a lot of Producers:

!image-2023-04-25-13-47-25-703.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/23 05:47;datariver;image-2023-04-25-13-47-25-703.png;https://issues.apache.org/jira/secure/attachment/13057535/image-2023-04-25-13-47-25-703.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 06:19:01 UTC 2023,,,,,,,,,,"0|z1hhhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 02:09;chouc;{color:#000000}The FlinkKafkaInternalProducer is registered in Closer and closer is closed when Flink Writer invoke close.

{color};;;","25/Apr/23 06:17;datariver;[~chouc] Thanks for the reply, I found that the 1.15.2 version has been fixed.;;;","25/Apr/23 06:19;datariver;相关记录：https://issues.apache.org/jira/browse/FLINK-28250;;;",,,,,,,,,,,,,,,,,,,,,,,,
sql-client.sh does not respect env.java.opts.all/client,FLINK-31913,13533790,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/Apr/23 08:24,11/May/23 18:55,04/Jun/24 20:41,11/May/23 18:55,1.17.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Deployment / Scripts,Table SQL / Client,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 11 18:55:51 UTC 2023,,,,,,,,,,"0|z1hhh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/23 18:55;chesnay;master: 49a5810f0eafad277f81c652435842e65b632169;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade bytebuddy to 14.4.1,FLINK-31912,13533789,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/Apr/23 08:19,25/Apr/23 14:40,04/Jun/24 20:41,25/Apr/23 14:40,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 14:40:32 UTC 2023,,,,,,,,,,"0|z1hhgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 14:40;chesnay;master: 4d50f78cf99ebdf1c57d225ac26904abd8b4bc64;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Bad address construction in SqlClientTest,FLINK-31911,13533787,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/Apr/23 08:14,27/Apr/23 12:17,04/Jun/24 20:41,27/Apr/23 12:17,1.16.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Client,Tests,,,,,0,pull-request-available,,,,,"The SqlClientTest constructs a host:port pair with this:

{code}
InetSocketAddress.createUnresolved(
                                    SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetAddress(),
                                    SQL_GATEWAY_REST_ENDPOINT_EXTENSION.getTargetPort())
                            .toString()
{code}

This is unnecessarily complicated and fails on Java 17 because the toString representation is _not_ guaranteed to return something of the form host:port.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 27 12:17:31 UTC 2023,,,,,,,,,,"0|z1hhgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 12:17;chesnay;master: 146847e53f10967dc2133cfdee6adbc57bccdd38;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Using BroadcastUtils#withBroadcast in iteration perround mode got stuck,FLINK-31910,13533785,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,zhangzp,zhangzp,24/Apr/23 07:49,29/Jun/23 10:19,04/Jun/24 20:41,29/Jun/23 10:19,ml-2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,,0,,,,,,"Using BroadcastUtils#withBroadcast in iteration perround mode got stuck. From the thread dump, it seems that the head operator and criteria node are stuck and waiting for a mail.

 
{code:java}
""output-head-Parallel Collection Source -> Sink: Unnamed (4/4)#0"" #228 prio=5 os_prio=31 tid=0x00007f9e1d083800 nid=0x19b07 waiting on condition [0x0000700013db6000]
   java.lang.Thread.State: TIMED_WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x0000000747a83270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
    at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:335)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task$$Lambda$1383/280145505.run(Unknown Source)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748) {code}
 

The demo for this bug could be found here: https://github.com/zhipeng93/flink-ml/tree/FLINK-31910-demo-case",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 18 08:19:48 UTC 2023,,,,,,,,,,"0|z1hhg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/23 08:19;Jiang Xin;From the demo, I'm not sure if the `BroadcastUtils.withBroadcastStream` mix up the order of the two parameters, because it is not broadcasting the `variableStreams`. I reversed the two parameters and the demo works well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Using BroadcastUtils#withBroadcast in iteration perround mode got stuck,FLINK-31909,13533784,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhangzp,zhangzp,24/Apr/23 07:43,29/Jun/23 10:48,04/Jun/24 20:41,24/Apr/23 07:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,,0,,,,,,"Using BroadcastUtils#withBroadcastStream in iterations in per round mode could possibly lead to stuck. 

 

It seems that the there is a task waiting for the mail from the mailbox.

 
{code:java}
 793    ""tail-map-head-Parallel Collection Source (1/1)#0"" #200 prio=5 os_prio=31 tid=0x00007faabb571800 nid=0x18c03 waiting on condition [0x0000700013aae000]
 793    java.lang.Thread.State: TIMED_WAITING (parking)
 794     at sun.misc.Unsafe.park(Native Method)
 795     - parking to wait for  <0x0000000747805568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
 796     at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
 797     at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)
 798     at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:149)
 799     at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:335)
 800     at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
 801     at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
 802     at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
 803     at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
 804     at org.apache.flink.runtime.taskmanager.Task$$Lambda$1430/1226027100.run(Unknown Source)
 805     at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
 806     at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
 807     at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
 808     at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
 809     at java.lang.Thread.run(Thread.java:748) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 24 07:52:13 UTC 2023,,,,,,,,,,"0|z1hhfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 07:52;zhangzp;Duplicate with https://issues.apache.org/jira/browse/FLINK-31910.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
cast expr to type with not null  should not change nullable of expr,FLINK-31908,13533783,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,24/Apr/23 07:43,25/May/23 10:02,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"{code:java}
Stream<TestSetSpec> getTestSetSpecs() {
    return Stream.of(
            TestSetSpec.forFunction(BuiltInFunctionDefinitions.CAST)
                    .onFieldsWithData(new Integer[]{1, 2}, 3)
                    .andDataTypes(DataTypes.ARRAY(INT()), INT())
                    .testSqlResult(
                            ""CAST(f0 AS ARRAY<DOUBLE NOT NULL>)"",
                            new Double[]{1.0d, 2.0d},
                            DataTypes.ARRAY(DOUBLE().notNull())));
} {code}
but the result type should DataTypes.ARRAY(DOUBLE())), the root cause is calcite bug",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 25 10:02:29 UTC 2023,,,,,,,,,,"0|z1hhfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 09:50;jark;Why the return type shouldn't be {{ARRAY<DOUBLE NOT NULL>}}?;;;","24/Apr/23 10:20;jackylau;hi [~jark] cast only make type take effective. and nullable should be use original expr's nullable.;;;","25/Apr/23 02:50;jackylau;hi [~jark] i will fix in calcite first here https://issues.apache.org/jira/browse/CALCITE-5674;;;","04/May/23 09:40;jackylau;[~jark] 

why i found this is here i supports the flink builtin functions like array_union [https://github.com/apache/flink/pull/22483/files]

for example in it
f0 is [1, 2, null] and type is INTEGER ARRAY
array_union(f0, [1.0E0, NULL, 4.0E0]) => it should return [1.0, 2.0, NULL, 4.0] but not.

the reasion is following  
{code:java}
[1, 2, null] => INTEGER ARRAY
[1.0E0, NULL, 4.0E0] => DOUBLE NOT NULL ARRAY NOT NULL

so the array_union function type is the common of the both DOUBLE ARRAY.
so the flink will insert implicit cast for f0, 
the cast should like cast(f0 as DOUBLE ARRAY), but it return cast(f0 as DOUBLE NOT NULL ARRAY),

this will cast [1, 2, null] -> [1.0, 2.0. 0.0] in the runtime eval method, make the result not correct. {code}
why it return cast(f0 as DOUBLE NOT NULL ARRAY)?
the root cause is calcite not transitive the nullable for array element;;;","25/May/23 10:02;jackylau;i fixed it in calcite, it will fix when upgrade calcite to 1.35;;;",,,,,,,,,,,,,,,,,,,,,,
Remove unused fields inside of ExecutionSlotSharingGroupBuilder,FLINK-31907,13533781,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,24/Apr/23 07:28,25/Apr/23 03:06,04/Jun/24 20:41,25/Apr/23 03:06,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"FLINK-22767 introduced `availableGroupsForJobVertex` to improve the performance during task to slot scheduler.

After FLINK-22767, the `executionSlotSharingGroups`[2] is unused, and it can be removed.

 

[1] https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/LocalInputPreferredSlotSharingStrategy.java#L153

[2] https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/LocalInputPreferredSlotSharingStrategy.java#L136",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 03:06:45 UTC 2023,,,,,,,,,,"0|z1hhf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 03:06;fanrui;Thanks [~wanglijie] for the review.

master(1.18) via ed42b094496ffaa503d3add44a91fad5669c9dd7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
typeof should only return type exclude nullable ,FLINK-31906,13533779,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,24/Apr/23 07:21,28/Apr/23 11:47,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"nullable is table level constraint, which can only show by showing schema

 

pg [https://www.postgresql.org/docs/9.3/functions-info.html]

spark ：https://spark.apache.org/docs/latest/api/sql/index.html#typeof
{code:java}
// code placeholder
select typeof(1Y), typeof(1S), typeof(1), typeof(1L)
-- !query schema
struct<typeof(1):string,typeof(1):string,typeof(1):string,typeof(1):string>
-- !query output
tinyint    smallint   int    bigint


-- !query
select typeof(cast(1.0 as float)), typeof(1.0D), typeof(1.2)
-- !query schema
struct<typeof(CAST(1.0 AS FLOAT)):string,typeof(1.0):string,typeof(1.2):string>
-- !query output
float  double decimal(2,1) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 28 11:47:44 UTC 2023,,,,,,,,,,"0|z1hheo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 08:38;jark;What's the problem of returning data type with nullability?  We should be careful when deciding to make such breaking changes. ;;;","28/Apr/23 11:47;martijnvisser;+1 to Jark's comment;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Exception thrown when accessing nested field of the result of Python UDF with complex result type,FLINK-31905,13533777,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianfu,dianfu,24/Apr/23 07:20,24/Apr/23 07:24,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"For the following job:
{code}
import logging, sys

from pyflink.common import Row
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import Schema, DataTypes, TableDescriptor, StreamTableEnvironment
from pyflink.table.expressions import col, row
from pyflink.table.udf import ACC, T, udaf, AggregateFunction, udf

logging.basicConfig(stream=sys.stdout, level=logging.ERROR, format=""%(message)s"")


class EmitLastState(AggregateFunction):
    """"""
    Aggregator that emits the latest state for the purpose of
    enabling parallelism on CDC tables.
    """"""

    def create_accumulator(self) -> ACC:
        return Row(None, None)

    def accumulate(self, accumulator: ACC, *args):
        key, obj = args
        if (accumulator[0] is None) or (key > accumulator[0]):
            accumulator[0] = key
            accumulator[1] = obj

    def retract(self, accumulator: ACC, *args):
        pass

    def get_value(self, accumulator: ACC) -> T:
        return accumulator[1]


some_complex_inner_type = DataTypes.ROW(
    [
        DataTypes.FIELD(""f0"", DataTypes.STRING()),
        DataTypes.FIELD(""f1"", DataTypes.STRING())
    ]
)

some_complex_type = DataTypes.ROW(
    [
        DataTypes.FIELD(k, DataTypes.ARRAY(some_complex_inner_type))
        for k in (""f0"", ""f1"", ""f2"")
    ]
    + [
        DataTypes.FIELD(""f3"", DataTypes.DATE()),
        DataTypes.FIELD(""f4"", DataTypes.VARCHAR(32)),
        DataTypes.FIELD(""f5"", DataTypes.VARCHAR(2)),
    ]
)

@udf(input_types=DataTypes.STRING(), result_type=some_complex_type)
def complex_udf(s):
    return Row(f0=None, f1=None, f2=None, f3=None, f4=None, f5=None)


if __name__ == ""__main__"":
    env = StreamExecutionEnvironment.get_execution_environment()
    table_env = StreamTableEnvironment.create(env)
    table_env.get_config().set('pipeline.classpaths', 'file:///Users/dianfu/code/src/workspace/pyflink-examples/flink-sql-connector-postgres-cdc-2.1.1.jar')

    # Create schema
    _schema = {
        ""p_key"": DataTypes.INT(False),
        ""modified_id"": DataTypes.INT(False),
        ""content"": DataTypes.STRING()
    }
    schema = Schema.new_builder().from_fields(
        *zip(*[(k, v) for k, v in _schema.items()])
    ).\
        primary_key(""p_key"").\
        build()

    # Create table descriptor
    descriptor = TableDescriptor.for_connector(""postgres-cdc"").\
        option(""hostname"", ""host.docker.internal"").\
        option(""port"", ""5432"").\
        option(""database-name"", ""flink_issue"").\
        option(""username"", ""root"").\
        option(""password"", ""root"").\
        option(""debezium.plugin.name"", ""pgoutput"").\
        option(""schema-name"", ""flink_schema"").\
        option(""table-name"", ""flink_table"").\
        option(""slot.name"", ""flink_slot"").\
        schema(schema).\
        build()

    table_env.create_temporary_table(""flink_table"", descriptor)

    # Create changelog stream
    stream = table_env.from_path(""flink_table"")\

    # Define UDAF
    accumulator_type = DataTypes.ROW(
        [
            DataTypes.FIELD(""f0"", DataTypes.INT(False)),
            DataTypes.FIELD(""f1"", DataTypes.ROW([DataTypes.FIELD(k, v) for k, v in _schema.items()])),
        ]
    )
    result_type = DataTypes.ROW([DataTypes.FIELD(k, v) for k, v in _schema.items()])
    emit_last = udaf(EmitLastState(), accumulator_type=accumulator_type, result_type=result_type)

    # Emit last state based on modified_id to enable parallel processing
    stream = stream.\
        group_by(col(""p_key"")).\
        select(
        col(""p_key""),
        emit_last(col(""modified_id""),row(*(col(k) for k in _schema.keys())).cast(result_type)).alias(""tmp_obj"")
    )

    # Select the elements of the objects
    stream = stream.select(*(col(""tmp_obj"").get(k).alias(k) for k in _schema.keys()))

    # We apply a UDF which parses the xml and returns a complex nested structure
    stream = stream.select(col(""p_key""), complex_udf(col(""content"")).alias(""nested_obj""))

    # We select an element from the nested structure in order to flatten it
    # The next line is the line causing issues, commenting the next line will make the pipeline work
    stream = stream.select(col(""p_key""), col(""nested_obj"").get(""f0""))

    # Interestingly, the below part does work...
    # stream = stream.select(col(""nested_obj"").get(""f0""))

    table_env.to_changelog_stream(stream).print()

    # Execute
    env.execute_async()
{code}

{code}
py4j.protocol.Py4JJavaError: An error occurred while calling o8.toChangelogStream.
: java.lang.IndexOutOfBoundsException: Index 1 out of bounds for length 1
        at java.base/jdk.internal.util.Preconditions.outOfBounds(Unknown Source)
        at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Unknown Source)
        at java.base/jdk.internal.util.Preconditions.checkIndex(Unknown Source)
        at java.base/java.util.Objects.checkIndex(Unknown Source)
        at java.base/java.util.ArrayList.get(Unknown Source)
        at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitLocalRef(RexProgramBuilder.java:975)
        at org.apache.calcite.rex.RexProgramBuilder$RegisterInputShuttle.visitLocalRef(RexProgramBuilder.java:924)
        at org.apache.calcite.rex.RexLocalRef.accept(RexLocalRef.java:75)
        at org.apache.calcite.rex.RexShuttle.visitFieldAccess(RexShuttle.java:198)
        at org.apache.calcite.rex.RexProgramBuilder$RegisterShuttle.visitFieldAccess(RexProgramBuilder.java:904)
        at org.apache.calcite.rex.RexProgramBuilder$RegisterShuttle.visitFieldAccess(RexProgramBuilder.java:887)
        at org.apache.calcite.rex.RexFieldAccess.accept(RexFieldAccess.java:92)
        at org.apache.calcite.rex.RexShuttle.visitList(RexShuttle.java:158)
        at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:110)
        at org.apache.calcite.rex.RexProgramBuilder$RegisterShuttle.visitCall(RexProgramBuilder.java:889)
        at org.apache.calcite.rex.RexProgramBuilder$RegisterShuttle.visitCall(RexProgramBuilder.java:887)
        at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
        at org.apache.calcite.rex.RexShuttle.visitList(RexShuttle.java:158)
        at org.apache.calcite.rex.RexShuttle.visitCall(RexShuttle.java:110)
        at org.apache.calcite.rex.RexProgramBuilder$RegisterShuttle.visitCall(RexProgramBuilder.java:889)
        at org.apache.calcite.rex.RexProgramBuilder$RegisterShuttle.visitCall(RexProgramBuilder.java:887)
        at org.apache.calcite.rex.RexCall.accept(RexCall.java:174)
        at org.apache.calcite.rex.RexProgramBuilder.registerInput(RexProgramBuilder.java:295)
        at org.apache.calcite.rex.RexProgramBuilder.addProject(RexProgramBuilder.java:206)
        at org.apache.calcite.rex.RexProgram.create(RexProgram.java:224)
        at org.apache.calcite.rex.RexProgram.create(RexProgram.java:193)
        at org.apache.flink.table.planner.plan.rules.logical.PythonCalcSplitRuleBase.onMatch(PythonCalcSplitRule.scala:98)
        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
        at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:64)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:78)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$2(FlinkGroupProgram.scala:59)
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
        at scala.collection.Iterator.foreach(Iterator.scala:937)
        at scala.collection.Iterator.foreach$(Iterator.scala:937)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
        at scala.collection.IterableLike.foreach(IterableLike.scala:70)
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
        at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
        at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1(FlinkGroupProgram.scala:56)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1$adapted(FlinkGroupProgram.scala:51)
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
        at scala.collection.immutable.Range.foreach(Range.scala:155)
        at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
        at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
        at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:51)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
        at scala.collection.Iterator.foreach(Iterator.scala:937)
        at scala.collection.Iterator.foreach$(Iterator.scala:937)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
        at scala.collection.IterableLike.foreach(IterableLike.scala:70)
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
        at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
        at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
        at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
        at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
        at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
        at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
        at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195)
        at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
        at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
        at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toChangelogStream(StreamTableEnvironmentImpl.java:263)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.base/java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
        at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
        at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.base/java.lang.Thread.run(Unknown Source)
{code}

PS: This issue is reported in https://apache-flink.slack.com/archives/C03G7LJTS2G/p1681918854457699",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-24 07:20:38.0,,,,,,,,,,"0|z1hhe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix current serveral flink nullable type handle,FLINK-31904,13533776,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,24/Apr/23 07:19,24/Apr/23 07:45,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31902,FLINK-31906,FLINK-31908,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 24 07:45:53 UTC 2023,,,,,,,,,,"0|z1hhe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 07:45;jackylau;hi [~danny0405] [~twalthr] [~snuyanzin] , i create three subtask to fix these problem,

do you have time to have a look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Caching records fails in BroadcastUtils#withBroadcastStream,FLINK-31903,13533775,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,zhangzp,zhangzp,24/Apr/23 07:09,11/May/23 12:26,04/Jun/24 20:41,11/May/23 12:26,ml-2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,,0,,,,,,"When caching more than 1,000,000 records using BroadcastUtils#withBroadcast, it throws exception as follows:
{code:java}
Caused by: org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
    at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(CheckpointFailureManager.java:206)
    at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:191)
    at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:124)
    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2078)
    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1038)
    at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103)
    at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
 {code}
It seems that the bug comes from caching too many records when calling 

AbstractBroadcastWrapperOperator#snapshot. 

 

The failed case could be found here: [https://github.com/zhipeng93/flink-ml/tree/FLINK-31903-fail-case]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 11 12:26:16 UTC 2023,,,,,,,,,,"0|z1hhds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/23 12:26;zhangzp;The root cause is that using memory as statebackend has limited state size. So this is not a bug.

"" java.io.IOException: Size of the state is larger than the maximum permitted memory-backed state. Size=5365814, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage."".;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
cast expr to type with not null should throw exception like calcite,FLINK-31902,13533769,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,24/Apr/23 06:51,30/May/23 08:19,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"{code:java}
// calcite cast type  not null, will throw exception

expr(""cast(x as int ^not^ null)"")
        .fails(""(?s).*Encountered \""not\"" at .*"");
expr(""cast(x as int ^not^ null array)"")
        .fails(""(?s).*Encountered \""not\"" at .*"");
expr(""cast(x as int array ^not^ null)"")
        .fails(""(?s).*Encountered \""not\"" at .*""); 

// while  the flink not
expr(""cast(x as array<int not null>)"")
        .ok(""(?s).*Encountered \""not\"" at .*"");
expr(""cast(x as array<int> not null)"")
        .ok(""(?s).*Encountered \""not\"" at .*"");{code}
the reason is flink add extended type, which will supports not null
{code:java}
// code placeholder
<#-- additional types are included here -->
<#-- put custom data types in front of Calcite core data types -->
<#list (parser.dataTypeParserMethods!default.parser.dataTypeParserMethods) as method>
        LOOKAHEAD(2)
        typeNameSpec = ${method}
    |
</#list> {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 30 08:19:18 UTC 2023,,,,,,,,,,"0|z1hhcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 06:55;jackylau;hi [~danny0405] [~snuyanzin]  please have a look;;;","24/Apr/23 08:41;jark;What's the problem of supporting CAST to not null types?;;;","24/Apr/23 09:59;jackylau;hi [~jark]  thanks for your response, it is not sql standard. not null is constraint info in ddl, and should not be changed by cast in sql standard.

it will make strange like here https://issues.apache.org/jira/browse/FLINK-31908;;;","24/Apr/23 10:02;jackylau;the standard grammar it here from iso 2016
{code:java}
// code placeholder
<cast specification> ::=
CAST <left paren> <cast operand> AS <cast target> <right paren>
<cast operand> ::=
<value expression>
| <implicitly typed value specification>
<cast target> ::=
<domain name>
| <data type>

<data type> ::=
<predefined type>
| <row type>
| <path-resolved user-defined type name>
| <reference type>
| <collection type>


<collection type> ::=
<array type>
| <multiset type>
<array type> ::=
<data type> ARRAY
[ <left bracket or trigraph> <maximum cardinality> <right bracket or trigraph> ] {code};;;","24/Apr/23 10:12;jark;Thank you for the explanation. ;;;","30/May/23 08:19;jackylau;i will fix it in calcite first https://issues.apache.org/jira/browse/CALCITE-5731;;;",,,,,,,,,,,,,,,,,,,,,
AbstractBroadcastWrapperOperator should not block checkpoint barriers when processing cached records,FLINK-31901,13533760,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhangzp,zhangzp,24/Apr/23 06:16,21/Jun/23 10:33,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,ml-2.4.0,,,,,,,,,Library / Machine Learning,,,,,,0,,,,,,"Currently `BroadcastUtils#withBroadcast` tries to caches the non-broadcast input until the broadcast inputs are all processed. After the broadcast variables are ready, we first process the cached records and then continue to process the newly arrived records.

 

Processing cached elements is invoked via `Input#processElement` and `Input#processWatermark`.  However, processing cached element may take a long time since there may be many cached records, which could potentially block the checkpoint barrier.

 

If we run the code snippet here[1], we are supposed to get logs as follows.
{code:java}
OneInputBroadcastWrapperOperator doing checkpoint with checkpoint id: 1 at time: 1682319149462
processed cached records, cnt: 10000 at time: 1682319149569
processed cached records, cnt: 20000 at time: 1682319149614
processed cached records, cnt: 30000 at time: 1682319149655
processed cached records, cnt: 40000 at time: 1682319149702
processed cached records, cnt: 50000 at time: 1682319149746
processed cached records, cnt: 60000 at time: 1682319149781
processed cached records, cnt: 70000 at time: 1682319149891
processed cached records, cnt: 80000 at time: 1682319150011
processed cached records, cnt: 90000 at time: 1682319150116
processed cached records, cnt: 100000 at time: 1682319150199
OneInputBroadcastWrapperOperator doing checkpoint with checkpoint id: 2 at time: 1682319150378
OneInputBroadcastWrapperOperator doing checkpoint with checkpoint id: 3 at time: 1682319150606
OneInputBroadcastWrapperOperator doing checkpoint with checkpoint id: 4 at time: 1682319150704
OneInputBroadcastWrapperOperator doing checkpoint with checkpoint id: 5 at time: 1682319150785
OneInputBroadcastWrapperOperator doing checkpoint with checkpoint id: 6 at time: 1682319150859
OneInputBroadcastWrapperOperator doing checkpoint with checkpoint id: 7 at time: 1682319150935
OneInputBroadcastWrapperOperator doing checkpoint with checkpoint id: 8 at time: 1682319151007{code}
 

We can find that from line#2 to line#11, there is no checkpoints and the barriers are blocked until all cached elements are processed, which takes ~600ms and much longer than checkpoint interval (i.e., 100ms)

 

  [1]https://github.com/zhipeng93/flink-ml/tree/FLINK-31901-demo-case",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-24 06:16:11.0,,,,,,,,,,"0|z1hhag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix some typo in java doc, comments and assertion message",FLINK-31900,13533744,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,24/Apr/23 03:11,28/Apr/23 11:49,04/Jun/24 20:41,25/Apr/23 02:50,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,As the title.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 28 11:49:00 UTC 2023,,,,,,,,,,"0|z1hh6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 02:50;Weijie Guo;master(1.18) via e21de2dbaeb6b624b3c1f9e5c204743d81841a86.;;;","25/Apr/23 02:50;Feifan Wang;Thanks [~Weijie Guo] for review and merge the PR !;;;","28/Apr/23 11:49;martijnvisser;[~Feifan Wang] Keep in mind for future PRs that documentation fixes like these don't require a Jira. ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Flink version of Flink ML to 1.17.0,FLINK-31899,13533743,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,24/Apr/23 03:02,28/Apr/23 12:14,04/Jun/24 20:41,28/Apr/23 12:14,,,,,,,,,,,,,,,,,,,,,ml-2.3.0,,,,,,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,,,Upgrade Flink version of Flink ML to 1.17.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 28 12:13:53 UTC 2023,,,,,,,,,,"0|z1hh6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 12:13;lindong;Merged to apache/flink-ml master branch a5b7b493e1d7967a7fcc8054a24132ddf02b69f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Flink k8s autoscaler does not work as expected,FLINK-31898,13533740,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,lufovic77,lufovic77,24/Apr/23 01:59,26/Apr/23 06:27,04/Jun/24 20:41,26/Apr/23 06:27,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,,,,,,"Hi I'm using Flink k8s autoscaler to automatically deploy jobs in proper parallelism.

I was using 1.4 version but I found that it does not scale down properly because TRUE_PROCESSING_RATE becoming NaN when the tasks are idled.

In the main branch, I checked the code was fixed to set TRUE_PROCESSING_RATE to positive infinity and make scaleFactor to very low value so I'm now experimentally using docker image built with main branch of Flink-k8s-operator repository in my job.

It now scales down properly but the problem is, it does not converge to the optimal parallelism. It scales down well but it jumps up again to high parallelism. 

 

Below is the experimental setup and my figure of parallelism changes result.
 * about 40 RPS
 * each task can process 10 TPS (intended throttling)

!image-2023-04-24-10-54-58-083.png|width=999,height=266!

Even using default configuration leads to the same result. What can I do more? Thank you.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/23 01:54;lufovic77;image-2023-04-24-10-54-58-083.png;https://issues.apache.org/jira/secure/attachment/13057500/image-2023-04-24-10-54-58-083.png","24/Apr/23 04:27;lufovic77;image-2023-04-24-13-27-17-478.png;https://issues.apache.org/jira/secure/attachment/13057503/image-2023-04-24-13-27-17-478.png","24/Apr/23 04:28;lufovic77;image-2023-04-24-13-28-15-462.png;https://issues.apache.org/jira/secure/attachment/13057504/image-2023-04-24-13-28-15-462.png","24/Apr/23 04:31;lufovic77;image-2023-04-24-13-31-06-420.png;https://issues.apache.org/jira/secure/attachment/13057505/image-2023-04-24-13-31-06-420.png","24/Apr/23 04:41;lufovic77;image-2023-04-24-13-41-43-040.png;https://issues.apache.org/jira/secure/attachment/13057506/image-2023-04-24-13-41-43-040.png","24/Apr/23 04:42;lufovic77;image-2023-04-24-13-42-40-124.png;https://issues.apache.org/jira/secure/attachment/13057507/image-2023-04-24-13-42-40-124.png","24/Apr/23 04:43;lufovic77;image-2023-04-24-13-43-49-431.png;https://issues.apache.org/jira/secure/attachment/13057508/image-2023-04-24-13-43-49-431.png","24/Apr/23 04:44;lufovic77;image-2023-04-24-13-44-17-479.png;https://issues.apache.org/jira/secure/attachment/13057509/image-2023-04-24-13-44-17-479.png","24/Apr/23 05:18;lufovic77;image-2023-04-24-14-18-12-450.png;https://issues.apache.org/jira/secure/attachment/13057511/image-2023-04-24-14-18-12-450.png","24/Apr/23 07:47;lufovic77;image-2023-04-24-16-47-35-697.png;https://issues.apache.org/jira/secure/attachment/13057513/image-2023-04-24-16-47-35-697.png",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 06:27:02 UTC 2023,,,,,,,,,,"0|z1hh60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 03:49;Zhanghao Chen;[~lufovic77] Could you provide more details about the job: the metrics of the operator input/output RPS、operator busy time?;;;","24/Apr/23 04:47;lufovic77;[~Zhanghao Chen] 

Hi metric data I provided above are expired now so I'll give you the new experiment information.

Experiment is done between *10:30 to 12:00* and I used Kafka as a data source. 

!image-2023-04-24-13-27-17-478.png|width=249,height=216!!image-2023-04-24-13-28-15-462.png|width=463,height=86!!image-2023-04-24-13-31-06-420.png|width=301,height=125!

Please see when scaling to new parallelism and lag count fluctuate.

As I described already, one task can process 10 TPS and I'm making about 35 RPS. 

I'm not sure why autoscaler does not stop when it reaches parallelism 4 (4->8 scaling happens around 11:20).

 

busy time for the map operator

!image-2023-04-24-13-41-43-040.png|width=268,height=201!

in/out per second for map operator

!image-2023-04-24-13-42-40-124.png|width=686,height=250!

in/out per second for source operator

!image-2023-04-24-14-18-12-450.png|width=673,height=248!

some metrics that might help.

!image-2023-04-24-13-43-49-431.png|width=307,height=265!!image-2023-04-24-13-44-17-479.png|width=277,height=265!

Sorry for the too many data. Let me know if you need more metrics.

Last but not least my flink operator's configuration.
{code:java}
pipeline.max-parallelism: ""8""
kubernetes.operator.job.autoscaler.enabled: ""true""
kubernetes.operator.job.autoscaler.stabilization.interval: 6m
kubernetes.operator.job.autoscaler.metrics.window: 3m
kubernetes.operator.job.autoscaler.target.utilization: ""0.6""
kubernetes.operator.job.autoscaler.target.utilization.boundary: ""0.3""
kubernetes.operator.job.autoscaler.restart.time: 2m
kubernetes.operator.job.autoscaler.catch-up.duration: 6m
kubernetes.operator.job.autoscaler.scale-down.max-factor: ""0.3"" {code}
Thank you so much and have a nice day!;;;","24/Apr/23 07:19;gyfora;cc [~mxm] ;;;","24/Apr/23 08:07;lufovic77;Hi I found that after upgrading flink version 1.16.1 to 1.17.0, the numRecordsIn/OutPerSecond metric of source operator doubled.

!image-2023-04-24-16-47-35-697.png|width=554,height=191!

I upgraded at 04/05.

Same issue posted below. 

 

https://issues.apache.org/jira/browse/FLINK-31752?jql=project%20%3D%20FLINK%20AND%20text%20~%20numrecordsout

 

I found that in 'computeTargetDataRate' method of main branch, the 'outputRateMultiplier' value is multiplied to the 'inputTargetRate' and I think it might effect on the 'TARGET_DATA_RATE' value which is used in calculating the scaleFactor (bigger than expected).

Again, I'm using the main branch(latest) not the 1.14.0 version.

I hope it helps. Thank you!

 

+ Even when I use the code fixing the bug above, the scale factor is decreased by half but autoscaler still increases the parallelism high and repeats the scale down again.;;;","25/Apr/23 06:17;gyfora;I am a bit confused here by all the graphs :) Based on some of your graphs the data rate fluctuates quite a lot.

But let's focus on the Flink operator side metrics and forget numrecordsin for a second.
It would be great to look at TRUE_PROCESSING_RATE, TARGET_DATA_RATE and the scale up/ down threshold metrics.
Also for this experiment please remove the `kubernetes.operator.job.autoscaler.scale-down.max-factor` config .

Not sure how many kafka partitions you have but pipeline.max-parallelism: ""8"" seems a bit limiting in the possible parallelism settings.
You could try max parallelism 120 and instead kubernetes.operator.job.autoscaler.vertex.max-parallelism: ""8"";;;","26/Apr/23 02:33;lufovic77;[~gyfora] 

After watching more metrics, I found out that the busyMsPerSecond metric does fluctuate a lot (It records only 1k or zero) and I think it results in incorrect TRUE_PROCESSING_RATE. 

It was because my test job throttles the number of record inputs per second.

I changed my job's behavior to allow all inputs, add some delay inside the map operator and change the configuration as you suggested. 

Autoscaler now works very well :). It finds the optimal parallelism. 

Sorry for the confusion and I think you can close the issue.

By the way can you let me know when you guys are planning to release 1.5 version? ;;;","26/Apr/23 06:27;gyfora;Thats good to hear [~lufovic77] I will close the ticket.

I am still waiting for some +1s on the dev list but if everyone agrees I will start preparing the 1.5.0 release next week so probably around the second week of may it should be released :) ;;;",,,,,,,,,,,,,,,,,,,,
Failing Unit Test: org.apache.flink.queryablestate.network.ClientTest.testRequestUnavailableHost,FLINK-31897,13533739,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,kurto,kurto,23/Apr/23 23:51,25/Apr/23 08:51,04/Jun/24 20:41,25/Apr/23 08:50,,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Runtime / Queryable State,,,,,,0,pull-request-available,,,,," 
 
{code:java}
[ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.612 s <<< FAILURE! - in org.apache.flink.queryablestate.network.ClientTest [ERROR] org.apache.flink.queryablestate.network.ClientTest.testRequestUnavailableHost Time elapsed: 0.006 s <<< FAILURE! java.lang.AssertionError: 
Expected: A CompletableFuture that will have failed within 3600000 milliseconds with: java.net.ConnectException but: Future completed with different exception: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedSocketException: Can't assign requested address: <redacted>/<redacted>:0 Caused by: java.net.BindException: Can't assign requested address  {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 08:47:08 UTC 2023,,,,,,,,,,"0|z1hh5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 02:33;tanyuxin;[~kurto] Thanks for reporting this, I'd like to take a look at this issue.;;;","25/Apr/23 08:47;chesnay;master: 918b873c0c0654b029140581c0ed49b44f9c5273
1.17: a0c734ffdecdb9fcbc31538c74f8da5762da8207
1.16: b3a52739c994494dd565db2c43963e5d15d79fbc;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Extend web interface to support failure labels,FLINK-31896,13533738,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,23/Apr/23 22:14,04/Jan/24 20:21,04/Jun/24 20:41,04/Jan/24 20:21,,,,,,,,,,,,,,,,,,,,,1.19.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,FLINK-31894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 04 20:21:39 UTC 2024,,,,,,,,,,"0|z1hh5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/24 20:21;dmvk;master: aba1ee85d6a3854fdb1f8a628fed0ad19460d086;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
End-to-end integration tests for failure labels,FLINK-31895,13533737,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangm92,pgaref,pgaref,23/Apr/23 22:14,18/Sep/23 08:50,04/Jun/24 20:41,18/Sep/23 08:50,,,,,,,,,,,,,,,,,,,,,1.19.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,FLINK-31894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32804,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 18 08:50:18 UTC 2023,,,,,,,,,,"0|z1hh5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 08:56;wangm92;[~pgaref] hi,  I have interested to take it, add a Demo about this inside Flink.;;;","06/Sep/23 15:51;pgaref;Totally fine by me [~wangm92] – let me know if you want to potentially split this in 2 tickets. One about testing and one adding the Demo;;;","07/Sep/23 02:33;wangm92;[~pgaref] Sorry, there is some ambiguity in my comment. Regarding the demo, I think [the document |https://issues.apache.org/jira/browse/FLINK-31889] should be enough, and we don't need to create another ticket. What I originally meant was that there will be a pluggable failure implementation in the E2E test as well, and users can refer to this implementation when they want to use this feature. Let's focus on the e2e test for this ticket for now.;;;","07/Sep/23 12:37;wangm92;[~pgaref] I would like to take this ticket if possible;;;","07/Sep/23 12:46;pgaref;Fine by me [~wangm92]  ;;;","18/Sep/23 08:50;huweihua;Resolved in master: d8630cb5db0608a630de95df0dd1d0c9f0b56aa2;;;",,,,,,,,,,,,,,,,,,,,,
ExceptionHistory and REST API failure label integration,FLINK-31894,13533736,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,23/Apr/23 22:13,02/Jun/23 09:13,04/Jun/24 20:41,02/Jun/23 09:13,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / REST,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31896,FLINK-31895,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32198,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 02 09:12:58 UTC 2023,,,,,,,,,,"0|z1hh54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 09:12;dmvk;master: 28c20ad70c7100ae2358fa3f8936663f30811f78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce AdaptiveBatchScheduler failure enrichment/labeling,FLINK-31893,13533735,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,23/Apr/23 22:13,24/May/23 11:57,04/Jun/24 20:41,24/May/23 11:57,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,"See discussion:
[https://github.com/apache/flink/pull/22506#discussion_r1195412026]

This can be labeled as a global failure",,,,,,,,,,FLINK-31888,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 24 11:56:57 UTC 2023,,,,,,,,,,"0|z1hh4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 11:56;dmvk;master: 1ff5a8a976995ee083e8ccce8aaa2edf9a696ca2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce AdaptiveScheduler global failure enrichment/labeling,FLINK-31892,13533734,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,23/Apr/23 22:13,31/May/23 10:50,04/Jun/24 20:41,31/May/23 10:40,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,,,,,,,,,,FLINK-31888,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 31 10:40:49 UTC 2023,,,,,,,,,,"0|z1hh4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 10:40;dmvk;master: 0c9403f8ad75372d7ff8843ee89990466d3209d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce AdaptiveScheduler per-task failure enrichment/labeling,FLINK-31891,13533733,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,23/Apr/23 22:12,24/May/23 07:25,04/Jun/24 20:41,24/May/23 07:25,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,,,,,,,,,,FLINK-31888,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 24 07:25:18 UTC 2023,,,,,,,,,,"0|z1hh4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 07:25;dmvk;master: 1cc81aa68cefbaa61bde5a213d9b34f23881eeb9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce DefaultScheduler failure enrichment/labeling,FLINK-31890,13533732,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,23/Apr/23 22:12,19/May/23 08:10,04/Jun/24 20:41,19/May/23 08:10,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,,,,,,,,,,FLINK-31888,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 19 08:10:23 UTC 2023,,,,,,,,,,"0|z1hh48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/23 08:10;dmvk;master: a9383fd4d51b1161292628145e2f427f574a07d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation for implementing/loading enrichers,FLINK-31889,13533731,13529017,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,23/Apr/23 21:29,11/Sep/23 08:24,04/Jun/24 20:41,11/Sep/23 08:24,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,Describe how enrichers can be implemented and loaded to Flink as part of documentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 11 08:24:54 UTC 2023,,,,,,,,,,"0|z1hh40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","07/Sep/23 13:47;wangm92;[~pgaref] if you create a ticket for chinese documention, you can aasign to me, i will translate it;;;","11/Sep/23 08:24;chesnay;master: 484da993e22c30b0c9e2bcf747036a79443519fc
1.18: 7339a5359cb1ea1e718c00037bc322b02cc523e2;;;",,,,,,,,,,,,,,,,,,,,,,,,
Introduce interfaces and utility classes related to enrichment/labelling of failures leading to job restart,FLINK-31888,13533730,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,23/Apr/23 21:26,02/May/23 03:17,04/Jun/24 20:41,30/Apr/23 21:16,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,We need to introduce new interfaces/implementations for FailureEnricher / Context / FailureEnricherUtils,,,,,,,,,,,,,,,,,,,,,FLINK-31890,FLINK-31891,FLINK-31892,FLINK-31893,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 30 21:16:04 UTC 2023,,,,,,,,,,"0|z1hh3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Apr/23 21:16;dmvk;master: f3598c50c0d3dcdf8058b01f13b7eb9fc5954f7c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Flink version of Flink ML to 1.16.1,FLINK-31887,13533703,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,23/Apr/23 11:39,25/Apr/23 06:19,04/Jun/24 20:41,25/Apr/23 06:19,,,,,,,,,,,,,,,,,,,,,ml-2.3.0,,,,,,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,,,Upgrade Flink version of Flink ML to 1.16.1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 06:19:05 UTC 2023,,,,,,,,,,"0|z1hgy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 06:19;lindong;Merged to apache/flink-ml master branch 5b193deedff2aa2be96fbfb5304f812caace9e12.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to fabric8 6.5.1 and JOSDK 4.3.0,FLINK-31886,13533679,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,23/Apr/23 06:54,26/Apr/23 09:10,04/Jun/24 20:41,26/Apr/23 09:01,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,Update the Kubernetes (fabric8 & josdk) dependencies to the latest versions which contain important fixes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 09:01:42 UTC 2023,,,,,,,,,,"0|z1hgtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/23 09:01;gaborgsomogyi;d9b3437 on main.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Trigger events on autoscaler errors,FLINK-31885,13533678,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,23/Apr/23 06:51,28/Apr/23 11:11,04/Jun/24 20:41,28/Apr/23 11:11,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,,,"We currently only log (and ignore) autoscaler errors, but we should trigger events so it's visible to the user when something went wrong.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 28 11:11:05 UTC 2023,,,,,,,,,,"0|z1hgt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 11:11;gyfora;merged to main 41f753fec40121274c8d0b27f327ffbd52500cf7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ExecNode to new version causes the old serialized plan failed to pass Json SerDe round trip,FLINK-31884,13533673,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,23/Apr/23 06:39,16/May/23 16:29,04/Jun/24 20:41,16/May/23 16:29,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"h4. How to Reproduce

Firstly, add a test to dump the compiled plan JSON.
{code:java}
@Test
public void debug() {
    tableEnv.executeSql(""create table foo (f0 int, f1 string) with ('connector' = 'datagen')"");
    tableEnv.executeSql(""create table bar (f0 int, f1 string) with ('connector' = 'print')"");
    tableEnv.compilePlanSql(""insert into bar select * from foo"")
            .writeToFile(new File(""/path/to/debug.json""));
}
{code}
The JSON context is as follows
{code:json}
{
  ""flinkVersion"" : ""1.18"",
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`foo`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""f0"",
              ""dataType"" : ""INT""
            }, {
              ""name"" : ""f1"",
              ""dataType"" : ""VARCHAR(2147483647)""
            } ],
            ""watermarkSpecs"" : [ ]
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""connector"" : ""datagen""
          }
        }
      }
    },
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""TableSourceScan(table=[[default_catalog, default_database, foo]], fields=[f0, f1])"",
    ""inputProperties"" : [ ]
  }, {
    ""id"" : 2,
    ""type"" : ""stream-exec-sink_1"",
    ""configuration"" : {
      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    },
    ""dynamicTableSink"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`bar`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""f0"",
              ""dataType"" : ""INT""
            }, {
              ""name"" : ""f1"",
              ""dataType"" : ""VARCHAR(2147483647)""
            } ],
            ""watermarkSpecs"" : [ ]
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""connector"" : ""print""
          }
        }
      }
    },
    ""inputChangelogMode"" : [ ""INSERT"" ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""Sink(table=[default_catalog.default_database.bar], fields=[f0, f1])""
  } ],
  ""edges"" : [ {
    ""source"" : 1,
    ""target"" : 2,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  } ]
}
{code}
Then upgrade the StreamExecSink to a new version
{code:java}
@ExecNodeMetadata(
        name = ""stream-exec-sink"",
        version = 1,
        consumedOptions = {
            ""table.exec.sink.not-null-enforcer"",
            ""table.exec.sink.type-length-enforcer"",
            ""table.exec.sink.upsert-materialize"",
            ""table.exec.sink.keyed-shuffle""
        },
        producedTransformations = {
            CommonExecSink.CONSTRAINT_VALIDATOR_TRANSFORMATION,
            CommonExecSink.PARTITIONER_TRANSFORMATION,
            CommonExecSink.UPSERT_MATERIALIZE_TRANSFORMATION,
            CommonExecSink.TIMESTAMP_INSERTER_TRANSFORMATION,
            CommonExecSink.SINK_TRANSFORMATION
        },
        minPlanVersion = FlinkVersion.v1_15,
        minStateVersion = FlinkVersion.v1_15)
@ExecNodeMetadata(
        name = ""stream-exec-sink"",
        version = 2,
        consumedOptions = {
            ""table.exec.sink.not-null-enforcer"",
            ""table.exec.sink.type-length-enforcer"",
            ""table.exec.sink.upsert-materialize"",
            ""table.exec.sink.keyed-shuffle""
        },
        producedTransformations = {
            CommonExecSink.CONSTRAINT_VALIDATOR_TRANSFORMATION,
            CommonExecSink.PARTITIONER_TRANSFORMATION,
            CommonExecSink.UPSERT_MATERIALIZE_TRANSFORMATION,
            CommonExecSink.TIMESTAMP_INSERTER_TRANSFORMATION,
            CommonExecSink.SINK_TRANSFORMATION
        },
        minPlanVersion = FlinkVersion.v1_18,
        minStateVersion = FlinkVersion.v1_15)
public class StreamExecSink extends CommonExecSink implements StreamExecNode<Object> {
}
{code}
And then load the previous plan and print it as JSON text
{code:java}
tableEnv.loadPlan(PlanReference.fromFile(""/path/to/debug.json"")).printJsonString();
{code}
The SerDe lost idempotence since the version for StreamExecSink became version 2.
{code:json}
{
  ""flinkVersion"" : ""1.18"",
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`foo`""
      }
    },
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""TableSourceScan(table=[[default_catalog, default_database, foo]], fields=[f0, f1])"",
    ""inputProperties"" : [ ]
  }, {
    ""id"" : 2,
    ""type"" : ""stream-exec-sink_2"",
    ""configuration"" : {
      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    },
    ""dynamicTableSink"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`bar`""
      }
    },
    ""inputChangelogMode"" : [ ""INSERT"" ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""Sink(table=[default_catalog.default_database.bar], fields=[f0, f1])""
  } ],
  ""edges"" : [ {
    ""source"" : 1,
    ""target"" : 2,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  } ]
}
{code}
h4. Root Cause

ExecNodeBase#getContextFromAnnotation always uses the newest ExecNode version for SerDe. As a result, although the deserialized CompilePlan object is correct, #printAsJson will create a new context with the newest version.
 
h4. Suggested Fix

If the member variable `isCompiled` is true, then #getContextFromAnnotation should return the context which reads from the JSON plan instead of instantiating a new one.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25217,,,,,,,,,,,,FLINK-31791,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 16 16:29:40 UTC 2023,,,,,,,,,,"0|z1hgs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 03:29;godfrey;Fixed in master: 3664609c7622ccae80e36e85099a1b79b5935fe9;;;","16/May/23 16:29;qingyue;Fixed in master: 3664609c7622ccae80e36e85099a1b79b5935fe9;;;",,,,,,,,,,,,,,,,,,,,,,,,,
StreamSQLExample.Order has nullability issues,FLINK-31883,13533668,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,chenyu-opensource,chenyu-opensource,chenyu-opensource,23/Apr/23 03:11,19/Apr/24 09:00,04/Jun/24 20:41,19/Apr/24 09:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tests,,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/23 03:17;chenyu-opensource;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13057488/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 10:35:00 UTC 2023,,,,,,,,,,"0|z1hgqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 03:27;chenyu-opensource;Using the unboxed types that prevent nullability issues ;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
SqlGateway will throw exception when executing DeleteFromFilterOperation,FLINK-31882,13533666,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,yzl,yzl,23/Apr/23 02:30,17/May/23 01:42,04/Jun/24 20:41,17/May/23 01:42,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Table SQL / API,Table SQL / Gateway,,,,,0,pull-request-available,,,,,"Reproduce step:

Our sink implements `SupportsDeletePushDown`, so when we test a DELETE statement, the `TableEnvironmentImpl` will call the 
`TableResultInternal executeInternal(DeleteFromFilterOperation deleteFromFilterOperation)` at line 895. This method won't return the JobClient, but the SqlGateway requires one, thus a exception occurs.
Stack:
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Can't get job client for the operation d4ba1029-664c-44c0-922b-021eb9e1c527.
at org.apache.flink.table.gateway.service.operation.OperationExecutor.lambda$callModifyOperations$6(OperationExecutor.java:521)
at java.base/java.util.Optional.orElseThrow(Optional.java:408)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:518)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:431)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:200)
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 17 01:42:07 UTC 2023,,,,,,,,,,"0|z1hgqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 02:38;luoyuxia;[~yzl] Thanks for reporting. I'll try to fix it.;;;","17/May/23 01:42;luoyuxia;master:

886dda2938c7fccfeb68ecbadc9c926124c42352

1.17:

ad7b6b45b494a32667453e69e9bf6c0c1e52ec0c;;;",,,,,,,,,,,,,,,,,,,,,,,,,
The Chinese documentation page of Flink official CEP is experiencing confusion.,FLINK-31881,13533639,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,zhaoshuo,zhaoshuo,22/Apr/23 13:55,25/Apr/23 07:35,04/Jun/24 20:41,25/Apr/23 07:34,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,chinese-translation,,,,,,0,pull-request-available,,,,,"The Chinese documentation page of Flink official CEP is experiencing confusion.

https://nightlies.apache.org/flink/flink-docs-master/zh/docs/libs/cep/#%E5%BE%AA%E7%8E%AF%E6%A8%A1%E5%BC%8F%E4%B8%AD%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%80%A7

 

!image-2023-04-22-21-55-15-225.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/23 13:55;zhaoshuo;image-2023-04-22-21-55-15-225.png;https://issues.apache.org/jira/secure/attachment/13057485/image-2023-04-22-21-55-15-225.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 07:34:58 UTC 2023,,,,,,,,,,"0|z1hgkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 04:53;Wencong Liu;Hello [~zhaoshuo], I'd like to fix errors in the markdown file. cc [~Weijie Guo] ;;;","25/Apr/23 07:34;martijnvisser;Fixed in master: d9eea81f3d92806486977952e15f03b70d5d3535;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Bad Test in OrcColumnarRowSplitReaderTest,FLINK-31880,13533605,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kurto,kurto,21/Apr/23 21:51,23/May/23 21:36,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / ORC,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"This is a development issue with, what looks like a buggy unit test.
 
I tried to build Flink with a clean copy of the repository and I get:
 
```
[INFO] Results:
[INFO]
[ERROR] Failures:
[ERROR] OrcColumnarRowSplitReaderTest.testReadFileWithTypes:365
expected: ""1969-12-31""
but was: ""1970-01-01""
[INFO]
[ERROR] Tests run: 26, Failures: 1, Errors: 0, Skipped: 0
```
 
I see the test is testing Date data types with `new Date(562423)` which is 9 minutes and 22 seconds after the epoch time, which is 1970-01-01 UTC time, or when I run that on my laptop in CST timezone, I get `Wed Dec 31 18:09:22 CST 1969`.
 
I have a simple pull request ready which fixes this issue and uses the Java 8 LocalDate API instead which avoids time zones entirely.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,,Tue May 16 03:27:50 UTC 2023,,,,,,,,,,"0|z1hgcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 03:27;kurto;Updated PR

https://github.com/apache/flink/pull/22586;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.avro.util.Utf8 cannot be serialized with avro when used in state ,FLINK-31879,13533593,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ferozedaud,ferozedaud,21/Apr/23 18:10,21/Apr/23 18:10,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Type Serialization System,,,,,,0,,,,,,"Scenario:

Write a flink app that reads avro messages from a kafka topic.

The avro pojos are generated with _org.apache.avro.util.Utf8_ type instead of _java.lang.String_

When this happens, Flink logs an error message as follows:
{noformat}
Class class org.apache.avro.util.Utf8 cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance. {noformat}
 

This is problematic because `Utf8` is designed to be a fast serialized/deserialized type for Avro. But since it is not inheriting from SpecificRecordBase, it seems as if it gets handled by Kryo serializer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-21 18:10:51.0,,,,,,,,,,"0|z1hga8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the wrong name of PauseOrResumeSplitsTask#toString in connector fetcher ,FLINK-31878,13533513,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,tanyuxin,tanyuxin,21/Apr/23 07:51,14/Jun/23 11:23,04/Jun/24 20:41,21/Apr/23 17:23,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Connectors / Common,,,,,,0,pull-request-available,,,,,The class name PauseOrResumeSplitsTask#toString is not right. Users will be very confused when calling the toString method of the class. So we should fix it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31838,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 14 11:23:16 UTC 2023,,,,,,,,,,"0|z1hfsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/23 17:23;Weijie Guo;master(1.18) via 0104427dc9e38e898ba3865b499cc515004041c9.;;;","14/Jun/23 11:23;mapohl;I accidentally added FLINK-31878 as the Jira issue to the message of a commit that actually fixes FLINK-31838. I'm linking the two tickets here to make this visible in Jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
StreamExecutionEnvironmentTests.test_from_collection_with_data_types is unstable,FLINK-31877,13533493,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,21/Apr/23 05:46,18/Aug/23 22:35,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,auto-deprioritized-critical,test-stability,,,,"{noformat}
Apr 21 05:11:45 =================================== FAILURES ===================================
Apr 21 05:11:45 _____ StreamExecutionEnvironmentTests.test_from_collection_with_data_types _____
Apr 21 05:11:45 
Apr 21 05:11:45 self = <pyflink.datastream.tests.test_stream_execution_environment.StreamExecutionEnvironmentTests testMethod=test_from_collection_with_data_types>
Apr 21 05:11:45 
Apr 21 05:11:45     def test_from_collection_with_data_types(self):
Apr 21 05:11:45         # verify from_collection for the collection with single object.
Apr 21 05:11:45         ds = self.env.from_collection(['Hi', 'Hello'], type_info=Types.STRING())
Apr 21 05:11:45         ds.add_sink(self.test_sink)
Apr 21 05:11:45 >       self.env.execute(""test from collection with single object"")
Apr 21 05:11:45 
Apr 21 05:11:45 pyflink/datastream/tests/test_stream_execution_environment.py:257: 
Apr 21 05:11:45 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Apr 21 05:11:45 pyflink/datastream/stream_execution_environment.py:764: in execute
Apr 21 05:11:45     return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
Apr 21 05:11:45 .tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py:1321: in __call__
Apr 21 05:11:45     return_value = get_return_value(
Apr 21 05:11:45 pyflink/util/exceptions.py:146: in deco
Apr 21 05:11:45     return f(*a, **kw)
Apr 21 05:11:45 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48320&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=31864",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:11 UTC 2023,,,,,,,,,,"0|z1hfo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/23 10:12;Sergey Nuyanzin;// cc [~hxbks2ks], [~jark];;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-queryable-state,FLINK-31876,13533488,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,Yanfei Lei,Yanfei Lei,Yanfei Lei,21/Apr/23 04:40,12/May/23 03:06,04/Jun/24 20:41,12/May/23 03:06,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Queryable State,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 27 02:47:08 UTC 2023,,,,,,,,,,"0|z1hfn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 02:47;Weijie Guo;master(1.18)
migrate flink-queryable-state-client via 8be25d15501744f809e31208757ff211558710ae.
migrate flink-queryable-state-runtime via 7f4e834babe7e8865123148e57ecdd6a138b609d.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
OSS throwns NoClassDefFoundError due to old hadoop-common version,FLINK-31875,13533477,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,masteryhx,masteryhx,21/Apr/23 02:33,24/Oct/23 15:43,04/Jun/24 20:41,,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,FileSystems,,,,,,0,pull-request-available,stale-assigned,,,,"h2. Problem

When using OSS in 1.17, an exception will be thrown:
{code:java}
java.lang.NoClassDefFoundError: org/apache/hadoop/thirdparty/com/google/common/base/Preconditions

at org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils.longOption(AliyunOSSUtils.java:221)
at org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem.initialize(AliyunOSSFileSystem.java:343)
at org.apache.flink.fs.osshadoop.OSSFileSystemFactory.create(OSSFileSystemFactory.java:147)
at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:508)
at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274){code}
 It could be reproduced in ITCASE of OSS if some envs has been configured.
h2. Why

After https://issues.apache.org/jira/browse/FLINK-27308 and  https://issues.apache.org/jira/browse/FLINK-29502 ，hadoop-aliyun has also be upgraded to 3.3.4 which relys on the newest version of hadoop-common.

OSS still uses the old version (2.10.2) extended from flink-parent so that some classes cannot be found.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 10:35:00 UTC 2023,,,,,,,,,,"0|z1hfko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/23 07:42;martijnvisser;I don't think that's the problem here. Flink has a minimum version of Hadoop 2.10.2, but the Flink filesystems have their own implementations. I believe that the issue is because OSS doesn't shade anything in with Hadoop (like is done with S3 Hadoop and S3 Presto), so if you would like to use OSS you also need to have Hadoop artifacts on the classpath. ;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Support truncate table statement in batch mode,FLINK-31874,13533474,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,21/Apr/23 01:52,10/Aug/23 01:38,04/Jun/24 20:41,10/Aug/23 01:38,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"Described in [FLIP-302: Support TRUNCATE TABLE statement in batch mode|https://cwiki.apache.org/confluence/display/FLINK/FLIP-302%3A+Support+TRUNCATE+TABLE+statement+in+batch+mode]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-21 01:52:10.0,,,,,,,,,,"0|z1hfk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add setMaxParallelism to the DataStreamSink Class,FLINK-31873,13533458,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,eric.xiao,eric.xiao,eric.xiao,20/Apr/23 20:24,28/Apr/23 14:35,04/Jun/24 20:41,28/Apr/23 14:35,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,API / DataStream,,,,,,0,pull-request-available,,,,,"When turning on Flink reactive mode, it is suggested to convert all {{setParallelism}} calls to {{setMaxParallelism}} from [elastic scaling docs|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/elastic_scaling/#configuration].

With the current implementation of the {{DataStreamSink}} class, only the {{[setParallelism|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSink.java#L172-L181]}} function of the {{[Transformation|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/dag/Transformation.java#L248-L285]}} class is exposed - {{Transformation}} also has the {{[setMaxParallelism|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/dag/Transformation.java#L277-L285]}} function which is not exposed.

 

This means for any sink in the Flink pipeline, we cannot set a max parallelism.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21711,,,,,,,,,,,,,,,,,,,,,,"20/Apr/23 20:33;eric.xiao;Screenshot 2023-04-20 at 4.33.14 PM.png;https://issues.apache.org/jira/secure/attachment/13057461/Screenshot+2023-04-20+at+4.33.14+PM.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 28 14:35:34 UTC 2023,,,,,,,,,,"0|z1hfgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 20:35;eric.xiao;I have an open PR to address this issue: https://github.com/apache/flink/pull/22438;;;","21/Apr/23 01:27;luoyuxia;[~eric.xiao] Thanks for raising it. Since it add a new public api, it must need a FLIP before we review it.;;;","21/Apr/23 07:38;martijnvisser;I've downgraded this to a Major, since this can't be considered a release blocker. I would recommend opening a discussion on the Dev mailing list first;;;","21/Apr/23 14:12;eric.xiao;Thanks [~martijnvisser] and [~luoyuxia], I will start with opening up a thread in the Dev mailing list before making a FLIP :).;;;","28/Apr/23 14:35;Weijie Guo;master(1.18) via 7a1c2b5750d9576f09023cf2810ed965072b699e.;;;",,,,,,,,,,,,,,,,,,,,,,
Add Support for Configuring AIMD Ratelimiting strategy parameters by Sink users for KinesisStreamsSink,FLINK-31872,13533444,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chalixar,chalixar,chalixar,20/Apr/23 17:35,16/Apr/24 10:32,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,aws-connector-4.4.0,,,,,,,,,Connectors / Kinesis,,,,,,0,,,,,,"h1. Issue

As part of FLINK-31772

I performed a complete benchmark for {{KinesisStreamsSink}} after configuring rate limiting strategy.
It appears that optimum values for rate limiting strategy parameters are dependent on use case (shard number/ parallellism/ record thouroughput)
We initially implemeted the {{AIMDRateLimitingStrategy}} in accordance with one used for TCP congestion control but since parameters are use case dependent we would like to allow sink users to adjust parameters as suitable.
h2. Requirements
 - we *must* allow users to configure increment rate and decrease factor of AIMDRateLimitingStrategy for {{KinesisStreamsSink}}
 - we *must* provide backward compatible default values identical to current values to introduce no further regressions.

h2. Appendix
h3. Performace Benchmark Results
|Parallelism/Shards/Payload|paralellism|shards|payload|records/sec|Async Sink|Async Sink With Configured Ratelimiting Strategy Thourouput (MB/s)|Async sink/ Maximum Thourouput|% of Improvement|
|Low/Low/Low|1|1|1024|10000|0.991|1|1|0.9|
|Low/Low/High|1|1|102400|100|0.9943|1|1|0.57|
|Low/Med/Low|1|8|1024|80000|4.12|4.57|0.57125|5.625|
|Low/Med/High|1|8|102400|800|4.35|7.65|0.95625|41.25|
|Med/Low/Low|8|1|1024|20000|0.852|0.846|0.846|-0.6|
|Med/Low/High|8|1|102400|200|0.921|0.867|0.867|-5.4|
|Med/Med/Low|8|8|1024|80000|5.37|4.76|0.595|-7.625|
|Med/Med/High|8|8|102400|800|7.53|7.69|0.96125|2|
|Med/High/Low|8|64|1024|80000|32.5|37.4|0.58438|7.65625|
|Med/High/High|8|64|102400|800|47.27|60.4|0.94375|20.51562|
|High/High/Low|256|256|1024|300000|127|127|0.49609|0|
|High/High/High|256|256|102400|3000|225|246|0.96094|8.20313|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 23 15:55:46 UTC 2023,,,,,,,,,,"0|z1hfdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 15:55;chalixar;CC [~danny.cranmer] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Interpret Flink MemoryUnits according to the actual user input,FLINK-31871,13533427,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sr.praneeth@gmail.com,afedulov,afedulov,20/Apr/23 15:58,20/Nov/23 12:31,04/Jun/24 20:41,20/Sep/23 05:56,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.7.0,,,,,,,,,API / Core,Kubernetes Operator,,,,,0,pull-request-available,,,,,"Currently all MemorySize.MemoryUnits are interpreted in ""bibyte"" notation, regardless of the units that users specify: [https://github.com/apache/flink/blob/release-1.17/flink-core/src/main/java/org/apache/flink/configuration/MemorySize.java#L352-L356]
(i.e. G = Gi)

Flink Kubernetes Operator utilizes these units for specifying resources in user-facing CR API (taskManager.resource.memory, jobManager.resource.memory). In other places this CR requires native K8S units specification (i.e. spec.containers[*].ephemeral-storage). 

There are two issues with this:
 * users cannot rely on the same units notation (taskManager.resource.memory = 16Gi fails)
 * taskManager.resource.memory = 16G is not interpreted as other units in the spec (16G is implicitly converted into 16Gi)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 20 05:56:58 UTC 2023,,,,,,,,,,"0|z1hf9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/23 22:20;mason6345;It would be nice and intuitive to support what Kubernetes supports (both notations e.g. 400 Mi and 400M).;;;","06/Sep/23 12:57;gyfora;[~afedulov] do you mind if [~sr.praneeth@gmail.com]  takes this ticket?;;;","12/Sep/23 17:02;sr.praneeth@gmail.com;[~gyfora]

Can you please help review

https://github.com/apache/flink-kubernetes-operator/pull/670;;;","20/Sep/23 05:56;gyfora;merged to main 83f92bf3e062410c0e23586580df21e646e297b7;;;",,,,,,,,,,,,,,,,,,,,,,,
 Cassandra input/output formats documentation is missing,FLINK-31870,13533413,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,echauchot,echauchot,20/Apr/23 14:53,20/Apr/23 14:53,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-20 14:53:53.0,,,,,,,,,,"0|z1hf6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_multi_sessionjob.sh gets stuck very frequently,FLINK-31869,13533409,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,20/Apr/23 14:25,21/Apr/23 12:54,04/Jun/24 20:41,21/Apr/23 12:54,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,The test_multi_sessionjob.sh gets stuck almost all the time on recent builds.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 21 12:54:34 UTC 2023,,,,,,,,,,"0|z1hf5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/23 12:54;gyfora;Merged to main 1f54ffa484c359c4b81a409f27092dbfba82157b

There are still frequent test failures but at least the CI doesnt get stuck for hours;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix DefaultInputSplitAssigner javadoc for class,FLINK-31868,13533388,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pvary,pvary,pvary,20/Apr/23 11:40,21/Apr/23 09:31,04/Jun/24 20:41,21/Apr/23 06:05,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,API / Core,,,,,,0,pull-request-available,,,,,"Based on the discussion[1] on the mailing list {{there
is no requirement of the order of splits by Flink itself}}, we should fix the discrepancy between the code and the comment by updating the comment.

 

[[1] https://lists.apache.org/thread/74m7z2kzgpzylhrp1oq4lz37pnqjmbkh|https://lists.apache.org/thread/74m7z2kzgpzylhrp1oq4lz37pnqjmbkh]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 21 06:05:59 UTC 2023,,,,,,,,,,"0|z1hf0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/23 06:05;zhuzh;Fixed via f240a5110a0d28473b534cf377d287d5d072a93e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Enforce a minimum number of observations within a metric window,FLINK-31867,13533387,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mxm,mxm,mxm,20/Apr/23 11:39,25/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,stale-assigned,,,,The metric window is currently only time-based. We should make sure we see a minimum number of observations to ensure we don't decide based on too few observations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 25 22:35:08 UTC 2023,,,,,,,,,,"0|z1hf0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler metric trimming reduces the number of metric observations on recovery,FLINK-31866,13533384,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,20/Apr/23 11:25,27/Apr/23 15:15,04/Jun/24 20:41,27/Apr/23 15:15,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,,,"The autoscaler uses a ConfigMap to store past metric observations which is used to re-initialize the autoscaler state in case of failures or upgrades.

Whenever trimming of the ConfigMap occurs, we need to make sure we also update the timestamp for the start of the metric collection, so any removed observations can be compensated with by collecting new ones. If we don't do this, the metric window will effectively shrink due to removing observations.

This can lead to triggering scaling decisions when the operator gets redeployed due to the removed items.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-20 11:25:30.0,,,,,,,,,,"0|z1hf00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink hive parser support order by non select column,FLINK-31865,13533378,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,20/Apr/23 10:46,20/Apr/23 10:46,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,,0,,,,,,"This issure can reproduced by the follow sql:
{code:java}
select name from tableA order by id{code}
b is not a column in select clause. The error message like follows

 
{code:java}
Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:37 Invalid table alias or column reference 'id': (possible column names are: name)
    at org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.genAllExprNodeDesc(HiveParserSemanticAnalyzer.java:2328)
    at org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer.genAllExprNodeDesc(HiveParserSemanticAnalyzer.java:2259)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.genOBLogicalPlan(HiveParserCalcitePlanner.java:1650)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.genLogicalPlan(HiveParserCalcitePlanner.java:2788)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.logicalPlan(HiveParserCalcitePlanner.java:283)
    ... 55 more{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-20 10:46:59.0,,,,,,,,,,"0|z1heyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate unaligned checkpoint type based on persisted data,FLINK-31864,13533372,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,fanrui,fanrui,20/Apr/23 10:32,20/Apr/23 11:49,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,,,"FLINK-31588 wants to correct the unaligned checkpoint type after aligned barrier timeout to unaligned barrier on PipelinedSubpartition.

Initially, we wanted to try to keep the semantic of that flag in sync with what StreamTask was actually doing. If checkpoint was unaligned, as it arrived unaligned, it should be reported as such, even if that particular subtask didn't persist any data.

However, we found the code is too complicated, and it probably not worth of making the code so much more complicated, so we prefer generate unaligned checkpoint type based on persisted data.

The detailed discussion: [https://github.com/apache/flink/pull/22392#discussion_r1165613057]

 

cc [~pnowojski] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27251,,FLINK-31588,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-20 10:32:23.0,,,,,,,,,,"0|z1hexc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add 'Hostname' enum val to k8s NodeAddress type.,FLINK-31863,13533364,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vince.feng,vince.feng,20/Apr/23 10:08,24/Oct/23 15:45,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,easyfix,kubernetes,pull-request-available,,,"Class io.fabric8.kubernetes.api.model.NodeAddress.type contains 'InternalIP','ExternalIP' and 'Hostname'. The InternalIP address is unavailable in the private cloud environment. But the hostname can be resolved by the DNS server. So 'org.apache.flink.kubernetes.configuration.NodePortAddressType.NodePortAddressType' should add 'Hostname' enumeration value.
{code:java}
//org.apache.flink.kubernetes.configuration.NodePortAddressType.NodePortAddressType
    public enum NodePortAddressType {
        InternalIP,
        ExternalIP,
        Hostname
    } {code}
 

!image-2023-04-20-17-53-30-969.png!","OS: CentOS 7

Kubernetes: v1.18.5",3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/23 09:53;vince.feng;image-2023-04-20-17-53-30-969.png;https://issues.apache.org/jira/secure/attachment/13057443/image-2023-04-20-17-53-30-969.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,2023-04-20 10:08:32.0,,,,,,,,,,"0|z1hevk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSinkITCase.testStartFromSavepoint is unstable,FLINK-31862,13533324,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,Sergey Nuyanzin,Sergey Nuyanzin,20/Apr/23 07:12,16/Oct/23 12:36,04/Jun/24 20:41,16/Oct/23 12:36,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,Tests,,,,,0,auto-deprioritized-major,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48243&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e&l=36611

{noformat}
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.
Apr 19 01:42:20 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Apr 19 01:42:20 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Apr 19 01:42:20 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Apr 19 01:42:20 	at org.apache.flink.connector.kafka.sink.testutils.KafkaSinkExternalContext.createTopic(KafkaSinkExternalContext.java:101)
Apr 19 01:42:20 	... 111 more

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26921,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:08 UTC 2023,,,,,,,,,,"0|z1hemo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 07:13;Sergey Nuyanzin;Probably related FLINK-26921;;;","23/Apr/23 12:47;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48375&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36288;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,
Introduce ByteArraySchema which serialize/deserialize data of type byte array,FLINK-31861,13533311,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianfu,dianfu,20/Apr/23 05:20,20/Apr/23 05:20,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Core,,,,,,0,,,,,,"The aim of this ticket is to introduce ByteArraySchema which serialize/deserialize data of type byte array. In this case, users could get the raw bytes from a data source. See [https://apache-flink.slack.com/archives/C03G7LJTS2G/p1681928862762699] for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-20 05:20:31.0,,,,,,,,,,"0|z1hejs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkDeployments never finalize when namespace is deleted,FLINK-31860,13533285,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhou_jiang,isugimpy,isugimpy,19/Apr/23 20:31,18/Apr/24 06:17,04/Jun/24 20:41,18/Apr/24 06:17,kubernetes-operator-1.3.1,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.9.0,,,,,,,,,Kubernetes Operator,,,,,,3,pull-request-available,stale-assigned,,,,"This appears to be a pretty straightforward issue, but I don't know the codebase well enough to propose a fix.  When a FlinkDeployment is present in a namespace, and the namespace is deleted, the FlinkDeployment never reconciles and fails to complete its finalizer.  This leads to the namespace being blocked from deletion indefinitely, requiring manual manipulation to remove the finalizer on the FlinkDeployment.

 

Namespace conditions:
{code:java}
conditions:
- lastTransitionTime: '2023-04-18T22:17:48Z'
  message: All resources successfully discovered
  reason: ResourcesDiscovered
  status: 'False'
  type: NamespaceDeletionDiscoveryFailure
- lastTransitionTime: '2023-03-23T18:27:37Z'
  message: All legacy kube types successfully parsed
  reason: ParsedGroupVersions
  status: 'False'
  type: NamespaceDeletionGroupVersionParsingFailure
- lastTransitionTime: '2023-03-23T18:27:37Z'
  message: All content successfully deleted, may be waiting on finalization
  reason: ContentDeleted
  status: 'False'
  type: NamespaceDeletionContentFailure
- lastTransitionTime: '2023-03-23T18:27:37Z'
  message: 'Some resources are remaining: flinkdeployments.flink.apache.org has 2
    resource instances'
  reason: SomeResourcesRemain
  status: 'True'
  type: NamespaceContentRemaining
- lastTransitionTime: '2023-03-23T18:27:37Z'
  message: 'Some content in the namespace has finalizers remaining: flinkdeployments.flink.apache.org/finalizer
    in 2 resource instances'
  reason: SomeFinalizersRemain
  status: 'True'
  type: NamespaceFinalizersRemaining
phase: Terminating {code}
FlinkDeployment example (some fields redacted):
{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  creationTimestamp: '2023-03-23T18:27:02Z'
  deletionGracePeriodSeconds: 0
  deletionTimestamp: '2023-03-23T18:27:35Z'
  finalizers:
  - flinkdeployments.flink.apache.org/finalizer
  generation: 3
  name: <name-redacted>
  namespace: <namespace-redacted>
  resourceVersion: '10565277081'
  uid: e50d2683-6c0c-467e-b10c-fe0f4e404692
spec:
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: '2'
  flinkVersion: v1_16
  image: <image-redacted>
  job:
    args: []
    entryClass: <class-redacted>
    jarURI: <uri-redacted>
    parallelism: 2
    state: running
    upgradeMode: stateless
  jobManager:
    replicas: 1
    resource:
      cpu: 1
      memory: 2048m
  logConfiguration:
    log4j-console.properties: '# This affects logging for both user code and Flink      rootLogger.level = INFO      rootLogger.appenderRef.console.ref = ConsoleAppender      rootLogger.appenderRef.rolling.ref = RollingFileAppender      # Uncomment this if you want to _only_ change Flink''s logging      #logger.flink.name = org.apache.flink      #logger.flink.level = INFO      # The following lines keep the log level of common libraries/connectors on      # log level INFO. The root logger does not override this. You have to manually      # change the log levels here.      logger.akka.name = akka      logger.akka.level = INFO      logger.kafka.name= org.apache.kafka      logger.kafka.level = INFO      logger.hadoop.name = org.apache.hadoop      logger.hadoop.level = INFO      logger.zookeeper.name = org.apache.zookeeper      logger.zookeeper.level = INFO      # Log all infos to the console      appender.console.name = ConsoleAppender      appender.console.type = CONSOLE      appender.console.layout.type = PatternLayout      appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x
      - %m%n      # Log all infos in the given rolling file      appender.rolling.name = RollingFileAppender      appender.rolling.type = RollingFile      appender.rolling.append = false      appender.rolling.fileName = ${sys:log.file}      appender.rolling.filePattern = ${sys:log.file}.%i      appender.rolling.layout.type = PatternLayout      appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x
      - %m%n      appender.rolling.policies.type = Policies      appender.rolling.policies.size.type = SizeBasedTriggeringPolicy      appender.rolling.policies.size.size=100MB      appender.rolling.strategy.type = DefaultRolloverStrategy      appender.rolling.strategy.max = 10      # Suppress the irrelevant (wrong) warnings from the Netty channel handler      logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline      logger.netty.level = OFF      '
  mode: standalone
  podTemplate:
    apiVersion: v1
    kind: Pod
    metadata:
      labels:
        app.kubernetes.io/managed-by: tilt
    spec:
      containers:
      - env:
        - name: APPLICATION_CONFIG_FILE
          value: /app-conf/feature-pipeline-config.yaml
        name: flink-main-container
        volumeMounts:
        - mountPath: /app-conf
          name: feature-pipeline-config-volume
      volumes:
      - configMap:
          items:
          - key: feature-pipeline-config.yaml
            path: feature-pipeline-config.yaml
          name: <name-redacted>
        name: feature-pipeline-config-volume
  serviceAccount: default
  taskManager:
    replicas: 2
    resource:
      cpu: 1
      memory: 2048m
status:
  clusterInfo: {}
  jobManagerDeploymentStatus: DEPLOYED_NOT_READY
  jobStatus:
    jobId: 07243f53058c9083c06d2000352b04ec
    savepointInfo:
      lastPeriodicSavepointTimestamp: 0
      savepointHistory: []
    state: RECONCILING
  reconciliationStatus:
    lastReconciledSpec: '{""spec"":{""job"":{""jarURI"":""<jar-redacted>"",""parallelism"":2,""entryClass"":""<class-redacted>"",""args"":[],""state"":""running"",""savepointTriggerNonce"":null,""initialSavepointPath"":null,""upgradeMode"":""stateless"",""allowNonRestoredState"":null},""restartNonce"":null,""flinkConfiguration"":{""taskmanager.numberOfTaskSlots"":""2""},""image"":""<image-redacted>"",""imagePullPolicy"":null,""serviceAccount"":""default"",""flinkVersion"":""v1_16"",""ingress"":{""template"":""<template-redacted>"",""className"":null,,""podTemplate"":{""apiVersion"":""v1"",""kind"":""Pod"",""metadata"":{""labels"":{""app.kubernetes.io/managed-by"":""tilt"",""spec"":{""containers"":[{""env"":[{""name"":""APPLICATION_CONFIG_FILE"",""value"":""/app-conf/feature-pipeline-config.yaml""}],""name"":""flink-main-container"",""volumeMounts"":[{""mountPath"":""/app-conf"",""name"":""feature-pipeline-config-volume""}]}],""volumes"":[{""configMap"":{""items"":[{""key"":""feature-pipeline-config.yaml"",""path"":""feature-pipeline-config.yaml""}],""name"":""<name-redacted>""},""name"":""feature-pipeline-config-volume""}]}},""jobManager"":{""resource"":{""cpu"":1.0,""memory"":""2048m""},""replicas"":1,""podTemplate"":null},""taskManager"":{""resource"":{""cpu"":1.0,""memory"":""2048m""},""replicas"":2,""podTemplate"":null},""logConfiguration"":{""log4j-console.properties"":""#
      This affects logging for both user code and Flink\nrootLogger.level = INFO\nrootLogger.appenderRef.console.ref
      = ConsoleAppender\nrootLogger.appenderRef.rolling.ref = RollingFileAppender\n#
      Uncomment this if you want to _only_ change Flink''s logging\n#logger.flink.name
      = org.apache.flink\n#logger.flink.level = INFO\n# The following lines keep the
      log level of common libraries/connectors on\n# log level INFO. The root logger
      does not override this. You have to manually\n# change the log levels here.\nlogger.akka.name
      = akka\nlogger.akka.level = INFO\nlogger.kafka.name= org.apache.kafka\nlogger.kafka.level
      = INFO\nlogger.hadoop.name = org.apache.hadoop\nlogger.hadoop.level = INFO\nlogger.zookeeper.name
      = org.apache.zookeeper\nlogger.zookeeper.level = INFO\n# Log all infos to the
      console\nappender.console.name = ConsoleAppender\nappender.console.type = CONSOLE\nappender.console.layout.type
      = PatternLayout\nappender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS}
      %-5p %-60c %x - %m%n\n# Log all infos in the given rolling file\nappender.rolling.name
      = RollingFileAppender\nappender.rolling.type = RollingFile\nappender.rolling.append
      = false\nappender.rolling.fileName = ${sys:log.file}\nappender.rolling.filePattern
      = ${sys:log.file}.%i\nappender.rolling.layout.type = PatternLayout\nappender.rolling.layout.pattern
      = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\nappender.rolling.policies.type
      = Policies\nappender.rolling.policies.size.type = SizeBasedTriggeringPolicy\nappender.rolling.policies.size.size=100MB\nappender.rolling.strategy.type
      = DefaultRolloverStrategy\nappender.rolling.strategy.max = 10\n# Suppress the
      irrelevant (wrong) warnings from the Netty channel handler\nlogger.netty.name
      = org.jboss.netty.channel.DefaultChannelPipeline\nlogger.netty.level = OFF\n""},""mode"":""standalone""},""resource_metadata"":{""apiVersion"":""flink.apache.org/v1beta1"",""metadata"":{""generation"":2},""firstDeployment"":true}}'
    reconciliationTimestamp: 1679596022457
    state: DEPLOYED
  taskManager:
    labelSelector: component=taskmanager,app=<label-redacted>
    replicas: 2
 {code}
My guess here would be that the best thing to do would be for the reconciler to check if the namespace is in a Terminating state, and skip the finalizer step that's blocking things.  Based on the following log messages, I'm inclined to believe the issue is that the operator is attempting to create an Event in the namespace as a part of deleting the FlinkDeployment, and because that fails the whole deletion aborts.  Log messages redacted.

 
{noformat}
2023-04-19 16:22:58,642 o.a.f.k.o.c.FlinkDeploymentController [INFO ][NAMESPACE/DEPLOYMENT1] Cleaning up FlinkDeployment
2023-04-19 16:22:58,645 o.a.f.k.o.c.FlinkDeploymentController [INFO ][NAMESPACE/DEPLOYMENT2] Cleaning up FlinkDeployment
2023-04-19 16:22:58,749 i.j.o.p.e.ReconciliationDispatcher [ERROR][NAMESPACE/DEPLOYMENT1] Error during event processing ExecutionScope{ resource id: ResourceID{name='DEPLOYMENT1', namespace='NAMESPACE'}, version: 10877039326} failed.
io.javaoperatorsdk.operator.OperatorException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.0.1/api/v1/namespaces/NAMESPACE/events. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. events ""Operator.1727008025"" is forbidden: unable to create new content in namespace NAMESPACE because it is being terminated.
    at io.javaoperatorsdk.operator.processing.Controller.cleanup(Controller.java:195)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleCleanup(ReconciliationDispatcher.java:277)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:79)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.0.1/api/v1/namespaces/NAMESPACE/events. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. events ""Operator.1727008025"" is forbidden: unable to create new content in namespace NAMESPACE because it is being terminated.
    at io.fabric8.kubernetes.client.KubernetesClientException.copyAsCause(KubernetesClientException.java:238)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:517)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:551)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:535)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:328)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:675)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:88)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)
    at io.fabric8.kubernetes.client.utils.internal.CreateOrReplaceHelper.createOrReplace(CreateOrReplaceHelper.java:50)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.createOrReplace(BaseOperation.java:296)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.createOrReplace(BaseOperation.java:88)
    at io.fabric8.kubernetes.client.extension.ResourceAdapter.createOrReplace(ResourceAdapter.java:121)
    at org.apache.flink.kubernetes.operator.utils.EventUtils.createOrUpdateEvent(EventUtils.java:107)
    at org.apache.flink.kubernetes.operator.utils.EventRecorder.triggerEvent(EventRecorder.java:59)
    at org.apache.flink.kubernetes.operator.utils.EventRecorder.triggerEvent(EventRecorder.java:50)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:90)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:55)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:182)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:145)
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
    at io.javaoperatorsdk.operator.processing.Controller.cleanup(Controller.java:144)
    ... 7 more
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.0.1/api/v1/namespaces/NAMESPACE/events. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. events ""Operator.1727008025"" is forbidden: unable to create new content in namespace NAMESPACE because it is being terminated.
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.requestFailure(OperationSupport.java:709)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.requestFailure(OperationSupport.java:689)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.assertResponseCode(OperationSupport.java:638)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.lambda$handleResponse$0(OperationSupport.java:576)
    at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.lambda$retryWithExponentialBackoff$2(OperationSupport.java:618)
    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientImpl$4.onResponse(OkHttpClientImpl.java:277)
    at okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:519)
    ... 3 more
2023-04-19 16:22:58,750 i.j.o.p.e.EventProcessor [ERROR][NAMESPACE/DEPLOYMENT1] Exhausted retries for ExecutionScope{ resource id: ResourceID{name='DEPLOYMENT1', namespace='NAMESPACE'}, version: 10877039326}
2023-04-19 16:22:58,754 i.j.o.p.e.ReconciliationDispatcher [ERROR][NAMESPACE/DEPLOYMENT2] Error during event processing ExecutionScope{ resource id: ResourceID{name='DEPLOYMENT2', namespace='NAMESPACE'}, version: 10877039321} failed.
io.javaoperatorsdk.operator.OperatorException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.0.1/api/v1/namespaces/NAMESPACE/events. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. events ""Operator.1333719523"" is forbidden: unable to create new content in namespace NAMESPACE because it is being terminated.
    at io.javaoperatorsdk.operator.processing.Controller.cleanup(Controller.java:195)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleCleanup(ReconciliationDispatcher.java:277)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:79)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.0.1/api/v1/namespaces/NAMESPACE/events. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. events ""Operator.1333719523"" is forbidden: unable to create new content in namespace NAMESPACE because it is being terminated.
    at io.fabric8.kubernetes.client.KubernetesClientException.copyAsCause(KubernetesClientException.java:238)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:517)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:551)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:535)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:328)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:675)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:88)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)
    at io.fabric8.kubernetes.client.utils.internal.CreateOrReplaceHelper.createOrReplace(CreateOrReplaceHelper.java:50)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.createOrReplace(BaseOperation.java:296)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.createOrReplace(BaseOperation.java:88)
    at io.fabric8.kubernetes.client.extension.ResourceAdapter.createOrReplace(ResourceAdapter.java:121)
    at org.apache.flink.kubernetes.operator.utils.EventUtils.createOrUpdateEvent(EventUtils.java:107)
    at org.apache.flink.kubernetes.operator.utils.EventRecorder.triggerEvent(EventRecorder.java:59)
    at org.apache.flink.kubernetes.operator.utils.EventRecorder.triggerEvent(EventRecorder.java:50)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:90)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:55)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:182)
    at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:145)
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
    at io.javaoperatorsdk.operator.processing.Controller.cleanup(Controller.java:144)
    ... 7 more
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.0.1/api/v1/namespaces/NAMESPACE/events. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. events ""Operator.1333719523"" is forbidden: unable to create new content in namespace NAMESPACE because it is being terminated.
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.requestFailure(OperationSupport.java:709)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.requestFailure(OperationSupport.java:689)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.assertResponseCode(OperationSupport.java:638)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.lambda$handleResponse$0(OperationSupport.java:576)
    at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.lambda$retryWithExponentialBackoff$2(OperationSupport.java:618)
    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)
    at java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientImpl$4.onResponse(OkHttpClientImpl.java:277)
    at okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:519)
    ... 3 more
2023-04-19 16:22:58,754 i.j.o.p.e.EventProcessor [ERROR][NAMESPACE/DEPLOYMENT2] Exhausted retries for ExecutionScope{ resource id: ResourceID{name='DEPLOYMENT2', namespace='NAMESPACE'}, version: 10877039321}{noformat}","Apache Flink Kubernetes Operator 1.3.1

Kubernetes 1.24.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 18 06:17:01 UTC 2024,,,,,,,,,,"0|z1hee0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 03:35;gyfora;Thank you for the detailed analysis , event triggering seems to be the culprit here , you are completely right.

We should avoid triggering events or sending status updates in the cleanup step. We can work on the fix and I will cc you for the review so you can get some familiarity with the code:);;;","26/Apr/23 16:37;gyfora;[~isugimpy] , the PR is ready, please help review/test this. ;;;","26/Apr/23 17:25;isugimpy;This looks very reasonable to me as written, greatly appreciate being tagged in on it!  I'll see if I can get a test of it done, but may not be able to reasonably do so until next week when I'm back to work.;;;","27/Apr/23 08:21;gyfora;I have to put this work on hold for now because any effort on the flink operator side cannot fix a bigger underlying problem. We are investigating if there is a workaround in kubernetes itself somehow. 

More details here: [java-operator-sdk/java-operator-sdk#1876|https://github.com/java-operator-sdk/java-operator-sdk/issues/1876];;;","05/May/23 07:03;rmetzger;We were also facing this problem, and we've solved it for now in this hacky way:

{code}
---
 .../kubernetes/operator/utils/EventUtils.java | 24 ++++++++++++++-----
 .../templates/rbac.yaml                       |  1 +
 2 files changed, 19 insertions(+), 6 deletions(-)

diff --git a/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java b/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java
index d993de2..36c49a4 100644
--- a/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java
+++ b/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java
@@ -22,6 +22,8 @@
 import io.fabric8.kubernetes.api.model.HasMetadata;
 import io.fabric8.kubernetes.api.model.ObjectReferenceBuilder;
 import io.fabric8.kubernetes.client.KubernetesClient;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.time.Instant;
 import java.util.function.Consumer;
@@ -31,6 +33,7 @@
  * https://github.com/EnMasseProject/enmasse/blob/master/k8s-api/src/main/java/io/enmasse/k8s/api/KubeEventLogger.java
  */
 public class EventUtils {
+    private static final Logger LOG = LoggerFactory.getLogger(EventUtils.class);
 
     public static String generateEventName(
             HasMetadata target,
@@ -58,14 +61,14 @@ public static boolean createOrUpdateEvent(
             String message,
             EventRecorder.Component component,
             Consumer<Event> eventListener) {
+        var namespace = target.getMetadata().getNamespace();
+        if (isNamespaceMarkedForDeletion(client, namespace)) {
+            LOG.info(""Ignoring event because namespace is marked for deletion"");
+            return true;
+        }
         var eventName = generateEventName(target, type, reason, message, component);
 
-        var existing =
-                client.v1()
-                        .events()
-                        .inNamespace(target.getMetadata().getNamespace())
-                        .withName(eventName)
-                        .get();
+        var existing = client.v1().events().inNamespace(namespace).withName(eventName).get();
 
         if (existing != null
                 && existing.getType().equals(type.name())
@@ -109,4 +112,13 @@ public static boolean createOrUpdateEvent(
             return true;
         }
     }
+
+    private static boolean isNamespaceMarkedForDeletion(KubernetesClient client, String namespace) {
+        try {
+            return client.namespaces().withName(namespace).get().isMarkedForDeletion();
+        } catch (Exception e) {
+            LOG.warn(""Error while checking namespace status"", e);
+            return false;
+        }
+    }
 }
diff --git a/helm/flink-kubernetes-operator/templates/rbac.yaml b/helm/flink-kubernetes-operator/templates/rbac.yaml
index f50852e..21d7071 100644
--- a/helm/flink-kubernetes-operator/templates/rbac.yaml
+++ b/helm/flink-kubernetes-operator/templates/rbac.yaml
@@ -29,6 +29,7 @@ rules:
       - events
       - configmaps
       - secrets
+      - namespaces
     verbs:
       - ""*""
 {{- if .Values.rbac.nodesRule.create }}
{code};;;","08/May/23 19:20;isugimpy;[~rmetzger] That looks awesome!  Thank you for sharing it!

 

[~gyfora] The above looks like it'd accomplish a fix for the immediate problem very easily and prevent this from impacting production clusters.  If I opened a PR with that, would that be something considered acceptable as a workaround until a more proper fix could be implemented?  I'd prefer to not have to maintain an internal fork of the operator when the community as a whole could benefit from this.;;;","09/May/23 15:45;gyfora;[~isugimpy] we can definitely add this improvement but it didn't fix it for me completely as I was getting an error from the Java Operator SDK itself as it cannot finish deletion of the CR itself (cannot remove the finalizer);;;","09/May/23 16:06;isugimpy;I *think* that only didn't work because of per-namespace permissions.  If cluster-wide permissions are in place, I would expect this to still work?;;;","24/Aug/23 08:35;clementcvl;Hi Folks, 

we're also facing this and all our dev process relies on namespace deletion so it's quite annoying for us. 

I didn't have the opportunity to test the fix proposed by [~rmetzger] , but it sounded like a reasonable workaround, until the [issue|https://github.com/operator-framework/java-operator-sdk/issues/1876] is fixed on java-operator-sdk/kubernetes. 

[~gyfora] did you test this simple fix with cluster-wide permissions ? ;;;","23/Sep/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","06/Mar/24 16:44;klam-shop;Hi, any updates on this issue? 

It seems that [https://github.com/kubernetes/kubernetes/issues/115070] was closed without any changes made.;;;","11/Mar/24 16:38;gyfora;I am not aware of any solution from the kubernetes / josdk side [~klam-shop] ;;;","11/Mar/24 18:11;klam-shop;Thanks [~gyfora]! Is there any consideration for putting a solution in place in the Flink Kubernetes Operator, if there will not be one from the kubernetes / josdk side? ;;;","11/Mar/24 18:50;gyfora;I don’t really know how that would be possible but I welcome any recommendation/ solution .;;;","18/Apr/24 06:17;gyfora;merged to main 4293d58329af562e9c50216c3005b4577a289b90;;;",,,,,,,,,,,,
Update maven cyclonedx plugin to 2.7.7,FLINK-31859,13533267,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,19/Apr/23 16:10,26/Oct/23 10:08,04/Jun/24 20:41,19/Apr/23 19:34,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Build System,,,,,,0,pull-request-available,,,,,"there are at least 2 related improvements

1. current version depends on jackson-databind 2.14.0 and has a memory issue described at [https://github.com/FasterXML/jackson-databind/issues/3665] which is fixed in later versions

2. current version leads to lots of traces in logs (e.g. {{mvn clean verify}} for {{flink-core}}) which is fixed in later versions
{noformat}
[ERROR] An error occurred attempting to read POM
org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) 
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)
    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)
    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)
    at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)
    at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)
    at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)
    at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)
    at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:370)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:351)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:171)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:163)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:294)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:960)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:196)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 20:40:54 UTC 2023,,,,,,,,,,"0|z1hea0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 19:34;Sergey Nuyanzin;Merged to master at [e107783d91f616814f503558b2e0b1f53dec696b|https://github.com/apache/flink/commit/e107783d91f616814f503558b2e0b1f53dec696b];;;","20/Apr/23 20:40;Sergey Nuyanzin;1.17: https://github.com/apache/flink/commit/1b9552cd4e9e8b44417d2675ccd4e9fbf6e6077b;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add Autoscaler support for Session Job,FLINK-31858,13533262,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,phoebemaomao,phoebemaomao,19/Apr/23 15:46,20/Apr/23 18:16,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,,,,,,0,,,,,,"Currently Autoscaler does not work on Session Jobs (submitted to Session Cluster Deployment).

It would be good to have Autoscaler for Session Jobs. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 18:16:29 UTC 2023,,,,,,,,,,"0|z1he8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 02:51;huwh;Hi, [~phoebemaomao] Can you describe the scenario in more detail? 
After [FLIP-291: Externalized Declarative Resource Management|https://cwiki.apache.org/confluence/display/FLINK/FLIP-291%3A+Externalized+Declarative+Resource+Management] , you could declare job resource requirements using the REST API, Does this meet your needs?;;;","20/Apr/23 18:16;phoebemaomao;Hi [~huwh]:

Thanks for your comment.

We tried out the autoscaler [autoscaling example|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/autoscaling/autoscaling.yaml] which is in Application Deployment mode, and it works. But when we tried to use session job deployment such as [basic-session-job-only.yaml|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/basic-session-job-only.yaml] to submit a job to session cluster, we didn't see any autoscale happening.

The autoscaler here is regarding the [FLIP-271: Autoscaling|https://cwiki.apache.org/confluence/display/FLINK/FLIP-271%3A+Autoscaling];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Support pluginable observers mechanism,FLINK-31857,13533252,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,darenwkt,darenwkt,19/Apr/23 15:00,11/Sep/23 22:35,04/Jun/24 20:41,,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,auto-deprioritized-major,pull-request-available,,,,"Currently, Kubernetes Operator uses AbstractFlinkResourceObserver to observe Flink Job state, cluster state, JM/TM state, etc and update Custom Resource: FlinkDeployment, FlinkSessionJob with the gathered info.

This Jira is to introduce a Flink plugin to allow user to implement custom observer logic after the default observer logic is run.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 11 22:35:08 UTC 2023,,,,,,,,,,"0|z1he6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 15:17;darenwkt;Pull Request: https://github.com/apache/flink-kubernetes-operator/pull/570;;;","03/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","11/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add support for Opensearch Connector REST client customization,FLINK-31856,13533239,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,reta,mhjtrifork,mhjtrifork,19/Apr/23 14:24,01/Feb/24 10:24,04/Jun/24 20:41,13/Jun/23 16:53,opensearch-1.0.0,,,,,,,,,,,,,,,,,,,,opensearch-1.0.2,opensearch-1.1.0,,,,,,,,Connectors / Opensearch,,,,,,0,pull-request-available,,,,,"It is not currently possible to customise the Opensearch REST client in all of the connectors.

We are currently using the using the OpensearchSink in [connector/opensearch/sink/OpensearchSink.java|https://github.com/apache/flink-connector-opensearch/blob/main/flink-connector-opensearch/src/main/java/org/apache/flink/connector/opensearch/sink/OpensearchSink.java] and need to be able to authenticate/authorise with Opensearch using OAuth2 and therefore need to be able to pass a bearer token to the bulk index calls.

The access token will expire and change during the jobs lifetime it must be possible to handle this, i.e. giving a token at when building the sink is not enough.

For reference see the mailing list discussion: https://lists.apache.org/thread/9rvwhzjwzm6yq9mg481sdxqx9nqr1x5g",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 15 07:45:43 UTC 2023,,,,,,,,,,"0|z1he3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 14:12;reta;[~martijnvisser] please feel free to assign it to me, thank you;;;","01/Jun/23 13:41;reta;[~mhjtrifork] we have a pull request opened, mind give it a try to see if your particular use case is unblocked with the change? thank you;;;","06/Jun/23 05:49;mhjtrifork;[~reta] Great, thanks. Is there a snapshot build or something I can use in my pom to test it or do I need to clone the repo and build locally?;;;","06/Jun/23 12:48;reta;Thanks [~mhjtrifork] , probably building locally is a way to go, I am not sure the snapshots of the external connectors are published now.;;;","13/Jun/23 05:46;mhjtrifork;[~reta] we have tested the PR and it solves our use case so that is great (y)

The main improvement we could suggest is that we had to copy the whole call to builder.setHttpClientConfigCallback(), i.e. lines 45-75 in DefaultRestClientFactory in order to add a call to httpClientBuilder.addInterceptorFirst();

I'm not sure if anything can be done to alleviate that so we can somehow inherit the default behaviour, but in general the PR is still a great improvement.;;;","13/Jun/23 13:14;reta;Thanks a lot [~mhjtrifork] , yeah, sadly there is no way to compose HttpClientConfigCallback, duplicating is not great, what we could do is:

 - open up {color:#000000}DefaultRestClientFactory
- have an overridable {color}{color:#000000}configureHttpClientBuilder() method{color}

{color:#000000}In this case, you could inherit from DefaultRestClientFactory and override / tailor the default {color}{color:#000000}HttpAsyncClientBuilder customizations, would it help?{color};;;","13/Jun/23 16:52;Sergey Nuyanzin;Great that the issue is solved
thanks [~reta] for working on it
thanks [~mhjtrifork]for checking

merged to main as [13c8338a249843ce52b819307698181ff1a0f23a|https://github.com/apache/flink-connector-opensearch/commit/13c8338a249843ce52b819307698181ff1a0f23a];;;","13/Jun/23 17:44;Sergey Nuyanzin;merged to 1.0.x as [193c360f7cfdd704f0fdfeb865dd8d329f310240|https://github.com/apache/flink-connector-opensearch/commit/193c360f7cfdd704f0fdfeb865dd8d329f310240];;;","14/Jun/23 05:53;mhjtrifork;[~reta] yes, I think your suggestion sounds sensible.

It would be nice to have, but is not critical as what you have done already solves most of the problem.

Thanks.;;;","14/Jun/23 13:22;reta;Thanks [~mhjtrifork] , made it in, should be easier to customize, thank you;;;","15/Jun/23 07:45;mhjtrifork;[~reta] Great, thanks (y);;;",,,,,,,,,,,,,,,,
Canceling a job through the UI isn't possible in standalone,FLINK-31855,13533236,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,gil_shmaya,gil_shmaya,19/Apr/23 13:58,18/May/23 09:54,04/Jun/24 20:41,20/Apr/23 13:47,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"The 'Cancel job' button does not exist in the UI in standalone mode while exists in native mode. 
!image-2023-04-19-16-57-50-316.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/23 13:57;gil_shmaya;image-2023-04-19-16-57-50-316.png;https://issues.apache.org/jira/secure/attachment/13057425/image-2023-04-19-16-57-50-316.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 18 09:54:00 UTC 2023,,,,,,,,,,"0|z1he34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 14:06;gyfora;The cancel button should never exist.;;;","20/Apr/23 13:47;gil_shmaya;Solved by setting the config - web.cancel.enable.;;;","16/May/23 14:23;gil_shmaya;Hey [~gyfora],
I've noticed that when the 'web.cancel.enable' configuration is enabled, the operator restarts the job instead of canceling it. Is there a way to cancel a job without triggering a restart?;;;","18/May/23 09:54;gyfora;You can set the spec.job.state = SUSPENDED to cancel the job. Or simply delete the FlinkDeployment if you dont want to resume again.;;;",,,,,,,,,,,,,,,,,,,,,,,
FLIP-307:[Phase 1] [Sink Support] Flink Connector Redshift ,FLINK-31854,13533204,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,19/Apr/23 12:00,11/Nov/23 13:12,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / AWS,,,,,,0,pull-request-available,,,,,"This is an umbrella Jira for 
[https://cwiki.apache.org/confluence/display/FLINK/FLIP-307%3A++Flink+Connector+Redshift]

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 21 07:11:21 UTC 2023,,,,,,,,,,"0|z1hdw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 17:11;chalixar;Thanks [~samrat007] for the proposal, we should raise a discussion in the mailing list, happy to assist with the JIRA.;;;","21/Apr/23 07:11;samrat007;Sure [~chalixar] , I will start working on draft FLIP , connect with you to collaborate on this epic . :D;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Remove use of  ExecutionAttemptMappingProvider,FLINK-31853,13533191,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,19/Apr/23 11:08,20/Apr/23 03:59,04/Jun/24 20:41,20/Apr/23 03:59,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"Currently, the ExecutionAttemptMappingProvider class used to map ExecutionAttemptID to ExecutionVertex, which is only used when reporting checkpoint statistics in checkpointCoordinator. However, after Flink-17295, we refactored ExecutionAttemptID to consist of executionGraphId, executionVertexId, and attemptNumber. Therefore, the ExecutionAttemptMappingProvider class can be removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 03:59:07 UTC 2023,,,,,,,,,,"0|z1hdt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 03:59;zhuzh;Thanks for reporting this issue and fixing it! [~JunRuiLi]

Done via bc859f6ffa9724459f7db1a7a4d3826a9ae67151;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
NumRecordsOut metrics broken for multiple outout chains,FLINK-31852,13533189,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,gyfora,gyfora,19/Apr/23 10:22,09/May/23 17:44,04/Jun/24 20:41,20/Apr/23 10:28,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,,,0,,,,,,"The numRecordsOut task metrics seem to be incorrect when there are multiple outputs in an operator chain. Consider a simple pipeline where you filter half of the stream for one sink and you get all the output for the other sink (and the sinks are not chained):

In Flink 1.16 we get the following incorrect metrics:

!image-2023-04-19-12-19-40-344.png!

The records sent/received metrics simply dont add up. The records sent seem to correspond to the output sent to the first downstream task (which contains the filtered stream, so about half of the events).

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18808,,,,,,,,,,,,,,,,,,,,,,"19/Apr/23 10:19;gyfora;image-2023-04-19-12-19-40-344.png;https://issues.apache.org/jira/secure/attachment/13057421/image-2023-04-19-12-19-40-344.png","19/Apr/23 10:21;gyfora;image-2023-04-19-12-21-35-385.png;https://issues.apache.org/jira/secure/attachment/13057420/image-2023-04-19-12-21-35-385.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 10:28:45 UTC 2023,,,,,,,,,,"0|z1hdso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 11:12;huwh;I think it's same with https://issues.apache.org/jira/browse/FLINK-31752;;;","19/Apr/23 11:17;gyfora;Part of it, but the part related to the chained output is a different issue;;;","19/Apr/23 11:18;gyfora;I will modify the ticket for that

 ;;;","20/Apr/23 09:29;Weijie Guo;IIUC, The reason for this bug is the same as FLINK-18808.;;;","20/Apr/23 10:28;gyfora;That is correct [~Weijie Guo] , I will close this! thank you;;;","20/Apr/23 10:28;gyfora;Duplicate of https://issues.apache.org/jira/browse/FLINK-18808;;;",,,,,,,,,,,,,,,,,,,,,
Remove unused KubernetesMultipleComponentLeaderElectionHaServicesFactory,FLINK-31851,13533175,13432464,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,Weijie Guo,Weijie Guo,19/Apr/23 09:03,23/Jun/23 07:58,04/Jun/24 20:41,23/Jun/23 07:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,"It seems that KubernetesMultipleComponentLeaderElectionHaServicesFactory is unused in the codebase and its code is exactly the same as KubernetesHaServicesFactory. We should remove it after FLIP-285. More aggressive, we can even remove both HaServicesFactory related to kubernetes and adopt a similar approach to HighAvailabilityServicesUtils#createZooKeeperHaServices.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32419,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 23 07:58:58 UTC 2023,,,,,,,,,,"0|z1hdpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/23 07:48;mapohl;I'm reopening that one because it's not only about removing {{KubernetesMultipleComponentLeaderElectionHaServicesFactory}} (which will be still covered by FLINK-32419) but also about changing the way how k8s HA is configured. I'm gonna rephrase the title and description to emphasize on the configuration part (which might require a ML discussion or FLIP since it's public API).

I'm gonna move the issue out of FLINK-26522 because it's out of scope for the FLIP-285 efforts.;;;","23/Jun/23 07:58;mapohl;I did some more code digging and noticed that we actually already support the fixed-string configuration in [HighAvailabilityServicesUtils:138|https://github.com/apache/flink/blob/fb05a7be9b828b7e582e75e4832443806fa4ff17/flink-runtime/src/main/java/org/apache/flink/runtime/highavailability/HighAvailabilityServicesUtils.java#L138]. There, we use the {{KubernetesHaServicesFactory}} and keep the generic class config option as a fallback. Therefore, there's nothing else to be done besides removing the {{KubernetesMultipleComponentLeaderElectionHaServicesFactory}} as part of FLINK-32419.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Fix args in JobSpec not being passed through to Flink in Standalone mode - 1.4.0,FLINK-31850,13533174,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,gil_shmaya,gil_shmaya,19/Apr/23 09:01,31/Aug/23 06:19,04/Jun/24 20:41,31/Aug/23 06:19,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"This issue is related to a previously fixed bug in version 1.2.0 -  [FLINK-29388] Fix args in JobSpec not being passed through to Flink in Standalone mode - ASF JIRA (apache.org)

I have noticed that while the args are successfully being passed when using version 1.2.0, this is not the case with version 1.4.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/23 15:54;gil_shmaya;image-2023-04-30-18-54-22-291.png;https://issues.apache.org/jira/secure/attachment/13057748/image-2023-04-30-18-54-22-291.png","30/Apr/23 16:42;gil_shmaya;image-2023-04-30-19-42-42-212.png;https://issues.apache.org/jira/secure/attachment/13057749/image-2023-04-30-19-42-42-212.png","30/Apr/23 16:56;gil_shmaya;image-2023-04-30-19-56-30-150.png;https://issues.apache.org/jira/secure/attachment/13057750/image-2023-04-30-19-56-30-150.png","30/Apr/23 16:56;gil_shmaya;image-2023-04-30-19-56-57-680.png;https://issues.apache.org/jira/secure/attachment/13057751/image-2023-04-30-19-56-57-680.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,FLINK-29388,https://github.com/apache/flink-kubernetes-operator/pull/379,,,,,,,,,,9223372036854775807,,,,,Mon May 08 10:43:36 UTC 2023,,,,,,,,,,"0|z1hdpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 07:30;gil_shmaya;Hi,
Could you kindly provide an update on the status, please? This bug blocks several tasks for us.
Thank you in advance for your help.;;;","28/Apr/23 02:36;bgeng777;hi [~gil_shmaya], I checked relevant codes in the [PR|https://github.com/apache/flink-kubernetes-operator/pull/379], it looks like the codes are not broken.
Could you please share your args example that worked in 1.2.0 while failed in 1.4.0?;;;","30/Apr/23 16:58;gil_shmaya;Hi [~bgeng777], thank you for your response.

I added a log that prints the argument array length at the beginning of the main  function of the flink job:
!image-2023-04-30-18-54-22-291.png|width=659,height=102!

The result when running with 1.2.0:
!image-2023-04-30-19-56-30-150.png!

The result when running with 1.4.0:
!image-2023-04-30-19-56-57-680.png!;;;","08/May/23 10:43;gil_shmaya;Hi [~bgeng777] , is there any update?;;;",,,,,,,,,,,,,,,,,,,,,,,
Enforce some some simple validation directly through the CRD,FLINK-31849,13533167,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ZhenqiuHuang,gyfora,gyfora,19/Apr/23 08:39,14/May/23 05:36,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"The CRD generator allows us to annotate some fields to add basic validation such as required, min/max etc. 

We should use these to simplify the validation logic.

https://github.com/fabric8io/kubernetes-client/blob/master/doc/CRD-generator.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 14 05:35:27 UTC 2023,,,,,,,,,,"0|z1hdns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/23 04:09;ZhenqiuHuang;[~gyfora]

May I take this?;;;","14/May/23 05:35;gyfora;Sure:);;;",,,,,,,,,,,,,,,,,,,,,,,,,
And Operator has side effect when operands have udf,FLINK-31848,13533160,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zju_zsx,zju_zsx,19/Apr/23 08:12,24/Apr/23 02:04,04/Jun/24 20:41,,1.13.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,," 
{code:java}
CREATE TABLE kafka_source (
   `content` varchar,
   `testid` bigint,
   `extra` int
   );
CREATE TABLE console_sink (
   `content` varchar,
   `testid` bigint
 )
  with (
    'connector' = 'print'
);
insert into console_sink
select 
   content,testid+1
from kafka_source where testid is not null and testid > 0 and my_udf(testid) != 0; {code}
my_udf has a constraint that the testid should not be null, but the testid is not null and testid > 0 does not take effect.

 

Im ScalarOperatorGens.generateAnd

!image-2023-04-19-14-54-46-458.png!

if left.nullTerm is true, right code will be execute 。

it seems that
{code:java}
if (!${left.nullTerm} && !${left.resultTerm}) {code}
can be safely replaced with 
{code:java}
if (!${left.resultTerm}){code}
? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/23 06:54;zju_zsx;image-2023-04-19-14-54-46-458.png;https://issues.apache.org/jira/secure/attachment/13057395/image-2023-04-19-14-54-46-458.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 24 02:04:05 UTC 2023,,,,,,,,,,"0|z1hdm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 08:49;csq;Hi,
The code block is under the null check required condition that the value of left.resultTerm might be null, then !${left.resultTerm} would cause syntax error.;;;","19/Apr/23 08:52;csq;And without null check, it can be simplified to be if (!${left.resultTerm}) as you mentioned and line 671 in ScalarOperatorGens.;;;","19/Apr/23 09:47;martijnvisser;[~csq] Thanks for the ticket, have you also verified this in the latest Flink version, since 1.13 is no longer supported?;;;","19/Apr/23 12:00;zju_zsx;[~csq] null check is checked in left.code,  left.resultTerm also set in left.code, the null check is just to avoid read incorrect value if field may be null, it seems won't cause syntax error.

[~martijnvisser] i have review the latest code,  it seems  still be  issue.;;;","21/Apr/23 13:02;jark;[~csq] do you have a simple case to reproduce the wrong result (and show the result)? And did you test it on the latest version? ;;;","21/Apr/23 18:04;csq;[~martijnvisser][~zju_zsx][~jark] Thanks for your reply. After running several test cases based on release-1.15 (which I previously analyzed) and the latest 1.18-SNAPSHOT branch, it seems that the left.resultTerm in the left.code has always been assigned with a boolean value and will never cause a syntax error by calling `if (!${left.resultTerm})`. Therefore I agree with [~zju_zsx] that the logic can be safely simplified. While with `if (!${left.nullTerm} && !${left.resultTerm})`, it would be a little bit more intuitive that ${left.nullTerm} indicates whether left result is UNKNOWN or not in three value logic.;;;","23/Apr/23 03:28;zju_zsx;[~csq]  Although using ${left.nullTerm} may make the code more intuitive, it could potentially lead to exceptions. On the other hand, if the left field is almost always null, omitting it could result in a more efficient program as it would skip the execution of the right code.

So i think we should simplify the code。;;;","23/Apr/23 08:58;csq;Hi [~zju_zsx]
take a table MyTable(a INT, b INT) for instance, there are three rows:
(1, 1), 
(null, 1), 
(3, 3)

and for the query

select * from MyTable where a < 2 and b < 2

the value of left.nullTerm and left.resultTerm would be:

false, true // a is not null, then evaluate the reuslt of a < 2 to be true, need to evaluate right code
true, false // a is null, means the result of a < 2 is UNKNOWN, no need to evaluate a < 2 but to evalute b < 2 as right.code
false, false // a is not null, then evaluate the result of a < 2 to be false, no need to evaluate right code

with if(!left.resultTerm), we assume null < 2 to be false, it would become:
left&&right
TRUE && 1 < 2  -> TRUE
FALSE && skipped -> FALSE
FALSE && skipped -> FALSE

the final result of the and operation  is:
nullTerm  resultTerm
false       false
false       false
false       true

but with if(!left.nullTerm && !left.resultTerm), it should be:
left&&right
TRUE && 1 < 2 -> TRUE
UNKNOWN && 1 < 2 -> UNKNWON
FALSE && skipped -> FALSE

the final result of the and operation is:
nullTerm  resultTerm
false       false
true       false
false       true

Seems after the simplification, the result is not consistent.;;;","23/Apr/23 09:14;csq;Here is a test case:

{code:scala}
  def testRowAndRow(): Unit = {
    val sqlQuery = ""SELECT cast(b > 2 and c < 3 as string),b,c FROM MyTableRow ""

    val data = List(
      Row.of(""Hello"", Int.box(1), Int.box(1)),
      Row.of(""Hello"", null, Int.box(1)),
      Row.of(""Hello again"", Int.box(3), Int.box(2)))

    implicit val tpe: TypeInformation[Row] = new RowTypeInfo(Types.STRING, Types.INT, Types.INT)

    val ds = env.fromCollection(data)

    val t = ds.toTable(tEnv, 'a, 'b, 'c)
    tEnv.createTemporaryView(""MyTableRow"", t)

    val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
    val sink = new TestingAppendSink
    result.addSink(sink)
    env.execute()

    val expected = List(""FALSE,1,1, TRUE,3,2, null,null,1"")
    assertEquals(expected.sorted, sink.getAppendResults.sorted)
  }
{code}
The expected result is List(FALSE,1,1, TRUE,3,2, null,null,1)
But get List(FALSE,1,1, FALSE,null,1, TRUE,3,2)
;;;","24/Apr/23 02:04;zju_zsx;[~csq] tks, you are right, this breaks three value logic.  Then the issue I am facing is that when the null check is true, Calcite should not optimize the expression 'testid is not null and testid > 0' to simply 'testid > 0'. I'll check if this resolved in latest version.;;;",,,,,,,,,,,,,,,,,
Flink Connector JDBC CI Failing ,FLINK-31847,13533153,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,dannycranmer,dannycranmer,19/Apr/23 07:14,19/Apr/23 07:32,04/Jun/24 20:41,19/Apr/23 07:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Build System / CI,Connectors / JDBC,,,,,0,,,,,,"The CI is failing, or Flaky:
 * [https://github.com/apache/flink-connector-jdbc/actions/runs/4712658186|https://github.com/apache/flink-connector-jdbc/pull/41]
 * https://github.com/apache/flink-connector-jdbc/actions/runs/4710470564",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31770,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-19 07:14:03.0,,,,,,,,,,"0|z1hdko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support cancel final checkpoint when all tasks are finished,FLINK-31846,13533127,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hongfanxo,hongfanxo,19/Apr/23 02:15,23/Apr/23 02:44,04/Jun/24 20:41,21/Apr/23 11:34,1.15.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,,,"As stated in [1], all tasks will wait for the final checkpoint before exiting. It also mentioned this mechanism will prolong the execution time.

So, can we provide configurations to make tasks NOT wait for the final checkpoint?

 

 [1]: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/#waiting-for-the-final-checkpoint-before-task-exit",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28386,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 23 02:44:08 UTC 2023,,,,,,,,,,"0|z1hdew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 07:39;martijnvisser;[~hongfanxo] What is your use case for not wanting to wait for the final checkpoint?;;;","20/Apr/23 01:55;hongfanxo;In essence, I am using Flink for processing bounded data streams. When dealing with large data, the checkpoint data can also become sizable. However, once the processing is successfully completed, all checkpoint data becomes no longer needed.

Therefore, I am looking to cancel the final checkpoint, which would also include all ongoing checkpoints.;;;","20/Apr/23 11:47;pnowojski;Hey, [~hongfanxo], your feature request would be very risky, as lack of final checkpoint could cause data losses. Records are committed do downstream systems only on checkpoints, so without final checkpoint, final records wouldn't be committed. Usually:

If your jobs are short (so you don't need to have checkpoints in case of failure recovery and you can simply start over), and you don't care about committing results, you can simply disable checkpointing.
If your jobs are bounded, but long lived, one extra checkpoint doesn't matter.;;;","21/Apr/23 07:12;hongfanxo;Hi, [~pnowojski] . Thank you for explaining. I am actually a novice when it comes to checkpoints.

If the final checkpoint cannot be cancelled, can we bring it forward to start upon completion of all tasks? It at least saves the time between the final checkpoint and the one before the final one.

 

Your suggestions are valuable and I will consider whether they can be applied to my work. In my cases, I use general codes to process bounded datasets of various sizes. Therefore, I am in search of a standardized checkpoint interval that can be uniformly applied to all jobs. Waiting for an extra checkpoint could be costly for certain bounded datasets.

 ;;;","21/Apr/23 11:07;pnowojski;{quote}
f the final checkpoint cannot be cancelled, can we bring it forward to start upon completion of all tasks?
{quote}
I'm afraid it's not possible at the moment, it's a valid feature request. There is even a ticket for that https://issues.apache.org/jira/browse/FLINK-28386.

I'm not sure how helpful is this, but you can also manually trigger checkpoints:
https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/rest_api/#jobs-jobid-checkpoints-1

Alternatively, you can also disable checkpoints with finished tasks. If your sources are finishing more or less all at once, the only thing you are sacrificing are:
* not being able to checkpoint job if some sources have already finished, while others are still working
* exactly-once semantic (like committing transactions to Kafka). But you could still use Kafka sink with at-least-once semantic.;;;","21/Apr/23 11:34;pnowojski;I will close this ticket, as I think it just duplicates FLINK-28386, feel free to re-open if I missed something. Otherwise, let's maybe move the discussion to the other ticket.;;;","23/Apr/23 02:44;hongfanxo;[~pnowojski]  Okay, I will explore these options further.;;;",,,,,,,,,,,,,,,,,,,,
Make KubernetesStepDecorator Pluggable,FLINK-31845,13533064,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,darenwkt,darenwkt,18/Apr/23 15:27,05/Jul/23 14:09,04/Jun/24 20:41,01/May/23 10:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Currently, KubernetesStepDecorator is used by StandaloneKubernetesJobManagerFactory and StandaloneKubernetesTaskManagerFactory to decorate the JM and TM flink pods such as renaming the containers.

 

This Jira propose an improvement to utilise Flink plugin mechanism to load custom KubernetesStepDecorator as a plugin JAR to decorate the JM or TM. The proposal is for Flink Kubernetes Operator at the moment but this can also be done in flink-kubernetes package.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 01 10:16:18 UTC 2023,,,,,,,,,,"0|z1hd0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 17:56;Weijie Guo;Thanks [~darenwkt] for bring this. IIUC, this should be the same issue as FLINK-28831 on flink side. I'm a bit curious:  Can we only make this change in {{flink-kubernetes}} and use it directly in the operator repository? cc [~gyfora] for future information.;;;","19/Apr/23 01:23;gyfora;I agree, this is the duplicate of FLINK-28831 and it should be fixed on the Flink-Kubernetes side first and not on the operator.;;;","01/May/23 10:16;gyfora;[~darenwkt] can you please help review on the original ticket https://issues.apache.org/jira/browse/FLINK-28831 ?;;;",,,,,,,,,,,,,,,,,,,,,,,,
DefaultPackagedProgramRetriever#getClasspathsFromUserLibDir supports loading profile,FLINK-31844,13533047,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Bo Cui,Bo Cui,18/Apr/23 13:04,18/Apr/23 13:06,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,,,,,,"now getClasspathsFromUserLibDir only load JarFile, i dont know why, but i think we can add non-jar files to classpath [~mapohl]
{code:java}
      private static List<URL> getClasspathsFromUserLibDir(@Nullable File userLibDir)
            throws IOException {
        if (userLibDir == null) {
            return Collections.emptyList();
        }

        final Path workingDirectory = FileUtils.getCurrentWorkingDirectory();
        final List<URL> relativeJarURLs =
                FileUtils.listFilesInDirectory(userLibDir.toPath(), FileUtils::isJarFile).stream()
                        .map(path -> FileUtils.relativizePath(workingDirectory, path))
                        .map(FunctionUtils.uncheckedFunction(FileUtils::toURL))
                        .collect(Collectors.toList());
        return Collections.unmodifiableList(relativeJarURLs);
    }
{code}
https://github.com/apache/flink/blob/9538fdaab2948a2e3dd068925d936ac0777301de/flink-clients/src/main/java/org/apache/flink/client/program/DefaultPackagedProgramRetriever.java#L227",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21445,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-18 13:04:09.0,,,,,,,,,,"0|z1hcx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select slots from SlotPool#freeSlots in bulk,FLINK-31843,13533039,13532177,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,18/Apr/23 12:13,08/Aug/23 02:45,04/Jun/24 20:41,23/Jul/23 08:55,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"we should also reduce the number of calls of ""getFreeSlotInformations"". In current implementation, the scheduler will batch request slots for tasks in the same pipeline region(ExecutionSlotAllocator#allocateSlotsFor), but the slot allocator will process these requests one by one, and call ""getFreeSlotInformations"" once for each request.

We can optimize it to call ""getFreeSlotInformations"" once for a bulk (of slot requests), instead of once for each slot request. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32768,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 23 08:55:43 UTC 2023,,,,,,,,,,"0|z1hcvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 08:55;guoyangze;master:
3efbae44b4a21c9735f1a3054c276c32bbc46cdd
f1a079f95e409d3a81ff5dd199ea3c0dc30470cc
a69f96514f3da893f6a048b107ea287411f58714
3cdf228191a073c7008f88faa0b088c6a2428012
72bff2a2d0072602e4e625476bf5480dc50dc76c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
calculate task executor's utilization only in EvenlySpreadOutLocationPreferenceSlotSelectionStrategy,FLINK-31842,13533038,13532177,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,18/Apr/23 12:09,25/Apr/23 07:43,04/Jun/24 20:41,25/Apr/23 07:43,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"Currently DefaultAllocatedSlotPool#getFreeSlotsInformation always calculates the taskExecutorUtilization. This causes task schedules to be too slow when there are lots of slots, such as 20000 slots total. But only the EvenlySpreadOutLocationPreferenceSlotSelectionStrategy uses this utilization.

So I would like to move the calculation of taskExecutorUtilization to usage. DefaultAllocatedSlotPool provides a function: getTaskExecutorUtilization, and is only used in EvenlySpreadOutLocationPreferenceSlotSelectionStrategy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 07:43:26 UTC 2023,,,,,,,,,,"0|z1hcv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 07:43;Weijie Guo;master(1.18) via fa053e3f1f3b7317d7b9dfbf91d7f4accc50b100.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Redundant local variables in AllWindowedStream#reduce,FLINK-31841,13533037,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Wencong Liu,Wencong Liu,Wencong Liu,18/Apr/23 12:00,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,API / DataStream,,,,,,0,pull-request-available,stale-assigned,,,,"Currently, there is two redundant local variables in AllWindowedStream#reduce.
{code:java}
public SingleOutputStreamOperator<T> reduce(ReduceFunction<T> function) {
    if (function instanceof RichFunction) {
        throw new UnsupportedOperationException(
                ""ReduceFunction of reduce can not be a RichFunction. ""
                        + ""Please use reduce(ReduceFunction, WindowFunction) instead."");
    }

    // clean the closure
    function = input.getExecutionEnvironment().clean(function);

    String callLocation = Utils.getCallLocationName();
    String udfName = ""AllWindowedStream."" + callLocation;

    return reduce(function, new PassThroughAllWindowFunction<W, T>());
} {code}
`callLocation` and `udfName` are not used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 10:35:01 UTC 2023,,,,,,,,,,"0|z1hcuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in operators.window.slicing.SliceAssigners$AbstractSliceAssigner.assignSliceEnd,FLINK-31840,13533036,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,rmetzger,rmetzger,18/Apr/23 11:55,18/Apr/23 14:24,04/Jun/24 20:41,18/Apr/23 11:57,,,,,,,,,,,,,,,,,,,,,1.16.0,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,,,"While running a Flink SQL Query (with a hop window), I got this error.
{code}
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at StreamExecCalc$11.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	... 23 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners$AbstractSliceAssigner.assignSliceEnd(SliceAssigners.java:558)
	at org.apache.flink.table.runtime.operators.aggregate.window.LocalSlicingWindowAggOperator.processElement(LocalSlicingWindowAggOperator.java:114)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	... 29 more
{code}

It was caused by a timestamp field containing NULL values.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 18 14:24:06 UTC 2023,,,,,,,,,,"0|z1hcuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 11:55;rmetzger;This issue was fixed in https://github.com/apache/flink/pull/20302/files;;;","18/Apr/23 11:56;rmetzger;Note: I've filed this ticket just for tracking purposes, because I couldn't find any information about this error on the internet. Now the problem and solution is at least publicly available.;;;","18/Apr/23 12:49;lincoln.86xy;[~rmetzger]thanks for creating this ticket! Now the issue can be searched more easily (it reminds me that it's really more important for users to create a jira rather than just a hotfix of #20302);;;","18/Apr/23 14:24;rmetzger;Yeah, I wanted to mention the stack trace and fixed version somewhere, Jira is ideal for that.

Ideally, your PR should have included a test case to make sure nobody is breaking this in the future.;;;",,,,,,,,,,,,,,,,,,,,,,,
Token delegation fails when both flink-s3-fs-hadoop and flink-s3-fs-presto plugins are used,FLINK-31839,13533035,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,18/Apr/23 11:52,20/Apr/23 13:38,04/Jun/24 20:41,19/Apr/23 16:50,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,FileSystems,,,,,,0,pull-request-available,,,,,"{code:java}
2023-04-07 09:18:32,814 [main] ERROR
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] -
Failed to initialize delegation token provider s3
java.lang.IllegalStateException: Delegation token provider with service
name {} has multiple implementations [s3]
at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:133)
~[flink-dist-1.17.0.jar:1.17.0]
at java.util.Iterator.forEachRemaining(Unknown Source) ~[?:?]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.loadProviders(DefaultDelegationTokenManager.java:156)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.<init>(DefaultDelegationTokenManager.java:111)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerFactory.create(DefaultDelegationTokenManagerFactory.java:50)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:392)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
~[flink-dist-1.17.0.jar:1.17.0]
at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
at javax.security.auth.Subject.doAs(Unknown Source) [?:?]
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
[hadoop-common-2.8.5.jar:?]
at
org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.main(StandaloneApplicationClusterEntryPoint.java:82)
[flink-dist-1.17.0.jar:1.17.0]
2023-04-07 09:18:32,824 [main] INFO
org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting
StandaloneApplicationClusterEntryPoint down with application status FAILED.
Diagnostics org.apache.flink.util.FlinkRuntimeException:
java.lang.IllegalStateException: Delegation token provider with service
name {} has multiple implementations [s3]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:151)
at java.base/java.util.Iterator.forEachRemaining(Unknown Source)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.loadProviders(DefaultDelegationTokenManager.java:156)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.<init>(DefaultDelegationTokenManager.java:111)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerFactory.create(DefaultDelegationTokenManagerFactory.java:50)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:392)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
at java.base/java.security.AccessController.doPrivileged(Native Method)
at java.base/javax.security.auth.Subject.doAs(Unknown Source)
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
at
org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
at
org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.main(StandaloneApplicationClusterEntryPoint.java:82)
Caused by: java.lang.IllegalStateException: Delegation token provider with
service name {} has multiple implementations [s3]
at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:133)
... 14 more
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 19 16:50:36 UTC 2023,,,,,,,,,,"0|z1hcug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 12:17;mbalassi;839b3b9 in master;;;","19/Apr/23 16:50;mbalassi;af4c68a in release-1.17.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Move thread handling from DefaultMultipleComponentLeaderElectionService into DefaultLeaderElectionService,FLINK-31838,13533030,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,18/Apr/23 11:14,14/Jun/23 11:23,04/Jun/24 20:41,05/May/23 10:18,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,"While working on FLINK-31773 we noticed in the PR (see [comment #1|https://github.com/apache/flink/pull/22380#discussion_r1168724716] and [comment #2|https://github.com/apache/flink/pull/22380#discussion_r1168771139]) that there's a different notion of responsibility of the thread handling between the FLINK-24038 and the pre-FLINK-24038 implementation of the leader election. The latter one (i.e. 
{{{}DefaultLeaderElectionService{}}}) doesn't need to deal with where {{onGrantLeadership}} and {{onRevokeLeadership}} are called because those are called in a separate thread in the driver. The new implementation (i.e. 
{{{}DefaultMultipleComponentLeaderElectionService{}}}) has to call the {{onGrantLeadership}} and {{onRevokeLeadership}} methods itself when registering new contenders. This would happen in the main thread of the {{LeaderContender}} which is not suitable. {{DefaultMultipleComponentLeaderElectionService}} solves this by introducing its own threadpool.

The separation of driver and contender in FLINK-31773 requires such a executionService in {{DefaultLeaderElectionService}} as well before going ahead with FLINK-31773.",,,,,,,,,,,,,,,,,,,,,FLINK-31773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31878,,,,,,,,,FLINK-31995,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 05 10:18:39 UTC 2023,,,,,,,,,,"0|z1hctc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 10:18;mapohl;master:
* b814c369256f011f18fc7283d60cd4a27593d988

with depending hotfix commits:
* ed195cfedbbcee45b9eee33b054a209b20f09b39
* 1d208f13d8954c68a10c1b380b3b0dfd00921f63
* 43688339721176c8b8070fb19ac1dcda990f9017;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Move LeaderElectionDriver instantiated into DefaultLeaderElectionService.register to instantiate the driver lazily,FLINK-31837,13533007,13542119,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,18/Apr/23 09:37,09/Jul/23 16:12,04/Jun/24 20:41,09/Jul/23 16:12,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"The {{LeaderElectionDriver}} should only be instantiated if there is a contender which wants to participate in the leader election. Therefore, we should call {{LeaderElectionDriver}} instantiation lazily in {{DefaultLeaderElectionService#register}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 09 16:12:13 UTC 2023,,,,,,,,,,"0|z1hco8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 11:27;mapohl;{quote}
With the {{MultipleComponentLeaderElection*}} classes we added a circular dependency between the {{DefaultLeaderElectionService}} and the {{DefaultMultipleComponentLeaderElectionService}} which calls the {{DefaultLeaderElectionService.onGrantLeadership}} while registering the service in [DefaultMultipleComponentLeaederElectionService:152|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultMultipleComponentLeaderElectionService.java#L152]. This call will result in accessing the {{DefaultLeaderElectionService}} instance which is still in instantiation phase.

We're losing the circular dependency with the {{MultipleComponentsLeaderElectionDriverAdapter}} class going away. Hence, we can go ahead and move the driver instantiation into the constructor.
{quote}

Instead of creating the driver in the constructor, we should instantiate it as late as possible (i.e. only if the first contender is registered). We ran into a problem where we wanted to move the {{DefaultLeaderElectionService}} instantiation into the constructor of the {{HighAvailabilityServices}} implementations. This then blocked certain tests because the instantiation of the driver caused the corresponding JVM process to participate in the leader election in any way (even if the JVM process does not mean to participate in the {{LeaderElections}} like the TaskManager). In a scenario where the TaskManager starts first, it would block the JobManager from acquiring the leadership.

I'm gonna update the Jira issues description and title accordingly.;;;","30/Jun/23 15:04;mapohl;Fixing this would also help removing the lazy instantiation that is introduced with FLINK-32409;;;","09/Jul/23 16:12;mapohl;master: d2e6b94b374a40551a5cb1a81af2683f4a9ce86d;;;",,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Calcite version to 1.34.0,FLINK-31836,13533005,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Apr/23 09:18,03/Feb/24 02:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,Calcite 1.34.0 has been released https://calcite.apache.org/news/2023/03/14/release-1.34.0/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31362,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 18 09:57:45 UTC 2023,,,,,,,,,,"0|z1hcns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 09:35;yunta;I wonder will this upgrade break the compatibility of FlinkSQL jobs?;;;","18/Apr/23 09:57;Sergey Nuyanzin;As far as I know there is no full support for compatibility between 2 major upgrades for Table API/SQL and this is explicitly mentioned in doc[1]
From another side there is a FLIP-190 to improve this situation [2]


[1] https://nightlies.apache.org/flink/flink-docs-master/docs/ops/upgrading/#table-api–sql

[2] https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489;;;",,,,,,,,,,,,,,,,,,,,,,,,,
DataTypeHint don't support Row<i Array<int>>,FLINK-31835,13533004,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,jeff-zou,jeff-zou,18/Apr/23 09:10,07/Dec/23 12:55,04/Jun/24 20:41,12/Jun/23 09:53,1.15.4,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,"Using DataTypeHint(""Row<t ARRAY<INT>>"") in a UDF gives the following error:

 
{code:java}
Caused by: java.lang.ClassCastException: class [I cannot be cast to class [Ljava.lang.Object; ([I and [Ljava.lang.Object; are in module java.base of loader 'bootstrap')
org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.toInternal(ArrayObjectArrayConverter.java:40)
org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:75)
org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:37)
org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
StreamExecCalc$251.processElement_split9(Unknown Source)
StreamExecCalc$251.processElement(Unknown Source)
org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) {code}
 

The function is as follows:
{code:java}
@DataTypeHint(""Row<t ARRAY<INT>>"")
public Row eval() {
int[] i = new int[3];
return Row.of(i);
} {code}
 

This error is not reported when testing other simple types, so it is not an environmental problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33523,FLINK-33547,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 07 12:55:31 UTC 2023,,,,,,,,,,"0|z1hcnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 04:02;jark;Hi [~jeff-zou], thanks for reporting this. I think this is expected. You should declare the type hint as ""ARRAY<INT NOT NULL>"" to map to {{int[]}}. ""ARRAY<INT>"" is recognized as a result of {{Integer[]}}. That's why the conversion class is {{ArrayObjectArrayConverter}}. 

But I think the exception message can be improved. ;;;","23/Apr/23 14:14;aitozi;In this case, ARRAY<INT NOT NULL> will still maps to {{ArrayObjectArrayConverter}}. I think it's not expected;;;","23/Apr/23 14:31;aitozi;When creating Array CollectionDataType it will use the array's element type to construct an array conversion class. But the conversionClass is {{Integer}} for both  {{INT NOT NULL}} and {{INT}}. So the conversion class for Array<INT NOT NULL> will become {{Integer[]}}


{code:java}
if (logicalType.getTypeRoot() == LogicalTypeRoot.ARRAY && clazz == null) {
    return Array.newInstance(elementDataType.getConversionClass(), 0).getClass();
}
{code}


;;;","23/Apr/23 15:13;aitozi;The first thought in my mind is that the conversion class for atomic type eg: {{IntType}} should respect to the nullability. So, when a {{IntType}} copy from nullable to not null. Its default conversionClass will change from {{Integer}} to {{int}}.

And in the DataType: {{AtomicType}} and {{CollectionDataType}} should also respect to the nullable and notNull call. The conversionClass of the dataType should be changed after these call.

I have verified this locally, it can solve this problem, what do you think this solution [~jark] ?;;;","24/Apr/23 09:05;jark;This sounds good to me. My only concern is the compatibility problem. What do you think [~twalthr]? Is there any potential problems?;;;","25/Apr/23 08:00;jeff-zou;hi [~jark] [~aitozi] 

Under Flink 1.15, how do I solve this problem?  Even I have change the hint to Array<int not null>, it's still reporting this error:
{code:java}
Caused by: java.lang.ClassCastException: [I cannot be cast to [Ljava.lang.Object;
    at org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.toInternal(ArrayObjectArrayConverter.java:40)
    at org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:75)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:37)
    at org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
    at StreamExecCalc$278.processElement_trueFilter1_split13(Unknown Source)
    at StreamExecCalc$278.processElement_trueFilter1(Unknown Source)
    at StreamExecCalc$278.processElement_split3(Unknown Source) {code};;;","25/Apr/23 10:25;aitozi;Yes, the reason is shown above. I have pushed a PR to try to solve this. But it needs some discussion to avoid break the compatibility. ;;;","26/Apr/23 05:39;aitozi;[~jark] The PR have passed the CI and I think the current solution will not cause compatibility problem by only fix the conversion class according to the nullability when creating CollectionDataType, could you help review that ?;;;","12/Jun/23 09:53;jark;Fixed in master: a6adbdda0cdf90635f0cd7a3427486bced301fbd;;;","15/Nov/23 03:15;xuyangzhong;Hi, [~aitozi]. Sorry for this noise, can you check if the incompatible changes that may be caused by this pr expected? 
 # FLINK-33523
 # FLINK-33547;;;","07/Dec/23 12:55;twalthr;{quote}
My only concern is the compatibility problem. What do you think Timo Walther? Is there any potential problems?
{quote}

Sorry that I did not answer earlier [~jark]. But my answer would have been yes, there are compatibility problems.

Hints are not perfect. Sometimes a user needs to override the {{getTypeInference()}} with
{code}
outputTypeStrategy(() -> DataTypes.ARRAY(INT().notNull()).bringedTo(int[].class))
{code}

if a class other that the default class should be used. The default class of INT is Integer, thus the default class for array is Integer[].;;;",,,,,,,,,,,,,,,,
Azure Warning: no space left on device,FLINK-31834,13533001,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,18/Apr/23 08:17,05/May/23 06:47,04/Jun/24 20:41,19/Apr/23 10:08,,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Build System / Azure Pipelines,,,,,,0,build-stability,pull-request-available,,,,"In this CI run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=841082b6-1a93-5908-4d37-a071f4387a5f&l=21

There was this warning:
{code}
Loaded image: confluentinc/cp-kafka:6.2.2
Loaded image: testcontainers/ryuk:0.3.3
ApplyLayer exit status 1 stdout:  stderr: write /opt/jdk-15.0.1+9/lib/modules: no space left on device
##[error]Bash exited with code '1'.
Finishing: Restore docker images
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 05 06:47:32 UTC 2023,,,,,,,,,,"0|z1hcmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 14:39;rmetzger;Looking closer at this issue, I notice:

a) this message didn't fail the build. Probably the caching didn't properly work
b) the cleanup script runs after this caching step. It reports 3.9 GB of free disk space: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=affb2083-df4e-5398-e502-35356824fd45&l=290 After the cleanup script. 32GB of disk space is available: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=affb2083-df4e-5398-e502-35356824fd45&l=1804

It looks like I can move the cleanup script before the caching. I'll try that.;;;","19/Apr/23 10:08;rmetzger;Merged to master in https://github.com/apache/flink/commit/15c4d88eb78cb8f702e7e56563cb10e49d424b14;;;","20/Apr/23 18:17;Sergey Nuyanzin;1.16: https://github.com/apache/flink/commit/1e2c2e69590d7a55e68bcb87ab0d1af1894a8ae5;;;","24/Apr/23 14:14;mapohl;[~rmetzger] [~snuyanzin] is there a reason why we backported the fix to 1.16 but not 1.17?;;;","05/May/23 06:47;rmetzger;No, I think that's just an oversight.
I've just pushed to 1.17: https://github.com/apache/flink/commit/91dfb22e0bc7ac10a9a9f59cd9da6d62a723dadd;;;",,,,,,,,,,,,,,,,,,,,,,
Support fusion codegen for multiple operators,FLINK-31833,13532997,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,godfrey,godfrey,18/Apr/23 08:02,25/Jul/23 11:26,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,Please see FLIP-315 for more detail: https://cwiki.apache.org/confluence/display/FLINK/FLIP-315+Support+Operator+Fusion+Codegen+for+Flink+SQL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 25 11:26:06 UTC 2023,,,,,,,,,,"0|z1hcm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/23 11:26;knaufk;[~lsy] Does this feature provide a user-facing change in Flink 1.18? If so, is it already documented?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add benchmarks for end to end  restarting tasks,FLINK-31832,13532977,13532177,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,18/Apr/23 06:04,25/May/23 14:14,04/Jun/24 20:41,25/May/23 10:53,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Benchmarks,Runtime / Coordination,,,,,0,pull-request-available,,,,,"As discussed in https://issues.apache.org/jira/browse/FLINK-31771. 

We need a benchmark for job failover and end to end restarting tasks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 25 14:14:13 UTC 2023,,,,,,,,,,"0|z1hci0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 14:14;wanglijie;Done via:
flink master: 7d529a5a167f52c1b76fd6f70918b27149cf8782
flink-benchmarks master: 8b32b75e06ec51ee6ec21304c6e473159e0ef3ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure is unstable,FLINK-31831,13532973,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wanglijie,Sergey Nuyanzin,Sergey Nuyanzin,18/Apr/23 05:40,26/Apr/23 06:32,04/Jun/24 20:41,26/Apr/23 06:32,1.18.0,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48212&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8399

{noformat}
Apr 18 04:17:09 [ERROR] org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure  Time elapsed: 2.844 s  <<< FAILURE!
Apr 18 04:17:09 java.lang.AssertionError: Failed to initialize the cluster entrypoint .
Apr 18 04:17:09 	at org.junit.Assert.fail(Assert.java:89)
Apr 18 04:17:09 	at org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure(TaskManagerDisconnectOnShutdownITCase.java:136)
Apr 18 04:17:09 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 18 04:17:09 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 18 04:17:09 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)


{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 06:32:06 UTC 2023,,,,,,,,,,"0|z1hch4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 06:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48358&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10562;;;","24/Apr/23 05:20;wanglijie;{code:java}
04:17:08,719 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting  down with application status FAILED. Diagnostics org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
  at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:288)
  at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:293)
  at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1938)
  at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
  at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
  at org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure(TaskManagerDisconnectOnShutdownITCase.java:117)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
  at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
  at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
  at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
  at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
  at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
  at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
  at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
  at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
  at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.net.BindException: Could not start rest endpoint on any port in port range 8081
  at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:281)
  at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:177)
  ... 65 more
{code}

This failure is caused by port bind exception. I will prepare a fix PR soon to use random port for this case.;;;","26/Apr/23 06:32;wanglijie;Fixed via:
master: aa2110fdb23db1f7b927c31c51b18bf1e01a1f5f
release-1.17: 1651908f02b7990e25d7fdde428aac44473cde65
release-1.16: 5eb7188f732c9091e0f8322acae77e4bbd806343;;;",,,,,,,,,,,,,,,,,,,,,,,,
Coalesce on nested fields with different nullabilities will get wrong plan,FLINK-31830,13532964,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,qingyue,lincoln.86xy,lincoln.86xy,18/Apr/23 02:06,21/May/24 12:40,04/Jun/24 20:41,,1.14.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,1,,,,,,"A test case similar to FLINK-31829, only changes the nullable field `a.np` to not null, will get a wrong plan in 1.14.x (reported from the community user):

{code}
  @Test
  def testCoalesceOnNestedColumns(): Unit = {
    val tEnv = util.tableEnv
    val tableDescriptor = TableDescriptor.forConnector(""datagen"")
        .schema(Schema.newBuilder
            .column(""id"", DataTypes.INT.notNull)
            .column(""a"", DataTypes.ROW(DataTypes.FIELD(""np"", DataTypes.INT.notNull())).nullable)
            .build)
        .build
    tEnv.createTemporaryTable(""t1"", tableDescriptor)
    tEnv.createTemporaryTable(""t2"", tableDescriptor)
    val res = tEnv.executeSql(""EXPLAIN SELECT a.id, COALESCE(a.a.np, b.a.np) c1, IFNULL(a.a.np, b.a.np) c2 FROM t1 a left JOIN t2 b ON a.id=b.id where a.a is null or a.a.np is null"")
    res.print()
}  

== Abstract Syntax Tree ==
LogicalProject(id=[$0], c1=[CAST($1.np):INTEGER], c2=[IFNULL($1.np, $3.np)])
+- LogicalFilter(condition=[OR(IS NULL($1), IS NULL(CAST($1.np):INTEGER))])
   +- LogicalJoin(condition=[=($0, $2)], joinType=[left])
      :- LogicalTableScan(table=[[default_catalog, default_database, t1]])
      +- LogicalTableScan(table=[[default_catalog, default_database, t2]])
{code}

the top project in the ast is wrong:  `LogicalProject(id=[$0], c1=[CAST($1.np):INTEGER], c2=[IFNULL($1.np, $3.np)])`, the `c1=[CAST($1.np):INTEGER]` relate to `COALESCE(a.a.np, b.a.np) c1` is incorrect,
but this works fine when using sql ddl to create tables
{code}
  @Test
  def testCoalesceOnNestedColumns2(): Unit = {
    val tEnv = util.tableEnv
    tEnv.executeSql(
      s""""""
         |create temporary table t1 (
         |  id int not null,
         |  a row<np int not null>
         |) with (
         | 'connector' = 'datagen'
         |)
         |"""""".stripMargin)
    tEnv.executeSql(
      s""""""
         |create temporary table t2 (
         |  id int not null,
         |  a row<np int not null>
         |) with (
         | 'connector' = 'datagen'
         |)
         |"""""".stripMargin)
    val res = tEnv.executeSql(
      ""EXPLAIN SELECT a.id, COALESCE(a.a.np, b.a.np) c1, IFNULL(a.a.np, b.a.np) c2 FROM t1 a left JOIN t2 b ON a.id=b.id where a.a is null or a.a.np is null"")
    res.print()
  }
{code}
from 1.15, the coalesce will be a new builtin function, and the ast looks correct in version 1.15+, while before 1.15 it was rewritten as `case when`


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32464,,,,,,,FLINK-33597,FLINK-30282,FLINK-31829,FLINK-33217,,,,,,,,,,,,"09/Jun/23 07:06;qingyue;image-2023-06-09-15-06-01-322.png;https://issues.apache.org/jira/secure/attachment/13058883/image-2023-06-09-15-06-01-322.png","09/Jun/23 07:21;qingyue;image-2023-06-09-15-21-13-720.png;https://issues.apache.org/jira/secure/attachment/13058884/image-2023-06-09-15-21-13-720.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 25 02:06:28 UTC 2023,,,,,,,,,,"0|z1hcf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 02:45;qingyue;I'd like to take this ticket. cc [~lincoln.86xy] ;;;","18/Apr/23 03:23;Weijie Guo;We'd better make sure, if 1.15 and later have no issues then there shouldn't be a need for a fix since 1.14 is no longer supported.;;;","09/Jun/23 07:23;qingyue;It takes some time to reason out the cause, and sorry for the late update. 
h4. 1. Identify the problem

The issue has been reproduced using Flink release-1.14.6 (which depends on calcite-1.26).
h4. 2. Why SQL gets a correct plan, while API doesn't

First, the resolved schema differs. You can verify this by the following code snippet.
{code:scala}
 @Test
  def testSchema(): Unit = {
    // create temporary table t1
    val tableDescriptor = TableDescriptor.forConnector(""datagen"")
      .schema(Schema.newBuilder
        .column(""id"", DataTypes.INT.notNull)
        .column(""a"", DataTypes.ROW(DataTypes.FIELD(""np"", DataTypes.INT.notNull())).nullable)
        .build)
      .build
    tableEnv.createTemporaryTable(""t1"", tableDescriptor)

    // create temporary table t2
    tableEnv.executeSql(
      s""""""
         |create temporary table t2 (
         |  id int not null,
         |  a row<np int not null>
         |) with (
         | 'connector' = 'datagen'
         |)
         |"""""".stripMargin)

    val catalogManager = tableEnv.asInstanceOf[StreamTableEnvironmentImpl].getCatalogManager
    val result1 =
      catalogManager.getTable(ObjectIdentifier.of(""default_catalog"", ""default_database"", ""t1""))
    val result2 =
      catalogManager.getTable(ObjectIdentifier.of(""default_catalog"", ""default_database"", ""t2""))
    println(result1.get().getResolvedSchema)
    println(result2.get().getResolvedSchema)
  }
{code}
The result will be
{code:sql}
--result1
(
  `id` INT NOT NULL,
  `a` ROW<`np` INT NOT NULL>
)

--result2  
(
  `id` INT NOT NULL,
  `a` ROW<`np` INT> -- changed by org.apache.calcite.sql.SqlDataTypeSpec#fixUpNullability
)
{code}
You can tell from the print result that the nullability specified by the user does not get respect. However, this is a by-design behavior for Calcite. The community has an in-depth discussion CALCITE-2464 on the semantics of setting nullability for structured type.

TL;DR

!image-2023-06-09-15-06-01-322.png|width=844,height=219!

This feature is introduced in Calcite 1.19.

 

As a result, the DDL `a` ROW<`np` INT NOT NULL> will be rewritten to `a` ROW<`np` INT> during the SQL-to-operation conversion.

(Please check org.apache.calcite.sql.SqlDataTypeSpec#fixUpNullability for more details).

 

As for the plan, I assume based on the schema produced by API
{code:java}
( `id` INT NOT NULL, `a` ROW<`np` INT NOT NULL> ) {code}
the optimization rules work as expected.

 

The filter condition after the left outer join is
{code:java}
where a.a is null or a.a.np is null {code}
and can be reduced and pushdown as 
{code:java}
where a.a is null {code}
since the nullability of a.np is always false.

 

And RemoveUnreachableCoalesceArgumentsRule matches the following case
{code:java}
COALESCE(a.a.np, b.a.np) c1 {code}
because a.a.np is never nullable, so the invocation of coalesce is reduced. 

!image-2023-06-09-15-21-13-720.png|width=892,height=575!

 
h4. 3. Conclusion

So in a nutshell, we should align the structured type nullability created through API with SQL. The correct result for API should follow the example `testNullabilityForSql` given in FLINK-30282. Last but not least, we should improve the document and add a description of the nullability of structured type, o.w. it might not be straightforward for users to understand.
h4.  ;;;","13/Jun/23 09:47;qingyue;The issue is described in the previous comment. However, aligning the behavior between Table API & SQL might be a breaking change to the current design. Since RowType(extends LogicalType), and FieldsDataType(extends DataType) are all public APIs, I think we need a discussion before rushing to the fix. 
h4. TL;DR

1. The nullability of row type created via SQL and Table API is inconsistent.
ROW<f0 INT NOT NULL> is not a valid type for SQL, but Table API accepts it.

2. Table API should align the behavior to SQL. 
2.1 RowType.copy(boolean nullable) should also set the inner fields to null if nullable is true.
2.2 RowType's constructor should also check nullability.
2.3 FieldsDataType.nullable() should also set the inner fields to null.

3. But this alignment will cause incompatible changes on public API.
3.1
{code:java}
DataTypes.ROW(DataTypes.FIELD(""f0"", DataTypes.INT().notNull())).notNull() cannot create 
ROW<INT NOT NULL>NOT NULL because the default nullability is true.
{code}
3.2 Row type will lose idempotence for chained call notNull().nullable().notNull()

4. Suggest introducing a Builder to DataTypes.ROW to fix 3.1
h4. Behavior Change

1. Currently, a RowType respects any nullability specification on the attribute and record levels. But after the fix, {*}a row type with attributes not null but record nullable{*}(e.g. ROW<f0 INT NOT NULL>) {*}should not get supported anymore{*}. See the following matrix.
||User-specified Row Type with Nullability||Current Supported||Plan to Support ||
|ROW<f0 INT NOT NULL> NOT NULL|Y|Y|
|ROW<f0 INT> NOT NULL|Y|Y|
|ROW<f0 INT NOT NULL>|Y|{color:#ff0000}N. {color}
{color:#ff0000}To align the behavior with SQL, where{color}
{color:#ff0000}ROW<f0 INT NOT NULL> is implicitly rewritten to {color}
{color:#ff0000}ROW<f0 INT> by Calcite[1].{color}|
|ROW<f0 INT>|Y|Y|

 

2. Currently, AbstractDataType(the interface) has two methods, *nullable()* and *notNull(),* to change the underlying logical type's nullability and get a reconfigured DataType instance. Although it is not explicitly stated in the JavaDoc, the current fact is that the conversion between these two methods can maintain the idempotence of the row type[2], and this feature may be broken after the fix.

Current Behavior 
1. copy(boolean) only changes the record's level of nullability
2. preserve idempotence

After the Fix
1. copy(boolean) changes the record's level nullability, and if nullable == true, the attribute's nullability is also set to true
2. might lose idempotence in some cases

See the following matrix.
||User-specified Row Type with Nullability||  Current Behavior|| After the Fix||
|r ROW<f0 INT NOT NULL> NOT NULL|r.nullable() => ROW<f0 INT NOT NULL>
 
r.notNull() => r
 
r.nullable().notNull() => r|r.nullable() => ROW<f0 INT>
 
r.notNull() => r
 
{color:#ff0000}r.nullable().notNull() => ROW<f0 INT> NOT NULL{color}|
|r ROW<f0 INT> NOT NULL|r.nullable() => ROW<f0 INT>
 
r.notNull() => r
 
r.nullable().notNull()=> r|r.nullable() => ROW<f0 INT>
 
r.notNull() => r
 
r.nullable().notNull() => r|
|r ROW<f0 INT NOT NULL>|r.nullable() => r
 
r.notNull() => ROW<f0 INT NOT NULL> NOT NULL
 
r.notNull().nullable() =>r|since ROW<f0 INT NOT NULL> is not considered a valid type anymore, ROW<f0 INT NOT NULL> will be first interpreted as ROW<f0 INT> 
 
r.nullable() => ROW<f0 INT>
 
r.notNull() => ROW<f0 INT> NOT NULL
 
r.notNull().nullable() => ROW<f0 INT>|
|r ROW<f0 INT>|r.nullable() => r
 
r.notNull() => ROW<f0 INT> NOT NULL
 
r.notNull().nullable() => r|r.nullable() => r
 
r.notNull() => ROW<f0 INT> NOT NULL
 
r.notNull().nullable() => r|

Since the default nullability for LogicalType is true, it's often the case to use DataTypes.ROW(...).notNull() to create `ROW<...> NOT NULL`. While we correct the behavior for DataTypes.ROW(...), it will not be possible to create a `ROW<...> NOT NULL` using DataTypes.ROW(...).notNull().
h4. Possible Plan

I think the core contradiction lies in the fact that, on the one hand, the {{notNull()}} and {{nullable()}} of {{FieldsDataType}} need to be consistent with the implementation of other classes that implement the interface AbstractDataType, such as {{{}AtomicDataType{}}}, which can continuously create reconfigured instances through {{notNull()}} and {{{}nullable(){}}}. On the other hand, the {{nullable()}} method of {{FieldsDataType}} needs to follow Calcite's rules and rewrite the nullability of the internal fields together.

One possible way is to introduce a method to DataTypes that can specify nullability when constructing RowType.
{code:java}
// interpreted as ROW<f0 INT NOT NULL> NOT NULL
DataTypes.ROW(false, DataTypes.FIELD(""f0"", DataTypes.INT().notNull()))

// We may only allow calls like 
DataType rowType = DataTypes.ROW(DataTypes.FIELD(""f0"", DataTypes.INT().nullable()))
{code}
Another way is to introduce a builder like
{code:java}
 // interpreted as ROW<f0 INT NOT NULL> NOT NULL
DataTypes.ROW.Builder builder = 
    DataTypes.ROW.newBuidler()
        .field(DataTypes.FIELD(""f0"", DataTypes.INT().notNull())
        .notNull()
DataType rowType = builder.build();{code}
and adapt the current DataTypes.ROW() to use the builder.
----
h4. Ref

[1] CALCITE-2464

[2] Idempotence means DataTypes.ROW(...).notNull().nullable() (can be repeated many times) equals DataTypes.ROW(...)

[3] [FLIP-37|https://cwiki.apache.org/confluence/display/FLINK/FLIP-37%3A+Rework+of+the+Table+API+Type+System]

 

I slightly prefer the builder pattern, but it would be great if you could share some suggestions. Cc [~lincoln.86xy] [~twalthr]  [~snuyanzin] 

 ;;;","13/Jul/23 22:16;Sergey Nuyanzin;Thanks for the very detailed investigation
The main issue could be here is connector's support which are testing against master and 2 previous major releases...
Even if such changes are applied to master connectors using this types can not benefit from it for a long period of time since they still to support old releases.
I wonder if we can keep current method/constructor signature and e.g. tweak {{org.apache.flink.table.api.DataTypes#ROW(org.apache.flink.table.api.DataTypes.Field...)}} in a way that if at least any fields is not null then set initial nullability of the record to {{false}}... Would it work in such a way?

;;;","22/Dec/23 09:29;qingyue;Hi [~Sergey Nuyanzin], sorry for the late reply. 
{quote}{{org.apache.flink.table.api.DataTypes#ROW(org.apache.flink.table.api.DataTypes.Field...)}} in a way that if at least any fields is not null then set initial nullability of the record to {{{}false{}}}... Would it work in such a way?
{quote}
If I understand correctly, you want to implicitly set the record's level of nullability to not null, if at least one of the inner fields is not null.

I suppose you want to adopt a practice similar to Calcite's by quietly altering the nullability, except that you suggest taking an approach opposite to Calcite regarding whose nullability to prioritize. (Calcite determines that if the record level is nullable, it will change the nullability of the fields within to true.)

I understand that it may be helpful for maintaining the connector in the short term. However, in the long run, this inconsistency in priority does not facilitate the unified maintenance of SQL and TableAPI.;;;","24/Dec/23 21:14;Sergey Nuyanzin;[~qingyue] thanks for the response

just thoughts out loud: 
from one side yes it would be great to adopt the behavior similar to Calcite, from another side it seems to be a breaking change

So what do you think if we could propose such a change for 2.0 ?
;;;","25/Dec/23 02:06;qingyue;Hi [~Sergey Nuyanzin], I've already responded above, please see the ""Behavior Change"" &""Possible Plan"" sections. Please correct me if I'm wrong, but I believe 2.0 is a version where breaking changes are allowable, so we can first discuss and agree on a change in the community (even if it may indeed be a breaking change ultimately), and then make the modifications during the actual development of version 2.0.;;;",,,,,,,,,,,,,,,,,,,
 Conversion to relational algebra failed to preserve datatypes' nullabilities,FLINK-31829,13532963,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,qingyue,lincoln.86xy,lincoln.86xy,18/Apr/23 01:36,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"AssertionError when run such a case:
{code}
  @Test
  def testCoalesceOnNestedColumns(): Unit = {
    val tEnv = util.tableEnv
    val tableDescriptor = TableDescriptor
      .forConnector(""datagen"")
      .schema(
        Schema.newBuilder
          .column(""id"", DataTypes.INT.notNull)
          .column(""a"", DataTypes.ROW(DataTypes.FIELD(""np"", DataTypes.INT)).nullable)
          .build)
      .build
    tEnv.createTemporaryTable(""t1"", tableDescriptor)
    tEnv.createTemporaryTable(""t2"", tableDescriptor)
    val res = tEnv.executeSql(
      ""EXPLAIN SELECT a.id, COALESCE(a.a.np, b.a.np) c1, IFNULL(a.a.np, b.a.np) c2 FROM t1 a left JOIN t2 b ON a.id=b.id where a.a is null or a.a.np is null"")
    res.print()
  }
{code}

stack:
{code}
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType(INTEGER B1, INTEGER NOT NULL B2, INTEGER BenchmarkId1, INTEGER NOT NULL BenchmarkIdWithIfNull, INTEGER NOT NULL BenchmarkId2) NOT NULL
converted type:
RecordType(INTEGER NOT NULL B1, INTEGER NOT NULL B2, INTEGER BenchmarkId1, INTEGER NOT NULL BenchmarkIdWithIfNull, INTEGER NOT NULL BenchmarkId2) NOT NULL
rel:
LogicalProject(B1=[$4.BenchmarkId], B2=[$2.BenchmarkId], BenchmarkId1=[IF(IS NOT NULL($4), $4.BenchmarkId, IF(true, $2.BenchmarkId, null:INTEGER))], BenchmarkIdWithIfNull=[IFNULL($4.BenchmarkId, $2.BenchmarkId)], BenchmarkId2=[COALESCE($4.BenchmarkId, $2.BenchmarkId)])
  LogicalFilter(condition=[OR(IS NULL($4), IS NULL($4.BenchmarkId))])
    LogicalJoin(condition=[=($3, $0)], joinType=[left])
      LogicalJoin(condition=[=($1, $0)], joinType=[inner])
        LogicalTableScan(table=[[default_catalog, default_database, dbo_book]])
        LogicalTableScan(table=[[default_catalog, default_database, static_book]])
      LogicalTableScan(table=[[default_catalog, default_database, onebook_book_benchmark]])


	at org.apache.calcite.sql2rel.SqlToRelConverter.checkConvertedType(SqlToRelConverter.java:500)
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:611)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:216)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:192)
	at org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:56)
	at org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)
	at org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:65)
	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:281)
	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:271)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:665)
{code}

but the equivalent tests using sql ddl to create table works fine:
{code}
  @Test
  def testCoalesceOnNestedColumns2(): Unit = {
    val tEnv = util.tableEnv
    tEnv.executeSql(s""""""
                       |create temporary table t1 (
                       |  id int not null,
                       |  a row<np int>
                       |) with (
                       | 'connector' = 'datagen'
                       |)
                       |"""""".stripMargin)
    tEnv.executeSql(s""""""
                       |create temporary table t2 (
                       |  id int not null,
                       |  a row<np int>
                       |) with (
                       | 'connector' = 'datagen'
                       |)
                       |"""""".stripMargin)
    val res = tEnv.executeSql(
      ""EXPLAIN SELECT a.id, COALESCE(a.a.np, b.a.np) c1, IFNULL(a.a.np, b.a.np) c2 FROM t1 a left JOIN t2 b ON a.id=b.id where a.a is null or a.a.np is null"")
    res.print()
  }


== Abstract Syntax Tree ==
LogicalProject(id=[$0], c1=[COALESCE($1.np, $3.np)], c2=[IFNULL($1.np, $3.np)])
+- LogicalFilter(condition=[OR(IS NULL($1), IS NULL($1.np))])
   +- LogicalJoin(condition=[=($0, $2)], joinType=[left])
      :- LogicalTableScan(table=[[default_catalog, default_database, t1]])
      +- LogicalTableScan(table=[[default_catalog, default_database, t2]])

== Optimized Physical Plan ==
Calc(select=[id, COALESCE(a.np, a0.np) AS c1, IFNULL(a.np, a0.np) AS c2])
+- Join(joinType=[LeftOuterJoin], where=[=(id, id0)], select=[id, a, id0, a0], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[id, a], where=[OR(IS NULL(a), IS NULL(a.np))])
   :     +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[id, a])
   +- Exchange(distribution=[hash[id]])
      +- TableSourceScan(table=[[default_catalog, default_database, t2]], fields=[id, a])

== Optimized Execution Plan ==
Calc(select=[id, COALESCE(a.np, a0.np) AS c1, IFNULL(a.np, a0.np) AS c2])
+- Join(joinType=[LeftOuterJoin], where=[(id = id0)], select=[id, a, id0, a0], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[id, a], where=[(a IS NULL OR a.np IS NULL)])
   :     +- TableSourceScan(table=[[default_catalog, default_database, t1]], fields=[id, a])
   +- Exchange(distribution=[hash[id]])
      +- TableSourceScan(table=[[default_catalog, default_database, t2]], fields=[id, a])
{code}


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30282,FLINK-31830,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 02 10:43:10 UTC 2024,,,,,,,,,,"0|z1hcew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 01:41;lincoln.86xy;[~qingyue] would you like to take over this jira and fix it?  since there has been some discussion offline about the problem;;;","18/Apr/23 02:10;lincoln.86xy;The similar case in 1.14.6 encounter another problem which get a wrong plan instead of an error (FLINK-31830), all the three issues have some relations on different nullabilities of nested type fields, so it is recommended to consider all these issues together;;;","18/Apr/23 02:44;qingyue;Sure. I'd like to take this ticket. I reproduced this issue in the local env and found that FlinkTypeFactory#createTypeWithNullability only changes the outer type's nullability for the row type, which is very similar to FLINK-30282.;;;","09/Jun/23 07:58;qingyue;After upgrading the Calcite from 1.29 to 1.30 FLINK-27998

There's no exception thrown anymore. A detailed explanation can be found at FLINK-31830.;;;","02/Jan/24 10:43;twalthr;{quote}There's no exception thrown anymore.
{quote}
[~qingyue] what do you mean with this? Can we close this issue after adding a test case for it. This would allows us to focus solely on FLINK-31830.;;;",,,,,,,,,,,,,,,,,,,,,,
List field in a POJO data stream results in table program compilation failure,FLINK-31828,13532952,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,netvl,netvl,17/Apr/23 22:46,18/Aug/23 22:35,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,,0,auto-deprioritized-major,pull-request-available,,,,"Suppose I have a POJO class like this:

{code:java}
public class Example {
    private String key;
    private List<Map<String, String>> values;

    // getters, setters, equals+hashCode omitted
}
{code}

When a DataStream with this class is converted to a table, and some operations are performed on it, it results in an exception which explicitly says that I should file a ticket:

{noformat}
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
{noformat}

Please find the example Java code and the full stack trace attached.

From the exception and generated code it seems that Flink is upset with the list field being treated as an array - but I cannot have an array type there in the real code.

Also note that if I _don't_ specify the schema explicitly, it then maps the {{values}} field to a `RAW('java.util.List', '...')` type, which also does not work correctly and fails the job in case of even simplest operations like printing.","Java 11
Flink 1.16.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 22:33;netvl;MainPojo.java;https://issues.apache.org/jira/secure/attachment/13057343/MainPojo.java","17/Apr/23 22:34;netvl;generated-code.txt;https://issues.apache.org/jira/secure/attachment/13057342/generated-code.txt","17/Apr/23 22:39;netvl;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/13057341/stacktrace.txt",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:08 UTC 2023,,,,,,,,,,"0|z1hccg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/23 17:47;aitozi;Hi [~netvl] Thanks for this detailed bug report.

I have reproduced your problem. And I also spend some time to dig the way to use RAW type to declare the List type in your case. I found that there's a bug in the cast rule (using the wrong serializer), so it will fail with EOF exception as you mentioned (hope it's the same error with you).

I will prepare a PR to solve this bug ;;;","21/Apr/23 19:56;netvl;Hi [~aitozi], thank you for the quick response! Yes, I had the EOF exception when trying to print the table with RAW columns. Great to know that it will be fixed :);;;","23/Apr/23 03:45;aitozi;I have pushed a fix for it. Before this fix the demo will fails with EOFException when print the raw type (as shown in the pull request)

After this, in this example, user can define the schema as below. Then 
 
{code:java}
final var schema =
        Schema.newBuilder()
                .column(""key"", DataTypes.STRING())
                .column(""values"", DataTypes.RAW(java.util.List.class))
                .build(); {code};;;","23/Apr/23 08:13;aitozi;[~twalthr] could help review this fix :) ? Since I see you help review the last commit for this part;;;","23/Apr/23 11:51;aitozi;Hi [~netvl] you could also work with this in your case.
{code:java}
final var schema =
        Schema.newBuilder()
                .column(""key"", DataTypes.STRING())
                .column(
                        ""values"",
                        DataTypes.ARRAY(
                                        DataTypes.MAP(
                                                DataTypes.STRING(), DataTypes.STRING()))
                                .bridgedTo(List.class))
                .build(); {code};;;","28/Apr/23 19:18;netvl;Cool, thank you very much!;;;","23/May/23 09:12;aitozi;Any sql guys can help verify this fix ?;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,
Incorrect estimation of the target data rate of a vertex when only a subset of its upstream vertex's output is consumed,FLINK-31827,13532910,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,Zhanghao Chen,Zhanghao Chen,17/Apr/23 15:43,14/Aug/23 11:49,04/Jun/24 20:41,14/Aug/23 11:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,,,,,,0,pull-request-available,stale-assigned,,,,"Currently, the target data rate of a vertex = SUM(target data rate * input/output ratio) for all of its upstream vertices. This assumes that all output records of an upstream vertex is consumed by the downstream vertex. However, it does not always hold. Consider the following job plan generated by a Flink SQL job. The middle vertex contains multiple chained Calc(select xx) operators, each connecting to a separate downstream sink tasks. As a result, each sink task only consumes a sub-portion of the middle vertex's output.

To fix it, we need operator level edge info to infer the upstream-downstream relationship as well as operator level output metrics. The metrics part is easy but AFAIK, there's no way to get the operator level edge info from the Flink REST API yet.

!image-2023-04-17-23-37-35-280.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31826,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 15:37;Zhanghao Chen;image-2023-04-17-23-37-35-280.png;https://issues.apache.org/jira/secure/attachment/13057331/image-2023-04-17-23-37-35-280.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 14 11:44:46 UTC 2023,,,,,,,,,,"0|z1hc34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 16:03;gyfora;The same issue can also be easily reproduced with side outputs. The main problem is that Flink does not provide target jobvertex level in/out record metrics only “aggregated” ones. 

I have a prototype fix that would add the missing metrics on the Flink side and then the autoscaler can be improved to use that information from Flink 1.18 and later;;;","17/Apr/23 16:32;Zhanghao Chen;Thanks [~gyfora]. By ""target jobvertex level in/out record metrics"", do you mean a jobvertex level in/out record metric with its name containing both the current job vertex name and the downstream job vertex name?;;;","17/Apr/23 16:41;gyfora;Exactly, like:

target.\{jobvertexId}.numRecordsOut

we don’t need the current jobvertex in the name as the metric already belongs to that ;;;","20/Apr/23 10:43;mxm;Great find [~Zhanghao Chen]! This need to be fixed ASAP.;;;","20/Apr/23 10:45;mxm;We hadn't considered that a vertex with chained operators could produce different number of records for each output.;;;","21/Apr/23 16:23;gyfora;The first fix on the operator side is merged for this: ff4bbd2a7bbed4ba0b1443d53731c883a230b6d4
This covers most cases without additional Flink metrics. Will folow up with the optional flink metrics.;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Aug/23 11:44;mxm;Shall we close this issue since the originally described issue has been resolved? We can create a new issue for the remaining situations where we can't do correct accounting without additional Flink metrics.;;;",,,,,,,,,,,,,,,,,,,
Incorrect estimation of the target data rate of a vertex when only a subset of its upstream vertex's output is consumed,FLINK-31826,13532908,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Zhanghao Chen,Zhanghao Chen,17/Apr/23 15:37,17/Apr/23 16:01,04/Jun/24 20:41,17/Apr/23 15:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,,,,,,0,,,,,,"Currently, a vertex's target data rate = the sum of its upstream vertex's target data rate * input/output ratio. This assumes that all of the upstream vertex output goes into the current vertex. However, it does not always hold. Consider the following job plan generated by a Flink SQL job. The vertex in the middle has multiple Calc(select xx) operators chained, each connects to a separate downstream tasks. The total num_rec_out_rate of the middle vertex = SUM num_rec_in_rate of its downstream tasks.

To fix this problem, we need operator level output metrics and edge info. The operator level metrics part is easy, but AFAIK, there's no way to get the operator level edge info from the current Flink REST APIs.

!LHL7VKOG4B.jpg!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31827,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 15:24;Zhanghao Chen;LHL7VKOG4B.jpg;https://issues.apache.org/jira/secure/attachment/13057330/LHL7VKOG4B.jpg",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 16:01:43 UTC 2023,,,,,,,,,,"0|z1hc2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 15:45;Zhanghao Chen;I'm sorry that I created duplicated issues due to a network issue. I'll close this issue. Please turn to [FLINK-31827] Incorrect estimation of the target data rate of a vertex when only a subset of its upstream vertex's output is consumed - ASF JIRA (apache.org);;;","17/Apr/23 16:01;gyfora;The same issue can also be easily reproduced with side outputs. The main problem is that Flink does not provide target jobvertex level in/out record metrics only “aggregated” ones. 



I have a prototype fix that would add the missing metrics on the Flink side and then the autoscaler can be improved to use that information from Flink 1.18 and later;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Stopping minikube fails with timeout,FLINK-31825,13532890,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,17/Apr/23 13:30,30/Sep/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Hadoop Compatibility,Tests,,,,,0,auto-deprioritized-major,test-stability,,,,"Currently there is not so much information in logs...
{noformat}
Apr 13 01:27:02 * Stopping node ""minikube""  ...
==========================================================================================
=== WARNING: This task took already 95% of the available time budget of 286 minutes ===
==========================================================================================
==============================================================================
The following Java processes are running (JPS)
==============================================================================
243413 Jps
==============================================================================
Printing stack trace of Java process 243413
==============================================================================
243413: No such process
==============================================================================
The following Java processes are running (JPS)
==============================================================================
243516 Jps
==============================================================================
Printing stack trace of Java process 243516
==============================================================================
243516: No such process
=============================
=== WARNING: Killing task ===
=============================
Terminated
Apr 13 05:48:53 [FAIL] Test script contains errors.
{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48096&view=logs&j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6&t=ba24ad14-6ea3-5ee3-c4ec-9e7cd2c9e754&l=5290",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Sep 30 22:35:08 UTC 2023,,,,,,,,,,"0|z1hbyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/23 13:28;qingyue;I encountered a similar issue too.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51652&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117]
{code:java}
2023-07-24T10:21:24.6278824Z Jul 24 10:21:24 job.batch ""flink-job-cluster"" deleted
2023-07-24T10:21:24.7205128Z Jul 24 10:21:24 service ""flink-job-cluster"" deleted
2023-07-24T10:21:24.7983021Z Jul 24 10:21:24 deployment.apps ""flink-task-manager"" deleted
2023-07-24T10:21:25.6672220Z Jul 24 10:21:25 pod/flink-task-manager-5cdf8bd98f-ldcp7 condition met
2023-07-24T10:21:25.6714103Z Jul 24 10:21:25 Stopping minikube ...
2023-07-24T10:21:25.7608214Z Jul 24 10:21:25 * Stopping node ""minikube""  ...
2023-07-24T10:21:36.1800492Z Jul 24 10:21:36 * 1 node stopped.
2023-07-24T10:21:36.1834421Z Jul 24 10:21:36 [FAIL] Test script contains errors.
2023-07-24T10:21:36.1845259Z Jul 24 10:21:36 Checking for errors...
2023-07-24T10:21:36.2116654Z Jul 24 10:21:36 No errors in log files.
2023-07-24T10:21:36.2117326Z Jul 24 10:21:36 Checking for exceptions...
2023-07-24T10:21:36.2402194Z Jul 24 10:21:36 No exceptions in log files.
2023-07-24T10:21:36.2403093Z Jul 24 10:21:36 Checking for non-empty .out files...
2023-07-24T10:21:36.2418604Z grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
2023-07-24T10:21:36.2434169Z Jul 24 10:21:36 No non-empty .out files.
2023-07-24T10:21:36.2434637Z Jul 24 10:21:36 
2023-07-24T10:21:36.2435264Z Jul 24 10:21:36 [FAIL] 'Run Kubernetes test' failed after 4 minutes and 14 seconds! Test exited with exit code 1 {code};;;","24/Jul/23 13:52;Sergey Nuyanzin;[~qingyue] in fact your issue is different
if you look at {{View raw logs}} button you'll see that your issue is same as FLINK-32632
so probably you need to rebase;;;","24/Jul/23 14:40;qingyue;Hi [~Sergey Nuyanzin], thanks for the kind reminder, will take a look right away.;;;","22/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","30/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,
flink sql TO_TIMESTAMP error,FLINK-31824,13532877,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,leishuiyu,leishuiyu,17/Apr/23 12:07,18/Apr/23 10:50,04/Jun/24 20:41,18/Apr/23 10:50,1.14.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,," 

 

 

 

 

 

!image-2023-04-17-20-07-17-569.png!",the verion is 1.14.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 12:07;leishuiyu;image-2023-04-17-20-07-17-569.png;https://issues.apache.org/jira/secure/attachment/13057328/image-2023-04-17-20-07-17-569.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 18 10:50:11 UTC 2023,,,,,,,,,,"0|z1hbvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 12:13;leishuiyu;[~jark] 

Can you help me check if this is a bug;;;","17/Apr/23 13:58;hackergin;[~leishuiyu]  You can try this : 
{code:java}
//代码占位符
Flink SQL> select TO_TIMESTAMP('2022-04-18T12:34:56.000z', 'yyyy-MM-dd''T''HH:mm:ss.SSS''z'''); {code};;;","18/Apr/23 10:09;taoran;[~leishuiyu] 

1.The date pattern with T need double-quoted such as ""yyyy-MM-dd'T'HH:mm:ss.SSS"", while Z is no need to do this.

why u try to match a 'Z' placeholder but without date zone content?

 

I think the common usage could be like:
{code:java}
Flink SQL> select TO_TIMESTAMP('2022-04-18T12:34:56.000+0800', 'yyyy-MM-dd''T''HH:mm:ss.SSSZ'); {code}
 ;;;","18/Apr/23 10:50;leishuiyu;thank you [~hackergin] [~lemonjing] ，Your answers are all functioning properly;;;",,,,,,,,,,,,,,,,,,,,,,,
RestHandlerConfigurationTest.testWebRescaleFeatureFlagWithReactiveMode is unstable,FLINK-31823,13532865,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,Sergey Nuyanzin,Sergey Nuyanzin,17/Apr/23 11:22,18/Apr/23 13:30,04/Jun/24 20:41,18/Apr/23 13:30,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48177&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8509]

{noformat}
Apr 16 01:15:08 [ERROR] Failures: 
Apr 16 01:15:08 [ERROR]   RestHandlerConfigurationTest.testWebRescaleFeatureFlagWithReactiveMode:84 
Apr 16 01:15:08 expected: false
Apr 16 01:15:08  but was: true

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31471,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 18 13:30:34 UTC 2023,,,,,,,,,,"0|z1hbt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 11:24;Sergey Nuyanzin;// cc [~dmvk] since test were added within  FLINK-31471 ;;;","17/Apr/23 11:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48164&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8434;;;","18/Apr/23 05:30;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48212&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8509;;;","18/Apr/23 08:34;dmvk;This did not appear on PR because we only smoke test AdaptiveScheduler against the regular test suite on scheduled builds against the master branch.;;;","18/Apr/23 13:30;dmvk;master: 1d9f12a9b40c74fac83a84ee4e7e7d4e9fd52e00;;;",,,,,,,,,,,,,,,,,,,,,,
Support configure maxRows when fetch result ,FLINK-31822,13532854,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hackergin,hackergin,17/Apr/23 09:36,21/May/23 16:11,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,,0,,,,,,"The default value of maxRow during fetch result is 5000. When requested from a web page, too many results in a single request may cause the web page to freeze.

 

Therefore, we can support configuring the maximum number of request results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 21 16:11:47 UTC 2023,,,,,,,,,,"0|z1hbqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/23 13:02;hackergin;[~fsk119] [~jark]   Do you think this is a reasonable feature t? If so, I can work on implementing it. ;;;","21/May/23 16:11;jark;Sounds good to me. What do you think [~fsk119];;;",,,,,,,,,,,,,,,,,,,,,,,,,
FlinkSQL set parallelism for each operator.,FLINK-31821,13532850,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,lenault,lenault,17/Apr/23 09:19,18/Apr/23 13:12,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"Currently, FlinkSQL can  set a unified parallelism in the job,it cannot set parallelism for each operator.
This can cause resource waste  On the occasion of  high parallelism and small data volume.there may also be too many small file  for  writing HDFS Scene.",,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/23 09:31;lenault;image-2023-04-17-17-31-34-709.png;https://issues.apache.org/jira/secure/attachment/13057312/image-2023-04-17-17-31-34-709.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 09:53:55 UTC 2023,,,,,,,,,,"0|z1hbps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 09:23;lenault;I can modify FlinkSQL to support operator parallelism.Is it meaningful to do this？Let's discuss.;;;","17/Apr/23 09:24;martijnvisser;[~lenault] It's better to first discuss this on the Dev mailing list and not in Jira;;;","17/Apr/23 09:27;lenault;OK  , I 'ii send to mailing list, let's discuss it.;;;","17/Apr/23 09:31;lenault;I have already posted it, but no one has discussed this point. what should I do? !image-2023-04-17-17-31-34-709.png!;;;","17/Apr/23 09:39;martijnvisser;[~lenault] Given that there are no replies yet on https://lists.apache.org/thread/y6y8f39dxg644c5hvl2l86q8jv2z0rmf, it appears that a) users need more time or b) that there isn't so much interest in the feature. Adding a feature like this touches on a lot of public interfaces, so it needs a broad discussion and consensus. ;;;","17/Apr/23 09:53;lenault;[Martijn Visser|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=martijnvisser] To reduce interface dependencies,I can solve this problem in a configurable way;;;",,,,,,,,,,,,,,,,,,,,,
Support data source sub-database and sub-table,FLINK-31820,13532848,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,complone,complone,17/Apr/23 09:04,22/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,auto-deprioritized-major,pull-request-available,,,,"At present, apache/flink-connector-jdbc does not support sub-database and table sub-database. Now three commonly used databases Mysql, Postgres and Oracle support sub-database and sub-table

 

Taking oracle as an example, users only need to configure the following format to use

 
{code:java}
create table oracle_source (
    EMPLOYEE_ID BIGINT,
    START_DATE TIMESTAMP,
    END_DATE TIMESTAMP,
    JOB_ID VARCHAR,
    DEPARTMENT_ID VARCHAR
) with (
    type = 'oracle',    
    url = 'jdbc:oracle:thin:@//localhost:3306/order_([0-9]{1,}),jdbc:oracle:thin:@//localhost:3306/order_([0-9]{1,})',
   userName = 'userName',
    password = 'password',
    dbName = 'hr', 
    table-name = 'order_([0-9]{1,})',
    timeField = 'START_DATE',
    startTime = '2007-1-1 00:00:00'
); {code}
In the above code, the dbName attribute corresponds to the schema-name attribute in oracle or postgres, and the mysql database needs to manually specify the dbName

 

At the same time, I am also developing the CDAS whole database synchronization syntax for the company, and the data source supports sub-database and table as part of it. Add unit tests. For now, please keep this PR in draft status.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 22 22:35:06 UTC 2023,,,,,,,,,,"0|z1hbpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 09:22;martijnvisser;[~complone] I've done a quick Google search for ""mysql subtable"" and ""postgres sybtable"" but neither result in documentation from these providers. You mention that these are common, but given that they aren't documented on their pages, I'm not sure they are. Before creating a PR, I think it's important that we reach consensus that this is a) something that we want and b) how we want to introduce this in Flink. ;;;","18/Apr/23 04:39;Thesharing;According to this [reference|https://www.elephdev.com/database/344.html?ref=addtabs&lang=en], the {{sub-table}} referred by Xingyuan may denotes that the data are split and stored into multiple databases. And these sub-databases forms one database. This is common before the appearance of big data technique. However, at present, is MySQL distributed cluster a better solution?;;;","19/Apr/23 14:29;martijnvisser;I don't immediately see the use cases that adding this feature would unlock, which I would like to understand before saying that it's a good idea that we should add this.;;;","05/May/23 05:58;complone;[~martijnvisser] Hello, sorry for the late reply after the May Day holiday. I did some simple research. At present, in the case of many single databases, in order to reduce performance loss, some domestic companies adopt the scheme of sub-database and sub-table, and foreign companies adopt schemes such as distributed database. Regarding the difference between domestic and foreign use, the reason for using sub-database and sub-table is because the storage medium of the stock data has been determined, and the cost of migrating to a distributed database is too high, and the enterprise business cannot tolerate it. Therefore, it is necessary to transform the connector to support sub-database and sub-table.

 

The reference given by [~Thesharing]  is a powerful explanation of mysql sub-database and sub-table. I will update the documentation in the near future, and explain this logic in the form of a flow chart.;;;","25/May/23 13:00;martijnvisser;I would argue that each table should be mapped to its own Flink SQL table. If you want to create an overachieving view over multiple tables, you should create a view. Mapping multiple tables into one Flink SQL table isn't the right approach imho. ;;;","14/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","22/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,
Add document for using watermark advanced functions in sql,FLINK-31819,13532841,13529416,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,catyee,catyee,catyee,17/Apr/23 08:01,17/Jul/23 09:54,04/Jun/24 20:41,17/Jul/23 09:53,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Documentation,,,,,,0,,,,,,"Add document for using watermark advanced functions in sql

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 17 09:53:43 UTC 2023,,,,,,,,,,"0|z1hbns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 09:53;yunta;merged in master: 5cacd1216b018f47939a5a6fa671e022a0836256;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
parsing error of 'security.kerberos.access.hadoopFileSystems' in flink-conf.yaml,FLINK-31818,13532827,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,seung-min,seung-min,seung-min,17/Apr/23 05:57,17/Apr/23 15:17,04/Jun/24 20:41,17/Apr/23 15:17,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Runtime / Configuration,,,,,,0,bug,pull-request-available,,,,"There is a parsing error when I gave two or more hdfs namenodes URI separated by commas as the value of key attribute 'security.kerberos.access.hadoopFileSystems'.

 

For example, I set this key attribute and value like below in flink-conf.yaml,
{code:java}
security.kerberos.access.hadoopFileSystems: hdfs://hadoop-nn1.testurl.com:8020,hdfs://hadoop-nn2.testurl.com:8020 {code}
 

then, the slash ""/"" is missing in second URI in parsed value
{code:java}
hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020{code}
 

 

Received error message is here.
{code:java}
Caused by: org.apache.flink.util.FlinkRuntimeException: java.io.IOException: Incomplete HDFS URI, no host: hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$getFileSystemsToAccess$2(HadoopFSDelegationTokenProvider.java:168) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_362]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:157) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$obtainDelegationTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_362]
   at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_362]
   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1966) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.java:108) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$obtainDelegationTokensAndGetNextRenewal$1(DefaultDelegationTokenManager.java:228) ~[flink-dist-1.17.0.jar:1.17.0]
   ... 13 more
Caused by: java.io.IOException: Incomplete HDFS URI, no host: hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020
   at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:156) ~[hadoop-hdfs-client-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3241) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:122) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3290) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3258) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:471) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$getFileSystemsToAccess$2(HadoopFSDelegationTokenProvider.java:163) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_362]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:157) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$obtainDelegationTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_362]
   at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_362]
   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1966) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.java:108) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$obtainDelegationTokensAndGetNextRenewal$1(DefaultDelegationTokenManager.java:228) ~[flink-dist-1.17.0.jar:1.17.0]
   ... 13 more {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 15:09:01 UTC 2023,,,,,,,,,,"0|z1hbkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 06:16;JunRuiLi;[~seung-min] I think this issue is due to the type of security.kerberos.access.hadoopFileSystems being ConfigOption<List<String>>, which requires "";"" as the separator instead of "","" for the value of the configuration option. You can retry with ""hdfs://hadoop-nn1.testurl.com:8020;hdfs://hadoop-nn2.testurl.com:8020"" as the value. If this modification is correct, would you be willing to create a pr to correct the documentation? If not, I am happy to do it.;;;","17/Apr/23 06:54;seung-min;[~JunRuiLi] 

Oh, I will test, and if this modification is correct than i will create pr to correct the documentation  :D;;;","17/Apr/23 07:17;seung-min;[~JunRuiLi]

It works

I will create pr!;;;","17/Apr/23 08:11;gaborgsomogyi;Yeah, the doc is wrong. Thanks for fixing it(y);;;","17/Apr/23 15:09;Weijie Guo;master(1.18) via fe8ff46123ee2f9278066c801228416e38e2cbd4.
release-1.17 via fa2263d1de1789a5200f909333cfe2dc30104b2f.;;;",,,,,,,,,,,,,,,,,,,,,,
Skip meaningless testForUnsplittable in FileInputFormat,FLINK-31817,13532817,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zouxxyy,zouxxyy,17/Apr/23 03:17,25/Jun/23 08:26,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"In `FileInputFormat`

The function of `testForUnsplittable` is to detect whether the pathFile is Unsplittable, and when pathFile is Unsplittable, assign true to the member variable `unsplittable`

Currently this function will always be executed, in fact, we can only execute it when `Unsplittable` is false. This can reduce the calls to `testForUnsplittable`
{code:java}
protected boolean testForUnsplittable(FileStatus pathFile) {
    if (getInflaterInputStreamFactory(pathFile.getPath()) != null) {
        unsplittable = true;
        return true;
    }
    return false;
}{code}
 

I am new to flink and have written a patch to solve this, can someone assign this ticket to me?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-17 03:17:41.0,,,,,,,,,,"0|z1hbio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor EmbeddedLeaderElectionService,FLINK-31816,13532792,13542119,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,16/Apr/23 14:51,10/Jul/23 07:24,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,{{DefaultLeaderElectionService}} can be used as a generic implementation. The embedded functionality can be provided through a {{LeaderElectionDriver}} implementation.,,,,,,,,,,FLINK-26522,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26630,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 07 14:10:35 UTC 2023,,,,,,,,,,"0|z1hbdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/23 16:10;mapohl;This is actually a legacy issue. I moved it out of FLINK-26522. We could tackle it after we're done with FLINK-26522.;;;","07/Jul/23 12:14;mapohl;{quote}
DefaultLeaderElectionService can be used as a generic implementation. The embedded functionality can be provided through a LeaderElectionDriver implementation.
{quote}

The initial intention of this Jira issue was to simplify the embedded leader election functionality by utilizing the existing {{DefaultLeaderElectionService}} and coming up with a specific {{LeaderElectionDriver}} implementation. This doesn't work, though: The {{DefaultLeaderElectionService}} is implemented in a way that it only allows one contender per JVM process. The embedded leader election, in contrast, works with multiple contenders living in the same JVM process.

What could be done, instead, is to align the {{HighAvailabilityServices}}:
* {{EmbeddedLeaderService}} could implement {{LeaderElectionService}}
* We could introduce {{StandaloneLeaderElectionService}} again that implements {{LeaderElectionService}} as well.
* {{HighAvailabilityServices}} could get a single implementation that requires a factory method for creating {{JobGraphStore}}, {{JobResultStore}}, {{CheckpointRecoveryFactory}}, and {{BlobStore}}. Additionally, it would require a {{LeaderElectionService}} (which is essentially a factory for {{LeaderElection}} instances).

This would fix FLINK-26630 because we could now easily configure an embedded setup with in-memory {{JobGraphStore}}. It would also make the {{EmbeddedHaServicesWithLeadershipControl}} obsolete because we could give access to the internal {{EmbeddedLeaderService}} instances;;;","07/Jul/23 14:10;mapohl;I'm wondering whether a structure like this would make sense:
 * {{{}HighAvailabilityServices{}}}: Collects the job state-related artifacts and the {{LeaderElectionServices}}
 ** Methods
 *** getJobGraphStore()
 *** getJobResultStore()
 *** getBlobStore()
 *** getCheckpointRecoveryFactory()
 *** getLeaderElectionServices() // returns LeaderElectionServices

 * {{{}LeaderElectionServices{}}}: Interface for all the different components that rely on leader election. It abstracts away the {{componentId}}
 ** Methods:
 *** {{getDispatcherLeaderElection()}}
 *** {{getDispatcherLeaderRetrieval()}}
 *** {{getResourceManagerLeaderElection()}}
 *** {{getResourceManagerLeaderRetrieval()}}
 *** {{getJobMasterLeaderElection(jobId)}}
 *** {{getJobMasterLeaderRetrieval(jobId)}}
 *** {{getRestEndpointLeaderElection()}}
 *** {{getRestEndpointLeaderRetrieval()}}
 ** Implementations
 *** {{DefaultLeaderElectionServices}}
 *** {{EmbeddedLeaderElectionServices}}
 *** {{StandaloneLeaderElectionServices}}

 * {{LeaderElectionService}} (singular): Provides actual leader election for a set of {{LeaderElection}} instances. Implementations:
 ** methods
 *** {{createLeaderElection(componentId)}}
 *** {{close()}}
 ** Implementations
 *** {{{}DefaultLeaderElectionService{}}}: Single driver wrapper (can be also used for one driver per contender)
 *** {{{}EmbeddedLeaderElectionService{}}}: Embedded implementation for a list of {{LeaderElection}} instances competing with each other (i.e. current {{{}EmbeddedLeaderService{}}});;;",,,,,,,,,,,,,,,,,,,,,,,,
Fixing the container vulnerability by upgrade the SnakeYaml Maven dependency,FLINK-31815,13532784,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,highfei2011@126.com,highfei2011@126.com,16/Apr/23 12:04,02/Nov/23 01:19,04/Jun/24 20:41,04/May/23 08:26,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"_Fix the container vulnerability, here is [cve|https://github.com/advisories/GHSA-mjmj-j48q-9wg2]._",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33408,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 14:47:40 UTC 2023,,,,,,,,,,"0|z1hbbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 08:26;gyfora;From the operator side this is now fixed in f23e38d6505e68535906cd6a7d77ca94fa1758e0.

We still pull the old snakeyaml for flink standalone due to the old flink fabric8 usage.
That has to be fixed as part of: https://issues.apache.org/jira/browse/FLINK-31997;;;","31/Oct/23 14:47;danielchow;Hi, [~gyfora], the issue [FLINK-31997|https://issues.apache.org/jira/browse/FLINK-31997] didn't fix the problem you mentioned, so I upgrade snakeyaml to version 2.0 in the issue [FLINK-33408|https://issues.apache.org/jira/browse/FLINK-33408].;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Enable Precondition in DefaultLeaderElectionService.close after the MultipleComponentLeaderElectionDriverAdapter is removed,FLINK-31814,13532778,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,16/Apr/23 07:54,07/Jul/23 07:54,04/Jun/24 20:41,07/Jul/23 07:54,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,"Currently, we have a cyclic dependency in the leader election code where the {{DefaultLeaderElection.close()}} (pre FLIP-285 code: {{{}DefaultLeaderElectionService.stop(){}}}) call will trigger the deregistration of the {{LeaderElectionEventHandler}} (which is the very same {{DefaultLeaderElectionService}} that is in the midst of being closed). The deregistration is forwarded to the {{DefaultMultipleComponentLeaderElectionService}} that will also trigger an {{onRevokeLeadership()}} call on the {{{}LeaderElectionEventHandler{}}}.

There is an intended Precondition in {{DefaultLeaderElectionService.onRevokeLeadership()}} introduced in FLINK-31773 that should be valid if the service has uni-directed ownership of the driver (i.e. the driver doesn't call the service again). This Precondition is commented out and marked with this Jira issue FLINK-31814 right now due to the cyclic dependency described above.

We can add the Precondition after {{MultipleComponentLeaderElectionDriverAdapter}} is removed (and, therefore, the cyclic dependency resolved).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 07 07:54:53 UTC 2023,,,,,,,,,,"0|z1hbag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 07:54;mapohl;master: c5fc72361f198692421dfb50e00e251b84e8d4cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Initial implementation of Kinesis Source using FLIP-27,FLINK-31813,13532667,13404461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,liangtl,liangtl,liangtl,14/Apr/23 13:30,02/Feb/24 10:12,04/Jun/24 20:41,12/Jun/23 17:59,,,,,,,,,,,,,,,,,,,,,aws-connector-4.2.0,,,,,,,,,,,,,,,1,,,,,,"Implement a base implementation of the Kinesis source based on FLIP-27

 ",,,,,,,,,,,,,,,,,,,,,FLINK-31987,FLINK-31988,FLINK-31989,FLINK-31986,FLINK-31990,FLINK-32097,FLINK-32218,FLINK-32229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 12 17:59:15 UTC 2023,,,,,,,,,,"0|z1hals:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 17:59;dannycranmer;Merged commit [{{43e1295}}|https://github.com/apache/flink-connector-aws/commit/43e1295e402ed59e6db3689f6bd16b69e6f4e12e] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SavePoint from /jars/:jarid:/run api on body is not anymore set to null if empty,FLINK-31812,13532664,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,14/Apr/23 13:26,07/Jun/23 03:02,04/Jun/24 20:41,07/Jun/23 03:02,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,,,,,,,,,,0,pull-request-available,,,,,"Since https://issues.apache.org/jira/browse/FLINK-29543 the 
savepointPath from the body is not anymore transform to null if empty: [https://github.com/apache/flink/pull/21012/files#diff-c6d9a43d970eb07642a87e4bf9ec6a9dc7d363b1b5b557ed76f73d8de1cc5a54R145]
 
This leads to issue running a flink job in release 1.17 with lyft operator which set savePoint in body to empty string: [https://github.com/lyft/flinkk8soperator/blob/master/pkg/controller/flinkapplication/flink_state_machine.go#L721]
 
Issue faced by the job as the savepointPath is setto empty string:
{code:java}
org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
3	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
4	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
5	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
6	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
7	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
8	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
9	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
10	at java.base/java.lang.Thread.run(Thread.java:829)
11Caused by: java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: empty checkpoint pointer
12	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
13	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
14	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702)
15	... 3 more
16Caused by: java.lang.IllegalArgumentException: empty checkpoint pointer
17	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
18	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:240)
19	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpoint(AbstractFsCheckpointStorageAccess.java:136)
20	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1824)
21	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:223)
22	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:198)
23	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:365)
24	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:210)
25	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:136)
26	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)
27	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
28	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:371)
29	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:348)
30	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
31	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
32	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
33	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
34	... 3 more
35 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 06 13:58:50 UTC 2023,,,,,,,,,,"0|z1hal4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 09:57;Weijie Guo;IIUC, It seems that this [change|https://github.com/apache/flink/pull/21012/files#diff-c6d9a43d970eb07642a87e4bf9ec6a9dc7d363b1b5b557ed76f73d8de1cc5a54R145] in FLINK-29543 is not unnecessary. 
cc [~ConradJam] for more context.;;;","21/Apr/23 08:24;ConradJam; 
 emptyToNull is missing here, and honestly I think it would be nice to have a check here, if the Savepoint path is empty until the end, should we throw an exception? Now there doesn't seem to be a mechanism for that, so it just keeps going down

cc [~nfraison.datadog] ;;;","21/Apr/23 08:36;Weijie Guo;Thanks [~ConradJam] for the confirm! [~nfraison.datadog] you are assigned.;;;","06/Jun/23 13:58;Weijie Guo;master(1.18) via bc2fee44a87591d08ab0df6cba06bbf9ac065095.
release-1.17 via c003709916540aa272e15a4c8df4374f594ebad7.;;;",,,,,,,,,,,,,,,,,,,,,,,
Unsupported complex data type for Flink SQL,FLINK-31811,13532660,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jirawech.s,jirawech.s,14/Apr/23 12:54,17/Apr/23 07:44,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,,0,,,,,,"I found this issue when I tried to write data on local filesystem using Flink SQL
{code:java}
19:51:32,966 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - compact-operator (1/4) (4f2a09b638c786f74262c675d248afd9_80fe6c4f32f605d447b391cdb16cc1ff_0_4) switched from RUNNING to FAILED on 69ed2306-371b-4bfc-a98e-bf75fb41748f @ localhost (dataPort=-1).
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
    at java.util.ArrayList.rangeCheck(ArrayList.java:659) ~[?:1.8.0_301]
    at java.util.ArrayList.get(ArrayList.java:435) ~[?:1.8.0_301]
    at org.apache.parquet.schema.GroupType.getType(GroupType.java:216) ~[parquet-column-1.12.2.jar:1.12.2]
    at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector(ParquetSplitReaderUtil.java:523) ~[flink-parquet-1.16.1.jar:1.16.1]
    at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector(ParquetSplitReaderUtil.java:503) ~[flink-parquet-1.16.1.jar:1.16.1]
    at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createWritableVectors(ParquetVectorizedInputFormat.java:281) ~[flink-parquet-1.16.1.jar:1.16.1]
    at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createReaderBatch(ParquetVectorizedInputFormat.java:270) ~[flink-parquet-1.16.1.jar:1.16.1]
    at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createPoolOfBatches(ParquetVectorizedInputFormat.java:260) ~[flink-parquet-1.16.1.jar:1.16.1]
     {code}
What i tried to do is writing complex data type to parquet file
Here is the schema of sink table. The problematic data type is ARRAY<MAP<STRING,STRING>>
{code:java}
CREATE TEMPORARY TABLE local_table (
 `user_id` STRING, `order_id` STRING, `amount` INT, `restaurant_id` STRING, `experiment` ARRAY<MAP<STRING,STRING>>, `dt` STRING
) PARTITIONED BY (`dt`) WITH (
  'connector'='filesystem',
  'path'='file:///tmp/test_hadoop_write',
  'format'='parquet',
  'auto-compaction'='true',
  'sink.partition-commit.policy.kind'='success-file'
) {code}
PS. It is used to work in Flink version 1.15.1

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31197,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 07:44:16 UTC 2023,,,,,,,,,,"0|z1hak8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 13:31;KristoffSC;I think this might be a duplicate of https://issues.apache.org/jira/browse/FLINK-31197 that manifest in SQL API.

P.S.
Are you sure that this was working in 1.15.1?
I know that writing ""simple"" complex types like Array of Intigers or Strings bBut not sure about this one -> Arrays Of Map.;;;","14/Apr/23 14:01;jirawech.s;[~KristoffSC] I guess what you mentioned is parquet reader. I was able to write complex type to FileSystem fine on Flink 1.15.1.
P.S. Writing Array of String works fine tho in 1.16.0;;;","14/Apr/23 14:17;KristoffSC;[~jirawech.s] I've pasted a wrong ticket number, already edited my previous comment sory.

I was talking about this one https://issues.apache.org/jira/browse/FLINK-31197 which is about parquet writer.;;;","15/Apr/23 02:46;jirawech.s;[~KristoffSC] I see. It may be duplicated issue, yet i see different error here. Could you share me reproducible code? I can assure you that it works with Flink version 1.15.1 in my case.;;;","17/Apr/23 07:44;KristoffSC;Hi [~jirawech.s]
??Could you share me reproducible code???

The code is attached to the issue I've created  https://issues.apache.org/jira/browse/FLINK-31197;;;",,,,,,,,,,,,,,,,,,,,,,
RocksDBException: Bad table magic number on checkpoint rescale,FLINK-31810,13532650,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rmetzger,rmetzger,14/Apr/23 10:56,28/Feb/24 08:25,04/Jun/24 20:41,,1.15.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,,1,,,,,,"While rescaling a job from checkpoint, I ran into this exception:

{code:java}
SinkMaterializer[7] -> rob-result[7]: Writer -> rob-result[7]: Committer (4/4)#3 (c1b348f7eed6e1ce0e41ef75338ae754) switched from INITIALIZING to FAILED with failure cause: java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:265)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:703)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:679)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:646)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for SinkUpsertMaterializer_7d9b7588bc2ff89baed50d7a4558caa4_(4/4) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)
	... 11 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:395)
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:483)
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:97)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	... 13 more
Caused by: java.io.IOException: Error while opening RocksDB instance.
	at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.openDB(RocksDBOperationUtils.java:92)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreDBInstanceFromStateHandle(RocksDBIncrementalRestoreOperation.java:465)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithRescaling(RocksDBIncrementalRestoreOperation.java:321)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:164)
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:315)
	... 18 more
Caused by: org.rocksdb.RocksDBException: Bad table magic number: expected 9863518390377041911, found 4096 in /tmp/job_00000000000000000000000000000000_op_SinkUpsertMaterializer_7d9b7588bc2ff89baed50d7a4558caa4__4_4__uuid_d5587dfc-78b3-427c-8cb6-35507b71bc4b/46475654-5515-430e-b215-389d42cddb97/000232.sst
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:306)
	at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.openDB(RocksDBOperationUtils.java:80)
	... 22 more
{code}

I haven't found any other cases of this issue on the internet, except for this, but it is not related to Flink: https://github.com/facebook/rocksdb/issues/3438.
I'm posting this to track if other users are affected by this as well, and to collect information about the circumstances when this issue occurs.
What I have done to run into this situation:
a) killed the JobManager
b) removed the jobgraph information from the cluster config map so that the JobGraph gets regenerated with the changed parallelism
c) restarted the JobManager, which regenerated the job graph with the new parallelism
d) During state recovery, above exception occurred.

In my understanding, rescaling from a checkpoint is legal, because that's what reactive mode is also doing.
Also, removing the jobgraph information from the cluster config map is ok, because the Flink Kubernetes operator is also doing that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 28 08:25:43 UTC 2024,,,,,,,,,,"0|z1hai0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 09:53;yunta;First of all, we can certainly rescale from a checkpoint. Would this problem still exist if not rescaled? I just want to confirm whether the file (000232.sst) is corrupted already.;;;","18/Apr/23 14:51;rmetzger;Thanks for confirming. I will try and see if this file is still around, or if I can reproduce the issue.;;;","12/May/23 17:08;david.artiga;Something quite similar just happened to us (using {{1.15.4}}):
{code}
20230512T182327.440+0200 ERROR Caught unexpected exception. java.io.IOException: Error while opening RocksDB instance.
	at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.openDB(RocksDBOperationUtils.java:92)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBHandle.loadDb(RocksDBHandle.java:134)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBHandle.openDB(RocksDBHandle.java:124)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreFromLocalState(RocksDBIncrementalRestoreOperation.java:240)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreFromRemoteState(RocksDBIncrementalRestoreOperation.java:219)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithoutRescaling(RocksDBIncrementalRestoreOperation.java:186)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:166)
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:325)
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:494)
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:101)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:265)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:703)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:679)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:646)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.rocksdb.RocksDBException: Bad table magic number: expected 9863518390377041911, found 370301222607884801 in /mnt/1/hadoop/yarn/nm-local-dir/usercache/splat/appcache/application_1677581391115_0019/tm_container_e02_1677581391115_0019_01_000006/tmp/job_06041d3a256afa3e088b474eb626985e_op_RBEAStreamOperator_707eed42a9b74f065cc8bb6798b04782__64_180__uuid_dfc34320-a336-42dc-b4f4-d48ccdfe2c09/db/045922.sst
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:306)
	at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.openDB(RocksDBOperationUtils.java:80)
	... 25 common frames omitted
 [org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build() @ 415]
{code};;;","15/May/23 05:49;yunta;[~david.artiga] what distributed-file-system did you use to store the checkpoint?;;;","15/May/23 06:45;david.artiga;RocksDB files are stored in local filesystem on DataProc nodes, then checkpoints/savepoints are written into Google bucket.

We migrated recently from 1.14 (haven't seen this issue previously);;;","15/May/23 10:27;yunta;First of all, the RocksDB version between 1.14 and 1.15 does not change. I think this shall not be a problem introduced by upgrading RocksDB version. [~david.artiga] did you ever enable local recovery when meeting this problem?;;;","15/May/23 13:14;david.artiga;Not sure what you mean by ""enable local recovery"" but in any case we just restore from older checkpoint/savepoint as a workaround.

Edit: Found we have {{state.backend.local-recovery: true}};;;","16/May/23 08:57;yunta;To be honest, I doubt that this problem might be related with broken file uploading instead of RocksDB itself, [~rmetzger], [~david.artiga] could you share the broken SST file?;;;","16/May/23 09:08;david.artiga;I will if it happens again;;;","31/May/23 10:20;david.artiga;Happened again (and the file is already gone):
{code:java}
2023-05-31 12:18:16java.lang.Exception: Exception while creating StreamOperatorStateContext.    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255)    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:265)    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:703)    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:679)    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:646)    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)    at java.base/java.lang.Thread.run(Thread.java:829)Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for RBEAStreamOperator_707eed42a9b74f065cc8bb6798b04782_(68/180) from any of the 1 provided restore options.    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)    ... 11 moreCaused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:416)    at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:494)    at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:101)    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)    ... 13 moreCaused by: java.io.IOException: Error while opening RocksDB instance.    at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.openDB(RocksDBOperationUtils.java:92)    at org.apache.flink.contrib.streaming.state.restore.RocksDBHandle.loadDb(RocksDBHandle.java:134)    at org.apache.flink.contrib.streaming.state.restore.RocksDBHandle.openDB(RocksDBHandle.java:124)    at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreFromLocalState(RocksDBIncrementalRestoreOperation.java:240)    at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreFromRemoteState(RocksDBIncrementalRestoreOperation.java:219)    at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithoutRescaling(RocksDBIncrementalRestoreOperation.java:186)    at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:166)    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:325)    ... 18 moreCaused by: org.rocksdb.RocksDBException: Bad table magic number: expected 9863518390377041911, found 7075548869028885811 in /mnt/1/hadoop/yarn/nm-local-dir/usercache/splat/appcache/application_1684317883065_0005/tm_container_e02_1684317883065_0005_01_000005/tmp/job_850cab2d0c2ae40fdcb378960377f5e3_op_RBEAStreamOperator_707eed42a9b74f065cc8bb6798b04782__68_180__uuid_cca94cae-928e-4a10-b52a-0d494d336b30/db/089142.sst    at org.rocksdb.RocksDB.open(Native Method)    at org.rocksdb.RocksDB.open(RocksDB.java:306)    at org.apache.flink.contrib.streaming.state.RocksDBOperationUtils.openDB(RocksDBOperationUtils.java:80)    ... 25 more
 {code};;;","31/May/23 11:23;yunta;[~david.artiga] One possible reason might be the duplicated stream, which is used if local recovery is enabled, has written the file corruptly. And the restored file is broken. However, I did not prove it by comparing the changed code. And what kind of DFS did you use to store the checkpoint, S3, HDFS or what?

Please keep the broken file or maybe you could disable local-recovery to see whether this problem happened again.;;;","31/May/23 12:00;david.artiga;We are using GCP Storage buckets for both checkpoints and savepoints. I think the files are automatically deleted when the job restarts. I'll try to run some dummy clone with local-recovery disabled but hard to reproduce ... ;;;","28/Feb/24 08:25;easonqin;Hi [~yunta] ,

> First of all, we can certainly rescale from a checkpoint. Would this problem still exist if not rescaled? I just want to confirm whether the file (000232.sst) is corrupted already.
Can you share how to verify if an SST file is corrupt? Thank you.;;;",,,,,,,,,,,,,,
Improve the efficiency of ListStateWithCache#snapshotState,FLINK-31809,13532640,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hongfanxo,hongfanxo,14/Apr/23 09:39,19/Apr/23 01:27,04/Jun/24 20:41,19/Apr/23 01:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,,0,,,,,,"In the current implementation of {{{}ListStateWithCache{}}}, the {{snapshotState}} function writes the full data to the file system every time, even if the stored data has not changed since initialization. This can result in high IO costs, especially when working with large data sets. Additionally, this method is called in the same thread as operators, which can negatively impact job efficiency.

Furthermore, when using local file systems, the full data is also written to Flink state storage, which doubles the costs.

To address these issues, an incremental snapshot approach should be considered to reduce IO and network costs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-14 09:39:52.0,,,,,,,,,,"0|z1hafs:",9223372036854775807,Duplicate of FLINK-31410.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong examples of how to set operator name  in documents,FLINK-31808,13532639,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,14/Apr/23 09:26,15/Apr/23 05:14,04/Jun/24 20:41,15/Apr/23 05:14,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,"[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/#name-and-description]

 
{code:java}
.setName(""filter""){code}
 should be
{code:java}
.name(""filter""){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 15 05:14:12 UTC 2023,,,,,,,,,,"0|z1hafk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/23 05:14;libenchao;Fixed via https://github.com/apache/flink/commit/97dee4bd2ade278805241a245385df3ceeb90150 (1.18.0)

[~huwh] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Test architecture tests don't cover all test code,FLINK-31807,13532622,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Apr/23 08:16,19/Apr/24 09:24,04/Jun/24 20:41,20/Apr/23 09:58,cassandra-4.0.0,,,,,,,,,,,,,,,,,,,,cassandra-3.2.0,,,,,,,,,Connectors / Cassandra,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 09:58:33 UTC 2023,,,,,,,,,,"0|z1habs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 09:58;chesnay;main: 7d8942b264fc0c7364cced23a93f14157e9c9470;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Prod architecture tests didn't detect non-public API usage,FLINK-31806,13532621,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,echauchot,chesnay,chesnay,14/Apr/23 08:15,12/Jun/23 13:08,04/Jun/24 20:41,12/Jun/23 13:08,1.18.0,cassandra-3.0.0,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Connectors / Cassandra,Tests,,,,,0,pull-request-available,,,,,"FLINK-31805 wasn't detected by the production architecture tests.

Not sure if this is an issue on the cassandra or Flink side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 12 13:08:57 UTC 2023,,,,,,,,,,"0|z1habk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 13:08;echauchot;merged: 28abe81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra Source shouldn't use IOUtils,FLINK-31805,13532620,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Apr/23 08:11,14/Apr/23 10:28,04/Jun/24 20:41,14/Apr/23 08:42,cassandra-4.0.0,,,,,,,,,,,,,,,,,,,,cassandra-3.1.0,,,,,,,,,Connectors / Cassandra,,,,,,0,pull-request-available,,,,,IOUtils is not part of the public API and shouldn't be used.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 14 10:28:00 UTC 2023,,,,,,,,,,"0|z1habc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 08:42;chesnay;main: 1b8794380b268aad3b74d2b67ded8f59c1f07ea9;;;","14/Apr/23 10:28;dannycranmer;Given this is backwards compatible I am moving to v3.1.0
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ITCase MiniCluster test architecture rule should accept MiniClusterTestEnvironment,FLINK-31804,13532619,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Apr/23 08:04,09/Oct/23 09:52,04/Jun/24 20:41,20/Apr/23 07:45,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Connectors / Common,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33219,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 16 08:04:38 UTC 2023,,,,,,,,,,"0|z1hab4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 07:45;chesnay;master: d2e4b74e4a291f36e0771b2c6a7ded76d393f235;;;","17/May/23 17:08;a.pilipenko;Change in error message causing build failures for externalized connectors when building against 1.17 and 1.18-SNAPSHOT.

Examples of failed nightly builds:

cassandra[: https://github.com/apache/flink-connector-cassandra/actions/runs/4969574482/jobs/8892836772|https://github.com/apache/flink-connector-cassandra/actions/runs/4969574482/jobs/8892836772] 
aws: [https://github.com/apache/flink-connector-aws/actions/runs/4997824092/jobs/8952585780]

Adding new text of violation in ArchUnit store does not seem like a good solution since ArchUnit updates ignored violations by deleting ones that are not encountered during build. Because of that developers working on connectors should remember not to commit updated violation files when submitting PRs.

Can this change be made backwards compatible or backported to currently supported Flink versions?;;;","16/Jun/23 06:27;Weijie Guo;It seems that more than one connector has encountered this issue, as Nightly CI tests 1.17 and 1.18 at the same time, and inconsistent violation between the two versions can lead to CI failure. I wonder if we can backport this to release-1.17 and release-1.16(if it is also need this). [~chesnay] WDYT?;;;","16/Jun/23 08:04;chesnay;imo connectors should just skip the architecture tests when running against later versions.;;;",,,,,,,,,,,,,,,,,,,,,,,
UpdateJobResourceRequirementsRecoveryITCase.testRescaledJobGraphsWillBeRecoveredCorrectly(Path) is unstable on azure,FLINK-31803,13532617,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,Sergey Nuyanzin,Sergey Nuyanzin,14/Apr/23 07:58,21/Apr/23 10:39,04/Jun/24 20:41,21/Apr/23 10:39,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,Tests,,,,,0,pull-request-available,test-stability,,,,"{noformat}
Apr 07 01:28:23 java.util.concurrent.CompletionException: 
Apr 07 01:28:23 org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.NotFoundException: Job d3538259fba86dfc0bd9bd5680076836 not found
Apr 07 01:28:23 	at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:99)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.lambda$getExecutionGraphInternal$0(DefaultExecutionGraphCache.java:109)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:260)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1275)
Apr 07 01:28:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47996&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7713",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31470,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 21 10:39:44 UTC 2023,,,,,,,,,,"0|z1haao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 08:01;Sergey Nuyanzin;[~dmvk] I noticed that you've added this test under https://issues.apache.org/jira/browse/FLINK-31470, may I ask you to have a look?;;;","21/Apr/23 10:39;dmvk;master: c2ab806a3624471bb36f87ba98d51f672b7894fe;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Python py37-cython: commands failed, error at setup of ProcessWindowTests.test_count_sliding_window",FLINK-31802,13532614,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hxbks2ks,Sergey Nuyanzin,Sergey Nuyanzin,14/Apr/23 07:38,19/Feb/24 10:02,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,stale-assigned,test-stability,,,,"{noformat}
Apr 06 05:55:13 ___________________________________ summary ____________________________________
Apr 06 05:55:13 ERROR:   py37-cython: commands failed
Apr 06 05:55:13   py38-cython: commands succeeded
Apr 06 05:55:13   py39-cython: commands succeeded
Apr 06 05:55:13   py310-cython: commands succeeded
{noformat}
in logs there is such error for 37 not sure if it is related
{noformat}
Apr 06 04:26:30 ________ ERROR at setup of ProcessWindowTests.test_count_sliding_window ________
Apr 06 04:26:30 
Apr 06 04:26:30 cls = <class 'pyflink.datastream.tests.test_window.ProcessWindowTests'>
Apr 06 04:26:30 
Apr 06 04:26:30     @classmethod
Apr 06 04:26:30     def setUpClass(cls):
Apr 06 04:26:30         super(PyFlinkStreamingTestCase, cls).setUpClass()
Apr 06 04:26:30         cls.env.set_parallelism(2)
Apr 06 04:26:30 >       cls.env.set_runtime_mode(RuntimeExecutionMode.STREAMING)
Apr 06 04:26:30 
Apr 06 04:26:30 pyflink/testing/test_case_utils.py:193: 
Apr 06 04:26:30 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 {noformat}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47960&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=24593]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/23 07:45;Sergey Nuyanzin;logs-cron_azure-test_cron_azure_python-1680752943.zip;https://issues.apache.org/jira/secure/attachment/13057274/logs-cron_azure-test_cron_azure_python-1680752943.zip",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 10:35:02 UTC 2023,,,,,,,,,,"0|z1haa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 14:55;Sergey Nuyanzin;//cc [~jark] , [~hxbks2ks] ;;;","16/May/23 08:43;renqs;Downgraded to major as this issue only appeared once. ;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
Missing elasticsearch connector on maven central repository ,FLINK-31801,13532588,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,zjureel,zjureel,14/Apr/23 00:59,19/Jun/23 12:26,04/Jun/24 20:41,19/Jun/23 12:26,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,,0,,,,,,There are no versions 3.0.0-1.17 of flink-connector-elasticsearch6 and flink-connector-elasticsearch7 on maven central repository in document https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/elasticsearch/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 14 06:29:32 UTC 2023,,,,,,,,,,"0|z1ha48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 05:59;Weijie Guo;Thanks [~zjureel] for reporting this. This is because es connector has not yet released a version bump to flink-1.17. We use a unified form connector_version_flink_version in the doc. Maybe we can make some improvement to the shortcode of {{connector_artifact}} to make it prompt more information for unsupported flink versions instead of providing a non-existent artifact. 
Looking directly at it, we may should be able to extend the {{data/xxx.yml}} in the externalized connector repository to declare the supported {connector version & flink verion} or retrieve it from {{https://repo.maven.apache.org/}}.;;;","14/Apr/23 06:05;ruibin;Hi [~Weijie Guo], to clarify, does that mean if our project depends on elastic search connector, we can't upgrade to 1.17.0 for now? May I ask when will the 1.17.0 version for es connector be released? Thanks!;;;","14/Apr/23 06:15;Weijie Guo;[~ruibin] Actually, we have already start releasing connectors for Flink 1.17, see this [mail|https://lists.apache.org/thread/7h1n80oktpvzykndm6wnqhpfxghdx9s5].

As for blocking migrate to flink-1.17, I don't think this is a big problem. For es connector, 3.0.0-1.16 has a high probability of being compatible with flink-1.17. You can try it, or wait for the release process.;;;","14/Apr/23 06:21;Weijie Guo;You can vote for https://lists.apache.org/thread/zyc6r380td78733t4cg20z34gww8hl7z to verify and  speed up the progress of the release of es connector.;;;","14/Apr/23 06:29;ruibin;[~Weijie Guo] Thanks, I will look into it!;;;",,,,,,,,,,,,,,,,,,,,,,
Update tika to current,FLINK-31800,13532533,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,satanicmechanic,satanicmechanic,13/Apr/23 15:55,30/Jun/23 08:27,04/Jun/24 20:41,14/Apr/23 08:12,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,security,,,,,"Multiple vulns in org.apache.tika:tika-core-1.28.1
https://nvd.nist.gov/vuln/detail/CVE-2022-30126
https://nvd.nist.gov/vuln/detail/CVE-2022-30973
https://nvd.nist.gov/vuln/detail/CVE-2022-30126
https://nvd.nist.gov/vuln/detail/CVE-2022-25169",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 30 08:27:43 UTC 2023,,,,,,,,,,"0|z1h9s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 08:12;martijnvisser;[~satanicmechanic] I would recommend to first check if this dependency is really bundled with Flink. When running a {{mvn dependency:tree}}, there's no occurrence of tika anywhere as a (transitive) dependency. ;;;","29/Jun/23 18:38;satanicmechanic;[~martijnvisser] I'm being told its in opt/flink/bin/bash-java-utils.jar - would you mind checking?;;;","30/Jun/23 08:27;chesnay;It's not in there.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Python connector download link should refer to the url defined in externalized repository ,FLINK-31799,13532525,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,13/Apr/23 15:17,24/Apr/23 03:22,04/Jun/24 20:41,24/Apr/23 03:22,1.18.0,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Connectors / Common,,,,,,0,pull-request-available,,,,,Each externalized connector has its own yml file to manage the url of uber jar. We should introduce a new shortcode to support this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 24 03:22:30 UTC 2023,,,,,,,,,,"0|z1h9q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 03:22;Weijie Guo;master(1.18) via f5f69af1cedc9853fec2867b28ee9b4243cc817c.
release-1.17 via 4aef22bc5404b7efce8aa2f061eec436d59d0b68.
release-1.16 via ea0f5ef2d7bc8df99a3769b60d2ee43fef76348f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add config to enable/disable Kubernetes Events,FLINK-31798,13532520,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,darenwkt,darenwkt,13/Apr/23 14:39,05/Jul/23 14:09,04/Jun/24 20:41,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,Kubernetes Events are additional information to users that like logs should be configurable. This Jira propose to add a new operator config to allow user to enable/disable triggering or emitting Kubernetes Events for use cases such as performance optimisation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 14 08:22:46 UTC 2023,,,,,,,,,,"0|z1h9p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 08:22;gyfora;I don’t really see how events are a performance concern, can you describe the use-case why you would want to disable it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Move LeaderElectionService out of LeaderContender,FLINK-31797,13532514,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,13/Apr/23 14:12,16/Jun/23 15:56,04/Jun/24 20:41,15/Jun/23 06:34,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 15 06:34:20 UTC 2023,,,,,,,,,,"0|z1h9ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 06:34;mapohl;master: 55630de18baf6345072113f587006dac94f61c78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support service mesh istio with Flink kubernetes (both native and operator) for secure communications,FLINK-31796,13532513,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sergiosp,sergiosp,13/Apr/23 14:08,13/Apr/23 14:22,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,,,,,,"Currently Flink Native Kubernetes does not support istio + TLS cleanly : Flink assumes that pods will be able to communicate by ip-address, meanwhile istio + TLS does not allow routing by pod's ip-address.

This ticket is to track effort to support Flink with istio. Some workaround is to disable the istio sidecar container, but this is not secure [https://doc.akka.io/docs/akka-management/current/bootstrap/istio.html]. Akka allows to  secure the channel manually, but there is no documentation how to do this in the context of Flink. One potential solution for this is to have the documentation about how to configure akka cluster + TLS in flink.

For example when using native kubernetes deployment mode with high-availability (HA), and when new TaskManager pod is started to process a job, the TaskManager pod will attempt to register itself to the resource manager (JobManager). the TaskManager looks up the resource manager per ip-address (akka.tcp://flink@192.168.140.164:6123/user/rpc/resourcemanager_1).

Other affected features is metric collection.

Please see FLINK-31775 and FLINK-28171. Especially comment ""{_}Flink currently just doesn't support Istio.{_}""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-13 14:08:02.0,,,,,,,,,,"0|z1h9nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Config to enable/disable savepoint cleanup,FLINK-31795,13532511,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,darenwkt,darenwkt,13/Apr/23 13:48,03/May/23 15:39,04/Jun/24 20:41,03/May/23 15:39,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Proposed to add config: `KubernetesOperatorConfigOptions.{color:#9876aa}OPERATOR_SAVEPOINT_CLEANER_ENABLED` or {color}

{color:#6a8759}""savepoint.cleaner.enabled""{color}

 

{color:#6a8759}This proposal is to give user the flexibility to disable savepoint clean up by SavepointObserver for use cases such as u{color}{color:#6a8759}ser wants to keep full savepoint history.{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 03 15:39:56 UTC 2023,,,,,,,,,,"0|z1h9n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 02:43;yunta;[~darenwkt] If users want to keep the full savepoint history, why not make the {{savepoint.history.max.count}} unlimited? By doing so, we can avoid to introduce another configuration here.;;;","03/May/23 15:39;gyfora;merged to main 6df91ae17873c274b2b993ab7bef30f1297e5c3e;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Ignore unknown CRD fields,FLINK-31794,13532492,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,darenwkt,darenwkt,13/Apr/23 13:02,20/Apr/23 13:28,04/Jun/24 20:41,20/Apr/23 13:28,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Currently, Kubernetes Operator fails to create a CR (FlinkDeployment or FlinkSessionJob) if there is a new field in the CR that's not included in the CRD.

This change is to propose removing this restriction for FlinkSessionJobStatus, FlinkSessionJobSpec, FlinkDeploymentStatus, FlinkDeploymentSpec by adding class-level annotation `{color:#bbb529}@JsonIgnoreProperties{color}(ignoreUnknown = {color:#cc7832}true{color})`.

 

This means that any new fields in CR will be ignored if it's not registered in CRD preventing failures to create the CR. This helps users who need to deploy CRD and Kubernetes Operator separately to ensure backward compatibility.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 13:28:54 UTC 2023,,,,,,,,,,"0|z1h9iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 05:54;gyfora;If we are targeting the cases where the operator is upgraded before the CRD should we only add the annotation to the status fields? The operator doesn't normally update the spec itself.;;;","19/Apr/23 06:01;gyfora;But even then I don't understand how any change in the operator would solve the problem because the Kubernetes API will simply reject objects with invalid schemas.

I think we need to introduce the `additionalProperties` fields in the CRD for the different objects;;;","19/Apr/23 12:37;darenwkt;Hi Gyula, as discussed on Slack and reposting here for clarity, we are considering 2 cases here where CRD has upgraded (i.e new fields added):

Case 1: Operator is changed before CRD changes.
 * User attempts to ""k apply -f basicsessionjob.yaml"" with the new field added.
 * Outcome: No problem as Kubernetes will prune this new field. Operator is able to deserialize as new field is pruned.

 

Case 2: CRD is changed before Operator changes.
 * User attempts to ""k apply -f basicsessionjob.yaml"" with the new field added.
 * Outcome: Created FlinkSessionJob contains the new field. Operator is unable to deserialize this new/unknown field using Jackson. Hence, we propose to add {color:#172b4d}JsonIgnoreProperties for this case{color};;;","20/Apr/23 13:28;gyfora;merged to main 956dec079e6fcfa63ea409b4c076ee88b28f22dc;;;",,,,,,,,,,,,,,,,,,,,,,,
Remove dependency on flink-shaded for flink-connector-jdbc,FLINK-31793,13532476,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Wencong Liu,dannycranmer,dannycranmer,13/Apr/23 10:19,24/May/23 15:04,04/Jun/24 20:41,25/Apr/23 14:32,,,,,,,,,,,,,,,,,,,,,jdbc-3.1.1,jdbc-3.2.0,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,"The JDBC connector relies on flink-shaded and uses Flinks' shaded Guava. With the externalization of connector, connectors shouldn't rely on Flink-Shaded but instead shade dependencies such as this one themselves",,,,,,,,,,,,,,,,,,,,,FLINK-32032,,,,,,,,,,,,,FLINK-31722,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 14:32:00 UTC 2023,,,,,,,,,,"0|z1h9fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 10:26;Wencong Liu;Hello [~dannycranmer] , I think this could follow the same logic with FLINK-31722. Could you please assign to me?;;;","13/Apr/23 11:13;martijnvisser;[~Wencong Liu] Done!;;;","14/Apr/23 04:06;Wencong Liu;I've opened a pull request. Please take a look :)   [~dannycranmer] [~martijnvisser] ;;;","25/Apr/23 14:32;martijnvisser;Fixed in main: 7f5a8b7310671ae1f2a244e14de6cf233e1cf05b;;;",,,,,,,,,,,,,,,,,,,,,,,
Errors are not reported in the Web UI,FLINK-31792,13532442,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,dmvk,dmvk,13/Apr/23 05:59,28/Apr/23 16:58,04/Jun/24 20:41,13/Apr/23 09:14,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,,"After FLINK-29747, NzNotificationService can no longer be resolved by injector, and because we're using the injector directly, this is silently ignored.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29747,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 09:13:45 UTC 2023,,,,,,,,,,"0|z1h97s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 09:13;dmvk;master: 3f70900fadb3e88d98530aa61b39956ed223fe46

release-1.17: c10393826c4d9c5335e5e46f2d7afba89187de41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-292: Enhance COMPILED PLAN to support operator-level state TTL configuration,FLINK-31791,13532429,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,13/Apr/23 03:13,28/Jun/23 01:24,04/Jun/24 20:41,28/Jun/23 01:24,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,This is the parent task of FLIP-292.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31884,FLINK-31917,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-13 03:13:18.0,,,,,,,,,,"0|z1h94w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem batch sink should also respect to the PartitionCommitPolicy,FLINK-31790,13532423,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,aitozi,aitozi,13/Apr/23 02:26,09/Jun/23 06:18,04/Jun/24 20:41,09/Jun/23 06:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,,0,pull-request-available,,,,,"Currently, the {{PartitionCommitPolicy}} only take effect in the streaming file sink and hive file sink. The filesystem sink in batch mode should also respect to the commit policy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28773,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 09 06:18:43 UTC 2023,,,,,,,,,,"0|z1h93k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 05:23;aitozi;CC [~luoyuxia] ;;;","09/Jun/23 06:18;luoyuxia;master: 31e17bd13ef7f6cc4799441f8d6b1029f1468e6b;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Update axel to current,FLINK-31789,13532388,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,satanicmechanic,satanicmechanic,12/Apr/23 17:57,17/Apr/23 07:57,04/Jun/24 20:41,17/Apr/23 07:57,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,security,,,,,"Flink is shipping with version 2.6, which contains CVE-2020-13614. This was fixed in 2.17.8.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 14 12:40:03 UTC 2023,,,,,,,,,,"0|z1h8vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 08:07;martijnvisser;[~satanicmechanic] Where are you seeing that Flink is using/shipped with Axel? Are you referring to a Docker image or something? ;;;","13/Apr/23 15:52;satanicmechanic;This is coming from a Black Duck BA scan.  It is part of an image but that image doesn't include axel.  However, it could well be a false positive.  The scanner indicates that it is bring brought in by way of gettext. I am attempting to verify with my colleague.;;;","14/Apr/23 12:40;martijnvisser;[~satanicmechanic] I'm inclined to also close this as an invalid. ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add back Support emitValueWithRetract for TableAggregateFunction,FLINK-31788,13532369,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,hackergin,hackergin,12/Apr/23 15:13,29/Jan/24 07:31,04/Jun/24 20:41,14/Jan/24 05:30,1.14.0,1.14.2,1.14.3,1.14.4,1.14.5,1.14.6,1.15.0,1.15.1,1.15.2,1.15.3,1.15.4,1.16.0,1.16.1,1.16.2,1.16.3,1.17.0,1.17.1,1.17.2,1.18.0,1.18.1,1.18.2,1.19.0,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"This feature was originally implemented in the old planner: [https://github.com/apache/flink/pull/8550/files]

However, this feature was not implemented in the new planner , the Blink planner. 

With the removal of the old planner in version 1.14 [https://github.com/apache/flink/pull/16080] , this code was also removed.

 

We should add it back. 

 

origin discuss link: 

https://lists.apache.org/thread/rnvw8k3636dqhdttpmf1c9colbpw9svp",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33015,,,,,,,FLINK-34115,,,,,FLINK-34060,,,,,,,,,FLINK-34258,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 12 01:48:11 UTC 2024,,,,,,,,,,"0|z1h8rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 15:21;martijnvisser;[~twalthr] WDYT?;;;","13/Apr/23 08:26;twalthr;This looks like a regression to me. Does anyone from the original Blink mergers have context on this? [~jark] [~lzljs3620320];;;","13/Apr/23 09:54;jark;Hi [~hackergin], it seems the {{emitValueWithRetract}} was never introduced. [FLIP-29|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97552739] proposed {{emitValueWithRetract}}, but it was implemented as {{emitUpdateWithRetract}} in the first [PR| https://github.com/apache/flink/pull/8550/files] (I don't know why the method name was changed), and the {{emitUpdateWithRetract}} is still supported in the latest version in the blink planner. ;;;","13/Apr/23 11:11;hackergin;[~jark]  I'm sorry, I wrote the wrong name.  It should be {{{}emitUpdateWithRetract{}}}.   

I found that there is a relevant definition of {{emitUpdateWithRetract}} function in the code, and it is indeed checked during the validation phase. However, I did not find any relevant methods during codegen;;;","13/Apr/23 11:38;jark;Thanks for pointing out this [~hackergin]. If it's true, I think it makes sense to add it back. 

By the way, are you using this UDTAF?  I'm wondering why it is not found for so many years. Does it mean users rarely use it?;;;","13/Apr/23 11:42;hackergin;[~jark]  This question is from a user mail.   

The origin discuss link: 

[https://lists.apache.org/thread/rnvw8k3636dqhdttpmf1c9colbpw9svp];;;","01/Sep/23 08:03;qingyue;Sorry for just noticing this issue now. This problem should be a feature rollback when deprecating the old planner since 1.15. The method `emitUpdateWithRetract` was not called in the codegen, but the [user-defined table agg func doc|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/udfs/#retraction-example] still gives the retract example that evaluates this method.
{quote}Jark: I'm wondering why it is not found for so many years. Does it mean users rarely use it?
{quote}
IMO, we can evaluate whether users still need this feature versus the cost of fixing it. If we no longer intend to support this method, we can open a discussion to deprecate it. Alternatively, it's reasonable to add it back.;;;","04/Sep/23 07:09;martijnvisser;[~qingyue] Will you open a discussion thread on it? I feel like it's a regression and we should consider making it available again, but also +1 for seeing how many users actually use this;;;","07/Sep/23 03:47;qingyue;Hi, [~martijnvisser], I've started a [discussion|https://lists.apache.org/thread/879yfhy2rkq0mxpot7c6fw4s0cj3dsmp], and let's talk there. Cc [~jark] ;;;","13/Sep/23 03:48;qingyue;Hi, according to the discussion, we're on the consensus that we should support this feature, please assign the ticket to me, thanks.;;;","13/Sep/23 04:34;lincoln.86xy;[~qingyue] assigned to you.;;;","13/Sep/23 06:52;martijnvisser;Thanks for driving this [~qingyue];;;","12/Jan/24 01:48;qingyue;Fixed in master 01569644aedb56f792c7f7e04f84612d405b0bdf

Fixed in release-1.18 d7e9abe73a27edc2a27182b55307ff15d88f1042;;;",,,,,,,,,,,,,,
Add the explicit ROW constructor to the system function doc,FLINK-31787,13532364,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,aitozi,aitozi,aitozi,12/Apr/23 14:46,18/Apr/23 13:21,04/Jun/24 20:41,18/Apr/23 13:21,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18027,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 18 13:21:54 UTC 2023,,,,,,,,,,"0|z1h8qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 14:47;aitozi;see discussion in this ticket: https://issues.apache.org/jira/browse/FLINK-18027;;;","18/Apr/23 13:21;libenchao;Fixed via https://github.com/apache/flink/commit/fc7d821dfa16bad8ca58cb716b1b429d55b8317a

[~aitozi] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Removing unused HighAvailabilityServices implementations,FLINK-31786,13532363,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Apr/23 14:35,30/May/23 14:27,04/Jun/24 20:41,23/May/23 06:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 23 06:42:27 UTC 2023,,,,,,,,,,"0|z1h8q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 06:42;mapohl;master: 119b8c584dc865ee8a40a5c6410dddf8b36bac5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Move LeaderElectionService.stop() into LeaderElection.close(),FLINK-31785,13532361,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Apr/23 14:20,30/May/23 16:40,04/Jun/24 20:41,30/May/23 16:40,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 30 16:40:01 UTC 2023,,,,,,,,,,"0|z1h8ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/23 16:40;mapohl;master: 0b8bba6117bf2e8fe767b03d95274bac0b33faa0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add multiple-component support to DefaultLeaderElectionService,FLINK-31784,13532360,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Apr/23 14:07,04/Jul/23 20:45,04/Jun/24 20:41,04/Jul/23 20:45,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 04 20:45:35 UTC 2023,,,,,,,,,,"0|z1h8pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 20:45;mapohl;master: db4b59430664778b8c4f25b2e0eb0765b4dc10f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Replace LeaderElectionDriver in DefaultLeaderElectionService with MultipleComponentLeaderElectionDriver,FLINK-31783,13532359,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Apr/23 14:06,20/Mar/24 14:51,04/Jun/24 20:41,03/Jul/23 08:52,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34897,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 03 08:52:28 UTC 2023,,,,,,,,,,"0|z1h8pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/23 08:52;mapohl;master: 728e679d354c2c120afe7091f42a91b2e4701036;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Make DefaultLeaderElectionService implement MultipleComponentLeaderElectionService.Listener,FLINK-31782,13532358,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Apr/23 14:05,19/Jun/23 09:48,04/Jun/24 20:41,19/Jun/23 09:48,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 19 09:48:43 UTC 2023,,,,,,,,,,"0|z1h8p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/23 09:48;mapohl;master: de0efcbe533b88ae7c2da807e3b14d3929313adf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce contender ID into LeaderElectionService interface,FLINK-31781,13532357,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Apr/23 14:05,16/Jun/23 07:14,04/Jun/24 20:41,16/Jun/23 07:14,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 16 07:14:20 UTC 2023,,,,,,,,,,"0|z1h8ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 07:14;mapohl;master: 62b792d58c4f8d5b494b50daad2e5fc5047af330;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Allow users to disable ""Ensemble tracking"" for ZooKeeper",FLINK-31780,13532353,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Oleksandr Nitavskyi,Oleksandr Nitavskyi,Oleksandr Nitavskyi,12/Apr/23 13:55,15/May/23 08:06,04/Jun/24 20:41,15/May/23 08:06,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"In Apache Curator an option to skip ensemble tracking was added since version 5.0.0 ([CURATOR-568|https://issues.apache.org/jira/browse/CURATOR-568])

This can be useful in certain scenarios in which CuratorFramework is accessing to ZK clusters via load balancer or Virtual IPs. 
Thus in case Zookeeper of Flink user is running behind LB or Virtual IP ensemble tracking could be disabled too.

In case ZooKeeper is hidden under VIP it can return URL during Ensemble Tracking, which would lead to Unresolved Host Exception inside Curator Framework. On Flink level it would lead to cluster restart.

Currently HA with ZooKeeper can even lead to the JobManager failure. The scenario of the failure is next:

# Flink connects to ZooKeeper via configured URL.
# Ensemble tracking gets a new URL of ensemble, which is not obligatory accessible for Flink, because Zookeeper is under VIP.
# In case of reconnect Flink fails to Zookeeper, moreover due to ""UnresolvedHostException"" Flink's jobManager is killed.

*Acceptance Criteria:* Users of Apache Flink has a Zookeeper config option to disable ensemble tracking for ZooKeeper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CURATOR-568,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 15 08:06:59 UTC 2023,,,,,,,,,,"0|z1h8o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 08:06;mapohl;master: 4cbaa6ffd1ecc9c2382862eb8dfe399cc3952572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Track stable branch of externalized connector instead of specific release tag,FLINK-31779,13532342,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,12/Apr/23 12:31,17/Apr/23 03:20,04/Jun/24 20:41,17/Apr/23 03:20,1.16.1,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Connectors / Common,Documentation,,,,,0,pull-request-available,,,,,"As we [discussed|https://github.com/apache/flink-connector-opensearch/pull/17], docs for externalized connectors should point to the branch where that version of the docs are published, otherwise documentation fixes are not visible until a new release is made. After this, we can safely delete all dedicated doc branches.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 03:20:25 UTC 2023,,,,,,,,,,"0|z1h8lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/23 16:42;dannycranmer;merged commit [{{43bd5e5}}|https://github.com/apache/flink/commit/43bd5e5efa2ae6e90947905a44d770eec80f7802] into apache:master;;;","15/Apr/23 16:42;dannycranmer;[~Weijie Guo] can you open a PR for 1.16/1.17 branches too please?;;;","15/Apr/23 17:04;Weijie Guo;Thanks [~dannycranmer]. 
The backport pull request for these two branches has now been created:
[release-1.17|https://github.com/apache/flink/pull/22402]
[release-1.16|https://github.com/apache/flink/pull/22403];;;","17/Apr/23 03:20;Weijie Guo;merged commit [43bd5e5|https://github.com/apache/flink/commit/43bd5e5efa2ae6e90947905a44d770eec80f7802] into apache:master
merged commit [e77ea54|https://github.com/apache/flink/commit/e77ea54b4c2c314c0c9e7214ef314942b0fe58b6] into apache:release-1.17
merged commit [09e8e57|https://github.com/apache/flink/commit/09e8e57a1005e0e3452c32132c3f88620b917b8b] into apache:release-1.16;;;",,,,,,,,,,,,,,,,,,,,,,,
Casting array of rows produces incorrect result,FLINK-31778,13532339,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,soin08,soin08,soin08,12/Apr/23 12:08,01/Oct/23 16:56,04/Jun/24 20:41,29/Sep/23 17:55,1.15.4,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,1,pull-request-available,stale-assigned,,,,"{code:java}
select CAST(commissions AS ARRAY<ROW<amount DOUBLE>>) as commissions             from (select ARRAY[ROW(123), ROW(234)] commissions){code}
Expected output:
{code:java}
+----+--------------------------------+
| op |                    commissions |
+----+--------------------------------+
| +I |             [(123.0), (234.0)] |
+----+--------------------------------+
 {code}
Actual output:
{code:java}
+----+--------------------------------+
| op |                    commissions |
+----+--------------------------------+
| +I |             [(234.0), (234.0)] |
+----+--------------------------------+ {code}
Full working example: https://gist.github.com/soin08/5e0038dbefeba9192706e05a78ef3bc1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32296,,,,,,,,,,,,,,,,,,,,,,"12/Apr/23 12:23;soin08;image-2023-04-12-15-23-46-669.png;https://issues.apache.org/jira/secure/attachment/13057210/image-2023-04-12-15-23-46-669.png","12/Apr/23 12:19;soin08;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13057209/screenshot-2.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 01 16:56:12 UTC 2023,,,,,,,,,,"0|z1h8kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 12:24;soin08;The problem appears to be in the codegen of casting row to row: [https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/casting/RowToRowCastRule.java#L166]
{code:java}
context.declareClassField(
    className(BinaryRowData.class),
    rowTerm,
    constructorCall(BinaryRowData.class, inputFields.size()));
context.declareClassField(
    className(BinaryRowWriter.class),
    writerTerm, 
    constructorCall(BinaryRowWriter.class, rowTerm)); {code}
 Because we declare these as class fields, in the generated cast the same reference to row is used to populate all array elements, so in the end all elements reference the same (last) object.

!image-2023-04-12-15-23-46-669.png|width=967,height=590!

!screenshot-2.png|width=967,height=583!;;;","12/Apr/23 12:33;soin08;Proposed solution: move these to statements instead of class fields:
{code:java}
final CastRuleUtils.CodeWriter writer =
                new CastRuleUtils.CodeWriter()
                        .declStmt(
                                BinaryRowData.class,
                                rowTerm,
                                constructorCall(BinaryRowData.class, inputFields.size()))
                        .declStmt(
                                BinaryRowWriter.class,
                                writerTerm,
                                constructorCall(BinaryRowWriter.class, rowTerm));
 {code}
And add tests to [https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/functions/casting/CastRulesTest.java]

Tested it locally – works as expected.

Would like to discuss this approach and submit a PR once consensus is reached.;;;","12/Apr/23 12:53;martijnvisser;[~soin08] Nice ticket, thanks for the reproducer. Pinging [~jark] and [~lincoln.86xy] for their thoughts on this one;;;","13/Apr/23 10:13;jark;Hi [~soin08], nice catch! However, constructing BinaryRowWriter and BinaryRow is expensive. I think it is intended to reuse them. Maybe we can copy the result at the end. ;;;","16/Apr/23 20:18;soin08;Hi [~jark], thanks for the input! As I understand, copying BinaryRowData involves [constructing|https://github.com/apache/flink/blob/d84bf8064c2d0b66d1e2cd7e668376c171316530/flink-table/flink-table-common/src/main/java/org/apache/flink/table/data/binary/BinaryRowData.java#L416] a new instance anyway, but we can indeed reuse BinaryRowWriter. 

Copying like this:
{code:java}
writer.stmt(methodCall(writerTerm, ""complete""))
        .assignStmt(returnVariable, methodCall(rowTerm, ""copy"")); {code}
solves the problem.

Shall we proceed with this?;;;","17/Apr/23 02:18;jark;[~soin08] sure, I assigned this issue to you. Looking forward to the pull request. ;;;","21/Apr/23 08:43;soin08;Hi [~jark], done: https://github.com/apache/flink/pull/22412;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","12/Sep/23 13:50;nictownsend;Hi, I have encountered this in an additional scenario:

 

 
{code:java}
select commissions from (select ROW(ARRAY[ROW(123), ROW(234)]) commissions); {code}
 
{code:java}
+----+--------------------------------+
| op |                    commissions |
+----+--------------------------------+
| +I |           ([(123.0), (234.0)]) |
+----+--------------------------------+   {code}
 
{code:java}
create temporary view test as select commissions from (select ROW(ARRAY[ROW(123), ROW(234)]) commissions);
select commissions from test; {code}
 
{code:java}
+----+--------------------------------+
| op |                    commissions |
+----+--------------------------------+
| +I |           ([(234.0), (234.0)]) |
+----+--------------------------------+   {code}
 

Is this the same problem with the `BinaryRowWriter` or should I raise a new issue? Many thanks!;;;","29/Sep/23 12:34;davidradl;[~nictownsend]  I have done a test on the scenario you describe in your comment with the fix on and it does fix this scenario as well. ;;;","29/Sep/23 14:34;davidradl;[~soin08]  It looks like this is fixed in master [https://github.com/apache/flink/commit/6d62f9918ea2cbb8a10c705a25a4ff6deab60711]  - we should close out this issue.;;;","29/Sep/23 17:55;Sergey Nuyanzin;It looks like this issue has been fixed under https://issues.apache.org/jira/browse/FLINK-32296
sorry I didn't know about this one till today;;;","01/Oct/23 16:56;soin08;!https://www.meme-arsenal.com/memes/f74648c11d0945281e4f3c79711ace57.jpg!;;;",,,,,,,,,,,,,,
"Upsert Kafka use Avro Confluent, key is ok, but all values are null.",FLINK-31777,13532335,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,ge.bugman,ge.bugman,12/Apr/23 11:48,17/Apr/23 02:22,04/Jun/24 20:41,17/Apr/23 02:20,1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"I use debezium send data to kafka with confluent avro format,  when I use 'upsert-kafka' connector, all values are null (primary key has value), but in 'kafka' connector all values are well.

My upsert-kafka table like this:
{code:java}
// code placeholder

create table TEA02
(
    SUB_SYSTEM_ENAME varchar(255),
    REC_CREATOR      varchar(255),
    REC_CREATE_TIME  varchar(255),
    REC_REVISOR      varchar(255),
    REC_REVISE_TIME  varchar(255),
    ARCHIVE_FLAG     varchar(255),
    SUB_SYSTEM_CNAME varchar(255),
    SUB_SYSTEM_FNAME varchar(255),
    SUB_SYSTEM_LEVEL varchar(255),
    primary key (SUB_SYSTEM_ENAME) not enforced
) WITH (
 'connector' = 'upsert-kafka',
 'topic' = 'dev.oracle.JNMMM1.TEA02',
 'properties.bootstrap.servers' = '10.0.170.213:9092,10.0.170.214:9092,10.0.170.215:9092',
 'properties.group.id' = 'TEA02',
 'key.format' = 'avro-confluent',
 'key.avro-confluent.url' = 'http://10.0.170.213:8081',
 'value.format' = 'avro-confluent',
 'value.avro-confluent.url' = 'http://10.0.170.213:8081',
 'value.fields-include' = 'EXCEPT_KEY'
); {code}
query result:
||SUB_SYSTEM_ENAME(this columns is pk)||REC_CREATOR||REC_CREATE_TIME||.......||
|CJ|null|null|null|

Specified subject still not working.
{code:java}
// code placeholder
 'key.avro-confluent.subject' = 'dev.oracle.JNMMM1.TEA02-key',
 'value.avro-confluent.subject' = 'dev.oracle.JNMMM1.TEA02-value' {code}
BTW: All debezium events are READ operation.

The confluent schemas are here:
{code:java}
// code placeholder
[{
    ""subject"": ""dev.oracle-key"",
    ""version"": 1,
    ""id"": 1,
    ""schema"": ""{\""type\"":\""record\"",\""name\"":\""SchemaChangeKey\"",\""namespace\"":\""io.debezium.connector.oracle\"",\""fields\"":[{\""name\"":\""databaseName\"",\""type\"":\""string\""}],\""connect.version\"":1,\""connect.name\"":\""io.debezium.connector.oracle.SchemaChangeKey\""}""
}, {
    ""subject"": ""dev.oracle-value"",
    ""version"": 1,
    ""id"": 2,
    ""schema"": ""{\""type\"":\""record\"",\""name\"":\""SchemaChangeValue\"",\""namespace\"":\""io.debezium.connector.oracle\"",\""fields\"":[{\""name\"":\""source\"",\""type\"":{\""type\"":\""record\"",\""name\"":\""Source\"",\""fields\"":[{\""name\"":\""version\"",\""type\"":\""string\""},{\""name\"":\""connector\"",\""type\"":\""string\""},{\""name\"":\""name\"",\""type\"":\""string\""},{\""name\"":\""ts_ms\"",\""type\"":\""long\""},{\""name\"":\""snapshot\"",\""type\"":[{\""type\"":\""string\"",\""connect.version\"":1,\""connect.parameters\"":{\""allowed\"":\""true,last,false,incremental\""},\""connect.default\"":\""false\"",\""connect.name\"":\""io.debezium.data.Enum\""},\""null\""],\""default\"":\""false\""},{\""name\"":\""db\"",\""type\"":\""string\""},{\""name\"":\""sequence\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""schema\"",\""type\"":\""string\""},{\""name\"":\""table\"",\""type\"":\""string\""},{\""name\"":\""txId\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""scn\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""commit_scn\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""lcr_position\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""rs_id\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""ssn\"",\""type\"":[\""null\"",\""int\""],\""default\"":null},{\""name\"":\""redo_thread\"",\""type\"":[\""null\"",\""int\""],\""default\"":null},{\""name\"":\""user_name\"",\""type\"":[\""null\"",\""string\""],\""default\"":null}],\""connect.name\"":\""io.debezium.connector.oracle.Source\""}},{\""name\"":\""ts_ms\"",\""type\"":\""long\""},{\""name\"":\""databaseName\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""schemaName\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""ddl\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""tableChanges\"",\""type\"":{\""type\"":\""array\"",\""items\"":{\""type\"":\""record\"",\""name\"":\""Change\"",\""namespace\"":\""io.debezium.connector.schema\"",\""fields\"":[{\""name\"":\""type\"",\""type\"":\""string\""},{\""name\"":\""id\"",\""type\"":\""string\""},{\""name\"":\""table\"",\""type\"":{\""type\"":\""record\"",\""name\"":\""Table\"",\""fields\"":[{\""name\"":\""defaultCharsetName\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""primaryKeyColumnNames\"",\""type\"":[\""null\"",{\""type\"":\""array\"",\""items\"":\""string\""}],\""default\"":null},{\""name\"":\""columns\"",\""type\"":{\""type\"":\""array\"",\""items\"":{\""type\"":\""record\"",\""name\"":\""Column\"",\""fields\"":[{\""name\"":\""name\"",\""type\"":\""string\""},{\""name\"":\""jdbcType\"",\""type\"":\""int\""},{\""name\"":\""nativeType\"",\""type\"":[\""null\"",\""int\""],\""default\"":null},{\""name\"":\""typeName\"",\""type\"":\""string\""},{\""name\"":\""typeExpression\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""charsetName\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""length\"",\""type\"":[\""null\"",\""int\""],\""default\"":null},{\""name\"":\""scale\"",\""type\"":[\""null\"",\""int\""],\""default\"":null},{\""name\"":\""position\"",\""type\"":\""int\""},{\""name\"":\""optional\"",\""type\"":[\""null\"",\""boolean\""],\""default\"":null},{\""name\"":\""autoIncremented\"",\""type\"":[\""null\"",\""boolean\""],\""default\"":null},{\""name\"":\""generated\"",\""type\"":[\""null\"",\""boolean\""],\""default\"":null},{\""name\"":\""comment\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""defaultValueExpression\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""enumValues\"",\""type\"":[\""null\"",{\""type\"":\""array\"",\""items\"":\""string\""}],\""default\"":null}],\""connect.version\"":1,\""connect.name\"":\""io.debezium.connector.schema.Column\""}}},{\""name\"":\""comment\"",\""type\"":[\""null\"",\""string\""],\""default\"":null}],\""connect.version\"":1,\""connect.name\"":\""io.debezium.connector.schema.Table\""}}],\""connect.version\"":1,\""connect.name\"":\""io.debezium.connector.schema.Change\""}}}],\""connect.version\"":1,\""connect.name\"":\""io.debezium.connector.oracle.SchemaChangeValue\""}""
}, {
    ""subject"": ""dev.oracle.JNMMM1.TEA02-key"",
    ""version"": 1,
    ""id"": 3,
    ""schema"": ""{\""type\"":\""record\"",\""name\"":\""Key\"",\""namespace\"":\""dev.oracle.JNMMM1.TEA02\"",\""fields\"":[{\""name\"":\""SUB_SYSTEM_ENAME\"",\""type\"":{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""default\"":\"" \""}],\""connect.name\"":\""dev.oracle.JNMMM1.TEA02.Key\""}""
}, {
    ""subject"": ""dev.oracle.JNMMM1.TEA02-value"",
    ""version"": 1,
    ""id"": 4,
    ""schema"": ""{\""type\"":\""record\"",\""name\"":\""Envelope\"",\""namespace\"":\""dev.oracle.JNMMM1.TEA02\"",\""fields\"":[{\""name\"":\""before\"",\""type\"":[\""null\"",{\""type\"":\""record\"",\""name\"":\""Value\"",\""fields\"":[{\""name\"":\""REC_CREATOR\"",\""type\"":[{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""null\""],\""default\"":\"" \""},{\""name\"":\""REC_CREATE_TIME\"",\""type\"":[{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""null\""],\""default\"":\"" \""},{\""name\"":\""REC_REVISOR\"",\""type\"":[{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""null\""],\""default\"":\"" \""},{\""name\"":\""REC_REVISE_TIME\"",\""type\"":[{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""null\""],\""default\"":\"" \""},{\""name\"":\""ARCHIVE_FLAG\"",\""type\"":{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""default\"":\"" \""},{\""name\"":\""SUB_SYSTEM_ENAME\"",\""type\"":{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""default\"":\"" \""},{\""name\"":\""SUB_SYSTEM_CNAME\"",\""type\"":{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""default\"":\"" \""},{\""name\"":\""SUB_SYSTEM_FNAME\"",\""type\"":{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""default\"":\"" \""},{\""name\"":\""SUB_SYSTEM_LEVEL\"",\""type\"":{\""type\"":\""string\"",\""connect.default\"":\"" \""},\""default\"":\"" \""}],\""connect.name\"":\""dev.oracle.JNMMM1.TEA02.Value\""}],\""default\"":null},{\""name\"":\""after\"",\""type\"":[\""null\"",\""Value\""],\""default\"":null},{\""name\"":\""source\"",\""type\"":{\""type\"":\""record\"",\""name\"":\""Source\"",\""namespace\"":\""io.debezium.connector.oracle\"",\""fields\"":[{\""name\"":\""version\"",\""type\"":\""string\""},{\""name\"":\""connector\"",\""type\"":\""string\""},{\""name\"":\""name\"",\""type\"":\""string\""},{\""name\"":\""ts_ms\"",\""type\"":\""long\""},{\""name\"":\""snapshot\"",\""type\"":[{\""type\"":\""string\"",\""connect.version\"":1,\""connect.parameters\"":{\""allowed\"":\""true,last,false,incremental\""},\""connect.default\"":\""false\"",\""connect.name\"":\""io.debezium.data.Enum\""},\""null\""],\""default\"":\""false\""},{\""name\"":\""db\"",\""type\"":\""string\""},{\""name\"":\""sequence\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""schema\"",\""type\"":\""string\""},{\""name\"":\""table\"",\""type\"":\""string\""},{\""name\"":\""txId\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""scn\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""commit_scn\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""lcr_position\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""rs_id\"",\""type\"":[\""null\"",\""string\""],\""default\"":null},{\""name\"":\""ssn\"",\""type\"":[\""null\"",\""int\""],\""default\"":null},{\""name\"":\""redo_thread\"",\""type\"":[\""null\"",\""int\""],\""default\"":null},{\""name\"":\""user_name\"",\""type\"":[\""null\"",\""string\""],\""default\"":null}],\""connect.name\"":\""io.debezium.connector.oracle.Source\""}},{\""name\"":\""op\"",\""type\"":\""string\""},{\""name\"":\""ts_ms\"",\""type\"":[\""null\"",\""long\""],\""default\"":null},{\""name\"":\""transaction\"",\""type\"":[\""null\"",{\""type\"":\""record\"",\""name\"":\""block\"",\""namespace\"":\""event\"",\""fields\"":[{\""name\"":\""id\"",\""type\"":\""string\""},{\""name\"":\""total_order\"",\""type\"":\""long\""},{\""name\"":\""data_collection_order\"",\""type\"":\""long\""}],\""connect.version\"":1,\""connect.name\"":\""event.block\""}],\""default\"":null}],\""connect.version\"":1,\""connect.name\"":\""dev.oracle.JNMMM1.TEA02.Envelope\""}""
}] {code}
 

 

---------------

 

I tried kafka connector, the values are still null, may be confluent version 7.3.3 is not siutable flink?

my table look like:

 

 
{code:java}
// code placeholder
create table TEA02
(
    SUB_SYSTEM_ENAME string,
    REC_CREATOR      string,
    REC_CREATE_TIME  string,
    REC_REVISOR      string,
    REC_REVISE_TIME  string,
    ARCHIVE_FLAG     string,
    SUB_SYSTEM_CNAME string,
    SUB_SYSTEM_FNAME string,
    SUB_SYSTEM_LEVEL string
) WITH (
 'connector' = 'kafka',
 'topic' = 'dev.oracle.JNMMM1.TEA02',
 'properties.bootstrap.servers' = '10.0.170.213:9092,10.0.170.214:9092,10.0.170.215:9092',
 'properties.group.id' = 'TEA02',
 'key.format' = 'avro-confluent',
 'key.fields' = 'SUB_SYSTEM_ENAME',
 'key.avro-confluent.url' = 'http://10.0.170.213:8081',
 'value.format' = 'avro-confluent',
 'value.avro-confluent.url' = 'http://10.0.170.213:8081',
 'value.fields-include' = 'EXCEPT_KEY',
 'key.avro-confluent.subject' = 'dev.oracle.JNMMM1.TEA02-key',
 'value.avro-confluent.subject' = 'dev.oracle.JNMMM1.TEA02-value',
 'scan.startup.mode' = 'earliest-offset'
); {code}
 

 ","Flink: 1.16.0

Confluent version: 7.3.3

Debezium version: 2.1.0/2.0.0

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 02:22:18 UTC 2023,,,,,,,,,,"0|z1h8k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 12:12;martijnvisser;[~ge.bugman] Are you sure you've used Flink 1.6.0 for testing, or was this Flink 1.16.0 ?;;;","12/Apr/23 12:13;ge.bugman;[~martijnvisser] Sorry, that's 1.16.... my fault...;;;","12/Apr/23 12:59;ge.bugman;[~martijnvisser] Hello, do you know what json format can use upsert-kafka? I tried debezium style json and `value.format` specifie `debezium-json`, I got an error debezium-json is not insert only, than I specifie `json`, values are still null, I don't know how to do this...

Sorry this is my first time to use upsert-kafka.

 

Thanks.;;;","13/Apr/23 10:27;jark;Hi [~ge.bugman], ""upsert-kafka"" doesn't support JSON or AVRO in debezium style (as you can see, there is a ""not insert only"" exception). ""upsert-kafka"" deserializes messages using the given format, say ""json"" or ""avro-confluent"" in your case. For example, REC_CREATOR can be extracted if there is a such a field in the first level in the JSON, e.g., 
{code}
{""SUB_SYSTEM_ENAME"": ""CJ"", ""REC_CREATOR"": ""Jark""}
{code}
But the actual ""REC_CREATOR"" field is nested in ""before"" or ""after"" field. That's why it's failed to extract the field and get null. 


In your case, it is recommended to use ""kafka"" connector + ""debezium-avro-conflluent"" format. ;;;","17/Apr/23 01:34;ge.bugman;[~jark] Thank you!;;;","17/Apr/23 02:22;jark;I closed the issue because this is a by-design behavior. Feel free to continue to discuss if you have further problems. ;;;",,,,,,,,,,,,,,,,,,,,,
 Introducing sub-interface LeaderElectionService.LeaderElection,FLINK-31776,13532333,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Apr/23 11:25,26/May/23 05:43,04/Jun/24 20:41,26/May/23 05:43,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 26 05:43:12 UTC 2023,,,,,,,,,,"0|z1h8jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 05:43;mapohl;master: 3b53f30d396e5a7f22330d6522ea26450e238628;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
High-Availability not supported in kubernetes when istio enabled,FLINK-31775,13532233,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,sergiosp,sergiosp,11/Apr/23 19:16,12/Apr/23 15:18,04/Jun/24 20:41,12/Apr/23 07:27,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,,,,,,"When using native kubernetes deployment mode with high-availability (HA), and when new TaskManager pod is started to process a job, the TaskManager pod will attempt to register itself to the resource manager (JobManager). the TaskManager looks up the resource manager per ip-address (akka.tcp://flink@192.168.140.164:6123/user/rpc/resourcemanager_1)

 

Nevertheless when istio is enabled, the resolution by ip address is blocked, and hence we see that the job cannot start because task manager cannot register with the resource manager:

2023-04-10 23:24:19,752 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Could not resolve ResourceManager address akka.tcp://flink@192.168.140.164:6123/user/rpc/resourcemanager_1, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@192.168.140.164:6123/user/rpc/resourcemanager_1.

 

Notice that when HA is disabled, the resolution of the resource manager is made by service name and so the resource manager can be found

 

2023-04-11 00:49:34,162 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka.tcp://flink@myenv-dev-flink-cluster.myenv-dev:6123/user/rpc/resourcemanager_* under registration id 83ad942597f86aa880ee96f1c2b8b923.

 

Notice in my case , it is not possible to disable istio as explained here: [https://doc.akka.io/docs/akka-management/current/bootstrap/istio.html]

 

Although similar to https://issues.apache.org/jira/browse/FLINK-28171 , logging as separate defect as I believe the fix of FLINK-28171 won't fix this case. FLINK-28171  is about Flink Kubernetes Operator and this is about native kubernetes deployment.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 15:17:50 UTC 2023,,,,,,,,,,"0|z1h7xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 03:54;huwh;Thanks [~sergiosp] report this. 

IMHO, this is the same issue as FLINK-28171.
After FLINK-28171 is fixed, you can define the appProtocol  in podTemplate both for kubernetes-operator and native kubernetes.;;;","12/Apr/23 07:26;martijnvisser;I also don't think this is a bug; Flink currently just doesn't support Istio. If anything, it's a new feature/improvement. ;;;","12/Apr/23 07:27;martijnvisser;+1 for [~huwh] his comment on FLINK-28171: the involved component is ""Deployment/Kubernetes"", not ""Kubernetes Operator"" ;;;","12/Apr/23 14:13;sergiosp;Hi [~huwh] , [~martijnvisser] 

Thanks for the help triaging !

 

One question on the solution of adding ""appProtocol"" into the podTemplate from FLINK-28171. Even after we set the appProtocol, do we need to bypass the istio sidecar?

 

Because we could not bypass istio sidecar in our case ~;;;","12/Apr/23 15:17;sergiosp;Another confusion about how the fix for  FLINK-28171 can help this case is becase *appProtocol* applies to services. Meanwhile this defect is about ip-addresses.

Could not resolve ResourceManager address akka.tcp://flink@{*}192.168.140.164:6123{*}/user/rpc/resourcemanager_1,

Then, we are not sure adding appProtocol could solve the ip-address routing (as there is no service involved)

 ;;;",,,,,,,,,,,,,,,,,,,,,,
Add document for delete and update statement,FLINK-31774,13532208,13517829,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,11/Apr/23 15:40,08/Jun/23 12:24,04/Jun/24 20:41,08/Jun/23 12:24,,,,,,,,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,I do not find the declaration about the usage of DELETE and UPDATE statement in the SQL section. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 08 01:52:05 UTC 2023,,,,,,,,,,"0|z1h7s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 15:48;aitozi;I think we should add the description for this two statements similar to the INSERT statement. CC [~luoyuxia] I'm willing to work on this.;;;","12/Apr/23 01:15;luoyuxia;[~aitozi] Thanks for raising it and voluntering. I miss to add these statements since current no connector had integrated the delete & update api . But I think it worths a document since Flink itself supports such synatax. Assign to you now, please ping me to review after the pr is ready.;;;","14/Apr/23 05:48;aitozi;Hi [~luoyuxia] The pr is ready for review, please take a look when you are free.;;;","08/Jun/23 01:52;luoyuxia;master: 77214f138cf759a3ee5466c9b2379e717227a0ae

1.17: 423e72c10629205f36c83f47e8ec4ee9534f5641;;;",,,,,,,,,,,,,,,,,,,,,,,
Separate DefaultLeaderElectionService.start(LeaderContender) into two separate methods for starting the driver and registering a contender,FLINK-31773,13532184,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,11/Apr/23 12:58,13/Jun/23 09:42,04/Jun/24 20:41,22/May/23 07:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,FLINK-31838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32013,,,,,,,FLINK-32311,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 22 07:42:25 UTC 2023,,,,,,,,,,"0|z1h7mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 07:42;mapohl;master: 4576e4384ff36623712043564039f654c3b44a30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisStreamsSink Performance regression due to AIMD rate limiting strategy,FLINK-31772,13532179,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,chalixar,chalixar,11/Apr/23 12:07,16/May/23 13:43,04/Jun/24 20:41,16/May/23 13:43,1.16.1,aws-connector-4.1.0,,,,,,,,,,,,,,,,,,,aws-connector-4.2.0,,,,,,,,,Connectors / Kinesis,,,,,,1,pull-request-available,,,,,"h1. Issue

While benchmarking the {{KinesisStreamSink}} for 1.15 against the legacy {{FlinkKinesisProduced}} , it is observed that the new sink has a performance regression against the deprecated sink for same environment setting.

Further investigation identified that the AIMD Ratelimiting strategy is the bottleneck for the regression. 

Attached results for {{KinesisStreamSink}}  against {FlinkKinesisProducer} and {KinesisStreamSink} after disabling {{AIMDRatelimitingStrategy}}



h2. Environment Settings
- Benchmarking was performed on AWS KDA.
- Application logic is just sending records downstream
- Application parallelism was tested to be 1.
- Kinesis stream number of shards was tested with 8 and 12.
- payload size was 1Kb and 100Kb.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/23 12:02;chalixar;Screenshot 2023-04-11 at 12.56.10.png;https://issues.apache.org/jira/secure/attachment/13057191/Screenshot+2023-04-11+at+12.56.10.png","11/Apr/23 12:02;chalixar;Screenshot 2023-04-11 at 12.58.09.png;https://issues.apache.org/jira/secure/attachment/13057190/Screenshot+2023-04-11+at+12.58.09.png","11/Apr/23 12:02;chalixar;Screenshot 2023-04-11 at 13.01.47.png;https://issues.apache.org/jira/secure/attachment/13057189/Screenshot+2023-04-11+at+13.01.47.png","17/Apr/23 12:04;chalixar;Screenshot 2023-04-17 at 13.02.31.png;https://issues.apache.org/jira/secure/attachment/13057325/Screenshot+2023-04-17+at+13.02.31.png","17/Apr/23 12:04;chalixar;Screenshot 2023-04-17 at 13.03.24.png;https://issues.apache.org/jira/secure/attachment/13057326/Screenshot+2023-04-17+at+13.03.24.png","17/Apr/23 12:32;chalixar;Screenshot 2023-04-17 at 13.03.34-1.png;https://issues.apache.org/jira/secure/attachment/13057329/Screenshot+2023-04-17+at+13.03.34-1.png","17/Apr/23 12:04;chalixar;Screenshot 2023-04-17 at 13.03.34.png;https://issues.apache.org/jira/secure/attachment/13057327/Screenshot+2023-04-17+at+13.03.34.png",,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 16 13:42:13 UTC 2023,,,,,,,,,,"0|z1h7ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 12:27;dannycranmer;Thanks [~chalixar] , I have assigned the Jira to you;;;","17/Apr/23 12:34;chalixar;Thanks [~dannycranmer]

I have published a fix for the issue [https://github.com/apache/flink-connector-aws/pull/70]

Attaching Results of performance benchmark and KDS performance after applying fix in regression cases.

 

 
h2. Performace Benchmark Results

 
|Parallelism/Shards/Payload|paralellism|shards|payload|records/sec|Async Sink|Async Sink With Configured Ratelimiting Strategy Thourouput (MB/s)|KPL Thourouput (MB/s)|Percentage of Maximum Thourouput|Improvement Percentage against KPL|Improvement Percentage against Async Sink|
|Low/Low/Low|1|1|1024|10000|0.991|1|0.958|1|4.2|0.9|
|Low/Low/High|1|1|102400|100|0.9943|1|0.975|1|2.5|0.57|
|Low/Med/Low|1|8|1024|80000|4.12|4.57|6.45|0.57125|-23.5|5.625|
|Low/Med/High|1|8|102400|800|4.35|7.65|7.45|0.95625|2.5|41.25|
|Med/Low/Low|8|1|1024|20000|0.852|0.846|0.545|0.846|30.1|-0.6|
|Med/Low/High|8|1|102400|200|0.921|0.867|0.975|0.867|-10.8|-5.4|
|Med/Med/Low|8|8|1024|80000|5.37|4.76|5.87|0.595|-13.875|-7.625|
|Med/Med/High|8|8|102400|800|7.53|7.69|5.95|0.96125|21.75|2|
|Med/High/Low|8|64|1024|80000|32.5|37.4|40.7|0.58438|-5.15625|7.65625|
|Med/High/High|8|64|102400|800|47.27|60.4|56.27|0.94375|6.45312|20.51562|
|High/High/Low|256|256|1024|300000|127|127|131|0.49609|-1.5625|0|
|High/High/High|256|256|102400|3000|225|246|215|0.96094|12.10938|8.20313|

 

 
h2. KDS Sink performance after applying fix


!Screenshot 2023-04-17 at 13.03.34.png!




!Screenshot 2023-04-17 at 13.02.31.png!;;;","16/May/23 13:42;dannycranmer;Merged commit [{{0be8192}}|https://github.com/apache/flink-connector-aws/commit/0be819249cfb2930b9356a8228bdea025c04d74e] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Improve select available slot from SlotPool,FLINK-31771,13532177,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,11/Apr/23 11:56,23/Jul/23 09:16,04/Jun/24 20:41,23/Jul/23 09:16,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,"DefaultScheduler will request slots from SlotPool for tasks one by one.
For each task, the PhysicalSlotProviderImpl#tryAllocateFromAvailable will retrieve all available slots from DefaultAllocatedSlotPool#getFreeSlotsInformation, and then select the best slot by SlotSelectionStrategy.

Currently DefaultAllocatedSlotPool#getFreeSlotsInformation always calculates the taskExecutorUtilization.  This causes task schedules to be too slow when there are lots of slots, such as 20000 slots total. But only the EvenlySpreadOutLocationPreferenceSlotSelectionStrategy uses this utilization.

So I would like to move the calculation of taskExecutorUtilization to usage. DefaultAllocatedSlotPool provides a function: getTaskExecutorUtilization, and is only used in EvenlySpreadOutLocationPreferenceSlotSelectionStrategy.

This change could reduce the latency of allocated 20000 slots from 72s to 12s in my local IDE.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 14 08:03:58 UTC 2023,,,,,,,,,,"0|z1h7lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 06:11;wanglijie;[~huwh] This makes sense to me. Besides, I think we should also reduce the number of calls of ""getFreeSlotInformations"". In current implementation, the scheduler will batch request slots for tasks in the same pipeline region(ExecutionSlotAllocator#allocateSlotsFor), but the slot allocator will process these requests one by one, and call ""getFreeSlotInformations"" once for each request. 

We can optimize it to call ""getFreeSlotInformations"" once for a bulk (of slot requests), instead of once for each slot request. WDYT?;;;","14/Apr/23 07:14;huwh;[~wanglijie] Thanks for the reply. I think select slots for a bulk will be great helpful. How about create a new issue to track it?

And, Currently select slot from getFreeSlotInformations only occur in failover. IMO we also add this scenario to benchmark.

 

I would like to create tickets for these if these make sense;;;","14/Apr/23 07:59;wanglijie;All things make sense to me :). Thanks [~huwh];;;","14/Apr/23 08:03;wanglijie;cc  [~zhuzh], You may also be interested in this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,
OracleExactlyOnceSinkE2eTest.testInsert fails for JDBC connector,FLINK-31770,13532142,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,11/Apr/23 08:10,13/Jun/23 09:38,04/Jun/24 20:41,13/Jun/23 09:38,jdbc-3.1.0,,,,,,,,,,,,,,,,,,,,jdbc-3.1.1,,,,,,,,,Connectors / JDBC,,,,,,0,test-stability,,,,,"{code:java}
Caused by: org.apache.flink.util.FlinkRuntimeException: unable to start XA transaction, xid: 201:cea0dbd44c6403283f4050f627bed37c020000000000000000000000:e0070697, error -3: resource manager error has occurred. [XAErr (-3): A resource manager error has occured in the transaction branch. ORA-2045 SQLErr (0)]
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.wrapException(XaFacadeImpl.java:369)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.access$800(XaFacadeImpl.java:67)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl$Command.lambda$fromRunnable$0(XaFacadeImpl.java:301)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl$Command.lambda$fromRunnable$4(XaFacadeImpl.java:340)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.execute(XaFacadeImpl.java:280)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.start(XaFacadeImpl.java:170)
	at org.apache.flink.connector.jdbc.xa.XaFacadePoolingImpl.start(XaFacadePoolingImpl.java:84)
	at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.beginTx(JdbcXaSinkFunction.java:316)
	at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.open(JdbcXaSinkFunction.java:241)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:731)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:706)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:672)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:750)
{code}

https://github.com/apache/flink-connector-jdbc/actions/runs/4647776511/jobs/8224977183#step:13:325",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30790,,,,,FLINK-31847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 13 09:38:33 UTC 2023,,,,,,,,,,"0|z1h7dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 08:11;martijnvisser;Other failed runs: 

https://github.com/apache/flink-connector-jdbc/actions/runs/4611631661/jobs/8151551281#step:13:328

https://github.com/apache/flink-connector-jdbc/actions/runs/4616119228/jobs/8160730869#step:13:328;;;","17/Apr/23 06:42;martijnvisser;https://github.com/apache/flink-connector-jdbc/actions/runs/4710470564/jobs/8354139817#step:13:354

[~eskabetxe] Any idea why these issues now appear? Is it because of the refactoring? ;;;","17/Apr/23 08:31;eskabetxe;Hi [~martijnvisser],
On Jdbc the [PR is not merged|https://github.com/apache/flink-connector-jdbc/pull/22] (only on 1.16 that I made a small PR)

The current PR has more changes as this problem was discovered during that changes..
But all Jdbc PRs are stuck and not being reviewed.;;;","13/Jun/23 09:38;martijnvisser;This problem was caused by one commit that wasn't backported from {{main}} to {{v3.1}}. This is now resolved via:

apache/flink-connector-jdbc:v3.1 - b6991d8e888ed3504fefcbedf8a47277c5e088f9 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Add percentiles to aggregated metrics,FLINK-31769,13532140,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,11/Apr/23 08:04,17/Apr/23 16:48,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Runtime / Metrics,,,,,0,,,,,,"*Background*

Currently only min/avg/max of metrics are exposed via REST API. Flink Autoscaler relies on these aggregated metrics to make predictions, and the type of aggregation plays an import role. [FLINK-30652] Use max busytime instead of average to compute true processing rate - ASF JIRA (apache.org) suggests that using max aggregator instead of avg of busy time can handle data skew more robustly. However, we found that for large-scale jobs, using max aggregation may be too sensitive. As a result, the true processing rate is underestimated with severe turbulence.

The graph below is the true processing rate estimated with different aggregators of a real production data transmission job with a parallelism of 750.

!image-2023-04-11-15-11-51-471.png!

*Proposal*

Add percentiles (p50, p90, p99) to aggregated metrics. Apache common maths can be used for computing that.

A follow up would be making Flink autoscaler make use of the new aggregators.

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/23 07:11;Zhanghao Chen;image-2023-04-11-15-11-51-471.png;https://issues.apache.org/jira/secure/attachment/13057186/image-2023-04-11-15-11-51-471.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 16:48:06 UTC 2023,,,,,,,,,,"0|z1h7d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 16:48;gyfora;I think this would be a great addition if we can add it with small overhead. Maybe make it configurable whether the user wants these or not;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Remove contender description from LeaderElectionDriverFactory interface,FLINK-31768,13532138,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,11/Apr/23 07:42,13/Apr/23 07:57,04/Jun/24 20:41,13/Apr/23 07:57,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 07:57:30 UTC 2023,,,,,,,,,,"0|z1h7co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 07:57;mapohl;master: 36145d4e3f035fe83a356d8c8463476c1004de65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve the implementation for ""analyze table"" execution on partitioned table",FLINK-31767,13532135,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfrey,godfrey,godfrey,11/Apr/23 06:43,13/Apr/23 04:04,04/Jun/24 20:41,13/Apr/23 04:04,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Currently, for partitioned table, the ""analyze table"" command will generate a separate SQL statement for each partition. When there are too many partitions, the compilation/submission/execution time will be very long. This issue aims to improve it: we can combine the sql statements for each partition into one with ""union all"", and just need to execution one sql.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 04:04:47 UTC 2023,,,,,,,,,,"0|z1h7c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 04:04;godfrey;Fixed in master: 0191cf7d0064003bd2d4b60ebb6e329f791c924a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Restoring from a retained checkpoint that was generated with changelog backend enabled might fail due to missing files,FLINK-31766,13532134,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,11/Apr/23 06:43,25/Apr/23 09:41,04/Jun/24 20:41,,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,,0,,,,,,"in FLINK-31593 we discovered a instability when generating the test data for {{StatefulJobSavepointMigrationITCase}} and {{StatefulJobWBroadcastStateMigrationITCase}}. It appears that files are deleted that shouldn't be deleted (see [~Yanfei Lei]'s [comment in FLINK-31593|https://issues.apache.org/jira/browse/FLINK-31593?focusedCommentId=17706679&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17706679]).

It's quite reproducible when generating the 1.17 test data for {{StatefulJobWBroadcastStateMigrationITCase}} and doing a test run to verify it.

I'm attaching the debug logs of such two runs that I generated for FLINK-31593 in this issue as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31765,,,,,,,,,,,,,FLINK-31593,,,"11/Apr/23 06:43;mapohl;FLINK-31593.StatefulJobSavepointMigrationITCase.create_snapshot.log;https://issues.apache.org/jira/secure/attachment/13057184/FLINK-31593.StatefulJobSavepointMigrationITCase.create_snapshot.log","11/Apr/23 06:44;mapohl;FLINK-31593.StatefulJobSavepointMigrationITCase.verify_snapshot.log;https://issues.apache.org/jira/secure/attachment/13057183/FLINK-31593.StatefulJobSavepointMigrationITCase.verify_snapshot.log",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 19 04:23:14 UTC 2023,,,,,,,,,,"0|z1h7bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 04:23;Yanfei Lei;After reproducing FLINK-31593  locally, I think the root cause is that `StatefulJobSavepointMigrationITCase` partially moves the snapshot files to a new [directory|https://github.com/apache/flink/blob/master/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L337-L342]. 

All state backends in `StatefulJobSavepointMigrationITCase`  are non-incremental, all files are placed in chk-x folder. But the files of changelog state backend are not completely placed under chk-x, some files are placed under taskowned folder, something like:
{code:java}
├── chk-2
│   ├── 5487d0fd-a361-4085-8ee0-7364ffd4511a
│   ├── _metadata
│   └── d3596cf7-3c6e-4081-b37b-f5a3e1a40086
├── shared
└── taskowned
    ├── 01aefc31-8ee1-41a8-9cd3-a94ccf85052f
    ├── 02bf09d3-73db-4c45-b6a1-15987659e3e6
    ├── 0c456b9b-9f90-4696-a2be-16e5938358ae {code}
This also explains why this issue didn't show up earlier:

1. If the version <= 1.15, changelog state backend is disabled.

2. If the version >= 1.16, change state backend is randomly turned on, when the changelog is turned off, this issue would not be triggered.

 

So I have two questions:
 # Whether the incremental rocksdb state backend should be tested here?
 # Do we need to change the move function to support testing of changelog state backend?

 [~roman] could you please help take a look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Disable changelog backend for the StatefulJobSavepointMigrationITCase and StatefulJobWBroadcastStateMigrationITCase with RocksDB and checkpoints,FLINK-31765,13532133,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,11/Apr/23 06:38,13/Apr/23 06:03,04/Jun/24 20:41,13/Apr/23 06:03,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,Tests,,,,,0,pull-request-available,test-stability,,,,"In FLINK-31593 we discovered an instability when generating the migration test data for 1.17 for {{StatefulJobSavepointMigrationITCase}} and {{StatefulJobWBroadcastStateMigrationITCase}}. According to the discussion in FLINK-31593, we concluded that it's caused by a non-determinism that's happening in the changelog backend code. As a workaround, we're going to disable the changelog backend in these tests for now.

We're not touching 1.16 because it didn't appear in that branch. The non-determinism seems to kick in only when generating the checkpoint files. 

For 1.17, we're gonna create a backport for consistency reasons because we have to enable it for the data generation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31766,,,,,,,,,,,,,,FLINK-31593,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 06:03:51 UTC 2023,,,,,,,,,,"0|z1h7bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 16:57;mapohl;After a [discussion|https://github.com/apache/flink/pull/22377/files#r1163828497] we had in the PR, we concluded that no backport is necessary.;;;","13/Apr/23 06:03;mapohl;master: a11a8b48b2090677f811f1d735f2a2f8ccaba54d;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Get rid of numberOfRequestedOverdraftMemorySegments in LocalBufferPool,FLINK-31764,13532127,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,11/Apr/23 05:46,07/May/23 03:14,04/Jun/24 20:41,06/May/23 02:41,1.16.1,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,"After FLINK-31763, we don't need the specific field {{numberOfRequestedOverdraftMemorySegments}} to record the overdraft buffers has been requested anymore since we regard all buffers exceeding the \{{currentPoolSize}} as overdraft.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31610,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 06 02:41:51 UTC 2023,,,,,,,,,,"0|z1h7a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/23 02:41;Weijie Guo;master(1.18) via 3379c2c085da43bb452536c981a7fc13f39482ee.
release-1.17 via 2edc003004d0d0f1e40ef2b7e14189965183f46c.
release-1.16 via 344c3b0896f64f86f8ec5eaadfa80caaea89a132.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Convert requested buffers to overdraft  buffers when pool size is decreased,FLINK-31763,13532124,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,11/Apr/23 05:38,17/Apr/23 09:29,04/Jun/24 20:41,17/Apr/23 09:29,1.16.1,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,"As we discussed in FLINK-31610, new buffers can be requested only when ""{_}numOfRequestedMemorySegments + numberOfRequestedOverdraftMemorySegments < poolSize + maxOverdraftBuffersPerGate""{_}.

Consider such a scenario, the {{{}CurrentPoolSize = 5{}}}, {{{}numOfRequestedMemorySegments = 7{}}}, {{{}maxOverdraftBuffersPerGate = 2{}}}. If {{{}numberOfRequestedOverdraftMemorySegments = 0{}}}, then 2 buffers can be requested now. 

We should convert {{numberOfRequestedMemorySegments}} to {{numberOfRequestedOverdraftMemorySegments}} when poolSize is decreased. Further more, we can changes the definition of overdraft buffer from static to dynamic: 
 * When _numberOfRequestedMemorySegments <= poolSize,_ all buffers are ordinary buffer
 * When _numberOfRequestedMemorySegments > poolSize,_ the `{_}ordinary buffer size = poolSize`{_}, and `{_}the overdraft buffer size = numberOfRequestedMemorySegments - poolSize`{_}

This allows us to remove {{{}numberOfRequestedOverdraftMemorySegments{}}}, which helps us simplify logic and maintain consistency.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31610,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 09:29:07 UTC 2023,,,,,,,,,,"0|z1h79k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 09:29;Weijie Guo;master(1.18) via ffc6f3bfabd22b49b08f027400c194a8e7c9c51a.
release-1.17 via be0f9293c2ce00465154ef03f7cef29dd3116b8e.
release-1.16 via ab2ba9612a6a7f27c2921c7c329b3077ea4c33b2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Subscribe to multiple Kafka topics may cause partition assignment skew,FLINK-31762,13532102,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liam8,liam8,11/Apr/23 00:19,30/May/23 15:42,04/Jun/24 20:41,,1.13.0,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,1,,,,,,"To simplify the demonstration, let us assume that there are two topics, and each topic has four partitions. We have set the parallelism to eight to consume these two topics. However, the current partition assignment method may lead to some subtasks being assigned two partitions while others are left with none.

!image-2023-04-11-08-00-16-054.png|width=500,height=143!

In my case, the situation is even worse as I have ten topics, each with 100 partitions. If I set the parallelism to 1000, some slots may be assigned seven partitions while others remain unassigned.

To address this issue, I propose a new partition assignment solution. In this approach, round-robin assignment takes place between all topics, not just one.

For example, the ideal assignment for the case mentioned above is presented below:

 

!https://imgr.whimsical.com/object/A4jSJwgQNrc5mgpGddhghq|width=513,height=134!

This new solution can also handle cases where each topic has more partitions.

!image-2023-04-11-08-12-24-115.png|width=444,height=127!

Let us work together to reach a consensus on this proposal. Thank you!

 

FYI: how the partition be assigned currently
{code:java}
public class KafkaTopicPartitionAssigner {    
    public static int assign(KafkaTopicPartition partition, int numParallelSubtasks) {
        return assign(partition.getTopic(), partition.getPartition(), numParallelSubtasks);
    }    public static int assign(String topic, int partition, int numParallelSubtasks) {
        int startIndex = ((topic.hashCode() * 31) & 0x7FFFFFFF) % numParallelSubtasks;        // here, the assumption is that the id of Kafka partitions are always ascending
        // starting from 0, and therefore can be used directly as the offset clockwise from the
        // start index
        return (startIndex + partition) % numParallelSubtasks;
    }
 {code}
for Kafka Source, it's implemented in the KafkaSourceEnumerator as below
{code:java}
    static int getSplitOwner(TopicPartition tp, int numReaders) {
        int startIndex = ((tp.topic().hashCode() * 31) & 0x7FFFFFFF) % numReaders;        // here, the assumption is that the id of Kafka partitions are always ascending
        // starting from 0, and therefore can be used directly as the offset clockwise from the
        // start index
        return (startIndex + tp.partition()) % numReaders;
    } {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/23 00:00;liam8;image-2023-04-11-08-00-16-054.png;https://issues.apache.org/jira/secure/attachment/13057178/image-2023-04-11-08-00-16-054.png","11/Apr/23 00:12;liam8;image-2023-04-11-08-12-24-115.png;https://issues.apache.org/jira/secure/attachment/13057177/image-2023-04-11-08-12-24-115.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 30 15:42:12 UTC 2023,,,,,,,,,,"0|z1h74o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 12:57;martijnvisser;[~tzulitai] WDYT?;;;","15/May/23 02:34;liam8;Hi master [~martijnvisser] [~tzulitai] any comment?;;;","30/May/23 15:42;tzulitai;Thanks for opening the ticket [~liam8]. I think your analysis is correct, and the distribution will indeed be sub-optimal when multiple topics are being read. Especially if these topics have varying number of partitions (probably likely the case).

Quick intuition is to first sort the partitions, and then round-robin assign the partitions starting from subtask 0. Whenever we discover new partitions (if continuous partition discovery is enabled), we continue assigning subtasks starting from the last assignment.

We should double check if this would break anything in terms of state restores though. With the new {{KafkaSource}} on top of source V2, it shouldn't break anything as partition state restore is decoupled from the discovery and assignments which happen on the split enumerator.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix Some Typo And Improve lambda expressions.,FLINK-31761,13532097,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slfan1989,slfan1989,slfan1989,10/Apr/23 22:14,13/Apr/23 14:16,04/Jun/24 20:41,13/Apr/23 14:08,1.17.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,,,"When reading the code, I found 2 typo errors and improved the lambada expression.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 14:16:54 UTC 2023,,,,,,,,,,"0|z1h73k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 14:52;martijnvisser;[~slfan1989] Next time, as outlined on https://flink.apache.org/how-to-contribute/contribute-code/, you don't need to create a Jira for this. 

| ""Note: trivial hot fixes such as typos or syntax errors can be opened as a [hotfix] pull request, without a Jira ticket."";;;","13/Apr/23 01:25;slfan1989;[~martijnvisser] Thanks for your message!;;;","13/Apr/23 08:12;martijnvisser;[~fanrui] Since you've merged the PR, did you deliberately not close this Jira ticket yet, or did you just miss it? ;;;","13/Apr/23 14:08;fanrui;Hi [~martijnvisser] , sorry for the late update.

Merged master commit: 0c6e51ffe3db98b2fa2a57cf7ee731b3f6885a6d;;;","13/Apr/23 14:16;martijnvisser;[~fanrui] No worries, I would have closed it for you but didn't know if there was still anything open :);;;",,,,,,,,,,,,,,,,,,,,,,
COALESCE() with NULL arguments throws error,FLINK-31760,13532081,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,morezaei00,morezaei00,10/Apr/23 18:46,11/Apr/23 15:30,04/Jun/24 20:41,,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"All arguments may not be nullable:

{code}
SELECT COALESCE(NULL, NULL)  FROM UnnamedTable$0
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 17 to line 1, column 20: Illegal use of 'NULL'
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:186)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:703)
	at CoalesceTest.main(CoalesceTest.java:58)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 17 to line 1, column 20: Illegal use of 'NULL'
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4867)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1837)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.inferUnknownTypes(SqlValidatorImpl.java:1912)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:419)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4061)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3347)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:182)
	... 5 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Illegal use of 'NULL'
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
	... 21 more
{code}

As [documented|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/systemfunctions/#conditional-functions], supports all nullable arguments:

{quote}
Returns the first argument that is not NULL.

If all arguments are NULL, it returns NULL as well. The return type is the least restrictive, common type of all of its arguments. The return type is nullable if all arguments are nullable as well.
{quote}

Spark SQL supports as documented:

{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.2
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.16)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.sql(""select coalesce(null, null)"").show
+--------------------+
|coalesce(NULL, NULL)|
+--------------------+
|                null|
+--------------------+
{code}

Since this is coming back from Calcite, do we want to support all nullable arguments? If no, then this will become a simple documentation fix.",Flink 1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 11 15:30:01 UTC 2023,,,,,,,,,,"0|z1h700:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 03:03;taoran;[~morezaei00]  Hi, Mohsen, I think it's a bug, coalesce need return null if all args are null. spark sql is a good example, and we can see mysql/sql server and other mature engines support return null.
would you like fix it?  Or i will try to fix it.;;;","11/Apr/23 06:29;lincoln.86xy;Currently it's by-design behavior because we didn't enable the type coercion for NULL literals([FLINK-17484|https://issues.apache.org/jira/browse/FLINK-17484).
All NULL literals should declare an explict type for now, e.g.,`select coalesce(cast(null as varchar), cast(null as varchar))`
can work. Btw, the current exception message can be improved.;;;","11/Apr/23 06:37;taoran;thanks lee for explanations. cast works well.;;;","11/Apr/23 15:30;morezaei00;[~lincoln.86xy], if I understand correctly, the error message is a generic SQL validation thrown for similar uses in the SQL, e.g. [{{testValuesWithoutTypeCoercion}}|https://github.com/mohsenrezaeithe/flink/blob/7dd42cf22e85ce2e9dbbde5210edd65bcb94f459/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/stream/sql/NullTypeTest.java#L47-L47].

Are you suggesting to improve the error at the root, or wrap it somewhere in the stack and add a better message?

I'll also go ahead and add the {{CAST(NULL AS VARCHAR)}} note in the docs for clarity.;;;",,,,,,,,,,,,,,,,,,,,,,,
Update some external connectors to use sql_connector_download_table and connector_artifact shortcode,FLINK-31759,13532062,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,10/Apr/23 15:22,24/May/23 14:55,04/Jun/24 20:41,16/May/23 06:22,,,,,,,,,,,,,,,,,,,,,jdbc-3.1.1,jdbc-3.2.0,,,,,,,,Connectors / ElasticSearch,Connectors / Google Cloud PubSub,Connectors / HBase,Connectors / JDBC,Connectors/ RabbitMQ,,0,pull-request-available,,,,,"# FLINK-30291 introduce a new shortcode({{{}sql_connector_download_table{}}}) for externalized connectors table document. Som externalized connectors does not use it. Including: elasticsearch, hbase, jdbc.
 # Some externalized connectors does not use {{connector_artifact}} shortcode for datastream document. Including: jdbc, rabbitmq, gcp-pubsub.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 16 06:22:00 UTC 2023,,,,,,,,,,"0|z1h6vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 15:07;Weijie Guo;flink-connector-elasticsearch:
main via 8f75d4e059c09b55cc3a44bab3e64330b1246d27. 
v3.0 via 1d7559ee8acc5ac952d92634f9c723f543ec9074.

flink-connector-jdbc
main via b6c3cb23aa566791c3123b5e46a8a88979acdc73.
v3.0 via b329ae8397b7b3b4e77eb1293c7243c485cb6a69.

flink-connector-rabbitmq
main via b6c3cb23aa566791c3123b5e46a8a88979acdc73.
v3.0 via 223f18f2c3b4ea82f86c8bc765c693740be2bc01.

flink-connctor-hbase
main via 80261498ffc5856496ec276fbaf1dcfc473475b4.
v3.0 via e0f60b287b11d84bdc07791f7ab95a7c29f62217.

flink-connector-gcp-pubsub
main via 18e4dbc4d45386000b7f82daa7c53d84bda3d5e2.
v3.0 via fd4f6f67647c9ffa6cb2b89eccb0644f4cfca91b.;;;","04/May/23 03:48;leonard;JDBC connector has not sql jar, the link is wrong, reopen this issue to fix


https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/jdbc/;;;","04/May/23 04:10;Weijie Guo;Thanks [~leonard], good catch. 

Maybe we only need to modify the sql_url in jdbc.yml.;;;","16/May/23 06:22;Weijie Guo;Fix the incorrect sql_url in jdbc.yml:
main via 94e1d17738dd2e2820621160895af741d031364e.
v3.0 via d1befa1407b7ab01801df340a8c2468ca069530e.
;;;",,,,,,,,,,,,,,,,,,,,,,,
Some external connectors sql client jar has a wrong download url in document,FLINK-31758,13532057,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,10/Apr/23 14:59,12/Apr/23 10:11,04/Jun/24 20:41,12/Apr/23 10:11,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Connectors / Common,Documentation,,,,,0,pull-request-available,,,,,"After FLINK-30378, we can load sql connector data from external connector's own data file. However, we did not replace \{{$full_version}}, resulting in an incorrect URL in the download link. for example: {{https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-mongodb/$full_version/flink-sql-connector-mongodb-$full_version.jar.}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 10:11:12 UTC 2023,,,,,,,,,,"0|z1h6uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 10:11;Weijie Guo;master(1.18) via 91e405dd285ecb62312254b4b906dac4fcdfa4de.
release-1.17 via d00bc40a33426bf0e336fc675447237e472da594.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-370: Support Balanced Tasks Scheduling,FLINK-31757,13532014,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,10/Apr/23 02:56,30/Oct/23 15:02,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"This is an umbrella JIRA of [FLIP-370|https://cwiki.apache.org/confluence/x/U56zDw].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/23 15:04;fanrui;image-2023-04-13-08-04-04-667.png;https://issues.apache.org/jira/secure/attachment/13057256/image-2023-04-13-08-04-04-667.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 01 22:35:07 UTC 2023,,,,,,,,,,"0|z1h6l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/23 03:02;Weijie Guo;Hi [~RocMarshal], Can you add more details to this ticket? I don't really know what unbalanced scheduling is.;;;","10/Apr/23 03:06;RocMarshal;[~Weijie Guo] Glad to get your attention and reminding~:)

I'll add the background and cases description later.
Looking forward your discussion after that~
Thanks a lot.;;;","12/Apr/23 14:49;martijnvisser;[~RocMarshal] When the ticket is updated, I can re-open it again. Right now, it's an invalid ticket;;;","13/Apr/23 09:23;RocMarshal;h1. Problem description and impact
h1. The case

Supposed a Job has 21 tasks:
 * Task A has the parallelism of 100,
 * The every remained task has the parallelism of 5.

Each TM slot = 1, so the tasks in the job need to apply for 100 TMs.
h2. Problem Description

Assuming that the TM number is 0-99, from the perspective of Task, the actual result after scheduling is:

After the job deployed. There are 5 TMs loading with 21 sub-tasks, while other TMs only load a sub-task.
h2. Influence

If the user allocates resources to TM: All TM resources are applied according to the 5 TMs (loading 21-subtasks), then subsequent TM resources will be wasted. If apply the resources based on other TM(only loading a subtask), the 5 TMs resources are insufficient, tasks running on its may have lag.

 

From the perspective of resource usage, we expect all subtasks to be evenly distributed on each TM.;;;","13/Apr/23 11:32;huwh;Hi [~RocMarshal] , thanks for reporting this.

For now, you could set different slot sharing group for tasks to prevent too may tasks in same slots


But, we, in bytedance, also face this unbalanced issue.

And we did some work on our internal custom flink.
We did two things to spread the tasks evenly.
 # enrich LocalInputPreferredSlotSharingStrategySplit to assign tasks to SlotSharingGroup evenly. 
I think that could solve the problem you described. In this case, 200 tasks will be assigned to 100 slots, and each slot should have 2 tasks.
 # Schedule tasks (slots) to TaskManager evenly. This is for slots with different task numbers, such as we have 10 source and 20 sink operators, so we need 20 slots, and there are 10 slots with 2 sub-tasks and 10 slots with 1 sub-task. If we have 10 task managers with 20 slots in total, we should schedule a slot with 2 sub-tasks, and a slot with 1 sub-task, for one task manager.

Do you think this would be valuable to the flink community? I would be glad to contribute it. [~Weijie Guo] [~martijnvisser] 

 

 ;;;","13/Apr/23 13:10;RocMarshal;Based on this problem, we has achieved balanced task distribution on TaskManager. I would very much like to be able to contribute it. 
Would you [~huwh]  like to contribute it together ?  ;;;","13/Apr/23 13:21;huwh;[~RocMarshal] sure, I would like to. Will you prepare a design document?;;;","13/Apr/23 13:36;fanrui;Thanks [~RocMarshal] 's reporting and [~huwh] 's feedback.
{quote}If the user allocates resources to TM: All TM resources are applied according to the 5 TMs (loading 21-subtasks), then subsequent TM resources will be wasted. If apply the resources based on other TM(only loading a subtask), the 5 TMs resources are insufficient, tasks running on its may have lag.
{quote}
From the information, flink users have 2 options:
 # Set different slot sharing group for tasks.
 # Set the TM resources according to the high load TM to ensure the performance.

Option 1 is not friendly to flink users, and flink sql doesn't support set slot sharing group.

Option 2 will waste some TM resources.

As I understand, the balance of the number of tasks on the TM can make the actual resource usage of all TMs closer, it should be valuable for flink users and flink community from my side.

Please go ahead and prepare a detailed design doc first, thanks.:)

 ;;;","13/Apr/23 13:46;Weijie Guo;[~RocMarshal] I have updated the description of this ticket based on my understanding.If it is not what you want to express, feel free to modify it.:);;;","13/Apr/23 13:49;RocMarshal;Thank you  [~fanrui] & [~huwh] very much.
I'll prepare a design document for discussion.;;;","13/Apr/23 13:52;RocMarshal;Thanks for your [~Weijie Guo] sorted concise and precise description. There's nothing better.(y);;;","13/Apr/23 14:38;chesnay;This scenario can only occur if the user explicitly configured a parallelism of 100 for vertex A and 5 for the remaining vertices.

The obvious solution for the user is to set the parallelism to 100 for everything if the describe issues are a problem.

In the proposed scenarios of 100 TMs with 1 slot each this is literally the only option. Just deciding to use p=100 for all vertices ignores an explicit configuration by the user (bad!), as would mucking around with slot sharing groups (which would require more than 100 slots).;;;","13/Apr/23 14:57;fanrui;Hi [~chesnay] , thanks for your reply.
{quote}The obvious solution for the user is to set the parallelism to 100 for everything if the describe issues are a problem.
{quote}
In some scenarios, setting all parallelism globally will waste resources or setting low parallelism for some tasks is a good choice. For example, flink job has too many sources, each source has only 5 partitions. So setting parallelism to 5 for each source is enough.

Or the business logic is very complex, the flink job has dozens of tasks, and the user sets a reasonable parallelism according to the busy ratio of the tasks (similar to FLIP-AutoScalar).

In general, it is a common scenario that the parallelism of multiple tasks is different. For this scenario, it is unreasonable for resource balance that the front TM runs a large number of tasks and the subsequent TMs run a small number of tasks.

 

This is a Flink job DAG in our production, it's too complex. Setting all parallelism globally will cause some problems:
 * Need too many network memory
 * JM schedules more tasks and starts jobs slower
 * Create too many task threads

!image-2023-04-13-08-04-04-667.png!;;;","14/Apr/23 03:13;huwh;I would like to bring in a common scenario that can't set tasks in the same parallelism.


Some ETL pipeline jobs consume Kafka data, and then do some heavy transformation in Map operation. In this scenario, we can't set all parallelism globally because of Kafka partition number limitations. ;;;","24/Apr/23 05:07;RocMarshal;I am still working on this jira, and due to the Labor Day holiday, I will provide a design draft as soon as possible after it.
Thanks a lot.;;;","29/May/23 08:25;RocMarshal;I have compiled a draft  [https://docs.google.com/document/d/14WhrSNGBdcsRl3IK7CZO-RaZ5KXU2X1dWqxPEFr3iS8/edit?usp=sharing]
Looking forward   your discussion.  [~fanrui] [~huwh] [~Weijie Guo] [~chesnay] 
Thank you.;;;","09/Jun/23 08:42;fanrui;Hi [~xtsong] [~zhuzh] [~wanglijie] , you may also be interested in this JIRA and look forward to your feedback, thanks~

BTW, I'm not sure whether it should be discussed as a FLIP, because:
 * It may add the public option.
 * It includes 2 parts, the task to slot and slot to TM, it may be done in multiple subtasks.;;;","28/Jul/23 03:29;heigebupahei;Is there any progress on this topic? We are also facing this problem now, which is quite a headache.
 ;;;","02/Aug/23 03:53;RocMarshal;Hi, [~heigebupahei] Thanks for your attention. we've updated the new edition design docs in [https://docs.google.com/document/d/14WhrSNGBdcsRl3IK7CZO-RaZ5KXU2X1dWqxPEFr3iS8/edit?usp=sharing]

Would you mind having a look on the document ? Any suggestion is appreciated~;;;","01/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,
KafkaTableITCase.testStartFromGroupOffsetsNone fails due to UnknownTopicOrPartitionException,FLINK-31756,13531972,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,Sergey Nuyanzin,Sergey Nuyanzin,09/Apr/23 05:36,16/Oct/23 12:36,04/Jun/24 20:41,16/Oct/23 12:36,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,auto-deprioritized-critical,test-stability,,,,"The following build fails with {{UnknownTopicOrPartitionException}}
{noformat}
Dec 03 01:10:59 Multiple Failures (1 failure)
Dec 03 01:10:59 -- failure 1 --
Dec 03 01:10:59 [Any cause is instance of class 'class org.apache.kafka.clients.consumer.NoOffsetForPartitionException'] 
Dec 03 01:10:59 Expecting any element of:
Dec 03 01:10:59   [java.lang.IllegalStateException: Fail to create topic [groupOffset_json_dc640086-d1f1-48b8-ad7a-f83d33b6a03c partitions: 4 replication factor: 1].
Dec 03 01:10:59 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:143)
Dec 03 01:10:59 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.startFromGroupOffset(KafkaTableITCase.java:881)
Dec 03 01:10:59 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testStartFromGroupOffsetsWithNoneResetStrategy(KafkaTableITCase.java:981)
Dec 03 01:10:59 	...(64 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
Dec 03 01:10:59     java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: The request timed out.
Dec 03 01:10:59 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
Dec 03 01:10:59 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
Dec 03 01:10:59 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Dec 03 01:10:59 	...(67 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
Dec 03 01:10:59     org.apache.kafka.common.errors.TimeoutException: The request timed out.
Dec 03 01:10:59 ]
Dec 03 01:10:59 to satisfy the given assertions requirements but none did:
Dec 03 01:10:59 
Dec 03 01:10:59 java.lang.IllegalStateException: Fail to create topic [groupOffset_json_dc640086-d1f1-48b8-ad7a-f83d33b6a03c partitions: 4 replication factor: 1].
Dec 03 01:10:59 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:143)
Dec 03 01:10:59 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.startFromGroupOffset(KafkaTableITCase.java:881)
Dec 03 01:10:59 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testStartFromGroupOffsetsWithNoneResetStrategy(KafkaTableITCase.java:981)
Dec 03 01:10:59 	...(64 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
Dec 03 01:10:59 error: 
Dec 03 01:10:59 Expecting actual throwable to be an instance of:
Dec 03 01:10:59   org.apache.kafka.clients.consumer.NoOffsetForPartitionException
Dec 03 01:10:59 but was:
Dec 03 01:10:59   java.lang.IllegalStateException: Fail to create topic [groupOffset_json_dc640086-d1f1-48b8-ad7a-f83d33b6a03c partitions: 4 replication factor: 1].
[...]

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47892&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36657",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30298,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:11 UTC 2023,,,,,,,,,,"0|z1h6bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/23 05:37;Sergey Nuyanzin;similar to https://issues.apache.org/jira/browse/FLINK-30298;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
ROW function can not work with RewriteIntersectAllRule,FLINK-31755,13531866,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,aitozi,aitozi,07/Apr/23 13:05,18/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,auto-deprioritized-major,pull-request-available,,,,"Reproduce case:


{code:java}
create table row_sink (
  `b` ROW<b1 STRING, b2 INT>
) with (
  'connector' = 'values'
)

util.verifyRelPlanInsert(
""INSERT INTO row_sink "" +
  ""SELECT ROW(a, b) FROM complex_type_src intersect all "" +
  ""SELECT ROW(c, d) FROM complex_type_src "")

{code}

It will fails with 


{code:java}
Caused by: java.lang.IllegalArgumentException: Type mismatch:
rel rowtype: RecordType(RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" EXPR$0, INTEGER EXPR$1) NOT NULL EXPR$0) NOT NULL
equiv rowtype: RecordType(RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" EXPR$0, INTEGER EXPR$1) NOT NULL EXPR$0) NOT NULL
Difference:
EXPR$0: RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" EXPR$0, INTEGER EXPR$1) NOT NULL -> RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" EXPR$0, INTEGER EXPR$1) NOT NULL

	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:592)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:613)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:144)
	... 68 more
{code}


The reason is:

ROW function will generates the {{FULLY_QUALIFIED}} type. But after the {{RewriteIntersectAllRule}} optimization, it will produce the {{PEEK_FIELDS_NO_EXPAND}}. So the volcano planner complains with type mismatch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:08 UTC 2023,,,,,,,,,,"0|z1h5o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 03:27;aitozi;CC [~lincoln.86xy] [~snuyanzin] ;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
Build flink master error with Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: 19,FLINK-31754,13531856,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,dragonpic1988,dragonpic1988,07/Apr/23 11:33,28/Aug/23 11:31,04/Jun/24 20:41,17/Apr/23 08:51,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"maven 3.25

jdk 1.8

scala 2.12

window 10

[E:\Project\flink\flink\flink-table\flink-table-planner]$ mvn package -DskipTests -e

 
{code}
[INFO] Error stacktraces are turned on.[INFO] Scanning for projects...[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.flink:flink-table-planner_2.12:jar:1.18-SNAPSHOT
[WARNING] 'artifactId' contains an expression but should be a constant. @ org.apache.flink:flink-table-planner_${scala.binary.version}:[unknown-version], E:\Project\flink\flink\flink-table\flink-table-planner\pom.xml, line 29, column 14
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Flink : Table : Planner 1.18-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] — maven-checkstyle-plugin:3.1.2:check (validate) @ flink-table-planner_2.12 —
[WARNING] Old version of checkstyle detected. Consider updating to >= v8.30
[WARNING] For more information see: [https://maven.apache.org/plugins/maven-checkstyle-plugin/examples/upgrading-checkstyle.html]
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] — spotless-maven-plugin:2.27.1:check (spotless-check) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-enforcer-plugin:3.1.0:enforce (enforce-maven-version) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-enforcer-plugin:3.1.0:enforce (enforce-maven) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-enforcer-plugin:3.1.0:enforce (ban-unsafe-snakeyaml) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-enforcer-plugin:3.1.0:enforce (ban-unsafe-jackson) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-enforcer-plugin:3.1.0:enforce (forbid-log4j-1) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-enforcer-plugin:3.1.0:enforce (forbid-direct-akka-rpc-dependencies) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-enforcer-plugin:3.1.0:enforce (forbid-direct-table-planner-dependencies) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-enforcer-plugin:3.1.0:enforce (enforce-versions) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — directory-maven-plugin:0.1:directory-of (directories) @ flink-table-planner_2.12 —
[INFO] Directory of org.apache.flink:flink-parent set to: E:\Project\flink\flink
[INFO] 
[INFO] — maven-remote-resources-plugin:1.5:process (process-resource-bundles) @ flink-table-planner_2.12 —
[INFO] 
[INFO] — maven-resources-plugin:3.1.0:resources (default-resources) @ flink-table-planner_2.12 —
[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] Copying 1 resource[INFO] Copying 3 resources[INFO] 
[INFO] — scala-maven-plugin:3.2.2:add-source (scala-compile-first) @ flink-table-planner_2.12 —
[INFO] Add Source directory: E:\Project\flink\flink\flink-table\flink-table-planner\src\main\scala
[INFO] Add Test Source directory: E:\Project\flink\flink\flink-table\flink-table-planner\src\test\scala
[INFO] 
[INFO] — scala-maven-plugin:3.2.2:compile (scala-compile-first) @ flink-table-planner_2.12 —
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] — maven-compiler-plugin:3.8.0:compile (default-compile) @ flink-table-planner_2.12 —
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] — maven-resources-plugin:3.1.0:testResources (default-testResources) @ flink-table-planner_2.12 —
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 371 resources[INFO] Copying 3 resources[INFO] 
[INFO] — scala-maven-plugin:3.2.2:testCompile (scala-test-compile) @ flink-table-planner_2.12 —
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] — maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ flink-table-planner_2.12 —
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] — maven-surefire-plugin:3.0.0-M5:test (default-test) @ flink-table-planner_2.12 —
[INFO] Tests are skipped.
[INFO] 
[INFO] — maven-jar-plugin:2.4:jar (default-jar) @ flink-table-planner_2.12 —
[INFO] Building jar: E:\Project\flink\flink\flink-table\flink-table-planner\target\flink-table-planner_2.12-1.18-SNAPSHOT.jar[INFO] 
[INFO] — maven-jar-plugin:2.4:test-jar (default) @ flink-table-planner_2.12 —
[INFO] Building jar: E:\Project\flink\flink\flink-table\flink-table-planner\target\flink-table-planner_2.12-1.18-SNAPSHOT-tests.jar
[INFO] 
[INFO] — maven-shade-plugin:3.4.1:shade (shade-flink) @ flink-table-planner_2.12 —
[INFO] Including com.google.guava:guava:jar:29.0-jre in the shaded jar.
[INFO] Including com.google.guava:failureaccess:jar:1.0.1 in the shaded jar.
[INFO] Including org.apache.flink:flink-sql-parser:jar:1.18-SNAPSHOT in the shaded jar.
[INFO] Including org.checkerframework:checker-qual:jar:3.10.0 in the shaded jar.
[INFO] Including org.apache.calcite:calcite-core:jar:1.29.0 in the shaded jar.
[INFO] Including org.apache.calcite:calcite-linq4j:jar:1.29.0 in the shaded jar.
[INFO] Including com.esri.geometry:esri-geometry-api:jar:2.2.0 in the shaded jar.
[INFO] Including org.apache.calcite.avatica:avatica-core:jar:1.20.0 in the shaded jar.
[INFO] Including commons-codec:commons-codec:jar:1.15 in the shaded jar.
[INFO] Including commons-io:commons-io:jar:2.11.0 in the shaded jar.
[INFO] Including org.apache.flink:flink-shaded-force-shading:jar:16.1 in the shaded jar.
[INFO] Excluding com.google.guava:listenablefuture:jar:9999.0-empty-to-avoid-conflict-with-guava from the shaded jar.
[INFO] Excluding com.google.errorprone:error_prone_annotations:jar:2.3.4 from the shaded jar.
[INFO] Excluding com.google.j2objc:j2objc-annotations:jar:1.3 from the shaded jar.
[INFO] Excluding org.immutables:value:jar:2.8.8 from the shaded jar.
[INFO] Excluding org.immutables:value-annotations:jar:2.8.8 from the shaded jar.
[INFO] Excluding org.codehaus.janino:commons-compiler:jar:3.1.9 from the shaded jar.
[INFO] Excluding org.codehaus.janino:janino:jar:3.1.9 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-table-api-java-bridge:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-table-api-java:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-table-api-bridge-base:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-java:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding com.twitter:chill-java:jar:0.7.6 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-streaming-java:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-scala_2.12:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-core:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-asm-9:jar:9.3-16.1 from the shaded jar.
[INFO] Excluding org.scala-lang:scala-reflect:jar:2.12.7 from the shaded jar.
[INFO] Excluding org.scala-lang:scala-library:jar:2.12.7 from the shaded jar.
[INFO] Excluding org.scala-lang:scala-compiler:jar:2.12.7 from the shaded jar.
[INFO] Excluding org.scala-lang.modules:scala-xml_2.12:jar:1.0.6 from the shaded jar.
[INFO] Excluding com.twitter:chill_2.12:jar:0.7.6 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-annotations:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apiguardian:apiguardian-api:jar:1.1.2 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-table-runtime:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-table-common:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-cep:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-annotations:jar:2.13.4 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-core:jar:2.13.4 from the shaded jar.
[INFO] Excluding com.fasterxml.jackson.core:jackson-databind:jar:2.13.4.2 from the shaded jar.
[INFO] Excluding com.jayway.jsonpath:json-path:jar:2.4.0 from the shaded jar.
[INFO] Excluding net.minidev:json-smart:jar:2.3 from the shaded jar.
[INFO] Excluding net.minidev:accessors-smart:jar:1.2 from the shaded jar.
[INFO] Excluding org.ow2.asm:asm:jar:5.0.4 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-runtime:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-rpc-core:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-rpc-akka-loader:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-queryable-state-client-java:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-hadoop-fs:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-netty:jar:4.1.82.Final-16.1 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-zookeeper-3:jar:3.7.1-16.1 from the shaded jar.
[INFO] Excluding commons-cli:commons-cli:jar:1.5.0 from the shaded jar.
[INFO] Excluding org.javassist:javassist:jar:3.24.0-GA from the shaded jar.
[INFO] Excluding org.xerial.snappy:snappy-java:jar:1.1.8.3 from the shaded jar.
[INFO] Excluding org.lz4:lz4-java:jar:1.8.0 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-metrics-core:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-jackson:jar:2.13.4-16.1 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-lang3:jar:3.12.0 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-text:jar:1.10.0 from the shaded jar.
[INFO] Excluding com.esotericsoftware.kryo:kryo:jar:2.24.0 from the shaded jar.
[INFO] Excluding com.esotericsoftware.minlog:minlog:jar:1.2 from the shaded jar.
[INFO] Excluding commons-collections:commons-collections:jar:3.2.2 from the shaded jar.
[INFO] Excluding org.apache.commons:commons-compress:jar:1.21 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-shaded-guava:jar:30.1.1-jre-16.1 from the shaded jar.
[INFO] Excluding com.ibm.icu:icu4j:jar:67.1 from the shaded jar.
[INFO] Excluding org.apache.flink:flink-file-sink-common:jar:1.18-SNAPSHOT from the shaded jar.
[INFO] Excluding org.apache.commons:commons-math3:jar:3.6.1 from the shaded jar.
[INFO] Excluding org.slf4j:slf4j-api:jar:1.7.36 from the shaded jar.
[INFO] Excluding com.google.code.findbugs:jsr305:jar:1.3.9 from the shaded jar.
[INFO] Excluding org.objenesis:objenesis:jar:2.1 from the shaded jar.
[INFO] No artifact matching filter io.netty:netty
[INFO] Dependency-reduced POM written at: E:\Project\flink\flink\flink-table\flink-table-planner\dependency-reduced-pom.xml
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:17 min
[INFO] Finished at: 2023-04-07T19:32:35+08:00
[INFO] Final Memory: 59M/1517M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.4.1:shade (shade-flink) on project flink-table-planner_2.12: Error creating shaded jar: Problem shading JAR E:\Project\flink\flink\flink-table\flink-table-planner\target\flink-table-planner_2.12-1.18-SNAPSHOT.jar entry org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: 19 -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.4.1:shade (shade-flink) on project flink-table-planner_2.12: Error creating shaded jar: Problem shading JAR E:\Project\flink\flink\flink-table\flink-table-planner\target\flink-table-planner_2.12-1.18-SNAPSHOT.jar entry org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: Error creating shaded jar: Problem shading JAR E:\Project\flink\flink\flink-table\flink-table-planner\target\flink-table-planner_2.12-1.18-SNAPSHOT.jar entry org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class
    at org.apache.maven.plugins.shade.mojo.ShadeMojo.execute(ShadeMojo.java:640)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
    ... 19 more
Caused by: java.io.IOException: Problem shading JAR E:\Project\flink\flink\flink-table\flink-table-planner\target\flink-table-planner_2.12-1.18-SNAPSHOT.jar entry org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class
    at org.apache.maven.plugins.shade.DefaultShader.shadeJars(DefaultShader.java:277)
    at org.apache.maven.plugins.shade.DefaultShader.shade(DefaultShader.java:128)
    at org.apache.maven.plugins.shade.mojo.ShadeMojo.execute(ShadeMojo.java:500)
    ... 21 more
Caused by: org.apache.maven.plugin.MojoExecutionException: Error in ASM processing class org/apache/calcite/sql/validate/SqlValidatorImpl$NavigationExpander.class
    at org.apache.maven.plugins.shade.DefaultShader.addRemappedClass(DefaultShader.java:564)
    at org.apache.maven.plugins.shade.DefaultShader.shadeJarEntry(DefaultShader.java:310)
    at org.apache.maven.plugins.shade.DefaultShader.shadeJars(DefaultShader.java:272)
    ... 23 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 19
    at org.objectweb.asm.ClassReader.readLabel(ClassReader.java:2679)
    at org.objectweb.asm.ClassReader.createLabel(ClassReader.java:2695)
    at org.objectweb.asm.ClassReader.readTypeAnnotations(ClassReader.java:2760)
    at org.objectweb.asm.ClassReader.readCode(ClassReader.java:1928)
    at org.objectweb.asm.ClassReader.readMethod(ClassReader.java:1514)
    at org.objectweb.asm.ClassReader.accept(ClassReader.java:744)
    at org.objectweb.asm.ClassReader.accept(ClassReader.java:424)
    at org.apache.maven.plugins.shade.DefaultShader.addRemappedClass(DefaultShader.java:560)
    ... 25 more
[ERROR] 
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] [http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException]
{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 28 11:31:15 UTC 2023,,,,,,,,,,"0|z1h5m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 12:44;martijnvisser;It looks like you're not using a Unix-like environment (like Cygwin or WSL) to compile, I'm suspecting that's causing issues. ;;;","14/Apr/23 03:28;Small Wong;[~martijnvisser] hi, I met the same problem in Linux env.;;;","14/Apr/23 07:36;martijnvisser;[~Small Wong] Which version of Scala are you using and are you absolutely sure that you're using JDK 8 or JDK 11 Temurin to compile? Because if Flink was broken, the CI would also be broken and that has not been the case, so this must be a local issue;;;","17/Apr/23 08:35;Small Wong;[~martijnvisser]  I build & deploy Flink by gitlab CI/CD with image maven:3.5.0-jdk-8 in gitlab-ci.yaml. And the pipeline is always failed with the exception above.;;;","17/Apr/23 08:51;martijnvisser;Maven 3.5.0 is a different version then the 3.2.5 that you've mentioned in the ticket. Please either use 3.2.5 or one of the latest Maven versions;;;","17/Apr/23 10:32;Small Wong;[~martijnvisser] It is ok by Maven 3.2.5. Thank you.;;;","21/Apr/23 06:13;thexia;[~Small Wong] i meet the same problem.

maven 3.63

jdk 1.8

scala 2.12

window 10;;;","21/Apr/23 06:18;Small Wong;[~thexia]  pls try in Unix-like environment *Not Window* env as what [~martijnvisser] said. ;;;","03/Aug/23 11:58;wenlingzhang;If we are only using a windows computer and have idea installed, and want to compile flink. How to solve this problem? Since Flink version 1.17, compiling with windows has run into this problem;;;","28/Aug/23 11:31;martijnvisser;[~wenlingzhang] Use the correct version of Maven, since that has changed. ;;;",,,,,,,,,,,,,,,,,
Support DataStream CoGroup in stream Mode with similar performance as DataSet CoGroup,FLINK-31753,13531840,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lindong,lindong,lindong,07/Apr/23 09:52,29/Jun/23 10:47,04/Jun/24 20:41,19/Apr/23 01:26,,,,,,,,,,,,,,,,,,,,,ml-2.3.0,,,,,,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,,,"DataSet has been deprecated and will be removed from Flink. However, DataStream CoCroup is still considerably slower than DataSet when co-grouping two bounded streams.

Here are the benchmark results of co-grouping two bounded streams with 4*10^6 records from each stream under different modes. The co-group function is chosen to be very lightweight so that benchmark is dominated by the Flink's co-group overhead.

DataSet: 5.6 sec
DataStream batch mode: 15.4 sec
DataStream stream mode with rocksdb: 81 sec

We should be able to performance co-group operation in DataStream stream mode so that users' don't have to take big regression in order to migrate from DataSet to DataStream.

We will first add util function in Flink ML to unblock the migration of some algorithms from Alink to Flink ML.

Here is the code used to benchmark DataSet's CoGroup.

{code:java}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
env.getConfig().enableObjectReuse();
env.getConfig().disableGenericTypes();
env.setRestartStrategy(RestartStrategies.noRestart());
env.setParallelism(1);

DataSet<Tuple3<Integer, Integer, Double>> data1 =
        env.fromCollection(
                new DataGenerator(numRecords),
                Types.TUPLE(Types.INT, Types.INT, Types.DOUBLE));
DataSet<Tuple3<Integer, Integer, Double>> data2 =
        env.fromCollection(
                new DataGenerator(numRecords),
                Types.TUPLE(Types.INT, Types.INT, Types.DOUBLE));

data1.coGroup(data2)
        .where((KeySelector<Tuple3<Integer, Integer, Double>, Integer>) tuple -> tuple.f0)
        .equalTo((KeySelector<Tuple3<Integer, Integer, Double>, Integer>) tuple -> tuple.f0)
        .with(
                new RichCoGroupFunction<
                        Tuple3<Integer, Integer, Double>,
                        Tuple3<Integer, Integer, Double>,
                        Integer>() {

                    @Override
                    public void open(Configuration parameters) throws Exception {
                        super.open(parameters);
                    }

                    @Override
                    public void close() throws Exception {
                        super.close();
                    }

                    @Override
                    public void coGroup(
                            Iterable<Tuple3<Integer, Integer, Double>> iterable,
                            Iterable<Tuple3<Integer, Integer, Double>> iterable1,
                            Collector<Integer> collector)
                            throws Exception {
                        collector.collect(1);
                    }
                })
        .write(new CountingAndDiscardingSink(), ""/tmp"");
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 08:44:09 UTC 2023,,,,,,,,,,"0|z1h5ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 08:44;lindong;Merged to apache/flink-ml master branch d7c9c8b5242a3c161d430a03fc4e4c3b0d1d78ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SourceOperatorStreamTask increments numRecordsOut twice,FLINK-31752,13531831,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,huwh,huwh,07/Apr/23 08:00,13/Dec/23 08:46,04/Jun/24 20:41,23/Apr/23 06:26,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Runtime / Metrics,,,,,,0,pull-request-available,,,,,"The counter of numRecordsOut was introduce to ChainingOutput to reduce the function call stack depth in 
https://issues.apache.org/jira/browse/FLINK-30536

But SourceOperatorStreamTask.AsyncDataOutputToOutput increments the counter of numRecordsOut too. This results in the source operator's numRecordsOut are doubled.

We should delete the numRecordsOut.inc in SourceOperatorStreamTask.AsyncDataOutputToOutput.

[~xtsong][~lindong] Could you please take a look at this.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 07:51;huwh;image-2023-04-07-15-51-44-304.png;https://issues.apache.org/jira/secure/attachment/13057133/image-2023-04-07-15-51-44-304.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 24 05:07:34 UTC 2023,,,,,,,,,,"0|z1h5gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/23 09:04;lindong;[~huwh] Thanks for reporting this bug. I will look into this.;;;","14/Apr/23 11:58;lindong;[~yunfengzhou] Could you help fix this bug?;;;","23/Apr/23 06:26;lindong;Merged to apache/flink master branch 26bd5fe390e638e97925245da4ccb706b9e658e2.;;;","24/Apr/23 05:07;lindong;Merged to apache/flink release-1.17 branch be13d05b7f66dc5f0c926565ff139aa8313fcdc5;;;",,,,,,,,,,,,,,,,,,,,,,,
array return type SpecificTypeStrategies.ARRAY and ifThenElse return type is not correct,FLINK-31751,13531827,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,jackylau,jackylau,07/Apr/23 07:34,14/Apr/23 08:28,04/Jun/24 20:41,14/Apr/23 08:28,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"like array return type

Type strategy that returns a \{@link DataTypes#ARRAY(DataType)} with element type equal to the type of the first argument, which is not equals calcite semantic.

for example
{code:java}
ARRAY<BIGINT NOT NULL> and ARRAY<BIGINT> NOT NULL
it should return  ARRAY<BIGINT> instead of ARRAY<BIGINT NOT NULL>{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 14 08:28:27 UTC 2023,,,,,,,,,,"0|z1h5fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/23 07:35;jackylau;hi [~twalthr] [~jark] what do you think?;;;","14/Apr/23 08:28;jackylau;the inputTypeStrategy/outputTypeStrategy infers function argument types. the return type is using type which is infered by inputTypeStrategy. So it is not problem. 

 

close it;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Hash Keys are duplicate when join reorder happens in stream mode,FLINK-31750,13531814,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,zhengyiweng,zhengyiweng,zhengyiweng,07/Apr/23 02:47,17/Apr/23 10:57,04/Jun/24 20:41,17/Apr/23 10:57,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"When I run  `JoinReorderTestBase#testAllInnerJoin` in the case that isBushyJoinReorder is false, I find hash keys are duplicate.  !image-2023-04-07-10-39-13-831.png|width=571,height=263!

The reason why it happens is that when join reorder, the join condition will change and generate the same column condition, for example，the condition of T1 join（T4 join T5）is a1 = a4 and a1 = a5.  It can de fixed if columns in  `StreamPhysicalJoinRuleBase#onMatch#toHashTraitByColumns` are not duplicate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 02:39;zhengyiweng;image-2023-04-07-10-39-13-831.png;https://issues.apache.org/jira/secure/attachment/13057126/image-2023-04-07-10-39-13-831.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 17 10:56:53 UTC 2023,,,,,,,,,,"0|z1h5co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 10:56;zhengyiweng;It is not a bug. If duplicate hash keys are removed, the hash code on both sides of the join are inconsistent, resulting in partitioning errors.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The Using Hadoop OutputFormats example is not avaliable for DataStream,FLINK-31749,13531810,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,echauchot,easonqin,easonqin,07/Apr/23 02:38,19/Apr/24 09:25,04/Jun/24 20:41,16/Aug/23 08:21,1.15.4,1.17.0,,,,,,,,,,,,,,,,,,,cassandra-3.2.0,,,,,,,,,Documentation,,,,,,0,pull-request-available,stale-assigned,,,,"The following example shows how to use Hadoop’s {{TextOutputFormat from the doc: [https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/formats/hadoop/#using-hadoop-outputformats] . But the DataStream has no output().}}
{code:java}
// Obtain the result we want to emit
DataStream<Tuple2<Text, IntWritable>> hadoopResult = [...]

// Set up the Hadoop TextOutputFormat.
HadoopOutputFormat<Text, IntWritable> hadoopOF =
  // create the Flink wrapper.
  new HadoopOutputFormat<Text, IntWritable>(
    // set the Hadoop OutputFormat and specify the job.
    new TextOutputFormat<Text, IntWritable>(), job
  );
hadoopOF.getConfiguration().set(""mapreduce.output.textoutputformat.separator"", "" "");
TextOutputFormat.setOutputPath(job, new Path(outputPath));

// Emit data using the Hadoop TextOutputFormat.
hadoopResult.output(hadoopOF); {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 22:35:08 UTC 2023,,,,,,,,,,"0|z1h5bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 14:48;martijnvisser;[~echauchot] WDYT? Did we make a mistake by including the Hadoop Outputformats for the DataStream API, or is the code wrong?;;;","14/Apr/23 08:32;echauchot;[~martijnvisser] I'd say both:
* Datastream output is supported through DataStream#addSink(SinkFunction)
* There is no SinkFunction for Hadoop so then the OutputFormat is only for batch mode and the DataSet API
=> So I'd remove the output section of hadoop format for DataStream API. The input section code seem correct but I don't know if it would work in streaming mode as there will not be any continuously incomming data. Should we remove Hadoop as a whole from the DataStream doc ?

And by the way I checked for the Cassandra case that I know well and I can see we are missing docs about Dataset input/output formats ;;;","18/Apr/23 13:40;echauchot;[~martijnvisser] WDYT ?;;;","20/Apr/23 12:53;martijnvisser;[~echauchot] I don't think that the DataSet part is important, but we should have correct info for the DataStream API. Want to fix it?;;;","20/Apr/23 14:54;echauchot;yes please assign to me, I'll update the hadoop part. I'd still add the missing dataSet cassandra doc in case people still use DataSet. See https://issues.apache.org/jira/browse/FLINK-31870;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,
Adapt SplitFetcherManager#removeSplit for flink-connector-pulsar,FLINK-31748,13531809,13420525,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,07/Apr/23 02:28,25/Apr/23 05:20,04/Jun/24 20:41,25/Apr/23 05:19,pulsar-4.0.0,,,,,,,,,,,,,,,,,,,,pulsar-4.0.1,,,,,,,,,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 25 05:19:22 UTC 2023,,,,,,,,,,"0|z1h5bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 05:19;Weijie Guo;Temporarily fixed in main via bbb636a433ded42f61b1e54811d046b590c4d514.

Feel free to reopen this for further fixing. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize debezium from flink-json,FLINK-31747,13531800,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mason6345,mason6345,06/Apr/23 22:57,16/Oct/23 07:32,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,auto-deprioritized-major,pull-request-available,,,,"debezium code from Flink-json should move to the external Kafka repo. however, we need to ensure backward compatibility with dependencies referencing `Flink-json`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 07:32:19 UTC 2023,,,,,,,,,,"0|z1h59k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","24/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","16/Oct/23 07:32;martijnvisser;[~tzulitai] This ticket has been resolved via https://github.com/apache/flink-connector-kafka/commit/910fc5a3f06ab7d95f79c0fc4b5ce64e2aaa8db2 right?;;;",,,,,,,,,,,,,,,,,,,,,,,,
Batch workload output completes while the job client fails,FLINK-31746,13531776,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,functioner,functioner,06/Apr/23 18:40,10/Apr/23 17:23,04/Jun/24 20:41,,1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"We are doing testing on Flink-1.14.0 (We know 1.14.0 is not supported now so we are also testing Flink-1.17.0 to see if it has the same issue). We run a batch processing job. The input of the job is a file in the disk; the output of the job is a Kafka topic, which should receive 170 messages when the workload finishes. In the testing, we introduce a fault (an IOException) in a taskmanager, then the batch processing job client fails:
{code:java}
2023-03-26T19:05:48,922 ERROR cli.CliFrontend (CliFrontend.java:handleError(923)) - Error while running the command.org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 85c9bd56d6dd111f858b4b5a99551c53) {code}
The IOException occurs in `BoundedBlockingSubpartitionDirectTransferReader$FileRegionReader` when running `FileChannel.open`. It has multiple chances to occur in a workload.
{code:java}
    FileRegionReader(Path filePath) throws IOException {
        this.fileChannel = FileChannel.open(filePath, StandardOpenOption.READ);
        this.headerBuffer = BufferReaderWriterUtil.allocatedHeaderBuffer();
    }
 {code}
The call stack of this fault site:
{code:java}
(org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionDirectTransferReader$FileRegionReader,<init>,200), (org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionDirectTransferReader,<init>,74), (org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition,createReadView,221), (org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition,createSubpartitionView,205), (org.apache.flink.runtime.io.network.partition.ResultPartitionManager,createSubpartitionView,76), (org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel,requestSubpartition,133), (org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate,internalRequestPartitions,330), (org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate,requestPartitions,299), (org.apache.flink.runtime.taskmanager.InputGateWithMetrics,requestPartitions,127), (org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1,runThrowing,50),
(org.apache.flink.streaming.runtime.tasks.mailbox.Mail,run,90), (org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor,processMailsNonBlocking,353), (org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor,processMail,319), (org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor,runMailboxLoop,201), (org.apache.flink.streaming.runtime.tasks.StreamTask,runMailboxLoop,809),
(org.apache.flink.streaming.runtime.tasks.StreamTask,invoke,761),
(org.apache.flink.runtime.taskmanager.Task,runWithSystemExitMonitoring,958),
(org.apache.flink.runtime.taskmanager.Task,restoreAndInvoke,937),
(org.apache.flink.runtime.taskmanager.Task,doRun,766),
(org.apache.flink.runtime.taskmanager.Task,run,575),
(java.lang.Thread,run,748) {code}
We inspect the name of the threads where the fault occurs, we find that our workload can be divided into these tasks:

Split Reader: Custom File Source -> Flat Map (1/8)#0
...
Split Reader: Custom File Source -> Flat Map (8/8)#0
Keyed Aggregation -> Map -> Sink Unnamed Writer (1/8)#0
...
Keyed Aggregation -> Map -> Sink Unnamed Writer (8/8)#0
Sink Unnamed Committer (1/1)#0

 

Such fault during “Split Reader” or “Keyed Aggregation” will trigger this “Job failed” message and our Kafka topic can’t receive the complete correct output (i.e., less than 170 messages). However, if the exception happens during “Sink Unnamed Committer”, the client still recognizes the “Job failed”, while our Kafka topic already completely got what it wants.

We assume that our workload is translated into a few steps: “Custom File Source -> Flat Map”, “Keyed Aggregation -> Map -> Sink Unnamed Writer”, and “Sink Unnamed Committer”. The last one is responsible for some “commit” for it does not affect our end-to-end results. However, the fault in the “commit” stage still reports a “failure” to the job client, while the job client may get confused.

We have some questions about the design rationales:
 # In some workloads such as our case, the “commit” at last seems not to matter that much. Can it be seen as tolerable?
 # The client log is confusing. It shows tons of exceptions but it does not show in which stage of the workload the failure happens. The most useful information for the client is something like “Sink Unnamed Committer (1/1)#0 (7b19f0a2f247b8f38fe9141c9872ef58) switched from RUNNING to FAILED”, which is not shown.

P.S. The complete failure log of the job client is:
{code:java}
2023-04-03T11:36:25,464 ERROR cli.CliFrontend (CliFrontend.java:handleError(923)) - Error while running the comm
and.
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: org.apache.flink.cl
ient.program.ProgramInvocationException: Job failed (JobID: 8a169709de74948b5a9fed7d52c13f8d)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist
_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.jav
a:222) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist_2.11-1.14.0.jar
:1.14.0]
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812) ~[flink-dist_2.11-1.14.0
.jar:1.14.0]
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246) ~[flink-dist_2.11-1.14.0.jar:1.14.0
]
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054) ~[flink-dist_2.11-1.14.0.j
ar:1.14.0]
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132) ~[flink-dist_2.11-1.14.
0.jar:1.14.0]
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28
) [flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132) [flink-dist_2.11-1.14.0.jar:1.14.
0]
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.client.program.ProgramInvocationException:
Job failed (JobID: 8a169709de74948b5a9fed7d52c13f8d)
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_221]
        at org.apache.flink.client.program.StreamContextEnvironment.getJobExecutionResult(StreamContextEnvironme
nt.java:123) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:80) ~[
flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironm
ent.java:1917) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at edu.jhu.order.mcgray.fl_1_14_0.FlinkGrayBatchClientMain.run(FlinkGrayBatchClientMain.java:69) ~[?:?]
        at edu.jhu.order.mcgray.fl_1_14_0.FlinkGrayClientMain.run(FlinkGrayClientMain.java:66) ~[?:?]
        at edu.jhu.order.mcgray.fl_1_14_0.FlinkGrayClientMain.main(FlinkGrayClientMain.java:92) ~[?:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_221]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_221]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        ... 8 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: Job failed (JobID: 8a169709de74948b5a9fed7d52c13f8d)
        at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:125) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_221]
        at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_221]
        at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$26(RestClusterClient.java:698) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_221]
        at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) ~[?:1.8.0_221]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
        at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.client.deployment.ClusterClientJobClientAdapter.lambda$null$6(ClusterClientJobClientAdapter.java:123) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_221]
        at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_221]
        at org.apache.flink.client.program.rest.RestClusterClient.lambda$pollResourceAsync$26(RestClusterClient.java:698) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962) ~[?:1.8.0_221]
        at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:403) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:929) ~[?:1.8.0_221]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) ~[?:1.8.0_221]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_221]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_221]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:228) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:218) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:209) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:679) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_221]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_221]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[?:?]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[?:?]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[?:?]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) ~[?:?]
        at akka.actor.ActorCell.invoke(ActorCell.scala:548) ~[?:?]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_221]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_221]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_221]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_221]
Caused by: java.io.IOException
        at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionDirectTransferReader$FileRegionReader.<init>(BoundedBlockingSubpartitionDirectTransferReader.java:229) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartitionDirectTransferReader.<init>(BoundedBlockingSubpartitionDirectTransferReader.java:82) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.io.network.partition.BoundedBlockingSubpartition.createReadView(BoundedBlockingSubpartition.java:226) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.createSubpartitionView(BufferWritingResultPartition.java:209) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.createSubpartitionView(ResultPartitionManager.java:76) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.requestSubpartition(LocalInputChannel.java:133) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:330) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:299) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:127) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:358) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:322) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:202) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14.0.jar:1.14.0]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221] {code}
We feel that probably the job client should improve its logging, by adding more details about the failure, such as the information about “Sink Unnamed Committer”.

We are also checking Flink-1.17.0 to see if it has this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 10 17:23:48 UTC 2023,,,,,,,,,,"0|z1h548:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 19:12;martijnvisser;[~functioner] Please let us know what the results are with Flink 1.17. ;;;","10/Apr/23 16:23;functioner;[~martijnvisser] I reproduced the similar issue in Flink-1.17.0. It has the similar stage of ""Sink Committer (1/1)#0"" and the exception can cause this issue.;;;","10/Apr/23 17:23;Weijie Guo;Details about the failed task should be found in the jobmanager.log. IIRC, job client only got the first root exception that was not recoverable and triggered job failure does not contain task information. Most of the time, we submit jobs in detached mode(job client does not wait for the job execution to complete), and then look for the reason for job failover from the JM log.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Performance regression on serializerHeavyString since April 3rd,FLINK-31745,13531746,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Not A Problem,,martijnvisser,martijnvisser,06/Apr/23 13:32,24/Apr/23 13:02,04/Jun/24 20:41,14/Apr/23 08:49,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,,,,,,,0,,,,,,"serializerHeavyString baseline=241.682406 current_value=203.24132

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=serializerHeavyString&extr=on&quarts=on&equid=off&env=2&revs=200",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 24 13:02:53 UTC 2023,,,,,,,,,,"0|z1h4xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 08:49;pnowojski;It just restored performance back to the level from ~12th Jan probably yet another ""unexplained"" performance change, that has something todo with either environment, or JIT compiler, or sth like that

The suspected commit range 
{noformat}
git ls 9190ea9624..f3b63b7c96
f3b63b7c962 [2 weeks ago] [FLINK-31656][runtime][security] Obtain delegation tokens early to support external file system usage in blob server [Gabor Somogyi]
6c1ffe544e3 [9 days ago] [FLINK-31690][python] Fix KeyedCoProcessFunction to set the current key into the context [Dian Fu]
6101ad313be [10 days ago] [FLINK-31688][docs] Fix the broken links in docs for Azure Table Storage [Yuxin Tan]
9e83858c7dc [12 days ago] [FLINK-31652][k8s] Handle the deleted event in case pod is deleted during the pending phase [sunxia]
{noformat}

reveals nothing relevant in that range. ;;;","24/Apr/23 13:02;Yanfei Lei;I submitted two benchmark-request to verify, looks like it's related to FLINK-31656.
 * [before-31656|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/215/artifact/jmh-result.csv/*view*/] : 253.575579+-1.751539
 * [after-31656|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/lastSuccessfulBuild/artifact/jmh-result.csv/*view*/]: 218.058900+-1.018727;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Extend Adaptive Scheduler sparse EG to contain maxParallelism,FLINK-31744,13531745,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,06/Apr/23 13:15,16/Oct/23 12:01,04/Jun/24 20:41,12/Apr/23 08:20,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,Runtime / REST,,,,,0,pull-request-available,,,,,"When a job is WaitingForResources the adpative scheduler returns a sparse execution graph that omits many details that are only know at execution time (like subtasks).

We could include all JobVertex-level information though, which would cover things like the vertex id/name and the maxParallelism.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31316,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 08:20:36 UTC 2023,,,,,,,,,,"0|z1h4xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 08:20;chesnay;master: be05eaff39678672e696cf458015573ce4a0d565;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid relocating the RocksDB's log failure when filename exceeds 255 characters,FLINK-31743,13531718,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,assassinj,assassinj,06/Apr/23 08:54,19/May/23 15:41,04/Jun/24 20:41,19/May/23 05:44,1.15.4,1.16.1,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,,,"Since FLINK-24785 , the file name of the rocksdb LOG is generated by parsing the db path, when the db path is long and the filename exceeds 255 characters, the creation of the file will fail, so the relevant rocksdb LOG cannot be seen in the flink log dir.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 19 15:40:06 UTC 2023,,,,,,,,,,"0|z1h4rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 12:34;yunta;[~assassinj] Thanks for creating this ticket, please go ahead.;;;","11/Apr/23 19:45;roman;[~assassinj] are you still planning to work on this issue?;;;","13/Apr/23 03:13;assassinj;Hi [~roman] 

Previously, Tang Yun and I discussed a two-step approach to addressing this issue: 
firstly, avoiding relocation of logs in RocksDB when the path name exceeds 255 characters on the Flink side. 
Secondly, renaming the file from the FRocksDB perspective. 
I have completed development of the second step, but I am still working on the first. 
It's important to consider the potential impact on distinguishing between multiple operator logs in the previous database directory. 
While I am quite busy this week, I plan to allocate time to address this over the next couple of days.

What do you think?;;;","22/Apr/23 12:45;assassinj;H,[~yunta] 
Sorry that due to the adjustment of my work direction, this issue will be followed up by my colleague. Please assign the task to [~Feifan Wang] . 
Thanks a lot.;;;","23/Apr/23 05:37;yunta;[~assassinj] Thanks for the information, already assigned to him.;;;","08/May/23 11:04;yunta;merged in master: bdb0233eb83629c3bb1b1487057b22891c41c437

For older Flink versions, please take a look at my comment https://github.com/apache/flink/pull/22458/files#r1187314041 to create another PR (for 1.17) [~Feifan Wang];;;","09/May/23 01:40;Feifan Wang;Thanks [~yunta] , I submit a new [PR|https://github.com/apache/flink/pull/22545] to fix this in 1.17, PTAL.;;;","11/May/23 12:52;yunta;merged in flink
release-1.17: c29af139a8f3055c96c641016a31cd3a92ca022a
release-1.16: 7b41bd2b6df4f4794b0b66b52a6af2693a299ca9;;;","19/May/23 11:45;yunta;[~Feifan Wang] You can create another ticket to focus on the changes on FRocksDB repo.;;;","19/May/23 15:40;Feifan Wang;Thanks for reminding [~yunta] , I created a new ticket, can you assign it to me ?
And I have submitted a [PR on FRocksDB|https://github.com/ververica/frocksdb/pull/66], can you help me review it ?;;;",,,,,,,,,,,,,,,,,
Replace deprecated TableSchema in flink-table-planner test,FLINK-31742,13531685,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,taoran,taoran,06/Apr/23 03:28,11/Apr/23 08:37,04/Jun/24 20:41,,1.17.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,we can try to remove deprecated TableSchema and use Schema & ResolvedSchema to replace it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 11 08:37:57 UTC 2023,,,,,,,,,,"0|z1h4k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 05:16;icshuo;Hi, [~lemonjing] , changes of public API generally should not be introduced in patch release.;;;","06/Apr/23 05:27;taoran;[~icshuo] Yes. i changed the issue name. we can replace these old usages for next or 2.x released version. WDYT?;;;","11/Apr/23 08:37;icshuo;[~lemonjing] LGTM;;;",,,,,,,,,,,,,,,,,,,,,,,,
Supports data conversion according to type for executor,FLINK-31741,13531683,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,06/Apr/23 02:35,05/May/23 11:10,04/Jun/24 20:41,18/Apr/23 07:38,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,"Currently the results in StatementResult are string, they should be convert to different according to the data type",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 18 07:38:38 UTC 2023,,,,,,,,,,"0|z1h4js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 07:38;libenchao;Fixed via https://github.com/apache/flink/commit/9538fdaab2948a2e3dd068925d936ac0777301de

[~zjureel] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Allow setting boundedness for upsert-kafka SQL connector,FLINK-31740,13531666,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tzulitai,tzulitai,tzulitai,05/Apr/23 21:48,16/Oct/23 12:38,04/Jun/24 20:41,16/Oct/23 12:38,,,,,,,,,,,,,,,,,,,,,kafka-3.1.0,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,stale-assigned,,,,"With FLINK-24456, we added boundedness options for streaming mode to the SQL Kafka Connector. This was mostly just an exposure of existing functionality that was already available at the DataStream API level.

We should do the same for the SQL Upsert Kafka Connector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 22:35:08 UTC 2023,,,,,,,,,,"0|z1h4g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 15:59;tzulitai;Fix via fc31198d864ebafd36cf23ce302b6c1b4038b579 in apache/flink-connector-kafka:main;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ElasticSearch and Cassandra connector v3.0 branch's CI is not working properly,FLINK-31739,13531631,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,05/Apr/23 15:54,06/Apr/23 14:01,04/Jun/24 20:41,06/Apr/23 13:59,cassandra-3.0.1,elasticsearch-3.0.1,,,,,,,,,,,,,,,,,,,cassandra-3.0.1,elasticsearch-3.0.1,,,,,,,,Connectors / Cassandra,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,,"After FLINK-30963, we no longer manually set {{flink_url}}, but it is required in some connector's own {{ci.yml}}, which causes CI to fail to run like [this|https://github.com/apache/flink-connector-elasticsearch/actions/runs/4620241065]). The root of this problem is that these branch does not use the {{ci.yml}} in {{flink-connector-shared-utils}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 13:59:18 UTC 2023,,,,,,,,,,"0|z1h488:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 13:59;martijnvisser;Elasticsearch v3.0: 57e7883e270b1e3d5814bf52e808ce6099e45481
Cassandra v3.0: 9d930f58ce3ae4379ff8aca42dadb4cbff978b09;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
FlameGraphTypeQueryParameter#Type clashes with java.reflect.Type in generated clients,FLINK-31738,13531610,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Apr/23 14:38,11/Apr/23 10:30,04/Jun/24 20:41,11/Apr/23 10:30,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Documentation,Runtime / REST,,,,,0,pull-request-available,,,,,"Generating a client with the openapi generators causes compile errors because the generated file imports java.reflect.Type, but also the generated ""Type"" model.

For convenience it would be neat to give this enum a slightly different name, because working around this issue is surprisingly annoying.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 11 06:55:37 UTC 2023,,,,,,,,,,"0|z1h43k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 06:55;dmvk;master: 4f15da77dcb7d83e211eb65f24e1b49cb46618a2

release-1.17: e026cb58e1771f21cba5b5dfb4950ef004f6a261;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Hide the rescaling buttons for batch jobs in the WEB UI,FLINK-31737,13531585,13527017,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dmvk,dmvk,05/Apr/23 13:30,05/Apr/23 13:31,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,,0,,,,,,"When the rescaling is enabled (and supported by the cluster -> we're using AdaptiveScheduler) and the user submits a batch job, we fall back to a different scheduler (AdaptiveBatchScheduler) that doesn't support rescaling. However, we still show the rescaling buttons in the WebUI.",,,,,,,,,,FLINK-31736,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-05 13:30:54.0,,,,,,,,,,"0|z1h3y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose per-job capabilities via REST API,FLINK-31736,13531584,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dmvk,dmvk,05/Apr/23 13:29,05/Apr/23 13:31,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / REST,,,,,,0,,,,,,"This is a follow-up to the [https://github.com/apache/flink/pull/22350#discussion_r1158133570]

 

We only have a per cluster /config endpoint that allows us to enable/hide features on the client side (web UI). Still, sometimes we need to be able to match these features with the Job's capabilities to provide a great experience.

 

An example is that when the rescaling is enabled (and supported by the cluster -> we're using AdaptiveScheduler) and the user submits a batch job, we fallback to a different scheduler (AdaptiveBatchScheduler) that doesn't support rescaling. However, we still show the rescaling buttons in the WebUI.

 

Another example of Job's capabilities could be whether it supports checkpointing/savepoints.",,,,,,,,,,,,,,,,,,,,,FLINK-31737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-05 13:29:48.0,,,,,,,,,,"0|z1h3xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobDetailsInfo plan incorrectly documented as string,FLINK-31735,13531583,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Apr/23 13:19,06/Apr/23 15:22,04/Jun/24 20:41,06/Apr/23 15:22,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Documentation,Runtime / REST,,,,,0,pull-request-available,,,,,"The {{plan}} field in the JobDefaultsInfo contains an object, not a string. Internally we handle it as a string, but write it out as an object.
The docs generators aren't aware of this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 15:22:37 UTC 2023,,,,,,,,,,"0|z1h3xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 15:22;chesnay;master: 8ee66535879ce90882f0320b0c0d8ab7ef44ed37
1.17: a70554b51a4f73527c92e2f739538727d81e391d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Align behaviour of REST API / WEB UI feature flags,FLINK-31734,13531575,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dmvk,dmvk,05/Apr/23 12:22,09/May/23 16:25,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Configuration,Runtime / REST,Runtime / Web Frontend,,,,1,,,,,,"Currently we have following three feature flags for the web UI and REST API:

1) web.submit.enabled

2) web.cancel.enabled

3) web.rescale.enabled

 

2) and 3) only hide the web UI elements, while 1) also removes the REST API endpoint; We should introduce equivalent options for the REST API (rest.xxx.enabled), which would give the user flexibility to choose whether both REST API and WEB UI should be disabled for a given feature.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 09 16:25:52 UTC 2023,,,,,,,,,,"0|z1h3vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/23 16:25;zxcoccer;hi, I would like to give a try on this, can  I take this ticket?I will try my best to complete it
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Model name clashes in OpenAPI spec,FLINK-31733,13531564,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Apr/23 10:32,06/Apr/23 14:59,04/Jun/24 20:41,06/Apr/23 14:59,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Documentation,Runtime / REST,,,,,0,pull-request-available,,,,,"The OpenAPi spec uses simple class names for naming models. There are however several models, usually inner classes, that share simple names, like ""Summary"".

This goes undetected and breaks the model for some API calls.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 14:59:21 UTC 2023,,,,,,,,,,"0|z1h3tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 14:59;chesnay;master:
76d1a4678c383a927caf596ee594ba0ae5b5e1df
dcca520cb47a22354301174d0e1d2bde7f65061e
97a4cc88eee32ad766e5e0c1b647ebce5dc2bfee
1.17:
aadbc0e7d380c588dd63a3d00e2eebd19202b8ba
75c4a7c67fcebb03d073cc4eedbf495e1355b01e
3a69b075ed16a5827bd8539623543b4b277b1b7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
flink-ml-uber module should include statefun as a dependency,FLINK-31732,13531523,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,zhangzp,zhangzp,05/Apr/23 02:14,06/Apr/23 03:18,04/Jun/24 20:41,06/Apr/23 03:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 03:18:08 UTC 2023,,,,,,,,,,"0|z1h3k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 03:18;zhangzp;We leave state-fun as a third party dependency for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
No suitable constructor found for DebeziumAvroSerializationSchema,FLINK-31731,13531496,13541653,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tzulitai,martijnvisser,martijnvisser,04/Apr/23 19:21,28/Jun/23 02:26,04/Jun/24 20:41,22/Jun/23 08:15,1.18.0,kafka-4.0.0,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,test-stability,,,,"{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-kafka: Compilation failure
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTest.java:[939,16] no suitable constructor found for DebeziumAvroSerializationSchema(org.apache.flink.table.types.logical.RowType,java.lang.String,java.lang.String,<nulltype>)
Error:      constructor org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroSerializationSchema.DebeziumAvroSerializationSchema(org.apache.flink.table.types.logical.RowType,java.lang.String,java.lang.String,java.lang.String,java.util.Map<java.lang.String,?>) is not applicable
Error:        (actual and formal argument lists differ in length)
Error:      constructor org.apache.flink.formats.avro.registry.confluent.debezium.DebeziumAvroSerializationSchema.DebeziumAvroSerializationSchema(org.apache.flink.formats.avro.AvroRowDataSerializationSchema) is not applicable
Error:        (actual and formal argument lists differ in length)
Error:  -> [Help 1]
Error:  
Error:  To see the full stack trace of the errors, re-run Maven with the -e switch.
Error:  Re-run Maven using the -X switch to enable full debug logging.
Error:  
Error:  For more information about the errors and possible solutions, please read the following articles:
Error:  [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
Error:  
Error:  After correcting the problems, you can resume the build with the command
Error:    mvn <args> -rf :flink-connector-kafka
Error: Process completed with exit code 1.
{code}

https://github.com/apache/flink-connector-kafka/actions/runs/4610715024/jobs/8149513647#step:13:153",,,,,,,,,,FLINK-30859,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 22 08:14:59 UTC 2023,,,,,,,,,,"0|z1h3e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 19:29;tzulitai;We'll piggy-back the fix for this as part of the final externalization effort of the confluent-avro modules to the flink-connector-kafka repo.

i.e. once we resolve FLINK-30859, this ticket can be automatically resolved as well.;;;","30/May/23 08:17;renqs;[~tzulitai] Any update on this issue? Thanks;;;","22/Jun/23 08:14;tzulitai;Merged for apache/flink via 274aa0debffaa57926c474f11e36be753b49cbc5;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support Ephemeral Storage in KubernetesConfigOptions,FLINK-31730,13531483,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ZhenqiuHuang,ZhenqiuHuang,04/Apr/23 17:42,11/Apr/23 09:55,04/Jun/24 20:41,,1.17.1,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,,,,,,There is a common need to config flink main container with Ephemeral Storage size. It will be more user friendly to support it as a flink config.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 11 09:55:14 UTC 2023,,,,,,,,,,"0|z1h3bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 12:13;Weijie Guo;Do you mean to configure the requests and limit of ephemeral-storage? If yes, can [this|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#external-resource-%3Cresource_name%3E-kubernetes-config-key] meet your needs?;;;","11/Apr/23 09:55;wangyang0918;Do you think using the pod template[1] to configure the ephemeral storage is enough?

 

[1]. https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/resource-providers/native_kubernetes/#pod-template;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected UPDATE_BEFORE output record in LEFT OUTER JOIN,FLINK-31729,13531481,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,smiralex,smiralex,04/Apr/23 17:11,11/Mar/24 12:43,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,,,"Currently, in streaming LEFT/RIGHT/FULL OUTER JOIN Flink SQL doesn't emit UPDATE_BEFORE/UPDATE_AFTER records, but instead explicitly change RowKind of output records to INSERT/DELETE for simplicity. However, it doesn't work as expected, because sometimes UPDATE_BEFORE rows can be emitted. What is more confusing - after UPDATE_BEFORE record there will be INSERT record (not UPDATE_AFTER), which can cause bugs in case when downstream operators process UPDATE records in a different way than INSERT/DELETE (for example, it can assume, that after UPDATE_BEFORE there should be UPDATE_AFTER record at some point of time).

How to reproduce:

Suppose we have tables ""source1"" and ""source2"":
CREATE TABLE source1(
  id int PRIMARY KEY,
  c3 bigint
) WITH (
  'connector' = 'kafka',
   ...
  'format' = 'debezium-json'
);

 

CREATE TABLE source2(
  id int PRIMARY KEY,
  c3 bigint
) WITH (
  'connector' = 'kafka',
   ...
  'format' = 'debezium-json'
);

And we execute the following query:
""select  t1.id, t1.c3,t2.id, t2.c3 from source1 t1 left join source2 t2 on t1.id = t2.id""

Then we insert records one by one:
source1: 
{noformat}
{""before"":null,""after"":{""id"":2,""c3"":7121},""op"":""c""}{noformat}
source2: 
{noformat}
{""before"":null,""after"":{""id"":2,""c3"":364},""op"":""c""}{noformat}
source1: 
{noformat}
{""before"":{""id"":2,""c3"":7121},""after"":{""id"":2,""c3"":7222},""op"":""u""}{noformat}
source2: 
{noformat}
{""before"":{""id"":2,""c3"":364},""after"":{""id"":2,""c3"":564},""op"":""u""}{noformat}
The result will be as in the following screenshot:

!image-2023-04-05-00-08-32-984.png!

Note, that after implementing ticket https://issues.apache.org/jira/browse/FLINK-17337 (support emitting UPDATE_BEFORE/UPDATE_AFTER records not only in inner join) the described bug won't be relevant anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32778,,,,,,,,,,,"04/Apr/23 17:08;smiralex;image-2023-04-05-00-08-32-984.png;https://issues.apache.org/jira/secure/attachment/13057041/image-2023-04-05-00-08-32-984.png","06/Apr/23 07:02;smiralex;image-2023-04-06-14-02-30-826.png;https://issues.apache.org/jira/secure/attachment/13057092/image-2023-04-06-14-02-30-826.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://issues.apache.org/jira/browse/FLINK-17337,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 07:00:00 UTC 2023,,,,,,,,,,"0|z1h3aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 01:59;luoyuxia;[~smiralex] Thanks for reporting. But I'm wondering why there's `-U`. Shoudn't it always be -D, +I, -D, +I? Is it a bug ?;;;","06/Apr/23 07:00;smiralex;[~luoyuxia] Yes, according to the expected logic it should be always -D, +I, -D, +I. Here is the current JavaDoc above StreamingJoinOperator#processElement, which states that RowKing is being forwarded only in INNER JOIN. 
!image-2023-04-06-14-02-30-826.png!

Actually, the bug is because of this [line|https://github.com/apache/flink/blob/master/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingJoinOperator.java#L288]. Instead, there should be 
{code:java}
if (inputIsOuter || otherIsOuter) { {code}
However, I decided to implement ticket https://issues.apache.org/jira/browse/FLINK-17337 right away (emit -U/+U not only in inner join), because with new logic bug won't be present anymore. Here is a PR - [https://github.com/apache/flink/pull/22351];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Remove Scala API dependencies from batch/streaming examples,FLINK-31728,13531448,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Apr/23 13:41,06/Apr/23 12:51,04/Jun/24 20:41,06/Apr/23 12:51,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Build System,Examples,,,,,0,pull-request-available,,,,,"The example modules have leftover Scala API dependencies and build infrastructure. Remove them, along with the scala suffix on these modules.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 12:51:44 UTC 2023,,,,,,,,,,"0|z1h33k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 12:51;chesnay;master: 0415f7d1f9f32f93cbafa1000f1a33d66b06e1d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Source parallelism should match number of Kafka partitions,FLINK-31727,13531442,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,YordanPavlov,YordanPavlov,04/Apr/23 13:07,04/Apr/23 13:07,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"We seem to have hit a problem in how Flink fires windows, the problem presents itself on job recovery. To simplify the problem I am attaching a simple self sustained Flink job which illustrates the problem. What we have inside is, a KafkaSource consuming a topic with 3 partitions with job parallelism of 1. Right after the source data is consumed we have a TimeWindow, watermarks are constructed based on event data. For simplicity, data in the Kafka topic is just integers, which are also used as watermarks. The topic looks like so:
{quote}partition 0: 0, 3, 6, 9, 12 ...

partition 1: 1, 4, 7, 10, 13 ...

partition 2: 2, 5, 8, 11, 14 ...
{quote}
What we expect, and what is the case before a restart, is for Flink to wait for watermarks to progress on each of the 3 partitions and only then trigger windows. If this is met, we have windows triggered for each consecutive number.

The problem we observe happens on restart, then Flink would start fire windows even though it seems to be reading data only from some of the partitions.

Please find attached program, I am also attaching a simple Bash script used to generate the Kafka input data.

[^main.scala]

[^fill-topic.sh]

 

We do not see the problem if we set parallelism to 3 and the job have 3 slots per task manager. Going through the documentation I did not see such requirement though, this may be hard to enforce for all jobs.

Regards",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/23 13:03;YordanPavlov;fill-topic.sh;https://issues.apache.org/jira/secure/attachment/13057029/fill-topic.sh","04/Apr/23 13:03;YordanPavlov;main.scala;https://issues.apache.org/jira/secure/attachment/13057030/main.scala",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-04 13:07:52.0,,,,,,,,,,"0|z1h328:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PyFlink module java.base does not ""opens java.lang"" to unnamed module",FLINK-31726,13531436,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,padavan,padavan,04/Apr/23 12:26,07/Apr/23 08:11,04/Jun/24 20:41,07/Apr/23 08:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"I want to run simple example from Flink documentation. And after start i got exception:
{code:java}
Unable to make field private final byte[] java.lang.String.value accessible: module java.base does not ""opens java.lang"" to unnamed module @228575c0{code}
Installed:
{code:java}
Python 3.10.6

openjdk version ""19.0.2"" 2023-01-17 
OpenJDK Runtime Environment (build 19.0.2+7-Ubuntu-0ubuntu322.04) 
OpenJDK 64-Bit Server VM (build 19.0.2+7-Ubuntu-0ubuntu322.04, mixed mode, sharing){code}
Simple code from flink site:

[https://nightlies.apache.org/flink/flink-docs-master/api/python/examples/datastream/word_count.html]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 07 08:11:46 UTC 2023,,,,,,,,,,"0|z1h30w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/23 08:11;hxb;Flink doesn't support JDK 19, you can try to use jdk 8 or jdk 11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Synchronize dependency version between Flink and flink-connector-pulsar,FLINK-31725,13531415,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,04/Apr/23 09:31,04/Apr/23 11:27,04/Jun/24 20:41,04/Apr/23 11:27,,,,,,,,,,,,,,,,,,,,,pulsar-4.0.0,,,,,,,,,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 11:27:08 UTC 2023,,,,,,,,,,"0|z1h2w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 11:27;martijnvisser;Fixed in main: 438770156ff21971297cb57487768a4be9425095;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"SqlClientITCase.testMatchRecognize fails with ""bash -c rm -rf /opt/flink/checkpoint/*"" returned non-zero exit code 1",FLINK-31724,13531411,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,renqs,Sergey Nuyanzin,Sergey Nuyanzin,04/Apr/23 09:01,23/May/24 07:22,04/Jun/24 20:41,,1.18.0,1.19.0,1.20.0,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,,,0,stale-assigned,test-stability,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47893&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=12715]
{noformat}
2023-04-04T08:11:47.8601739Z Apr 04 08:11:47 [ERROR] Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 205.218 s <<< FAILURE! - in SqlClientITCase
2023-04-04T08:11:47.8736401Z Apr 04 08:11:47 [ERROR] SqlClientITCase.testMatchRecognize  Time elapsed: 42.257 s  <<< ERROR!
2023-04-04T08:11:47.8736940Z Apr 04 08:11:47 java.lang.IllegalStateException: 
2023-04-04T08:11:47.8737556Z Apr 04 08:11:47 Command ""bash -c rm -rf /opt/flink/checkpoint/*"" returned non-zero exit code 1. 
2023-04-04T08:11:47.8737861Z Apr 04 08:11:47 STDOUT: 
2023-04-04T08:11:47.8738297Z Apr 04 08:11:47 STDERR: rm: cannot remove '/opt/flink/checkpoint/e2b7cbfc940e5f066e587037f80e74af': Directory not empty
2023-04-04T08:11:47.8738611Z Apr 04 08:11:47 
2023-04-04T08:11:47.8738971Z Apr 04 08:11:47 	at org.apache.flink.connector.testframe.container.FlinkContainers.deleteJobManagerTemporaryFiles(FlinkContainers.java:471)
2023-04-04T08:11:47.8740127Z Apr 04 08:11:47 	at org.apache.flink.connector.testframe.container.FlinkContainers.stop(FlinkContainers.java:241)
2023-04-04T08:11:47.8740803Z Apr 04 08:11:47 	at SqlClientITCase.tearDown(SqlClientITCase.java:114)
2023-04-04T08:11:47.8741144Z Apr 04 08:11:47 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-04-04T08:11:47.8741677Z Apr 04 08:11:47 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-04-04T08:11:47.8742090Z Apr 04 08:11:47 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-04-04T08:11:47.8742463Z Apr 04 08:11:47 	at java.lang.reflect.Method.invoke(Method.java:498)
2023-04-04T08:11:47.8742825Z Apr 04 08:11:47 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-04-04T08:11:47.8743253Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-04-04T08:11:47.8743709Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-04-04T08:11:47.8744443Z Apr 04 08:11:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-04-04T08:11:47.8744880Z Apr 04 08:11:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:128)
2023-04-04T08:11:47.8745318Z Apr 04 08:11:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptAfterEachMethod(TimeoutExtension.java:110)
2023-04-04T08:11:47.8745812Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-04-04T08:11:47.8746540Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-04-04T08:11:47.8747033Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-04-04T08:11:47.8747515Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-04-04T08:11:47.8747969Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-04-04T08:11:47.8748418Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-04-04T08:11:47.8748845Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-04-04T08:11:47.8749300Z Apr 04 08:11:47 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-04-04T08:11:47.8749890Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeMethodInExtensionContext(ClassBasedTestDescriptor.java:520)
2023-04-04T08:11:47.8750393Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$synthesizeAfterEachMethodAdapter$24(ClassBasedTestDescriptor.java:510)
2023-04-04T08:11:47.8751271Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAfterEachMethods$10(TestMethodTestDescriptor.java:243)
2023-04-04T08:11:47.8751861Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAllAfterMethodsOrCallbacks$13(TestMethodTestDescriptor.java:276)
2023-04-04T08:11:47.8752334Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-04-04T08:11:47.8752977Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAllAfterMethodsOrCallbacks$14(TestMethodTestDescriptor.java:276)
2023-04-04T08:11:47.8753405Z Apr 04 08:11:47 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2023-04-04T08:11:47.8753827Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeAllAfterMethodsOrCallbacks(TestMethodTestDescriptor.java:275)
2023-04-04T08:11:47.8754862Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeAfterEachMethods(TestMethodTestDescriptor.java:241)
2023-04-04T08:11:47.8755320Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:142)
2023-04-04T08:11:47.8755758Z Apr 04 08:11:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-04-04T08:11:47.8756184Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-04-04T08:11:47.8756808Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-04-04T08:11:47.8757258Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-04-04T08:11:47.8757761Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-04-04T08:11:47.8758168Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-04-04T08:11:47.8758600Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-04-04T08:11:47.8759690Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-04-04T08:11:47.8760112Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-04-04T08:11:47.8760607Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-04-04T08:11:47.8761676Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-04-04T08:11:47.8762600Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-04-04T08:11:47.8763091Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-04-04T08:11:47.8763523Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-04-04T08:11:47.8763942Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-04-04T08:11:47.8764766Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-04-04T08:11:47.8765629Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-04-04T08:11:47.8766644Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-04-04T08:11:47.8768161Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-04-04T08:11:47.8768494Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-04-04T08:11:47.8768888Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-04-04T08:11:47.8769343Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-04-04T08:11:47.8769946Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-04-04T08:11:47.8770401Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-04-04T08:11:47.8770751Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-04-04T08:11:47.8771861Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-04-04T08:11:47.8772183Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-04-04T08:11:47.8772528Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-04-04T08:11:47.8772875Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-04-04T08:11:47.8773277Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-04-04T08:11:47.8774075Z Apr 04 08:11:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-04-04T08:11:47.8774539Z Apr 04 08:11:47 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-04-04T08:11:47.8774830Z Apr 04 08:11:47 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2023-04-04T08:11:47.8775105Z Apr 04 08:11:47 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2023-04-04T08:11:47.8775389Z Apr 04 08:11:47 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2023-04-04T08:11:47.8775676Z Apr 04 08:11:47 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 23 07:21:42 UTC 2024,,,,,,,,,,"0|z1h2vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 08:01;renqs;I checked the log but didn't find anything helpful. I'll downgrade it to Major first as this case only happened once. ;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","12/Jan/24 10:44;mapohl;Happened again: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56285&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=13957;;;","22/Jan/24 14:58;jiabao.sun;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56285&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=15295;;;","23/May/24 07:21;Weijie Guo;1.20 but another case(testUpsert)

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59753&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=15645;;;",,,,,,,,,,,,,,,,,,,,,,
DispatcherTest#testCancellationDuringInitialization is unstable,FLINK-31723,13531409,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,Sergey Nuyanzin,Sergey Nuyanzin,04/Apr/23 08:46,21/Apr/23 13:12,04/Jun/24 20:41,21/Apr/23 13:12,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47889&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=6761

{noformat}
Apr 04 02:26:26 [ERROR] org.apache.flink.runtime.dispatcher.DispatcherTest.testCancellationDuringInitialization  Time elapsed: 0.033 s  <<< FAILURE!
Apr 04 02:26:26 java.lang.AssertionError: 
Apr 04 02:26:26 
Apr 04 02:26:26 Expected: is <CANCELLING>
Apr 04 02:26:26      but: was <CANCELED>
Apr 04 02:26:26 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Apr 04 02:26:26 	at org.junit.Assert.assertThat(Assert.java:964)
Apr 04 02:26:26 	at org.junit.Assert.assertThat(Assert.java:930)
Apr 04 02:26:26 	at org.apache.flink.runtime.dispatcher.DispatcherTest.testCancellationDuringInitialization(DispatcherTest.java:389)
[...]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 21 13:11:59 UTC 2023,,,,,,,,,,"0|z1h2uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 07:47;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47960&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8489;;;","14/Apr/23 10:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48095&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8390;;;","17/Apr/23 11:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48188&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8574;;;","21/Apr/23 06:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48319&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8386;;;","21/Apr/23 13:11;dmvk;master: 378d3ca0d4b487b1ebc9354e9ebe8952cc3a9d11 52bf14b0ba949e048c78862be2ed8ebfb58c780e;;;",,,,,,,,,,,,,,,,,,,,,,
Remove dependency on flink-shaded,FLINK-31722,13531408,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,martijnvisser,martijnvisser,04/Apr/23 08:42,05/May/23 09:09,04/Jun/24 20:41,11/Apr/23 08:38,,,,,,,,,,,,,,,,,,,,,cassandra-3.0.1,cassandra-3.1.0,,,,,,,,Connectors / Cassandra,,,,,,0,pull-request-available,,,,,"The Cassandra connector relies on flink-shaded and uses Flinks' shaded Guava. With the externalization of connector, connectors shouldn't rely on Flink-Shaded but instead shade dependencies such as this one themselves",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31793,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 11 08:38:04 UTC 2023,,,,,,,,,,"0|z1h2uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Apr/23 09:12;echauchot;[~martijnvisser] Can you assign the ticket to me ?;;;","05/Apr/23 13:36;echauchot;[~martijnvisser] indeed cassandra connector uses *org.apache.flink.shaded.guava30.* but the dependency *flink-shaded-guava* is brought by the *flink-streaming-java* dependency not by *flink-shaded-force-shading* which comes from the connectors pom template. You mean that we should explicitly declare the guava dependency (as we use it) and shade it in the cassandra connector ?;;;","06/Apr/23 07:35;chesnay;??we should explicitly declare the guava dependency (as we use it) and shade it in the cassandra connector ???

yes. You can just use the guava version we pull in from the cassandra driver + adjust the checkstyle rules to allow guava.;;;","11/Apr/23 08:38;chesnay;cassandra-main: a63379d3a439af30dec3cf0e4ef5e36efbfe7e7b
cassandra-3.0: 25de5b58107c5211e7a5100a3c6b0e2cc352da67;;;",,,,,,,,,,,,,,,,,,,,,,,
Move JobStatusHook to flink-core module,FLINK-31721,13531407,13436816,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,tartarus,tartarus,04/Apr/23 08:20,19/Jun/23 02:27,04/Jun/24 20:41,19/Jun/23 02:27,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,API / Core,,,,,,0,pull-request-available,,,,,"Flink Sql needs to use JobStatusHook mechanism to implement atomic CTAS semantics, but the Table part module can't access flink-runtime module, so we need to move JobStatusHook to flink-core module",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 19 02:27:42 UTC 2023,,,,,,,,,,"0|z1h2ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/23 02:27;luoyuxia;master: fa94fb5a0271590c89edf6f3c0d4f274cf995e7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector v3.0 branch's configuration html files are empty,FLINK-31720,13531399,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,04/Apr/23 07:25,04/Apr/23 09:43,04/Jun/24 20:41,04/Apr/23 09:43,pulsar-3.0.1,,,,,,,,,,,,,,,,,,,,pulsar-3.0.1,,,,,,,,,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,"Currently, the generated configuration html files of Pulsar connector v3.0 branch are empty. We should add it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 09:43:10 UTC 2023,,,,,,,,,,"0|z1h2so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 09:43;Weijie Guo;v3.0 via 11765a00b1db95c73ae209f2f7af50d93829f948.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update Netty to 4.1.91-Final,FLINK-31719,13531392,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,04/Apr/23 06:49,12/Apr/23 09:40,04/Jun/24 20:41,12/Apr/23 09:40,,,,,,,,,,,,,,,,,,,,,shaded-17.0,,,,,,,,,BuildSystem / Shaded,,,,,,0,pull-request-available,,,,,"This is a bug fix release, which contains an important fix for Netty's native SSL implementation that fixes a problem that could lead to problems in state-machines.

https://netty.io/news/2023/04/03/4-1-91-Final.html

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 09:40:14 UTC 2023,,,,,,,,,,"0|z1h2r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 09:40;martijnvisser;Fixed in master: d18d236329cecd1dbad8ceba470d8b30f2e8ffd4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
pulsar connector v3.0 branch's CI is not working properly,FLINK-31718,13531379,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,04/Apr/23 04:43,04/Apr/23 06:38,04/Jun/24 20:41,04/Apr/23 06:38,pulsar-3.0.1,,,,,,,,,,,,,,,,,,,,pulsar-3.0.1,,,,,,,,,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,"After FLINK-30963, we no longer manually set {{flink_url}}, but it is required in pulsar connector's own {{ci.yml}}, which causes CI to fail to run normally. The root of the problem is that the v3.0 branch does not use the {{ci.yml}} in {{flink-connector-shared-utils}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 06:38:03 UTC 2023,,,,,,,,,,"0|z1h2og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 06:38;Weijie Guo;v3.0 via 4719bab7dde1e5082ddbf9cdfd9dd71217e81ccd.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests running with local kube config,FLINK-31717,13531340,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mateczagany,morhidi,morhidi,03/Apr/23 20:28,12/May/23 11:52,04/Jun/24 20:41,12/May/23 11:52,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Some unit tests are using local kube environment. This can be dangerous when pointing to sensitive clusters e.g. in prod.

{quote}2023-04-03 12:32:53,956 i.f.k.c.Config                 [DEBUG] Found for Kubernetes config at: [/Users/<redacted>/.kube/config].
{quote}

A misconfigured kube config environment revealed the issue:

{quote}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.012 s <<< FAILURE! - in org.apache.flink.kubernetes.operator.FlinkOperatorTest
[ERROR] org.apache.flink.kubernetes.operator.FlinkOperatorTest.testConfigurationPassedToJOSDK  Time elapsed: 0.008 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.kubernetes.operator.FlinkOperatorTest.testConfigurationPassedToJOSDK(FlinkOperatorTest.java:63)

[ERROR] org.apache.flink.kubernetes.operator.FlinkOperatorTest.testLeaderElectionConfig  Time elapsed: 0.004 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.kubernetes.operator.FlinkOperatorTest.testLeaderElectionConfig(FlinkOperatorTest.java:108){quote}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 12 11:52:55 UTC 2023,,,,,,,,,,"0|z1h2fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 12:15;mbalassi;To clarify the errors only appear when your local k8s context points to an unreacheable API server (e.g. one behind a VPN that you are not connected to at the moment).

I suggest to use this JOSDK utility to solve the issue:
https://csviri.medium.com/introducing-jenvtest-kubernetes-api-server-tests-made-easy-for-java-4d02a9bb26d4;;;","04/Apr/23 12:38;morhidi;SGTM;;;","19/Apr/23 18:11;mateczagany;Sorry, I didn't have much time to work on this ticket so far.

 

I've investigated and found 3 test cases that load ~/.kube/config in some ways:
 * FlinkOperatorTest
 * FlinkOperatorITCase
 * HealthProbeTest


I could add jenvtest to FlinkOperatorTest easily. I had to re-write some of the code in HealthProbeTest.java so it doesn't try to call FlinkOperator.run() to start OperatorHealthService.

But I could not get around FlinkOperatorITCase since that test class is about testing a Kubernetes cluster that already has the operator installed.
What do you suggest we do with that test case?;;;","20/Apr/23 06:33;mbalassi;Thanks, [~mateczagany]. I think the `FlinkOperatorITCase` can be retired in favor of the e2e tests:

https://github.com/apache/flink-kubernetes-operator/blob/main/.github/workflows/ci.yml#L94
https://github.com/apache/flink-kubernetes-operator/actions/runs/4607822219/jobs/8142835592

What do you think [~morhidi]?;;;","08/May/23 12:22;mateczagany;We have agreed offline to instead get rid of FlinkOperatorITCase and add set a system property during execution of FlinkOperatorTest and HealthProbeTest to make sure they don't try to access ~/.kube/config. 

I will also tweak the CI workflow so that minikube doesn't get started for the unit tests, so any new tests added in the future that might use the config file will fail.;;;","12/May/23 11:52;gyfora;merged to main e399c9bbd37d2dca352d973de01be0cb403fec55;;;",,,,,,,,,,,,,,,,,,,,,
Event UID field is missing the first time that an event is consumed,FLINK-31716,13531330,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rodmeneses,rodmeneses,rodmeneses,03/Apr/23 18:08,19/Apr/23 08:05,04/Jun/24 20:41,19/Apr/23 08:05,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"on `EventUtils.createOrUpdateEvent` we use a `Consumer<Event>` instance to `accept` the underlying event that is being created or updated.

The first time an event is created, we are calling `client.resource(event).createOrReplace()` but we are discarding the return value of such method, and we are returning the `event` that we just created, which has an empty UID field.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 19 08:05:03 UTC 2023,,,,,,,,,,"0|z1h2dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 18:32;mbalassi;Good catch [~rodmeneses];;;","04/Apr/23 21:29;mbalassi;658ad63 in main;;;","19/Apr/23 08:05;mbalassi;Additionally a5aca7c in main;;;",,,,,,,,,,,,,,,,,,,,,,,,
Warning - 'An illegal reflective access operation has occurred',FLINK-31715,13531321,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,ferozedaud,ferozedaud,03/Apr/23 17:00,04/Apr/23 07:44,04/Jun/24 20:41,04/Apr/23 07:44,1.14.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"I am seeing the following exception when my app starts up.

 
{noformat}
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/Users/ferozed/.gradle/caches/modules-2/files-2.1/org.apache.flink/flink-core/1.14.4/1c397865a94743deb286c658384fae954a381df/flink-core-1.14.4.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
09:51:49.626 [main] INFO  com.zillow.clickstream.preprocess.utils.SchemaRegistryHelper  - Schema id = 9636
An illegal reflective access operation has occurredIllegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/Users/ferozed/.gradle/caches/modules-2/files-2.1/org.apache.flink/flink-core/1.14.4/1c397865a94743deb286c658384fae954a381df/flink-core-1.14.4.jar) to field java.lang.String.valuePlease consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleanerUse --illegal-access=warn to enable warnings of further illegal reflective access operationsAll illegal access operations will be denied in a future release
 {noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 07:44:06 UTC 2023,,,,,,,,,,"0|z1h2bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 07:44;martijnvisser;These warnings can be safely ignored;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Conjars.org has died,FLINK-31714,13531309,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,nielsbasjes,nielsbasjes,03/Apr/23 15:05,04/Apr/23 07:37,04/Jun/24 20:41,03/Apr/23 15:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Recently conjars.org has died.
The effect is that it is now *impossible* to build Flink on a clean machine.
Chris Wensel has setup a readonly mirror https://conjars.wensel.net/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31705,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 07:37:40 UTC 2023,,,,,,,,,,"0|z1h28w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 07:30;martijnvisser;[~nielsbasjes] I don't think it was impossible to build Flink on a clean machine. Since FLINK-27640 it at least works for Flink 1.16 and Flink 1.17, because Flink didn't need Conjars anymore since Pentaho was excluded from the dependency tree.;;;","04/Apr/23 07:37;nielsbasjes;You are right. I ran into this while trying to build a local branch based on an old master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
k8s operator should gather job version metrics,FLINK-31713,13531303,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mateczagany,mbalassi,mbalassi,03/Apr/23 14:40,10/Apr/23 11:26,04/Jun/24 20:41,10/Apr/23 11:26,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,Runtime / Metrics,,,,,0,pull-request-available,,,,,"Similarly to the FLINK-31303 we should expose the number of times each Flink version is used in applications on a per namespace basis, this is sufficient for FlinkDeployments imho (no need to try to dig into session jobs) as the main purpose is to be able to gain visibility to the distribution of version used and be able to nudge users along to upgrade.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 10 11:26:34 UTC 2023,,,,,,,,,,"0|z1h27k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/23 11:26;mbalassi;8d7758c in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Allow skipping of archunit tests for nightly connector builds,FLINK-31712,13531281,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,03/Apr/23 13:05,24/Apr/23 12:46,04/Jun/24 20:41,24/Apr/23 12:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Build System,Connectors / Common,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-03 13:05:55.0,,,,,,,,,,"0|z1h22o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenAPI spec omits complete-statement request body,FLINK-31711,13531280,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,03/Apr/23 13:03,04/Apr/23 08:36,04/Jun/24 20:41,04/Apr/23 08:36,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Documentation,Runtime / REST,,,,,0,pull-request-available,,,,,"The OpenAPI generator omits request bodies for get requests because it is usually a bad idea.

Still, the generator shouldn't omit this on it's own.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28796,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 08:36:20 UTC 2023,,,,,,,,,,"0|z1h22g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 08:36;chesnay;master: 46aeebd150fe56995d85365cf5d44dd54854224e
1.17: 8d7b42c9480dd61bc0bbe935ca5b746697bc0ad8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Remove <curator.version/> and rely on the curator dependency that's provided by flink,FLINK-31710,13531278,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,taoran,mapohl,mapohl,03/Apr/23 12:44,04/Apr/23 02:36,04/Jun/24 20:41,,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,"Currently, we're relying on a dedicated curator dependency in tests to use the {{TestingZooKeeperServer}} (see [Flink's parent pom|https://github.com/apache/flink/blob/97cff0768d05e4a7d0217ddc92fd9ea3c7fae2c2/pom.xml#L143]). Besides that, we're using {{flink-shaded}} to provide the zookeeper and curator dependency that is used in Flink's production code.

The flaw of that approach is that we have to maintain two curator versions. This Jira issue is about investigating whether we could just remove the curator test dependency and rely on the {{flink-shaded}} curator sources.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 02:36:50 UTC 2023,,,,,,,,,,"0|z1h220:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 13:32;taoran;[~mapohl] Hi, Matthias. In fact, we have encountered other similar double references in related projects of the flink ecology in our internal flink version, such as guava and flink-shaded-guava and hadoop and flink-shaded-hadoop. It is a good practice to constrain the use of the shaded way. I'd be happy to work on this issue. Can u assign this ticket to me? (If we decide to use shaded-curator);;;","03/Apr/23 13:36;mapohl;Sure, but this requires more research on the different dependencies. Sometimes, using a dedicated dependency version different from the one used in {{flink-shaded}} might be have been done purposely. ;;;","03/Apr/23 13:39;mapohl;FYI: This Jira issue was created as an idea after having a [brief discussion in a flink-shaded PR|https://github.com/apache/flink-shaded/pull/119#discussion_r1153473283]. We might still conclude that it makes more sense to keep the current state. The only benefit I see so far is that we don't have to maintain two curator versions in the pom file(s).;;;","04/Apr/23 02:36;taoran;got it. thanks for your explanations.;;;",,,,,,,,,,,,,,,,,,,,,,,
JobResultStore and ExecutionGraphInfoStore could be merged,FLINK-31709,13531276,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,03/Apr/23 12:35,03/Apr/23 12:37,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,1,,,,,,"This is a initial proposal for an improvement in coordination layer:

The {{JobResultStore}} (JRS) was introduced as part of [FLIP-194|https://cwiki.apache.org/confluence/display/FLINK/FLIP-194%3A+introduce+the+jobresultstore]. For now, it only stores the JobResult. Through the JRS, jobs can be marked as finished even when the JobManager fails and the information from the {{ExecutionGraphInfoStore}} is lost (see FLINK-11813).

While implementing {{FLIP-194}}, it became apparent, that we have some redundancy between the JRS and the {{ExecutionGraphInfoStore}}. Both components store some meta information of a finished job. The {{ExecutionGraphInfoStore}} is used to make information about the finished job available in user-facing APIs (REST, web-UI). The JRS is used to expose the job's state to the cleanup logic and stores limited data.

This proposal is about merging the two and making the {{ArchivedExecutionGraph}} information available even after a JobManager is restarted. That way, completed jobs can be still listed in the job overview after a Flink cluster restart. Additionally, we could provide the last checkpoint information. The JRS would be a way to access this information even after the Flink cluster is shut down. The latter feature would be also a way to improve the Flink Kubernetes Operator's latest-state handling.

I want to point out that this issue is just a braindump. It might require coming up with a proper FLIP since we need to discuss what information should be made available through the JRS and whether that's considered public API in some way.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-03 12:35:24.0,,,,,,,,,,"0|z1h21k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RuntimeException/KryoException thrown when deserializing an empty protobuf record,FLINK-31708,13531261,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,shenjiaqi,shenjiaqi,shenjiaqi,03/Apr/23 11:39,20/Apr/23 07:05,04/Jun/24 20:41,20/Apr/23 07:05,1.10.0,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Type Serialization System,,,,,,0,pull-request-available,,,,,"h1. Problem description

I am using protobuf defined Class in Flink job. When the application runs on production, the job throws following Exception:
{code:java}
java.lang.RuntimeException: Could not create class com.MYClass <==== generated by protobuf
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:76)
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40)
        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
        at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:346)
        at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:205)
        at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46)
        at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:55)
        at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:141)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:121)
        at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:185)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:319)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:494)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:478)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)
        at java.lang.Thread.run(Thread.java:748)
Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: No more bytes left.
        at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:127)
        at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:332)
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73)
        ... 16 common frames omitted
 {code}
h1. How to reproduce

I think this is similar to another issue: FLINK-29347.

Follwing is an example to reproduce the problem:
{code:java}
package com.test;

import com.test.ProtobufGeneratedClass;

import com.google.protobuf.Message;
import com.twitter.chill.protobuf.ProtobufSerializer;
import lombok.extern.slf4j.Slf4j;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.restartstrategy.RestartStrategies;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.api.common.time.Time;
import org.apache.flink.api.java.utils.MultipleParameterTool;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.streaming.api.datastream.BroadcastStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.co.KeyedBroadcastProcessFunction;
import org.apache.flink.streaming.api.functions.sink.SinkFunction;
import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;
import org.apache.flink.util.Collector;
import org.apache.flink.util.OutputTag;

import java.util.Random;
@Slf4j
public class app {
  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_1 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-1"") {
  };

  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_2 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-2"") {
  };

  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_3 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-3"") {
  };

  public static class MySourceFunction extends RichParallelSourceFunction<ProtobufGeneratedClass> {
    Random rnd = new Random();
    private final String name;

    private boolean running = true;

    private MySourceFunction(String name) {
      this.name = name;
    }

    @Override
    public void run(SourceContext<ProtobufGeneratedClass> sourceContext) throws Exception {
      final int index = getRuntimeContext().getIndexOfThisSubtask();
      int counter = 0;

      while (running) {
        synchronized (sourceContext.getCheckpointLock()) {
          ++counter;
          ProtobufGeneratedClass.Builder builder = ProtobufGeneratedClass.newBuilder();
          if (rnd.nextBoolean()) {

            builder.addGraphIds(rnd.nextInt(10));
            byte[] bytes;
            if (rnd.nextInt(10) == 1) {
              // make sure record is large enough to reproduce the problem
              // in which case, SpillingAdaptiveSpanningRecordDeserializer#spanningWrapper may be activated
              bytes = new byte[rnd.nextInt(5000000)];
            } else if (rnd.nextInt(10) == 2) {
              bytes = new byte[rnd.nextInt(50000)];
            } else {
              bytes = new byte[rnd.nextInt(50)];
            }
            builder.addUserTagNames(new String(bytes));
          } else {
				// create an empty record by do nothing.
          }
          sourceContext.collect(builder.build());
          Thread.sleep(5);
        }
      }
    }

    @Override
    public void cancel() {
      running = false;
    }
  }

  public static void main(String[] args) throws Exception {

    final int SHARD_NUM = 64;
    final MultipleParameterTool params = MultipleParameterTool.fromArgs(args);

    // set up the execution environment
    Configuration config = new Configuration();
    config.setInteger(""state.checkpoints.num-retained"", 5);
    config.setInteger(""taskmanager.numberOfTaskSlots"", 1);
    config.setInteger(""local.number-taskmanager"", 4);
    StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(3, config);

    RocksDBStateBackend rocksDBStateBackend =
        new RocksDBStateBackend(""file:///Users/shenjiaqi/Workspace/state/checkpoints/"", true);

    env.setParallelism(3);
    env.setStateBackend(rocksDBStateBackend);
    env.getCheckpointConfig().setCheckpointTimeout(100000);
    env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1000, Time.seconds(10)));
    env.addDefaultKryoSerializer(Message.class, ProtobufSerializer.class); // make sure ProtobufSerializer is serialized/deserialized by protobuf.

    // make parameters available in the web interface
    env.getConfig().setGlobalJobParameters(params);

    String[] words = new String[100000];
    Random rnd = new Random();
    for (int i = 0; i < words.length; ++i) {
      words[i] = String.valueOf(rnd.nextInt(10));
    }
    DataStreamSource<ProtobufGeneratedClass> stream1 = env.addSource(new MySourceFunction(""randomProtobufGeneratedClass1"")).setParallelism(4);
    BroadcastStream<ProtobufGeneratedClass> stream2 = env.addSource(new MySourceFunction(""randomProtobufGeneratedClass2"")).setParallelism(3)
        .broadcast(new MapStateDescriptor[0]);
    SingleOutputStreamOperator<ProtobufGeneratedClass> output = stream1.shuffle()
        .map(new MapFunction<ProtobufGeneratedClass, ProtobufGeneratedClass>() {

          @Override
          public ProtobufGeneratedClass map(ProtobufGeneratedClass value) throws Exception {
            return value;
          }
        }).setParallelism(2).disableChaining()
        .keyBy(x -> x.hashCode() % 10)
        .connect(stream2)
        .process(new MyProcessFunction()).disableChaining();

    output.getSideOutput(OUTPUT_TAG_1).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 1"");
          }
        }).setParallelism(1);

    output.getSideOutput(OUTPUT_TAG_2).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 2"");
          }
        }).setParallelism(2);

    output.getSideOutput(OUTPUT_TAG_3).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 3"");
          }
        }).setParallelism(3);

    output.map(new MapFunction<ProtobufGeneratedClass, String>() {
      @Override
      public String map(ProtobufGeneratedClass value) throws Exception {
        return """" + value.toString().length();
      }
    }).print();
    env.execute(""reproduce-the-problem"");
  }

  public static class MyProcessFunction extends
      KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass> {

    @Override
    public void processElement(ProtobufGeneratedClass ProtobufGeneratedClass,
        KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass>.ReadOnlyContext readOnlyContext,
        Collector<ProtobufGeneratedClass> collector) throws Exception {
      collector.collect(ProtobufGeneratedClass);
    }

    @Override
    public void processBroadcastElement(ProtobufGeneratedClass s,
        KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass>.Context context,
        Collector<ProtobufGeneratedClass> collector) throws Exception {
      context.output(OUTPUT_TAG_1, s);
      context.output(OUTPUT_TAG_2, s);
      context.output(OUTPUT_TAG_3, s);
    }
  }
}

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 07:02:12 UTC 2023,,,,,,,,,,"0|z1h1y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 12:09;shenjiaqi;pull request:https://github.com/apache/flink/pull/22335;;;","20/Apr/23 07:02;gaoyunhaii;Merged on master via d9e9d1ca741c84a18dbfac0ed44bd38cb5e11f3f

Merged on release-1.17 via 729043df687a96711d3591fcdf5e8e712cd21b87

Merged on release-1.16 via 389389fabd0eeb7894065e42f395db67f3e722e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Constant string cannot be used as input arguments of Pandas UDAF,FLINK-31707,13531260,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,03/Apr/23 11:39,04/Apr/23 05:26,04/Jun/24 20:41,04/Apr/23 02:18,,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,API / Python,,,,,,0,pull-request-available,,,,,"It will throw exceptions as following when using constant strings in Pandas UDAF:
{code}
E                       raise ValueError(""field_type %s is not supported."" % field_type)
E                   ValueError: field_type type_name: CHAR
E                   char_info {
E                     length: 3
E                   }
E                    is not supported.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 02:18:01 UTC 2023,,,,,,,,,,"0|z1h1y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 02:18;dianfu;Fixed in:
- master via 7c6d8b0134cbcdc60d56b87d39ff2f28c310b1eb
- release-1.17 via 9c5ca0590806932e4e8f9d3f942f0a2a5442fe2d
- release-1.16 via 3291e4d6f9afff40e1e9718e23388610577de741;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The default source parallelism should be the same as execution's default parallelism under adaptive batch scheduler,FLINK-31706,13531254,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunta,yunta,03/Apr/23 11:24,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"Currently, the sources need to set {{execution.batch.adaptive.auto-parallelism.default-source-parallelism }} in the adaptive batch scheduler mode, otherwise, the source parallelism is only 1 by default. A better solution might be set as the default execution parallelism if no user configured. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 12 13:13:54 UTC 2023,,,,,,,,,,"0|z1h1wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 11:25;yunta;cc [~zhuzh];;;","11/May/23 13:36;wanglijie;I think it's a good idea to use {{paralleism.default}} instread of the {{execution.batch.adaptive.auto-parallelism.default-source-parallelism}}.

Regarding the parallelism of Source in the adaptive batch scheduler, we also have some other ideas/actions in plan: dynamically infer the Source paralleism at runtime (according to the amount of data that Source actually needs to read after Dynamic Partition Pruning). One possible way is that the source coordinator can infer the parallelism based on the splits information actually consumed.

At that time, if the parallelism of Source are not specified by the user, the source coorinator will be responseible for inferring the parallelism automatically(if it supports). If the Source does not support inferring parallelism automatically, {{parallelism.default}} will be used as the parallelism of the Source. (An initial thought :));;;","12/May/23 00:21;zxcoccer;I am currently implementing the use `paralleism.default` instread of the `execution.batch.adaptive.auto-parallelism.default-source-parallelism.` in [pr-22555|[https://github.com/apache/flink/pull/22555|https://github.com/apache/flink/pull/22555)]]

can you spare some precious time to review and make some comments? thanks a lot.

[~wanglijie] ;;;","12/May/23 03:39;wanglijie;Thanks for your attention and effort, but we cannot do it now, because it will change the behavior of {{parallelism.default}} and {{execution.batch.adaptive.auto-parallelism.default-source-parallelism}} , needs to be discussed and approved on the mailing list (See [FLIP|https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals] for details).

In addition, as I said above, we have some other ideas about Source paralleism in adaptive batch scheduler, and hope to merge the idea here into one FLIP and discuss them together on the mailing list.

This FLIP is in preparation and we will drive a discussion on the mailing list once it is ready. If you are interested, welcome to participate in the discussion and coding at that time.

WDYT? [~zxcoccer] ;;;","12/May/23 13:13;zxcoccer;(y)(y):D Thank you for your reply, I see [~wanglijie] ;;;",,,,,,,,,,,,,,,,,,,,,,
Remove Conjars,FLINK-31705,13531252,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,03/Apr/23 11:23,04/Apr/23 07:53,04/Jun/24 20:41,04/Apr/23 07:53,,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Build System,,,,,,0,pull-request-available,,,,,"With Conjars no longer being available (only https://conjars.wensel.net/ is there), we should remove all the notices to Conjars in Flink. We've already removed the need for Conjars because we've excluded Pentaho as part of FLINK-27640, which eliminates having any dependency that relies on Conjars. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31714,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 07:53:35 UTC 2023,,,,,,,,,,"0|z1h1w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 07:53;martijnvisser;Fixed in

master: 7ee5780e77de5d272d06de85f2c21757fa88b764
release-1.17: e6acbcb652e8cd289d047ba1be73945ae98bdb87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar docs should be pulled from dedicated branch,FLINK-31704,13531243,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,dannycranmer,dannycranmer,03/Apr/23 10:58,07/Jun/23 15:35,04/Jun/24 20:41,06/Apr/23 07:34,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Connectors / Pulsar,Documentation,,,,,0,pull-request-available,,,,,"Pulsar docs are pulled from the {{main}} [branch|https://github.com/apache/flink/blob/release-1.17/docs/setup_docs.sh#L49]. This is dangerous for final versions since we may include features in the docs that are not supported. Update Pulsar to pull from a dedicated branch or tag.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 07 15:35:59 UTC 2023,,,,,,,,,,"0|z1h1u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 12:19;Weijie Guo;Thanks [~dannycranmer], I think {{v3.0}} is the right branch/tag. But there is a problem that the v3.0 branch in {{flink-pulsar-connector}} uses {{<artifact flink-connector-pulsar>}} as the shortcode 'artifact', which will cause the version in the documentation to be incorrectly replaced with flink-version. 
We should probably fix this before making the flink repository track the v3.0 branch. And I'd like to take this ticket if you are not start to do this now.

I have also noticed that {{ElsaticSearch}} connector has the same problem, see FLINK-31670. 
;;;","03/Apr/23 12:27;dannycranmer;[~Weijie Guo] we had the same issue in MongoDB and I created a dedicated {{v1.0.0-docs}} branch. I think we should do the same here, and can delete it once the next version is released. ;;;","03/Apr/23 12:33;Weijie Guo;[~dannycranmer], Introduce a new branch is indeed a feasible way. But I want to share another idea. 

I had an offline discussion with [~renqs], and the conclusion is that for external connectors, Maybe we can let flink track a stable branch, but not a released tag. The advantage of this approach is that users can see our improvements and fixes to the document timely. At the same time, since we are based on a stable branch that already has a release version and will not introduce new features, there are no problems like this ticket. WDYT?

;;;","03/Apr/23 18:55;dannycranmer;[~Weijie Guo] it sounds like you are proposing something similar to the `main` branch in the gitflow model [1]. This would work but would require 1 stable branch per version line (3.x, 4.x, 5.x etc). This would not work for the case when we drop support for old Flink versions. For example if we reference the stable branch 3.x in Flink 1.16, once Flink 1.18 is released we would want to stop taking updates for the 1.16 branch.

We could also consider mutable tags for docs. For example, if Flink references the tag `v1.0.0-docs` then we could rewrite this tag as required in the connector repo. This has the same issue as above for old Flink versions.

For now I think being pragmatic and using the tag where possible, and a dedicated branch when not possible is a good compromise. For example, MongoDB now uses {{v1.0.0-docs}} branch and AWS connectors uses {{4.1.0-docs}} branch, both due to build issues :( 

[1] https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow;;;","04/Apr/23 03:13;Weijie Guo;Thanks [~dannycranmer], fair enough. I will create the 3.0.0-docs branch for pulsar connector and delete it once the next version is released.;;;","06/Apr/23 07:34;Weijie Guo;master(1.18) via 887464f150bc4eefda6fe226a2e9f3ad31db372a.
release-1.17 via b1b87252a0809fb4f25e9b182eb93611b7a918ed.;;;","07/Jun/23 14:10;aitozi;Hi [~Weijie Guo] [~dannycranmer] I have a CI failed with missing branch 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49737&view=logs&j=c5d67f7d-375d-5407-4743-f9d0c4436a81&t=38411795-40c9-51fa-10b0-bd083cf9f5a5&l=27

The target 3.0.0-docs do not exists in the flink-connector-pulsar. Does the branch name should be change ?;;;","07/Jun/23 14:48;Weijie Guo;[~aitozi] The current master branch should no longer track `xxx-docs` branch of externalized connectors. Would you like to try rebase your pull request on the latest master?;;;","07/Jun/23 15:35;aitozi;thanks, will try it.;;;",,,,,,,,,,,,,,,,,,
Update Flink docs for AWS v4.1.0,FLINK-31703,13531241,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,03/Apr/23 10:56,03/Apr/23 16:35,04/Jun/24 20:41,03/Apr/23 16:35,,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Connectors / AWS,Documentation,,,,,0,pull-request-available,,,,,Update Flink docs for 1.16/1.17/1.18 to pull in the AWS connector docs for v4.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31702,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 16:35:49 UTC 2023,,,,,,,,,,"0|z1h1ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 16:35;dannycranmer;Merged commit [{{2f3b701}}|https://github.com/apache/flink/commit/2f3b7016dcdcb1e21f817165091b5dffe9d19fa1] into apache:master
Merged commit [{{40ee201}}|https://github.com/apache/flink/commit/40ee2019c61736865c09732f431c545f09ffe665] into apache:release-1.17
Merged commit [{{95db617}}|https://github.com/apache/flink/commit/95db6179d509f122b6099118e9c3789d1e497c3d] into apache:release-1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate Opensearch connector docs into Flink docs v1.17/master,FLINK-31702,13531237,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,dannycranmer,dannycranmer,03/Apr/23 10:31,10/Apr/23 10:03,04/Jun/24 20:41,10/Apr/23 10:03,,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Connectors / Opensearch,Documentation,,,,,0,pull-request-available,,,,,Update Flink docs for 1.17/1.18 to pull in the opensearch docs from flink-connector-opensearch repo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31703,FLINK-30436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 10 10:03:03 UTC 2023,,,,,,,,,,"0|z1h1sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Apr/23 08:12;dannycranmer;Assigning to [~darenwkt] as discussed offline;;;","10/Apr/23 10:03;dannycranmer;Merged to master [https://github.com/apache/flink/commit/65710b437318364ec19c0369d038ac2222c10498]

Merged to 1.17 https://github.com/apache/flink/commit/ca18f203b1d5e6c322421a48ad20473b53416177;;;",,,,,,,,,,,,,,,,,,,,,,,,,
RabbitMQ nightly CI failure,FLINK-31701,13531226,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,dannycranmer,dannycranmer,03/Apr/23 09:52,04/Apr/23 12:28,04/Jun/24 20:41,04/Apr/23 09:18,,,,,,,,,,,,,,,,,,,,,rabbitmq-3.0.1,,,,,,,,,Connectors/ RabbitMQ,,,,,,0,pull-request-available,,,,,"Investigate and fix the nightly CI failure. Example https://github.com/apache/flink-connector-rabbitmq/actions/runs/4585880781

 
{code:java}
Error:  /home/runner/work/flink-connector-rabbitmq/flink-connector-rabbitmq/flink-connector-rabbitmq/src/test/java/org/apache/flink/streaming/connectors/rabbitmq/RMQSourceITCase.java:[89,76] cannot find symbol
124  symbol:   variable RABBITMQ
125  location: class org.apache.flink.util.DockerImageVersions{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31700,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30626,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 12:28:01 UTC 2023,,,,,,,,,,"0|z1h1qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 09:18;martijnvisser;Include local DockerImageVersions instead of relying on the one from the Flink repository: 650e8fceaab6ffcaa53f8fd83a61f4b4dcc39122

Upgrade and sync related dependencies from Flink 1.16.0 to Flink 1.17.0 / Flink 1.17.0-SNAPSHOT: 4b2a19868fcdde8de1bb8cbd9464869501b63a86

Upgrade Archunit to 1.0.0: 2c0afa421ac74ce70b8a48ddc82bea62745bb122

Set japicmp.referenceVersion to the version of the latest released connector: 0a4c5be21eeea0d7ef45602a5b8ff200b6b5c5bb;;;","04/Apr/23 12:28;martijnvisser;Remove versioned Archunit: ab920cbdfec51be108508e2f09a1cd9cd3bf4f4c;;;",,,,,,,,,,,,,,,,,,,,,,,,,
MongoDB nightly CI failure,FLINK-31700,13531225,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,dannycranmer,dannycranmer,03/Apr/23 09:51,13/Apr/23 15:07,04/Jun/24 20:41,13/Apr/23 15:07,,,,,,,,,,,,,,,,,,,,,mongodb-1.0.1,,,,,,,,,Connectors / MongoDB,,,,,,0,pull-request-available,,,,,"Investigate and fix the nightly CI failure. Example https://github.com/apache/flink-connector-mongodb/actions/runs/4585933750

 
{code:java}
Error:  /home/runner/work/flink-connector-mongodb/flink-connector-mongodb/flink-connector-mongodb/src/main/java/org/apache/flink/connector/mongodb/sink/writer/context/DefaultMongoSinkContext.java:[33,8] org.apache.flink.connector.mongodb.sink.writer.context.DefaultMongoSinkContext is not abstract and does not override abstract method getAttemptNumber() in org.apache.flink.api.connector.sink2.Sink.InitContext{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31701,FLINK-31699,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 14:56:23 UTC 2023,,,,,,,,,,"0|z1h1q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 10:00;jiabao.sun;Hi [~dannycranmer].

Do we need to upgrade the flink version of flink-connector-mongodb to 1.17.0?;;;","03/Apr/23 10:05;dannycranmer;[~jiabao.sun] ideally the connector should have supported both Flink 1.17 and 1.16 in parallel. Seems like v1.0.0 might not build against Flink 1.17. If this is the case we will need to do a v1.1.0 that will support Flink 1.16 and 1.17. Yes we should update the default version to Flink 1.17.0 now. Shall I assign this to you?;;;","03/Apr/23 10:08;jiabao.sun;Thanks Danny.
Please assign this to me.;;;","12/Apr/23 12:42;dannycranmer;Merged commit [{{19673ca}}|https://github.com/apache/flink-connector-mongodb/commit/19673caaaa260b65e5550489d731da417e74e2f7] into apache:main ;;;","12/Apr/23 12:48;dannycranmer;[~jiabao.sun] the CI is till failing on arch unit [https://github.com/apache/flink-connector-mongodb/actions/runs/4678420809/jobs/8287130965,] can you take a look?;;;","12/Apr/23 12:59;jiabao.sun;Thanks [~dannycranmer].

I think we should specific the archunit.version from 0.22.0 to 1.0.0 when build with flink 1.17.0.
I'm not sure what the solution is other than to specify the version of archunit.version, but using an older version of archunit doesn't seem to find arch test cases correctly under flink 1.17.0.;;;","12/Apr/23 13:45;jiabao.sun;Fixed by https://github.com/apache/flink-connector-mongodb/pull/6;;;","13/Apr/23 09:48;jiabao.sun;Hey [~dannycranmer].
Could you help close this issue?;;;","13/Apr/23 14:56;dannycranmer;CI now passes on main [https://github.com/apache/flink-connector-mongodb/actions/runs/4690597221,] thanks [~jiabao.sun] 

 

And on v1.0 https://github.com/apache/flink-connector-mongodb/actions/runs/4690775426;;;",,,,,,,,,,,,,,,,,,
JDBC nightly CI failure,FLINK-31699,13531224,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,dannycranmer,dannycranmer,03/Apr/23 09:50,13/Apr/23 13:49,04/Jun/24 20:41,13/Apr/23 13:49,,,,,,,,,,,,,,,,,,,,,jdbc-3.1.0,,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,"Investigate and fix the nightly CI failure. Example [https://github.com/apache/flink-connector-jdbc/actions/runs/4585903259]

 
{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-jdbc: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(Ljava/lang/Class;)Ljava/lang/Object; -> [Help 1]{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31700,FLINK-31698,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31624,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 13:49:39 UTC 2023,,,,,,,,,,"0|z1h1q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 11:01;ruanhang1993;Hi, Danny, I would like to help. Please assign this to me, thanks ~;;;","03/Apr/23 11:15;dannycranmer;[~ruanhang1993] thanks, have assigned to you;;;","03/Apr/23 14:39;matriv;[~ruanhang1993] FYI, I had such failures when using jdk > 8 to build locally.

Also, maybe worths checking: [https://github.com/apache/flink-connector-jdbc/pull/30] and [https://github.com/apache/flink-connector-jdbc/pull/31]

if they address the issue.;;;","04/Apr/23 17:44;martijnvisser;Fixed in main: 90179da68dacd6e65a399ae8e8b7c453edbfbb80;;;","05/Apr/23 02:55;ruanhang1993;Thanks for suggestions. Actually I apply the changes in PR#31 and PR#30. This test error may still occur.

I will use the Martijn's fixed version to do more tests. Thanks ~;;;","13/Apr/23 09:38;dannycranmer;CI is still failing for v3.0 branch [https://github.com/apache/flink-connector-jdbc/actions/runs/4687686153/jobs/8307285788]

 ;;;","13/Apr/23 13:49;dannycranmer;main branch is good https://github.com/apache/flink-connector-jdbc/actions/runs/4689703187;;;",,,,,,,,,,,,,,,,,,,,
Cassandra nightly CI failure,FLINK-31698,13531223,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,dannycranmer,dannycranmer,03/Apr/23 09:49,19/Apr/24 09:25,04/Jun/24 20:41,24/Apr/23 12:46,,,,,,,,,,,,,,,,,,,,,cassandra-3.2.0,,,,,,,,,Connectors / Cassandra,,,,,,0,pull-request-available,,,,,"Investigate and fix the nightly CI failure. Example [https://github.com/apache/flink-connector-cassandra/actions/runs/4585936901]

 
{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-cassandra_2.12: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'junit-jupiter' failed to discover tests: org/junit/jupiter/api/io/CleanupMode: org.junit.jupiter.api.io.CleanupMode -> [Help 1]{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31699,FLINK-31697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-04-03 09:49:32.0,,,,,,,,,,"0|z1h1ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenSearch nightly CI failure,FLINK-31697,13531221,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,reta,dannycranmer,dannycranmer,03/Apr/23 09:48,03/Apr/23 17:02,04/Jun/24 20:41,03/Apr/23 17:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Opensearch,,,,,,0,pull-request-available,,,,,"Investigate and fix the nightly CI failure. Example [https://github.com/apache/flink-connector-opensearch/actions/runs/4585851921]

 

 
{code:java}
Error: Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-opensearch: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: 'java.lang.Object com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(java.lang.Class)' -> [Help 1]{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31698,FLINK-31696,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 17:02:33 UTC 2023,,,,,,,,,,"0|z1h1pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 11:12;martijnvisser;This is caused by Flink being updated to a newer version of Archunit, while Elasticsearch hasn't been updated for that issue yet. ;;;","03/Apr/23 17:02;Sergey Nuyanzin;Merged to main as a0569724ae41ff663a6ffcb07385f223d03f202e;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Flink 1.18,FLINK-31696,13531220,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,reta,dannycranmer,dannycranmer,03/Apr/23 09:47,13/Apr/23 09:12,04/Jun/24 20:41,11/Apr/23 18:23,,,,,,,,,,,,,,,,,,,,,elasticsearch-3.1.0,,,,,,,,,Connectors / ElasticSearch,,,,,,0,pull-request-available,,,,,"Investigate and fix the nightly CI failure. Example [https://github.com/apache/flink-connector-elasticsearch/actions/runs/4585918498/jobs/8098357503]

 
{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-elasticsearch-base: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: 'java.lang.Object com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(java.lang.Class)' -> [Help 1] {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 11 18:23:05 UTC 2023,,,,,,,,,,"0|z1h1p4:",9223372036854775807,"This version of the Elasticsearch connector includes support for Flink 1.18.x, while dropping support for Flink 1.16.x",,,,,,,,,,,,,,,,,,,"03/Apr/23 11:12;martijnvisser;This is caused by Flink being updated to a newer version of Archunit, while Elasticsearch hasn't been updated for that issue yet. ;;;","11/Apr/23 18:23;martijnvisser;Fixed in main: 85f0f4057a6b241a7d9b0ae2996ac1147e0e2428;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Calling bin/flink stop when flink-shaded-hadoop-2-uber-2.8.3-10.0.jar is in lib directory throws NoSuchMethodError,FLINK-31695,13531214,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,TsReaper,TsReaper,03/Apr/23 09:07,04/Apr/23 01:39,04/Jun/24 20:41,,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,Command Line Client,,,,,,0,,,,,,"To reproduce this bug, follow these steps:

1. Download [flink-shaded-hadoop-2-uber-2.8.3-10.0.jar|https://mvnrepository.com/artifact/org.apache.flink/flink-shaded-hadoop-2-uber/2.8.3-10.0] and put it in the {{lib}} directory.
2. Run {{bin/flink stop}}

The exception stack is

{code}
java.lang.NoSuchMethodError: org.apache.commons.cli.CommandLine.hasOption(Lorg/apache/commons/cli/Option;)Z
	at org.apache.flink.client.cli.StopOptions.<init>(StopOptions.java:53)
	at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:539)
	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1102)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1165)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1165)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25744,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 09:32:29 UTC 2023,,,,,,,,,,"0|z1h1ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 09:32;martijnvisser;I would expect this not to work for 1.17, given we've bumped the minimum Hadoop version to 2.10.2 with FLINK-29710;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Bump ua-parser-js from 0.7.31 to 0.7.33,FLINK-31694,13531187,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,03/Apr/23 08:20,03/Apr/23 12:45,04/Jun/24 20:41,03/Apr/23 12:45,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Web Frontend,,,,,,0,,,,,,Dependabot PR: https://github.com/apache/flink/pull/21767,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 12:45:20 UTC 2023,,,,,,,,,,"0|z1h1hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 12:45;martijnvisser;Fixed in master: ab7b2b308ab6bd1e0db7bae8b0a97702e0509661;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Bump http-cache-semantics from 4.1.0 to 4.1.1,FLINK-31693,13531185,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,03/Apr/23 08:17,03/Apr/23 12:44,04/Jun/24 20:41,03/Apr/23 12:44,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Web Frontend,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 12:44:44 UTC 2023,,,,,,,,,,"0|z1h1hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 08:17;martijnvisser;Dependabot PR https://github.com/apache/flink/pull/21850;;;","03/Apr/23 12:44;martijnvisser;Fixed in master: 77b1d8f70156396ea4b47d5fd6cfea64d81d1390;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Integrate MongoDB connector docs into Flink website,FLINK-31692,13531182,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,03/Apr/23 07:59,03/Apr/23 11:57,04/Jun/24 20:41,03/Apr/23 11:57,,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Connectors / MongoDB,,,,,,0,pull-request-available,,,,,"Update Flink docs build to pull the MongoDB connector docs [1]

 

 

[1] https://github.com/apache/flink-connector-mongodb",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 11:57:45 UTC 2023,,,,,,,,,,"0|z1h1go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 11:57;dannycranmer;Merged commit [{{803a4c6}}|https://github.com/apache/flink/commit/803a4c6454e760bb5d846f1bc88818f4ede34626] into apache:master
Merged commit [{{319b07d}}|https://github.com/apache/flink/commit/319b07d251902f2d387a503ac9a0357c2dd519b3] into apache:release-1.17
Merged commit [{{3bdb855}}|https://github.com/apache/flink/commit/3bdb85582975e0f14313a0d1a7bceb3cd7fc0cc3] into apache:release-1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add MAP_FROM_ENTRIES supported in SQL & Table API,FLINK-31691,13531154,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,03/Apr/23 04:08,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,stale-assigned,,,,"map_from_entries(map) - Returns a map created from an arrays of row with two fields. Note that the number of fields in a row array should be 2 and the key of a row array should not be null.

Syntax:
map_from_entries(array_of_rows)

Arguments:
array_of_rows: an arrays of row with two fields.

Returns:

Returns a map created from an arrays of row with two fields. Note that the number of fields in a row array should be 2 and the key of a row array should not be null.

Returns null if the argument is null
{code:sql}
> SELECT map_from_entries(map[1, 'a', 2, 'b']);
 [(1,""a""),(2,""b"")]{code}
See also
presto [https://prestodb.io/docs/current/functions/map.html]

spark https://spark.apache.org/docs/latest/api/sql/index.html#map_from_entries",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 19 07:42:06 UTC 2024,,,,,,,,,,"0|z1h1ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 11:48;jackylau;hi [~snuyanzin] do you have time to help review this pr https://github.com/apache/flink/pull/22745? ;;;","26/Jun/23 13:13;Sergey Nuyanzin;[~jackylau] i see there are several PRs opened for this issue
Could you please close all except the one which is actual first?;;;","26/Jun/23 14:43;jackylau;hi [~Sergey Nuyanzin] if you review this pr [https://github.com/apache/flink/pull/22745] you will find i comment the reason why need a new pr.

because the first pr just aligns with spark. but spark has two behavior last_win/exception, and calcite and flink  map both only supports last_win. so i just do not break the current behavior now. when we need support it, i will discuss in the mail to supports exception behavior of all map function.;;;","12/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","19/Jan/24 07:42;jackylau;hi [~Sergey Nuyanzin] will you help to review this pr again [https://github.com/apache/flink/pull/22745] , it is for long time.

and i rebase it to fix conflict;;;",,,,,,,,,,,,,,,,,,,,,,
The current key is not set for KeyedCoProcessOperator,FLINK-31690,13531142,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,03/Apr/23 02:32,03/Apr/23 05:44,04/Jun/24 20:41,03/Apr/23 05:44,,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,API / Python,,,,,,0,pull-request-available,,,,,See https://apache-flink.slack.com/archives/C03G7LJTS2G/p1680294701254239 for more details.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 05:44:33 UTC 2023,,,,,,,,,,"0|z1h17s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 05:44;dianfu;Fixed in:
- master via 6c1ffe544e31bb67df94175a559f2f40362795a4
- release-1.17 via e3d612e7e98bedde42c365df3f2ed2a2ca76aefa
- release-1.16 via 01cdaee25cdc41773a5c42638f4c5209373b5aa4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem sink fails when parallelism of compactor operator changed,FLINK-31689,13531127,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jirawech.s,jirawech.s,jirawech.s,02/Apr/23 16:09,30/Sep/23 02:14,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,,0,pull-request-available,,,,,"I encounter this error when i tried to use Filesystem sink with Table SQL. I have not tested with Datastream API tho. You may refers to the error as below
{code:java}
// code placeholder
java.util.NoSuchElementException
	at java.util.ArrayList$Itr.next(ArrayList.java:864)
	at org.apache.flink.connector.file.table.stream.compact.CompactOperator.initializeState(CompactOperator.java:119)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:286)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:750) {code}
I cannot attach the full reproducible code here, but you may follow my pseudo code in attachment and reproducible steps below
1. Create Kafka source

2. Set state.savepoints.dir

3. Set Job parallelism to 1

4. Create FileSystem Sink

5. Run the job and trigger savepoint with API
{noformat}
curl -X POST localhost:8081/jobs/:jobId/savepoints -d '{""cancel-job"": false}'{noformat}
{color:#172b4d}6. Cancel job, change parallelism to 2, and resume job from savepoint{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/23 16:09;jirawech.s;HelloFlinkHadoopSink.java;https://issues.apache.org/jira/secure/attachment/13056978/HelloFlinkHadoopSink.java",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,English,,Sat Sep 30 02:14:30 UTC 2023,,,,,,,,,,"0|z1h14g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 03:11;luoyuxia;Please remember the state is not compatible as you have changed parallelism. So, it throw the exception.;;;","03/Apr/23 09:56;jirawech.s;Is this normal behavior? If i use Kafka Sink, i am able to increase/decrease parallelism fine. ;;;","03/Apr/23 11:29;luoyuxia;[~jirawech.s] Yes, it's normal behavior. Maybe you can disable auto compaction and try it again.  By say ""the state"", I mean the state of {{CompactOperator}} in File Sink. Other sinks have their own implementation which can be state compatible.

But after diving into the {{{}CompactOperator{}}}, I think  it can be implemented in state compatible style which then won't throw exception though we change parallelism.;;;","03/Apr/23 16:26;jirawech.s;[~luoyuxia] I see. So, we could say that it is normal behaviour of CompactOperator in File Sink for now. If we were to improve, we could do that by implementing state compatible CompactOperator right? Could you point me to the code/class i should check out. I am not so familiar with Flink development;;;","04/Apr/23 01:47;luoyuxia;It's in [here|https://github.com/apache/flink/blob/0915c9850d861165e283acc0f60545cd836f0567/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/stream/compact/CompactOperator.java#L114].

I think the these code line :
{code:java}
this.expiredFiles.putAll(this.expiredFilesState.get().iterator().next()); {code}
, we can check where this.expiredFilesState.get().iterator().hasNext(), and then put. 

I think you can modify this code line, build file-connector, and then try it again to see whether it works.;;;","04/Apr/23 07:26;jirawech.s;[~luoyuxia] Thank you so much for detailed explanation.
What about i tried on my local, built and tested it and PR back? I see that i need this first to contribute back
*Only start working on the implementation if there is consensus on the approach (e.g. you are assigned to the ticket)*

[link|https://flink.apache.org/how-to-contribute/overview/];;;","05/Apr/23 12:57;jirawech.s;[~luoyuxia] Is it possible that you assign me to do this task? Or what do i do next?;;;","06/Apr/23 03:51;luoyuxia;[~jirawech.s] Assign to you~ You can tried in your local and pr back. I can help view.;;;","14/Apr/23 09:29;jirawech.s;[~luoyuxia] I open [PR|https://github.com/apache/flink/pull/22400]. Could you help me review?;;;","29/May/23 10:58;jirawech.s;Anyone can help me review PR pls

https://github.com/apache/flink/pull/22400;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","13/Aug/23 03:16;jirawech.s;waiting for PR review https://github.com/apache/flink/pull/22400;;;","12/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Sep/23 02:14;jirawech.s;[~martijnvisser] Could you help with this PR, seems like no one review it;;;",,,,,,,,,,,,,
Broken links in docs for Azure Table Storage,FLINK-31688,13531121,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,02/Apr/23 13:40,03/Apr/23 05:39,04/Jun/24 20:41,03/Apr/23 05:39,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,"The doc page of https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/dataset/formats/azure_table_storage/ has a broken links, we should fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 05:39:29 UTC 2023,,,,,,,,,,"0|z1h134:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 05:39;Weijie Guo;master(1.18) via 6101ad313be78a658cc25487b9d3976252625feb.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Flink jdbc driver get rid of flink core,FLINK-31687,13531101,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,02/Apr/23 02:59,19/May/23 11:03,04/Jun/24 20:41,19/May/23 11:03,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 19 11:03:25 UTC 2023,,,,,,,,,,"0|z1h0yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/23 11:03;libenchao;Fixed via https://github.com/apache/flink/commit/06688f345f6793a8964ec00002175f44cda13c33

[~zjureel] Thanks for the PR!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem connector should replace the shallow copy with deep copy,FLINK-31686,13531065,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanjialiang,tanjialiang,01/Apr/23 08:18,10/Apr/23 02:47,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,,0,,,,,,"Hi team, when i using the following sql
{code:java}
CREATE TABLE student (
    `id` STRING,
    `name` STRING,
    `age` INT
) WITH (
  'connector' = 'filesystem',
  'path' = '...',
  'format' = 'orc'
);

select
    t1.total,
    t2.total
from
    (
        select
            count(*) as total,
            1 as join_key
        from student
        where name = 'tanjialiang'
    ) t1
    LEFT JOIN (
        select
            count(*) as total,
            1 as join_key
        from student;
    ) t2 
    ON t1.join_key = t2.join_key; {code}
 

it will throw an error

!image-2023-04-01-16-18-48-762.png!

 

I tried to solve it, and i found filesystem connector's copy function using a shallow copy instread of deep copy.   It lead to all of query from a same table source reuse the same bulkWriterFormat, and my query have filter condition, which will push down into the bulkWriterFormat, so the filter condition maybe reuse.   

I found the DynamicTableSource and DynamicTableSink's copy function comment to ask we should impletement it with deep copy, but i found every connector are using shallow copy to impletement it.   So i think not only the filesystem connector have this problem.

!image-2023-04-01-16-18-56-075.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/23 08:18;tanjialiang;image-2023-04-01-16-18-48-762.png;https://issues.apache.org/jira/secure/attachment/13056967/image-2023-04-01-16-18-48-762.png","01/Apr/23 08:18;tanjialiang;image-2023-04-01-16-18-56-075.png;https://issues.apache.org/jira/secure/attachment/13056968/image-2023-04-01-16-18-56-075.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 10 02:47:04 UTC 2023,,,,,,,,,,"0|z1h0qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/23 09:37;tanjialiang;Maybe i can take this ticket, i would like to try it.;;;","08/Apr/23 12:58;jark;I think you are right. The current implementation has some problems. The root cause is {{DecodingFormat}} doesn't support {{copy()}}, which makes the DecodingFormat resued after filter/projection is pushed down. 

Therefore, we need to first come up with a new API for {{DecodingFormat#copy()}} which may need a public discussion. 

What do you think [~luoyuxia] [~lincoln.86xy] [~twalthr]? ;;;","10/Apr/23 02:47;luoyuxia;+1 for adding a new api. +1 for public discussion.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint job folder not deleted after job is cancelled,FLINK-31685,13531049,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sergiosp,sergiosp,31/Mar/23 23:49,07/Sep/23 07:29,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,,,"When flink job is being checkpointed, and after the job is cancelled, the checkpoint is indeed deleted (as per {{{}execution.checkpointing.externalized-checkpoint-retention: DELETE_ON_CANCELLATION{}}}), but the job-id folder still remains:
 
[sergio@flink-cluster-54f7fc7c6-k6km8 JobCheckpoints]$ ls
01eff17aa2910484b5aeb644bc531172  3a59309ef018541fc0c20856d0d89855  78ff2344dd7ef89f9fbcc9789fc0cd79  a6fd7cec89c0af78c3353d4a46a7d273  dbc957868c08ebeb100d708bbd057593
04ff0abb9e860fc85f0e39d722367c3c  3e09166341615b1b4786efd6745a05d6  79efc000aa29522f0a9598661f485f67  a8c42bfe158abd78ebcb4adb135de61f  dc8e04b02c9d8a1bc04b21d2c8f21f74
05f48019475de40230900230c63cfe89  3f9fb467c9af91ef41d527fe92f9b590  7a6ad7407d7120eda635d71cd843916a  a8db748c1d329407405387ac82040be4  dfb2df1c25056e920d41c94b659dcdab
09d30bc0aaaaff786994a6a3bb06abd3  455525b76a1c6826b6eaebd5649c5b6b  7b1458424496baaf3d020e9fece525a4  aa2ef9587b2e9c123744e8940a66a287

All folders in the above list, like {{01eff17aa2910484b5aeb644bc531172}}  , are empty ~

 

*Expected behaviour:*

The job folder id should also be deleted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-5009,FLINK-3094,FLINK-10346,FLINK-15012,FLINK-11789,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 07 07:29:54 UTC 2023,,,,,,,,,,"0|z1h0nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/23 07:15;Wencong Liu;Hello [~sergiosp] , thanks for proposing this ticket! I think the key code path is 
{code:java}
FsCompletedCheckpointStorageLocation#disposeStorageLocation {code}
We could delete the parent folder in this method.;;;","23/Aug/23 02:39;masteryhx;I just linked many related tickets.

It's valid and many users want to resolve.

I think we could just introduce an option whether generate the job id directory and make them compatible.

As for the job id layout, I think it's still useful if user want to save some historical checkpoints with NO_CLAIM mode.

[~tangyun]  WDYT?;;;","02/Sep/23 16:25;zakelly;[~masteryhx]  I think there are two issue within this problem:
 # User may not need the job-id directory (to simplify the cp dir layout especially in CLAIM node), I will create another ticket to address this.
 # Deleting the job-id directory if all the checkpoint files are deleted. Different from [~Wencong Liu]'s opinion, I think it is the ```CompletedCheckpointStore```'s responsibility to delete the job-id directory, since it has the global view of whether it is needed by any other checkpoint.

WDYT?;;;","07/Sep/23 07:29;masteryhx;[~Zakelly] 
This makes sense to me.
_FsCompletedCheckpointStorageLocation_ losts global view of checkpoint dir info.
+1 for deleting the directory only when we know all checkpoint files are deleted.;;;",,,,,,,,,,,,,,,,,,,,,,,
Autoscaler metrics are only visible after metric window is full,FLINK-31684,13531025,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,31/Mar/23 19:36,02/Aug/23 15:32,04/Jun/24 20:41,24/May/23 18:15,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.6.0,,,,,,,,,Autoscaler,Kubernetes Operator,,,,,0,pull-request-available,,,,,The metrics get reported only after the metric window is full. This is not helpful for observability after rescaling. We need to make sure that metrics are reported even when the metric window is not yet full.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-31 19:36:53.0,,,,,,,,,,"0|z1h0i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align the outdated Chinese filesystem connector docs,FLINK-31683,13530941,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,31/Mar/23 09:27,31/Mar/23 13:55,04/Jun/24 20:41,31/Mar/23 13:55,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,1.15.5,1.16.2,1.17.1,1.18.0,,,,,,Documentation,,,,,,0,pull-request-available,,,,,"The current Chinese doc of the file system SQL connector is outdated from Flink-1.15, we should fix it to avoid misunderstanding.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 13:55:45 UTC 2023,,,,,,,,,,"0|z1gzzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 13:55;yunta;merged.
master: 370929c838fbac21cc51a225f3c690c1ee8931a8
release-1.17: 49db0c628e50182f160a99ec7bbd5148ed725792
release-1.16: 701fccf4556765c5c75eb7b7c66f0a4d9edfc957
release-1.15: 27f93da8f4a6813af5030691e72d132bb35ec1c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
map_from_arrays should take whether allow duplicate keys and null key into consideration,FLINK-31682,13530928,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,31/Mar/23 08:40,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"after research the spark/presto/maxcompute about map_from_arrays/map_from_entries, there all support duplicate keys and null key  for the most part

 

spark https://github.com/apache/spark/pull/21258/files

maxcompute [https://www.alibabacloud.com/help/en/maxcompute/latest/complex-type-functions#section-7ue-e91-m0s]

presto https://prestodb.io/docs/current/functions/map.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 10:52:11 UTC 2023,,,,,,,,,,"0|z1gzwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 09:11;Sergey Nuyanzin;Can you provide some cases to reproduce the issue?
It's weird to go to the links and see nothing related to that topic;;;","20/Apr/23 10:52;jackylau;hi [~Sergey Nuyanzin] , i supported this here https://issues.apache.org/jira/browse/FLINK-31691 , when it merged, i will fix this 

do you have time to review it ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Network connection timeout between operators should trigger either network re-connection or job failover,FLINK-31681,13530923,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lindong,lindong,31/Mar/23 08:18,12/Apr/23 08:45,04/Jun/24 20:41,12/Apr/23 08:45,1.15.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,,0,,,,,,"If a network connection error occurs between two operators, the upstream operator may log the following error message in the method PartitionRequestQueue#handleException and subsequently close the connection. When this happens, the Flink job may become stuck without completing or failing. 

To avoid this issue, we can either allow the upstream operator to reconnect with the downstream operator, or enable job failover so that users can take corrective action promptly.

org.apache.flink.runtime.io.network.netty.PartitionRequestQueue - Encountered error while consuming partitions org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors#NativeIOException: writeAccess(...) failed: Connection timed out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 08:45:29 UTC 2023,,,,,,,,,,"0|z1gzvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 08:36;martijnvisser;[~pnowojski] Any thought on this one?;;;","31/Mar/23 08:50;pnowojski;Which version does this issue affect? Why doesn't {{PartitionRequestQueue#handleException}} failover the subtask? Shouldn't this be triggered by {{PartitionRequestQueue#releaseAllResources}}?;;;","31/Mar/23 14:25;lindong;This happens with Flink version 1.15.1 when we were testing Flink ML with parallelism = 200.

Upgrading the internal Flink library and related connectors needed by Flink ML would take some time. Thus we have not tried to reproduce this issue with Flink 1.17.

Thus I choose to write down the phenomena and the error message in this JIRA to make sure this issue will be tracked. I will close this JIRA if we can not reproduce the issue with the latest Flink version.;;;","12/Apr/23 08:45;lindong;I didn't not get to reproduce this issue myself and the users no longer report this issue. Will close this issue for now.;;;",,,,,,,,,,,,,,,,,,,,,,,
Add priorityClassName to flink-operator's pods,FLINK-31680,13530921,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,boniek,boniek,boniek,31/Mar/23 08:12,31/Mar/23 16:26,04/Jun/24 20:41,31/Mar/23 16:26,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,I can't set pod priorityClassName for flink operator's pods using helm chart.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 16:26:03 UTC 2023,,,,,,,,,,"0|z1gzuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 16:26;gyfora;merged to main 72926b8222e8b0b61c72f93afb869a8639a224e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink][UI] Show data exchange type on web ui,FLINK-31679,13530907,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,huwh,huwh,31/Mar/23 07:19,01/Apr/23 10:13,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,,0,,,,,,"Flink supports multiple data exchange types (ResultPartitionType), which subject to multiple parameters, such as whether job is streaming or batch.

I think display the data exchange types on the UI would be helpful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 01 10:00:55 UTC 2023,,,,,,,,,,"0|z1gzrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 07:27;yunta;Do you think this ticket might be related to FLINK-25561 ?;;;","31/Mar/23 07:42;huwh;[~yunta]Thanks for the quickly reply. 
Not exactly, I mean show the data exchange type on edge in job DAG.

A real scenario is, we use pipeline mode to speed up the olap queries, but some sql will produce a BLOCK edge (by DeadlockBreakupProcessor) to prevent deadlock in DAG. This confused us as to why not all of the tasks were scheduled together.;;;","31/Mar/23 08:41;libenchao;+1 for this.;;;","01/Apr/23 07:44;Wencong Liu;Thanks [~huwh]  for proposing this ticket. It is meaningful to display the actual ResultPartitionType used for connecting upstream and downstream operators on the Web UI. However, there are currently many types of ResultPartitionType, and exposing specific names to users would make it difficult for them to understand related behaviors. 

My option is that the consuming behavior of data exchange can be described by the ConsumingConstraint enum in ResultPartitionType.java :

 
{code:java}
private enum ConsumingConstraint {
    /** Upstream must be finished before downstream consume. */
    BLOCKING,
    /** Downstream can consume while upstream is running. */
    CAN_BE_PIPELINED,
    /** Downstream must consume while upstream is running. */
    MUST_BE_PIPELINED
} {code}
 

Perhaps displaying BLOCKING/CAN_BE_PIPELINED/MUST_BE_PIPELINED rather than the ResultPartitionType on the web UI would be more friendly for users to understand and devs to troubleshoot bugs. It would also decouple it from many ResultPartitionTypes. 

Of course, these all require further discussion:). cc [~Weijie Guo] [~xtsong] ;;;","01/Apr/23 09:51;Weijie Guo;Thanks for the proposal, but I'm a bit curious about whether this feature is really helpful to the vast majority of users. The main reasons are as follows:
- For streaming job, all edges are always {{PIPELINED_BOUNDED}}. For batch job, in most cases all edges are {{BLOCKING}}. In this two cases, this can make the UI a bit messy, at least to some extent.
- Most users will not use pipelined shuffle in batch scenario, and this may have a potential scheduling deadlock problem. This is also part of the reason why we introduced hybrid shuffle.
- Keep this in mind: We plan to make hybrid shuffle the only shuffle implementation for batch jobs in the next few releases, and will also cover streaming scenarios in the future.

It seems that the only significance of showing the {{ResultPartitionType}} in the edge is that in batch job which pipelined shuffle are used, but the planner will make some of them become blocking. Because hybrid shuffle also supports scheduling constraints of blocking between upstream and downstream, planer does not need to introduce additional blocking edges. So this will no longer be the case in the near future. In most other cases, this information is of little significance.;;;","01/Apr/23 10:00;Weijie Guo;{code:java}
My option is that the consuming behavior of data exchange can be described by the ConsumingConstraint enum in ResultPartitionType.java.
{code}

[~Wencong Liu] I'm not sure if exposing it to users will cause trouble to them, after all, this is still only a concept inside the framework. After we unify the implementation of shuffle service, it becomes more meaningful for the scheduler to judge the scheduling constraints between upstream and downstream, and then expose them to user. At least at this stage, users are more aware of the specific types of shuffle like Blocking/Pipelined/Hybrid. WDYT?;;;",,,,,,,,,,,,,,,,,,,,,
NonHAQueryableStateFsBackendITCase.testAggregatingState: Query did no succeed,FLINK-31678,13530903,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,masteryhx,mapohl,mapohl,31/Mar/23 06:50,23/Aug/23 02:43,04/Jun/24 20:41,23/Aug/23 02:43,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Queryable State,Tests,,,,,0,stale-assigned,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47748&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=40484

{code}
ava.lang.AssertionError: Did not succeed query
Mar 31 01:24:32 	at org.junit.Assert.fail(Assert.java:89)
Mar 31 01:24:32 	at org.junit.Assert.assertTrue(Assert.java:42)
Mar 31 01:24:32 	at org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.testAggregatingState(AbstractQueryableStateTestBase.java:1094)
[...]
Mar 31 01:24:32 	Suppressed: java.util.concurrent.TimeoutException
Mar 31 01:24:32 		at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1769)
Mar 31 01:24:32 		at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
Mar 31 01:24:32 		at org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase$AutoCancellableJob.close(AbstractQueryableStateTestBase.java:1351)
Mar 31 01:24:32 		at org.apache.flink.queryablestate.itcases.AbstractQueryableStateTestBase.testAggregatingState(AbstractQueryableStateTestBase.java:1096)
Mar 31 01:24:32 		... 52 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 23 02:43:45 UTC 2023,,,,,,,,,,"0|z1gzqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","23/Aug/23 02:43;masteryhx;Closed this because:
 # not reproduced more than 5 months 
 # Querable State has been marked as deprecated in 1.18;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add MAP_ENTRIES supported in SQL & Table API,FLINK-31677,13530888,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,31/Mar/23 04:00,08/Jun/23 09:37,04/Jun/24 20:41,08/Jun/23 09:37,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"map_entries(map) - Returns an unordered array of all entries in the given map.

Syntax:
map_entries(map)

Arguments:
map: An MAP to be handled.

Returns:

Returns an unordered array of all entries in the given map.

Returns null if the argument is null
{code:sql}

> SELECT map_entries(map[1, 'a', 2, 'b']);
 [(1,""a""),(2,""b"")]{code}
See also
presto [https://prestodb.io/docs/current/functions/map.html]

spark https://spark.apache.org/docs/latest/api/sql/index.html#map_entries",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 08 09:37:38 UTC 2023,,,,,,,,,,"0|z1gznk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 10:55;jackylau;hi [~Sergey Nuyanzin] do you have time to review it ?

 ;;;","08/Jun/23 09:37;luoyuxia;master:

79b06cf95389491d6a800ccebdcc8feb2b6f9025;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector should not rely on Flink Shaded,FLINK-31676,13530851,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,30/Mar/23 18:15,04/Apr/23 01:38,04/Jun/24 20:41,31/Mar/23 13:53,,,,,,,,,,,,,,,,,,,,,pulsar-4.0.0,,,,,,,,,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,"The Pulsar connector currently depends on Flink Shaded for Guava. However, externalized connectors must not rely on flink-shaded. This will just not be possible if we want them to work against different Flink versions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 17:43:33 UTC 2023,,,,,,,,,,"0|z1gzfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 19:03;mason6345;[~martijnvisser] I happen to see this. Would we still be able to use guava in external connectors? Ideally yes and maybe we need to change the shading process to make it possible to work against different Flink versions;;;","30/Mar/23 19:08;martijnvisser;[~mason6345] Yeah I don’t see why that wouldn’t be possible. The problem that I ran into is that we were updating Guava in Flink Shaded and then seeing some tests fail in Flink, because the module is changing from guava30 to guava31 because of the version update. I think there are already other connectors that are using Guava directly too;;;","30/Mar/23 19:15;mason6345;Yeah, [~stevenz3wu] who works on Icebergg and I discussed this long time back since it was a little cumbersome to change that number repeatedly. It could map to a fixed string, if we don't want to use/expose guava directly and keep the package name from breaking (Iceberg and other libs do this too). ;;;","31/Mar/23 06:59;chesnay;Use your own guava version and relocate it.

We will continue to encode the guava version in the package to avoid conflicts if multiple flink-shaded-guava versions are put on the classpath.;;;","31/Mar/23 13:53;martijnvisser;Fixed in main: 104bdc378b4f4d12ffeaf550cccd2d5633bef58d;;;","31/Mar/23 17:43;mason6345;Got it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,
Deadlock in AWS Connectors following content-length AWS SDK exception,FLINK-31675,13530819,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,antoniovespoli,antoniovespoli,antoniovespoli,30/Mar/23 14:34,26/Apr/23 14:33,04/Jun/24 20:41,26/Apr/23 14:33,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,aws-connector-3.1.0,aws-connector-4.2.0,,,,,,,,Connectors / AWS,,,,,,0,pull-request-available,,,,,"Connector calls to AWS services can hang on a canceled future following a content-length mismatch that isn't handled gracefully by the SDK:

 
{code:java}
org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.FutureCancelledException: java.io.IOException: Response had content-length of 31 bytes, but only received 0 bytes before the connection was closed.
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.lambda$null$3(NettyRequestExecutor.java:136)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
at org.apache.flink.kinesis.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
at org.apache.flink.kinesis.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Response had content-length of 31 bytes, but only received 0 bytes before the connection was closed.
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler.validateResponseContentLength(ResponseHandler.java:163)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler.access$700(ResponseHandler.java:75)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler$PublisherAdapter$1.onComplete(ResponseHandler.java:369)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher.complete(HandlerPublisher.java:447)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher.channelInactive(HandlerPublisher.java:430)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
at org.apache.flink.kinesis.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
at org.apache.flink.kinesis.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
at org.apache.flink.kinesis.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
at org.apache.flink.kinesis.shaded.io.netty.handler.codec.http2.AbstractHttp2StreamChannel$Http2ChannelUnsafe$2.run(AbstractHttp2StreamChannel.java:737)
... 6 more {code}
Related AWS SDK issue: [https://github.com/aws/aws-sdk-java-v2/issues/3335]

AWS SDK fix: [https://github.com/aws/aws-sdk-java-v2/pull/3855/files]

 

This mishandled exception creates a deadlock situation that prevents the connectors from making any progress.

We should update the AWS SDK v2 to 2.20.32: [https://github.com/aws/aws-sdk-java-v2/commit/eb5619e24e4eaca6f80effa1c43c0cd409cdd53e]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 14:33:31 UTC 2023,,,,,,,,,,"0|z1gz88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 14:37;dannycranmer;Thanks for raising [~antoniovespoli] , I have assigned the issue to you;;;","26/Apr/23 14:32;dannycranmer;Merged commit [{{9a51b52}}|https://github.com/apache/flink-connector-aws/commit/9a51b5242563f2870148a62e07ef6e9763050519] into apache:main
Merged commit [{{721772e}}|https://github.com/apache/flink-connector-aws/commit/721772eef0f409bb2d9b522d42d810110d9387d6] into apache:v3.0;;;","26/Apr/23 14:33;dannycranmer;Dropping from 1.15.5 since 1.15.x is no longer supported;;;",,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner (BatchAbstractTestBase),FLINK-31674,13530815,13485220,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,rskraba,rskraba,rskraba,30/Mar/23 14:26,27/Aug/23 17:10,04/Jun/24 20:41,27/Aug/23 17:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"This is one sub-subtask related to the flink-table-planner migration (FLINK-29541).

While most of the JUnit migrations tasks are done by modules, a number of abstract test classes in flink-table-planner have large hierarchies that cross module boundaries.  This task is to migrate all of the tests that depend on {{BatchAbstractTestBase}} to JUnit5.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30815,,,,,,,FLINK-29541,,,,FLINK-27885,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 27 17:09:30 UTC 2023,,,,,,,,,,"0|z1gz7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 14:27;rskraba;Can this be assigned to me?;;;","27/Aug/23 16:03;jiabao.sun;Hi [~Sergey Nuyanzin], this issue is duplicate with FLINK-30815.

I think we can close this issue now.;;;","27/Aug/23 17:09;Sergey Nuyanzin;yes probably you are right, thanks for highlighting;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add E2E tests for flink jdbc driver,FLINK-31673,13530787,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,libenchao,libenchao,30/Mar/23 11:26,27/Jun/23 10:52,04/Jun/24 20:41,15/Jun/23 09:16,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,Tests,,,,,0,pull-request-available,,,,,"Since jdbc driver will be used by third party projects, and we've introduced a bundled jar in flink-sql-jdbc-driver-bundle, we'd better to have e2e tests to verify and ensure it works fine (in case of the dependency management).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32370,FLINK-32382,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 15 09:16:11 UTC 2023,,,,,,,,,,"0|z1gz14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 09:16;libenchao;Implemented via [https://github.com/apache/flink/commit/089e9edcb6f250fbd18ae4cb0f285d47c629deb4] (master)

[~zjureel] Thanks for your PR!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Requirement validation does not take user-specified or scheduler-generated maxParallelism into account,FLINK-31672,13530783,13527017,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,30/Mar/23 11:06,31/Mar/23 12:53,04/Jun/24 20:41,31/Mar/23 12:53,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 12:53:52 UTC 2023,,,,,,,,,,"0|z1gz08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 12:53;chesnay;master: 7e16a0fa61177a51cb50d0f198464939310508a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
[Connectors/HBase] Update flink to 1.17.0,FLINK-31671,13530772,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,tanyuxin,tanyuxin,tanyuxin,30/Mar/23 09:37,30/Mar/23 10:59,04/Jun/24 20:41,30/Mar/23 10:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / HBase,,,,,,0,pull-request-available,,,,,Bump Flink version to 1.17.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 30 10:59:19 UTC 2023,,,,,,,,,,"0|z1gyxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 10:59;martijnvisser;[~tanyuxin] This is not needed yet, as HBase isn't yet externalized from Flink itself. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticSearch connector's document was not incorrect linked to external repo,FLINK-31670,13530769,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,30/Mar/23 09:06,11/Apr/23 13:15,04/Jun/24 20:41,06/Apr/23 07:17,1.16.1,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,"In the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/elasticsearch/], It still use ""flink-version"" for flink-connector-elastiacsearch instead of the version in the external repository.

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-elasticsearch6</artifactId>
    <version>1.18-SNAPSHOT</version>
</dependency>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 11 13:15:54 UTC 2023,,,,,,,,,,"0|z1gyx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 09:47;Weijie Guo;It seems that we sync the doc of {{elasticsearch}} connector with the tag {{v3.0.0 (integrate_connector_docs elasticsearch v3.0.0)}}, but we allows connectors to specify it's own version in FLINK-29958 after this tag. I'm not sure whether we should directly sync with the latest v3.0(integrate_connector_docs elasticsearch v3.0) branch or create another release(i.e. rc3) for v3.0 branch, while also including the v3.0.0 tag. But I'm not very familiar with the management process of external connectors. cc [~martijnvisser] for further information.;;;","06/Apr/23 07:16;Weijie Guo;master(1.18) via d60431b05ed8adbd320e6413b55b748666206319.
release-1.17 via bc1aaa66b9539885bfcfec94a06012396f7aa9a1.
release-1.16 via 8e961e4cb6ee89c286568e3e37d3366f88e1bd96.;;;","11/Apr/23 13:15;martijnvisser;[~Weijie Guo] Sorry that I missed this. 

In my opinion, docs should point to the branch where that version of the docs are published (because else documentation fixes are not visible until a new release is made). 
The connector_artifact shortcode should point to a released version, since you should be able to download it from the repository;;;",,,,,,,,,,,,,,,,,,,,,,,,
DynamicFilteringITCase failed fataly because of NoClassDefFoundError of akka/actor/SuppressedDeadLetter,FLINK-31669,13530742,13537639,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,chesnay,mapohl,mapohl,30/Mar/23 06:39,13/Jun/23 21:32,04/Jun/24 20:41,13/Jun/23 21:32,1.16.1,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,test-stability,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47692&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=13752

{code}
[...]
Mar 30 06:02:56 [ERROR] Error occurred in starting fork, check output in log
Mar 30 06:02:56 [ERROR] Process Exit Code: 239
Mar 30 06:02:56 [ERROR] Crashed tests:
Mar 30 06:02:56 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.DynamicFilteringITCase
Mar 30 06:02:56 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30299,FLINK-26037,,,,,,,,,,,,,,"30/Mar/23 06:43;mapohl;mvn.FLINK-31669.log;https://issues.apache.org/jira/secure/attachment/13056910/mvn.FLINK-31669.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 22 08:12:23 UTC 2023,,,,,,,,,,"0|z1gyr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 06:45;mapohl;I attached the logs for this specific test failure. It looks like a duplicate of FLINK-30299/FLINK-26037;;;","30/Mar/23 07:57;mapohl;I verified that the {{IllegalStateException}} is handled properly. It is caught in [CollectResultFetcher:225|https://github.com/apache/flink/blob/release-1.16/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectResultFetcher.java#L225] when checking the job status. So that's not triggering the MiniCluster shutdown hook;;;","30/Mar/23 08:38;mapohl;Based on the thread dump it appears that \{{MemoryExecutionGraphInfoStore}}'s shutdown hook is called:
{code:java}
""MemoryExecutionGraphInfoStore shutdown hook"" Id=54 BLOCKED on [I@e2dfcf owned by ""MemoryExecutionGraphInfoStore shutdown hook"" Id=171
        at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.clear(LocalCache.java:3198)
        -  blocked on [I@e2dfcf
        at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.clear(LocalCache.java:4281)
        at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.invalidateAll(LocalCache.java:4897)
        at org.apache.flink.runtime.dispatcher.MemoryExecutionGraphInfoStore.close(MemoryExecutionGraphInfoStore.java:151)
        at org.apache.flink.util.ShutdownHookUtil.lambda$addShutdownHook$0(ShutdownHookUtil.java:39)
        at org.apache.flink.util.ShutdownHookUtil$$Lambda$2127/2097505549.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:748)

        Number of locked synchronizers = 1
        - java.util.concurrent.locks.ReentrantLock$NonfairSync@17ebd791
{code}
and the RpcSystem is closed due to the {{{}MiniClusterResource{}}}'s shutdown hook:
{code:java}
""Thread-2"" Id=17 RUNNABLE
        at sun.misc.URLClassPath.closeLoaders(URLClassPath.java:166)
        -  locked sun.misc.URLClassPath@220eca6d
        at java.net.URLClassLoader.close(URLClassLoader.java:292)
        at org.apache.flink.runtime.rpc.akka.CleanupOnCloseRpcSystem.close(CleanupOnCloseRpcSystem.java:56)
        at org.apache.flink.runtime.testutils.MiniClusterResource.lambda$static$0(MiniClusterResource.java:74)
        at org.apache.flink.runtime.testutils.MiniClusterResource$$Lambda$371/447208471.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:748)
{code};;;","30/Mar/23 08:49;mapohl;[~chesnay] can you come up with a conclusion based on the thread dump? Could it be that we receive a akka message while shutting down the MiniCluster with the akka infrastructure not being fully cleaned up, yet (because there is still a thread pool {{akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@7d67e730}} present that)?;;;","02/May/23 09:29;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48486&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=22604;;;","16/May/23 08:10;renqs;[~chesnay] Could you take a look at this one? Thanks;;;","22/May/23 08:12;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49123&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=25935
HiveTableSinkITCase failed with {noformat}05:30:58,039 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache
05:30:58,039 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache
05:30:58,044 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:46707
05:30:58,045 [flink-akka.actor.internal-dispatcher-3] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.internal-dispatcher-3' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/actor/SuppressedDeadLetter
        at akka.actor.EmptyLocalActorRef.publishSupressedDeadLetter(ActorRef.scala:712) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.EmptyLocalActorRef.specialHandle(ActorRef.scala:706) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.DeadLetterActorRef.specialHandle(ActorRef.scala:740) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.DeadLetterActorRef.$bang(ActorRef.scala:728) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailboxes$$anon$1.systemEnqueue(Mailboxes.scala:53) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:313) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:273) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.actor.SuppressedDeadLetter
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:149) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:112) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 13 more
05:30:58,046 [flink-akka.actor.default-dispatcher-6] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-6' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/actor/SuppressedDeadLetter
        at akka.actor.EmptyLocalActorRef.publishSupressedDeadLetter(ActorRef.scala:712) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.EmptyLocalActorRef.specialHandle(ActorRef.scala:706) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.DeadLetterActorRef.specialHandle(ActorRef.scala:740) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.actor.DeadLetterActorRef.$bang(ActorRef.scala:728) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailboxes$$anon$1.systemEnqueue(Mailboxes.scala:53) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:313) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:230) ~[flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_fed3d292-8e56-4744-acfd-0b51e8fd93fe.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]{noformat} in logs
;;;",,,,,,,,,,,,,,,,,,,,
Gateway should close select query when session is timeout,FLINK-31668,13530729,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,30/Mar/23 05:55,30/Mar/23 05:55,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,,0,,,,,,"When a session in Gateway is timeout, Gateway should close the `select query` when it closes the session",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-30 05:55:09.0,,,,,,,,,,"0|z1gyog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not enforce attached mode in RemoteEnvironment & RemoteStreamEnvironment,FLINK-31667,13530726,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjffdu,zjffdu,30/Mar/23 05:16,06/Apr/23 08:46,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Client / Job Submission,,,,,,0,,,,,,"Currently, Flink enforce to use attached mode in `RemoteEnvironment` & `RemoteStreamEnvironment` even user try to use detached mode. This doesn't make sense to me.

* https://github.com/apache/flink/blob/master/flink-java/src/main/java/org/apache/flink/api/java/RemoteEnvironment.java#L151

* https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java#L211

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 08:46:59 UTC 2023,,,,,,,,,,"0|z1gyns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 08:46;Weijie Guo;Emm, I don't remember why it is forced to be set to {{ATTACHED}} here, if anyone knows the reason, please tell me. But at the moment it seems that this is unnecessary. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_OVERLAP supported in SQL & Table API,FLINK-31666,13530724,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,30/Mar/23 05:01,11/Mar/24 12:44,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-30 05:01:34.0,,,,,,,,,,"0|z1gync:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_CONCAT supported in SQL & Table API,FLINK-31665,13530723,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hanyuzheng,jackylau,jackylau,30/Mar/23 05:00,29/Jun/23 15:13,04/Jun/24 20:41,19/Jun/23 10:42,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"This is an implementation of ARRAY_CONCEPT

The array_concat() function concatenates at least one array, creating an array that contains all the elements in the first array followed by all the elements in the second array ... followed by all the elements in the n array.
h3. Brief change log

ARRAY_CONCAT for Table API and SQL

Syntax:
{code:java}
ARRAY_CONCAT(arr1, tail...){code}

Arguments:
array: at least one ARRAY to be handled.

Returns:
The function returns NULL if any input argument is NULL.

Examples:
{code:java}
Flink SQL> SELECT array_concat(array[1, 2, 3, null, 3], array[3], array[5]); [1, 2, 3, null, 3, 3]{code}
see also:
Google Cloud BigQuery: [https://cloud.google.com/bigquery/docs/reference/standard-sql/array_functions#array_concat]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 19 10:42:20 UTC 2023,,,,,,,,,,"0|z1gyn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/23 01:56;chenbowen;Can i contributing this function ?;;;","05/Jun/23 19:00;Sergey Nuyanzin;thanks for volunteering however it would be much better to understand first what this task is implied...
E.g. what is expected behavior, since it is not a part of SQL standard it it is not obious? What other engines provide regarding this. 

Also why this name?

e.g. snowflake[1] and postgres[2] provide {{ARRAY_CAT}}
why should Flink have a different name?

[1] https://docs.snowflake.com/en/sql-reference/functions/array_cat
[2] https://www.postgresql.org/docs/15/functions-array.html;;;","07/Jun/23 08:11;twalthr;Personally, I prefer {{ARRAY_CONCAT}} as it reads nicely and you know what it does. ARRAY_CAT sounds like an animal. But agree that we should strive for consistency across vendors. How is it with consistency within Flink do we have already other functions containing `CONCAT`?;;;","07/Jun/23 08:31;Sergey Nuyanzin;To be honest there is no 100% consistency across vendors.
e.g., as it was mentioned above {{ARRAY_CAT}} for snowflake and postgres

also {{ARRAY_CONCAT}} for e.g. BigQuery[1] and ClickHouse[2](in fact ClickHouse has name without underscore {{arrayConcat}}), DuckDB[3]
however the main difference is that these 3 vendors allow to concat arbitrary number of arrays not just 2 (while {{ARRAY_CAT}} for mentioned above is limited to 2 args)

Regarding your question [~twalthr]
{quote}do we have already other functions containing `CONCAT`
{quote}
there is already existing {{CONCAT}} function Flink[4] to concatenate strings.
And it also allows to concatenate arbitrary number(>=1) of strings.

To be consistent doesn't it make sense to go same way as BigQuery, ClickHouse, DuckDB

like
{code:sql}
SELECT array_concat(array1, array2, ...);
{code}
?
[1] [https://cloud.google.com/bigquery/docs/reference/standard-sql/array_functions#array_concat]
[2] [https://clickhouse.com/docs/en/sql-reference/functions/array-functions#arrayconcat]
[3] [https://duckdb.org/docs/sql/functions/nested.html]
[4] https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/systemfunctions/#string-functions;;;","07/Jun/23 14:10;twalthr;Thanks for the detailed investigation [~Sergey Nuyanzin]. I would say the fact that BigQuery uses `ARRAY_CONCAT` and we already have a Flink function called `CONCAT` justifies calling this function `ARRAY_CONCAT` as well for consistency.

I would also be up for supporting var args instead of just 2 arguments from the very beginning. What do you think [~hanyuzheng]?;;;","07/Jun/23 23:34;hanyuzheng;Thanks for [~twalthr] the addition, I've done the change to turn two input parameters into multiple input parameters.;;;","12/Jun/23 13:14;marast;I personally think that ARRAY_CONCAT is most intuitive option and the fact that other vendors like BQ use it also helps.;;;","19/Jun/23 10:42;Sergey Nuyanzin;Merged to master as [6e22b94e708052ac450f84bce954a4d3ec7bb772|https://github.com/apache/flink/commit/6e22b94e708052ac450f84bce954a4d3ec7bb772];;;",,,,,,,,,,,,,,,,,,,
Add ARRAY_INTERSECT supported in SQL & Table API,FLINK-31664,13530722,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,30/Mar/23 04:59,04/Jun/24 06:53,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 06 06:27:18 UTC 2023,,,,,,,,,,"0|z1gymw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 19:47;Sergey Nuyanzin;[~jackylau] Please follow the guide regarding creating jira issue https://flink.apache.org/how-to-contribute/contribute-code/

also there already an existing PR... it is better first to check if another author will continue work on this on not;;;","06/Jun/23 06:08;jackylau;hi [~Sergey Nuyanzin] thanks for your suggestion

1) this issue i created before but i do not have permission to assign to myself.

2) the pr submit by another author implementation has problem, we can not use set directly, so i submit another pr.;;;","06/Jun/23 06:27;Sergey Nuyanzin;{quote}
2) the pr submit by another author implementation has problem, we can not use set directly, so i submit another pr.
{quote}
Please check contribution guide about that
it says
{quote}
Make sure the pull request refers to the respective Jira, and that each Jira issue is assigned to exactly one pull request (in case of multiple pull requests for one Jira; resolve that situation first)
{quote}
https://flink.apache.org/how-to-contribute/contribute-code/#3-open-a-pull-request;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_EXCEPT supported in SQL & Table API,FLINK-31663,13530714,13076759,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hanyuzheng,luoyuxia,luoyuxia,30/Mar/23 02:40,22/May/24 14:58,04/Jun/24 20:41,07/Mar/24 14:42,,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,FLINK-35423,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 11 11:13:28 UTC 2024,,,,,,,,,,"0|z1gyl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 05:07;jackylau;should links to https://issues.apache.org/jira/browse/FLINK-22484;;;","07/Mar/24 14:42;dwysakowicz;Implemented in 2429c296a60bf0a0e8a4acebc04a059008708d1f;;;","08/Mar/24 03:22;jackylau;hi [~dwysakowicz] [~hanyuzheng] [~luoyuxia] , i found the result is not correct.

the pr description is [https://github.com/apache/flink/pull/22588]

we can see it is not contain duplicates, which is same result with spark [https://spark.apache.org/docs/latest/api/sql/index.html#array_except]

but the code logic is contain duplicates. and i think we should align with spark. 
Since the behavior of array_union is already aligned with Spark's, if we don't align here, the logic of the entire function would seem inconsistent. What do you think?
 
 
{code:java}
Flink SQL> SELECT array_except(array[1,2,2], array[2,3,4]);
[1]

Flink SQL> SELECT array_except(array[1,2,2], array[1]);
[2]

Flink SQL> SELECT array_except(array[1,2,2], array[42]);
[1, 2] {code};;;","08/Mar/24 08:29;dwysakowicz;[~jackylau] Please see the discussion here: https://github.com/apache/flink/pull/23173#discussion_r1491044219;;;","08/Mar/24 09:34;jackylau;[~dwysakowicz] i see, thanks for your response.

Since the behavior of array_union is already aligned with Spark's(without duplicates ), if we don't align here, the logic of the entire function would seem inconsistent. should we change the behavior if array_union. but if we change it ,it will cause version compatibility problem;;;","08/Mar/24 09:39;dwysakowicz;I don't see how `array_union` is related to `array_except`. Those are two separate functions. Yes, we should've investigated it further when implementing `ARRAY_UNION`, but I think it's too late by now, imo. Unless [~MartijnVisser] thinks otherwise and we should reinvestigate `array_union`.;;;","08/Mar/24 09:53;jackylau;[~dwysakowicz] Even though they are different functions, we need to maintain basic behavioral consistency. In Spark, many array functions such as array_except, array_union, and array_intersect have consistent behaviors regarding whether duplicates are allowed. For example, otherwise users might find the behavior to be inconsistent and tricky to use. For another example, in Spark, functions like array_contains(array, element) and array_position(array, element) return null when the element is null. In contrast, in Flink, array_contains returns true if the array contains null values. However, if the element is null, array_position returns null, which is a different behavior and can be confusing for users. Unfortunately, the current behavior in Flink for array_contains and array_position is not consistent, so I've submitted a PR to correct this issue, but it has not been reviewed yet [https://github.com/apache/flink/pull/22946]  What do you think?;;;","11/Mar/24 02:16;jackylau;hi [~snuyanzin] what is your opinion?;;;","11/Mar/24 07:23;Sergey Nuyanzin;{quote}
Since the behavior of array_union is already aligned with Spark's(without duplicates ), if we don't align here, the logic of the entire function would seem inconsistent. should we change the behavior if array_union. but if we change it ,it will cause version compatibility problem
{quote}
I don't think we should copy everything that is present in Spark.

there is Snowflake, ClickHouse, PostgreSQL, DuckDB and etc.

{{ARRAY_EXCEPT}} keeps duplicates (as in Snowflake) and it allows to cover some cases not covered by the version eliminating duplicates. In case there is a need to eliminate duplicates there is {{ARRAY_DISTINCT}}.
And Flink follows this way
Yep there is {{ARRAY_UNION}} which eliminates duplicates
However there is also {{ARRAY_CONCAT}} which concatenates arrays without duplicates elimination, moreover it can concatenate more than 2 arrays at once (like in BigQuery, ClickHouse, DuckDB)
;;;","11/Mar/24 10:15;jackylau;[~Sergey Nuyanzin] in Flink, array_contains returns true if the array contains null values. However, if the element is null, array_position returns null, which is also a different inconsistent behavior.
because the two functions handle null values inconsistently. and can also be confusing for users. should we change the behavior ?;;;","11/Mar/24 10:36;Sergey Nuyanzin;let's not mix everything in one place please.
This is a jira issue for {{ARRAY_EXCEPT}}.
if you feel there is some other problem, feel free to file a jira issue with detailed description what and why is wrong.;;;","11/Mar/24 11:13;jackylau;[~Sergey Nuyanzin] ok. if the array_except and array_concat have duplicates while array_union do not have. we should add description in the docs;;;",,,,,,,,,,,,,,,
Migrate flink-connectors-aws to new structure,FLINK-31662,13530676,13523636,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,liangtl,liangtl,liangtl,29/Mar/23 19:29,30/Jun/23 18:41,04/Jun/24 20:41,30/Jun/23 18:41,,,,,,,,,,,,,,,,,,,,,aws-connector-4.2.0,,,,,,,,,Connectors / AWS,Connectors / DynamoDB,Connectors / Firehose,Connectors / Kinesis,,,0,pull-request-available,,,,,"Migrate flink-connectors-aws to follow a new structure of

 

 
{code:java}
\-flink-connector-aws-base
\-flink-connector-aws-e2e-tests
  \-flink-connector-aws-kinesis-firehose-e2e-tests
  \-flink-connector-aws-kinesis-streams-e2e-tests
  \-flink-connector-kinesis-e2e-tests
  \-flink-formats-avro-glue-schema-registry-e2e-tests
  \-flink-formats-json-glue-schema-registry-e2e-tests
\-flink-connector-aws
  \-flink-connector-aws-kinesis-firehose
  \-flink-connector-aws-kinesis-streams
  \-flink-connector-dynamodb
  \-flink-connector-kinesis
  \-flink-sql-connector-aws-kinesis-firehose
  \-flink-sql-connector-aws-kinesis-streams
  \-flink-sql-connector-dynamodb
  \-flink-sql-connector-kinesis
\-flink-formats-aws
  \-flink-avro-glue-schema-registry
  \-flink-json-glue-schema-registry{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 30 18:41:20 UTC 2023,,,,,,,,,,"0|z1gycw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 08:56;dannycranmer;Assigned to you [~liangtl] , thanks for driving;;;","30/Jun/23 18:41;dannycranmer;Merged commit [{{63ceb4f}}|https://github.com/apache/flink-connector-aws/commit/63ceb4fbe5995a663725cfc994027f4fe64e39f0] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add parity between `ROW` value function and it's type declaration,FLINK-31661,13530672,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,morezaei00,morezaei00,29/Mar/23 18:40,16/May/23 19:52,04/Jun/24 20:41,,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"Currently the [{{ROW}} table type|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/types/#row] allows for a name and type, and optionally a description, but [its value constructing function|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/types/#row] only supports an arbitrary list of expressions.

This prevents users from providing human-readable names for the fields provded to a {{ROW()}} or {{()}} value function call, resulting in system-defined {{EXPR$n}} names that lose their meaning as they are mixed in with other queries.

For example, the following SQL query:

{code}
SELECT (id, name) as struct FROM t1;
{code}

results in the following consumable data type for the `ROW` column:

{code}
ROW<`EXPR$0` DECIMAL(10, 2), `EXPR$1` STRING> NOT NULL
{code}

I'd be happy to contribute to this change, but I need some guidance and pointers on where to start making changes for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 16 19:52:57 UTC 2023,,,,,,,,,,"0|z1gyc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 17:35;Sergey Nuyanzin;This should help to have field names, i guess
{code:sql}
select cast((1, 'name') as row(id int, name string));
{code};;;","31/Mar/23 22:22;morezaei00;Thanks for bringing that up [~Sergey Nuyanzin], and I should've mentioned in the description that I'm looking to contribute to a native support for the field names, one without a secondary function call, if possible. The {{CAST}} solution is possible, but suboptimal.

[~Sergey Nuyanzin], would you consider this a small change or a bigger feature for the `ROW()` value function?;;;","08/Apr/23 12:29;jark;This can be a big effort because we need to contribute this feature to the upstream project Calcite that the Calcite parser handles the ROW expression. In addition, we need to investigate how/whether other mature database systems support this. ;;;","16/May/23 08:23;renqs;As this is a new feature, we'd like to downgrade it to major first. ;;;","16/May/23 19:52;Sergey Nuyanzin;it seems in spark it is called {{named_struct}} https://spark.apache.org/docs/3.4.0/api/sql/index.html#named_struct
there is a similar discussion about that in Calcite at https://issues.apache.org/jira/browse/CALCITE-5701;;;",,,,,,,,,,,,,,,,,,,,,,
flink-connectors-kafka ITCases are not runnable in the IDE,FLINK-31660,13530662,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nateab,nateab,nateab,29/Mar/23 17:27,08/May/23 14:08,04/Jun/24 20:41,08/May/23 14:08,1.18.0,,,,,,,,,,,,,,,,,,,,1.17.1,,,,,,,,,Table SQL / Ecosystem,,,,,,0,pull-request-available,,,,,"The following exception is thrown when trying to run {{KafkaChangelogTableITCase}} or {{KafkaTableITCase}}
{code:java}
java.lang.NoClassDefFoundError: org/apache/flink/table/shaded/com/jayway/jsonpath/spi/json/JsonProvider    at java.base/java.lang.Class.getDeclaredMethods0(Native Method)
    at java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3166)
    at java.base/java.lang.Class.getMethodsRecursive(Class.java:3307)
    at java.base/java.lang.Class.getMethod0(Class.java:3293)
    at java.base/java.lang.Class.getMethod(Class.java:2106)
    at org.apache.calcite.linq4j.tree.Types.lookupMethod(Types.java:309)
    at org.apache.calcite.util.BuiltInMethod.<init>(BuiltInMethod.java:670)
    at org.apache.calcite.util.BuiltInMethod.<clinit>(BuiltInMethod.java:357)
    at org.apache.calcite.rel.metadata.BuiltInMetadata$PercentageOriginalRows.<clinit>(BuiltInMetadata.java:344)
    at org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows$RelMdPercentageOriginalRowsHandler.getDef(RelMdPercentageOriginalRows.java:231)
    at org.apache.calcite.rel.metadata.ReflectiveRelMetadataProvider.reflectiveSource(ReflectiveRelMetadataProvider.java:134)
    at org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows.<clinit>(RelMdPercentageOriginalRows.java:42)
    at org.apache.calcite.rel.metadata.DefaultRelMetadataProvider.<init>(DefaultRelMetadataProvider.java:42)
    at org.apache.calcite.rel.metadata.DefaultRelMetadataProvider.<clinit>(DefaultRelMetadataProvider.java:28)
    at org.apache.calcite.plan.RelOptCluster.<init>(RelOptCluster.java:97)
    at org.apache.calcite.plan.RelOptCluster.create(RelOptCluster.java:106)
    at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$.create(FlinkRelOptClusterFactory.scala:36)
    at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory.create(FlinkRelOptClusterFactory.scala)
    at org.apache.flink.table.planner.delegation.PlannerContext.<init>(PlannerContext.java:132)
    at org.apache.flink.table.planner.delegation.PlannerBase.<init>(PlannerBase.scala:121)
    at org.apache.flink.table.planner.delegation.StreamPlanner.<init>(StreamPlanner.scala:65)
    at org.apache.flink.table.planner.delegation.DefaultPlannerFactory.create(DefaultPlannerFactory.java:65)
    at org.apache.flink.table.planner.loader.DelegatePlannerFactory.create(DelegatePlannerFactory.java:36)
    at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:58)
    at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:127)
    at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:122)
    at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:94)
    at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.setup(KafkaTableTestBase.java:93)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.ClassNotFoundException: Class 'org.apache.flink.table.shaded.com.jayway.jsonpath.spi.json.JsonProvider' not found. Perhaps you forgot to add the module 'flink-table-runtime' to the classpath?
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:123)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    ... 61 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.shaded.com.jayway.jsonpath.spi.json.JsonProvider
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromOwnerOnly(ComponentClassLoader.java:164)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentFirst(ComponentClassLoader.java:158)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:104)
    ... 62 more {code}
This is a similar problem to https://issues.apache.org/jira/browse/FLINK-25525 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25525,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 08 14:08:59 UTC 2023,,,,,,,,,,"0|z1gy9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 21:23;twalthr;This is a very annoying issue but iirc should not be a problem once the Kafka connector is externalized into a dedicated repository.;;;","29/Mar/23 21:26;twalthr;[~dwysakowicz] You also ran into this problem recently, how did you fix it for development?;;;","30/Mar/23 19:32;nateab;[~pnowojski] was able to get a rough working pom by looking at {{flink-connector-hive}} and added a few dependencies and a new maven plugin, and then [~pgaref] helped me by telling me to have intellij use the maven wrapper, and we fiddled with the pom a bit more. I will try to polish things up and put out a bug fix;;;","31/Mar/23 08:51;dwysakowicz;I think the way I did it is that I added one of the table modules (either planner or the uber jar) to the classpath of the test configuration in IntelliJ. (""Modify options"" > ""Modify classpath"") That way I didn't need to change a valid pom configuration which is not handled 100% correctly by IntelliJ.;;;","31/Mar/23 09:16;chesnay;Bundling json-path in the table-planner also does the trick btw; at least for the kafka tests.;;;","08/May/23 14:08;chesnay;1.17: 7d813376e905dee04041ada03cb4f3b8b33280fc;;;",,,,,,,,,,,,,,,,,,,,,
java.lang.ClassNotFoundException: org.apache.flink.table.planner.delegation.DialectFactory when bundled Hive connector jar is in classpath,FLINK-31659,13530605,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,TsReaper,TsReaper,29/Mar/23 11:40,06/Dec/23 02:54,04/Jun/24 20:41,06/Dec/23 02:54,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Connectors / Hive,,,,,,0,,,,,,"Steps to reproduce this bug:
1. Download a fresh 1.16.1 Flink distribution.
2. Add both {{flink-sql-connector-hive-2.3.9_2.12-1.16.0.jar}} and {{flink-shaded-hadoop-2-uber-2.8.3-10.0.jar}} into {{lib}} directory.
3. Start a standalone cluster with {{bin/start-cluster.sh}}.
4. Start SQL client and run the following SQL.

{code:sql}
create table T (
  a int,
  b string
) with (
  'connector' = 'filesystem',
  'format' = 'csv',
  'path' = '/tmp/gao.csv'
);

create table S (
  a int,
  b string
) with (
  'connector' = 'print'
);

insert into S select * from T;
{code}

The following exception will occur.

{code}
org.apache.flink.table.client.gateway.SqlExecutionException: Failed to parse statement: insert into S select * from T;
	at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:174) ~[flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.cli.SqlCommandParserImpl.parseCommand(SqlCommandParserImpl.java:45) ~[flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.cli.SqlMultiLineParser.parse(SqlMultiLineParser.java:71) ~[flink-sql-client-1.16.1.jar:1.16.1]
	at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964) ~[flink-sql-client-1.16.1.jar:1.16.1]
	at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778) ~[flink-sql-client-1.16.1.jar:1.16.1]
	at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679) ~[flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:295) [flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:280) [flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:228) [flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client-1.16.1.jar:1.16.1]
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client-1.16.1.jar:1.16.1]
Caused by: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.T'.

Table options are:

'connector'='filesystem'
'format'='json'
'path'='/tmp/gao.json'
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:166) ~[flink-table-api-java-uber-1.16.1.jar:1.16.1]
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:191) ~[flink-table-api-java-uber-1.16.1.jar:1.16.1]
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175) ~[?:?]
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3619) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2559) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2175) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2095) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2038) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:669) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:657) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3462) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:570) ~[?:?]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:215) ~[?:?]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:191) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:1498) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:1253) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:374) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNodeOrFail(SqlToOperationConverter.java:384) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:828) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:351) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:262) ~[?:?]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
	at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
	... 12 more
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/table/planner/delegation/DialectFactory
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.8.0_361]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:756) ~[?:1.8.0_361]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.8.0_361]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:473) ~[?:1.8.0_361]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:74) ~[?:1.8.0_361]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:369) ~[?:1.8.0_361]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:363) ~[?:1.8.0_361]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_361]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:362) ~[?:1.8.0_361]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_361]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355) ~[?:1.8.0_361]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:405) ~[?:1.8.0_361]
	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:67) ~[flink-dist-1.16.1.jar:1.16.1]
	at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:65) ~[flink-dist-1.16.1.jar:1.16.1]
	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:51) ~[flink-dist-1.16.1.jar:1.16.1]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_361]
	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192) ~[flink-dist-1.16.1.jar:1.16.1]
	at java.lang.Class.forName0(Native Method) ~[?:1.8.0_361]
	at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_361]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:370) ~[?:1.8.0_361]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_361]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_361]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_361]
	at org.apache.flink.connector.file.table.FileSystemTableFactory.formatFactoryExists(FileSystemTableFactory.java:206) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.table.FileSystemTableFactory.discoverDecodingFormat(FileSystemTableFactory.java:172) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.table.FileSystemTableFactory.createDynamicTableSource(FileSystemTableFactory.java:77) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:163) ~[flink-table-api-java-uber-1.16.1.jar:1.16.1]
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:191) ~[flink-table-api-java-uber-1.16.1.jar:1.16.1]
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175) ~[?:?]
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3619) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2559) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2175) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2095) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2038) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:669) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:657) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3462) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:570) ~[?:?]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:215) ~[?:?]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:191) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:1498) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:1253) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:374) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNodeOrFail(SqlToOperationConverter.java:384) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:828) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:351) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:262) ~[?:?]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
	at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
	... 12 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.planner.delegation.DialectFactory
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_361]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_361]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355) ~[?:1.8.0_361]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_361]
	at java.lang.ClassLoader.defineClass1(Native Method) ~[?:1.8.0_361]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:756) ~[?:1.8.0_361]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[?:1.8.0_361]
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:473) ~[?:1.8.0_361]
	at java.net.URLClassLoader.access$100(URLClassLoader.java:74) ~[?:1.8.0_361]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:369) ~[?:1.8.0_361]
	at java.net.URLClassLoader$1.run(URLClassLoader.java:363) ~[?:1.8.0_361]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_361]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:362) ~[?:1.8.0_361]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_361]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355) ~[?:1.8.0_361]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:405) ~[?:1.8.0_361]
	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:67) ~[flink-dist-1.16.1.jar:1.16.1]
	at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:65) ~[flink-dist-1.16.1.jar:1.16.1]
	at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:51) ~[flink-dist-1.16.1.jar:1.16.1]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_361]
	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192) ~[flink-dist-1.16.1.jar:1.16.1]
	at java.lang.Class.forName0(Native Method) ~[?:1.8.0_361]
	at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_361]
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:370) ~[?:1.8.0_361]
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_361]
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_361]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_361]
	at org.apache.flink.connector.file.table.FileSystemTableFactory.formatFactoryExists(FileSystemTableFactory.java:206) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.table.FileSystemTableFactory.discoverDecodingFormat(FileSystemTableFactory.java:172) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.connector.file.table.FileSystemTableFactory.createDynamicTableSource(FileSystemTableFactory.java:77) ~[flink-connector-files-1.16.1.jar:1.16.1]
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:163) ~[flink-table-api-java-uber-1.16.1.jar:1.16.1]
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:191) ~[flink-table-api-java-uber-1.16.1.jar:1.16.1]
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175) ~[?:?]
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3619) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2559) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2175) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2095) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2038) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:669) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:657) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3462) ~[?:?]
	at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:570) ~[?:?]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:215) ~[?:?]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:191) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:1498) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:1253) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:374) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNodeOrFail(SqlToOperationConverter.java:384) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:828) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:351) ~[?:?]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:262) ~[?:?]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
	at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
	... 12 more
{code}

The [document|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/connectors/table/hive/overview/#moving-the-planner-jar] states that if we're not using Hive dialect, we don't have to move the planner jar. However we clearly did not use Hive dialect in the above SQL, but the exception still happens.

This bug only happens with filesystem tables. Other tables do not have this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 06 02:54:43 UTC 2023,,,,,,,,,,"0|z1gxx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 11:51;martijnvisser;I believe the documentation is wrong: whenever you want to use Hive, you must move the Planner jar. That's because of the tight coupling between Hive and the planner at this moment;;;","30/Mar/23 06:46;luoyuxia;Not extactly,  it's not whenever as mentioned  ""this bug only happens with filesystem tables. Other tables do not have this issue"" by the reporter.

Back to this issue, the reason is we can't find the {{DialecFactory}} in FlinkUserClassloader and then we throw the excpetion. Also, I found 

we will catch the {{NoClassDefFoundError}} in {{{}FactoryUtil#discoverFactories,{}}}the context that why we catch {{NoClassDefFoundError}} can be found in [https://github.com/apache/flink/pull/17897.|https://github.com/apache/flink/pull/17897]

""when users bring in a format module, implementing {{{}BulkReaderFormatFactory{}}}/{{{}BulkWriterFormatFactory{}}}, but don't have flink-connector-files in the classpath because they don't use it. E.g. you're using {{flink-connector-kafka}} and you want to use {{{}flink-avro{}}}, which brings in an implementation of {{{}BulkWriterFormatFactory{}}}. ""

The reason of the exception reported in this Jira is similar, we bring  Hive connector as well as  {{HiveDialectFactory}} . So, in method {{formatFactoryExists#formatFactoryExists}} it will try to load {{org.apache.flink.table.planner.delegation.hive.HiveDialectFactory}} by java SPI, but can't find the class defination. So the exception throws.

To fix this issue, I would like to catch the {{NoClassDefFoundError}} just like what we did in {{{}FactoryUtil#discoverFactories{}}}.

But it'll be just a fix to 1.17 & 1.16 for I'm intended to decouple Hive with planner in 1.18, and after that the problem won't happen.;;;","30/Mar/23 07:09;martijnvisser;[~luoyuxia] I'm referring to the fact that the Hive connector is added and therefore loaded by Flink. That means that you're then ""using"" Hive (even if you don't specify it in your SQL statement). But happy to see that you have found how we can mitigate it :) Thanks for that;;;","06/Dec/23 02:54;luoyuxia;Should be fixed after https://issues.apache.org/jira/browse/FLINK-26603;;;",,,,,,,,,,,,,,,,,,,,,,,
Failed to receive dependency in Hive module,FLINK-31658,13530600,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mapohl,mapohl,29/Mar/23 11:12,05/Sep/23 16:15,04/Jun/24 20:41,29/Mar/23 19:33,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,,0,test-stability,,,,,"{{org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde}} failed to be retrieved:
{code}
[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.16-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-exec:jar:2.3.9 -> org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to conjars (http://conjars.org/repo): transfer failed for http://conjars.org/repo/org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde/pentaho-aggdesigner-algorithm-5.1.5-jhyde.pom, status: 503 Service Unavailable: Back-end server is at capacity -> [Help 1]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47662&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=7555",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19469,FLINK-27640,,,FLINK-33040,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 29 19:33:10 UTC 2023,,,,,,,,,,"0|z1gxw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 11:13;mapohl;Similar error appeared in FLINK-19469.;;;","29/Mar/23 11:15;mapohl;It's an infrastructure issue. [1.16 Build 20230328.3|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47631&view=results] ran on the same commit [822461b|https://github.com/flink-ci/flink-mirror/commit/822461b0bfb86a22efdbe68599b0a7b042c9d518].;;;","29/Mar/23 11:53;martijnvisser;It's Conjars who has an issue indeed;;;","29/Mar/23 12:06;mapohl;Do we know why it only affects 1.16? Or is it a coincident that the repo was available while the nightly for master and 1.17 passed this module build and 1.16 came too late?;;;","29/Mar/23 12:13;Sergey Nuyanzin;in 1.17.0 there was exclusion added https://issues.apache.org/jira/browse/FLINK-27640

probably need to do backport (i will do that);;;","29/Mar/23 19:33;Sergey Nuyanzin;I close it as a duplicate since originally PR was done under https://issues.apache.org/jira/browse/FLINK-27640;;;",,,,,,,,,,,,,,,,,,,,,
ConfigurationInfo generates incorrect openapi schema,FLINK-31657,13530599,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Mar/23 11:08,30/Mar/23 08:31,04/Jun/24 20:41,30/Mar/23 08:31,1.16.0,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Documentation,Runtime / REST,,,,,0,pull-request-available,,,,,"ConfigurationInfo extends ArrayList, and the schema generator picks up List#isEmpty as a property.
This results in an invalid schema, as arrays cant have properties.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 30 08:31:36 UTC 2023,,,,,,,,,,"0|z1gxvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 08:31;chesnay;master: bb24ec9cd84d6754ec1d99c6183e8c4ec0db7422
1.17: b5a6c5c9a0be89ccd70544546793db21c9a2c3b1
1.16: 8e470b52e6055ddf6871bf9e3f7dc8bbe9cc1447;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Obtain delegation tokens early to support external file system usage in blob server,FLINK-31656,13530590,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,29/Mar/23 09:56,06/Apr/23 18:01,04/Jun/24 20:41,03/Apr/23 07:32,1.18.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 11:29:33 UTC 2023,,,,,,,,,,"0|z1gxts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 07:32;gyfora;master: f3b63b7c962d9548d739698e00c4b5dde9a715a5;;;","03/Apr/23 11:29;gyfora;release-1.17: e827ea6fb69babf42d83a14f4bd3b2eabfb5720c;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive Channel selection for partitioner,FLINK-31655,13530574,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,tartarus,tartarus,tartarus,29/Mar/23 08:38,18/Jul/23 04:03,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,,,,,,"In Flink, if the upstream and downstream operator parallelism is not the same, then by default the RebalancePartitioner will be used to select the target channel.

In our company, users often use flink to access redis, hbase or other rpc services, If some of the Operators are slow to return requests (for external service reasons), then because Rebalance/Rescale are Round-Robin the Channel selection policy, so the job is easy to backpressure.

Because the Rebalance/Rescale policy does not care which subtask the data is sent to downstream, so we expect Rebalance/Rescale to refer to the processing power of the downstream subtask when choosing a Channel.

Send more data to the free subtask, this ensures the best possible throughput of job!

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 29 12:46:37 UTC 2023,,,,,,,,,,"0|z1gxq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 08:43;tartarus;[~gaoyunhaii]  [~fanrui]  Please take a look at this issue, after our company use the Adaptive Rebalance/Rescale, job throughput improved by about 20% on average compared to the community version.

We can discuss how we can contribute this feature to the community!

 

please assign to me, thanks~;;;","29/Mar/23 08:58;tartarus;cc [~zhuzh] ;;;","30/Mar/23 06:52;fanrui;Hi [~tartarus] , thanks for the good proposal, it's really a useful feature for flink users. And I see it has been assigned to you.

Would you mind using google doc to sort out the design doc first? It includes:
 * Motivation
 * User Interface: How flink users use this feature?
 * Judgment of pressure: How to judge the downstream channel pressure?
 * Rule for picking channel: How to pick a channel?;;;","31/Mar/23 04:08;gaoyunhaii;Hi [~tartarus] thanks for the proposal, it also looks useful from my side. 

There are also some discussion about it in the community previously , and some points are mentioned, like we might not introduce new locks and we might design carefully about the structure to maintain the active channels to avoid additional overhead, thus I also think if convenient you may propose a design doc thus we could first get consensus on the overall design. 

Also cc [~pltbkd] , who has also implemented the functionality previously in the internal version. ;;;","31/Mar/23 06:11;tartarus;Hi [~gaoyunhaii] [~fanrui] 

Thanks for your suggestions, I will produce a design doc as soon.;;;","11/Apr/23 13:34;akalash;Hi [~tartarus] thanks for this discussion.

I think I remember the same discussion which [~gaoyunhaii] mentioned above. I just want to add that I tried to implement it as well. You can take a look at it here - https://github.com/apache/flink/pull/16224. My solution works without any extra synchronization and locks but the performance for other rebalances(non-Adaptive) was impacted anyway. I changed only `BufferWritingResultPartition#emitRecord` and added a non-volatile variable to `PipelinedSubpartition` which was enough for small degradation in the benchmark since it is a hot path. 
I don't remember exactly why we stopped discussing it but we should be careful about it since Adaptive Rebalance is not so common usage and it will be unfortunate if we slow down more common rebalances in favor to have this less common one.;;;","13/Apr/23 03:42;tartarus;Hi [~akalash]  thanks for your reminder!

In our company, adaptive Partitioner is an optional optimization feature that is not enabled by default, and is usually enabled for jobs that have external heavy IO access. The performance overhead of select Channel is acceptable compared to the backpressure and lag caused by high load nodes, and can bring a positive gain of about 20%.

I wrote a doc the other day, [Adaptive Channel selection for partitioner|https://docs.google.com/document/d/1nH4hma8wyT8IcrwmJNfc9sBc5FXeyTtt96N-ZJ-ZgFk/edit?usp=sharing]. You are welcome to give some professional advice.

{*}Solution 1{*}：Simple to implement, but each selectChannel needs to be traversed, there will be a certain performance overhead; [Already verified online, I'm trying to test out the performance overhead at different parallelism in a similar way to benchmark.]
{*}Solution 2{*}：almost no performance overhead for select channel, but requiring additional operations in the hot path and the need to operate within locks, this solution would be more prudent and require more discussion and validation.

I looked at your implementation of LoadBasedRecordWriter, If I haven't missed any details, {color:#ff8b00}SubpartitionStatistic{color} is only updated at the time of emit, this statistic does not represent the true computing power of downstream operators. Maybe we want to achieve different goals, what I want to solve is to avoid the impact of high load nodes on flink job throughput.

Looking forward to more feedback.;;;","13/Apr/23 08:39;pltbkd;Hi [~akalash], thanks for the remind. 
I agree that the impaction to the normal rebalance must be measured. Though I wonder why the implementation in the pull request could impact significantly. Hopefully we can implement this time in a more controllable way and make sure the impaction is acceptable.

On the other hand, in my opinion, if the adaptive rebalance does little regression to the performance, maybe we can make it 'suggested' or even 'default', unless the users need all subtasks process exactly the same amount of records. 

Hi [~tartarus], 
Thanks for the doc! As we discussed offline, we'd better make the issue a FLIP and raise a formal discussion in the mailing list. As others mentioned, the feature was discussed before and faced quite some problems. I think we need a formal proposal with some implementation plans, then provide some benchmark results with a POC version, which include the gain of performance in applicable scenes, and the impaction to normal rebalance.;;;","13/Apr/23 14:10;pnowojski;I think there are ways to address problems with [~akalash] proposal, without adding too much overhead (also checking {{getWritingThreadTotalNumberOfSentBytes}} for the channel that we haven't selected?). I think both of your proposed solutions would be quite costly [~tartarus], adding extra synchronisations that we don't necessarily need with some clever heuristic.

I agree with [~pltbkd], we would probably need a FLIP, as this will be changing public APIs (either changing behaviour of an existing {{rebalance()}}, or adding a new one like {{loadRebalance()}}). ;;;","13/Apr/23 15:41;akalash;??I wonder why the implementation in the pull request could impact significantly??

It wasn't significant impact. As I remember there were two sensitive microbenchamrks which showed a drop of about 2-3%. Maybe we even could accept it but we haven't discussed it properly


??I looked at your implementation of LoadBasedRecordWriter If I havent missed any details SubpartitionStatistic is only updated at the time of emit this statistic does not represent the true computing power of downstream operators??

Yes, you are absolutely right that it is updated only at the time of emit which is not so perfect. But it was my small POC I just tried to show that it is possible to do without extra locks.

The logic in my current PR:
* During the emit we take `currentChannel` and `nextChannel`(currentChannel + 1)
* If `bytesInQueue(currentChannel)` > `bytesInQueue(nextChannel)` then we set `nextChannel` as `currentChannel` otherwise leave `currentChannel` as is

As already was noticed, the current solution can be stuck on one channel forever since it never updates the bytesInQueue for other channels but current.

The improved logic can look like this:
* We have `roundRobinChannel`(the channel which increased by 1 on every emit)
* During the emit we increase `roundRobinChannel` by 1
* We update `bytesInQueue` for `roundRobinChannel`
* Now, If `bytesInQueue(currentChannel)` > `bytesInQueue(roundRobinChannel)` then we set `roundRobinChannel` as `currentChannel` otherwise leave `currentChannel` as is

So if we have N channels then each channel will be updated with each N-th record which is accurate enough for our goal I think. But it is not clear yet how to update `roundRobinChannel` cheaply.


??Solution 1??
I still wonder about the implementation of getting buffer queue sizes since it is the tricky part. I solved this problem somehow in my PR but anyway, it still questions there.
As well as [~pnowojski], I also have concerns about the efficiency of this solution since it is O(ChannelsCount) that doesn't look optimal. But we can just optimize that algorithm to something like I describe before or similar.

??Solution 2??
I doubt that we will be ready for extra synchronization on the hot path especially in common code for all partitioners. But of course, we can discuss it anyway but only with more specific implementation ideas.

In conclusion, let's proceed with FLIP. And we can think:
* How to effectively get channel queue size
* How to effectively calculate the next channel;;;","14/Apr/23 02:51;tartarus;[~akalash] [~pnowojski] [~pltbkd] Thank you very much for your suggestions and information, it was very valuable to me!

I will prepare a FLIP as soon, I tend to propose a new API to for Adaptive Partitioner, so as not to bring any impact on the user's existing jobs, adaptive Partitioner as an optional optimization attempt, 

even though there is some performance overhead, users may be able to accept.;;;","14/Jun/23 07:56;fanrui;Hi [~tartarus] , some of our flink scenarios also need to use adaptive rebalance, for example: sink data to hdfs, and some subtasks meet slow datanode, flink can forward data to idle subtasks. 

We have finished the internal version, and it works well. I read all discussions in detail and I want to share our solution here. It's similar your solution1, and solved the performance issue.

We defined an option: dynamic-rebalance.max-traverse-size. It means we don't traverse all sub-partitions, we just traverse the next traverse size based on current channel. For general logic, we find the most idle channel from channel[currentChannel+1] to channel[currentChannel+maxTraverseSize].

For example, *dynamic-rebalance.max-traverse-size=10* , *sub-partitions size = 100* and {*}currentChannel=5{*}. When sending the a record, we find the most idle channel from channel-6 to channel-15 instead of all channels, and mark the most idle channel as the currentChannel.
h2. Why do we choose this solution?
 # On our production, just a small number of subtasks is very slow, and a large number of subtasks is healthy. And the max-traverse-size=10 is enough to skip these slow subtasks.
 # It solved the performance issue of solution1.
 # It doesn't introduce extra lock or collection to maintain the idle channel set than solution2.
 # We improved the *max-traverse-size* strategy. We will choose the channel directly if we found the pending size of any channel is 0 from channel[currentChannel+1] to channel[currentChannel+maxTraverseSize]. Because the channel is most idle channel when pending size is 0.

Here is core code:
{code:java}
private void chooseLessLoadedSubPartition() {
    long bytesInQueue = Long.MAX_VALUE;
    int candidateChannel = 0;
    for (int i = 1; i <= maxTraverseSize; i++) {
        int channel = (currentChannel + i) % numberOfSubpartitions;
        long bytesInQueueCurrent = targetPartition.getBytesInQueueUnsafe(channel);

        if (bytesInQueueCurrent == 0) {
            // If there isn't any pending data in the current channel, choose this channel
            // directly.
            currentChannel = channel;
            return;
        }
        if (bytesInQueueCurrent < bytesInQueue) {
            candidateChannel = channel;
            bytesInQueue = bytesInQueueCurrent;
        }
    }
    currentChannel = candidateChannel;
} {code}
 

Looking forward to your feedback and suggestion, thanks.;;;","16/Jun/23 09:33;tartarus;[~fanrui]  Thank you very much for sharing your solution! 

Internally, I also optimized solution 1, not to find the global optimal channel, but to find the optimal one in most of the channels.

 

I did a performance test for the channel select policy and collected information on the time taken for 10,000 calls, like follows:

*Time consumption in nanoseconds:*

 
||subpartition number||{color:#de350b}10(avg){color}||10(P95)||10(P99)||{color:#de350b}100(avg){color}||100(P95)||100(P99)||{color:#de350b}200(avg){color}||200(P95)||200(P99)||{color:#de350b}300(avg){color}||300(P95)||300(P99)||{color:#de350b}500(avg){color}||500(P95)||500(P99)||{color:#de350b}800(avg){color}||800(P95)||800(P99)||{color:#de350b}1000(avg){color}||1000(P95)||1000(P99)||{color:#de350b}2000(avg){color}||2000(P95)||2000(P99)||{color:#de350b}3000(avg){color}||3000(P95)||3000(P99)||{color:#de350b}5000(avg){color}||5000(P95)||5000(P99)||{color:#de350b}8000(avg){color}||8000(P95)||8000(P99)||{color:#de350b}10000(avg){color}||10000(P95)||10000(P99)||
|Rebalance|{color:#de350b}68{color}|90|102|{color:#de350b}66{color}|94|116|{color:#de350b}69{color}|86|96|{color:#de350b}68{color}|104|199|{color:#de350b}68{color}|89|99|{color:#de350b}74{color}|118|131|{color:#de350b}67{color}|88|99|{color:#de350b}67{color}|90|104|{color:#de350b}58{color}|93|110|{color:#de350b}70{color}|89|95|{color:#de350b}61{color}|81|92|{color:#de350b}63{color}|86|99|
|solution 1(Global Optimal)|{color:#de350b}203{color}|372|1411|{color:#de350b}836{color}|1285|3488|{color:#de350b}1152{color}|2419|6547|{color:#de350b}1486{color}|3672|9346|{color:#de350b}1926{color}|5105|12107|{color:#de350b}2899{color}|9285|21338|{color:#de350b}4569{color}|13939|25574|{color:#de350b}8023{color}|7929|24980|{color:#de350b}11701{color}|12371|34079|{color:#de350b}19364{color}|19910|53594|{color:#de350b}32792{color}|42199|104582|{color:#de350b}37397{color}|39548|73330|
|optimized solution 1(Most of the best)|{color:#de350b}214{color}|360|1652|{color:#de350b}1223{color}|1447|4509|{color:#de350b}1508{color}|2613|5816|{color:#de350b}1808{color}|3815|8756|{color:#de350b}2124{color}|5962|12336|{color:#de350b}3048{color}|9012|21896|{color:#de350b}3277{color}|10599|28300|{color:#de350b}7403{color}|18832|37264|{color:#de350b}10543{color}|22218|43845|{color:#de350b}15944{color}|20266|62732|{color:#de350b}28271{color}|41126|54055|{color:#de350b}36096{color}|52062|82243|
|Traverse 5 channels|{color:#de350b}213{color}|357|1268|{color:#de350b}190{color}|461|984|{color:#de350b}169{color}|290|941|{color:#de350b}239{color}|393|1221|{color:#de350b}220{color}|566|1240|{color:#de350b}213{color}|510|1100|{color:#de350b}209{color}|478|1041|{color:#de350b}230{color}|638|1336|{color:#de350b}190{color}|472|964|{color:#de350b}183{color}|388|945|{color:#de350b}178{color}|516|978|{color:#de350b}224{color}|511|1148|
|Traverse 10 channels|{color:#de350b}226{color}|440|1776|{color:#de350b}305{color}|536|1601|{color:#de350b}227{color}|477|1785|{color:#de350b}297{color}|520|1564|{color:#de350b}243{color}|507|1672|{color:#de350b}220{color}|472|1422|{color:#de350b}276{color}|487|1443|{color:#de350b}280{color}|536|2042|{color:#de350b}254{color}|535|1895|{color:#de350b}254{color}|495|1976|{color:#de350b}249{color}|472|1691|{color:#de350b}293{color}|548|1631|
|Traverse 20 channels|{color:#de350b}244{color}|495|1706|{color:#de350b}298{color}|421|973|{color:#de350b}296{color}|413|961|{color:#de350b}394{color}|725|844|{color:#de350b}279{color}|387|854|{color:#de350b}406{color}|695|845|{color:#de350b}360{color}|499|1144|{color:#de350b}335{color}|478|1112|{color:#de350b}302{color}|495|960|{color:#de350b}392{color}|703|844|{color:#de350b}400{color}|688|866|{color:#de350b}343{color}|681|1030|

The above table shows that the time complexity of Rebalance is O(1), while our optimization is O( n ), which is consistent with expectations. We are constantly trying to reduce the N.

`maxTraverseSize` solution, which can be selected by the user according to the actual situation to make the best choice!

I suggest we use a combination of option and api to set `maxTraverseSize`,  

If maxTraverseSize is set through api, we can override the value of option; if not set through api, then use the value of option, because api can do the granularity of each adaptiveRebalance, while option is flink job granularity; but in SQL job the user can only set through option;

The public api can be defined as follows:

 
{code:java}
public DataStream<T> adaptiveRebalance(int maxTraverseSize) {code}
 

 

[~pnowojski] [~akalash] [~pltbkd]   what do you think?

Looking forward to your feedback and suggestion, thanks.

 ;;;","24/Jun/23 05:29;fanrui;Hi [~tartarus] , thanks for the hard testing.

From the result, I think the maxTraverseSize is a good choice! The *Time consumption* won't increase when the subpartition number is high.

I still have some questions:
 * What's the difference between solution 1(Global Optimal) and optimized solution 1(Most of the best)?
 * When maxTraverseSize > the subpartition number, the maxTraverseSize should be the subpartition number. For example,  maxTraverseSize = 20, and the subpartition number is 10.
 * Do we need to set maxTraverseSize through api? The option is enough for most of cases.
 * How the SQL job to use the adaptiveRebalance? The rebalance should be the default.

Anyway, it's the time to start a FLIP, and anyone can discuss the adaptiveRebalance in the mail list~;;;","26/Jun/23 02:55;wangm92;[~tartarus] thanks for the hard testing too, and can you share how much this time-consuming increase has affected the performance of the entire job?

we also use a similar functions internally, but the implementation is slightly different. In order to avoid the traversal of each operation in most cases, we limit the maximum buffersInBacklog for each Channel(In extreme cases it can also lead to a global traversal):
1. As long as the buffersInBacklog of this Channel does not exceed the threshold, use this Channel directly, otherwise, search for the next channel;
2. When processing the next data, the same logical judgment will be made for lastChannelId + 1.

Looking forward to the FLIP discussion;;;","28/Jun/23 11:12;pnowojski;I also like the idea with {{maxTraverseSize}} option. I have a feeling that in the real world, values as small as 5 could be good enough. If not, and we really need values like ~20 or larger, we could also think about something like:
* For {{N}}'th {{adaptiveRebalance}} call, where {{N % 10 == 1}}
** iterate 20 channels
** discard 10 worst channels, use 1st best channel
* For {{N}}'th {{adaptiveRebalance}} call, where {{N % 10 != 1}}
** use remaining 9 best channels from last iteration

But I doubt it's really needed. Even for {{maxTraverseSize==5}}, we can always completely skip up to 4 slow channels if they are completely bottlenecked and bundled together. If we have more slow channels bundled together, those channels would be only getting {{20%}} load compared to the fast channels. So this solution would be still working perfectly fine, if ""slow"" machine/machines are up to 5x slower compared to normal machines.

I'm also looking forward to the FLIP. If I haven't made a mistake in the previous paragraph I think it would be good to add it to the FLIP as a justification why we don't need large {{maxTraverseSize}}.

Btw, before publishing a FLIP, could you run {{mapRebalanceMapSink.F27_UNBOUNDED}} benchmark from [flink-benchmarks|https://github.com/apache/flink-benchmarks] (you can check readme and especially [the general remarks|https://github.com/apache/flink-benchmarks#general-remarks]) and compare normal rebalance with your adaptive one? This benchmark is defined in {{src/main/java/org/apache/flink/benchmark/InputBenchmark.java}} file.

;;;","29/Jun/23 12:34;tartarus;hi [~fanrui]  thank you for your reply, I will answer your concerns one by one

{color:#c1c7d0}_>What's the difference between solution 1(Global Optimal) and optimized solution 1(Most of the best)?_{color}

{color:#172b4d}the solution 1 (Global Optimal) is Each record of data is traversed through all subpartitions to find the best channel. So the more subpartitions, the more time it takes!{color}

{color:#172b4d}_optimized solution 1(Most of the best) is When a record is comes in, suppose it should be written to subpartition x, and if index x is in the first half of the subpartition array, then we find the optimal subpartition index from x to the end of subpartition array; if index x is in the second half of the subpartition array, then we find the optimal subpartition index from 0 to x;_ {color}

{color:#c1c7d0}_>When maxTraverseSize > the subpartition number, the maxTraverseSize should be the subpartition number. For example,  maxTraverseSize = 20, and the subpartition number is 10._{color}

Yes, I have taken this case into account when testing

{color:#c1c7d0}_>How the SQL job to use the adaptiveRebalance? The rebalance should be the default._{color}

If we introduce the new api adaptiveRebalance, how will SQL jobs use it and will SQL jobs not support adaptiveRebalance for now?

 

I will refer to everyone's comments to complete the FLIP;;;","29/Jun/23 12:41;tartarus;Hi [~wangm92]  Thank you for sharing, your solution sounds similar to [~fanrui] 

I tested solution 1 (global optimum) with 100 parallels, Throughput is roughly 40% lower (without any business logic, completely testing select channel performance).

So I think maxTraverseSize is a good solution.;;;","29/Jun/23 12:46;tartarus;Hi [~pnowojski]  thank you for your reply.

I currently tend to use maxTraverseSize==5 to implement, more complex algorithmic strategies, we can discuss later, I will refer to your suggestion and use flink benchmark to test it.

FLIP will come soon;;;",,,,,,,,
DataStreamUtils.reinterpretAsKeyedStream() should not override the user specified chaining strategy.,FLINK-31654,13530570,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,becket_qin,becket_qin,becket_qin,29/Mar/23 08:21,11/Mar/24 12:44,04/Jun/24 20:41,,1.14.6,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,API / DataStream,,,,,,0,,,,,,"Currently {{DataStreamUtils.reinterpretAsKeyedStream()}} does not work well with batch jobs. Currently the chaining strategy of the StreamOperators applied to a KeyedStream is always overridden to HEAD. This is because in batch execution mode the records have to be sorted by keys before they are fed to the stateful operators. The runtime relies on the shuffle to do the sort so a shuffle is needed for the stateful operators.

However, for {{DataStreamUtils.reinterpretAsKeyedStream()}} this results in unexpected behavior. It breaks the operator chain and defeats the purpose of reinterpreting the stream instead of calling {{keyBy.}}

To fix this issue, we need to do the following for reinterpretAsKeyedStream:
 # Add a sort operator instead of relying on the shuffle to do the sort.
 # Stop overriding the chaining strategy specified by the user for the operators applied to the result KeyedStream.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-29 08:21:58.0,,,,,,,,,,"0|z1gxpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using`if` statement for a string subtype of the row type may meet npe in code generated by codegen,FLINK-31653,13530563,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,catyee,catyee,catyee,29/Mar/23 07:37,30/Mar/23 06:31,04/Jun/24 20:41,30/Mar/23 06:31,1.15.4,1.16.1,1.17.1,1.18.0,,,,,,,,,,,,,,,,,1.15.5,1.16.2,1.17.1,1.18.0,,,,,,Table SQL / Planner,,,,,,0,,,,,,"When I use the `if` statement for the string subtype of the row type, the code generated by `IfCallGen` may meet npe. The case is:
{code:java}
-- if the value of 'nested' colmun is row(null, 10), the job will meet npe
CREATE TABLE NestedTable (
  nested row<name string, `value` int>
) WITH (
  ...
);

select
    if (nested.name is null, 'JKL', nested.name) AS nestedName
from NestedTable {code}
Three conditions are required to reproduce the bug:

1、use `if` statement

2、string subtype of row type

3、the value of string subtype element is null

 

After excluding the effect of java code spilitter, the picture shows the original code generated by codegen(you can find the complete code generated by codegen in attached file: [^StreamExecCalc$17])，the assignment logic for `isNull$7` is between the line 140 and the line 152, so the value of `isNull$7` in line 94 is always false(the default value), and the line 96 will meet npe when `result$7` is null.

!image-2023-03-29-16-06-19-351.png|width=703,height=533!

To fix this issue, we can change the location of the logic to generate casted codes in `IfCallGen`, like this:

!image-2023-03-29-15-33-37-983.png|width=829,height=361!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31007,FLINK-30559,FLINK-30966,,,FLINK-24413,,,,,,,,,,,,,,,,,"29/Mar/23 07:50;catyee;StreamExecCalc$17;https://issues.apache.org/jira/secure/attachment/13056886/StreamExecCalc%2417","29/Mar/23 07:33;catyee;image-2023-03-29-15-33-37-983.png;https://issues.apache.org/jira/secure/attachment/13056881/image-2023-03-29-15-33-37-983.png","29/Mar/23 08:06;catyee;image-2023-03-29-16-06-19-351.png;https://issues.apache.org/jira/secure/attachment/13056887/image-2023-03-29-16-06-19-351.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 30 06:25:08 UTC 2023,,,,,,,,,,"0|z1gxns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 08:16;yunta;[~catyee] Thanks for creating this ticket.
I think this is a valid and easy-to-reproduce bug. cc [~jark], [~twalthr], [~matriv];;;","29/Mar/23 08:56;martijnvisser;[~catyee] Is this a duplicate of FLINK-31007 ?;;;","30/Mar/23 06:25;catyee;[~martijnvisser] It does seem to be duplicated with the others, I will mark this jira duplicated with the smaller jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Flink should handle the delete event if the pod was deleted while pending,FLINK-31652,13530560,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiasun,xiasun,xiasun,29/Mar/23 06:57,03/Apr/23 08:05,04/Jun/24 20:41,03/Apr/23 08:05,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Deployment / Kubernetes,,,,,,0,pull-request-available,,,,,"I found that in kubernetes deployment, if the taskmanager pod is deleted in 'Pending' phase, the flink job will get stuck and keep waiting for the pod scheduled. We can reproduce this issue with the 'kubectl delete pod' command to delete the pod when it is in the pending phase.
 
The cause reason is that the pod status will not be updated in time in this case, so the KubernetesResourceManagerDriver won't detect the pod is terminated, and I also verified this by logging the pod status in KubernetesPod#isTerminated(), and it shows as follows.
{code:java}
public boolean isTerminated() {
    log.info(""pod status: "" + getInternalResource().getStatus());
    if (getInternalResource().getStatus() != null) {
        final boolean podFailed =
                PodPhase.Failed.name().equals(getInternalResource().getStatus().getPhase());
        final boolean containersFailed =
                getInternalResource().getStatus().getContainerStatuses().stream()
                        .anyMatch(
                                e ->
                                        e.getState() != null
                                                && e.getState().getTerminated() != null);
        return containersFailed || podFailed;
    }
    return false;
} {code}
In the case, this function will return false because `containersFailed` and `podFailed` are both false.
{code:java}
PodStatus(conditions=[PodCondition(lastProbeTime=null, lastTransitionTime=2023-03-28T12:35:10Z, reason=Unschedulable, status=False, type=PodScheduled, additionalProperties={})], containerStatuses=[], ephemeralContainerStatuses=[], hostIP=null, initContainerStatuses=[], message=null, nominatedNodeName=null, phase=Pending, podIP=null, podIPs=[], qosClass=Guaranteed, reason=null, startTime=null, additionalProperties={}){code}
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 08:05:32 UTC 2023,,,,,,,,,,"0|z1gxn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 07:10;xiasun;Hi [~xtsong] , could you please help to take a look at this ticket in your free time, thanks!;;;","29/Mar/23 11:24;xtsong;[~xiasun], thanks for reporting this. I think this is a valid issue.

Would you like to open a pull request to fix this?;;;","29/Mar/23 12:23;xiasun;[~xtsong] Thanks for the reply! I am glad to take this ticket.;;;","30/Mar/23 01:10;xtsong;[~xiasun] You are assigned. Please go ahead.;;;","03/Apr/23 08:05;xtsong;- master (1.18): 9e83858c7dc309f272a03c62b1e295d192acaf89
- release-1.17: 98d8e2712feb5077c4b35698cea5ecbdd72e4c06
- release-1.16: 99c025b438a99eb8ddcf8214aba5f285972106ca;;;",,,,,,,,,,,,,,,,,,,,,,
Improve logging of granting/revoking leadership in JobMasterServiceLeadershipRunner to INFO level,FLINK-31651,13530532,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,29/Mar/23 03:17,30/Mar/23 09:59,04/Jun/24 20:41,30/Mar/23 09:59,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"Currently, the log level of granting/revoking leadership of JobMasterServiceLeadershipRunner is DEBUG. However, we usually configure it to INFO level in production jobs, which make it hard to understand the behaviour from Flink's logs because JobMasterServiceLeadershipRunner may suddenly be stopped. I suggest to improve the logging to INFO level.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 30 09:59:05 UTC 2023,,,,,,,,,,"0|z1gxgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 09:59;wanglijie;Done via

master: 4882fbd9744d456e09ca60b6c7cf7a5b60326c73

release-1.17: b398162c5f0212505eb3fe6e6d8ff4e2485eb4db

release-1.16: ec386ae8e8febd14a6a5ca03769ff7ea0510fd4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect busyMsTimePerSecond metric value for FINISHED task,FLINK-31650,13530529,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,wanglijie,wanglijie,29/Mar/23 02:49,26/Dec/23 02:00,04/Jun/24 20:41,26/Dec/23 01:59,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,1.17.3,1.18.1,1.19.0,,,,,,,Runtime / Metrics,Runtime / REST,,,,,0,pull-request-available,,,,,"As shown in the figure below, the busyMsTimePerSecond of the FINISHED task is 100%, which is obviously unreasonable.
!busyMsTimePerSecond.png|width=1048,height=432!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/23 02:49;wanglijie;busyMsTimePerSecond.png;https://issues.apache.org/jira/secure/attachment/13056876/busyMsTimePerSecond.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 26 01:56:44 UTC 2023,,,,,,,,,,"0|z1gxg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 03:14;wanglijie;I guess this is because the value of busyMsTimePerSecond is only periodically updated while the task is running. Once the task is finished, this metric will never be updated, so the currently displayed value is the lastest updated results while the task was running;;;","30/Mar/23 08:13;JunRuiLi;[~wanglijie] Could I take this ticket? Thx.;;;","29/Aug/23 03:01;Zhanghao Chen;Hi [~JunRuiLi], we encountered the same problem. Are you still working on it? If not, I'd be willing to take over this ticket.;;;","30/Aug/23 02:09;JunRuiLi;Hi [~Zhanghao Chen], thank you for your interest in the issue. I'm sorry for not being able to start working on it due to my busy schedule. If you are willing to take over this ticket, please feel free to proceed. I appreciate your help! cc [~wanglijie] ;;;","30/Aug/23 02:11;wanglijie;Assigned to you [~Zhanghao Chen] ;;;","20/Sep/23 16:47;Zhanghao Chen;[~wanglijie] [~JunRuiLi] Hi, I've created a PR that solves this issue by cleaning up terminal subtask attempt metrics and FINISHED tasks will show N/A for busy/backpressured on UI now. Could you help review it when you are free?;;;","26/Dec/23 01:56;wanglijie;Fixed via:
master(1.19) : dd028282e8ab19c0d1cd05fad02b63bbda6c1358
release-1.18: ff1ef789eba612d008424d5fc28fe5905f96fb9c
release-1.17: 024e970e27781a9fd1925d6c5938efbb364c6462;;;",,,,,,,,,,,,,,,,,,,,
[Connectors/Jdbc] Fix archunit violations,FLINK-31649,13530458,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,eskabetxe,eskabetxe,eskabetxe,28/Mar/23 15:01,06/Jun/23 13:28,04/Jun/24 20:41,18/Apr/23 02:59,,,,,,,,,,,,,,,,,,,,,jdbc-3.1.1,jdbc-3.2.0,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 06 13:27:45 UTC 2023,,,,,,,,,,"0|z1gx0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 02:58;wanglijie;Fixed in main: 508b152bf7ee50643b6b3844f20289b6461e3c25 ... 396f205bcbed133d1e7dc0d6c3ce9487dcbc1dd7;;;","06/Jun/23 13:27;martijnvisser;Also fixed in branch v3.1: 8492962db3ccef3412e4fde53f540bdfa64cfec7 .. 50e57080892adf3b9859d80bcc5bcb8695bdfbcf;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch supports retrying writes when concurrency issues occur,FLINK-31648,13530400,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,empcl,empcl,empcl,28/Mar/23 10:24,11/Aug/23 22:35,04/Jun/24 20:41,,1.14.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,,0,pull-request-available,stale-assigned,,,,"Elasticsearch supports retrying writes when concurrency issues occur.The exception information is as follows:

ElasticsearchException[Elasticsearch exception [type=version_conflict_engine_exception, reason=[******][******]: version conflict, current version [2] is different than the one provided [1]]]]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 22:35:10 UTC 2023,,,,,,,,,,"0|z1gwnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 12:31;martijnvisser;[~empcl] Please open your PR towards the connector repository at https://github.com/apache/flink-connector-elasticsearch ;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Introduce remote storage tier reader,FLINK-31647,13530371,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,tanyuxin,tanyuxin,28/Mar/23 07:33,18/Jul/23 02:03,04/Jun/24 20:41,18/Jul/23 02:03,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 18 02:03:28 UTC 2023,,,,,,,,,,"0|z1gwh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 02:03;Weijie Guo;master(1.18) via 9c3c8afbd9325b5df8291bd831da2d9f8785b30a.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce remote storage tier,FLINK-31646,13530370,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,28/Mar/23 07:33,06/Jul/23 13:08,04/Jun/24 20:41,06/Jul/23 13:07,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 06 13:07:37 UTC 2023,,,,,,,,,,"0|z1gwgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/23 13:07;tanyuxin;master(1.18) via 32044ddd358ba0954a841962d93cce4c4afe264a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce disk tier reader,FLINK-31645,13530369,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,tanyuxin,tanyuxin,28/Mar/23 07:32,04/Jul/23 11:56,04/Jun/24 20:41,04/Jul/23 11:56,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 04 11:56:57 UTC 2023,,,,,,,,,,"0|z1gwgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/23 11:56;Weijie Guo;master(1.18) via c4e55b588d1e5831702a76adbca7c1428d754a9b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce disk tier,FLINK-31644,13530368,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,28/Mar/23 07:32,18/Sep/23 06:14,04/Jun/24 20:41,30/Jun/23 14:26,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 30 14:26:38 UTC 2023,,,,,,,,,,"0|z1gwgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 14:26;Weijie Guo;master(1.18) via 00e10cd034d1a92c7f83b42bdd9a293775d33a3f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the configurations for tiered storage,FLINK-31643,13530367,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,tanyuxin,tanyuxin,28/Mar/23 07:31,23/Jul/23 16:18,04/Jun/24 20:41,23/Jul/23 16:18,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 23 16:18:00 UTC 2023,,,,,,,,,,"0|z1gwg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/23 16:18;Weijie Guo;master(1.18) via 48d4d5ccd8dc2aaf8a3651583b32950f6ec21d9c.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce memory tier reader,FLINK-31642,13530366,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,tanyuxin,tanyuxin,28/Mar/23 07:29,25/Jun/23 04:24,04/Jun/24 20:41,20/Jun/23 11:39,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 20 11:39:34 UTC 2023,,,,,,,,,,"0|z1gwg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 11:39;Weijie Guo;master(1.18) via a83dd9385928a9693bbda859aeff016a956d0647.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce memory tier,FLINK-31641,13530365,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,28/Mar/23 07:29,18/Sep/23 06:12,04/Jun/24 20:41,14/Jun/23 02:46,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 14 02:46:33 UTC 2023,,,,,,,,,,"0|z1gwfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 02:46;Weijie Guo;master(1.18) via 54e9eca28cf74bb498b857d5a421b574f7302abc.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Write the accumulated buffers to the right storage tier,FLINK-31640,13530364,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,28/Mar/23 07:24,05/Jun/23 08:12,04/Jun/24 20:41,05/Jun/23 07:49,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 05 07:49:22 UTC 2023,,,,,,,,,,"0|z1gwfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 07:49;Weijie Guo;master(1.18) via daf476f76bd3ff58e34a01848a76ee1c985b2fc3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce tiered storage memory manager,FLINK-31639,13530362,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,28/Mar/23 07:22,18/Sep/23 06:10,04/Jun/24 20:41,23/May/23 11:33,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 23 11:33:25 UTC 2023,,,,,,,,,,"0|z1gwf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 11:33;xtsong;master (1.18): c6d7747eaef166fb7577de55cb2943fa5408d54e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Downstream supports reading buffers from tiered storage,FLINK-31638,13530360,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,tanyuxin,tanyuxin,28/Mar/23 07:19,05/Jul/23 02:37,04/Jun/24 20:41,15/May/23 03:00,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 15 03:00:12 UTC 2023,,,,,,,,,,"0|z1gweo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 03:00;xtsong;master (1.18): 7d9027dbb3ae551a86c385f985e5fe9af2cbdbac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement HashBufferAccumulator,FLINK-31637,13530359,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,28/Mar/23 07:18,31/May/23 07:49,04/Jun/24 20:41,31/May/23 03:13,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 31 03:13:41 UTC 2023,,,,,,,,,,"0|z1gweg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/23 03:13;Weijie Guo;master(1.18) via 9880ba5324d4a1252d6ae1a3f0f061e4469a05ac.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upstream supports reading buffers from tiered storage,FLINK-31636,13530357,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,tanyuxin,tanyuxin,28/Mar/23 07:16,05/Jul/23 02:37,04/Jun/24 20:41,09/Jun/23 14:49,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 09 14:49:26 UTC 2023,,,,,,,,,,"0|z1gwe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 14:49;Weijie Guo;master(1.18) via 4e82ab0c37ace2a10dbfa3c4bf72659a23f3c8f1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support writing records to the new tiered storage architecture,FLINK-31635,13530356,13530330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,28/Mar/23 07:14,18/Sep/23 06:06,04/Jun/24 20:41,11/May/23 01:37,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,"Support writing records to the new tiered store architecture.

To achieve the goal, this mainly includes the following two parts. 
1. Introduces the tiered storage architecture.
2. The producer-side implementation of the architecture",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32094,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 11 01:38:08 UTC 2023,,,,,,,,,,"0|z1gwds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/23 01:38;xtsong;master (1.18): 80a924309ce910715c4079c7e52e9d560318bd38;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-301: Hybrid Shuffle supports Remote Storage,FLINK-31634,13530330,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,28/Mar/23 04:36,01/Aug/23 10:08,04/Jun/24 20:41,24/Jul/23 08:27,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,,1,Umbrella,,,,,"This is an umbrella ticket for [FLIP-301|https://cwiki.apache.org/confluence/display/FLINK/FLIP-301%3A+Hybrid+Shuffle+supports+Remote+Storage].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32675,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 26 07:27:55 UTC 2023,,,,,,,,,,"0|z1gw88:",9223372036854775807,"Hybrid Shuffle supports storing the shuffle data in remote storage. The remote storage path can be configured with the option `taskmanager.network.hybrid-shuffle.remote.path`. Hybrid Shuffle uses less network memory than before by decoupling the memory usage from the number of parallelisms, improving the stability and ease of use.  For more detailed information, please refer to https://nightlies.apache.org/flink/flink-docs-master/docs/ops/batch/batch_shuffle/#hybrid-shuffle",,,,,,,,,,,,,,,,,,,"25/Jul/23 11:54;knaufk;[~tanyuxin] Is there user facing documentation for this already? If not, is there a sub-task that tracks this?;;;","26/Jul/23 02:36;tanyuxin;[~knaufk] Thanks for helping check the doc. I have created a task https://issues.apache.org/jira/browse/FLINK-32675 , and I will do it ASAP.;;;","26/Jul/23 07:27;knaufk;Thanks, [~tanyuxin].;;;",,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect can't order by an unselected column,FLINK-31633,13530327,13510724,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,28/Mar/23 03:53,28/Mar/23 03:53,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,,0,,,,,,"With hive dialect, the following sql will throw exception:
{code:java}
create table (a int, b int);
select a from t order by b; {code}
It has been fixed by HIVE-15160. We can apply this patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-28 03:53:01.0,,,,,,,,,,"0|z1gw7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
watermark aligned idle source can't resume,FLINK-31632,13530316,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,haishui,haishui,haishui,28/Mar/23 02:07,26/Apr/23 02:57,04/Jun/24 20:41,20/Apr/23 16:56,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,API / DataStream,,,,,,0,pull-request-available,,,,," 
{code:java}
WatermarkStrategy<String> watermarkStrategy = WatermarkStrategy
        .<String>forBoundedOutOfOrderness(Duration.ofMillis(0))
        .withTimestampAssigner((element, recordTimestamp) -> Long.parseLong(element))
        .withWatermarkAlignment(""group"", Duration.ofMillis(10), Duration.ofSeconds(2))
        .withIdleness(Duration.ofSeconds(10)); 
DataStreamSource<String> s1 = env.fromSource(kafkaSource(""topic1""), watermarkStrategy, ""S1"");
DataStreamSource<String> s2 = env.fromSource(kafkaSource(""topic2""), watermarkStrategy, ""S2"");{code}
send ""0"" to kafka topic1 and topic2

 

After 10s, source1 and source2 is idle，and logs are

 
{code:java}
09:44:30,403 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:30,404 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:32,019 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:32,019 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:32,417 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:32,418 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:34,028 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:34,028 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:34,423 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:34,424 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:36,023 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=-9223372036854775799 to subTaskIds=[0]
09:44:36,023 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=-9223372036854775799 to subTaskIds=[0]
09:44:36,433 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:36,433 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0 {code}
send message to topic1 or topic2 now, the message can't be consumed。

 

the reason is: 

when a source is marked idle, the lastEmittedWatermark = Long.MAX_VALUE and 
currentMaxDesiredWatermark = Long.MAX_VALUE + maxAllowedWatermarkDrift in org.apache.flink.streaming.api.operators.SourceOperator.
currentMaxDesiredWatermark is negative and always less than lastEmittedWatermark
operatingMode always is WAITING_FOR_ALIGNMENT",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 20 16:56:42 UTC 2023,,,,,,,,,,"0|z1gw54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 14:42;martijnvisser;[~tzulitai] WDYT?;;;","29/Mar/23 03:23;haishui;This problem seems to be caused by the overflow of maxAllowedWatermark in SourceCoordinator.;;;","14/Apr/23 08:28;haishui;Hi there. The PR may be ready to merge. When can it get into 1.15 1.16 and 1.17 branch? It's the first time for me to report and fix a bug, and I don't know what next to do.;;;","14/Apr/23 16:44;tzulitai;Hey [~haishui], I'm currently taking a look at the PR as well.

Looks like a real issue, thanks for reporting it. I'll backport it to 1.17 / 1.15 / 1.16 once it is merged to \{{master}} .;;;","20/Apr/23 16:56;tzulitai;* 1.18.x: a1daa989e96c7cd1d574a8e998c39b54a4cc503e
 * 1.17.x298fd32b1286e1e5f47fcedbb56d4fee7cfc4eb1
 * 1.16.x: 8d98e6981f9ec137557148a520fac5b81fee1131;;;",,,,,,,,,,,,,,,,,,,,,,
Upgrade GCS connector to 2.2.11.,FLINK-31631,13530275,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,cnauroth,cnauroth,cnauroth,27/Mar/23 17:23,11/Dec/23 17:45,04/Jun/24 20:41,31/Mar/23 06:32,1.17.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,FileSystems,,,,,,0,pull-request-available,,,,,"Upgrade the [GCS Connector|https://github.com/GoogleCloudDataproc/hadoop-connectors/tree/v2.2.11/gcs] bundled in the Flink distro from version 2.2.3 to 2.2.11. The new release contains multiple bug fixes and enhancements discussed in the [Release Notes|https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v2.2.11/gcs/CHANGES.md]. Notable changes include:
 * Improved socket timeout handling.
 * Trace logging capabilities.
 * Fix bug that prevented usage of GCS as a [Hadoop Credential Provider|https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html].
 * Dependency upgrades.
 * Support OAuth2 based client authentication.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33793,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 06:32:35 UTC 2023,,,,,,,,,,"0|z1gvw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 17:23;cnauroth;I have a patch in progress for this.;;;","31/Mar/23 06:32;martijnvisser;Fixed in master: e3f9df66e404d80faa9926f3fef43827a7d28683;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Support max-age for checkpoints during last-state upgrades,FLINK-31630,13530268,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,27/Mar/23 16:55,03/Apr/23 11:02,04/Jun/24 20:41,03/Apr/23 11:02,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"The kubernetes operator currently does not consider the age of the last completed checkpoint before using it for upgrades.

We should have a configurable max age beyond which the operator will use savepoint or wait for in-progress checkpoints to complete.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 11:02:24 UTC 2023,,,,,,,,,,"0|z1gvug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 11:02;gyfora;merged to main 4c944403f466e36c151c356c9f30d8dd6074fa30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Trying to access closed classloader when submit query to restSqlGateway via SqlClient,FLINK-31629,13530229,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,27/Mar/23 10:53,21/Apr/23 03:36,04/Jun/24 20:41,21/Apr/23 03:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,Table SQL / Client,,,,,0,,,,,,"When I attempted to resubmit the same SQL job(Using HiveCatalog) to SqlGateway through SqlClient, I encountered the error shown in the figure.

!screenshot-1.png|width=649,height=263!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31398,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/23 10:54;Weijie Guo;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13056835/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 21 03:36:26 UTC 2023,,,,,,,,,,"0|z1gvls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/23 12:15;jark;cc [~fsk119] [~luoyuxia];;;","08/Apr/23 15:01;Weijie Guo;By offline discuss with [~luoyuxia], this issue can be fixed by FLINK-31398. Let's track this issue on that ticket.

 ;;;","21/Apr/23 03:36;Weijie Guo;Fixed by FLINK-31398.;;;",,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in watermark processing,FLINK-31628,13530214,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,michael.helmling,michael.helmling,27/Mar/23 09:25,13/Apr/23 08:06,04/Jun/24 20:41,13/Apr/23 08:06,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Runtime / Task,,,,,,0,pull-request-available,,,,,"After upgrading a job from Flink 1.16.1 to 1.17.0, my task managers throw the following exception:

 

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: Index -2147483648 out of bounds for length 5
	at org.apache.flink.streaming.runtime.watermarkstatus.HeapPriorityQueue.removeInternal(HeapPriorityQueue.java:155)
	at org.apache.flink.streaming.runtime.watermarkstatus.HeapPriorityQueue.remove(HeapPriorityQueue.java:100)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve$InputChannelStatus.removeFrom(StatusWatermarkValve.java:300)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve$InputChannelStatus.access$200(StatusWatermarkValve.java:266)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.markWatermarkUnaligned(StatusWatermarkValve.java:222)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.inputWatermarkStatus(StatusWatermarkValve.java:140)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:153)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.base/java.lang.Thread.run(Unknown Source){code}
I never saw this before. The job has multiple Kafka inputs, but doesn't use watermark alignment.

 

 

Initially reported [on Slack|https://apache-flink.slack.com/archives/C03G7LJTS2G/p1679908171461309], where a relation to FLINK-28853 was suspected.",Kubernetes with Flink operator 1.4.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30544,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 08:06:30 UTC 2023,,,,,,,,,,"0|z1gvig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 09:34;martijnvisser;[~mxm] My initial thinking was that this might be caused by FLINK-28853, WDYT?;;;","07/Apr/23 04:27;wanglijie;This is caused by FLINK-30544, in which we introduced a heap to help find the minimum watermark. This error occurs when the channel status changes like this:

phase 1: channel becomes IDLE (receives WatermarkStatus.IDLE)
phase 2: channel becomes ACTIVE (receives WatermarkStatus.ACTIVE), but current watermark of the channel is less than the last output watermark (last watermark sent to downstream tasks)
phase 3: channel becomes IDLE again (receives WatermarkStatus.IDLE again)

In phase 1, we remove the channel from the heap because it is idle, should no longer participate in the calculation of the minimum watermark. In phase 2, although the channel becomes active, but its watermark is expired(less than the last output watermark), so we don't add it back to the heap. And then in phase 3, we try to remove the channel again, but the channel is not in the heap, which causes the above problem.;;;","07/Apr/23 04:30;wanglijie;We should check if the channel is in the heap before removing it(or let the heap does not throw exception when trying to remove an absent element). I'll prepare a fix soon.;;;","07/Apr/23 04:36;wanglijie;This problem may occur when the source configures {{table.exec.source.idle-timeout}} or uses the {{WatermarkStrategyWithIdleness}} strategy;;;","13/Apr/23 08:06;wanglijie;Fixed via 

master: a9bc791abdff5bd8e4cb3d67bb1417b2f02c990a

release-1.17: 508de2f41fcbd2a4d64a5e8dd291926e273b4f50;;;",,,,,,,,,,,,,,,,,,,,,,
docker-build.sh build fails on Linux machines,FLINK-31627,13530212,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,27/Mar/23 09:16,29/Mar/23 14:33,04/Jun/24 20:41,29/Mar/23 14:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Project Website,,,,,,0,pull-request-available,,,,,"Building the Flink website on Linux fails due to how Docker is used as a Daemon running under root in Linux (see [this blog|https://jtreminio.com/blog/running-docker-containers-as-current-host-user/#ok-so-what-actually-works] for more details).

Building the website will fail when copying the artifacts because they are owned by the root user. 

{code}
./docker-build.sh build                                                                                     
latest: Pulling from jakejarvis/hugo-extended                                                                                                
Digest: sha256:7d7eb41d7949b5ed338c27926098b84e152e7e1d8ad8f1955c29b383a2336548                                                              
Status: Image is up to date for jakejarvis/hugo-extended:latest                                                                              
docker.io/jakejarvis/hugo-extended:latest                                                                                                    
Start building sites …                                                                                                                       
hugo v0.111.3-5d4eb5154e1fed125ca8e9b5a0315c4180dab192+extended linux/amd64 BuildDate=2023-03-12T11:40:50Z VendorInfo=docker                 
Error: Error building site: open /src/target/news/2014/08/26/release-0.6.html: permission denied                                             
Total in 153 ms                                                                                                                              
mv: cannot move 'docs/target/2014' to 'content/2014': Permission denied                                                                      
mv: cannot move 'docs/target/2015' to 'content/2015': Permission denied                                                                      
mv: cannot move 'docs/target/2016' to 'content/2016': Permission denied                                                                      
mv: cannot move 'docs/target/2017' to 'content/2017': Permission denied                                                                      
mv: cannot move 'docs/target/2018' to 'content/2018': Permission denied                                                                      
mv: cannot move 'docs/target/2019' to 'content/2019': Permission denied                                                                      
mv: cannot move 'docs/target/2020' to 'content/2020': Permission denied                                                                      
mv: cannot move 'docs/target/2021' to 'content/2021': Permission denied                                                                      
mv: cannot move 'docs/target/2022' to 'content/2022': Permission denied                                                                                                                                                                                                                   
mv: cannot move 'docs/target/2023' to 'content/2023': Permission denied                                                                      
mv: cannot move 'docs/target/categories' to 'content/categories': Permission denied
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22922,,,,,,,,,,,,,,FLINK-31081,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 29 14:33:24 UTC 2023,,,,,,,,,,"0|z1gvi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 14:33;mapohl;asf-site: a935b1201c1197ab05406e0db5a5206cdba0608d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
HsSubpartitionFileReaderImpl should recycle skipped read buffers.,FLINK-31626,13530203,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,27/Mar/23 08:45,31/Mar/23 09:42,04/Jun/24 20:41,31/Mar/23 09:42,1.17.1,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,"In FLINK-30189, {{HsSubpartitionFileReaderImpl}} will skip the buffer has been consumed from memory to avoid double-consumption. But these buffers were not returned to the {{BatchShuffleReadBufferPool}}, resulting in read buffer leaks. In addition, all loaded buffers should also be recycled after data view released.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 09:42:16 UTC 2023,,,,,,,,,,"0|z1gvg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 09:42;Weijie Guo;master(1.18) via 59462197ba725b9fc0118ec54ec9f1325b8a874a.
release-1.17 via 0f59c8f7b161e96f026529f542c00b1094107371.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Memory and computation inefficiency in KBinsDiscretizer,FLINK-31625,13530202,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hongfanxo,hongfanxo,27/Mar/23 08:42,28/Mar/23 03:39,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,,0,,,,,,"In KBinsDiscretizer, the main computation `findBinEdgesWithXXXStrategy` is accomplished in a single subtask.

While data sampling can reduce the number of samples, the memory and computation costs may still be high for large input vectors, potentially leading to inefficient computation and OOM errors. As such, further algorithm optimization is necessary to improve memory and computation efficiency.

A potential solution is to implement parallel computation, distributing the data evenly among all workers, either by samples or by features.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-27 08:42:58.0,,,,,,,,,,"0|z1gvfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Connectors/Jdbc] Update flink to 1.17.0,FLINK-31624,13530198,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,eskabetxe,eskabetxe,27/Mar/23 08:23,04/Apr/23 17:51,04/Jun/24 20:41,04/Apr/23 17:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,Bump Flink version to 1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31699,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-27 08:23:53.0,,,,,,,,,,"0|z1gvew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix DataStreamUtils#sample with approximate uniform sampling,FLINK-31623,13530191,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hongfanxo,hongfanxo,27/Mar/23 07:25,03/Apr/23 07:16,04/Jun/24 20:41,03/Apr/23 07:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,,,"Current implementation employs two-level sampling method.

However, when data instances are imbalanced distributed among partitions (subtasks), the probabilities of instances to be sampled are different in different partitions (subtasks), i.e., not a uniform sampling.

 

In addition, one side-effect of current implementation is: one subtask has a memory footprint of `2 * numSamples * sizeof(element)`, which could cause unexpected OOM in some situations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 07:16:56 UTC 2023,,,,,,,,,,"0|z1gvdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 07:16;zhangzp;Resolved on master via fe338b194b73fd51218f4d842fa7b0065fb76c56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_APPEND supported in SQL & Table API,FLINK-31622,13530181,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jackylau,jackylau,27/Mar/23 05:51,27/Mar/23 11:00,04/Jun/24 20:41,27/Mar/23 09:50,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"array_append(array, element) - Append the element at the end of the array.

Syntax:
array_append(array, element)

Arguments:
array: An ARRAY to be handled.

Returns:

Append the element at the end of the array.

This function does not return null when the elements are null. It appends null at the end of the array. But returns null if the array is null.
{code:sql}
> SELECT array_append(array(3, 2, 1), 1);
 3 {code}
See also
spark not in docs https://spark.apache.org/docs/latest/api/sql/index.html#array but in code. [https://github.com/apache/spark/blob/c55c7ea6fc92c3733543d5f3d99eb00921cbe564/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L5059]

snowflake [https://docs.snowflake.com/en/sql-reference/functions/array_append]

postgresql [https://www.postgresql.org/docs/12/functions-array.html#ARRAY-FUNCTIONS-TABLE]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 27 11:00:50 UTC 2023,,,,,,,,,,"0|z1gvb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 09:47;Sergey Nuyanzin;this is a duplicate of https://issues.apache.org/jira/browse/FLINK-27891;;;","27/Mar/23 11:00;jackylau;[~Sergey Nuyanzin] ok. thanks for your tip;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_REVERSE supported in SQL & Table API,FLINK-31621,13530178,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jackylau,jackylau,jackylau,27/Mar/23 05:24,31/Mar/23 11:11,04/Jun/24 20:41,31/Mar/23 11:11,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"array_reverse(array) - Returns an array in reverse order.

Syntax:
array_reverse(array)

Arguments:
array: An ARRAY to be handled.

Returns:

Returns an array in reverse order.

Returns null if the argument is null
{code:sql}
> SELECT array_reverse(array(1, 2, 2, NULL));
 NULL, 2, 2, 1{code}
See also
bigquery [https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#array_reverse]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 11:11:37 UTC 2023,,,,,,,,,,"0|z1gvag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 09:34;jackylau;hi [~Sergey Nuyanzin] , could you please help review it?;;;","31/Mar/23 11:11;fsk119;Merged into master: 122429f99b9b75737cd218a56a37a7ced750582f;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ReducingUpsertWriter does not flush the wrapped writer,FLINK-31620,13530148,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Gerrrr,Gerrrr,Gerrrr,26/Mar/23 22:33,12/Apr/23 17:11,04/Jun/24 20:41,12/Apr/23 17:05,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,,kafka-3.0.0,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,"According to {{SinkWriter#flush}} [javadoc|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/sink2/SinkWriter.java#L43-L47], the writer must flush its records to guarantee AT_LEAST_ONCE.

{{upsert-kafka}}'s {{ReducingUpsertWriter}} inserts buffered records into the wrapped writer, but does not flush it:
1. SinkWriter#flush implementation - https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java#L88-L91.
2. The actual flush code - https://github.com/apache/flink/blob/f3c653ed2e4264315ed83a5b4b2494a7dcc41474/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java#L143-L150.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 17:05:47 UTC 2023,,,,,,,,,,"0|z1gv3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 17:05;tzulitai;Merged to flink-connector-kafka via 15b51f17a1daa2edb4cd17b4100eb407d83c8eb2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Stateful Functions to Flink 1.16.1,FLINK-31619,13530144,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,galenwarren,nowke,nowke,26/Mar/23 20:04,10/Sep/23 18:55,04/Jun/24 20:41,07/Sep/23 07:46,,,,,,,,,,,,,,,,,,,,,statefun-3.3.0,,,,,,,,,Stateful Functions,,,,,,0,pull-request-available,,,,,Upgrade Statefun to use Flink 1.16.1. Could you assign this to me?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Sep 10 18:55:50 UTC 2023,,,,,,,,,,"0|z1gv2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/23 17:31;galenwarren;Created pull request: [[FLINK-31619] Upgrade Stateful Functions to Flink 1.16.1 by galenwarren · Pull Request #331 · apache/flink-statefun (github.com)|https://github.com/apache/flink-statefun/pull/331];;;","07/Sep/23 07:46;martijnvisser;Fixed in apache/flink-statefun:master b06c0a23a5a622d48efc8395699b2e4502bd92be;;;","10/Sep/23 08:34;fransking;Will there be a 3.3.0 build published to maven?  ;;;","10/Sep/23 18:55;martijnvisser;If the vote on the release gets accepted, yes;;;",,,,,,,,,,,,,,,,,,,,,,,
Broken links in docs for Pulsar connector ,FLINK-31618,13530121,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,taoran,taoran,26/Mar/23 12:26,28/Mar/23 07:05,04/Jun/24 20:41,28/Mar/23 07:05,pulsar-4.0.0,,,,,,,,,,,,,,,,,,,,pulsar-4.0.0,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,"[https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/pulsar/]

 

some compress types has error 404 links.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 28 07:05:02 UTC 2023,,,,,,,,,,"0|z1guxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 06:46;Yanfei Lei;I'd like to open a pr to fix, could you please assign it to me?;;;","27/Mar/23 07:32;tanyuxin;[~Yanfei Lei] Sorry. I apologize for not noticing the comments before submitting my PR.;;;","28/Mar/23 07:05;Weijie Guo;master(4.0) via 33526f5e36a2d27eb7a80a1aeed130fa2914cc8d.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Upgrade the SnakeYaml Maven dependency,FLINK-31617,13530118,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,highfei2011@126.com,highfei2011@126.com,26/Mar/23 12:20,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.1,1.18.0,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Deployment / Scripts,,,,,,0,pull-request-available,,,,,"Fixing the container vulnerability by updating maven dependency.Here is [cve|https://github.com/advisories/GHSA-mjmj-j48q-9wg2].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-26 12:20:21.0,,,,,,,,,,"0|z1gux4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add user docs for canary resources,FLINK-31616,13530099,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gyfora,gyfora,26/Mar/23 09:47,24/May/23 17:47,04/Jun/24 20:41,24/May/23 17:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation,Kubernetes Operator,,,,,0,,,,,,We should add some docs on how to use the recently introduced canary resources.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-26 09:47:19.0,,,,,,,,,,"0|z1gusw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix some parts forgot to translate in ""Table API"" page of ""Table API & SQL"" ",FLINK-31615,13530094,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ruanhang1993,ruanhang1993,ruanhang1993,26/Mar/23 06:08,20/Dec/23 07:41,04/Jun/24 20:41,20/Dec/23 07:41,1.18.0,,,,,,,,,,,,,,,,,,,,1.19.0,,,,,,,,,Documentation,,,,,,0,auto-deprioritized-minor,chinese-translation,,,,"The query_state_warning in ""Table API"" page of ""Table API & SQL""  is still in English. And some comments in codes are in English.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 20 07:41:06 UTC 2023,,,,,,,,,,"0|z1gurs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/23 08:13;Wencong Liu;Hello [~ruanhang1993] , I'd like to take this ticket.;;;","28/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Oct/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","15/Dec/23 09:55;ruanhang1993;[~Wencong Liu] Sorry for the late reply. 

I have raised a PR to fix this part. Maybe you could help to review it(https://github.com/apache/flink/pull/22272/files). Thanks.;;;","20/Dec/23 07:41;leonard;Fixed in master(1.19): 7ded6089a5fc827b9d09ef490c8d1e4965a0c4bf;;;",,,,,,,,,,,,,,,,,,,,,,
Upgrade autoscaling example to 1.17.0 image,FLINK-31614,13530081,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xxxinli1,gyfora,gyfora,25/Mar/23 19:29,29/Mar/23 14:32,04/Jun/24 20:41,29/Mar/23 14:32,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,starter,,,,"The autoscaling example yaml uses 1.17 snapshot images, we should upgrade to the released one and update any related documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 29 14:32:32 UTC 2023,,,,,,,,,,"0|z1guow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 13:50;xxxinli1;May I try this one as a starter? [flink-kubernetes-operator/Dockerfile at main · apache/flink-kubernetes-operator (github.com)|https://github.com/apache/flink-kubernetes-operator/blob/main/examples/autoscaling/Dockerfile];;;","27/Mar/23 15:44;gyfora;sure [~xxxinli1] ;;;","29/Mar/23 14:32;gyfora;merged to main 9690d89caf317b909340bd3e4dd57275651a8e63;;;",,,,,,,,,,,,,,,,,,,,,,,,
Some default operator config values are overwritten by values.yaml,FLINK-31613,13530064,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,mateczagany,mateczagany,25/Mar/23 13:30,31/Aug/23 06:19,04/Jun/24 20:41,31/Aug/23 06:19,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"It's a bit confusing that in the documentation it's stated that 'kubernetes.operator.reconcile.interval' is 1 min by default and 'kubernetes.operator.observer.progress-check.interval' is 10 sec when they are being overwritten to 15 sec and 5 sec respectively in the default values.yaml.

 

A possible solution might be to change the default values to 15 and 5 sec in the configuration values and remove/comment them in values.yaml, however this will introduce a change in configuration for users that have set a custom 'defaultConfiguration.flink-conf.yaml' value.

 

Please let me know what you think and if the solution sounds good feel free to assign me this ticket and I'll create a PR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Mar 25 21:18:47 UTC 2023,,,,,,,,,,"0|z1gul4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/23 21:18;gyfora;I agree that this could be confusing. The rationale behind this is to improve the local testing setup when using minikube and avoid longer waiting periods.

I don't think we should change the config default value as that may cause a bad surprise for users. Still at the same time I feel that the current values.yaml gives a good out of the box behaviour for local testing. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException when using GCS path as HA directory,FLINK-31612,13530050,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,mohit.aggarwal,mohit.aggarwal,25/Mar/23 08:13,28/Mar/23 14:32,04/Jun/24 20:41,28/Mar/23 14:32,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,FileSystems,,,,,,0,pull-request-available,,,,,"Hi,

When I am trying to run Flink job in HA mode with GCS path as a HA directory (eg: [gs://flame-poc/ha]) or while starting a job from checkpoints in GCS I am getting following exception:
{code:java}
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not found
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2712) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.<init>(Groups.java:107) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.<init>(Groups.java:102) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:338) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575) ~[?:?]
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getUgiUserName(GoogleHadoopFileSystemBase.java:1226) ~[?:?]
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:858) ~[?:?]
	at org.apache.flink.fs.gs.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[?:?] {code}
{*}Observations{*}:

While using File system as a HA path and GCS as checkpointing directory the job is able to write checkpoints to GCS checkpoint path. 

After debugging what I found was all the *org.apache.hadoop* paths are shaded to {*}org.apache.flink.fs.shaded.hadoop3.org{*}{*}.apache.hadoop{*}. Ideally the code should look for  {*}org.apache.flink.fs.shaded.hadoop3.org{*}{*}.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback{*} instead of  *org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.*
I think it is not getting shaded over here due to reflection being used here:
[https://github.com/apache/hadoop/blob/branch-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java#L108]

As a workaround I rebuilt *flink-gs-fs-hadoop* plugin removing this relocation and it worked for me.
{code:java}
<relocation>
<pattern>org.apache.hadoop</pattern>
<shadedPattern>org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop</shadedPattern>
</relocation> {code}","Flink Kuberenetes operator: 1.4

Flink version: 1.17

GKE Kubernetes cluster.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 28 09:14:24 UTC 2023,,,,,,,,,,"0|z1gui0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 01:30;czchen;The following information are from slack:
 * From Ta-Chun in slack, this is possible caused by https://github.com/apache/flink/pull/21318.
 * From Jessy, this issue alsp happens in Azure blob storage and ADLS.;;;","27/Mar/23 08:18;martijnvisser;I'll check with [~chesnay] on this;;;","27/Mar/23 08:25;chesnay;The problem is that the azure/gs filesystems still relocated hadoop which isn't required anymore.;;;","27/Mar/23 11:12;martijnvisser;Could those who are running into this problem validate that this is resolved when using https://github.com/apache/flink/pull/22279 ?;;;","28/Mar/23 01:05;czchen;We have confirmed this PR can fix GCS backend problem.;;;","28/Mar/23 09:14;martijnvisser;Fixed in:

master: 3924ba85e62f04a0a181007d573cb84e3c09a636
release-1.17: 52a2b98bb5af842633df0c051b5da95d437a6b2f;;;",,,,,,,,,,,,,,,,,,,,,
Add delayed restart to failed jobs,FLINK-31611,13530019,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,morhidi,morhidi,24/Mar/23 18:13,31/Aug/23 15:45,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"Operator is able to restart failed jobs already using:
{{kubernetes.operator.job.restart.failed: true}}

It's beneficial however to keep a failed job around for a while for inspection:
{{kubernetes.operator.job.restart.failed.delay: 5m}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-24 18:13:31.0,,,,,,,,,,"0|z1gub4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactoring of LocalBufferPool,FLINK-31610,13529980,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,akalash,akalash,24/Mar/23 13:14,23/Apr/23 10:32,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,,0,,,,,,"FLINK-31293 bug highlighted the issue with the internal mutual consistency of different fields in LocalBufferPool. ex.:
-  `numberOfRequestedOverdraftMemorySegments`
-  `numberOfRequestedMemorySegments`
-  `availableMemorySegment`
-  `currentPoolSize`

Most of the problem was fixed already(I hope) but it is a good idea to reorganize the code in such a way that all invariants between all fields inside will be clearly determined and difficult to break.

As one example I can propose getting rid of numberOfRequestedOverdraftMemorySegments and using existing numberOfRequestedMemorySegments instead. That means:

- the pool will be available when `!availableMemorySegments.isEmpty() && unavailableSubpartitionsCount == 0`
- we don't request a new `ordinary` buffer when `numberOfRequestedMemorySegments >=  currentPoolSize` but we request the overdraft buffer instead
- `setNumBuffers` should work automatically without any changes

I think we can come up with a couple of such improvements to simplify the code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31764,,,FLINK-31763,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 23 10:32:19 UTC 2023,,,,,,,,,,"0|z1gu2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 13:18;akalash;[~Weijie Guo], [~fanrui], What do you think about the idea in the description? Do you have any other ideas for simplification the code?

CC [~pnowojski];;;","24/Mar/23 14:43;Weijie Guo;Thanks [~akalash] for driving this! 

From my point of view, this is really a very meaningful work. {{BufferPool}} is a very low-level and basic component in Flink, but it is now so complicated that we can hardly be confident that there are no bugs in it. To be honest, issues like {{FLINK-31293}} and {{FLINK-29298}} are very difficult to troubleshoot, in part because our current implementation is too complex and there are no strong guarantees at the code level for some assumptions.

Back to this ticket, the simplifications in the description are in the right direction. The existence of {{numberOfRequestedOverdraftMemorySegments}} field makes our consistency maintenance very fragile. But there seems to be a small problem with removing it: Consider such a scenario, the {{CurrentPoolSize = 5}}, {{numOfRequestedMemorySegments = 7}}, {{maxOverdraftBuffersPerGate = 2}}. If {{numberOfRequestedOverdraftMemorySegments = 0}}, then 2 buffers can be requested now. If the counter for requested overdraft-buffers is removed, is it still allowed to request buffers in this case, and if so, how many buffers can be requested at most?

But it is possible that this is the culprit of the current difficulty in maintaining consistency. IIUC, the solution you describe actually changes the definition of overdraft buffer from static to dynamic. If we consider the part exceeding {{CurrentPoolSize}} as overdraft, then things will be much simpler. If this part of overdraft buffers exceeds the upper limit(i.e. {{maxOverdraftBuffersPerGate}}), then no more buffers can be requested. However, there may be other problems in doing so, at least we have broken the previous behavior to some extent. I need to think in more detail about the correctness of this solution.

Any other comments are welcome. I'd like to also cc [~kevin.cyj] to see if he can give more input.
;;;","28/Mar/23 14:19;akalash;??Consider such a scenario, the CurrentPoolSize = 5, numOfRequestedMemorySegments = 7, maxOverdraftBuffersPerGate = 2. If numberOfRequestedOverdraftMemorySegments = 0, then 2 buffers can be requested now. If the counter for requested overdraft-buffers is removed, is it still allowed to request buffers in this case, and if so, how many buffers can be requested at most???

Do I understand correctly that this scenario is possible only when it had been requested 7 buffers and after that CurrentPoolSize was decreased(vis setNumBuffers) to 5? We need to think about how important/relevant this scenario is;;;","28/Mar/23 14:45;Weijie Guo;Yes, It only happens when poolSize changes.;;;","30/Mar/23 14:01;pnowojski;{{LocalBufferPool}} size can change everytime a new subtask is starting/finishing on a TaskManager, so it's pretty common. Especially for session clusters, but not only. ;;;","06/Apr/23 03:15;fanrui;Thanks [~akalash]  driving this JIRA, and thanks [~Weijie Guo] [~pnowojski]  's discussion. Sorry for the late response.

 
{quote}Consider such a scenario, the {{{}CurrentPoolSize = 5{}}}, {{{}numOfRequestedMemorySegments = 7{}}}, {{{}maxOverdraftBuffersPerGate = 2{}}}. If {{{}numberOfRequestedOverdraftMemorySegments = 0{}}}, then 2 buffers can be requested now. 

It only happens when poolSize changes.
{quote}
From this case and the definition of the overdraft buffer, it shouldn't be able to request now. General principle: new buffers can be requested when ""{_}numOfRequestedMemorySegments + numberOfRequestedOverdraftMemorySegments < poolSize + maxOverdraftBuffersPerGate""{_}.

For the code design, [~Weijie Guo]  and me discussed it in this PR[1] before, I didn't find this case at that time. If the case can happen, I think we should convert _{{numberOfRequestedMemorySegments}}_ to {{_numberOfRequestedOverdraftMemorySegments_ when poolSize is decreased.}}

 
{quote}I can propose getting rid of numberOfRequestedOverdraftMemorySegments and using existing numberOfRequestedMemorySegments instead.
{quote}
Overall, it think it is feasible. The new buffers can be requested when {_}""numberOfRequestedMemorySegments < poolSize + maxOverdraftBuffersPerGate""{_}:
 * When _numberOfRequestedMemorySegments <= poolSize,_ all buffers are ordinary buffer 
 * When _numberOfRequestedMemorySegments > poolSize,_ the `{_}ordinary buffer size = poolSize`{_}, and `{_}the overdraft buffer size = numberOfRequestedMemorySegments - poolSize`{_}

 

Please correct me if I'm wrong.

 

[1] https://github.com/apache/flink/pull/22084/files#r1128926904;;;","06/Apr/23 09:50;Weijie Guo;Thanks [~fanrui] for the clarification.

{quote}
General principle: new buffers can be requested when ""numOfRequestedMemorySegments + numberOfRequestedOverdraftMemorySegments < poolSize + maxOverdraftBuffersPerGate"".{quote}

Totally agreed with this principle. If the previous behavior was not what it was designed for, I will create a ticket to do this(i.e. convert {{numberOfRequestedMemorySegments}} to {{numberOfRequestedOverdraftMemorySegments}} when poolSize is decreased).

{quote}
When numberOfRequestedMemorySegments <= poolSize, all buffers are ordinary buffer.
When numberOfRequestedMemorySegments > poolSize, the `ordinary buffer size = poolSize`, and `the overdraft buffer size = numberOfRequestedMemorySegments - poolSize`
{quote}

UUIC, That's exactly what I said in my first comment: {{{}changes the definition of overdraft buffer from static to dynamic(i.e. Whether it is an overdraft buffer depends on the poolSize at the current moment.){}}}., This approach avoid a lot of complexity, I'm +1 for this.;;;","13/Apr/23 10:41;kevin.cyj;Sorry for the delayed reply.

Totally agree that we should simply the LocalBufferPool implementation. IMO, the most complicated part of the buffer pool comes from the mechanism of interaction between global pool and local pool.

One idea I can come up with is to make requesting buffer from global pool and registering of the global pool availability callback atomic, that is, we has such a method in the global NetworkBufferPool, like tryRequestMemorySegment(AvailableCallback callback), if we can request a buffer, we do not register the callback, otherwise, we register the callback.

In the current implementation, there are too many branches, for example, if we can not request a buffer from global pool, we register a callback, that callback can be called immediately if the global pool availability is already completed and the local pool will try to request buffer from global pool again but the request can fail, if fail, it needs to register the callback again.

What do you think? ;;;","23/Apr/23 10:32;Weijie Guo;>> One idea I can come up with is to make requesting buffer from global pool and registering of the global pool availability callback atomic.

Thanks [~kevin.cyj] for bringing this idea. I agree with you that the mechanism of interaction between {{LocalBufferPool}} and {{NetworkBufferPool}} is too complicated.

Since the callback can be triggered by multiple threads (the current thread or threads of other tasks), it leads to code complexity and bugs (such as FLINK-29298). If we turn requesting buffers from the global pool and registering callbacks into atomic operations, then {{AvailabilityStatus}} class and related logic can be removed. The only possible concern is that after doing this, the current thread will no longer be allowed to trigger the callback immediately. I don't see the side effects of doing this at the moment, but we do need to be careful to double confirm. 

[~akalash] and [~fanrui], I'd like to hear your thoughts on this proposal.;;;",,,,,,,,,,,,,,,,,,
Fatal error in ResourceManager caused YARNSessionFIFOSecuredITCase.testDetachedMode to fail,FLINK-31609,13529972,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ferenc-csaky,mapohl,mapohl,24/Mar/23 12:33,15/May/23 15:09,04/Jun/24 20:41,15/May/23 15:09,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Deployment / YARN,,,,,,0,pull-request-available,test-stability,,,,"This looks like FLINK-30908. I created a follow-up ticket because we reached a new minor version.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47547&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461

{code}
Mar 24 09:32:29 2023-03-24 09:31:50,001 ERROR org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Exception on heartbeat
Mar 24 09:32:29 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Mar 24 09:32:29 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1461) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1403) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at com.sun.proxy.$Proxy33.allocate(Unknown Source) ~[?:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-2.10.2.jar:?]
Mar 24 09:32:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
Mar 24 09:32:29 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
Mar 24 09:32:29 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
Mar 24 09:32:29 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at com.sun.proxy.$Proxy34.allocate(Unknown Source) ~[?:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:297) ~[hadoop-yarn-client-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:274) [hadoop-yarn-client-2.10.2.jar:?]
Mar 24 09:32:29 Caused by: java.lang.InterruptedException
Mar 24 09:32:29 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
Mar 24 09:32:29 	at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1177) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1456) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	... 17 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30908,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 15 15:09:49 UTC 2023,,,,,,,,,,"0|z1gu0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 08:24;knaufk;[~ferenc-csaky] Hi Ferenc, do you or someone in your team have time to look into this?;;;","18/Apr/23 12:06;ferenc-csaky;Yeah, we can give it a check in the next 1-2 weeks, feel free to assign it to me.;;;","08/May/23 12:06;ferenc-csaky;According to the logs, the fatal exception in the {{ResourceManager}} does not happen anymore (cause of the changes made in FLINK-30908). According to [~xtsong]'s analysis on FLINK-30908, the AMRM heartbeat interruption can happen anyways, so the ex about the interrupt is written to the logs, hence the failure, because the string
{code}
java.io.InterruptedIOException: Interrupted waiting to send RPC request to server {code}
is not whitelisted. If we are okay whitelisting that exactly, it should fix the tests.;;;","15/May/23 15:09;gaborgsomogyi;4415c5150eda071b219db5532c359ca29730a378 on master;;;",,,,,,,,,,,,,,,,,,,,,,,
Re-evaluate 'min-parallelism-increase' option,FLINK-31608,13529944,13527017,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,24/Mar/23 09:47,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Runtime / Configuration,Runtime / Coordination,,,,,0,,,,,,"This option was meant to prevent scale up operations where the benefit doesn't outweigh the cost, like scaling up to increase a single vertices parallelism by 1. Meanwhile, scale-down operations were always immediately executed, because they were always the result of a stopped TaskManager, causing the job to restart anyway.

Now that users can change the requirements at will this has changed, and the expected behavior is overall undefined.

We need to answer:
 * should there be a dedicated option for limiting scale-down operations if the requirements were changed?
 * should the min-parallelism-{*}increase{*} option be generalized to a min-parallelism-{*}change{*} option?
 * How shall operations be handled that scale different vertices up or down at the same? So far the decision was made on the cumulative parallelism change, but in this case the parallelism distribution can change significantly while the cumulative change is 0.
 * If a rescale operation was not applied due to these limits, should they be _eventually_ applied anyway (e.g., after a timeout)?

 

See [https://github.com/apache/flink/pull/22169#discussion_r1147447567] for the background.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-24 09:47:01.0,,,,,,,,,,"0|z1gtug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor the logic for HiveExecutableOperation,FLINK-31607,13529936,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,24/Mar/23 09:05,23/Jul/23 02:14,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,,0,,,,,,"In FLINK-31409, we will warp the extra operation customized for Hive with {{HiveExecutableOperation.}}

We should refactor it to make each of the extra operation customized for Hive execute it's own logic in it's own inner `execute`  method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-24 09:05:50.0,,,,,,,,,,"0|z1gtso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""sqlClient.md"" page of ""table"" into Chinese",FLINK-31606,13529931,13285584,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zjureel,zjureel,zjureel,24/Mar/23 08:26,27/May/24 13:07,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 22:35:10 UTC 2023,,,,,,,,,,"0|z1gtrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Table#to_pandas check table boundedness,FLINK-31605,13529929,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,24/Mar/23 08:21,24/Mar/23 08:21,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"When `Table#to_pandas` is invoked on a table with an unbounded source, this method would not throw exceptions. Instead, it would be infinitely executing without exiting. It would be better to check the boundedness of a table before execution and throw exceptions on unbounded cases.

It might be possible to reference the code in `StreamGraphGenerator#existsUnboundedSource` to judge whether a Table is bounded or not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-24 08:21:26.0,,,,,,,,,,"0|z1gtr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce usage of CatalogTableImpl in planner,FLINK-31604,13529925,13359276,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,24/Mar/23 07:33,30/Mar/23 21:41,04/Jun/24 20:41,30/Mar/23 21:41,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,The task is similar to https://issues.apache.org/jira/browse/FLINK-30896 however about CatalogTableImpl which is deprecated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 30 21:41:20 UTC 2023,,,,,,,,,,"0|z1gtq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 21:41;Sergey Nuyanzin;Merged to master at [cfb213040a08021da3a954a1c0e7f94f22a80f1e|https://github.com/apache/flink/commit/cfb213040a08021da3a954a1c0e7f94f22a80f1e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Line break should be removed in create table with-clauses, load module with-clauses and table hints for both keys and values",FLINK-31603,13529913,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,paul8263,paul8263,24/Mar/23 06:25,08/Apr/23 12:10,04/Jun/24 20:41,08/Apr/23 12:09,1.16.0,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Given a SQL like this:
{code:sql}
CREATE TABLE MyTable (
  `user_id` BIGINT,
  `name` STRING,
  `timestamp` TIMESTAMP_LTZ(3) METADATA
) WITH (
  'connector' = 'kaf
ka'
  ...
);
{code}
After parsing the SQL, the option value 'connector' is 'kaf\nka', which will lead to problems.

The line break inside keys/values in with-clauses and table hints should be removed when parsing SQLs.

If this is the issue that needs to fix, I would like to do it, as I am currently working on it.",Flink 1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 08 12:09:26 UTC 2023,,,,,,,,,,"0|z1gtnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 07:23;paul8263;Hi community,

For some of the properties like 'line.delim' in Hive, users might set the value '\n' explicitly. So it might not be a good practice to replace all line breaks in with clauses.

We may consider to narrow it down to replacing the line break in Flink Table Hints only, which needs some further discussion.;;;","30/Mar/23 09:11;martijnvisser;I would like to understand what the SQL standard says with regards to line breaks in a statement. I'm concerned that replacing all line breaks will actually introduce a lot of complexity with certain exceptions; why not say that a line break is considered a line break and throw a meaningful error that the SQL statement is incorrect? I don't think this is a bug; this is a user error, not a Flink bug;;;","07/Apr/23 11:35;jark;Hi [~paul8263], this is an expected behavior in SQL, because there is indeed a line break in the string literal. MySQL and other databases also have the same behavior. 

{code}
mysql> select 'abc
edf' as str;
+---------+
| str     |
+---------+
| abc
edf |
+---------+
1 row in set (0.01 sec)
{code};;;","08/Apr/23 12:09;jark;I will close this issue first, feel free to continue the discussion in the issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_POSITION supported in SQL & Table API,FLINK-31602,13529908,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,24/Mar/23 05:37,03/Apr/23 04:06,04/Jun/24 20:41,03/Apr/23 04:06,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"array_position(array, element) - Returns the (1-based) index of the first element of the array as long.

Syntax:
array_position(array, element)

Arguments:
array: An ARRAY to be handled.

Returns:

Returns the position of the first occurrence of element in the given array as long.

Returns 0 if the given value could not be found in the array.

Returns null if either of the arguments are null
{code:sql}
> SELECT array_position(array(3, 2, 1), 1);
 3 {code}
See also
spark https://spark.apache.org/docs/latest/api/sql/index.html#array_position

postgresql [https://www.postgresql.org/docs/12/functions-array.html#ARRAY-FUNCTIONS-TABLE]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 04:06:06 UTC 2023,,,,,,,,,,"0|z1gtmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 04:06;jackylau;Merged to master as [a1e4ba2a0ac39a667b9c3169f254253bd98330b8|https://github.com/apache/flink/commit/a1e4ba2a0ac39a667b9c3169f254253bd98330b8];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"While waiting for resources, resources check might be scheduled unlimited number of times (Adaptive Scheduler)",FLINK-31601,13529869,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,roman,roman,23/Mar/23 21:57,24/Oct/23 15:45,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,"See [https://github.com/apache/flink/pull/22169#discussion_r1136395017]
{quote}when {{resourceStabilizationDeadline}} is not null, should we skip scheduling {{checkDesiredOrSufficientResourcesAvailable}} (on [line 166|https://github.com/apache/flink/blob/a64781b1ef8f129021bdcddd3b07548e6caa4a72/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/WaitingForResources.java#L166])?
Otherwise, we schedule as many checks as there are changes in resources.
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-23 21:57:34.0,,,,,,,,,,"0|z1gtds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix producer leaks in flink-connector-kafka tests,FLINK-31600,13529868,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,Gerrrr,Gerrrr,Gerrrr,23/Mar/23 21:37,11/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,stale-assigned,,,,"Local runs of flink-connector-kafka tests fail with 

 
{noformat}
 java.lang.AssertionError: Detected producer leak. Thread name: kafka-producer-network-thread | producer-transactionalId    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.checkProducerLeak(FlinkKafkaProducerITCase.java:829)
    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.testRestoreToCheckpointAfterExceedingProducersPool(FlinkKafkaProducerITCase.java:169)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.apache.flink.testutils.junit.RetryRule$RetryOnFailureStatement.evaluate(RetryRule.java:135)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
    at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
    at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
    at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
    at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54){noformat}
 

This happens because some test suites running prior to 
FlinkKafkaProducerITCase do not properly close Kafka producers. To be clear, this issue is only about producer leaks in *tests.*
The goal of this issue is to fix known producer leaks in tests until FlinkKafkaProducerITCase passes locally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 11 22:35:11 UTC 2023,,,,,,,,,,"0|z1gtdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 22:30;Gerrrr;Patches:
 * [https://github.com/apache/flink-connector-kafka/pull/12]
 * [https://github.com/apache/flink-connector-kafka/pull/13]
 * [https://github.com/apache/flink-connector-kafka/pull/14];;;","24/Mar/23 05:00;tzulitai;List of commits merged to \{{apache/flink-connector-kafka:main}}
 * 1e19e4f1f791ed6d257a8aa3b7e10879ccc3a310
 * e2e50b4f6f03e9c2f623a3b6d2748cfa3a72e1a4
 * f36ba96010b94c43e2a169fb49f5a3a7d422719f

(will update as we have more leak fixes in tests);;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
Update Kafka dependency in flink-connector-kafka to 3.4.0,FLINK-31599,13529849,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Gerrrr,Gerrrr,Gerrrr,23/Mar/23 18:12,08/Nov/23 07:17,04/Jun/24 20:41,24/Mar/23 20:33,,,,,,,,,,,,,,,,,,,,,kafka-3.1.0,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,"There is a number of reasons to upgrade to the latest version.
 
First, the Kafka connector uses reflection, so internal changes in Kafka clients' implementation might require changes in the connector. With more frequent upgrades, the amount of work per upgrade is smaller.
 
Second, there were a number of relevant bug fixes since 3.2.3:
* [KAFKA-14303] - Producer.send without record key and batch.size=0 goes into infinite loop
* [KAFKA-14379] - consumer should refresh preferred read replica on update metadata
* [KAFKA-14422] - Consumer rebalance stuck after new static member joins a group with members not supporting static members
* [KAFKA-14417] - Producer doesn't handle REQUEST_TIMED_OUT for InitProducerIdRequest, treats as fatal error
* [KAFKA-14532] - Correctly handle failed fetch when partitions unassigned
* ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29977,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 24 20:33:02 UTC 2023,,,,,,,,,,"0|z1gt9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 22:29;Gerrrr;Patch - https://github.com/apache/flink-connector-kafka/pull/11.;;;","24/Mar/23 20:33;tzulitai;Merged to {{apache/flink-connector-kafka:main}} via 06789ec7b86a3b2edae1dc7f9f29d672ebd3610b;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated TableEnvironment#registerTable,FLINK-31598,13529806,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,23/Mar/23 15:22,28/Mar/23 08:18,04/Jun/24 20:41,28/Mar/23 08:18,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 28 08:18:48 UTC 2023,,,,,,,,,,"0|z1gszs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 08:18;jark;Fixed in master: 2a3652d856347ddc333a476aacb0e750f6502a7b and 46a46195618fddb1c511e01543583802972dc34c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated TableEnvironment#registerFunction,FLINK-31597,13529805,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,23/Mar/23 15:21,22/Nov/23 14:53,04/Jun/24 20:41,22/Nov/23 14:25,,,,,,,,,,,,,,,,,,,,,1.19.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 22 14:25:43 UTC 2023,,,,,,,,,,"0|z1gszk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","22/Nov/23 14:25;Sergey Nuyanzin;Merged to master as [c61c09e464073fae430cab2dd56bd608f9d275fd|https://github.com/apache/flink/commit/c61c09e464073fae430cab2dd56bd608f9d275fd];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated methods in TableEnvironment,FLINK-31596,13529804,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jark,jark,23/Mar/23 15:20,01/Dec/23 06:40,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"This is a preparation to remove the deprecated methods in Table API for Flink v2.0. Currently, the deprecated methods of TableEnvironment and StreamTableEnvironment are still used in many places. This is an umbrella issue to clean up the usage.

[1] [https://docs.google.com/spreadsheets/d/1dZBNHLuAHYJt3pFU8ZtfUzrYyf2ZFQ6wybDXGS1bHno/edit?pli=1#gid=0]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 24 03:41:47 UTC 2023,,,,,,,,,,"0|z1gszc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 03:41;taoran;hi, jark. it's a good improvement to remove deprecated methods in TableEnv. found other deprecated places int TableEnvironment.

getCompletionHints
scan (can be replaced by from methods) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
MiniBatchLocalGroupAggFunction produces wrong aggregate results with state clean up,FLINK-31595,13529802,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Bo Cui,Bo Cui,23/Mar/23 15:15,18/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-critical,pull-request-available,,,,"If the upstream operator supports retract data, and the first data in a batch may be retract data, and the retract data should be ignored.

https://github.com/apache/flink/blob/a64781b1ef8f129021bdcddd3b07548e6caa4a72/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchLocalGroupAggFunction.java#L68",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:11 UTC 2023,,,,,,,,,,"0|z1gsyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/23 23:11;Bo Cui;like the sql:

select a, max(b), max(c) from (

  select * from (

    select *, row_number(...) as rn from T

  ) where rn=1

) group by a

 

the ttl of job is 1h and minibatch is enabled, so row_number ttl is 1.5h, group ttl is 1h.

The time of retract data may be 1.2h,  and minbatch state has expired,  so the aggregation result is incorrect.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
Updates the docs stable version,FLINK-31594,13529796,13529600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,mapohl,mapohl,23/Mar/23 14:32,24/Mar/23 06:22,04/Jun/24 20:41,24/Mar/23 06:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Update docs to ""stable"" in {{docs/config.toml}} in the branch of the _just-released_ version:
 * Change V{{{}ersion{}}} from {{{}x.y-SNAPSHOT }}to \{{{}x.y.z{}}}, i.e. {{1.6-SNAPSHOT}} to {{1.6.0}}
 * Change V{{{}ersionTitle{}}} from {{x.y-SNAPSHOT}} to {{{}x.y{}}}, i.e. {{1.6-SNAPSHOT}} to {{1.6}}
 * Change Branch from {{master}} to {{{}release-x.y{}}}, i.e. {{master}} to {{release-1.6}}
 * Change {{baseURL}} from {{//[ci.apache.org/projects/flink/flink-docs-master|http://ci.apache.org/projects/flink/flink-docs-master]}} to {{//[ci.apache.org/projects/flink/flink-docs-release-x.y|http://ci.apache.org/projects/flink/flink-docs-release-x.y]}}
 * Change {{javadocs_baseurl}} from {{//[ci.apache.org/projects/flink/flink-docs-master|http://ci.apache.org/projects/flink/flink-docs-master]}} to {{//[ci.apache.org/projects/flink/flink-docs-release-x.y|http://ci.apache.org/projects/flink/flink-docs-release-x.y]}}
 * Change {{IsStable}} to {{true}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 24 06:22:08 UTC 2023,,,,,,,,,,"0|z1gsxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 04:12;Wencong Liu;Hello [~mapohl] , I'd like to take this ticket.;;;","24/Mar/23 06:22;renqs;[~Wencong Liu] This issue has been resolved by [df4d2ff7|https://github.com/apache/flink/commit/df4d2ff797ad8e1d1b570a98d18d593fc8d94fce] on release-1.17. Thanks for the help anyway!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Update reference data for Migration Tests,FLINK-31593,13529795,13529600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,23/Mar/23 14:31,24/Apr/23 04:05,04/Jun/24 20:41,24/Apr/23 04:05,,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,,,,,,,0,pull-request-available,,,,,"# Update {{CURRENT_VERSION in TypeSerializerUpgradeTestBase}}  with the new version. This will likely fail some tests because snapshots are missing for that version. Generate them, for example in {{TypeSerializerUpgradeTestBase.}} 
 # (major/minor only) Update migration tests in master to cover migration from new version: (search for usages of FlinkV{{{}ersion{}}})
 ** AbstractOperatorRestoreTestBase
 ** CEPMigrationTest
 ** BucketingSinkMigrationTest
 ** FlinkKafkaConsumerBaseMigrationTest
 ** ContinuousFileProcessingMigrationTest
 ** WindowOperatorMigrationTest
 ** StatefulJobSavepointMigrationITCase
 ** StatefulJobWBroadcastStateMigrationITCase",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28440,,FLINK-27518,,,,,,,,,,,,,FLINK-31765,FLINK-31766,"28/Mar/23 12:36;mapohl;FLINK-31593.StatefulJobSavepointMigrationITCase.create_snapshot.log;https://issues.apache.org/jira/secure/attachment/13056861/FLINK-31593.StatefulJobSavepointMigrationITCase.create_snapshot.log","28/Mar/23 12:49;mapohl;FLINK-31593.StatefulJobSavepointMigrationITCase.verify_snapshot.log;https://issues.apache.org/jira/secure/attachment/13056862/FLINK-31593.StatefulJobSavepointMigrationITCase.verify_snapshot.log",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 24 04:05:51 UTC 2023,,,,,,,,,,"0|z1gsxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 18:09;mapohl;{{BucketingSinkMigrationTest}} not available anymore?;;;","23/Mar/23 19:00;mapohl;{{FlinkKafkaProducerMigrationTest}} is missed in the description;;;","28/Mar/23 11:55;mapohl;I'm moving the conversation with [~Yanfei Lei] (which happened initially in [FLINK-28440|https://issues.apache.org/jira/browse/FLINK-28440?focusedCommentId=17705953&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17705953]) in here:
{quote}
Matthias Pohl It seems that these are two problems. FLINK-28440 is the DSTL file of the changelog that can‘t be found, and FLINK-31593 is the rocksdb's file of the changelog state backend  that can‘t be found. 
Did the job restored from a native savepoint? currently, changelog does not support native savepoint( https://issues.apache.org/jira/browse/FLINK-29802).

I found some log in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47590&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba, and found the following log, it seems that job was restored from native savepoint. 

11:35:10,088 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Reset the checkpoint ID of job 12f8ba2f80aca97baf6f50f5117c8128 to 3.11:35:10,088 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 12f8ba2f80aca97baf6f50f5117c8128 from Savepoint 2 @ 0 for 12f8ba2f80aca97baf6f50f5117c8128 located at file:/__w/1/s/flink-tests/target/test-classes/stateful-scala-udf-migration-itcase-flink1.17-rocksdb-checkpoint. 
{quote};;;","28/Mar/23 12:01;Yanfei Lei;We can see the snapshot type in  CheckpointCoordinator is savepoint, but the snapshot type in filePath ""file:/__w/1/s/flink-tests/target/test-classes/stateful-scala-udf-migration-itcase-flink1.17-rocksdb-checkpoint"" is [checkpoint|https://github.com/apache/flink/blob/master/flink-tests/src/test/scala/org/apache/flink/api/scala/migration/StatefulJobSavepointMigrationITCase.scala#L128], this is a bit strange.;;;","28/Mar/23 12:07;mapohl;The specific test run that failed used {{SnapshotType.CHECKPOINT}}. {{ChangelogStateBackend}} is explicitly disabled for {{SAVEPOINT_NATIVE}} (see [SnapshotMigrationTestBase:272ff|https://github.com/apache/flink/blob/3856c49af77601cf7943a5072d8c932279ce46b4/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L272]).

I created the snapshot reference data by updating {{StatefulJobSavepointMigrationITCase.currentVersion}} to {{FlinkVersion.v1_17}}. and changing {{StatefulJobSavepointMigrationITCase.executionMode}} to {{ExecutionMode.CREATE_SNAPSHOT}}. The test data generation was executed while having {{release-1.17.0}} checked out. This worked without problems for any other test specification. Only RocksDB state backend + checkpoint seems to have this problem.

Essentially, it's the following test setup that is problematic:
{code}
      SnapshotSpec.withVersions(
        StateBackendLoader.ROCKSDB_STATE_BACKEND_NAME,
        SnapshotType.CHECKPOINT,
        FlinkVersion.rangeOf(FlinkVersion.v1_15, currentVersion)))
{code};;;","28/Mar/23 12:18;mapohl;{quote}
We can see the snapshot type in  CheckpointCoordinator is savepoint, but the snapshot type in filePath ""file:/__w/1/s/flink-tests/target/test-classes/stateful-scala-udf-migration-itcase-flink1.17-rocksdb-checkpoint"" is checkpoint, this is a bit strange.
{quote}
After going through the code, it's indeed odd.;;;","28/Mar/23 12:40;mapohl;I attached the debug logs of the snapshot creation test run  [^FLINK-31593.StatefulJobSavepointMigrationITCase.create_snapshot.log] .

The actual snapshot creation is triggered after the log print ""Triggering snapshot."". in the logs that follow you see that it's a checkpointtype snapshot that is created:
{code}
[...]
8776 [main] INFO  org.apache.flink.test.checkpointing.utils.SnapshotMigrationTestBase [] - Triggering snapshot.
8778 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.jobmaster.JobMaster [] - Triggering a manual checkpoint for job b0d7b4f94a20c2f353f65dfd8e8d3682.
[...]
9122 [Flat Map -> Sink: Unnamed (4/4)#0] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask [] - Starting checkpoint 1 CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD} on task Flat Map -> Sink: Unnamed (4/4)#0
[...]
{code};;;","28/Mar/23 12:51;mapohl;I uploaded the debug logs of the corresponding verify test run, too. We also see that restoring happens from a savepoint  [^FLINK-31593.StatefulJobSavepointMigrationITCase.verify_snapshot.log] . But you can check the snapshot's {{_metadata}} file: There we see a {{CheckpointType}} when opening the binary file with a text editor. ;;;","28/Mar/23 13:56;mapohl;Ok, I did a bit more digging. The savepoint phrase in the logs is generated, because the checkpoint properties are set in [CheckpointCoordinator.restoreSavepoint|https://github.com/apache/flink/blob/dca819556fb9b675852df99ada45e0f22262cb28/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1828]. That means, even if we restore from a checkpoint, the logs will state ""Savepoint"". ...at least if we rely on {{RestoreMode.LEGACY}} or {{RestoreMode.NO_CLAIM}} (which seems to be [the default|https://github.com/apache/flink/blob/c0936deaf99390fc727acc8633e3be22e62f4bf5/flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/RestoreMode.java#L57] as far as I can see).

Therefore, the savepoint phrase shouldn't bother us in the logs.;;;","28/Mar/23 14:07;mapohl;[~roman] can you give some guidance here on what's causing the build to fail? It appears to be reproducible. But I'm struggling to dig through the statebackend code myself. You might have more context to come up with a conclusion.;;;","29/Mar/23 03:07;Yanfei Lei;I found the following log in  [^FLINK-31593.StatefulJobSavepointMigrationITCase.create_snapshot.log], files that cannot be found in restore are deleted during shutdown, If all checkpoints are considered that it should be discarded on shutdown, then [lowestRetained|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StandaloneCompletedCheckpointStore.java#L149-L184] will be set to {{{}long.max{}}}. I think the [lowestRetained|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StandaloneCompletedCheckpointStore.java#L149-L184] should be set according to `maxNumberOfCheckpointsToRetain`, WDYT?  cc [~roman] 

 
{code:java}
9284 [flink-akka.actor.default-dispatcher-9] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Discard state created before checkpoint 9223372036854775807 and not used afterwards
9284 [flink-akka.actor.default-dispatcher-9] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Scheduled delete of state handle Wrapped{IncrementalRemoteKeyedStateHandle{backendIdentifier=813300b3-b788-4d77-a369-cb9d1a8e29c5, stateHandleId=0063721e-db9a-40d0-9350-2a954e97a6e9, keyGroupRange=KeyGroupRange{startKeyGroup=3, endKeyGroup=3}, checkpointId=1, sharedState={}, privateState={MANIFEST-000004=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/83ba12da-f5d7-4ec6-bc3b-f05aeb5af869 [1504 bytes], 000037.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/693caeea-6ca5-435b-97b9-6097074b4af3 [978 bytes], 000031.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/0c7a6f22-2e37-4d4d-b11e-807e7754910b [985 bytes], 000033.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/3ff030db-db0b-44f1-b622-412999fd522f [971 bytes], 000035.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/398f55fa-819a-4094-adac-aea6064b5a12 [974 bytes], CURRENT=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/5d13630c-1f30-47c9-8e29-536fce74609a [16 bytes], 000039.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/b233c3d8-8654-48a1-bb84-aa3f39d9d98d [968 bytes], OPTIONS-000029=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/5429c603-2c50-4ae2-90ae-4be14cb00645 [46014 bytes], 000038.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/f1c03c83-bd20-4878-9049-23f0e1d94049 [971 bytes], 000030.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/e64cbdb2-fa10-4d93-bfc1-a25bbd3f78f6 [976 bytes], 000036.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/7e12fafb-a46a-4af1-bc99-6168fb5f2523 [968 bytes], 000032.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/f7d435f9-e9fa-496b-8d74-8a2c039e7db4 [975 bytes], 000034.sst=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/dec0bd1a-b937-4e59-9143-5eeb71de6089 [2099 bytes]}, metaStateHandle=File State: file:/tmp/junit1099191650015089228/checkpoints_5594496b-29d3-4834-ac07-d7503137513a/b0d7b4f94a20c2f353f65dfd8e8d3682/taskowned/a3f92dc5-1604-4e75-a7ee-f680f5401205 [8135 bytes], registered=true}}. {code}
 

 ;;;","29/Mar/23 06:31;mapohl;But I would have expected for this issue to pop up already with the 1.16 release ([e85ba55|https://github.com/apache/flink/commit/e85ba5514f259064ae9f2c550b56a64b06a0fdcf]). Or am I wrong here? I'm just asking because I still don't want to rule out that I did something wrong when triggering the test data generation. But tbh, for this specific test the documentation is actually quite clear.;;;","29/Mar/23 07:07;mapohl;After going through the code a bit more, I tend to agree with your conclusion, [~Yanfei Lei]. But it still leaves question marks on why this issue didn't show up earlier.;;;","30/Mar/23 04:03;Yanfei Lei;> But it still leaves question marks on why this issue didn't show up earlier.

I guess it is some corner case, 1.16 did not trigger this problem when creating checkpoint.

There are two possibilities for [lowestRetained|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StandaloneCompletedCheckpointStore.java#L149-L184] = Long.Max:
 # The checkpoints deque is empty. Will manually triggered checkpoints be added to the deque?
 # The chk-3's props.discardOnJobCancelled is true. This contradicts CheckpointRetentionPolicy.RETAIN_ON_CANCELLATION, there may be some corner cases that have been ignored.

Have you tried re-creating the 1.17 checkpoint? Can the problem be reproduced?;;;","30/Mar/23 09:14;mapohl;I recreated the checkpoint at least 3 times with consistent failure. Could you try it on your end? I'm pretty sure you should run into the same issue:
#  git checkout release-1.17.0
# Change {{StatefulJobSavepointMigrationITCase#CURRENT_VERSION}} to {{1.17}} and {{executionMode}} to {{CREATE_SNAPSHOT}}.
# Run the test
# Change {{StatefulJobSavepointMigrationITCase#executionMode}} back to {{VERIFY_SNAPSHOT}} and run the test again.;;;","30/Mar/23 10:27;mapohl;Ok, just now I tried it once more and it worked. I'm gonna push that one and see whether ci succeeds.

But anyway, we should come up with a follow-up issue for that one?!;;;","30/Mar/23 10:58;mapohl;It feels like the previous try was a lucky shot. I wanted to do the same with {{StatefulJobWBroadcastStateMigrationITCase}} since it's also failing. But there are constantly some incremental checkpoints that are cleaned up based on the logs:
{code}
14072 [flink-akka.actor.default-dispatcher-8] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Discard state created before checkpoint 9223372036854775807 and not used afterwards
14072 [flink-akka.actor.default-dispatcher-8] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Scheduled delete of state handle Wrapped{IncrementalRemoteKeyedStateHandle{backendIdentifier=a8221137-35b5-4a81-b6b6-dbd42f26b72e, stateHandleId=f6d026b2-070c-4f65-906b-9c8a76f464a2, keyGroupRange=KeyGroupRange{startKeyGroup=1, endKeyGroup=1}, checkpointId=1, sharedState={}, privateState={MANIFEST-000004=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/20c33d68-4eee-446f-a577-8846ad357e71 [1504 bytes], 000037.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/09d29222-4e74-4f96-b386-fada894a5659 [978 bytes], 000031.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/2dbec345-6e8a-4c4e-8e7e-a091a26450a0 [985 bytes], 000033.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/92b35651-65f2-400f-bb0c-3a6985dbacc1 [971 bytes], 000035.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/affd3ff7-a386-4792-949c-3a45046ad43f [974 bytes], CURRENT=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/9e8e1d35-c776-4453-8ccf-2f293af4ab8d [16 bytes], 000039.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/fe7f7100-2f61-4b52-bf86-3d5c0a465cca [968 bytes], OPTIONS-000029=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/85b3ccab-fc2d-4cc8-89ff-bc59468eb7cf [46013 bytes], 000038.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/dd46893b-9c5e-4848-80bb-06df0cec3b15 [971 bytes], 000030.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/ed5b7433-701c-4985-84cd-d1fdb02746d0 [976 bytes], 000036.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/ed0db297-3bf6-4665-8007-e72dc9cf96a0 [968 bytes], 000032.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/8778f89c-af40-418b-b445-c5a4caac69aa [975 bytes], 000034.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/c5d7a495-2e4b-4c6e-80ab-02a06f28d730 [2097 bytes]}, metaStateHandle=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/c42cfb16-dafe-47cb-b746-8d7f1a2987b5 [8135 bytes], registered=true}}.
14072 [flink-akka.actor.default-dispatcher-8] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Scheduled delete of state handle EmptyDiscardStateObject{b74fb79e-ffa0-4715-bd37-530e42c8295a}.
14072 [flink-akka.actor.default-dispatcher-8] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Scheduled delete of state handle Wrapped{IncrementalRemoteKeyedStateHandle{backendIdentifier=1323d441-6ee1-4550-9f5a-2c9371b18163, stateHandleId=c9e952de-f7fc-4fb4-8542-47a416b1503f, keyGroupRange=KeyGroupRange{startKeyGroup=3, endKeyGroup=3}, checkpointId=1, sharedState={}, privateState={MANIFEST-000004=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/bd41577a-6ad5-4949-8aa6-a212e7cc5ee7 [1504 bytes], 000037.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/a45ba92c-86cf-4b87-b821-f24062bc707a [978 bytes], 000031.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/1f5a6a50-f07a-43a4-97e0-4ff4bb4b7c5e [985 bytes], 000033.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/a7ac64ac-ba25-405e-9a71-b3dacb6be146 [971 bytes], 000035.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/579219a0-f75a-4969-8a4e-f1fb42e96cd8 [974 bytes], CURRENT=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/5482073c-531c-4ba9-a9c6-969a30c5bee5 [16 bytes], 000039.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/4b7ed710-9eca-4f47-80e1-96ea2646f084 [968 bytes], OPTIONS-000029=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/c1d4a8a8-3889-494d-96a7-443dbec20754 [46013 bytes], 000038.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/b03df3a0-33bf-41cd-aefa-5483478164a7 [971 bytes], 000030.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/51c777e6-973e-4630-9849-2e184b88596d [976 bytes], 000036.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/82f3464d-ebc5-4a20-8200-3769520dea00 [968 bytes], 000032.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/55ac8a74-9a5c-4e8a-a7b6-c6fa8a9ade31 [975 bytes], 000034.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/e091ef0b-fb7f-43be-ba46-5e9998f52a73 [2097 bytes]}, metaStateHandle=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/23720006-b2ca-438f-b7a5-95e9c7c02483 [8135 bytes], registered=true}}.
14072 [flink-akka.actor.default-dispatcher-8] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Scheduled delete of state handle EmptyDiscardStateObject{fd48cd55-c1d3-4844-a976-572a80bde8e2}.
14072 [flink-akka.actor.default-dispatcher-8] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Scheduled delete of state handle EmptyDiscardStateObject{7ed77adc-c7ea-4e59-a0b9-8745e5d4770a}.
14072 [flink-akka.actor.default-dispatcher-8] DEBUG org.apache.flink.runtime.state.SharedStateRegistryImpl [] - Scheduled delete of state handle Wrapped{IncrementalRemoteKeyedStateHandle{backendIdentifier=c3e64345-2042-41d2-beba-02d354aa725b, stateHandleId=74b12755-6624-4997-b545-f36df4c344d6, keyGroupRange=KeyGroupRange{startKeyGroup=2, endKeyGroup=2}, checkpointId=1, sharedState={}, privateState={MANIFEST-000004=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/a471413a-140a-41ab-a8d2-3f186eadc07a [1581 bytes], 000037.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/24c1a635-16d2-486a-9c0f-b5265b490c5b [1002 bytes], 000031.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/d6d69265-47e8-4616-b13c-e58a6a7f37fc [1004 bytes], 000033.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/c1f11a24-217d-4646-a4ab-a255d5ad3590 [995 bytes], 000035.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/f6c8d992-876e-4159-8292-7cb571fbd975 [998 bytes], CURRENT=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/7ebe0727-a2d4-42da-a09d-0b2d6ad2b2d6 [16 bytes], 000039.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/20e9d31a-1c31-4c96-90f0-93bd2bf1a5ce [987 bytes], OPTIONS-000029=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/fb6137e9-2036-46e7-a9c1-e8fa284b9144 [46013 bytes], 000040.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/41f9838b-0bdd-4a6c-ae2b-51544e522ac5 [977 bytes], 000038.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/43124311-c3c2-4143-a117-7b7be285c08c [2097 bytes], 000030.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/698051e4-e19e-4d3a-9b56-26165b84bc6f [999 bytes], 000036.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/d0bc852a-8581-4a2e-969e-b1030afc5164 [982 bytes], 000032.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/b9fba9c0-64a2-44ef-97c0-ad61f89d0b68 [991 bytes], 000034.sst=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/f0130bb7-d225-488e-9764-03bfbb62f78b [2097 bytes]}, metaStateHandle=File State: file:/tmp/junit3473564016309490181/checkpoints_b273b9b5-0c95-47eb-ba6d-b889b906a752/9680ae77c09d7de75d7fc0df6f0ffb08/taskowned/e97af735-7c1e-4ec9-92a8-411fbc7c6c87 [8135 bytes], registered=true}}.
{code}

We see the same outcome here as already described by [~Yanfei Lei] with these missing artifacts causing the verification step to fail.;;;","03/Apr/23 09:21;mapohl;[~roman] sorry for re-pinging you. It just would be nice to finalize the 1.17 release. May you have a look at this issue? 
* Is there away to make the test data generation succeed? 
* Do we need to adapt the test or is this an issue that needs to be handled in production code as [~Yanfei Lei] suggested?;;;","03/Apr/23 21:16;roman;Sorry for the late reply [~mapohl], I'll try to look into it tomorrow.;;;","04/Apr/23 23:28;roman;I looked into StatefulJobWBroadcastStateMigrationITCase failiure which fails CI for your PRs.

It failed because TM was unable to find the referenced sst file:

 
{code:java}
java.io.FileNotFoundException: /tmp/junit6895540657846214402/checkpoints_e6e27005-a3fb-4c23-81e8-f6a2e827079b/4b3009354700b5a3a966f015556c8fb2/taskowned/1d9945c1-44b9-4d7c-9b20-d5ff90e68d7b (No   such file or directory) {code}
The file is indeed referenced in _metadata and is missing in resources (stateful-scala-with-broadcast-udf-migration-itcase-flink1.17-rocksdb-checkpoint/_metadata).

It was likely generated during changelog materialization (becuase it uses taskowned/ dir). Materialization is not deterministic, which might explain why it didn't show up earlier.

 

It should be [possible|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/ops/state/state_backends/#upgrading-existing-jobs] to restore from such a checkpoint (given all the files are present) even with changelog disabled.

Alternatively, we can just generate the checkpoints with changelog (or materialization) disabled. 

 

Am I missing something [~mapohl] , [~Yanfei Lei] ?

 

(if the above sounds reasonable, [~mapohl]  please let me know if I can help in checkpoints generation)

 ;;;","05/Apr/23 06:42;mapohl;Thanks [~roman]. 

{quote}
It should be possible to restore from such a checkpoint (given all the files are present) even with changelog disabled.
{quote}
With this statement, you're saying that eventually, the test data generation should succeed because of the non-determinism?! Isn't that an issue users could face as well?

{quote}
Alternatively, we can just generate the checkpoints with changelog (or materialization) disabled. 
{quote}
Based on this statement, I'd assume that we should disable changelog behavior for this test to ensure determinism and a successful test run from the start. I'm just wondering whether that's going against the intent of this test.;;;","06/Apr/23 14:06;mapohl;[~Yanfei Lei] Roman has a point with temporary folders being referenced in the generated _metadata file: Do we know why this is like that? The state of the checkpoint shouldn't be stored in a temporary folder as far as I understand. The checkpoint should be rather self-contained so that we can safely copy it over into the tests' resources subfolder.

[~roman]'s suggestion to disable the changelog statebackend for the checkpoint test runs (i.e. extending the if condition in [SnapshotMigrationTestBase:273|https://github.com/apache/flink/blob/c89e400ae379c8b7490d9af20f82f49319895dce/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L273]) actually works. Would that be a viable solution? [~Yanfei Lei] Or do we hide a different problem with this issue? I saw that you re-enabled the changelog backend for the tests in [93146a78|https://github.com/apache/flink/commit/93146a78].;;;","07/Apr/23 02:42;Yanfei Lei;[~roman]

> It was likely generated during changelog materialization (becuase it uses taskowned/ dir). Materialization is not deterministic, which might explain why it didn't show up earlier.

 

Although ""Materialization is not deterministic and uses taskowned/ dir"", the materialized SSTs in use would not be deleted, because materialized handles are registered separately to JM(FLINK-25478).

 According to the [^FLINK-31593.StatefulJobSavepointMigrationITCase.create_snapshot.log]  it can be seen that when JM shutdown, all shared materialized SSTs are deleted,  because the [lowestRetained|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StandaloneCompletedCheckpointStore.java#L149-L184] was set as {{long.max. }}I think there is something wrong with `StandaloneCompletedCheckpointStore#shutdown()`.

 

[~mapohl] I agree with disabling the changelog statebackend for the checkpoint test runs in order not to block the validation of 1.17-release, and we can open another ticket to investigate this bug.;;;","11/Apr/23 06:44;mapohl;thanks for verification. I create FLINK-31765 to disable changelog backend and FLINK-31766 to look into the actual issue.;;;","18/Apr/23 10:38;Yanfei Lei;[~mapohl] Sorry for the late reply,  after reproducing this issue locally, I think the root cause is that `StatefulJobSavepointMigrationITCase` partially moves the snapshot files to a new [directory|https://github.com/apache/flink/blob/master/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L337-L342]. 

All state backends in `StatefulJobSavepointMigrationITCase`  are non-incremental, all files are placed in chk-x folder. But the files of changelog state backend are not completely placed under chk-x, some files are placed under taskowned folder, something like:
{code:java}
├── chk-2
│   ├── 5487d0fd-a361-4085-8ee0-7364ffd4511a
│   ├── _metadata
│   └── d3596cf7-3c6e-4081-b37b-f5a3e1a40086
├── shared
└── taskowned
    ├── 01aefc31-8ee1-41a8-9cd3-a94ccf85052f
    ├── 02bf09d3-73db-4c45-b6a1-15987659e3e6
    ├── 0c456b9b-9f90-4696-a2be-16e5938358ae {code}
 

This also explains why this issue didn't show up earlier:

1. If the version <= 1.15, changelog state backend is disabled.

2. If the version >= 1.16, change state backend is randomly turned on, when the changelog is turned off, this issue would not be triggered.

 

So I have two questions:
 # Whether the incremental rocksdb state backend should be tested here?
 # Do we need to change the move function to support testing of changelog state backend ?;;;","18/Apr/23 19:21;mapohl;I guess the solution should be discussed in FLINK-31766. I'll leave the answer to these question to you and [~roman] as you are more familiar with the requirements of the state backends.;;;","24/Apr/23 04:05;mapohl;I kept the commits separate to be able to revert individual tests
master:
* f9b3e0b7bc0
* b696e4125b8
* 888d0489801
* 86e8df4fe8a
* 800984babaf
* fca07c6484b
* a6e2e56baa3
* 601df8e728d
* 4f8bb1843f2
* 2d8cba222cc
* e60fb06aa15
* a11a8b48b20
1.17:
* cae1ad3296a
* f533a9208a7
* 162635f2434
* 6f16dfe2a04
* 13634524c2c
* 2e2ecd30b2c
* 988c1b6b2eb
* 16e0658277b
* 6d98e684e0f
* 4503dd8d40b
* 1de67ffc1ab;;;"
Configurable deletion propagation for Flink resources,FLINK-31592,13529781,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,23/Mar/23 13:39,30/Mar/23 11:26,04/Jun/24 20:41,30/Mar/23 11:26,,,,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Currently we use the default (background) deletion propagation when we delete Kubernetes resources such as the JobManager deployment.

In most cases this is acceptable however this can lead to lingering resources when the garbage collection is slow on the k8s cluster. We therefore propose to make this configurable and also change the default to Foreground which is more predictable and works better for our purposes in the operator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 30 11:26:43 UTC 2023,,,,,,,,,,"0|z1gsu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 11:26;gyfora;merged to main 6620466d31df7b22e4cae4ed0bd2eab70280c8c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Extend JobGraphWriter to persist requirements,FLINK-31591,13529768,13527017,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Mar/23 12:52,27/Mar/23 09:03,04/Jun/24 20:41,27/Mar/23 09:03,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 27 09:03:42 UTC 2023,,,,,,,,,,"0|z1gsrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 09:03;chesnay;master: ca90244e84f0a15b2e76e102dd512d20d2ff6b3b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Allow setting JobResourceRequirements through JobMasterGateway,FLINK-31590,13529767,13527017,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Mar/23 12:51,29/Mar/23 09:16,04/Jun/24 20:41,29/Mar/23 09:16,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 29 09:16:37 UTC 2023,,,,,,,,,,"0|z1gsr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 09:16;chesnay;master: 678789afb4277e20694c70cba8261bd21555dc48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce the pressure of the list pod method on the k8s cluster,FLINK-31589,13529757,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,spoon-lz,spoon-lz,23/Mar/23 11:42,30/Mar/23 08:33,04/Jun/24 20:41,30/Mar/23 08:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,,,,,,"In the production environment, we found that if a cluster-level failure occurs, the jobs of a cluster will failover at the same time, which puts a lot of pressure on the k8s cluster.
Through the monitoring of k8s, we found that the method of list pod caused relatively high pressure.
By querying the relevant information of fabric8, we found an optimization scheme for the list pod, that is, adding the parameter of 
{code:java}
ResourceVersion=0{code}
 in the list pod can greatly reduce the pressure on the k8s cluster.

After adding this configuration, even if a large number of jobs failover, it does not cause too much pressure on the cluster and has achieved certain results

[link|https://github.com/fabric8io/kubernetes-client/issues/4670]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27925,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 30 08:33:21 UTC 2023,,,,,,,,,,"0|z1gsow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 11:46;spoon-lz;hi [~wangyang0918]  Do you think this improvement is necessary?;;;","23/Mar/23 11:55;huwh;[~spoon-lz] thanks for reporting this. 

I think this is duplicated with https://issues.apache.org/jira/browse/FLINK-27925;;;","24/Mar/23 02:15;spoon-lz;[~huwh]  Thank you for your reminder, it is indeed the same problem, maybe we can solve this problem by upgrading the version of 
{code:java}
<kubernetes.client.version>{code}
 instead of modifying the code, because in version 6.4.0, this has been used as the default value

[link|https://github.com/fabric8io/kubernetes-client/pull/4676];;;","30/Mar/23 08:33;Weijie Guo;Thanks [~spoon-lz] for reporting this, let's track this in FLINK-27925.;;;",,,,,,,,,,,,,,,,,,,,,,,
The unaligned checkpoint type is wrong at subtask level,FLINK-31588,13529747,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,23/Mar/23 10:45,25/Apr/23 02:23,04/Jun/24 20:41,25/Apr/23 02:23,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,,,,,"FLINK-20488 supported show checkpoint type for each subtask, and it based on received `CheckpointOptions` and it's right.

However, FLINK-27251 supported timeout aligned to unaligned checkpoint barrier in the output buffers. It means the received `CheckpointOptions` can be converted from aligned checkpoint to unaligned checkpoint.

So, the unaligned checkpoint type may be wrong at subtask level. For example, as shown in the figure below, Unaligned checkpoint type is false, but it is actually Unaligned checkpoint (persisted data > 0).

 

!image-2023-03-23-18-45-01-535.png|width=1879,height=797!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27251,,FLINK-27251,,,,FLINK-31864,,,,,,,,,,,"23/Mar/23 10:45;fanrui;image-2023-03-23-18-45-01-535.png;https://issues.apache.org/jira/secure/attachment/13056617/image-2023-03-23-18-45-01-535.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Apr 23 02:15:51 UTC 2023,,,,,,,,,,"0|z1gsmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 11:02;fanrui;Hi [~pnowojski] [~yuanmei] , please help take a look this ticket in your free time, thanks!

I prefer generate unaligned checkpoint type based on persisted data. If it is switched to an unaligned checkpoint and no data is persisted, it is still considered an aligned checkpoint for flink users. WDYT?;;;","07/Apr/23 20:21;pnowojski;Sorry for late response, I've just found an old tab with WIP comment that I wanted to write, but somehow didn't send as something must have interrupted me :(

Thanks for reporting the issue. I see the problem. I think ideally we should try to keep the semantic of that flag in sync with what {{StreamTask}} was actually doing. If checkpoint was unaligned, as it arrived unaligned, it should be reported as such, even if that particular subtask didn't persist any data. Can we still achieve that? ;;;","08/Apr/23 02:11;fanrui;Thanks for your feedback.
{quote} If checkpoint was unaligned, as it arrived unaligned, it should be reported as such, even if that particular subtask didn't persist any data.
{quote}
Sounds make sense. I will prepare this PR next week.;;;","23/Apr/23 02:15;fanrui;Thanks [~pnowojski] for the review and discussion.

Merged master commit: d46d8d0f6b590f185608b23fbe8b2fcbded111de

Merged 1.17-release commit : a3c00eed371f8a2c9bfe557fa07dc7bc8a04f14e

Merged 1.16-release commit : 60cadec7c1f3beb3c9eb7e45cbd7ab9d99062e48;;;",,,,,,,,,,,,,,,,,,,,,,,
Vote on the release candidate,FLINK-31587,13529738,13529734,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:52,23/Mar/23 09:56,04/Jun/24 20:41,23/Mar/23 09:56,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:55:59 UTC 2023,,,,,,,,,,"0|z1gsko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:55;renqs;[https://lists.apache.org/thread/d9o0tgnv0fl9goqsdo8wmq9121b9wolv];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Propose a pull request for website updates,FLINK-31586,13529737,13529734,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:52,23/Mar/23 09:55,04/Jun/24 20:41,23/Mar/23 09:55,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 # update [apache/flink-web:_config.yml|https://github.com/apache/flink-web/blob/asf-site/_config.yml]
 ## update {{FLINK_VERSION_STABLE}} and {{FLINK_VERSION_STABLE_SHORT}} as required
 ## update version references in quickstarts ({{{}q/{}}} directory) as required
 ## (major only) add a new entry to {{flink_releases}} for the release binaries and sources
 ## (minor only) update the entry for the previous release in the series in {{flink_releases}}
 ### Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number.
 ## add a new entry to {{release_archive.flink}}
 # add a blog post announcing the release in _posts
 # add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/]). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project.

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]
 * (major only) Check {{docs/config.toml}} to ensure that
 ** the version constants refer to the new version
 ** the {{baseurl}} does not point to {{flink-docs-master}}  but {{flink-docs-release-X.Y}} instead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:55:19 UTC 2023,,,,,,,,,,"0|z1gskg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:55;renqs;Release note: [https://github.com/apache/flink/pull/22146]

Web PR: [https://github.com/apache/flink-web/pull/618];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Stage source and binary releases on dist.apache.org,FLINK-31585,13529736,13529734,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:52,23/Mar/23 09:54,04/Jun/24 20:41,23/Mar/23 09:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:54:46 UTC 2023,,,,,,,,,,"0|z1gsk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:54;renqs;* Source and binary: [https://dist.apache.org/repos/dist/dev/flink/flink-1.17.0-rc3/];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Build and stage Java and Python artifacts,FLINK-31584,13529735,13529734,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:52,23/Mar/23 09:54,04/Jun/24 20:41,23/Mar/23 09:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:54:20 UTC 2023,,,,,,,,,,"0|z1gsk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:54;renqs;* [Key|https://dist.apache.org/repos/dist/release/flink/KEYS] fingerprint: A1BD477F79D036D2C30CA7DBCA8AEEC2F6EB040B
 * Maven central deployment:  [https://repository.apache.org/content/repositories/orgapacheflink-1600]
 * Tag {{{}release-1.17.0-rc3{}}}: [https://github.com/apache/flink/releases/tag/release-1.17.0-rc3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Build Release Candidate: 1.17.0-rc3,FLINK-31583,13529734,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:52,04/Apr/23 08:29,04/Jun/24 20:41,23/Mar/23 09:57,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.
h4. Prerequisites

Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code:java}
RC_NUM=""3""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,,,,,,,,,,,,FLINK-31562,,,,,,,,,,,,,FLINK-31578,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:56:53 UTC 2023,,,,,,,,,,"0|z1gsjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:56;renqs;1.17.0 RC3 is accepted: [https://lists.apache.org/thread/cgz83tktlg25klf9fm9m3bp0n1dzwm5n];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Vote on the release candidate,FLINK-31582,13529732,13529728,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:42,23/Mar/23 09:51,04/Jun/24 20:41,23/Mar/23 09:51,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:50:54 UTC 2023,,,,,,,,,,"0|z1gsjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:50;renqs;Voting thread: [https://lists.apache.org/thread/mh5cy45bxkbssxzxrk42jt264k5kt6jr];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Propose a pull request for website updates,FLINK-31581,13529731,13529728,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:42,23/Mar/23 09:48,04/Jun/24 20:41,23/Mar/23 09:48,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 # update [apache/flink-web:_config.yml|https://github.com/apache/flink-web/blob/asf-site/_config.yml]
 ## update {{FLINK_VERSION_STABLE}} and {{FLINK_VERSION_STABLE_SHORT}} as required
 ## update version references in quickstarts ({{{}q/{}}} directory) as required
 ## (major only) add a new entry to {{flink_releases}} for the release binaries and sources
 ## (minor only) update the entry for the previous release in the series in {{flink_releases}}
 ### Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number.
 ## add a new entry to {{release_archive.flink}}
 # add a blog post announcing the release in _posts
 # add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/]). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project.

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]
 * (major only) Check {{docs/config.toml}} to ensure that
 ** the version constants refer to the new version
 ** the {{baseurl}} does not point to {{flink-docs-master}}  but {{flink-docs-release-X.Y}} instead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:48:14 UTC 2023,,,,,,,,,,"0|z1gsj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:48;renqs;Release note: [https://github.com/apache/flink/pull/22146]

Web PR: [https://github.com/apache/flink-web/pull/618];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Stage source and binary releases on dist.apache.org,FLINK-31580,13529730,13529728,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:42,23/Mar/23 09:46,04/Jun/24 20:41,23/Mar/23 09:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:46:25 UTC 2023,,,,,,,,,,"0|z1gsiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:46;renqs;* Source and binary: [https://dist.apache.org/repos/dist/dev/flink/flink-1.17.0-rc2/];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Build and stage Java and Python artifacts,FLINK-31579,13529729,13529728,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:42,23/Mar/23 09:46,04/Jun/24 20:41,23/Mar/23 09:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:45:33 UTC 2023,,,,,,,,,,"0|z1gsio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:45;renqs;* [Key|https://dist.apache.org/repos/dist/release/flink/KEYS] fingerprint: A1BD477F79D036D2C30CA7DBCA8AEEC2F6EB040B
 * Maven central deployment:  [https://repository.apache.org/content/repositories/orgapacheflink-1595]
 * Tag {{{}release-1.17.0-rc2{}}}: [https://github.com/apache/flink/releases/tag/release-1.17.0-rc2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Build Release Candidate: 1.17.0-rc2,FLINK-31578,13529728,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,23/Mar/23 09:42,14/Dec/23 15:17,04/Jun/24 20:41,23/Mar/23 09:52,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.
h4. Prerequisites

Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code:java}
RC_NUM=""2""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,,,,,,,,,,,,FLINK-31562,,,,,,,,,,,,FLINK-31583,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:52:06 UTC 2023,,,,,,,,,,"0|z1gsig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:52;renqs;1.17.0 RC2 is canceled because of:
 * Blocker FLINK-31485;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade documentation to improve Operator,FLINK-31577,13529725,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,highfei2011@126.com,highfei2011@126.com,highfei2011@126.com,23/Mar/23 09:26,27/Mar/23 15:28,04/Jun/24 20:41,27/Mar/23 15:28,kubernetes-operator-1.3.0,kubernetes-operator-1.4.0,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Currently, when I refer to the documentation for operator upgrade, I found that there are some descriptions in the documentation that need to be improved, otherwise the upgrade may fail due to misuse.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 27 15:28:28 UTC 2023,,,,,,,,,,"0|z1gshs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:28;highfei2011@126.com;Hi, [~gyfora], Can you help to assign this jira to me? Thanks.;;;","24/Mar/23 03:48;highfei2011@126.com;Honestly, when users upgrade the version of operator, I would like to refer to a structure similar to the kafka upgrade documentation to guide the upgrade, which has the advantage that users just need to find the corresponding version and install the documentation step by step.
https://kafka.apache.org/33/documentation.html#upgrade
;;;","27/Mar/23 15:28;gyfora;merged to main 6893c88057e7f8af3e0308f69a5de4465fcb20b4;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add JavaDoc that SqlNodeConverter.getCatalogManager is discouraged to use,FLINK-31576,13529640,13528594,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jark,jark,22/Mar/23 15:00,22/Mar/23 15:02,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"See the comment: https://issues.apache.org/jira/browse/FLINK-31464?focusedCommentId=17703555&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17703555

New SqlNodeCovnerters doesn't need to resolve identifiers using the CatalogManager. Operation should be a simple mapping to SqlNode. There should be no resolution logic involved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-22 15:00:27.0,,,,,,,,,,"0|z1gryw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't swap table-planner-loader and table-planner to use hive dialect,FLINK-31575,13529614,13433321,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,22/Mar/23 12:38,23/Jul/23 02:14,04/Jun/24 20:41,23/Jul/23 02:14,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Connectors / Hive,,,,,,0,pull-request-available,,,,,"From Flink 1.15,  to use Hive dialect, user have to swap the flink-table-planner-loader jar with flink-table-planner.jar.

It really bothers some users who want to use Hive dialect like FLINK-27020, FLINK-28618

Althogh we has paid much effort like FLINK-29350, FLINK-29045 to tell users to do the swap, but it'll still not convenient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 23 02:14:20 UTC 2023,,,,,,,,,,"0|z1grt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 13:09;luoyuxia;To decouple Hive with Planner, in  [FLIP-216|https://cwiki.apache.org/confluence/display/FLINK/FLIP-216%3A++Introduce+pluggable+dialect+and+plan+for+migrating+Hive+dialect] , as part of decoupling, we propose to introduce a slim module called flink-table-calcite-bridge which contains the Calcite dependencies for writing planner plugins (e.g. SQL dialects) that interact with Calcite APIs.

Then the Hive connector will depend on the module flink-table-calcite-bridge, the flink-table-planner will also depend on flink-table-calcite-bridge. For decoupling, we can make flink-table-planner.jar pack the module flink-table-calcite-bridge.

But it will still require jar swap. With flink-table-planner-loader, the classes in flink-table-planner.jar will be loaded by a

[submoduleClassLoader|#L117]].  So the calcite related classes will also be loaded by this submoduleClassLoader. 

But the classes releated to Hive dialect, like HiveParser will be loaded FlinkClassLoader(more exactly is AppClassLoader). HiveParser depends on calcite classes,  and then it'll try to load calcite classes but can't find them as they can't be found in FlinkClassLoader.

So, I would like to propose to make flink-table-calcite-bridge to be a seperate jar which only packs calcite dependencies. So that the calcite classes can be loaded and will loaded by same class loader whenever it's for default dialect and Hive dialect.

Hi, [~twalthr], what do you think of this idea?;;;","22/Mar/23 14:14;twalthr;[~luoyuxia] my gut feeling is that the module structure becomes more and more complex. There is only a limited set of people that actually understands how all these modules and classloaders interact. Having a {{flink-table-calcite}} module sounds reasonable. But will it only contain `org.apache.calcite.*` classes including the ones that we override? Or what do you mean with ""contains the Calcite dependencies for writing planner plugins""?;;;","30/Mar/23 12:03;luoyuxia;[~twalthr] Sorry for late reply for I'm doing some invesgation about it. It'll only include calcite dependency in the pom without containing the {{org.apache.calcite.*}} we override. By saying ""for writing planner plugins"", I mean the planner can expose some interfaces releated to calcite for writign planner plugins. But currently, it'll only be used by Hive dialect to build Calcite RelNode. But I can image some day, we may expose some interfaces to allow devs to write some Calcite rules to  customize the planning phase in some degree in the future.

After discuss with [~lincoln.86xy]  and [~jark], I written a [doc|https://docs.google.com/document/d/1VV88R1O1IN-zX3su6RCAgJPuZiymgh4xoL8OJrW-_00/edit?usp=sharing] for the propose to avoid swaping table-planner-loader and table-planner while using Hive dialect. Much appreciated if you can have a look.

 ;;;","23/Jul/23 02:14;luoyuxia;master:

c868d2273086ab9c9deafe5936127ce858e2b1cf;;;",,,,,,,,,,,,,,,,,,,,,,,
Cleanup unused private methods in OperationConverterUtils,FLINK-31574,13529608,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,taoran,taoran,taoran,22/Mar/23 11:49,23/Mar/23 08:33,04/Jun/24 20:41,23/Mar/23 08:33,1.17.0,1.17.1,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,some private methods in OperationConverterUtils should be removed. because called public methods has been removed in FLINK-29585,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 08:33:46 UTC 2023,,,,,,,,,,"0|z1grrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 11:59;taoran;[~luoyuxia] can you help to take a look?;;;","23/Mar/23 08:33;jark;Fixed in master: a64781b1ef8f129021bdcddd3b07548e6caa4a72;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Nexmark performance drops in 1.17 compared to 1.13,FLINK-31573,13529607,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Invalid,,renqs,renqs,22/Mar/23 11:45,23/Aug/23 06:25,04/Jun/24 20:41,23/Aug/23 06:25,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,,,"The case was originally [reported|https://lists.apache.org/thread/n4x2xgcr2kb8vcbby7c6gwb22n0thwhz] in the voting thread of 1.17.0 RC3. 

Compared to Flink 1.13, the performance of Nexmark in 1.17.0 RC3 drops ~8% in query 18. Some details could be found in the [mailing list|https://lists.apache.org/thread/n4x2xgcr2kb8vcbby7c6gwb22n0thwhz]. 

A further investigation showed that with configuration {{execution.checkpointing.checkpoints-after-tasks-finish.enabled}} set to false, the performance of 1.17 is better than 1.16. 

A fully comparison of Nexmark result between 1.16 and 1.17 is ongoing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 23 06:25:11 UTC 2023,,,,,,,,,,"0|z1grrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 03:16;yunta;I think https://github.com/nexmark/nexmark/pull/43 could help mitigate this problem.;;;","23/Aug/23 02:37;masteryhx;Hi, [~renqs] 

Just kindly ping. What's the status of this ticket?

I saw the pr of nexmark is merged to resolve this.

Could we close this or wait the newest test result ?;;;","23/Aug/23 06:25;renqs;[~masteryhx] Thanks for bringing this up. I didn't see further evidence showing the performance issues. Feel free to close it and we can reopen it if there's any update;;;",,,,,,,,,,,,,,,,,,,,,,,,
Other announcements,FLINK-31572,13529605,13529600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,22/Mar/23 11:18,14/Dec/23 15:45,04/Jun/24 20:41,24/Mar/23 03:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"h3. Recordkeeping

Use [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink] to seed the information about the release into future project reports.

(Note: Only PMC members have access report releases. If you do not have access, ask on the mailing list for assistance.)
h3. Flink blog

Major or otherwise important releases should have a blog post. Write one if needed for this particular release. Minor releases that don’t introduce new major functionality don’t necessarily need to be blogged (see [flink-web PR #581 for Flink 1.15.3|https://github.com/apache/flink-web/pull/581] as an example for a minor release blog post).

Please make sure that the release notes of the documentation (see section ""Review and update documentation"") are linked from the blog post of a major release.
We usually include the names of all contributors in the announcement blog post. Use the following command to get the list of contributors:
{code:java}
# first line is required to make sort first with uppercase and then lower
export LC_ALL=C
export FLINK_PREVIOUS_RELEASE_BRANCH=<previousReleaseBranch>
export FLINK_CURRENT_RELEASE_BRANCH=<currentReleaseBranch>
# e.g.
# export FLINK_PREVIOUS_RELEASE_BRANCH=release-1.16
# export FLINK_CURRENT_RELEASE_BRANCH=release-1.17
git log $(git merge-base master $FLINK_PREVIOUS_RELEASE_BRANCH)..$(git show-ref --hash ${FLINK_CURRENT_RELEASE_BRANCH}) --pretty=format:""%an%n%cn"" | sort  -u | paste -sd, | sed ""s/\,/\, /g""
{code}
h3. Social media

Tweet, post on Facebook, LinkedIn, and other platforms. Ask other contributors to do the same.
h3. Flink Release Wiki page

Add a summary of things that went well or that went not so well during the release process. This can include feedback from contributors but also more generic things like the release have taken longer than initially anticipated (and why) to give a bit of context to the release process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 24 03:57:49 UTC 2023,,,,,,,,,,"0|z1grr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 03:57;renqs;* 1.17.0 has been registered with release date 2023/03/23: [https://reporter.apache.org/addrelease.html?flink]
 * Blog is online: [https://flink.apache.org/2023/03/23/announcing-the-release-of-apache-flink-1.17/]
 * Twitter: [https://twitter.com/ApacheFlink/status/1638839542403981312]
 * A ""Retrospective"" section has been added to [1.17 release wiki page|https://cwiki.apache.org/confluence/display/FLINK/1.17+Release];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Apache mailing lists announcements,FLINK-31571,13529604,13529600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,mapohl,mapohl,22/Mar/23 11:16,23/Mar/23 10:05,04/Jun/24 20:41,23/Mar/23 10:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Announce on the {{dev@}} mailing list that the release has been finished.

Announce on the release on the {{user@}} mailing list, listing major improvements and contributions.

Announce the release on the [announce@apache.org|mailto:announce@apache.org] mailing list.
{panel}
{panel}
|{{From: Release Manager}}
{{To: dev@flink.apache.org, user@flink.apache.org, user-zh@flink.apache.org, announce@apache.org}}
{{Subject: [ANNOUNCE] Apache Flink 1.2.3 released}}
 
{{The Apache Flink community is very happy to announce the release of Apache Flink 1.2.3, which is the third bugfix release for the Apache Flink 1.2 series.}}
 
{{Apache Flink® is an open-source stream processing framework for distributed, high-performing, always-available, and accurate data streaming applications.}}
 
{{The release is available for download at:}}
{{[https://flink.apache.org/downloads.html]}}
 
{{Please check out the release blog post for an overview of the improvements for this bugfix release:}}
{{<blob post link>}}
 
{{The full release notes are available in Jira:}}
{{<jira release notes link>}}
 
{{We would like to thank all contributors of the Apache Flink community who made this release possible!}}
 
{{Feel free to reach out to the release managers (or respond to this thread) with feedback on the release process. Our goal is to constantly improve the release process. Feedback on what could be improved or things that didn't go so well are appreciated.}}
 
{{Regards,}}
{{Release Manager}}|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 10:04:51 UTC 2023,,,,,,,,,,"0|z1grqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 10:04;renqs;Announcement in the mailing list: [https://lists.apache.org/thread/72nmfwsgs7sqkw7mykz4h36hgb7wo04d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Remove outdated versions,FLINK-31570,13529603,13529600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,22/Mar/23 11:15,21/Aug/23 08:15,04/Jun/24 20:41,21/Aug/23 08:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"h4. dist.apache.org

For a new major release remove all release files older than 2 versions, e.g., when releasing 1.7, remove all releases <= 1.5.

For a new bugfix version remove all release files for previous bugfix releases in the same series, e.g., when releasing 1.7.1, remove the 1.7.0 release.
# If you have not already, check out the Flink section of the {{release}} repository on {{[dist.apache.org|http://dist.apache.org/]}} via Subversion. In a fresh directory:
{code}
svn checkout https://dist.apache.org/repos/dist/release/flink --depth=immediates
cd flink
{code}
# Remove files for outdated releases and commit the changes.
{code}
svn remove flink-<version_to_remove>
svn commit
{code}
# Verify that files  are [removed|https://dist.apache.org/repos/dist/release/flink]
(!) Remember to remove the corresponding download links from the website.

h4. CI

Disable the cron job for the now-unsupported version from (tools/azure-pipelines/[build-apache-repo.yml|https://github.com/apache/flink/blob/master/tools/azure-pipelines/build-apache-repo.yml]) in the respective branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 08:15:05 UTC 2023,,,,,,,,,,"0|z1grqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 03:35;renqs;I'm wondering if we should remove 1.15 from svn and CI now, as there will be a final 1.15.5 [~mapohl] ;;;","24/Mar/23 08:14;mapohl;Yeah, we might create a ML post first starting the discussion on releasing 1.15.5. I'm pretty sure that there wouldn't be much interest since we just recently released 1.15.4. ;;;","03/Apr/23 06:40;mapohl;What's the state of this issue? Looks like we can close this one, can't we? Finalizing this one will help reducing the spam in CI with the 1.15 branch: We didn't backport FLINK-27640 to 1.15 but it seems to be a constant issue there now.;;;","11/May/23 10:53;mapohl;[~renqs] this issue is still hanging around? Was it already fixed?;;;","11/Aug/23 10:13;mapohl;Review of the changes:
(/) Cron job for 1.15 is not executed anymore (the Jira issue description is a bit misleading here because that is automatically done as soon as the new release branch is created because how the repo sync script works; FLINK-30965)
(/) [Flink's download site|https://flink.apache.org/downloads/] doesn't list 1.15 anymore
(x) The sources for Flink 1.15.4 are still around in https://dist.apache.org/repos/dist/release/flink/

I'm gonna go ahead and remove the 1.15.4 sources;;;","11/Aug/23 10:16;mapohl;Update: [Discussion on not releasing 1.15.5|https://lists.apache.org/thread/csm81mlcktobl00p37x9t2jkpwmj9m58];;;","11/Aug/23 10:18;mapohl;[~renqs] can you do a final review and close the issue (since I needed to do a final change).;;;","21/Aug/23 08:15;renqs;Verified that 1.15.x distributions have been removed from SVN. Thanks for the work! [~mapohl] ;;;",,,,,,,,,,,,,,,,,,,
Merge website pull request,FLINK-31569,13529602,13529600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,leonard,mapohl,mapohl,22/Mar/23 11:14,23/Mar/23 10:00,04/Jun/24 20:41,23/Mar/23 10:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Merge the website pull request to [list the release|http://flink.apache.org/downloads.html]. Make sure to regenerate the website as well, as it isn't build automatically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 10:00:40 UTC 2023,,,,,,,,,,"0|z1grqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 10:00;renqs;The website is on and correctly shows 1.17.0 informations:

Download page: [https://flink.apache.org/downloads/]

Announcement: [https://flink.apache.org/2023/03/23/announcing-the-release-of-apache-flink-1.17/]

Release note: [https://nightlies.apache.org/flink/flink-docs-release-1.17/release-notes/flink-1.17/];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update japicmp configuration,FLINK-31568,13529601,13529600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,22/Mar/23 11:14,24/Mar/23 08:57,04/Jun/24 20:41,24/Mar/23 08:57,,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,,,,,,,0,pull-request-available,,,,,"Update the japicmp reference version and wipe exclusions / enable API compatibility checks for {{@PublicEvolving}} APIs on the corresponding SNAPSHOT branch with the {{update_japicmp_configuration.sh}} script (see below).

For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.

Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.
{code:bash}
tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.sh
tools $ cd ..$ git add *$ git commit -m ""Update japicmp configuration for $RELEASE_VERSION"" {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 24 08:57:21 UTC 2023,,,,,,,,,,"0|z1grq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 08:57;mapohl;* master: b8bc610b31aea5e6fa160b17c7154fc269d52b4b
* 1.17: 76b44c9bf8cc24b8db2be20fd908350ac77ef8b3
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Promote release 1.17,FLINK-31567,13529600,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,,mapohl,mapohl,22/Mar/23 11:13,22/Aug/23 11:45,04/Jun/24 20:41,21/Aug/23 08:15,,,,,,,,,,,,,,,,,,,,,1.17.0,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,"Once the release has been finalized (FLINK-31562), the last step of the process is to promote the release within the project and beyond. Please wait for 24h after finalizing the release in accordance with the [ASF release policy|http://www.apache.org/legal/release-policy.html#release-announcements].

*Final checklist to declare this issue resolved:*
 # Website pull request to [list the release|http://flink.apache.org/downloads.html] merged
 # Release announced on the user@ mailing list.
 # Blog post published, if applicable.
 # Release recorded in [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink].
 # Release announced on social media.
 # Completion declared on the dev@ mailing list.
 # Update Homebrew: [https://docs.brew.sh/How-To-Open-a-Homebrew-Pull-Request] (seems to be done automatically - at least for minor releases  for both minor and major releases)
 # Update quickstart scripts in {{{}flink-web{}}}, under the {{q/}} directory
 # Updated the japicmp configuration
 ** corresponding SNAPSHOT branch japicmp reference version set to the just released version, and API compatibiltity checks for {{@PublicEvolving}}  was enabled
 ** (minor version release only) master branch japicmp reference version set to the just released version
 ** (minor version release only) master branch japicmp exclusions have been cleared
 # Update the list of previous version in {{docs/config.toml}} on the master branch.
 # Set {{show_outdated_warning: true}} in {{docs/config.toml}} in the branch of the _now deprecated_ Flink version (i.e. 1.15 if 1.17.0 is released)
 # Update stable and master alias in [https://github.com/apache/flink/blob/master/.github/workflows/docs.yml]",,,,,,,,,,FLINK-31562,,,,,,,,,,,,,,,,,,,,,,,FLINK-32912,,,,,,,,,,,,,,FLINK-27518,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:08 UTC 2023,,,,,,,,,,"0|z1grq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 11:45;mapohl;FLINK-27518 will make the test data generation for the migration tests easier.;;;","24/Mar/23 13:53;mapohl;# (/) Website pull request to [list the release|http://flink.apache.org/downloads.html] merged
 # (/) Release announced on the user@ mailing list: [announcement link|https://lists.apache.org/thread/72nmfwsgs7sqkw7mykz4h36hgb7wo04d]
 # (/) Blog post published, if applicable: [blog post|https://flink.apache.org/2023/03/23/announcing-the-release-of-apache-flink-1.17/]
 # (/) Release recorded in [reporter.apache.org: https://reporter.apache.org/addrelease.html?flink|https://reporter.apache.org/addrelease.html?flink]
 # (/) Release announced on social media: [Twitter|https://twitter.com/ApacheFlink/status/1638839542403981312?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1638839542403981312%7Ctwgr%5E7f3046f67668cf3ebbd929ef126a32473db2a1b5%7Ctwcon%5Es1_c10&ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2FApacheFlink2Fstatus2F1638839542403981312widget%3DTweet]
 # (?) Completion declared on the dev@ mailing list.
 # (/) Update Homebrew: [https://docs.brew.sh/How-To-Open-a-Homebrew-Pull-Request] (seems to be done automatically - at least for minor releases  for both minor and major releases): [https://formulae.brew.sh/formula/apache-flink#default]
 # (/) Update quickstart scripts in {{{}flink-web{}}}, under the {{q/}} directory: [422fd855|https://github.com/apache/flink-web/commit/422fd855]
 # (/) Updated the japicmp configuration: Done in FLINK-31568
 # (/) Update the list of previous version in {{docs/config.toml}} on the master branch: Done in [a5e05b7|https://github.com/apache/flink/commit/a5e05b713e237e91ca2128a7a517b93083178370]
 # (/) Set {{show_outdated_warning: true}} in {{docs/config.toml}} in the branch of the _previous_ Flink version: [PR #22270|https://github.com/apache/flink/pull/22270] (for 1.15)
 # (/) Update stable and master alias in [https://github.com/apache/flink/blob/master/.github/workflows/docs.yml] Done in [a9fa484a|https://github.com/apache/flink/commit/a9fa484a];;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,
Publish the Dockerfiles for the new release,FLINK-31566,13529598,13529593,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,22/Mar/23 11:00,27/Oct/23 11:41,04/Jun/24 20:41,24/Mar/23 07:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"Note: the official Dockerfiles fetch the binary distribution of the target Flink version from an Apache mirror. After publishing the binary release artifacts, mirrors can take some hours to start serving the new artifacts, so you may want to wait to do this step until you are ready to continue with the ""Promote the release"" steps in the follow-up Jira.

Follow the [release instructions in the flink-docker repo|https://github.com/apache/flink-docker#release-workflow] to build the new Dockerfiles and send an updated manifest to Docker Hub so the new images are built and published.

 
----
h3. Expectations
 * Dockerfiles in [flink-docker|https://github.com/apache/flink-docker] updated for the new Flink release and pull request opened on the Docker official-images with an updated manifest",,,,,,,,,,FLINK-31564,FLINK-31563,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 24 07:02:10 UTC 2023,,,,,,,,,,"0|z1grpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 19:20;rmetzger;Opened a PR with the official docker library: https://github.com/docker-library/official-images/pull/14324

Asked the Flink PMC to help with releasing from the apache/flink dockerHub repo (I hope to soon have access again);;;","24/Mar/23 02:51;renqs;Image is online now: [https://hub.docker.com/layers/apache/flink/1.17.0-java8/images/sha256-319e7293b5e31f3e2fdb62a48be4b0118d7b0b6b78ec1042c385ce5483aa12b7?context=explore] (thanks [~xtsong] !);;;","24/Mar/23 07:02;rmetzger;Also the official image is online https://hub.docker.com/_/flink/tags;;;",,,,,,,,,,,,,,,,,,,,,,,,
Create Git tag and mark version as released in Jira,FLINK-31565,13529596,13529593,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,22/Mar/23 10:58,23/Mar/23 09:37,04/Jun/24 20:41,23/Mar/23 09:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Create and push a new Git tag for the released version by copying the tag for the final release candidate, as follows:
{code:java}
$ git tag -s ""release-${RELEASE_VERSION}"" refs/tags/${TAG}^{} -m ""Release Flink ${RELEASE_VERSION}""
$ git push <remote> refs/tags/release-${RELEASE_VERSION}
{code}
In JIRA, inside [version management|https://issues.apache.org/jira/plugins/servlet/project-config/FLINK/versions], hover over the current release and a settings menu will appear. Click Release, and select today’s date.

(Note: Only PMC members have access to the project administration. If you do not have access, ask on the mailing list for assistance.)

If PRs have been merged to the release branch after the the last release candidate was tagged, make sure that the corresponding Jira tickets have the correct Fix Version set.

 
----
h3. Expectations
 * Release tagged in the source code repository
 * Release version finalized in JIRA. (Note: Not all committers have administrator access to JIRA. If you end up getting permissions errors ask on the mailing list for assistance)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:36:57 UTC 2023,,,,,,,,,,"0|z1grp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:36;renqs;* Release tag: [https://github.com/apache/flink/releases/tag/release-1.17.0]
 * 1.17.0 has been mark as released on JIRA: [https://issues.apache.org/jira/projects/FLINK/versions/12351585];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy artifacts to Maven Central Repository,FLINK-31564,13529595,13529593,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,22/Mar/23 10:55,23/Mar/23 10:09,04/Jun/24 20:41,23/Mar/23 10:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Use the [Apache Nexus repository|https://repository.apache.org/] to release the staged binary artifacts to the Maven Central repository. In the Staging Repositories section, find the relevant release candidate orgapacheflink-XXX entry and click Release. Drop all other release candidates that are not being released.
h3. Deploy source and binary releases to dist.apache.org

Copy the source and binary releases from the dev repository to the release repository at [dist.apache.org|http://dist.apache.org/] using Subversion.
{code:java}
$ svn move -m ""Release Flink ${RELEASE_VERSION}"" https://dist.apache.org/repos/dist/dev/flink/flink-${RELEASE_VERSION}-rc${RC_NUM} https://dist.apache.org/repos/dist/release/flink/flink-${RELEASE_VERSION}
{code}
(Note: Only PMC members have access to the release repository. If you do not have access, ask on the mailing list for assistance.)
h3. Remove old release candidates from [dist.apache.org|http://dist.apache.org/]

Remove the old release candidates from [https://dist.apache.org/repos/dist/dev/flink] using Subversion.
{code:java}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
$ cd flink
$ svn remove flink-${RELEASE_VERSION}-rc*
$ svn commit -m ""Remove old release candidates for Apache Flink ${RELEASE_VERSION}
{code}
 
----
h3. Expectations
 * Maven artifacts released and indexed in the [Maven Central Repository|https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.flink%22] (usually takes about a day to show up)
 * Source & binary distributions available in the release repository of [https://dist.apache.org/repos/dist/release/flink/]
 * Dev repository [https://dist.apache.org/repos/dist/dev/flink/] is empty
 * Website contains links to new release binaries and sources in download page
 * (for minor version updates) the front page references the correct new major release version and directs to the correct link",,,,,,,,,,,,,,,,,,,,,FLINK-31566,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 10:08:58 UTC 2023,,,,,,,,,,"0|z1grow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:33;renqs;Artifacts are available on Apache Nexus Repository. Here's an example of {{{}flink-annotations{}}}:

[https://repository.apache.org/content/groups/public/org/apache/flink/flink-annotations/1.17.0/]

It'll take some time for them to be shown on Maven Central. Will close the issue after those artifacts are visible. ;;;","23/Mar/23 09:35;renqs;* Source & binary are available at [https://dist.apache.org/repos/dist/release/flink/flink-1.17.0/]
 * Dev repository is clear: [https://dist.apache.org/repos/dist/dev/flink/]
 * Website contains links to new release binaries and sources in download page: [https://flink.apache.org/downloads/]

 ;;;","23/Mar/23 10:08;renqs;Maven artifacts are visible on Maven Central. Example of {{{}flink-core{}}}:

[https://central.sonatype.com/artifact/org.apache.flink/flink-core/1.17.0];;;",,,,,,,,,,,,,,,,,,,,,,,,
Deploy Python artifacts to PyPI,FLINK-31563,13529594,13529593,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,22/Mar/23 10:52,23/Mar/23 09:29,04/Jun/24 20:41,23/Mar/23 09:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Release manager should create a PyPI account and ask the PMC add this account to pyflink collaborator list with Maintainer role (The PyPI admin account info can be found here. NOTE, only visible to PMC members) to deploy the Python artifacts to PyPI. The artifacts could be uploaded using twine([https://pypi.org/project/twine/]). To install twine, just run:
{code:java}
pip install --upgrade twine==1.12.0
{code}
Download the python artifacts from dist.apache.org and upload it to pypi.org:
{code:java}
svn checkout https://dist.apache.org/repos/dist/dev/flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
cd flink-${RELEASE_VERSION}-rc${RC_NUM}
 
cd python
 
#uploads wheels
for f in *.whl; do twine upload --repository-url https://upload.pypi.org/legacy/ $f $f.asc; done
 
#upload source packages
twine upload --repository-url https://upload.pypi.org/legacy/ apache-flink-libraries-${RELEASE_VERSION}.tar.gz apache-flink-libraries-${RELEASE_VERSION}.tar.gz.asc
 
twine upload --repository-url https://upload.pypi.org/legacy/ apache-flink-${RELEASE_VERSION}.tar.gz apache-flink-${RELEASE_VERSION}.tar.gz.asc
{code}
If upload failed or incorrect for some reason (e.g. network transmission problem), you need to delete the uploaded release package of the same version (if exists) and rename the artifact to {{{}apache-flink-${RELEASE_VERSION}.post0.tar.gz{}}}, then re-upload.

(!) Note: re-uploading to pypi.org must be avoided as much as possible because it will cause some irreparable problems. If that happens, users cannot install the apache-flink package by explicitly specifying the package version, i.e. the following command ""pip install apache-flink==${RELEASE_VERSION}"" will fail. Instead they have to run ""pip install apache-flink"" or ""pip install apache-flink==${RELEASE_VERSION}.post0"" to install the apache-flink package.

 
----
h3. Expectations
 * Python artifacts released and indexed in the [PyPI|https://pypi.org/project/apache-flink/] Repository",,,,,,,,,,,,,,,,,,,,,FLINK-31566,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 23 09:29:48 UTC 2023,,,,,,,,,,"0|z1groo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:29;renqs;Packages are available on PyPI:

[https://pypi.org/project/apache-flink/1.17.0/]

[https://pypi.org/project/apache-flink-libraries/1.17.0/];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Finalize release 1.17.0,FLINK-31562,13529593,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,mapohl,mapohl,22/Mar/23 10:49,14/Dec/23 15:31,04/Jun/24 20:41,24/Mar/23 08:10,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Once the release candidate has been reviewed and approved by the community, the release should be finalized. This involves the final deployment of the release candidate to the release repositories, merging of the website changes, etc.",,,,,,,,,,FLINK-31154,FLINK-31578,FLINK-31583,,,,,,,,,FLINK-31567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 22 11:09:22 UTC 2023,,,,,,,,,,"0|z1grog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 11:09;mapohl;We should finalize the RC creation process before going ahead with finalizing the release. Current RC version is RC3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-master-regression-check is failing since March 15, 2023",FLINK-31561,13529585,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,mapohl,mapohl,22/Mar/23 10:18,27/Mar/23 11:29,04/Jun/24 20:41,23/Mar/23 13:17,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Benchmarks,,,,,,0,pull-request-available,,,,,"[flink-master-benchmarks-regression-check|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/] is failing since March 15, 2023:
{code}
Started by timer
hudson.plugins.git.GitException: Command ""git fetch --tags --progress --prune origin +refs/heads/cutoff:refs/remotes/origin/cutoff"" returned status code 128:
stdout: 
stderr: fatal: Couldn't find remote ref refs/heads/cutoff

	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:2372)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:1985)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$500(CliGitAPIImpl.java:80)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$1.execute(CliGitAPIImpl.java:563)
	at jenkins.plugins.git.GitSCMFileSystem$BuilderImpl.build(GitSCMFileSystem.java:348)
	at jenkins.scm.api.SCMFileSystem.of(SCMFileSystem.java:197)
	at jenkins.scm.api.SCMFileSystem.of(SCMFileSystem.java:173)
	at org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition.create(CpsScmFlowDefinition.java:115)
	at org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition.create(CpsScmFlowDefinition.java:69)
	at org.jenkinsci.plugins.workflow.job.WorkflowRun.run(WorkflowRun.java:299)
	at hudson.model.ResourceController.execute(ResourceController.java:97)
	at hudson.model.Executor.run(Executor.java:429)
Finished: FAILURE
{code}

As a consequence, no regressions are reported in Slack",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29825,,,,,,,,,,,,,,,,,"23/Mar/23 08:40;Yanfei Lei;image-2023-03-23-16-40-05-880.png;https://issues.apache.org/jira/secure/attachment/13056607/image-2023-03-23-16-40-05-880.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 27 11:29:00 UTC 2023,,,,,,,,,,"0|z1grmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 10:23;mapohl;[~Yanfei Lei] can you help out with this issue. I'm still a bit confused on why we're expecting a {{cutoff}} branch here? I couldn't find anything related to it in the {{flink-benchmarks}} code repository.;;;","22/Mar/23 10:31;Yanfei Lei;I updated the [regression check script|https://issues.apache.org/jira/browse/FLINK-29825] but forgot to update the Jenkins configuration. Fixed it now, let's wait for the result of next run.;;;","22/Mar/23 11:31;mapohl;Cool, thanks for the clarification (y) I'm a bit puzzled why we see the test failure starting on March 15 whereas the commit ended up on master on March 10 already.;;;","22/Mar/23 13:05;mapohl;The initial issue seems to be fixed but the builds are still failing:
{code:java}
/home/jenkins/workspace/flink-master-benchmarks-regression-check/flink-benchmarks@tmp/durable-9c5531ba/script.sh: 1: ./regression_report_v2.py: Permission denied {code};;;","23/Mar/23 07:32;Yanfei Lei;[~mapohl] I add execute permission for regression check script, could you please help take a look?

> The test failure starting on March 15 whereas the commit ended up on master on March 10 already.

 

Because the Jenkins was configured to FLINK-30870's branch, I updated the Jenkins configuration to the master branch on March 15, the new detection algorithm has not detected any regression recently, so I didn't notice that the pipeline failed🤦‍♂️

 ;;;","23/Mar/23 08:29;mapohl;There's still a permission. It seem to have worked temporarily?
{quote}Because the Jenkins was configured to FLINK-30870's branch, I updated the Jenkins configuration to the master branch on March 15, the new detection algorithm has not detected any regression recently, so I didn't notice that the pipeline failed🤦‍♂️
{quote}
Ah, FLINK-30870 is where the term ""cutoff"" comes from. :D Just a hint: It's easier to for investigations to use Jira issues for branch names (or at least add the Jira issue along the descriptive name) to make it easier investigate things. :);;;","23/Mar/23 08:43;Yanfei Lei;> It seem to have worked temporarily?

I temporarily configured Jenkins pipeline and [jenkinsfile|https://github.com/apache/flink-benchmarks/blob/master/jenkinsfiles/regression-check.jenkinsfile#L23] to my repo for testing, [#1559|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1559/] and [#1560|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1559/] temporarily can work. Now Jenkins pipeline(like below screenshot) is reverted to the master branch, it should work after we merging this PR into master.

!image-2023-03-23-16-40-05-880.png|width=530,height=211!;;;","23/Mar/23 13:17;pnowojski;Hopefully fixed by commit d797583 into flink-benchmarks:master;;;","27/Mar/23 11:29;mapohl;FYI: I verified that the script is not failing anymore. (y);;;",,,,,,,,,,,,,,,,,,
Savepoint failing to complete with ExternallyInducedSources,FLINK-31560,13529579,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yanfei Lei,fyang86,fyang86,22/Mar/23 09:34,04/May/23 13:00,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,,,"Flink version: 1.16.0

 

We are using Flink to run some streaming applications with Pravega as source and use window and reduce transformations. We use RocksDB state backend with incremental checkpointing enabled. We don't enable the latest changelog state backend.

When we try to stop the job, we encounter issues with the savepoint failing to complete for the job. This happens most of the time. On rare occasions, the job gets canceled suddenly with its savepoint get completed successfully.

Savepointing shows below error:
 
{code:java}
2023-03-22 08:55:57,521 [jobmanager-io-thread-1] WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 189 for job 7354442cd6f7c121249360680c04284d. (0 consecutive failed attempts so far)org.apache.flink.runtime.checkpoint.CheckpointException: Failure to finalize checkpoint.    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.finalizeCheckpoint(CheckpointCoordinator.java:1375) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1265) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1157) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist-1.16.0.jar:1.16.0]    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]    at java.lang.Thread.run(Thread.java:829) [?:?]

Caused by: java.io.IOException: Unknown implementation of StreamStateHandle: class org.apache.flink.runtime.state.PlaceholderStreamStateHandle    at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.serializeStreamStateHandle(MetadataV2V3SerializerBase.java:699) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.serializeStreamStateHandleMap(MetadataV2V3SerializerBase.java:813) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.serializeKeyedStateHandle(MetadataV2V3SerializerBase.java:344) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.serializeKeyedStateCol(MetadataV2V3SerializerBase.java:269) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.serializeSubtaskState(MetadataV2V3SerializerBase.java:262) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.serializeSubtaskState(MetadataV3Serializer.java:142) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.serializeOperatorState(MetadataV3Serializer.java:122) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.serializeMetadata(MetadataV2V3SerializerBase.java:146) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.serialize(MetadataV3Serializer.java:83) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.metadata.MetadataV4Serializer.serialize(MetadataV4Serializer.java:56) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:100) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:87) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:82) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.PendingCheckpoint.finalizeCheckpoint(PendingCheckpoint.java:333) ~[flink-dist-1.16.0.jar:1.16.0]    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.finalizeCheckpoint(CheckpointCoordinator.java:1361) ~[flink-dist-1.16.0.jar:1.16.0]    ... 7 more {code}
 

Prior to Flink 1.16, we did not observe this error. Since `PlaceholderStreamStateHandle` is used to indicate it's a reusable RocksDB data for incremental checkpoint, we believe that the new improvements of incremental checkpoint introduced in flink 1.16 release might be related to this issue.

We require assistance in investigating this issue and finding a solution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25256,,,,,,,,,,,"23/Mar/23 10:03;Brian Zhou;image-2023-03-23-18-03-05-943.png;https://issues.apache.org/jira/secure/attachment/13056611/image-2023-03-23-18-03-05-943.png","23/Mar/23 10:19;Brian Zhou;image-2023-03-23-18-19-24-482.png;https://issues.apache.org/jira/secure/attachment/13056613/image-2023-03-23-18-19-24-482.png","23/Mar/23 10:23;Brian Zhou;jobmanager_log.txt;https://issues.apache.org/jira/secure/attachment/13056614/jobmanager_log.txt","27/Mar/23 05:03;fyang86;taskmanager_172.28.17.19_6123-f2dbff_log;https://issues.apache.org/jira/secure/attachment/13056813/taskmanager_172.28.17.19_6123-f2dbff_log","27/Mar/23 05:56;fyang86;tmp_tm_172.28.17.19_6123-f2dbff_tmp_job_83ad4f408d0e7bf30f940ddfa5fe00e3_op_WindowOperator_137df028a798f504a6900a4081c9990c__1_1__uuid_edc681f0-3825-45ce-a123-9ff69ce6d8f1_db_LOG;https://issues.apache.org/jira/secure/attachment/13056817/tmp_tm_172.28.17.19_6123-f2dbff_tmp_job_83ad4f408d0e7bf30f940ddfa5fe00e3_op_WindowOperator_137df028a798f504a6900a4081c9990c__1_1__uuid_edc681f0-3825-45ce-a123-9ff69ce6d8f1_db_LOG",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 04 13:00:35 UTC 2023,,,,,,,,,,"0|z1grlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 08:17;Yanfei Lei;[~fyang86] I suspect that there is a problem with the registration of `IncrementalRemoteKeyedStateHandle`. Normally, the placeHolder in the registered `IncrementalRemoteKeyedStateHandle` will be replaced by file handle.

 

BTW, the call stack looks like the path of checkpoint/native savepoint, {*}NOT canonical savepoint{*}. Could you please share the whole JM log or how to reproduce it?;;;","23/Mar/23 10:31;Brian Zhou;Thanks [~Yanfei Lei]  to look into this.

I am in the same team with Fan and I can help to share more information with that.

 

We are running Flink in standalone mode in Kubernetes, the state backend is connected to NFS.

Here is the configuration for checkpointing,

 
!image-2023-03-23-18-03-05-943.png!

and here is the status for checkpoints/savepoints

!image-2023-03-23-18-19-24-482.png!

We are using the REST API  [https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/#jobs-jobid-savepoints] to trigger savepoints and it keeps retrying every 75 seconds. I want to ask if by default it is canonical savepoint.

Here is the JM log: [^jobmanager_log.txt]Currently there are three jobs still running in this situation in this cluster.

For reproduction, it is company internal code for the job, so not able to share directly, but it is simply reading Pravega data, perform window aggregation and print out, nothing else specific.

 

 ;;;","26/Mar/23 09:44;Yanfei Lei;[~Brian Zhou] Thanks for sharing, I tried to reproduce it in my local standalone cluster(Flink 1.16.1), but I failed. Here are my settings, is it different from your job?
 * checkpoint interval: 60s
 * state backend: Rocksdb state backend
 * enable incremental checkpoint
 * job: examples/streaming/SocketWindowWordCount.jar
 * savepoint request:

 
{code:java}
curl -d ""@req.json"" -H ""Content-Type: application/json""  -X POST http://localhost:8081/jobs/xxxxx/savepoints 

req.json:
{
    ""cancel-job"" : true,
    ""formatType"" : ""CANONICAL"",
    ""target-directory"" : ""file:///Users/leiyanfei/flink-1.16.1/sp""
}{code}
 

> I want to ask if by default it is canonical savepoint.

The default format type of savepoint is canonical, the following log in JM.log also confirms that your savepoints are canonical. 

 
{code:java}
2023-03-23 06:11:58,125 [flink-akka.actor.default-dispatcher-21] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Triggering cancel-with-savepoint for job 7354442cd6f7c121249360680c04284d.
2023-03-23 06:11:58,149 [Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2444 (type=SavepointType{name='Savepoint', postCheckpointAction=NONE, formatType=CANONICAL}) @ 1679551918125 for job 7354442cd6f7c121249360680c04284d. {code}
 

Normally, the handle corresponding to savepoint is [KeyGroupsSavepointStateHandle|[https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/FullSnapshotAsyncWriter.java#L82]], and the placeholder handle should not exist, could you please share the TM log of rocksdb state backend? I want to know the type of stream handle reported by TM to JM?

 ;;;","27/Mar/23 05:56;fyang86;Here's the TM log of rocksdb: [^tmp_tm_172.28.17.19_6123-f2dbff_tmp_job_83ad4f408d0e7bf30f940ddfa5fe00e3_op_WindowOperator_137df028a798f504a6900a4081c9990c__1_1__uuid_edc681f0-3825-45ce-a123-9ff69ce6d8f1_db_LOG];;;","29/Mar/23 07:02;fyang86;After dicussion with [~Yanfei Lei] , we think this issue might be related to FLINK-25256 because our Pravega source is still using legacy ExternallyInducedSource interface. Further we would like to know why we encounter the issue most of the time but sometimes it didn't occur with the job's savepointing succeeded.;;;","30/Mar/23 11:05;Yanfei Lei;Although the checkpoint type of  is no longer hardcoded in [SourceOperatorStreamTask|https://github.com/apache/flink/pull/19138/files#diff-c15a9905db161a6cc3ff209ec2f15628a2f105bc47fe109e35599fd13d6722dfL165],  savepoint will still be transformed into checkpoint in some cases. 
 
[~Leonard] [~renqs] Could you please help take a look?;;;","25/Apr/23 04:29;Yanfei Lei;[~fyang86] After reading the [code|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java#L136] of the legacy source, I found that  FLINK-25256 only focuses on the interface of FLIP-27, under the current architecture, it is difficult to support the savepoint of the legacy source,  because we can't pass CheckpointOptions to Pravega connector.;;;","04/May/23 13:00;Brian Zhou;[~Yanfei Lei]  I think it's fair for the community to not fix a plan-to-deprecate API for now, this issue is only from the legacy source, and from Pravega side discussed with the team, as we are also transitioning into FLIP-27 source, we will also have a way out.;;;",,,,,,,,,,,,,,,,,,,
Update the flink version to 1.18-SNAPSHOT in flink-connector-kafka,FLINK-31559,13529572,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,ruanhang1993,ruanhang1993,ruanhang1993,22/Mar/23 08:38,11/Oct/23 18:37,04/Jun/24 20:41,11/Oct/23 18:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,stale-assigned,,,,"For FLIP-208, there are some changes in flink-connector-base which the later changes in  flink-connector-kafka depend on.

So we need to update the version to 1.18-SNAPSHOT.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 11 18:37:40 UTC 2023,,,,,,,,,,"0|z1grjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 08:42;ruanhang1993;[~martijnvisser] , WDYT?   Thanks~;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Oct/23 18:37;martijnvisser;This has already been resolved, as shown in this nightly build https://github.com/apache/flink-connector-kafka/actions/runs/6486214259;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support updating schema type with restrictions in CDC sink of Table Store,FLINK-31558,13529557,13528325,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,22/Mar/23 07:52,22/Mar/23 12:03,04/Jun/24 20:41,22/Mar/23 12:03,table-store-0.4.0,,,,,,,,,,,,,,,,,,,,table-store-0.4.0,,,,,,,,,Table Store,,,,,,0,pull-request-available,,,,,"It is common for database users to update column types to a wider type, such as changing {{INT}} types to {{BIGINT}}s, and {{CHAR}}s to {{VARCHAR}}s. We should support these common usages.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 22 12:03:03 UTC 2023,,,,,,,,,,"0|z1grgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 12:03;TsReaper;master: 4322595dfc1809199e3a815c7fa82ccb0750165a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Metric viewUpdater and reporter task in a SingleThreadScheduledExecutor lead to inaccurate PerSecond related metrics,FLINK-31557,13529531,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,huwh,LiuZeshan,LiuZeshan,22/Mar/23 03:06,30/Mar/23 13:57,04/Jun/24 20:41,30/Mar/23 13:56,,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Runtime / Metrics,,,,,,0,pull-request-available,,,,,"Currently, metric viewUpdater and reporterTask share the same SingleThreadScheduledExecutor, and customized reporters may have unpredictable logic, such as unreasonable network timeout settings, which can affect viewUpdater's calculation of PerSecond related metrics. For example, a real online problem we encountered, the network timeout of the reporter is set to 10 seconds, and the reporting interval is 15 seconds. When the server is unavailable, the thread is blocked for 10s, resulting in 66.7% (5/3x) higher PerSecond related metrics.

Is it possible to optimize here, such as whether it can be changed to a ScheduledThreadPool executor?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 30 13:56:55 UTC 2023,,,,,,,,,,"0|z1grao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 04:06;huwh;[~LiuZeshan] The MeterView does not record timestamp for each event. This reduces memory usage, but on the other hand, it relies on a just-in-time invoke to update the rate.

May be we could split a new SingleThreadScheduledExecutor for ViewUpdater to get ride of reporters. cc [~chesnay];;;","22/Mar/23 08:15;chesnay;??a new SingleThreadScheduledExecutor for ViewUpdater??

This makes sense to me.
Alternatively we could define a contract where the report() method must not block for a significant amount of time, and any reporter needing more time should instead run the operation asynchronously.;;;","22/Mar/23 08:47;huwh;[~chesnay] could you assign this to me? I will add a new SingleThreadScheduledExecutor for ViewUpdater, and add the contract to our documents;;;","30/Mar/23 13:56;chesnay;master: e5b553a12068511949b0415fc7e4c7b6930819ad
1.17: ae487b1ac16f710194a6b90bd61475a84919f0d1;;;",,,,,,,,,,,,,,,,,,,,,,,
"Translate ""rest"" page of ""sql-gateway"" into Chinese",FLINK-31556,13529527,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,zjureel,zjureel,zjureel,22/Mar/23 01:55,27/Mar/23 01:37,04/Jun/24 20:41,27/Mar/23 01:37,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,chinese-translation,Documentation,,,,,0,pull-request-available,,,,,rest.md of sql-gateway,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 27 01:37:05 UTC 2023,,,,,,,,,,"0|z1gr9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 01:37;fsk119;Merged into master: fc38c923f68740a21013daea6938cf772dc3e08a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""hiveserver2"" page of ""sql-gateway"" into Chinese",FLINK-31555,13529526,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,zjureel,zjureel,zjureel,22/Mar/23 01:54,24/Mar/23 02:47,04/Jun/24 20:41,24/Mar/23 02:47,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,chinese-translation,Documentation,,,,,0,pull-request-available,,,,,hiveserver2.md,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 24 02:47:01 UTC 2023,,,,,,,,,,"0|z1gr9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/23 02:47;fsk119;Merged into master: c9de75052ea5ae009d212c7ed30414dee0725266;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Overview"" page of ""sql-gateway"" into Chinese",FLINK-31554,13529525,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,zjureel,zjureel,zjureel,22/Mar/23 01:53,27/Mar/23 01:39,04/Jun/24 20:41,27/Mar/23 01:38,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,chinese-translation,Documentation,,,,,0,pull-request-available,,,,,overview.md of sql-gateway,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 27 01:38:48 UTC 2023,,,,,,,,,,"0|z1gr9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 01:38;fsk119;Merged into master: 6620641db32a95feca12c1bbebdde1bdc1ee9056;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Choose Catalog, non-url based",FLINK-31553,13529491,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,matriv,matriv,21/Mar/23 17:29,03/Apr/23 08:05,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,1,,,,,,"Currently. the {*}Catalog{*}{*}/Dialect{*} (etc.) is chosen automatically based on the URL provided. This takes place in {*}{color:#000000}JdbcDialectFactory{color}#load{*}, using the: {*}{color:#000000}JdbcDialectFactory{color}#acceptsURL{*}, so for a URL *jdbc:postgresql://...* the PostgresCatalog/Dialect is used. *CrateDB* used the same driver and URL but needs it's own Catalog/Dialect (etc.) as it has it's own stack of internal tables, type sytem, etc. (see https://issues.apache.org/jira/browse/FLINK-31551) So if a user wants to use *CrateDB,* currently, needs to use the legacy CrateDB driver which uses the jdbc:crate://... url.

Ideally, there should be another way, not only url based, that allows the user to use a given url, but choose the dialect manually. It could be some parameter on the catalog definition or even a special URL parameter like *?dialect=CrateDB* or so.

 

Other DBs that use implement the PostgreSQL wire protocol and are compatible with the PostgreSQL JDBC driver, but need their own Catalog/Dialect etc. would benefit from this. (e.g.: CockroachDB)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 03 08:05:17 UTC 2023,,,,,,,,,,"0|z1gr1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 15:35;matriv;[~libenchao]  [~eskabetxe] Please take a look at this one, and provide any suggestions.

I would be happy to work on it.;;;","28/Mar/23 08:25;eskabetxe;Hi [~matriv],
I'm not an expert on those databases, but being compatible with Postgres driver, shouldn't the current implementation work? Or we must use a specific driver for each database?

In case that we need to use an CrateDB driver (or CockroachDB driver) being the url with postgres, I think that the option to have an property that indicates the dialect should be fine but we should open a FLIP ([~martijnvisser] this is the correct way no?)

I would say that a new connection config like ""connector.dialect"" should fit, and on JdbcDialectFactory use this config or by default use the acceptUrl, some checks must be implemented to not break the current implementation (maybe define a default implementation or something like that, as multiple postgres acceptUrl could exist)

But this must be discussed, as there may be a potential compatibility gap. ;;;","29/Mar/23 08:26;matriv;Yep, I agree, we need to keep the URL based discovery for a Dialect -> Catalog -> TypeMapping, etc. not only for compatibility reasons, but also for ease of use,

since with `jdbc:postgresql` a user would expect to get the PGDialect by default. And on top add a mechanism like a config param that you've mentioned

to be able to use the same driver but different Dialect->Catalog->etc.

 ;;;","03/Apr/23 08:05;matriv;Imho, I don't think that this requires a FLIP, it's a simple change/addition.
I created a discussion thread in the list, instead.;;;",,,,,,,,,,,,,,,,,,,,,,,
Cannot use schema other than public with TableEnvironment,FLINK-31552,13529487,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,matriv,matriv,matriv,21/Mar/23 17:14,29/Mar/23 12:41,04/Jun/24 20:41,29/Mar/23 12:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,,,,,,"Cannot use a schema other than *public* in TableEnvironment+flink-jdbc-connector.

Postgres:

 
{noformat}
psql (15.1)
Type ""help"" for help.


matriv=> create schema myschema;
CREATE SCHEMA

matriv=> create table myschema.t1(a int);
CREATE TABLE

matriv=> insert into myschema.t1(a) values (1), (2);
INSERT 0 2

matriv=> select * from myschema.t1;
 a  
---
 1
 2
(2 rows)

{noformat}
{noformat}
EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();
TableEnvironment tableEnv = TableEnvironment.create(settings);
String name            = ""my_catalog"";
String defaultDatabase = ""matriv"";
String username        = ""matriv"";
String password        = ""matriv"";
String baseUrl         = ""jdbc:postgresql://localhost:5432"";

JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl);
tableEnv.registerCatalog(""my_catalog"", catalog);

// set the JdbcCatalog as the current catalog of the session
tableEnv.listTables();
tableEnv.executeSql(""select * from myschema.t1"").print();{noformat}
Exception:
{noformat}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 15 to line 1, column 25: Object 'myschema' not found
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:186)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:723)
    at io.crate.streaming.TaxiRidesStreamingJob.main(TaxiRidesStreamingJob.java:109)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 15 to line 1, column 25: Object 'myschema' not found
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 15 to line 1, column 25: Object 'myschema' not found    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4867)
    at org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:179)
    at org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3085)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
    at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
    at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:182)
    ... 5 more
{noformat}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 29 12:41:32 UTC 2023,,,,,,,,,,"0|z1gr0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 12:41;matriv;This is invalid, the <schema.table> part must be escaped, and then it works:
{noformat}
tableEnv.executeSql(""select * from `myschema.t1`"").print();{noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support CrateDB in JDBC Connector,FLINK-31551,13529468,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,matriv,matriv,matriv,21/Mar/23 15:21,06/Jun/23 12:47,04/Jun/24 20:41,06/Jun/23 12:46,,,,,,,,,,,,,,,,,,,,,jdbc-3.2.0,,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,"Currently PostgreSQL is supported, but PostgresCatalog along with all the relevant classes don't support CrateDB, so a new stack must be implemented.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 06 12:45:34 UTC 2023,,,,,,,,,,"0|z1gqwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 17:08;matriv;FYI: [~martijnvisser] ;;;","24/Mar/23 13:16;matriv;[~libenchao] could you please take a look?;;;","25/Mar/23 12:46;libenchao;[~matriv] Will review it in the next following days.;;;","26/Mar/23 22:02;matriv;Thank you [~libenchao] !;;;","03/Jun/23 13:43;Sergey Nuyanzin;CrateDB jdbc connector merged as [d67de5f0f94b1de912470028ea1fc74376bd64b8|https://github.com/apache/flink-connector-jdbc/commit/d67de5f0f94b1de912470028ea1fc74376bd64b8]

there is still an open subtask about translation to Chinese

;;;","06/Jun/23 11:54;libenchao;[~Sergey Nuyanzin] The subtask about translation has been done now.;;;","06/Jun/23 12:45;Sergey Nuyanzin;great, thanks for the update [~libenchao] and for the review
thanks for your work [~matriv] ;;;",,,,,,,,,,,,,,,,,,,,
Replace deprecated TableSchema with Schema in OperationConverterUtils,FLINK-31550,13529462,13478113,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,jark,jark,21/Mar/23 14:46,08/Apr/23 12:23,04/Jun/24 20:41,08/Apr/23 12:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 08 12:23:01 UTC 2023,,,,,,,,,,"0|z1gqvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 06:22;aitozi;Is this stale? {{OperationConverterUtils}} do not use {{TableSchema}} now.;;;","08/Apr/23 12:23;jark;I might checkout an outdated branch. Thank you for pointing out this. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for jdbc driver,FLINK-31549,13529456,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Mar/23 14:26,20/Jun/23 08:17,04/Jun/24 20:41,20/Jun/23 08:17,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,"1. How to use jdbc driver in java code
2. How to use jdbc driver in tools",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 20 08:17:09 UTC 2023,,,,,,,,,,"0|z1gqu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 08:17;libenchao;Fixed via https://github.com/apache/flink/commit/4006de973525c5284e9bc8fa6196ab7624189261

[~zjureel] Thanks for the PR!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FlinkDataSource for jdbc driver,FLINK-31548,13529455,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Mar/23 14:24,17/May/23 10:00,04/Jun/24 20:41,17/May/23 10:00,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 17 10:00:26 UTC 2023,,,,,,,,,,"0|z1gqts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/23 10:00;libenchao;Fixed via https://github.com/apache/flink/commit/420b5e319e1f8e917c099530360f51f73a070575

[~zjureel] Thanks for the PR!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FlinkResultSetMetaData for jdbc driver,FLINK-31547,13529454,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Mar/23 14:23,05/May/23 11:10,04/Jun/24 20:41,31/Mar/23 01:17,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 01:17:57 UTC 2023,,,,,,,,,,"0|z1gqtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 01:17;libenchao;Fixed via https://github.com/apache/flink/commit/b81291864e69899191a3ef01599a67f3b08b384b (1.18.0)

[~zjureel]Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Close all statements when connection is closed,FLINK-31546,13529453,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Mar/23 14:20,10/May/23 05:13,04/Jun/24 20:41,10/May/23 05:13,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,Close all statements when connection is closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 10 05:13:08 UTC 2023,,,,,,,,,,"0|z1gqtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 05:13;libenchao;Fixed via https://github.com/apache/flink/commit/400530d30af3d3d56805e9ab49397ca7778ba510

 [~zjureel] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkConnection creates and manages statements,FLINK-31545,13529452,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Mar/23 14:18,05/May/23 11:10,04/Jun/24 20:41,04/Apr/23 03:54,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,FlinkConnection creates Executor for Statement and manages statements,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 04 03:54:51 UTC 2023,,,,,,,,,,"0|z1gqt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 03:54;libenchao;Fixed in https://github.com/apache/flink/commit/8c44d58c4c4aeb36c91d8b27f4128891970dc47d 

[~zjureel] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FlinkDatabaseMetaData for jdbc driver,FLINK-31544,13529451,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Mar/23 14:16,05/May/23 11:09,04/Jun/24 20:41,05/May/23 11:09,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,Introduce `FlinkDatabaseMetaData`,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 05 11:09:01 UTC 2023,,,,,,,,,,"0|z1gqsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 11:09;libenchao;Implemented via https://github.com/apache/flink/commit/2ff4e220945680eba21065480385148ca8d034da

[~zjureel] Thanks for your PR!
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FlinkStatement for jdbc driver,FLINK-31543,13529450,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Mar/23 14:15,05/May/23 11:10,04/Jun/24 20:41,26/Apr/23 11:53,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,Introduce `FlinkStatement` for jdbc driver,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 11:53:03 UTC 2023,,,,,,,,,,"0|z1gqso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/23 11:53;libenchao;Fixed via https://github.com/apache/flink/commit/63166226f65cc4da80cc7661d76eb90032ec2222

[~zjureel] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
FatalExceptionClassifier#isFatal returns false if the exception is fatal,FLINK-31542,13529432,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,DavidLiu001,samuelsiebenmann,samuelsiebenmann,21/Mar/23 12:33,12/Aug/23 10:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Common,,,,,,0,pull-request-available,stale-assigned,,,,"FatalExceptionClassifier#isFatal returns `false` if the passed throwable is fatal and `true` if it is not:
{code:java}
public boolean isFatal(Throwable err, Consumer<Exception> throwableConsumer){   
    if (validator.test(err)) {    
        throwableConsumer.accept(throwableMapper.apply(err));   
        return false;    
    }
    if (chainedClassifier != null) {     
        return chainedClassifier.isFatal(err, throwableConsumer);    
    } else {        
        return true;    
    }
}
{code}
([github|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/throwable/FatalExceptionClassifier.java#L44])

However, the semantics of the method would indicate that it should return `true` if the passed throwable is fatal and `false` if it is not (i.e. the opposite of what is currently the case).

Additionally, the method name doesn't clearly indicate its side effects.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 12 10:35:05 UTC 2023,,,,,,,,,,"0|z1gqoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 03:31;DavidLiu001;Please assign this to me, I will fix it.;;;","22/Mar/23 13:59;DavidLiu001;[~danny.cranmer]  Would you please help review this PR?  ;;;","24/Mar/23 18:35;dannycranmer;[~DavidLiu001] thanks for picking this up, I will take a look on Monday.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,
Get metrics in Flink WEB UI error,FLINK-31541,13529430,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,tanjialiang,tanjialiang,21/Mar/23 12:32,29/Mar/23 08:54,04/Jun/24 20:41,29/Mar/23 08:54,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.1,1.18.0,,,,,,,,Runtime / Metrics,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,"When i get a metrics from a operator which name contains '[' or ']', it will be return 400 from rest response.

The reason is we can not submit an GET request which params contains '[' or ']', it is invaild in REST.

 

!image-2023-03-21-20-28-56-348.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/23 12:28;tanjialiang;image-2023-03-21-20-28-56-348.png;https://issues.apache.org/jira/secure/attachment/13056542/image-2023-03-21-20-28-56-348.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 28 09:51:08 UTC 2023,,,,,,,,,,"0|z1gqo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 14:30;huwh;[~tanjialiang] Thanks report this. It's caused by we put parameters in url directly, so http component not encode the special characters. 

I would like to take this ticket.;;;","28/Mar/23 09:51;Weijie Guo;master(1.18) via de27786eba56ce37c14f413eb8d7ba404eab50e1.

release-1.17 via 1e0b58aa8d962469fa9dd7b470037aeaece43500.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Support to set source idle-timeout options in sql layer,FLINK-31540,13529423,13529416,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,catyee,catyee,catyee,21/Mar/23 11:47,17/Jul/23 09:53,04/Jun/24 20:41,17/Jul/23 09:53,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,,,"Support to configure the source idle-timeout for the source which implements the `SupportsWatermarkPushDown` interface.

User can configure the source idle-timeout in flink sql job with table options or 'OPTIONS' hint, as disgussed in [https://cwiki.apache.org/confluence/display/FLINK/FLIP-296%3A+Extend+watermark-related+features+for+SQL]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 17 09:53:25 UTC 2023,,,,,,,,,,"0|z1gqmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 09:53;yunta;merged in master: 2aa9af15b418fb4308d089dad37e6b483d8a772a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support to configure watermark alignment in sql layer,FLINK-31539,13529421,13529416,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,catyee,catyee,catyee,21/Mar/23 11:43,17/Jul/23 09:53,04/Jun/24 20:41,17/Jul/23 09:53,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,,,"Support to configure the watermark alignment for the source which implements the `SupportsWatermarkPushDown` interface.

User can configure watermark alignment in flink sql job with table options or 'OPTIONS' hint, as disgussed

in [https://cwiki.apache.org/confluence/display/FLINK/FLIP-296%3A+Extend+watermark-related+features+for+SQL]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 17 09:53:10 UTC 2023,,,,,,,,,,"0|z1gqm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 09:53;yunta;merged in master: 8f64775e577bfe2dd11236d795d32dabd3b0a970;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Supports parse catalog/database and properties for uri,FLINK-31538,13529420,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,21/Mar/23 11:41,05/May/23 11:10,04/Jun/24 20:41,28/Mar/23 10:01,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,Supports parse catalog/database and properties for uri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 28 10:01:20 UTC 2023,,,,,,,,,,"0|z1gqm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 10:01;libenchao;Fixed via [https://github.com/apache/flink/commit/f14a02ecac382042d1f92c835df674cb2dc82d7b] (1.18.0)

[~zjureel] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Derive a new TaskManagerTrackerConfiguration from the SlotManagerConfiguration.,FLINK-31537,13529419,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,huwh,huwh,21/Mar/23 11:37,11/May/23 12:49,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,"As discussion in https://github.com/apache/flink/pull/22196#discussion_r1141847905
We need derive a new TaskManagerTrackerConfiguration from the SlotManagerConfiguration after DeclarativeSlotManager removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-21 11:37:29.0,,,,,,,,,,"0|z1gqls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to set watermark emit strategy in sql layer,FLINK-31536,13529418,13529416,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,catyee,catyee,catyee,21/Mar/23 11:36,17/Jul/23 09:52,04/Jun/24 20:41,17/Jul/23 09:52,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,,,"Support to configure the watermark emit strategy for the source which implements the `SupportsWatermarkPushDown` interface. 

User could configure the watermark emit strategy in flink sql job with table options or 'OPTIONS' hint，as discussed in  [https://cwiki.apache.org/confluence/display/FLINK/FLIP-296%3A+Extend+watermark-related+features+for+SQL]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 17 09:52:44 UTC 2023,,,,,,,,,,"0|z1gqlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/23 09:52;yunta;merged in master: a3471eab7595f5fca35c38d09b29a5689808f339;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Extend watermark-related features for SQL,FLINK-31535,13529416,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,catyee,catyee,catyee,21/Mar/23 11:28,17/Jul/23 11:38,04/Jun/24 20:41,17/Jul/23 10:10,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Runtime,,,,,,0,pull-request-available,,,,,"This issue aims to extend watermark-related features which are discussed in https://cwiki.apache.org/confluence/display/FLINK/FLIP-296%3A+Extend+watermark-related+features+for+SQL

Features to extends:
 * Configurable watermark emit strategy
 * Dealing with idle sources
 * Watermark alignment",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-21 11:28:11.0,,,,,,,,,,"0|z1gql4:",9223372036854775807,Flink now enable user config watermark emit strategy/watermark alignment/watermark idle-timeout in flink sql job with dynamic table options and 'Options' hint.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE AS SELECT should support to define primary key,FLINK-31534,13529376,13436816,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,21/Mar/23 07:47,21/Mar/23 07:47,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-21 07:47:21.0,,,,,,,,,,"0|z1gqc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE AS SELECT should support to define partition,FLINK-31533,13529375,13436816,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,21/Mar/23 07:47,04/Jun/24 16:21,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 04 16:21:35 UTC 2024,,,,,,,,,,"0|z1gqc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 15:09;aitozi;[~luoyuxia] I think it's a useful feature, and I'd like to support this, can I take this ticket ?;;;","12/Apr/23 01:26;luoyuxia;[~aitozi] Thanks for voluntering,  but please remember with the syntax that proposed in

[FLIP-218|[https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=199541185]]
{code:java}
CREATE TABLE [ IF NOT EXISTS ] table_name 
[ WITH ( table_properties ) ]
[ AS query_expression ]  {code}
, we can't define partition. We need to extend the syntax which may need a FLIP. Also, please remember to consider FLINK-31534 while extending the syntax.;;;","12/Apr/23 09:09;aitozi;After some trying, I find it's seems easy to support this feature. It doesn't have to touch the syntax part. Because the CTAS reuse/(part of) the SqlCreateTable syntax. So the sql below can be parsed already
{code:java}
 CREATE TABLE MyCtasTable 
 PARTITIONED BY (`a`) 
 WITH (
  'connector' = 'filesystem',
  'format' = 'testcsv',
)  AS
 SELECT * FROM MyTable{code}
We can't define this  because it's manually banned in {{SqlCreateTableAs#validate}}

In my poc, it just need some minor change to support the partition by definition and do not have to touch the public api/syntax. Do you think we still needs a FLIP for this or not [~luoyuxia] ?;;;","12/Apr/23 09:38;luoyuxia;I mean the synax propsed in this FLIP donsen't include PARTITIONED defination:

 
{code:java}
CREATE TABLE [ IF NOT EXISTS ] table_name 
[ WITH ( table_properties ) ]
[ AS query_expression ]   {code}
We should define the syntax in the FLIP something like 
{code:java}
CREATE TABLE [ IF NOT EXISTS ] 
table_name 
PARTITIONED BY (xxx, xx) 
 [ WITH ( table_properties ) ]
 [ AS query_expression ]  {code}
 ;;;","04/Jun/24 16:21;spena;Hi [~luoyuxia] [~aitozi], I'd like to make some contribution to Flink to extend the CTAS statement to allow a custom schema definition (columns, partition & primary keys, watermarks, etc). I noticed this task and FLINK-31534 are meant for a subset of those changes. Are you still considering in working on this at some point? I see these comments were made a year ago, but I don't want to step on others people's work if there's some progress or interest on it. Are you ok if I take this task?

I'm going to write a FLIP with the proposal about the semantics for the schema definition, which is similar to Mysql CTAS. ;;;",,,,,,,,,,,,,,,,,,,,,,
Support StreamExecutionEnvironment.socketTextStream in Python DataStream API,FLINK-31532,13529364,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianfu,dianfu,21/Mar/23 07:05,21/Mar/23 07:05,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"Currently, StreamExecutionEnvironment.socketTextStream is still missing in Python DataStream API. It would be great to support it. It may be helpful to in special cases, e.g. testing, etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-21 07:05:39.0,,,,,,,,,,"0|z1gq9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test HiveCatalogHiveMetadataTest.testCreateTableWithConstraint failed on azure,FLINK-31531,13529351,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,leonard,leonard,21/Mar/23 04:50,22/Mar/23 08:02,04/Jun/24 20:41,22/Mar/23 06:10,1.18.0,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Connectors / Hive,,,,,,0,pull-request-available,,,,,"{noformat}
Mar 21 01:11:13 [ERROR] Failures: 
Mar 21 01:11:13 [ERROR]   HiveCatalogHiveMetadataTest.testCreateTableWithConstraints:295 
Mar 21 01:11:13 Expecting value to be true but was false
Mar 21 01:11:13 [INFO] 
Mar 21 01:11:13 [ERROR] Tests run: 370, Failures: 1, Errors: 0, Skipped: 0
{noformat}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47400&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 22 08:02:58 UTC 2023,,,,,,,,,,"0|z1gq6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 06:10;luoyuxia;master: e342f127e7e177e3d7be88c729ff440689614dd9;;;","22/Mar/23 08:02;mapohl;This build failure didn't contain the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47435&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=22877;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"For currently commonly used databases, MySQL and Postgres have implemented catalogs. Currently, catalogs are implemented based on Oracle",FLINK-31530,13529349,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,complone,complone,21/Mar/23 04:12,04/Mar/24 11:30,04/Jun/24 20:41,30/Mar/23 09:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"The use of OracleCatalog is similar to that of official PostgreSQL, you can refer to [https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/connectors/table/jdbc/#jdbc-catalog-for-postgresql].
Use Cases:

 
{code:java}
CREATE CATALOG ORACLE_CATALOG WITH(
'type' = 'jdbc',
'default-database' = 'orclgps', -- specify the database instance name
'username' = '****', 'password' = '****',
'base-url' = 'jdbc:oracle:thin:@//ip:port' -- do not add the database instance name
);
SELECT `LINE_NO`, `UP_PASSENGER`, `DOWN_PASSENGER`, `SITE_TIME`, `STATION_NAME`
-- Pay attention to the format of the full table name query data
FROM ORACLE_CATALOG.orclgps.`TMGPS.TM_BUS_PASSENGER_UPDOWN` LIMIT 1; The use of OracleCatalog is similar to that of official PostgreSQL, you can refer to https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/connectors/ table/jdbc/#jdbc-catalog-for-postgresql.
Use Cases:
CREATE CATALOG ORACLE_CATALOG WITH(
'type' = 'jdbc',
'default-database' = 'orclgps', -- specify the database instance name
'username' = '****', 'password' = '****',
'base-url' = 'jdbc:oracle:thin:@//ip:port' -- do not add the database instance name
);
SELECT `LINE_NO`, `UP_PASSENGER`, `DOWN_PASSENGER`, `SITE_TIME`, `STATION_NAME`
-- Pay attention to the format of the full table name query data
FROM ORACLE_CATALOG.orclgps.`TMGPS.TM_BUS_PASSENGER_UPDOWN` LIMIT 1; {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29672,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 03:03:11 UTC 2023,,,,,,,,,,"0|z1gq68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 16:20;jark;What is this issue intending to do? [~complone];;;","06/Apr/23 03:02;complone;[~jark] Hello, when we implement the CDAS whole database synchronization syntax based on Flink, we need to obtain the table information under a certain database based on the catalog of different databases, so we need the catalog implementation of different databases. The catalog of the existing database only supports Mysql and Postgresql. This change is to support Oracle Catalog;;;","06/Apr/23 07:18;martijnvisser;[~complone] Then it's a duplicate of FLINK-29672 - Feel free to create a PR on that ticket, but keep in mind that it needs to be in apache/flink-connector-jdbc and not the Flink repo itself;;;","11/Apr/23 09:35;complone;[~martijnvisser] Thank you for telling me that I am currently implementing a CDAS syntax similar to Alibaba Cloud for the company's data development platform, and the expansion of OracleCatalog is part of it. After my job ends this month, I will try to submit a PR to apache/flink-connector-jdbc and develop a new CDAS syntax for apache/flink.;;;","13/Apr/23 03:03;complone;The use of OracleCatalog is similar to that of official PostgreSQL, you can refer to https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/connectors/table/jdbc/#jdbc-catalog-for-postgresql.
Use Cases:

 
{code:java}
CREATE CATALOG ORACLE_CATALOG WITH(
'type' = 'jdbc',
'default-database' = 'orclgps', -- specify the database instance name
'username' = '****', 'password' = '****',
'base-url' = 'jdbc:oracle:thin:@//ip:port' -- do not add the database instance name
);
SELECT `LINE_NO`, `UP_PASSENGER`, `DOWN_PASSENGER`, `SITE_TIME`, `STATION_NAME`
-- Pay attention to the format of the full table name query data
FROM ORACLE_CATALOG.orclgps.`TMGPS.TM_BUS_PASSENGER_UPDOWN` LIMIT 1; The use of OracleCatalog is similar to that of official PostgreSQL, you can refer to https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/connectors/ table/jdbc/#jdbc-catalog-for-postgresql.
Use Cases:
CREATE CATALOG ORACLE_CATALOG WITH(
'type' = 'jdbc',
'default-database' = 'orclgps', -- specify the database instance name
'username' = '****', 'password' = '****',
'base-url' = 'jdbc:oracle:thin:@//ip:port' -- do not add the database instance name
);
SELECT `LINE_NO`, `UP_PASSENGER`, `DOWN_PASSENGER`, `SITE_TIME`, `STATION_NAME`
-- Pay attention to the format of the full table name query data
FROM ORACLE_CATALOG.orclgps.`TMGPS.TM_BUS_PASSENGER_UPDOWN` LIMIT 1; {code};;;",,,,,,,,,,,,,,,,,,,,,,
Let yarn client exit early before JobManager running,FLINK-31529,13529227,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,huwh,huwh,20/Mar/23 09:33,06/Apr/23 12:49,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment / YARN,,,,,,0,,,,,,"Currently the YarnClusterDescriptor always wait yarn application status to be RUNNING even if we use the detach mode. 

In batch mode, the queue resources is insufficient in most case. So the job manager may take a long time to wait resources. And client also keep waiting too. If flink client is killed(some other reason), the cluster will be shutdown too.

We need an option to let Flink client exit early. Use the detach option or introduce a new option are both OK.

Looking forward other suggestions


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 12:49:29 UTC 2023,,,,,,,,,,"0|z1gpf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 11:04;zlzhang0122;IMHO I agree with this, and we have implement an option to deal with this.;;;","20/Mar/23 11:05;hackergin;+1 for this. ;;;","20/Mar/23 11:35;bgeng777;Hi there, I want to share some thoughts/questions about this jira:
1. According to the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/cli/], the --detached option is used to notify if the client should wait the job to finish. I have seem some users' platform rely on this option to get the returned YARN app info to manage their flink jobs(e.g. whether the job is submitted successfully). Maybe introducing a new option is better than changing the behavior of the --detached option.
2. The description says ""In batch mode, the queue resources is insufficient in most case."" IIUC, the lack of resource should not be a normal case. One possible use case I can come up with is that to reduce costs, people may run flink batch jobs in night and utilize workflow frameworks like airflow to retry the submission. Is that the case? ;;;","20/Mar/23 12:05;huwh;Thanks all for reply.

If introducing a new option is acceptable, I would like to take this issue. 

In batch mode, the queue resources is insufficient in most case.
----
yes.  
In detail, we have two kinds of YARN clusters: Streaming and Batch. The Streaming cluster provide guaranteed resources to make all jobs long-running.
The Batch cluster does not guarantee that jobs could get resources Immediately, these application will queued in YARN scheduler. This strategy maximizes resource utilization. 
In addition, we do have a platform to schedule flink batch jobs. These jobs will run at hourly or daily levels. 

;;;","06/Apr/23 12:49;huwh;[~xtsong] Could you take a look at this;;;",,,,,,,,,,,,,,,,,,,,,,
Move SqlXXXCatalog conversion logic to SqlNodeConverter,FLINK-31528,13529203,13528594,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xzw0223,xzw0223,xzw0223,20/Mar/23 08:00,12/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,stale-assigned,,,,Introduce SqlNodeConverter for SqlCatalog-related conversion logic.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 12 10:35:05 UTC 2023,,,,,,,,,,"0|z1gp9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogRescalingITCase.test failed due to FileNotFoundException,FLINK-31527,13529202,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,20/Mar/23 07:56,22/Mar/23 17:00,04/Jun/24 20:41,22/Mar/23 17:00,1.17.0,,,,,,,,,,,,,,,,,,,,1.16.2,1.17.1,,,,,,,,Runtime / State Backends,,,,,,0,pull-request-available,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47369&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10238

{code}
Mar 20 01:31:54 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 15.209 s <<< FAILURE! - in org.apache.flink.test.state.ChangelogRescalingITCase
Mar 20 01:31:54 [ERROR] ChangelogRescalingITCase.test  Time elapsed: 8.492 s  <<< FAILURE!
Mar 20 01:31:54 org.opentest4j.AssertionFailedError: Graph is in globally terminal state (FAILED)
Mar 20 01:31:54 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:42)
Mar 20 01:31:54 	at org.junit.jupiter.api.Assertions.fail(Assertions.java:147)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.lambda$waitForAllTaskRunning$3(CommonTestUtils.java:213)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:150)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:208)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:181)
Mar 20 01:31:54 	at org.apache.flink.test.state.ChangelogRescalingITCase.test(ChangelogRescalingITCase.java:163)
Mar 20 01:31:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 20 01:31:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 20 01:31:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 20 01:31:54 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 20 01:31:54 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 20 01:31:54 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 20 01:31:54 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 20 01:31:54 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 22 17:00:24 UTC 2023,,,,,,,,,,"0|z1gp9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 07:57;mapohl;[~roman] May you verify whether that's a 1.17-specific issue justifying the blocker priority or whether we can downgrade it to Critical?;;;","20/Mar/23 13:03;roman;I'd suppose it's not 1.17-specific, I'll try to verify that.;;;","21/Mar/23 14:43;mapohl;Any updates?;;;","21/Mar/23 21:17;roman;The issue is indeed not 1.17-specific:
{code:java}
ChangelogRescalingITCase failed on 1.17 with FileNotFoundException after restore attempt from chk-73.
At the same time, the log contained a message that chk-74 was not deleted. Meaning chk-74, not chk-73 was the latest checkpoint.
This is caused by not properly waiting for job termination in ChangelogRescalingITCase.checkpointAndCancel.{code}
I've opened a PR to fix it and going to lower the priority since it's a pure-test-stability issue.;;;","22/Mar/23 17:00;roman;Fix merged into master as 9ba3b1a5863c1aeeca0be25b4bb375abfe02b940

1.16 as 80ee512f00a9a8873926626d66cdcc97164c4595;;;",,,,,,,,,,,,,,,,,,,,,,
DataStreamConversionTestCases.test_from_data_stream_with_schema failed due to org.apache.flink.table.types.logical.CharType not existing in the JVM,FLINK-31526,13529200,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,20/Mar/23 07:50,21/Aug/23 08:35,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,auto-deprioritized-major,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47328&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=34487

{code}
Mar 19 04:08:57 =================================== FAILURES ===================================
Mar 19 04:08:57 _______ DataStreamConversionTestCases.test_from_data_stream_with_schema ________
Mar 19 04:08:57 
Mar 19 04:08:57 self = <pyflink.table.tests.test_table_environment_api.DataStreamConversionTestCases testMethod=test_from_data_stream_with_schema>
Mar 19 04:08:57 
Mar 19 04:08:57     def test_from_data_stream_with_schema(self):
Mar 19 04:08:57         from pyflink.table import Schema
Mar 19 04:08:57     
Mar 19 04:08:57         ds = self.env.from_collection([(1, 'Hi', 'Hello'), (2, 'Hello', 'Hi')],
Mar 19 04:08:57                                       type_info=Types.ROW_NAMED(
Mar 19 04:08:57                                           [""a"", ""b"", ""c""],
Mar 19 04:08:57                                           [Types.INT(), Types.STRING(), Types.STRING()]))
Mar 19 04:08:57     
Mar 19 04:08:57         table = self.t_env.from_data_stream(ds,
Mar 19 04:08:57                                             Schema.new_builder()
Mar 19 04:08:57                                                   .column(""a"", DataTypes.INT())
Mar 19 04:08:57                                                   .column(""b"", DataTypes.STRING())
Mar 19 04:08:57                                                   .column(""c"", DataTypes.STRING())
Mar 19 04:08:57                                                   .build())
Mar 19 04:08:57         result = table.execute()
Mar 19 04:08:57 >       with result.collect() as result:
Mar 19 04:08:57 
Mar 19 04:08:57 pyflink/table/tests/test_table_environment_api.py:416: 
Mar 19 04:08:57 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Mar 19 04:08:57 pyflink/table/table_result.py:196: in collect
Mar 19 04:08:57     return CloseableIterator(j_iter, field_data_types)
Mar 19 04:08:57 pyflink/table/table_result.py:229: in __init__
Mar 19 04:08:57     self._data_types = [_from_java_data_type(j_field_data_type)
Mar 19 04:08:57 pyflink/table/table_result.py:229: in <listcomp>
Mar 19 04:08:57     self._data_types = [_from_java_data_type(j_field_data_type)
Mar 19 04:08:57 pyflink/table/types.py:1693: in _from_java_data_type
Mar 19 04:08:57     if is_instance_of(logical_type, gateway.jvm.CharType):
Mar 19 04:08:57 pyflink/util/java_utils.py:70: in is_instance_of
Mar 19 04:08:57     param = get_java_class(java_class)
Mar 19 04:08:57 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_gateway.py:475: in get_java_class
Mar 19 04:08:57     return java_class._java_lang_class
Mar 19 04:08:57 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Mar 19 04:08:57 
Mar 19 04:08:57 self = <py4j.java_gateway.JavaClass object at 0x7f12f8f855b0>
Mar 19 04:08:57 
Mar 19 04:08:57     @property
Mar 19 04:08:57     def _java_lang_class(self):
Mar 19 04:08:57         """"""Gets the java.lang.Class of the current JavaClass. This is
Mar 19 04:08:57         equivalent to calling .class in Java.
Mar 19 04:08:57         """"""
Mar 19 04:08:57         command = proto.REFLECTION_COMMAND_NAME +\
Mar 19 04:08:57             proto.REFL_GET_JAVA_LANG_CLASS_SUB_COMMAND_NAME +\
Mar 19 04:08:57             self._fqn + ""\n"" + proto.END_COMMAND_PART
Mar 19 04:08:57         answer = self._gateway_client.send_command(command)
Mar 19 04:08:57     
Mar 19 04:08:57         if len(answer) > 1 and answer[0] == proto.SUCCESS:
Mar 19 04:08:57             return get_return_value(
Mar 19 04:08:57                 answer, self._gateway_client, self._fqn, ""_java_lang_class"")
Mar 19 04:08:57         else:
Mar 19 04:08:57 >           raise Py4JError(
Mar 19 04:08:57                 ""{0} does not exist in the JVM"".format(self._fqn))
Mar 19 04:08:57 E           py4j.protocol.Py4JError: org.apache.flink.table.types.logical.CharType does not exist in the JVM
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 08:35:05 UTC 2023,,,,,,,,,,"0|z1gp94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 08:35;mapohl;I updated the title and issue description. The previous description wasn't pointing to the actual error. But it looks the issue never appeared again;;;",,,,,,,,,,,,,,,,,,,,,,,,
JdbcExactlyOnceSinkE2eTest.testInsert times out,FLINK-31525,13529198,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,20/Mar/23 07:46,21/Aug/23 09:13,04/Jun/24 20:41,21/Aug/23 09:13,1.16.1,jdbc-3.0.0,,,,,,,,,,,,,,,,,,,1.16.2,,,,,,,,,Connectors / JDBC,,,,,,0,auto-deprioritized-critical,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47317&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=16612

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007fa64400b800 nid=0x79e6 waiting on condition [0x00007fa64bbe2000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000d2736728> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2081)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2049)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2027)
	at org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.testInsert(JdbcExactlyOnceSinkE2eTest.java:224)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

There's another issue with this test in FLINK-28424 which has different stacktraces.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28424,FLINK-14100,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 09:13:16 UTC 2023,,,,,,,,,,"0|z1gp8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 15:58;eskabetxe;This is related with the Oracle dialect.. I already fix this on https://github.com/apache/flink-connector-jdbc/pull/22/files#diff-2ff031e329d42baa6422b58ef5cf30bf683b7cffd6e3ae5948878aea45daecb0

This is related with checkpoint timeout (1s) and a Random failure, that on Oracle leaves to start recovering over the 50m allowed..

I could try to make this change on a separated commit to allow this be merged on 1.16..;;;","27/Mar/23 09:27;eskabetxe;[~mapohl], [~martijnvisser] PR available;;;","29/Mar/23 11:19;mapohl;[~Leonard] may you have a look at it since you also reviewed the original [PR #17676|https://github.com/apache/flink/pull/17676] which is the PR for FLINK-14100?;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 09:13;mapohl;1.16: 18b6f33ddb9c2bf117cb8bf645402fde6990ec98;;;",,,,,,,,,,,,,,,,,,,,,
StreamDependencyTests.test_add_python_file failed,FLINK-31524,13529189,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hxb,mapohl,mapohl,20/Mar/23 07:28,12/Aug/23 10:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,,0,stale-assigned,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47318&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=24508

Being caused by a ""No module named 'test_dependency_manage_lib'"" error:
{code}
[...]
Mar 18 05:42:33 E                   Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 287, in _execute
Mar 18 05:42:33 E                       response = task()
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 360, in <lambda>
Mar 18 05:42:33 E                       lambda: self.create_worker().do_instruction(request), request)
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 597, in do_instruction
Mar 18 05:42:33 E                       getattr(request, request_type), request.instruction_id)
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 634, in process_bundle
Mar 18 05:42:33 E                       bundle_processor.process_bundle(instruction_id))
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1004, in process_bundle
Mar 18 05:42:33 E                       element.data)
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 227, in process_encoded
Mar 18 05:42:33 E                       self.output(decoded_value)
Mar 18 05:42:33 E                     File ""apache_beam/runners/worker/operations.py"", line 526, in apache_beam.runners.worker.operations.Operation.output
Mar 18 05:42:33 E                     File ""apache_beam/runners/worker/operations.py"", line 528, in apache_beam.runners.worker.operations.Operation.output
Mar 18 05:42:33 E                     File ""apache_beam/runners/worker/operations.py"", line 237, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
Mar 18 05:42:33 E                     File ""apache_beam/runners/worker/operations.py"", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
Mar 18 05:42:33 E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 169, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
Mar 18 05:42:33 E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 196, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/table/operations.py"", line 110, in process_element
Mar 18 05:42:33 E                       return self.func(value)
Mar 18 05:42:33 E                     File ""<string>"", line 1, in <lambda>
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/pyflink/table/tests/test_dependency.py"", line 51, in plus_two
Mar 18 05:42:33 E                       from test_dependency_manage_lib import add_two
Mar 18 05:42:33 E                   ModuleNotFoundError: No module named 'test_dependency_manage_lib'
Mar 18 05:42:33 E                   
Mar 18 05:42:33 E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Mar 18 05:42:33 E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Mar 18 05:42:33 E                   	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:61)
Mar 18 05:42:33 E                   	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
Mar 18 05:42:33 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
Mar 18 05:42:33 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:414)
Mar 18 05:42:33 E                   	... 7 more
Mar 18 05:42:33 E                   Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 287, in _execute
Mar 18 05:42:33 E                       response = task()
Mar 18 05:42:33 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 360, in <lambda>
Mar 18 05:42:33 E                       lambda: self.create_worker().do_instruction(request), request)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 12 10:35:06 UTC 2023,,,,,,,,,,"0|z1gp6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 07:40;mapohl;[~dianfu] [~hxbks2ks] can you verify that it's a not a blocker, i.e. it's either a test code issue or related to 1.17-specific changes?;;;","21/Mar/23 02:09;hxb;[~mapohl] Thanks a lot for reporting this issue. The failure of this test is related to the instability of the test environment. This test was first introduced in 1.14, and we have not made relevant changes in 1.17. So I lower the priority of it firstly, and keep watching.;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
Merge query files and processing in sql client and gateway,FLINK-31523,13529184,13500273,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,20/Mar/23 06:54,21/Mar/23 05:29,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,Table SQL / Gateway,,,,,0,pull-request-available,,,,,"There are independent .q files and processing in sql client and gateway, we can merge them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-20 06:54:05.0,,,,,,,,,,"0|z1gp5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FlinkResultSet and related classes for jdbc driver,FLINK-31522,13529179,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,20/Mar/23 06:27,05/May/23 11:10,04/Jun/24 20:41,29/Mar/23 08:39,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Gateway,Table SQL / JDBC,,,,,0,pull-request-available,,,,,Introduce FlinkResultSet and related classes for jdbc driver to support data iterator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 29 08:39:42 UTC 2023,,,,,,,,,,"0|z1gp4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 08:39;libenchao;Fixed via [https://github.com/apache/flink/commit/3814fe8ddf5e4c80218a878df33eb30fd5f39696] (1.18.0)

[~zjureel] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Initialize flink jdbc driver module in flink-table,FLINK-31521,13529176,13528918,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,20/Mar/23 06:08,05/May/23 11:10,04/Jun/24 20:41,21/Mar/23 08:43,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Gateway,Table SQL / JDBC,,,,,0,pull-request-available,,,,,Initialize jdbc driver module,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 21 08:43:51 UTC 2023,,,,,,,,,,"0|z1gp3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 08:43;libenchao;Implemented via https://github.com/apache/flink/commit/25798e8b2ed26307daaf4437aa144e345710843d (1.18.0)

[~zjureel] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Move SqlXXXModel conversion logic to SqlNodeConverter,FLINK-31520,13529160,13528594,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xzw0223,xzw0223,xzw0223,20/Mar/23 03:21,12/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,stale-assigned,,,,Introduce SqlNodeConverter for SqlModule-related conversion logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 12 10:35:06 UTC 2023,,,,,,,,,,"0|z1gp08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The watermark alignment docs is outdated after FLIP-217 finished,FLINK-31519,13529159,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mason6345,leonard,leonard,20/Mar/23 03:16,30/May/23 13:49,04/Jun/24 20:41,30/May/23 13:49,1.17.0,,,,,,,,,,,,,,,,,,,,1.17.2,1.18.0,,,,,,,,Documentation,,,,,,0,,,,,,"With FLIP-217 finished, the watermark alignment limitation has been resolved, the *beta* tag and *warning note* can be safely removed.

[1] https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/event-time/generating_watermarks/#watermark-alignment-_beta_",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28853,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 30 13:49:35 UTC 2023,,,,,,,,,,"0|z1gp00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 10:06;jingge;IMHO, the watermark alignment should still be marked as beta, because:
 * only Kafka and Pulsar connectors support watermark alignment of source splits
 * there are some hints left in the source code for future work[1] 

 

[1]https://github.com/apache/flink/blob/0180284381bf7999781d542219d2a097f3cbc098/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/splitreader/SplitReader.java#L94;;;","30/May/23 13:49;pnowojski;Fixed via FLINK-28853

merged commit 678370b into apache:master
merged commit 6ccc3da into apache:release-1.17;;;",,,,,,,,,,,,,,,,,,,,,,,,,
HighAvailabilityServiceUtils resolves wrong webMonitorAddress in Standalone mode,FLINK-31518,13529134,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,vineethNaroju,vineethNaroju,19/Mar/23 16:27,14/Mar/24 07:36,04/Jun/24 20:41,,1.16.0,1.16.1,1.17.0,1.17.1,,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,Runtime / Configuration,Runtime / REST,,,,,0,pull-request-available,stale-assigned,,,,"{{HighAvailabilityServiceUtils#createHighAvailabilityServices()}}  in {{HighAvailabilityMode.NONE}} mode uses {{HighAvailabilityServiceUtils#getWebMonitorAddress()}} to get web monitor address.

{{HighAvailabilityServiceUtils#getWebMonitorAddress()}} reads only from {{rest.port}} in {{flink-conf.yaml}} . If {{rest.port}} is not enabled, then it returns {{0}} port number. It should dynamically fetch port number if {{rest.port}} is disabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Sat Aug 12 10:35:06 UTC 2023,,,,,,,,,,"0|z1goug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 10:08;huwh;Could you describe the use scenario in more detail?

IIUC, when use HighAvailabilityMode.NONE, we should set the right 'rest.port' manually for flink client's configuration. And we can't fetch the real port number w/o high-available enabled. ;;;","21/Mar/23 07:53;prabhujoseph;[~huwh] We have a separate service runs part of JobManager which queries the REST API exposed by JobManager to access the Flink Metrics. It has to discover the Webmonitor Address. The rest.port is set to a range of port numbers. ClusterRestEndpointLeaderRetriever is used to discover the service which works fine when rest.port is set to a static port but returns 0 if a range is set. This jira intends to fix/improve this to return the right port number in case if range of ports is configured with rest.port.

{code}
LeaderRetrievalService webMonitorRetrievalService = highAvailabilityServices.getClusterRestEndpointLeaderRetriever();
try {
   webMonitorRetrievalService.start(new WebMonitorLeaderListener());
} catch (Exception e) {
  throw new RuntimeException(e);
}


private class WebMonitorLeaderListener implements LeaderRetrievalListener {
@Override
public void notifyLeaderAddress(final String leaderAddress, final UUID leaderSessionID) {
  System.out.println(leaderAddress);
}
{code}

;;;","21/Mar/23 08:30;huwh;[~prabhujoseph] Can I ask how do you run service in JobManager? IIUC, Flink does not provide an interface to start some other services.

It's not easy to let the StandaloneHaServices aware of the real port of webMonitorAddress. We create haServices before start web monitor service in ClusterEntrypoint#runCluster.
;;;","22/Mar/23 11:32;prabhujoseph;[~huwh]  It is a thread launched by AdaptiveScheduler which requires the webMonitorAddress. Currently there is no way to fetch the webMonitorAddress at AdaptiveScheduler initialization. There are two options we got

Option 1: Pass the webMonitorAddress all the way from DefaultDispatcherResourceManagerComponentFactory -> Dispatcher -> JobMaster -> DefaultSlotPoolServiceSchedulerFactory -> AdaptiveScheduler. There are so many factories, implementations, interface apis need to be changed to add webMonitorAddress parameter.

Option 2: Fix ClusterRestEndpointLeaderRetriever to provide the right webMonitorAddress when rest.port is a range.

The Option 1 does not look clean as so much api change required. I have to analyze further on Option 2. There is one more option we thought which is to have a shared context class with all the configs/informations that Dispatcher, JobMaster, SlotPoolService, AdaptiveScheduler can use. This requires change in only the context class if any new information has to be added later.;;;","23/Mar/23 04:17;paul8263;Hi [~prabhujoseph] ,

It seems that we can retrieve the web monitor address (including the port) by calling WebMonitorEndpoint::getServerAddress(). Or we can write the actual port back to configuration when WebMonitorEndpoint has been successfully started. But the challenge is, how can we retrieve WebMonitorEndpoint from HighAvailabilityServicesUtils? And another one is which one is invoked first, the start of WebMonitorEndpoint or the creation of high available service?;;;","23/Mar/23 17:19;prabhujoseph;Thanks for the pointers. [~vineethNaroju] and I have come up with a patch based on the above inputs.

{quote}And another one is which one is invoked first, the start of WebMonitorEndpoint or the creation of high available service?{quote}

High Availability Services {{StandaloneHaServices}} is initialised first with the configured {{clusterRestEndpointAddress}} which can be a range of ports, and then the {{RestServerEndpoint}} is started, which has the actual port.

The patch sets the configuration with the rest address and rest port, from which the {{StandaloneHaServices#getClusterRestEndpointLeaderRetriever}} retrieves the address and port. Similar ways are used in other places, like the one for the JobManager RPC Port.

This is the [WIP patch |https://github.com/PrabhuJoseph/flink/commit/45e268f3d446880b75a666b974d2188bec2e0132#diff-49ab603f69009cb6e7b652cb99570a0df71da897298a09acbc56bf29b42bbc6b]. Please review it and let us know your comments.;;;","27/Mar/23 12:13;huwh;[~prabhujoseph] 
how about update configuration with the real rest port after webmonitor start in DefaultDispatcherResourceManagerComponentFactory. Then in your own service use this configuration to create a ClientHighAvailabilityServices to retrieve the rest address.

;;;","29/Mar/23 06:35;prabhujoseph;[~huwh]  Yes, that works. Could you please review the [patch|https://github.com/apache/flink/pull/22293]  when you get some time?;;;","30/Mar/23 12:54;prabhujoseph;Could you assign this ticket to my team member [~samrat007] to continue working on this patch. ;;;","05/Apr/23 16:14;samrat007;[~huwh] please review the pr in free time ;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,
Move execution logic of AddJarOperation out from TableEnvironmentImpl,FLINK-31517,13529120,13527609,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,jark,jark,19/Mar/23 10:36,21/Mar/23 23:46,04/Jun/24 20:41,21/Mar/23 23:46,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"This should implement ExecutableOperation for the {{AddJarOperation}} to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 21 23:46:20 UTC 2023,,,,,,,,,,"0|z1gork:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 23:46;Sergey Nuyanzin;Merged to master as [0303806fc18deb3189c264f3f8eeade0e5a286c7|https://github.com/apache/flink/commit/0303806fc18deb3189c264f3f8eeade0e5a286c7];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Move execution logic of LoadModule and UnloadModule operations out from TableEnvironmentImpl,FLINK-31516,13529118,13527609,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,jark,jark,19/Mar/23 10:30,22/Mar/23 06:37,04/Jun/24 20:41,22/Mar/23 06:37,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"This should implement ExecutableOperation for LoadModule and UnloadModule operations to move the execution logic out from {{TableEnvironmentImpl#executeInternal().

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 22 06:37:07 UTC 2023,,,,,,,,,,"0|z1gor4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 06:37;Sergey Nuyanzin;Merged to master as [0c2db680f2346a11fdef5a6d13f88e5dcc7b4517|https://github.com/apache/flink/commit/0c2db680f2346a11fdef5a6d13f88e5dcc7b4517];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Move execution logic of AnalyzeTableOperation out from TableEnvironmentImpl,FLINK-31515,13529117,13527609,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jark,jark,jark,19/Mar/23 10:28,19/Mar/23 10:30,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"This should implement ExecutableOperation for the AnalyzeTableOperation to move the execution logic out from {{TableEnvironmentImpl#executeInternal().

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-03-19 10:28:42.0,,,,,,,,,,"0|z1goqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move query SqlNode conversion logic to SqlQueryConverter,FLINK-31514,13529111,13528594,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jark,jark,jark,19/Mar/23 08:29,21/Mar/23 12:36,04/Jun/24 20:41,21/Mar/23 12:36,,,,,,,,,,,,,,,,,,,,,1.18.0,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Introduce {{SqlQueryConverter}} and move the conversion logic of query {{SqlNode}} -> {{PlannerQueryOption}} to it.

Note that the conversion is complex for query SqlNodes because it's unclear what the specific query SqlNode classes are. But they should all belong to the {{SqlKind.QUERY}}. Therefore, we can introduce a new mapping path for the SqlKind. 

{code:java}
public interface SqlNodeConverter<S extends SqlNode> {
    /**
     * Returns the {@link SqlKind SqlKinds} of {@link SqlNode SqlNodes} that the {@link
     * SqlNodeConverter} supports to convert.
     *
     * <p>If a {@link SqlNodeConverter} return s a non-empty SqlKinds, the conversion framework
     * prefer to match SqlKind of SqlNode instead of matching class of SqlNode.
     *
     * @see SqlQueryConverter
     */
    default Optional<EnumSet<SqlKind>> supportedSqlKinds() {
        return Optional.empty();
    }

   ...

}
{code}



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 21 12:36:42 UTC 2023,,,,,,,,,,"0|z1gopk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 12:36;jark;Fixed in master: 47944ce00e0819231ae2e099f2382a188f4dae91;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
