Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Inward issue link (Required),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Flink Table API CSV streaming sink throws SerializedThrowable exception,FLINK-28513,13471197,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,samrat007,jaya.ananthram,jaya.ananthram,12/Jul/22 10:34,02/Jan/24 11:22,04/Jun/24 20:42,04/Sep/23 11:02,1.15.1,,,,,1.17.2,1.18.0,1.19.0,FileSystems,Table SQL / API,,,,,,1,pull-request-available,stale-assigned,,,"Table API S3 streaming sink (CSV format) throws the following exception,


{code:java}
Caused by: org.apache.flink.util.SerializedThrowable: S3RecoverableFsDataOutputStream cannot sync state to S3. Use persist() to create a persistent recoverable intermediate point.
at org.apache.flink.fs.s3.common.utils.RefCountedBufferingFileStream.sync(RefCountedBufferingFileStream.java:111) ~[flink-s3-fs-hadoop-1.15.1.jar:1.15.1]
at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.sync(S3RecoverableFsDataOutputStream.java:129) ~[flink-s3-fs-hadoop-1.15.1.jar:1.15.1]
at org.apache.flink.formats.csv.CsvBulkWriter.finish(CsvBulkWriter.java:110) ~[flink-csv-1.15.1.jar:1.15.1]
at org.apache.flink.connector.file.table.FileSystemTableSink$ProjectionBulkFactory$1.finish(FileSystemTableSink.java:642) ~[flink-connector-files-1.15.1.jar:1.15.1]
at org.apache.flink.streaming.api.functions.sink.filesystem.BulkPartWriter.closeForCommit(BulkPartWriter.java:64) ~[flink-file-sink-common-1.15.1.jar:1.15.1]
at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.closePartFile(Bucket.java:263) ~[flink-streaming-java-1.15.1.jar:1.15.1]
at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.prepareBucketForCheckpointing(Bucket.java:305) ~[flink-streaming-java-1.15.1.jar:1.15.1]
at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.onReceptionOfCheckpoint(Bucket.java:277) ~[flink-streaming-java-1.15.1.jar:1.15.1]
at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.snapshotActiveBuckets(Buckets.java:270) ~[flink-streaming-java-1.15.1.jar:1.15.1]
at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.snapshotState(Buckets.java:261) ~[flink-streaming-java-1.15.1.jar:1.15.1]
at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSinkHelper.snapshotState(StreamingFileSinkHelper.java:87) ~[flink-streaming-java-1.15.1.jar:1.15.1]
at org.apache.flink.connector.file.table.stream.AbstractStreamingWriter.snapshotState(AbstractStreamingWriter.java:129) ~[flink-connector-files-1.15.1.jar:1.15.1]
{code}

In my table config, I am trying to read from Kafka and write to S3 (s3a) using table API and checkpoint configuration using s3p (presto). Even I tried with a simple datagen example instead of Kafka with local file system as checkpointing (`file:///` instead of `s3p://`) and I am getting the same issue. Exactly it is fails when the code triggers the checkpoint.

Some related slack conversation and SO conversation [here|https://apache-flink.slack.com/archives/C03G7LJTS2G/p1657609776339389], [here|https://stackoverflow.com/questions/62138635/flink-streaming-compression-not-working-using-amazon-aws-s3-connector-streaming] and [here|https://stackoverflow.com/questions/72943730/flink-table-api-streaming-s3-sink-throws-serializedthrowable-exception]

Since there is no work around for S3 table API streaming sink, I am marking this as critical. if this is not a relevant severity, feel free to reduce the priority. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33536,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Sep 05 11:02:54 UTC 2023,,,,,,,,,,"0|z16spk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/22 08:50;dannycranmer;Assigning to [~samrat007] as discussed offline;;;","03/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","04/Sep/23 10:58;liangtl; merged commit [{{e921489}}|https://github.com/apache/flink/commit/e921489279ca70b179521ec4619514725b061491] into apache:master;;;","04/Sep/23 11:03;hong;[~samrat007]  Could we backport this bugfix to Flink 1.17 and 1.18 as well?;;;","04/Sep/23 11:38;samrat007;Hi [~liangtl] 

backport for 1.17 :- [https://github.com/apache/flink/pull/23351]
backport for 1.18 :-  [https://github.com/apache/flink/pull/23352]

Please review whenever time ;;;","04/Sep/23 11:48;samrat007;I dont have access to update the fix version . Please help updating the `Fix Version/s` for this issue;;;","04/Sep/23 15:16;hong;merged commit [{{d06a297}}|https://github.com/apache/flink/commit/d06a297422fd4884aa21655fdf1f1bce94cc3a8a] into apache:release-1.17;;;","04/Sep/23 15:16;hong;{quote}I dont have access to update the fix version . Please help updating the `Fix Version/s` for this issue
{quote}
Updated;;;","05/Sep/23 11:02;hong;merged commit [{{2437cf5}}|https://github.com/apache/flink/commit/2437cf568785a05ece70fde9f917637731740e46] into apache:release-1.18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select HashBasedDataBuffer and SortBasedDataBuffer dynamically based on the number of network buffers can be allocated for SortMergeResultPartition,FLINK-28512,13471196,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,12/Jul/22 10:29,21/Jul/22 10:16,04/Jun/24 20:42,21/Jul/22 10:16,,,,,,1.16.0,,,,,,,,,,0,pull-request-available,,,,"Currently, the SortMergeResultPartition select to use HashBasedDataBuffer and SortBasedDataBuffer based on the number of required buffers per result partition decided by 'taskmanager.network.sort-shuffle.min-buffers'. If the configured value is large enough, HashBasedDataBuffer will be used, otherwise, SortBasedDataBuffer will be used. Usually, the HashBasedDataBuffer has better performance. However, it is not easy to tune this value, because if a user tries to increase it for better performance, he/she is easy to encounter the 'Insufficient number of network buffers' error. This patch improves this case by selecting HashBasedDataBuffer and SortBasedDataBuffer dynamically based on the number of network buffers can be allocated. More specifically, if there is enough buffers at runtime, HashBasedDataBuffer will be used, otherwise, SortBasedDataBuffer will be used. To achieve better performance, the user only need to increase total amount of network memory per task manager.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 21 10:16:09 UTC 2022,,,,,,,,,,"0|z16spc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 10:16;kevin.cyj;Merged into master via 5f0c4ab91a8ffc80ff04d0324934b2993fc5b533;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Spark dependencies and document,FLINK-28511,13471191,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,12/Jul/22 09:54,13/Jul/22 07:01,04/Jun/24 20:42,13/Jul/22 07:01,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"- Include Hive metastore dependency in Spark bundled jar.
- Improve document of Spark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 07:01:57 UTC 2022,,,,,,,,,,"0|z16so8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 07:01;lzljs3620320;master: c11725774f9847ccf4b867d08d8620b3879cbd43;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support using new KafkaSink in PyFlink,FLINK-28510,13471188,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Juntao Hu,Juntao Hu,Juntao Hu,12/Jul/22 09:41,17/Oct/22 01:49,04/Jun/24 20:42,27/Jul/22 10:55,1.15.1,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26385,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 27 10:55:22 UTC 2022,,,,,,,,,,"0|z16snk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 10:55;hxbks2ks;Merged into master via c796a78671c8f201be4a298c7a7b934c70ea4398;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support REVERSE built-in function in Table API,FLINK-28509,13471186,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,12/Jul/22 09:31,18/Jul/22 05:43,04/Jun/24 20:42,18/Jul/22 05:43,,,,,,1.16.0,,,API / Python,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 18 05:43:22 UTC 2022,,,,,,,,,,"0|z16sn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 05:43;dianfu;Merged to master via e1d93566365e6fe6a8f780c88fc73df2b4466c29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SPLIT_INDEX and STR_TO_MAP built-in function in Table API,FLINK-28508,13471185,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,12/Jul/22 09:30,18/Jul/22 05:43,04/Jun/24 20:42,15/Jul/22 03:31,,,,,,1.16.0,,,API / Python,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 18 05:43:45 UTC 2022,,,,,,,,,,"0|z16smw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 05:43;dianfu;Merged to master via e85c3038d901db3696112c1add3babcce0b0bcbc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
supports set hadoop mount path in pod,FLINK-28507,13471176,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,12/Jul/22 09:00,13/Jul/22 07:21,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,"the hadoop configmap mounted path is hard code in flink k8s at /opt/hadoop/conf, but we want to set diffirent path because of my company has already set in /opt/flink/conf, and the conf is using in old code.

 
{code:java}
// just like the following

public static final ConfigOption<String> HADOOP_CONF_DIR =
        key(""kubernetes.hadoop.conf.dir"")
                .stringType()
                .defaultValue(""/opt/hadoop/conf"")
                .withDescription(
                        ""The hadoop conf directory that will be mounted in pod. The core-site.xml, ""
                                + ""hdfs-site.xml in this path will be overwritten from config map.""); {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 12 09:11:01 UTC 2022,,,,,,,,,,"0|z16skw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 09:01;jackylau;hi [~yangwang166] what you think?;;;","12/Jul/22 09:11;wangyang0918;Should we get the hadoop conf dir from the {{HADOOP_CONF_DIR}} environment?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar sink don't support zero timestamp which may be provided by flink,FLINK-28506,13471146,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,syhily,syhily,12/Jul/22 07:33,16/Sep/22 10:03,04/Jun/24 20:42,09/Sep/22 03:49,1.15.0,,,,,,,,Connectors / Pulsar,,,,,,,0,,,,,The timestamp from flink's SinkWriter.Context#timestamp() could be zero. This is not allowed in Pulsar. We should filter the invalid timestamp when sinking the message in the Pulsar connector.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29207,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 16 10:03:05 UTC 2022,,,,,,,,,,"0|z16se8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 10:03;nikola.sten;What is a workaround for this? How should i use the sink when context timestamp is 0?
Also when is this going to be released? In latest 1.15.2 this problem appears ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar sink doesn't support topic auto creation,FLINK-28505,13471145,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,syhily,syhily,syhily,12/Jul/22 07:30,13/Feb/23 13:52,04/Jun/24 20:42,13/Feb/23 13:52,1.15.0,,,,,,,,Connectors / Pulsar,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28351,,,,FLINK-28351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Feb 13 02:24:49 UTC 2023,,,,,,,,,,"0|z16se0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 02:24;syhily;[~tison] Can you help me close this issue? It's fixed in FLINK-28351.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local-Global aggregation causes watermark alignment (table.exec.source.idle-timeout) of idle partition invalid,FLINK-28504,13471142,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,nyingping,nyingping,nyingping,12/Jul/22 07:22,23/Aug/22 12:39,04/Jun/24 20:42,05/Aug/22 00:29,1.14.5,,,,,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,"I have a window topN test task, the code is as follows

 
{code:java}
 Configuration configuration = new Configuration();
        configuration.setInteger(RestOptions.PORT, 8082);
        StreamExecutionEnvironment streamExecutionEnvironment =
                StreamExecutionEnvironment.getExecutionEnvironment(configuration);
 
        StreamTableEnvironment st = StreamTableEnvironment.create(streamExecutionEnvironment);
 
        st.getConfig().getConfiguration().setString(""table.exec.source.idle-timeout"", ""10s"");
        st.executeSql(
                ""CREATE TABLE test (\n""
                        + ""  `key` STRING,\n""
                        + ""  `time` TIMESTAMP(3),\n""
                        + ""  `price` float,\n""
                        + ""  WATERMARK FOR `time` AS `time` - INTERVAL '10' SECOND""
                        + "") WITH (\n""
                        + ""  'connector' = 'kafka',\n""
                        + ""  'topic' = 'test',\n""
                        + ""  'properties.bootstrap.servers' = 'testlocal:9092',\n""
                        + ""  'properties.group.id' = 'windowGroup',\n""
                        + ""  'scan.startup.mode' = 'latest-offset',\n""
                        + ""  'format' = 'json'\n""
                        + "")""
    String sqlWindowTopN =
                ""select * from ("" +
                ""  select *, "" +
                ""   ROW_NUMBER() over (partition by window_start, window_end order by total desc ) as rownum "" +
                ""     from ("" +
                ""       select key,window_start,window_end,count(key) as `count`,sum(price) total from table ("" +
                ""           tumble(TABLE test, DESCRIPTOR(`time`), interval '1' minute)"" +
                ""        ) group by window_start, window_end, key"" +
                ""   )"" +
                "") where rownum <= 3"";
    st.executeSql(sqlWindowTopN).print(); {code}
 

 

Run and do not get result on long time after.

Watermark appears as follows on the UI

 

!image-2022-07-12-15-11-51-653.png|width=898,height=388!

I didn't set the parallelism manually, so it defaults to 12. The data source Kafka has only one partition, so there are free partitions. To align the watermarks for the entire task, I use the `table.exec. source. Idle-timeout` configuration.

 

As above show,I found that the system automatically split window-Topn SQL into local-global aggregation tasks. In the Local phase, watermark didn't work as well as I expected.

 

Manually setting the parallelism to 1 did what I expected.
{code:java}
streamExecutionEnvironment.setParallelism(1); {code}
!image-2022-07-12-15-19-29-950.png|width=872,height=384!

 

I can also manually configure the system not to split into local-global phases. At this point, the `table.exec.source-idle-timeout ` configuration takes effect and the watermark is aligned.
{code:java}
st.getConfig().getConfiguration().setString(""table.optimizer.agg-phase-strategy"", ""ONE_PHASE""); {code}
result:

!image-2022-07-12-15-20-06-919.png|width=866,height=357!

 

To sum up, when the parallelism of Kafka partition is different from that of Flink, and idle partitions are generated, I expect to use the configuration'table exec. source. Idle-timeout'to use watermark alignment, but here it seems to fail.","flink 1.14

kafka 2.4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/22 07:11;nyingping;image-2022-07-12-15-11-51-653.png;https://issues.apache.org/jira/secure/attachment/13046603/image-2022-07-12-15-11-51-653.png","12/Jul/22 07:19;nyingping;image-2022-07-12-15-19-29-950.png;https://issues.apache.org/jira/secure/attachment/13046602/image-2022-07-12-15-19-29-950.png","12/Jul/22 07:20;nyingping;image-2022-07-12-15-20-06-919.png;https://issues.apache.org/jira/secure/attachment/13046601/image-2022-07-12-15-20-06-919.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 23 12:39:23 UTC 2022,,,,,,,,,,"0|z16sdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 07:31;martijnvisser;[~nyingping] Can you verify this with the latest Flink 1.14 release? ;;;","12/Jul/22 07:39;nyingping;[~martijnvisser] 

yeah,I have tested it on version 1.14.5, and the problem remains.;;;","12/Jul/22 07:52;martijnvisser;[~nyingping] Based on your info, isn't it more likely that the issue is caused by the local-global phases? Given that the idle-timeout parameter works when you disable local-global? ;;;","12/Jul/22 08:02;nyingping;[~martijnvisser] 

yes,It is work when set like this
{code:java}
st.getConfig().getConfiguration().setString(""table.optimizer.agg-phase-strategy"", ""ONE_PHASE""); {code}
 ;;;","18/Jul/22 08:29;nyingping;[~martijnvisser] Hi, Could you assign this task to me? I'd love to finish the job. If the community thinks it's a BUG.;;;","18/Jul/22 08:40;martijnvisser;[~nyingping] I've assigned it to you, looking forward to your PR;;;","23/Aug/22 12:39;pnowojski;{quote}
After giving this a second look I think the current code behaviour is indeed correct and I don't see any bug here.

You idle source subtasks have never sent any watermark to some of the downstream local aggregations. So those LocalWindowAggregate correctly don't report any watermark, resulting in Low Watermark in the webUI not being reported. As correctly, low watermark is NaN for the task as a whole.

It might be a bit confusing the current behaviour. Ideally maybe WebUI should present the lowest non NaN watermark, with a caveate/asteriks, that some subtasks are idle. But to me the current behaviour is also valid.
{quote};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix invalid link in Python FAQ Document,FLINK-28503,13471135,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,bgeng777,bgeng777,12/Jul/22 06:52,21/Aug/23 22:35,04/Jun/24 20:42,,,,,,,,,,Documentation,,,,,,,0,auto-deprioritized-major,pull-request-available,,,"[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/faq/#preparing-python-virtual-environment]

The script for setting pyflink virtual environment is invalid now. The candidate is [https://nightlies.apache.org/flink/flink-docs-release-1.12/downloads/setup-pyflink-virtual-env.sh] or we can add this short script in the doc website directly.

!image-2022-07-12-14-51-01-434.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/22 06:51;bgeng777;image-2022-07-12-14-51-01-434.png;https://issues.apache.org/jira/secure/attachment/13046596/image-2022-07-12-14-51-01-434.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 21 22:35:21 UTC 2023,,,,,,,,,,"0|z16sbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for RegexTokenizer,FLINK-28502,13471116,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,12/Jul/22 03:58,21/Jul/22 06:10,04/Jun/24 20:42,20/Jul/22 09:12,,,,,,ml-2.2.0,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 21 06:10:04 UTC 2022,,,,,,,,,,"0|z16s7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 06:10;zhangzp;fixed via f9f802125d604f0155221804237fd4140e239602;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for VectorIndexer,FLINK-28501,13471115,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,12/Jul/22 03:57,26/Jul/22 02:04,04/Jun/24 20:42,26/Jul/22 02:04,,,,,,ml-2.2.0,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 26 02:04:56 UTC 2022,,,,,,,,,,"0|z16s7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 02:04;zhangzp;fixed via e31101fdb614ea6366111f761e3176c5c2ab6ef0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for Tokenizer,FLINK-28500,13471113,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,12/Jul/22 03:54,20/Jul/22 06:36,04/Jun/24 20:42,20/Jul/22 06:36,,,,,,ml-2.2.0,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 20 06:36:18 UTC 2022,,,,,,,,,,"0|z16s6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 06:36;zhangzp;Merged via 3b97256096d2c97abd3dc0b3810663e50e746b9e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
resource leak when job failed with unknown status In Application Mode,FLINK-28499,13471109,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,frozen stone,frozen stone,12/Jul/22 03:23,19/Jul/22 04:54,04/Jun/24 20:42,15/Jul/22 03:05,1.13.1,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,"I found a job restarted for thousands of times, and jobmanager tried to create a new taskmanager pod every time.  The jobmanager restarted because submitted with duplicate  job id[1] (we preset the jobId rather than generate), but I hadn't save the logs unfortunately. 

this job requires one taskmanager pod in normal circumstances, but thousands of pods were leaked finally.  you can find the screenshot in the attachment.



 

In application mode, cluster resources will be released  when job finished in succeeded, failed or canceled status[2][3] . When some exception happen, the job may be terminated in unknown status[4] . 

In this case, the job exited with unknown status , without releasing  taskmanager pods. So is it reasonable to not release taskmanager when job exited in unknown status ? 

 

 

one line in original logs:
2022-07-01 09:45:40,712 [main] INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Terminating cluster entrypoint process KubernetesApplicationClusterEntrypoint with exit code 1445.

 

[1] [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L452]

[2] [https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L90-L91]

[3] [https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L175]

[4] [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/clusterframework/ApplicationStatus.java#L39]

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28498,FLINK-28497,,,,,,,,,,,,,,,,,,,"12/Jul/22 03:23;frozen stone;cluster-pod-error.png;https://issues.apache.org/jira/secure/attachment/13046554/cluster-pod-error.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 19 04:54:56 UTC 2022,,,,,,,,,,"0|z16s60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 06:47;martijnvisser;[~frozen stone] Without logs and without validating this on a still supported version of Flink it will be quite hard to provide an answer to this. ;;;","12/Jul/22 09:35;yunta;I think the ticket focus on the question:
{{is it reasonable to not release taskmanager when job exited in unknown status ?}}

I feel like Flink currently does not give explicit reactions for the UNKNOWN status, [~wangyang0918] do you have some insights here?;;;","13/Jul/22 07:47;wangyang0918;Since all the TaskManager pods have been set the owner reference to the JobManager deployment, they will be deleted automatically when deleting JobManager deployment. I am curious how could they leaked.;;;","13/Jul/22 08:57;frozen stone;[~wangyang0918]  Thanks and I appreciate your comment!. 
The extra taskmanager pods will be deleted when we kill the jobmanager deployment, but in this case, the jobmanager tried to submit job   ->  create new taskmanager -> job exited in unknown status -> jobmanager restarted -> tried to submit job and create taskmanager again, it didn't end until we found this error and kill the jobmanager.  When a job failed,  I expected to get several pods in restarting or error status rather than thousands of pods.

I am not sure how much of an impact these existing pods on the cluster, maybe we can release these pods when job exited in unknown status to avoid this? [1]

[1]https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L170-L171

 ;;;","14/Jul/22 04:01;wangyang0918;I believe the Flink ResourceManager will not allocate more TaskManager pods if it recover enough TM after failover. In your case, what we need to fix is to release the Init:Error pod when failover. Currently, we only handle the containers, not including the init containers.;;;","14/Jul/22 04:07;wangyang0918;BTW, if the JobManager pod did not crash backoff continuously, we will not have so many residual init error TaskManager pods. Because Flink ResourceManager will release the TM which does not register in the timeout.;;;","15/Jul/22 03:03;frozen stone;[~wangyang0918]  Thanks for your explanation.
It seems that TaskManager pods inited error and JobManager crashed at the same time , so none of them would delete TaskManager pods, this is reasonable and I should resolve the problem during init-container. ;;;","19/Jul/22 04:54;wangyang0918;Maybe we could improve this by deleting the init error pods when recovering after JobManager failover.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
resource leak when job failed with unknown status In Application Mode,FLINK-28498,13471108,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,frozen stone,frozen stone,12/Jul/22 03:21,12/Jul/22 09:28,04/Jun/24 20:42,12/Jul/22 03:54,1.13.1,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,"I found a job restarted for thousands of times, and jobmanager tried to create a new taskmanager pod every time.  The jobmanager restarted because submitted with duplicate  job id[1] (we preset the jobId rather than generate), but I hadn't save the logs unfortunately. 

this job requires one taskmanager pod in normal circumstances, but thousands of pods were leaked finally.
!image-2022-07-12-11-02-43-009.png|width=666,height=366!



In application mode, cluster resources will be released  when job finished in succeeded, failed or canceled status[2][3] . When some exception happen, the job may be terminated in unknown status[4] .  

In this case, the job exited with unknown status , without releasing  taskmanager pods. So is it reasonable to not release taskmanager when job exited in unknown status ? 

 

 

one line in original logs:
2022-07-01 09:45:40,712 [main] INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Terminating cluster entrypoint process KubernetesApplicationClusterEntrypoint with exit code 1445.

 

[1] [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L452]

[2] [https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L90-L91]


[3] [https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L175]

[4] [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/clusterframework/ApplicationStatus.java#L39]

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28499,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/22 03:21;frozen stone;cluster-pod-error.png;https://issues.apache.org/jira/secure/attachment/13046553/cluster-pod-error.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-12 03:21:51.0,,,,,,,,,,"0|z16s5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
resource leak when job failed with unknown status In Application Mode,FLINK-28497,13471107,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,frozen stone,frozen stone,12/Jul/22 03:21,12/Jul/22 09:29,04/Jun/24 20:42,12/Jul/22 03:54,1.13.1,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,"I found a job restarted for thousands of times, and jobmanager tried to create a new taskmanager pod every time.  The jobmanager restarted because submitted with duplicate  job id[1] (we preset the jobId rather than generate), but I hadn't save the logs unfortunately. 

this job requires one taskmanager pod in normal circumstances, but thousands of pods were leaked finally.
!image-2022-07-12-11-02-43-009.png|width=666,height=366!



In application mode, cluster resources will be released  when job finished in succeeded, failed or canceled status[2][3] . When some exception happen, the job may be terminated in unknown status[4] .  

In this case, the job exited with unknown status , without releasing  taskmanager pods. So is it reasonable to not release taskmanager when job exited in unknown status ? 

 

 

one line in original logs:
2022-07-01 09:45:40,712 [main] INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Terminating cluster entrypoint process KubernetesApplicationClusterEntrypoint with exit code 1445.

 

[1] [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L452]

[2] [https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L90-L91]


[3] [https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L175]

[4] [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/clusterframework/ApplicationStatus.java#L39]

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-12 03:21:07.0,,,,,,,,,,"0|z16s5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose label selector support in k8s operator,FLINK-28496,13471057,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,mbalassi,mbalassi,11/Jul/22 19:16,14/Jul/22 14:44,04/Jun/24 20:42,14/Jul/22 14:44,kubernetes-operator-1.1.0,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The JOSDK has a [labelselector|https://github.com/java-operator-sdk/java-operator-sdk/blob/main/operator-framework-core/src/main/java/io/javaoperatorsdk/operator/api/config/ControllerConfigurationOverrider.java#L34] which can let users filter the custom resources watched. We should expose this via our config.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 14:44:03 UTC 2022,,,,,,,,,,"0|z16rug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 14:44;mbalassi;[be2aaf7|https://github.com/apache/flink-kubernetes-operator/commit/be2aaf79c88d02de0638215b1b4ed9c5263e1951] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typos or mistakes of Flink CEP Document in the official website,FLINK-28495,13471033,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bgeng777,bgeng777,bgeng777,11/Jul/22 15:38,01/Aug/22 13:52,04/Jun/24 20:42,01/Aug/22 13:52,,,,,,1.16.0,,,Library / CEP,,,,,,,0,pull-request-available,,,,"1. ""how you can migrate your job from an older Flink version to Flink-1.3."" -> ""how you can migrate your job from an older Flink version to Flink-1.13.""

2. ""Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: {quote}\\{ C A1 B\}, \{C A1 A2 B\}, \{C A1 A3 B\}, \{C A1 A4 B\}, \{C A1 A2 A3 B\}, \{C A1 A2 A4 B\}, \{C A1 A3 A4 B\}, \{C A1 A2 A3 A4 B\}{quote}"" -> ""Will generate the following matches for an input sequence: C D A1 A2 A3 D A4 B. with combinations enabled: {quote}\{C A1 B\}, \{C A1 A2 B\}, \{C A1 A3 B\}, \{C A1 A4 B\}, \{C A1 A2 A3 B\}, \{C A1 A2 A4 B\}, \{C A1 A3 A4 B\}, \{C A1 A2 A3 A4 B\}, \{C A2 B\}, \{C A2 A3 B\}, \{C A2 A4 B\}, \{C A2 A3 A4 B\}, \{C A3 B\}, \{C A3 A4 B\}, \{C A4 B\}{quote}""


3. ""For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no elements mapped to the specified variable."" -> ""For SKIP_TO_FIRST/LAST there are two options how to handle cases when there are no events mapped to the *PatternName*.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 01 13:52:41 UTC 2022,,,,,,,,,,"0|z16rp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 07:41;martijnvisser;[~bgeng777] Do you want to work on this?;;;","14/Jul/22 02:33;bgeng777;[~martijnvisser] Sure. I can take this ticket.;;;","01/Aug/22 13:52;martijnvisser;Fixed in master: c3e72be836e1d14871109ceb0ddb2e98392f4ff1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new job status states to CRD,FLINK-28494,13471007,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,darenwkt,darenwkt,darenwkt,11/Jul/22 12:47,30/Jul/22 06:41,04/Jun/24 20:42,30/Jul/22 06:41,,,,,,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The following job info are currently not available in the [jobStatus|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/crd/status/JobStatus.java] under the Operator [CRD|https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/crds/flinkdeployments.flink.apache.org-v1.yml]: # endTime – job’s end time, defaults to “-1” for streaming job.
 # duration – duration of the running job from start to now.
 # jobPlan – contains job graph info such as parallelism.

We propose to add the 3 states above into the CRD. Although this stems from an internal requirement for AWS KDA, we foresee that these 3 states could be beneficial to other Flink Kubernetes Operator users/applications as well. These 3 states are readily available via Flink REST client endpoints (I.e “/jobs/:jobid”), which means that the states are exposed for Flink dashboard and other applications. Therefore, we see potential value to others from exposing these 3 job status states in the CRD.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-11 12:47:50.0,,,,,,,,,,"0|z16rjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add document to describe ""ANALYZE TABLE"" syntax",FLINK-28493,13470999,13470990,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,11/Jul/22 12:01,25/Aug/22 09:06,04/Jun/24 20:42,25/Aug/22 09:06,,,,,,1.16.0,,,Documentation,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 25 09:06:36 UTC 2022,,,,,,,,,,"0|z16rhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 02:59;337361684@qq.com;Hi [~godfreyhe] , could you assign this to me. Thanks!;;;","25/Aug/22 09:06;godfrey;Fixed in master: 7166625ff787181a59a10f4d75218b4ac87254ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support ""ANALYZE TABLE"" execution",FLINK-28492,13470994,13470990,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,11/Jul/22 11:50,04/Aug/22 02:32,04/Jun/24 20:42,04/Aug/22 02:32,,,,,,1.16.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 04 02:32:50 UTC 2022,,,,,,,,,,"0|z16rgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 02:32;godfreyhe;Fixed in master: 8236644816a8070499ff4327b7ee71cbb4660afa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce APPROX_COUNT_DISTINCT aggregate function for batch sql,FLINK-28491,13470992,13470990,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,11/Jul/22 11:49,22/Jul/22 04:39,04/Jun/24 20:42,22/Jul/22 04:39,,,,,,1.16.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 22 04:39:09 UTC 2022,,,,,,,,,,"0|z16rg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 04:39;godfreyhe;Fixed in master:
9db9e19aec1c6aea7298fe152699ec927e4f3e02
c1e66fce8c89316e4f3b5c432a7aecaa50096be4
559964a656c3f85870cfb0fab4f9bbe28e1e3081;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Introduce ""ANALYZE TABLE"" Syntax in sql parser",FLINK-28490,13470991,13470990,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,11/Jul/22 11:48,20/Jul/22 02:48,04/Jun/24 20:42,19/Jul/22 13:04,,,,,,1.16.0,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 19 13:04:40 UTC 2022,,,,,,,,,,"0|z16rg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 13:04;godfreyhe;Fixed in master: e54c86415b106f253b58c54670fe5b84b5c40f25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FLIP-240: Introduce ""ANALYZE TABLE"" Syntax",FLINK-28489,13470990,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,11/Jul/22 11:47,25/Aug/22 09:06,04/Jun/24 20:42,25/Aug/22 09:06,,,,,,1.16.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"see [FLIP doc|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217386481] for more details",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-11 11:47:15.0,,,,,,,,,,"0|z16rfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaMetricWrapper does incorrect cast,FLINK-28488,13470978,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,11/Jul/22 10:45,17/Aug/22 21:59,04/Jun/24 20:42,17/Aug/22 21:59,1.15.1,1.16.0,,,,1.15.3,1.16.0,,Connectors / Kafka,,,,,,,0,pull-request-available,,,,"Same as FLINK-27487, which unfortunately missed 1 of 2 wrapper classes ({{{}KafkaMetricWrapper{}}}).

This only affects the deprecated KafkaConsumer/-Producer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27487,,,,,,,,,,"29/Jul/22 03:32;cyc;image-2022-07-29-11-32-05-733.png;https://issues.apache.org/jira/secure/attachment/13047351/image-2022-07-29-11-32-05-733.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 17 21:58:32 UTC 2022,,,,,,,,,,"0|z16rd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 03:32;cyc;I kown how to fix this. and  I want to try , how can I commit my code 

!image-2022-07-29-11-32-05-733.png!;;;","17/Aug/22 21:58;chesnay;master: 3c2fa303074453d4f4c95c2290b9b773684dac61
1.15: 51b3752ac320a45a20738fea28c8fdd923061f1c ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce configurable RateLimitingStrategy for Async Sink,FLINK-28487,13470960,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,liangtl,liangtl,liangtl,11/Jul/22 08:58,14/Sep/22 07:44,04/Jun/24 20:42,09/Aug/22 15:25,,,,,,1.16.0,,,Connectors / Common,,,,,,,0,pull-request-available,,,,"Introduce a configurable RateLimitingStrategy to the AsyncSinkWriter.

This change will allow sink implementers using AsyncSinkWriter to configure their own RateLimitingStrategy instead of using the default AIMDRateLimitingStrategy.

See [FLIP-242: Introduce configurable RateLimitingStrategy for Async Sink|https://cwiki.apache.org/confluence/display/FLINK/FLIP-242%3A+Introduce+configurable+RateLimitingStrategy+for+Async+Sink].

 ",,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28027,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 14 06:02:11 UTC 2022,,,,,,,,,,"0|z16r94:",9223372036854775807,"Supports configurable RateLimitingStrategy for the AsyncSinkWriter. This change allows sink implementers to change the behaviour of an AsyncSink when requests fail, for a specific sink. If no RateLimitingStrategy is specified, it will default to current default of AIMDRateLimitingStrategy. ",,,,,,,,,,,,,,,,,,,"09/Aug/22 15:25;dannycranmer;Merged [{{f53dd03}}|https://github.com/apache/flink/commit/f53dd03fd5fe87a366de0da1fdef4ad88d5b20af] into apache:master ;;;","13/Sep/22 12:19;godfreyhe;[~liangtl] could you update the release note of this issue ?;;;","14/Sep/22 06:02;liangtl;Sure [~godfreyhe] , done!

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[docs-zh] Flink FileSystem SQL Connector Doc is not right,FLINK-28486,13470959,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chenbowen,chenbowen,chenbowen,11/Jul/22 08:52,14/Jul/22 03:09,04/Jun/24 20:42,14/Jul/22 03:09,1.15.1,,,,,1.15.2,1.16.0,,Documentation,,,,,,,0,pull-request-available,starter,,,"[https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/filesystem/]

English documents and Chinese documents are inconsistent.

For English doc:

!image-2022-07-11-16-48-12-692.png!

For Chinese doc:

!image-2022-07-11-16-51-18-166.png!",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/22 08:48;chenbowen;image-2022-07-11-16-48-12-692.png;https://issues.apache.org/jira/secure/attachment/13046528/image-2022-07-11-16-48-12-692.png","11/Jul/22 08:51;chenbowen;image-2022-07-11-16-51-18-166.png;https://issues.apache.org/jira/secure/attachment/13046527/image-2022-07-11-16-51-18-166.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 03:09:25 UTC 2022,,,,,,,,,,"0|z16r8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 09:20;chenbowen;Hi,

[~jark] [~mvisser@pivotal.io] , please review this issue and assign this ticket to me, thx!;;;","13/Jul/22 06:31;chenbowen;[~lzljs3620320]  Hi ,please review the new pr [#20255|https://github.com/apache/flink/pull/20255], thx for your suggestion!;;;","14/Jul/22 03:09;lzljs3620320;master: 2124280084243c5d46056dd6aa264f98beeaee4a
release-1.15: 23381ad4036ae4b6063c13e07a81c6b54fec1935;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Structure examples better and add README/doc,FLINK-28485,13470930,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,gyfora,gyfora,11/Jul/22 07:32,14/Jul/22 09:05,04/Jun/24 20:42,14/Jul/22 08:57,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"With the growing number of (more complex) examples, the examples directory is simply becoming very messy.

We should try to structure the examples better, separete simple yaml based operator examples from example projects and add a general README/doc page for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 08:57:14 UTC 2022,,,,,,,,,,"0|z16r2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 07:32;gyfora;cc [~mbalassi] ;;;","14/Jul/22 08:57;gyfora;merged to main 0854f82dfead22b689b32c78a64edcfaa518dc22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UniqueKey inference should respect the semantics of null as well as consider the possibility of precision loss from implicit conversions,FLINK-28484,13470924,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Bug,,lincoln.86xy,lincoln.86xy,11/Jul/22 07:04,13/Apr/23 01:29,04/Jun/24 20:42,13/Apr/23 01:29,1.15.1,,,,,,,,Table SQL / Planner,,,,,,,0,,,,,"FLINK-15313 introduces a new branch in UniqueKey inference of projection, but does not respect the semantics of null and does not consider the possible loss of precision due to implicit conversion, which may lead to a wrong determination of whether a column is unique or not, and consequently to a wrong execution plan.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Apr 13 01:29:09 UTC 2023,,,,,,,,,,"0|z16r14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 01:29;lincoln.86xy;This is not a bug after revisit the code;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic schema evolution for table store,FLINK-28483,13470918,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Jul/22 06:33,12/Jul/22 09:45,04/Jun/24 20:42,12/Jul/22 09:45,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Currently Flink 1.15 does not have the DDL for modifying table schema, but we can expose the most basic modifications of Schema at Catalog level and Spark read side, such as adding fields.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 12 09:45:27 UTC 2022,,,,,,,,,,"0|z16qzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 09:45;lzljs3620320;master: a73350c3694d78f3bff4900c8be7f3f02983b6e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
num-sorted-run.stop-trigger introduced a unstable merging,FLINK-28482,13470917,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,11/Jul/22 06:29,20/Jul/22 10:22,04/Jun/24 20:42,20/Jul/22 10:19,,,,,,table-store-0.2.0,table-store-0.3.0,,Table Store,,,,,,,0,pull-request-available,,,,"We have introduced the num-sorted-run.stop-trigger. The default value is 10.
This means that the maximum number of runs generated is 10, and 10 runs may be merged at the same time during compaction or read.
Reading 10 ORC files at the same time may lead to OOM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 20 10:19:53 UTC 2022,,,,,,,,,,"0|z16qzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 06:42;lzljs3620320;Maybe we should introduce a compaction.max-input-runs;;;","20/Jul/22 10:19;lzljs3620320;master: 4cb870b74662fc22ea25fbc29cacf0b14f78a4bf
release-0.2: 95bb7a54d62e6b02492d33381cfb9d73ee2665fe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump the fabric8 kubernetes-client to 5.12.3,FLINK-28481,13470895,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bzhaoop,wangyang0918,wangyang0918,11/Jul/22 02:41,10/Aug/22 02:59,04/Jun/24 20:42,10/Aug/22 02:59,,,,,,1.16.0,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,,,,"The current fabric8 kubernetes-client(5.5.0) will swallow the {{KubernetesClientException}} and then the next renewing could not work properly until reach the deadline. This will be a serious problem because one time failure of renewing leader annotation will cause leadership lost.

 

Refer to the following ticket for more information.

https://github.com/fabric8io/kubernetes-client/issues/4246",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28832,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 10 02:59:21 UTC 2022,,,,,,,,,,"0|z16quo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 10:16;yunta;From the implementation of [LeaderElector#renew|https://github.com/fabric8io/kubernetes-client/blob/v4.9.2/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L118-L125] at 4.9.2 version. I think this bug also exists for flink-1.13, right?;;;","20/Jul/22 12:25;wangyang0918;Yes. If you believe it is necessary, we also need to push the upstream project to release a hotfix version for v4.9.;;;","28/Jul/22 02:39;bzhaoop;Let's waiting for the latest version(v5.12.3) mvn pkg from fabric8 upstream. Looks the v5.12.3 was just released 13 hours. May I contribute a PR as a volunteer?

 

Thanks;;;","28/Jul/22 02:42;wangyang0918;Thanks for volunteering. I have assigned this ticket to you. Happy coding.

Please remember to update the dependencies in the NOTICE file.;;;","28/Jul/22 02:44;bzhaoop;Cool, thank you very much.;;;","10/Aug/22 02:59;wangyang0918;Fixed via:

master: f2ec01241c5e5bcb860180dae5c0a660bc816926;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forward timeControllerExecution time as histogram for JOSDK Metrics interface,FLINK-28480,13470881,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,gyfora,gyfora,10/Jul/22 17:36,12/Jul/22 09:37,04/Jun/24 20:42,12/Jul/22 09:37,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently the JOSDK metrics forwarder logic doesn't implement the timeControllerExecution function.  We should implement it with the following logic.

1. Measure execution time for successful failed executions
2. Based on the name of the ControllerExectution (reconcile/cleanup) and controller name, track the following histogram metrics metrics

JOSDK.\{ControllerExecution.controllerName}.\{ControllerExecution.name}.\{ControllerExecution.successTypeName}/failed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 12 09:37:01 UTC 2022,,,,,,,,,,"0|z16qrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 07:26;nicholasjiang;[~gyfora], please help to assign this ticket to me. I have pushed a PR for this ticket.;;;","12/Jul/22 09:37;gyfora;merged to main b320516e40aba988c85135a5b6510b43f289ccd1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add latency histogram for resource state transitions,FLINK-28479,13470829,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,10/Jul/22 07:27,13/Jul/22 11:51,04/Jun/24 20:42,13/Jul/22 11:51,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"We should track how long operator lifecycle transitions take, between states:



SUSPENDED,
UPGRADING,
DEPLOYED,
STABLE,
ROLLING_BACK,
ROLLED_BACK,
FAILED",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 11:51:13 UTC 2022,,,,,,,,,,"0|z16qg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 11:51;gyfora;merged to main 924b4f0fb04eb6ca26dcedf83c1a852fc81807a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session Cluster will lost if it failed between status recorded and deploy,FLINK-28478,13470825,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,aitozi,aitozi,10/Jul/22 04:01,15/Jul/22 06:19,04/Jun/24 20:42,15/Jul/22 06:19,kubernetes-operator-1.1.0,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"When I test case with https://issues.apache.org/jira/browse/FLINK-28187 
I hit that the session cluster deploy can not be deployed if it fails between status recorded and deploy. Because, in the next reconcile loop, the spec is not detected changed by {{checkNewSpecAlreadyDeployed}}, so it will not try to start the session cluster again. 

The application mode have no problem, because the deployed spec SUSPEND state of the job is not equal to the desired state, so it will try to reconcile the spec change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 15 06:19:51 UTC 2022,,,,,,,,,,"0|z16qf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/22 04:03;aitozi;cc [~gyfora];;;","15/Jul/22 06:19;gyfora;Merged to main a6430378553d7405a037f948576bea5ef5be55ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to scan the history server dir with max depth,FLINK-28477,13470824,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,aitozi,aitozi,10/Jul/22 02:17,11/Jul/22 03:37,04/Jun/24 20:42,11/Jul/22 03:37,,,,,,,,,,,,,,,,0,,,,,"In the {{HistoryServerArchiveFetcher}}, we only list the archives specified by the historyserver.archive.fs.dir, but we have an extra hierarchy for the jobs, like:

/base/job_platform_id1/jobid1
/base/job_platform_id1/jobid2 
/base/job_platform_id2/jobid3

For this use case, we can not provide the full dir for history server to scan. So, we want to set the dir {{/base}} to scan. Can we support the HistoryServer to list the dir with max depth, I think it will make it more flexible for different usage. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 03:37:10 UTC 2022,,,,,,,,,,"0|z16qew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jul/22 02:21;aitozi;cc [~guoyangze] [~xtsong];;;","11/Jul/22 02:11;xtsong;-1 to this change.

At the cost of complexity, I don't see how this change benefit Flink's general use cases. The example where archives are grouped into sub-directories is quite specific to your platforms.;;;","11/Jul/22 03:37;aitozi;Got it, closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics for Kubernetes API server access,FLINK-28476,13470815,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,09/Jul/22 16:11,24/Nov/22 01:02,04/Jun/24 20:42,12/Jul/22 13:21,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"e.g.:
 * http response counter
 * http response latency histogram
 * http response status counter",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 12 13:21:41 UTC 2022,,,,,,,,,,"0|z16qcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 13:21;gyfora;merged to main e8f9e03396255efa3fab40549389a6969141d36d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka connector won't stop when the stopping offset is zero,FLINK-28475,13470810,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Leo zhang,Leo zhang,Leo zhang,09/Jul/22 14:30,16/Oct/23 06:50,04/Jun/24 20:42,16/Oct/23 06:50,1.15.0,,,,,1.17.0,,,Connectors / Kafka,,,,,,,0,pull-request-available,stale-assigned,,,"when use kafka connector in bounded mode,and the stopping offset hapends to be 0,the kafka connector won't stop,which is not expected.

I had traced the code, and found the stopping offset will be set to empty when it is zero, and an empty stopping offset means no stopping offset when serialized. This leads to a wrong execution.

I had fixed this in my personal branch,now I am logging this issue in Jira so that I can make merge request.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 15 22:35:01 UTC 2023,,,,,,,,,,"0|z16qbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 02:22;renqs;Merged to master: 55fc7fa95aae4304a8694c03adf79d0a609918b1;;;","10/Feb/23 12:41;martijnvisser;[~renqs] Can this ticket be closed? I believe this is ending up in 1.17, but the other PRs should be closed? ;;;","15/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChannelStateWriteResult may not fail after checkpoint abort,FLINK-28474,13470809,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,09/Jul/22 14:23,07/Oct/22 08:39,04/Jun/24 20:42,07/Oct/22 07:02,1.14.5,1.15.1,,,,1.17.0,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,"After Checkpoint abort, ChannelStateWriteResult should fail.

But if _channelStateWriter.start(id, checkpointOptions);_ is executed after Checkpoint abort, ChannelStateWriteResult will not fail.

 
h2. Cause Analysis:

When abort checkpoint, channelStateWriter.start(id, checkpointOptions); may not be executed yet. These checkpointIds will be stored in the abortedCheckpointIds of SubtaskCheckpointCoordinatorImpl, and when checkpointState is called, it will check if the checkpointId should be aborted.

_ChannelStateWriter.abort(checkpointId, exception, true) should also be executed here._

The unit test can reproduce this bug.

!image-2022-07-09-22-21-24-417.png|width=803,height=307!

 

Note: channelStateWriter.abort is only called in notifyCheckpointAborted, it doesn't account for channelStateWriter.start after notifyCheckpointAborted.

JIRA: FLINK-17869

commit: https://github.com/apache/flink/pull/12478/commits/22c99845ef4f863f1753d17b109fd2faecc8201e

 

The bug will affect the new feature FLINK-26803, because the channel state file can be closed only after the Checkpoints of all tasks of the shared file are complete or abort. So when the checkpoint of some tasks fails, if abort is not called, the file cannot be closed and all tasks sharing the file cannot execute inputChannelStateHandles.completeExceptionally(e); and resultSubpartitionStateHandles.completeExceptionally(e); , AsyncCheckpointRunnable will wait forever.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26803,,,,,,,,,,"09/Jul/22 14:21;fanrui;image-2022-07-09-22-21-24-417.png;https://issues.apache.org/jira/secure/attachment/13046501/image-2022-07-09-22-21-24-417.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Oct 07 08:39:01 UTC 2022,,,,,,,,,,"0|z16qbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 07:01;fanrui;Hi [~pnowojski] , I have submitted this PR, could you help take a look in your free time ? This bug sometimes cause the CI of FLINK-26803 failed.;;;","07/Oct/22 07:02;pnowojski;Merged to master as a6119121164^..a6119121164

Thanks for the fix [~fanrui].

I think there is no need to back port it to the previous releases, as this bug doesn't cause any symptoms without FLINK-26803;;;","07/Oct/22 08:39;fanrui;[~pnowojski]  thanks for your help and review~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager restart/failover doesn't trigger local recovery on TaskManagers,FLINK-28473,13470797,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lkokhreidze,lkokhreidze,09/Jul/22 10:51,25/Jul/22 10:55,04/Jun/24 20:42,,1.15.0,1.15.1,,,,,,,Runtime / State Backends,Runtime / Task,,,,,,0,,,,,"Hi! While experimenting with local recovery feature (Flink 1.15.1) I noticed that if JobManager is restarted TaskManagers always recover from Remote ({{{}IncrementalRemoteKeyedStateHandle{}}}). While if I restart task managers, local recovery is triggered.
 
Setup: * HA setup with Zookeeper and S3 remote storage.
 * JobManager runs as StatefulSet with PersistentVolume. Both {{process.jobmanager.working-dir}} and {{jobmanager.resource-id}} are correctly configured.
 * TaskManagers run as StatefulSets with PersistentVolume. Both {{process.taskmanager.working-dir}} and {{taskmanager.resource-id}} are correctly configured.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 25 10:55:00 UTC 2022,,,,,,,,,,"0|z16q94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/22 10:52;lkokhreidze;Hi [~chesnay] 

Sorry for the ping. But was wondering if this is a known limitation. It's not really clear from documentation if JobManager restart/failover should trigger Local recovery or not.

Thanks in advance.;;;","11/Jul/22 08:55;Ming Li;Hi，[~lkokhreidze] I think after JM failover, it may be difficult for us to know where the previous task was deployed, so local recovery is difficult to achieve in the current situation.;;;","11/Jul/22 09:03;lkokhreidze;Hi [~Ming Li] thanks for the comment.

Wondering as a future improvement (FLIP), when using HA mode this could be achieved by storing allocation metadata as leader info. So during the failover new leader could try to use the previous leader's information.

Of course it will be more complex than that, just thinking out loud because I guess this will be great improvement for folks who deploy JM on dynamic environment like Kubernetes.;;;","11/Jul/22 09:05;chesnay;I don't think JM failover is currently supported. The reason is that after a JM failover the IDs that identify tasks are regenerated, so any information that the TMs could supply isn't usable by the JM.;;;","11/Jul/22 09:50;Ming Li;[~chesnay], at present, after JM failover, all tasks must be restarted. Can we record the previous allocation as [~lkokhreidze] said? Going further, maybe the [Flink-7641|https://issues.apache.org/jira/browse/FLINK-7641] is a more ideal workaround.;;;","25/Jul/22 10:55;lkokhreidze;Hi [~chesnay] [~Ming Li] 

Do you think this is something worth working on? I can try formulating a FLIP and starting the discussing thread around this.

But of course wouldn't want to waste anyone's time if you think it is not worth pursuing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JmStatus as a group for JobManagerDeploymentStatus count metrics,FLINK-28472,13470790,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,morhidi,gyfora,gyfora,09/Jul/22 08:56,24/Nov/22 01:02,04/Jun/24 20:42,12/Jul/22 09:45,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently the metric simply looks like

...FlinkDeployment.READY.Count

We should add a group to clarify that READY is the status of the Jm deployment. This will make it possible to add other status count metrics in the future:

FlinkDeployment.JmStatus.READY.Count


So in the future we can track other states with similar counts:

FlinkDeployment.JobStatus.RUNNING
...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 12 09:45:35 UTC 2022,,,,,,,,,,"0|z16q7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 09:45;gyfora;merged to main 4d19d8bfcd5ae92e0314cef63efce73d082db969;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate hive_read_write.md into Chinese,FLINK-28471,13470742,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,eyys,eyys,eyys,08/Jul/22 17:54,29/Jul/22 12:40,04/Jun/24 20:42,29/Jul/22 12:40,,,,,,1.16.0,,,Connectors / Hive,Documentation,,,,,,0,pull-request-available,,,,"Translate hive_read_write.md into Chinese

将 hive_read_write.md翻译成中文",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/22 08:11;eyys;[FLINK-28471] Translate hive_read_write.md into Chinese - ASF JIRA.url;https://issues.apache.org/jira/secure/attachment/13046496/%5BFLINK-28471%5D+Translate+hive_read_write.md+into+Chinese+-+ASF+JIRA.url",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 29 12:40:32 UTC 2022,,,,,,,,,,"0|z16pww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 01:58;luoyuxia;[~eyys] Thanks for your contribution. I'll find some time to help review and find someone to merge.;;;","11/Jul/22 11:20;eyys;Thank you. I'll be more involved in the community in the future;;;","29/Jul/22 12:40;jark;Resolved in master: a1ba512e70538dab094cddd47fae7a56371bba6c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show source watermarks on dashboard,FLINK-28470,13470731,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sandys-Lumsdaine,Sandys-Lumsdaine,08/Jul/22 16:12,08/Jul/22 16:12,04/Jun/24 20:42,,,,,,,,,,Runtime / Web Frontend,,,,,,,0,,,,,"All the operators on the dashboard show the low-watermark which is useful to see at a glance where a blockage might be.

Can the same be added to sources (or at least legacy sources)? Currently I have to click on the node and add the ""output watermark"" metric. Would be useful to see source watermarks at a glance too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-08 16:12:03.0,,,,,,,,,,"0|z16pug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting a timer within broadcast applyToKeyedState(),FLINK-28469,13470730,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sandys-Lumsdaine,Sandys-Lumsdaine,08/Jul/22 16:09,03/Nov/22 08:00,04/Jun/24 20:42,,1.16.0,,,,,,,,API / DataStream,,,,,,,0,,,,,"I know we can’t set a timer in the processBroadcastElement() of the KeyedBroadcastProcessFunction as there is no key.

 

However, there is a context.applyToKeyedState() method which allows us to iterate over the keyed state in the scope of a key. So it is possible to add access to the TimerService onto the Context parameter passed into that delegate?

 

Since the code running in the applyToKeyedState() method is scoped to a key we should be able to set up timers for that key too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Nov 03 08:00:02 UTC 2022,,,,,,,,,,"0|z16pu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 08:00;gaoyunhaii;Hi [~Sandys-Lumsdaine]  logically it is a bit hard to achieve that since the timer service relies on the a global current key, but applyToKey does not modify the current key. May I have a double confirmation of why you need this functionality?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating benchmark channel in Apache Flink slack,FLINK-28468,13470719,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Yanfei Lei,akalashnikov,akalashnikov,08/Jul/22 14:47,25/Oct/22 10:09,04/Jun/24 20:42,25/Oct/22 10:08,,,,,,,,,,,,,,,,0,,,,,"It needs to create slack channel to which the result of benchmarks will be sent.
Steps:
* Create `dev-benchmarks` channel(with the help of admin of Apache Flink slack needed)
* Create `Slack app` for sending benchmark results to slack(admin help as well).
* Configure existing Jenkins with the new `Slack app`'s token to send the result of benchmarks to this new channel.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 25 10:08:32 UTC 2022,,,,,,,,,,"0|z16prs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/22 10:08;xtsong;Thanks [~Yanfei Lei] for setting this up.

The benchmark reports are now available on the slack channel #flink-dev-benchmarks
https://apache-flink.slack.com/archives/C0471S0DFJ9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Table API Avro format fails during schema evolution,FLINK-28467,13470692,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jaya.ananthram,jaya.ananthram,08/Jul/22 12:19,10/Jul/22 13:41,04/Jun/24 20:42,,1.14.4,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,,,,,"It looks like the Flink Table API fails during the schema evolution. ie - when we add a new optional field in the producer side, the consumer (Flink table API) fails to parse the message with the old schema. Following are the exact scenario to reproduce,
 # Create a schema X with two field
 # Send five messages to Kafka using schema X
 # Update the schema X to add one new optional field with default NULL (at last position)
 # Send five messages to Kafka using schema Y
 # Create a Flink SQL job to consume all the 10 messages using schema X (with two fields)
 # Exactly it will fail at the 7th message to get the exception *_Malformed data. Length is negative: -xx_* (the 6th message will pass successfully though).

The complete stack trace is available below,
{code:java}
Caused by: org.apache.avro.AvroRuntimeException: Malformed data. Length is negative: -56	at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:285)	at org.apache.avro.io.ResolvingDecoder.readString(ResolvingDecoder.java:208)	at org.apache.avro.generic.GenericDatumReader.readString(GenericDatumReader.java:469)	at org.apache.avro.generic.GenericDatumReader.readString(GenericDatumReader.java:459)	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:191)	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:160)	at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:259)	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:247)	at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:179)	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:160)	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)	at org.apache.flink.formats.avro.AvroDeserializationSchema.deserialize(AvroDeserializationSchema.java:142)	at org.apache.flink.formats.avro.AvroRowDataDeserializationSchema.deserialize(AvroRowDataDeserializationSchema.java:103)	... 19 more {code}
 

From [Avro specification|https://avro.apache.org/docs/1.10.0/spec.html] (refer to the section ""Single object Encoding"" or the attached image), it looks by default Avro provides the schema evolution support, so, as an end-user I expect the Flink table API to provide the same functionalities. I can also confirm that the old schema is able to decode all the 10 messages outside of Flink (ie - using simple hello world AvroDeserializer)

I am adding the root cause as a comment, as I am not exactly sure whether my finding is correct.

*Note:* I am marking this as a Major ticket as we have another open ticket ([here|https://issues.apache.org/jira/browse/FLINK-20091]) which lacks the functionalities to ignore failures. This means, even if the user is ready to miss some message (when a batch contains two types of messages), still we can't specify the property to ignore it (to update the DDL once it crosses the schema switch over messages). So it looks like the Avro table API can't be used in PROD when we expect to change the schema. I assume most of the cases, we are expected to change schema. If the severity is not correct please feel free to reduce the priority.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/22 12:01;jaya.ananthram;image-2022-07-08-15-01-49-648.png;https://issues.apache.org/jira/secure/attachment/13046452/image-2022-07-08-15-01-49-648.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 12:28:19 UTC 2022,,,,,,,,,,"0|z16pls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 12:28;jaya.ananthram;*Root cause:* It looks like the byte buffer is not cleared properly when the message contains a new field that is not in the schema. So, as a result, the 7th message contains a partial 6th message (technically the new field) + a partial 7th message. This happens due to resuing the object [binaryDecoder|https://github.com/apache/flink/blob/master/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroDeserializationSchema.java#L165]. If I create a new binaryDecoder object for each message, then it works fine in my local. Not 100% sure though, hence marking it as a comment.

 

*Note:* For the test case, we need to make sure to add a scenario for at least two messages during schema evolution as one message is parsed successfully after a schema change with the current bug.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update assertj to 3.23.1,FLINK-28466,13470681,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Jul/22 11:06,05/Apr/23 07:47,04/Jun/24 20:42,11/Jul/22 11:12,,,,,,1.16.0,,,Build System,Tests,,,,,,0,pull-request-available,,,,"Among others there is a performance improvement for {{containsExactly}} at https://github.com/assertj/assertj-core/issues/2548

the full release notes are available at https://assertj.github.io/doc/#assertj-core-3-23-1-release-notes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 11:12:38 UTC 2022,,,,,,,,,,"0|z16pjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 11:12;chesnay;master: 9c3d9be628cb6f9dcef9f9c1ebc3a4a4a32d3145;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wait compaction finished in endInput,FLINK-28465,13470665,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,08/Jul/22 09:53,13/Jul/22 07:06,04/Jun/24 20:42,13/Jul/22 07:06,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"The batch job should wait for asynchronized compact task to finish, so this config should always be true.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 07:06:34 UTC 2022,,,,,,,,,,"0|z16pfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 09:54;qingyue;[~lzljs3620320] please assign the ticket to me;;;","13/Jul/22 07:06;lzljs3620320;master: a407dd9cfd7ee62cfaed2debd0dc2fc21b69b191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support CsvReaderFormat in PyFlink,FLINK-28464,13470651,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,08/Jul/22 08:44,18/Jul/22 01:49,04/Jun/24 20:42,18/Jul/22 01:49,,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 18 01:49:46 UTC 2022,,,,,,,,,,"0|z16pco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 01:49;dianfu;Merged to master via cef3aa136edb555b229950fb09067e042dd4361f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL syntax support CREATE TABLE AS SELECT(CTAS),FLINK-28463,13470650,13436816,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,08/Jul/22 08:44,29/Jul/22 10:11,04/Jun/24 20:42,29/Jul/22 10:11,,,,,,1.16.0,,,,,,,,,,0,pull-request-available,,,,"Support CTAS(CREATE TABLE AS SELECT) syntax
{code:java}
CREATE TABLE [ IF NOT EXISTS ] table_name 
[ WITH ( table_properties ) ]
[ AS query_expression ] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 29 10:11:25 UTC 2022,,,,,,,,,,"0|z16pcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 10:11;jark;Fixed in master: ed8d3acf454e62edae458496473a6cf9c20daa24
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table API support CREATE TABLE AS SELECT(CTAS),FLINK-28462,13470649,13436816,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,tartarus,tartarus,tartarus,08/Jul/22 08:42,22/Jul/22 02:38,04/Jun/24 20:42,,,,,,,,,,,,,,,,,0,,,,,"{code:java}
TablePipeline tablePipeline = table.saveAs(""my_ctas_table"")
        .option(""connector"", ""filesystem"")
        .option(""format"", ""testcsv"")
        .option(""path"", ""/tmp/my_ctas_table/"")
        .create(true);
tablePipeline.execute(); {code}
Users can use CTAS with Table API",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-08 08:42:14.0,,,,,,,,,,"0|z16pc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink Table should add get_resolved_schema method,FLINK-28461,13470646,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,bzhaoop,xuannan,xuannan,08/Jul/22 08:36,07/Sep/22 07:12,04/Jun/24 20:42,,1.15.1,,,,,,,,API / Python,,,,,,,0,,,,,"The Table#getSchema method is deprecated and replaced with the Table#getResolvedSchema. 

We should add the get_resolved_schema method to the PyFlink Table.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Sep 07 07:12:35 UTC 2022,,,,,,,,,,"0|z16pbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 09:56;bzhaoop;May I file a PR for this? Looks like a lower hanging fruit for the newbee pyflink. Like me. ;);;;","07/Sep/22 07:12;hxb;Thanks [~bzhaoop] . I have assigned it to you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL supports atomic CREATE TABLE AS SELECT(CTAS),FLINK-28460,13470645,13436816,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,08/Jul/22 08:34,12/Jul/23 01:46,04/Jun/24 20:42,12/Jul/23 01:46,,,,,,,,,,,,,,,,0,,,,,"Enable support for atomic CTAS in stream and batch mode via option.
Active deletion of the created target table when the job fails or is cancelled, but requires Catalog can be serialized directly via Java Serialization",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32349,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 15 11:03:05 UTC 2023,,,,,,,,,,"0|z16pbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 11:03;tartarus;Resolved in FLIP-305, for details see https://issues.apache.org/jira/browse/FLINK-32349;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL supports non-atomic CREATE TABLE AS SELECT(CTAS),FLINK-28459,13470643,13436816,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,08/Jul/22 08:30,04/Aug/22 14:51,04/Jun/24 20:42,04/Aug/22 14:51,,,,,,1.16.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"Non-atomic CTAS support in Streaming and Batch modes.
Does not actively delete created target tables when a job fails or is cancelled",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 04 14:51:21 UTC 2022,,,,,,,,,,"0|z16paw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 14:51;jark;Fixed in master: bc29e574105a2e635f8f8b9a4ad8f251dfb89321;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document append-only mode,FLINK-28458,13470642,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,lzljs3620320,lzljs3620320,08/Jul/22 08:29,20/Jul/22 03:05,04/Jun/24 20:42,20/Jul/22 03:05,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 20 03:05:48 UTC 2022,,,,,,,,,,"0|z16pao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 03:05;lzljs3620320;master: 81eb8830e22b0e8411fbc9296229d0e8a3343a0e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink add JobStatusHook support,FLINK-28457,13470641,13436816,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,08/Jul/22 08:27,14/Jul/22 04:09,04/Jun/24 20:42,14/Jul/22 04:09,,,,,,1.16.0,,,,,,,,,,0,pull-request-available,,,,"{code:java}
/**
 * Hooks provided by users on job status changing.
 */
@Internal
public interface JobStatusHook extends Serializable {

    /** When Job become CREATED status. It would only be called one time. */
    void onCreated(JobID jobId);
 
    /** When job finished successfully. */
    void onFinished(JobID jobId);

    /** When job failed finally. */
    void onFailed(JobID jobId, Throwable throwable);

    /** When job get canceled by users. */
    void onCanceled(JobID jobId);
} {code}
Introduce JM-side execution state change hook",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 04:09:29 UTC 2022,,,,,,,,,,"0|z16pag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 04:09;gaoyunhaii;Merged on master via  ed236ee19d7bcaa0e09eff5875880e8e6326acaf.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document streaming query page,FLINK-28456,13470632,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,08/Jul/22 07:26,11/Jul/22 09:08,04/Jun/24 20:42,11/Jul/22 09:08,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 09:08:25 UTC 2022,,,,,,,,,,"0|z16p8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 09:08;lzljs3620320;master: 84e9130827c09bf5338978b8f92bed4c6220b204;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyflink tableResult collect result to local  timeout,FLINK-28455,13470624,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhou_yb,zhou_yb,08/Jul/22 06:21,21/Aug/23 22:35,04/Jun/24 20:42,,1.13.0,,,,,,,,API / Python,,,,,,,0,auto-deprioritized-major,flink,pyflink,,"when I used pyflink do this:

 
{code:java}
with party_enter_final_result.execute().collect() as results:
     for result in results:{code}
sometimes TimeoutException occured，the Exception as following:
{code:java}
[2022-07-07 01:18:55,843] {bash.py:173} INFO - Job has been submitted with JobID 017de55acf2a71552fc293626cfbbe67
[2022-07-07 01:20:02,384] {bash.py:173} INFO - Traceback (most recent call last):
[2022-07-07 01:20:02,384] {bash.py:173} INFO -   File ""/opt/airflow/data/repo/dags/chloe/chloe_counter.py"", line 80, in <module>
[2022-07-07 01:20:02,384] {bash.py:173} INFO -     main(date)
[2022-07-07 01:20:02,384] {bash.py:173} INFO -   File ""/opt/airflow/data/repo/dags/chloe/chloe_counter.py"", line 53, in main
[2022-07-07 01:20:02,384] {bash.py:173} INFO -     for result in results:
[2022-07-07 01:20:02,384] {bash.py:173} INFO -   File ""/space/flink/opt/python/pyflink.zip/pyflink/table/table_result.py"", line 236, in __next__
[2022-07-07 01:20:02,384] {bash.py:173} INFO -   File ""/space/flink/opt/python/py4j-0.10.8.1-src.zip/py4j/java_gateway.py"", line 1285, in __call__
[2022-07-07 01:20:02,384] {bash.py:173} INFO -   File ""/space/flink/opt/python/pyflink.zip/pyflink/util/exceptions.py"", line 147, in deco
[2022-07-07 01:20:02,384] {bash.py:173} INFO -   File ""/space/flink/opt/python/py4j-0.10.8.1-src.zip/py4j/protocol.py"", line 326, in get_return_value
[2022-07-07 01:20:02,384] {bash.py:173} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o66.hasNext.
[2022-07-07 01:20:02,384] {bash.py:173} INFO - : java.lang.RuntimeException: Failed to fetch next result
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at java.lang.reflect.Method.invoke(Method.java:498)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
[2022-07-07 01:20:02,385] {bash.py:173} INFO -     at java.lang.Thread.run(Thread.java:748)
[2022-07-07 01:20:02,385] {bash.py:173} INFO - Caused by: java.io.IOException: Failed to fetch job execution result
[2022-07-07 01:20:02,386] {bash.py:173} INFO -     at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:177)
[2022-07-07 01:20:02,386] {bash.py:173} INFO -     at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:120)
[2022-07-07 01:20:02,386] {bash.py:173} INFO -     at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
[2022-07-07 01:20:02,386] {bash.py:173} INFO -     ... 13 more
[2022-07-07 01:20:02,386] {bash.py:173} INFO - Caused by: java.util.concurrent.TimeoutException
[2022-07-07 01:20:02,386] {bash.py:173} INFO -     at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1771)
[2022-07-07 01:20:02,386] {bash.py:173} INFO -     at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
[2022-07-07 01:20:02,386] {bash.py:173} INFO -     at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175)
[2022-07-07 01:20:02,386] {bash.py:173} INFO -     ... 15 more
[2022-07-07 01:20:02,386] {bash.py:173} INFO - 
[2022-07-07 01:20:02,450] {bash.py:173} INFO - org.apache.flink.client.program.ProgramAbortException: java.lang.RuntimeException: Python process exits with code: 1   {code}
Help，How to fix this problem?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 21 22:35:21 UTC 2023,,,,,,,,,,"0|z16p6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 07:54;gaoyunhaii;[~hxb]  could you help to have a look at this issue?;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the wrong timestamp example of KafkaSource,FLINK-28454,13470617,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,08/Jul/22 05:05,13/Jul/22 08:39,04/Jun/24 20:42,13/Jul/22 07:43,1.13.6,1.14.5,1.15.1,,,1.14.6,1.15.2,1.16.0,Connectors / Kafka,Documentation,,,,,,0,pull-request-available,,,,"[https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/kafka/]

 

The timestamp unit of startingOffset is second in kafkaSource doc, but it should be milliseconds. It will mislead flink users.

 

!image-2022-07-08-13-04-59-993.png!

 

 

It should be milliseconds, because the timestamp is used in  OffsetSpec.fromTimestamp() , and the comment is : *@param timestamp in milliseconds*

 

By the way, the timestamp is milliseconds in old source doc. link: [https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/datastream/kafka/#kafka-consumers-start-position-configuration]

 

!image-2022-07-08-13-46-05-540.png!

 

!image-2022-07-08-13-42-49-902.png|width=1103,height=970!

 

!image-2022-07-08-13-43-14-278.png|width=697,height=692!

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22722,,,,,,,,,,,,,,,,"08/Jul/22 05:05;fanrui;image-2022-07-08-13-04-59-993.png;https://issues.apache.org/jira/secure/attachment/13046429/image-2022-07-08-13-04-59-993.png","08/Jul/22 05:42;fanrui;image-2022-07-08-13-42-49-902.png;https://issues.apache.org/jira/secure/attachment/13046430/image-2022-07-08-13-42-49-902.png","08/Jul/22 05:43;fanrui;image-2022-07-08-13-43-14-278.png;https://issues.apache.org/jira/secure/attachment/13046431/image-2022-07-08-13-43-14-278.png","08/Jul/22 05:46;fanrui;image-2022-07-08-13-46-05-540.png;https://issues.apache.org/jira/secure/attachment/13046432/image-2022-07-08-13-46-05-540.png",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 01:58:18 UTC 2022,,,,,,,,,,"0|z16p54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 01:58;renqs;Merged to master: ce56e01aeaa6ebd3b350fe089a952f238742823c
release-1.15: 9ffb7963e90b7bdcfbedaf8ef9ede4e0d7089fe8
release-1.14: 04bb6baa539d59052c1e038c92c339a9b5478726;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceLegacyITCase.testBrokerFailure hang on azure,FLINK-28453,13470609,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,hxbks2ks,hxbks2ks,08/Jul/22 02:50,30/Sep/22 08:42,04/Jun/24 20:42,11/Jul/22 07:28,1.16.0,,,,,,,,Connectors / Kafka,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-07-07T17:55:16.4876240Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fd0b000b800 nid=0x258c waiting on condition [0x00007fd0b77f2000]
2022-07-07T17:55:16.4876830Z    java.lang.Thread.State: WAITING (parking)
2022-07-07T17:55:16.4877232Z 	at sun.misc.Unsafe.park(Native Method)
2022-07-07T17:55:16.4877916Z 	- parking to wait for  <0x00000000a46ffae8> (a java.util.concurrent.CompletableFuture$Signaller)
2022-07-07T17:55:16.4878495Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2022-07-07T17:55:16.4879100Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2022-07-07T17:55:16.4879714Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2022-07-07T17:55:16.4880318Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2022-07-07T17:55:16.4880921Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-07-07T17:55:16.4881505Z 	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:67)
2022-07-07T17:55:16.4882182Z 	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runBrokerFailureTest(KafkaConsumerTestBase.java:1506)
2022-07-07T17:55:16.4883003Z 	at org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase.testBrokerFailure(KafkaSourceLegacyITCase.java:94)
2022-07-07T17:55:16.4883648Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-07T17:55:16.4884261Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-07T17:55:16.4884907Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-07T17:55:16.4885487Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-07T17:55:16.4886039Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-07T17:55:16.4886697Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-07T17:55:16.4887406Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-07T17:55:16.4888051Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-07T17:55:16.4888678Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-07T17:55:16.4889330Z 	at org.apache.flink.testutils.junit.RetryRule$RetryOnFailureStatement.evaluate(RetryRule.java:135)
2022-07-07T17:55:16.4889974Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-07T17:55:16.4890554Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-07T17:55:16.4891101Z 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-07T17:55:16.4891705Z 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-07T17:55:16.4892306Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-07T17:55:16.4892901Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-07T17:55:16.4893525Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-07T17:55:16.4894166Z 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-07T17:55:16.4894712Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-07T17:55:16.4895267Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-07T17:55:16.4895913Z 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-07T17:55:16.4896468Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-07T17:55:16.4897105Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-07T17:55:16.4897722Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-07T17:55:16.4898385Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-07T17:55:16.4898967Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-07T17:55:16.4899507Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-07T17:55:16.4900014Z 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-07T17:55:16.4900557Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-07T17:55:16.4901061Z 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-07T17:55:16.4901555Z 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-07T17:55:16.4902127Z 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-07T17:55:16.4902780Z 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-07T17:55:16.4903409Z 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-07T17:55:16.4904156Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-07T17:55:16.4904915Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-07T17:55:16.4905695Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-07T17:55:16.4906427Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator$$Lambda$200/1902430796.accept(Unknown Source)
2022-07-07T17:55:16.4907238Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-07T17:55:16.4908039Z 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-07T17:55:16.4908719Z 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-07T17:55:16.4909362Z 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-07T17:55:16.4910081Z 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-07T17:55:16.4910837Z 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-07T17:55:16.4911572Z 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-07T17:55:16.4912342Z 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-07T17:55:16.4913098Z 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-07T17:55:16.4913845Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-07T17:55:16.4914491Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-07T17:55:16.4915098Z 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-07T17:55:16.4915692Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37848&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 02:27:04 UTC 2022,,,,,,,,,,"0|z16p3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 02:50;hxbks2ks;cc [~renqs];;;","11/Jul/22 02:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Reduce"" and ""aggregate"" operations of ""window_all"" are supported in Python datastream API",FLINK-28452,13470604,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cun8cun8,cun8cun8,cun8cun8,08/Jul/22 01:12,20/Jul/22 10:23,04/Jun/24 20:42,20/Jul/22 10:23,,,,,,1.16.0,,,API / DataStream,API / Python,,,,,,0,pull-request-available,,,,"The window allocator based on ""window_all"" adds ""reduce"" and ""aggregate"" operations to align the reduce and aggregate already supported in the Java API",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 20 10:23:30 UTC 2022,,,,,,,,,,"0|z16p28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 10:23;dianfu;Merged to master via efff43cb2740db8b24d295299a2f73451a56658f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveFunctionDefinitionFactory load udf using user classloader instead of thread context classloader,FLINK-28451,13470581,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,lsy,lsy,07/Jul/22 17:45,23/Jul/22 04:44,04/Jun/24 20:42,23/Jul/22 04:44,1.16.0,,,,,1.16.0,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"Currently `HiveFunctionDefinitionFactory` load user defined function using thread context classloader, after we introducing user classloader to load function class from FLINK-27659, this maybe doesn't work if the user jar not in thread classloader. The following exception will be thrown:
{code:java}
 23478 [main] WARN  org.apache.flink.table.client.cli.CliClient [] - Could not execute SQL statement.org.apache.flink.table.client.gateway.SqlExecutionException: Failed to parse statement: SELECT id, func1(str) FROM (VALUES (1, 'Hello World')) AS T(id, str) ;    at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:174) ~[classes/:?]    at org.apache.flink.table.client.cli.SqlCommandParserImpl.parseCommand(SqlCommandParserImpl.java:45) ~[classes/:?]    at org.apache.flink.table.client.cli.SqlMultiLineParser.parse(SqlMultiLineParser.java:71) ~[classes/:?]    at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964) ~[jline-reader-3.21.0.jar:?]    at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778) ~[jline-reader-3.21.0.jar:?]    at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679) ~[jline-reader-3.21.0.jar:?]    at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:296) [classes/:?]    at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:281) [classes/:?]    at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:229) [classes/:?]    at org.apache.flink.table.client.cli.CliClientITCase.runSqlStatements(CliClientITCase.java:172) [test-classes/:?]    at org.apache.flink.table.client.cli.CliClientITCase.testSqlStatements(CliClientITCase.java:137) [test-classes/:?]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_291]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_291]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_291]    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_291]    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) [junit-4.13.2.jar:4.13.2]    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) [junit-4.13.2.jar:4.13.2]    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.13.2.jar:4.13.2]    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.2.jar:4.13.2]    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.13.2.jar:4.13.2]    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) [classes/:?]    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) [junit-4.13.2.jar:4.13.2]    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.13.2.jar:4.13.2]    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) [junit-4.13.2.jar:4.13.2]    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) [junit-4.13.2.jar:4.13.2]    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) [junit-4.13.2.jar:4.13.2]    at org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) [junit-4.13.2.jar:4.13.2]    at org.junit.runners.ParentRunner.run(ParentRunner.java:413) [junit-4.13.2.jar:4.13.2]    at org.junit.runner.JUnitCore.run(JUnitCore.java:137) [junit-4.13.2.jar:4.13.2]    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69) [junit-rt.jar:?]    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38) [junit-rt.jar:?]    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11) [idea_rt.jar:?]    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35) [junit-rt.jar:?]    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235) [junit-rt.jar:?]    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54) [junit-rt.jar:?]Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. Can't resolve udf class LowerUDF    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:182) ~[classes/:?]    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:109) ~[classes/:?]    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:241) ~[classes/:?]    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[classes/:?]    at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[classes/:?]    ... 54 moreCaused by: java.lang.RuntimeException: Can't resolve udf class LowerUDF    at org.apache.flink.table.catalog.CatalogFunctionImpl.isGeneric(CatalogFunctionImpl.java:90) ~[classes/:?]    at org.apache.flink.table.catalog.hive.factories.HiveFunctionDefinitionFactory.createFunctionDefinition(HiveFunctionDefinitionFactory.java:61) ~[classes/:?]    at org.apache.flink.table.factories.FunctionDefinitionFactory.createFunctionDefinition(FunctionDefinitionFactory.java:60) ~[classes/:?]    at org.apache.flink.table.catalog.FunctionCatalog.resolvePreciseFunctionReference(FunctionCatalog.java:586) ~[classes/:?]    at org.apache.flink.table.catalog.FunctionCatalog.lambda$resolveAmbiguousFunctionReference$3(FunctionCatalog.java:634) ~[classes/:?]    at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_291]    at org.apache.flink.table.catalog.FunctionCatalog.resolveAmbiguousFunctionReference(FunctionCatalog.java:634) ~[classes/:?]    at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:368) ~[classes/:?]    at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:94) ~[classes/:?]    at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:67) ~[calcite-core-1.26.0.jar:1.26.0]    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1183) ~[classes/:1.26.0]    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1200) ~[classes/:1.26.0]    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1169) ~[classes/:1.26.0]    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:945) ~[classes/:1.26.0]    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704) ~[classes/:1.26.0]    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:178) ~[classes/:?]    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:109) ~[classes/:?]    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:241) ~[classes/:?]    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[classes/:?]    at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[classes/:?]    ... 54 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28430,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 23 04:44:12 UTC 2022,,,,,,,,,,"0|z16ox4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 17:46;lsy;cc [~luoyuxia] ;;;","08/Jul/22 02:28;luoyuxia;[~lsy] Thanks for reporting it. I'll fix it. Previoulsy, I also have a similiar issue FLINK-28430 .;;;","23/Jul/22 04:44;jark;Fixed in master: e13ae26e806eff403787d5b75e90008bcc13d8dc and 9a4fd227a5824e707f4dbe52773a260fbb89854a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run helm linter in k8s operator ci,FLINK-28450,13470552,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,mbalassi,mbalassi,07/Jul/22 14:51,08/Jul/22 06:31,04/Jun/24 20:42,08/Jul/22 06:31,kubernetes-operator-1.1.0,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,To avoid issues lik FLINK-28300 in the future let us check for this in the CI.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 06:31:20 UTC 2022,,,,,,,,,,"0|z16oqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 06:31;gyfora;merged to main 967c22a84da7bb44d42262d7f76ffb75b84f0e61;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-parquet,FLINK-28449,13470545,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,07/Jul/22 13:21,04/Nov/22 16:28,04/Jun/24 20:42,04/Nov/22 16:28,,,,,,1.17.0,,,Connectors / FileSystem,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 04 16:28:31 UTC 2022,,,,,,,,,,"0|z16op4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 13:22;rskraba;Can this be assigned to me please?;;;","04/Nov/22 16:28;mapohl;master: db3244a6ac2b06cebae9be26d7bcac506cc69093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedDataTestBase has potential problem when compression is enable,FLINK-28448,13470533,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,07/Jul/22 11:59,28/Jul/22 02:42,04/Jun/24 20:42,28/Jul/22 02:42,,,,,,1.16.0,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"BoundedDataTestBase has potential problem when compression is enable,  I'm sure there is a bug in this part of the code. But unfortunately, the test can pass now, because the generated data used in the test cannot be compressed by LZ4 (i.e. the compressed size is larger than the original size), which causes the branch of enable compression actually the same as disable compression, So the bug didn't surface.

I think there are two steps of work that need to be done:
1. Fix incorrect code
2. Generate data that can be properly compressed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 28 02:42:17 UTC 2022,,,,,,,,,,"0|z16omg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 15:38;Weijie Guo;cc [~kevin.cyj] ;;;","28/Jul/22 02:42;kevin.cyj;Merged into master via 1b012cb2c402a91be58e8b201ffd1b6d38b58ef9 and a4ad7814009f3e799f198d54f24a055894b1ebce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Kubernetes Operator PR template,FLINK-28447,13470516,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,gyfora,gyfora,07/Jul/22 10:25,09/Jul/22 11:57,04/Jun/24 20:42,09/Jul/22 11:57,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,We should create a sensible PR template similar (but simpler) to Flink itself. This should help avoid PRs with poor description that are hard to review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 09 11:57:15 UTC 2022,,,,,,,,,,"0|z16oio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/22 11:57;gyfora;merged to main 11cba495e45710dccbc7a7c9c6fb0892d8340550;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose more information in PartitionDescriptor to support more optimized Shuffle Service,FLINK-28446,13470513,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tanyuxin,tanyuxin,tanyuxin,07/Jul/22 10:06,21/Jul/22 02:53,04/Jun/24 20:42,21/Jul/22 02:50,1.16.0,,,,,,,,,,,,,,,0,pull-request-available,,,,"To improve shuffle performance, more detailed fields should be added to PartitionDescriptor, for example, whether the partition is a broadcast partition or the distribution pattern of the intermediate result.

After the detailed information is added, the Shuffle Service can use different Shuffle strategies in different situations, which may lead to better shuffle performance.

In addition, we only add fields to PartitionDescriptor, which is compatible with the previous version, and the added fields are insensitive to earlier users.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 21 02:50:51 UTC 2022,,,,,,,,,,"0|z16oi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 02:50;kevin.cyj;Merged into master via e630778eb8b500148b01583d34ec195a80c99c3f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support operator configuration from multiple configmaps,FLINK-28445,13470510,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,07/Jul/22 10:00,24/Nov/22 01:02,04/Jun/24 20:42,08/Jul/22 12:15,,,,,,kubernetes-operator-1.1.0,,,,,,,,,,0,pull-request-available,,,,It is beneficial in certain scenarios to load operator configurations from multiple ConfigMaps. For example the `kubernetes.operator.watched.namespaces` is a typical property maintained by control planes and the rest is by the Operator. By allowing loading the configuration from multiple ConfigMaps the default configuration can be owned by the Operator and other environment specific overrides by a control plane. This also allows upgrading the Operator independently from control planes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 12:15:20 UTC 2022,,,,,,,,,,"0|z16ohc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 12:15;gyfora;Merged to main 56381ecf4515ff12d72fe6b1a245160580aacb72;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SUBSTR without parameter in SQL and Table API,FLINK-28444,13470504,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ana4,ana4,ana4,07/Jul/22 09:45,07/Jul/22 12:54,04/Jun/24 20:42,,1.15.1,,,,,,,,API / Python,Table SQL / API,,,,,,0,,,,,"SUBSTR() has no parameter functionality in the doc, but the code only supports SUBSTR(STRING, INT1) and SUBSTR(STRING, INT1, INT2). We should also support SUBSTR().

We can use the new built-in function stack to rewrite `generateSubString` method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28439,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 09:46:06 UTC 2022,,,,,,,,,,"0|z16og0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 09:46;ana4;I would like to take this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide official Flink image for PyFlink,FLINK-28443,13470502,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,07/Jul/22 09:40,05/Sep/22 12:57,04/Jun/24 20:42,,1.15.0,,,,,,,,API / Python,,,,,,,0,,,,,"Users must build a custom Flink image to include python and pyFlink to run a pyFlink job with Native Kubernetes application mode.

I think we can improve the user experience by providing an official image that pre-installs python and pyFlink.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 05 12:57:10 UTC 2022,,,,,,,,,,"0|z16ofk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jul/22 06:12;bgeng777;This improvement is good for user experience, but one challenge of this improvement may be how to manage the  version matrix: considering 3 popular python version(3.7,3.8,3.9) and 3 flink version(1.14, 1.15, 1.16), there should be 3 * 3 = 9 pyflink images. If we consider java version, the problem can be worse.;;;","05/Sep/22 12:57;bzhaoop;How about we just consider the major version for this request? Or the major stable versions?

Such as Python 3.9 + Java 8 + Flink&pyflink 1.15

and Python 3.7 + Java 8 + Flink&pyflink 1.14

 

How do you guys think ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitGenerator should use Ordered Packing,FLINK-28442,13470498,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,07/Jul/22 09:30,08/Jul/22 03:09,04/Jun/24 20:42,08/Jul/22 03:09,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"SplitGenerator should use Ordered Packing
- The full part data in the stream is consumed in an orderly way
- Ordered batch reads can make subsequent calculations more efficient, such as aggregation by partial primary key groupBy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 03:09:51 UTC 2022,,,,,,,,,,"0|z16oeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 03:09;lzljs3620320;master: 38257d6595596c8e7f9c8060ba4ea0b252594139;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The parsing result of configuration `pipeline.cached-files` is incorrect,FLINK-28441,13470497,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,xiaozilong,xiaozilong,07/Jul/22 09:26,08/Jul/22 02:35,04/Jun/24 20:42,08/Jul/22 02:35,1.13.0,,,,,,,,Runtime / Configuration,,,,,,,0,,,,,"The parsing process of the configuration will use the method `org.apache.flink.configuration.ConfigurationUtils#parseMap`, the configuration will be spliy by ':', unfortunately when we have the character ':' in the value, the parsing result is incorrect.

e.g.

input: name:file2,path:hdfs:///tmp/file2
output: [path, hdfs, ///tmp/file2]

Consider only the value containing the character ':' part",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-07 09:26:25.0,,,,,,,,,,"0|z16oeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EventTimeWindowCheckpointingITCase failed with restore,FLINK-28440,13470496,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,Yanfei Lei,hxbks2ks,hxbks2ks,07/Jul/22 09:25,31/May/24 13:20,04/Jun/24 20:42,,1.16.0,1.17.0,1.18.0,1.19.0,,1.20.0,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,auto-deprioritized-critical,pull-request-available,stale-assigned,test-stability,"{code:java}
Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:256)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:722)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:698)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:665)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_0a448493b4782967b150582570326227_(2/4) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
	... 11 more
Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit1835099326935900400/junit1113650082510421526/52ee65b7-033f-4429-8ddd-adbe85e27ced (No such file or directory)
	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:96)
	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:75)
	at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:92)
	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
	... 13 more
Caused by: java.io.FileNotFoundException: /tmp/junit1835099326935900400/junit1113650082510421526/52ee65b7-033f-4429-8ddd-adbe85e27ced (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
	at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87)
	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:72)
	at org.apache.flink.changelog.fs.StateChangeFormat.read(StateChangeFormat.java:92)
	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
	... 21 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37772&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8916

Other tests where this stacktrace was observed in test failures is {{ChangelogRecoveryITCase}} (FLINK-30107) and {{ChangelogRecoverySwitchStateBackendITCase}} (FLINK-28898).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33959,,,FLINK-30102,FLINK-30107,,FLINK-28898,,,,,,,FLINK-31593,,,,,,FLINK-30561,,,,,,,"31/Jan/23 16:51;Feifan Wang;image-2023-02-01-00-51-54-506.png;https://issues.apache.org/jira/secure/attachment/13054954/image-2023-02-01-00-51-54-506.png","31/Jan/23 17:10;Feifan Wang;image-2023-02-01-01-10-01-521.png;https://issues.apache.org/jira/secure/attachment/13054955/image-2023-02-01-01-10-01-521.png","31/Jan/23 17:19;Feifan Wang;image-2023-02-01-01-19-12-182.png;https://issues.apache.org/jira/secure/attachment/13054957/image-2023-02-01-01-19-12-182.png","01/Feb/23 08:47;Feifan Wang;image-2023-02-01-16-47-23-756.png;https://issues.apache.org/jira/secure/attachment/13054972/image-2023-02-01-16-47-23-756.png","01/Feb/23 08:57;Feifan Wang;image-2023-02-01-16-57-43-889.png;https://issues.apache.org/jira/secure/attachment/13054973/image-2023-02-01-16-57-43-889.png","02/Feb/23 02:52;Feifan Wang;image-2023-02-02-10-52-56-599.png;https://issues.apache.org/jira/secure/attachment/13055025/image-2023-02-02-10-52-56-599.png","03/Feb/23 02:09;Yanfei Lei;image-2023-02-03-10-09-07-586.png;https://issues.apache.org/jira/secure/attachment/13055071/image-2023-02-03-10-09-07-586.png","03/Feb/23 04:03;Feifan Wang;image-2023-02-03-12-03-16-155.png;https://issues.apache.org/jira/secure/attachment/13055076/image-2023-02-03-12-03-16-155.png","03/Feb/23 04:03;Feifan Wang;image-2023-02-03-12-03-56-614.png;https://issues.apache.org/jira/secure/attachment/13055077/image-2023-02-03-12-03-56-614.png",,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri May 31 13:20:15 UTC 2024,,,,,,,,,,"0|z16oe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 06:36;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37772&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10101;;;","26/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","08/Aug/22 02:04;xuyangzhong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39494&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","15/Aug/22 02:38;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39951&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","22/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","07/Sep/22 06:06;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40763&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","13/Oct/22 08:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41958&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=10798;;;","18/Oct/22 08:29;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42116&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9;;;","02/Nov/22 15:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42726&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8384;;;","30/Nov/22 07:01;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43604&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9304;;;","30/Nov/22 13:43;mapohl;FLINK-28898 and FLINK-28440 both are caused by some temporary file not being found anymore.;;;","30/Nov/22 13:54;mapohl;I'm closing this issue in favor of FLINK-28898. Both test failures are actually caused by the same error with the same stacktrace:
{code}
01:30:47,883 [SlidingEventTimeWindows (2/4)#1] WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for WindowOperator_0a448493b4782967b
150582570326227_(2/4) from alternative (1/1), will retry while more alternatives are available.
java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit16794614592942662589/junit13988066572861564685/3396e2c5ff76b0c6312193980a16e3b7/dstl/af0ddd3f-2ab1-4270-99fb-89a4eaadcb4d (No such file or directory)
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107) ~[flink-statebackend-changelog-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78) ~[flink-statebackend-changelog-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94) ~[flink-statebackend-changelog-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136) ~[flink-statebackend-changelog-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:265) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:727) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:703) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:670) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:938) [flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:907) [flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:731) [flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) [flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.io.FileNotFoundException: /tmp/junit16794614592942662589/junit13988066572861564685/3396e2c5ff76b0c6312193980a16e3b7/dstl/af0ddd3f-2ab1-4270-99fb-89a4eaadcb4d (No such file or directory)
        at java.io.FileInputStream.open0(Native Method) ~[?:?]
        at java.io.FileInputStream.open(FileInputStream.java:219) ~[?:?]
        at java.io.FileInputStream.<init>(FileInputStream.java:157) ~[?:?]
        at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89) ~[flink-dstl-dfs-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42) ~[flink-dstl-dfs-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        ... 21 more
{code}
The error appears while creating the {{StreamOperatorStateContext}}.;;;","11/Jan/23 10:30;mapohl;I'm reopneing this issue since closing it as a duplicate of FLINK-28440 was a mistake. FLINK-28440 handled a different kind of FileNotFoundException (the checkpoint's metadata file). This issue is about a {{FileNotFoundException}} being caused when starting the {{StreamOperatorStateContext}}:
{code}
Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:256)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:723)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:666)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_08a489791a4e7fcd83ae029ef13928c6_(4/4) from any of the 1 provided restore options.
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
        ... 11 more
Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit5354320522396578132/junit5523751389985007360/8145786fe678df6770655845be5f9dc9/dstl/95c98d3b-a010-44e8-813c-3f5d0e4af9a0 (No such file or directory)
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
        at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
        at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107)
        at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78)
        at org.apache.flink.state.changelog.DeactivatedChangelogStateBackend.restore(DeactivatedChangelogStateBackend.java:70)
        at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
        ... 13 more
Caused by: java.io.FileNotFoundException: /tmp/junit5354320522396578132/junit5523751389985007360/8145786fe678df6770655845be5f9dc9/dstl/95c98d3b-a010-44e8-813c-3f5d0e4af9a0 (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
{code}
There are several other build failures from linked Jira issues that should be considered as well.

I updated the issue description accordingly.;;;","11/Jan/23 10:54;mapohl;[~yuanmei], [~roman] could you have a look at this. This test affects the stability of our CI.;;;","18/Jan/23 07:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44989&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8285;;;","25/Jan/23 08:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45184&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10732;;;","25/Jan/23 10:05;mapohl;[~roman] [[~ym] any opinion on that one?;;;","25/Jan/23 16:21;roman;Should we disable randomization with changelog temporarily until this issue is resolved?
[~ym] WDYT?;;;","30/Jan/23 07:30;ym;[~Yanfei Lei] will take a look first. If it cannot be fixed before the code freeze (quick fix), I am fine to disable it temporarily.;;;","30/Jan/23 08:16;mapohl;Thanks for the information, [~ym] 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45314&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9936;;;","30/Jan/23 12:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45377&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10658;;;","31/Jan/23 10:18;Yanfei Lei;I suspect that it is caused by the concurrency of the snapshot thread and the materialization thread.
h2. The DSTL lifecycle in a checkpoint:

TM side:
 # {*}Snapshot sync phase{*}: ChangelogKeyedStateBackend.snapshot() -> RunnableFuture stateChangelogWriter.persist()
 # {*}Snapshot async phase{*}: Uploader upload DSTL, call TaskChangelogRegistry.startTracking()
 # {*}Snapshot async phase{*}: The callback of upload, build snapshot result: FsStateChangelogWriter.handleUploadSuccess()
 # {*}Snapshot async phase{*}: Report snapshot result to JM: SubtaskCheckpointCoordinatorImpl.checkpointState() -> finishAndReportAsync()

JM side:
 # {*}JM write metadata file{*}: CheckpointCoordinator.completePendingCheckpoint() -> finalizeCheckpoint() -> Checkpoints.storeCheckpointMetadata
 # {*}JM notify TM checkpoint is completed{*}: CheckpointCoordinator.completePendingCheckpoint() -> cleanupAfterCompletedCheckpoint() -> sendAcknowledgeMessages()

TM side:
 * *[{color:#ff0000}Bug{color}]* The pre-emptive upload is triggered immediately after the last checkpoint. Due to the existence of batch upload, exactly two changesets are placed in the same file. At this time, the task on TM is canceled, *truncateAndClose()* is called, the pre-emptive file is deleted, and part of the changeset of the last checkpoint is also deleted.
 * ChangelogKeyedStateBackend.notifyCheckpointComplete() -> StateChangelogWriter.confirm() -> TaskChangelogRegistry.stopTracking()

 

After discussing with [~Feifan Wang] offline, we plan to record the range SQN of StreamStateHandle and check the range before discarding StreamStateHandle.

!image-2023-02-03-10-09-07-586.png|width=504,height=314!;;;","31/Jan/23 17:54;Feifan Wang;Hi [~Yanfei Lei] ,[~ym] ,[~roman] , We also encountered the problem of changelog FileNotFoundException during restore. After a period of investigation, I found two reasons for this problem. They are all caused by pre-emptive upload and BatchingStateChangeUploadScheduler :
h3. *First case :*

Pre-emptive upload causes the stale StateChangeSet to be uploaded into the same file along with the latest StateChangeSet.

!image-2023-02-01-00-51-54-506.png|width=617,height=342!

The small square with a number in the middle represents a StateChange, and the number in it indicates the SequenceNumber to which the StateChange belongs.

StateChange-3 triggers a pre-emptive upload, and then StateChange-1 and StateChange-3 are uploaded to the same file.
{code:java}
if (activeChangeSetSize >= preEmptivePersistThresholdInBytes) {
    LOG.info(
            ""pre-emptively flush {}MB of appended changes to the common store"",
            activeChangeSetSize / 1024 / 1024);
    persistInternal(notUploaded.isEmpty() ? activeSequenceNumber : notUploaded.firstKey());
} {code}
When checkpoint-3 is completed, checkpoint-2 will be subsumed. When checkpoint-3 is completed, checkpoint-2 will be subsumed. Then StateChange-1 will be truncate, further causing file-3 to be deleted. If restoring from checkpoint-3, a FileNotFoundException will occur.
h3. *Second case :*

!image-2023-02-01-01-10-01-521.png|width=484,height=391!

StateChange-0,1,2 are uploaded in the same file due to dstl.dfs.batch.persist-delay and pre-emptive. Checkpoint-1 will be subsumed after checkpoint-2 complete, then file-1 will be delete. But actually checkpoint-2 needs file-1 ( checkpoint-2 needs StateChange-2 ).
h3. *Summarize :*

The root of the problem is FsStateChangelogWriter#notifyStateNotUsed.

!image-2023-02-01-01-19-12-182.png|width=759,height=312!

 

I wrote two test cases to reproduce the above problem, namely _FsStateChangelogWriterTest#testChangelogFileNotFound1()_ and {_}FsStateChangelogWriterTest#testChangelogFileNotFound2(){_}, [you can find it here|https://github.com/apache/flink/pull/21812].

 

I suggest to record the largest SequenceNumber in each StreamStateHandle in the FsStateChangelogWriter and checking it before calling changelogRegistry.notUsed(). 

WDYT [~ym] ,[~roman] ,[~Yanfei Lei]  ?;;;","01/Feb/23 03:35;Yanfei Lei;[~Feifan Wang] Thanks for the investigation, I don't think the materialization in Figure 1 is quite accurate. Generally speaking, the materialization interval is much longer than checkpoint interval. Once materialization will record all the previous state changes to the snapshot of delegated state backends(like RocksDB), and clean up the existing DSTL. If there is no materialization, the DSTL file will grow infinitely.

As you mentioned, the subsequent checkpoint will contain the state changes of the previous checkpoint:

 
{code:java}
chk-1: m0,c0,c1
chk-2: m0,c0,c1,c2,c3
{code}
 

Before chk-1 subsumed, TM would call `TaskChangelogRegistry.stopTracking()` to transfer control of DSTL files to JM. In most cases, JM decides when to delete files, and on JM side, each `ChangelogStateBackendHandle.registerSharedStates` would update the `SharedStateEntry.lastUsedCheckpointID` of DSTL file, for example, when chk-1 subsumed, the lastUsedCheckpointID of *c0* and *c1* are {*}chk-2{*}, so the c0,c1 would *not* be discarded.

On TM side, for `TaskChangelogRegistry.notUsed()`, if the `TaskChangelogRegistry.stopTracking()` be called before it, 

`TaskChangelogRegistry.notUsed()` would return directly at the first if statement.
{code:java}
public void notUsed(StreamStateHandle handle, UUID backendId) {
    PhysicalStateHandleID key = handle.getStreamStateHandleID();
    Set<UUID> backends = entries.get(key);
    if (backends == null) {
        return;  // if stopTracking() called before this, return directly, no more discard.
     }
    backends.remove(backendId);
    if (backends.isEmpty() && entries.remove(key) != null) {
        scheduleDiscard(handle);
    }
} {code}
Your test does not consider `TaskChangelogRegistry.stopTracking()`, may it is inaccurate.;;;","01/Feb/23 07:49;Feifan Wang;[~Yanfei Lei] , Thank you for your reminder, the previous description and test did not consider stopTracking(). But even if stopTracking() is considered, the problem I mentioned still exists. I updated the _FsStateChangelogWriterTest#testChangelogFileNotFound1_ , you can have a look.;;;","01/Feb/23 08:47;Feifan Wang;The updated _FsStateChangelogWriterTest#testChangelogFileNotFound1_ can be described by the following figure :

!image-2023-02-02-10-52-56-599.png|width=600,height=273!

When materialization-1 is triggered, ChangeSet-1 will be added to notUploaded, and it will remain there until ChangeSet-4 triggers pre-emptive upload and is uploaded to the same file with ChangeSet-4.

 

@ [~Yanfei Lei] ;;;","03/Feb/23 04:44;Feifan Wang;Hi [~ym] , [~roman] , I communicated with [~Yanfei Lei]  offline, we think there are two situations that will cause the changelog file not found problem。
h2. Case-1 ：

first is the “First case” I mentioned above :

!image-2023-02-03-12-03-16-155.png|width=708,height=315!

 
  # checkpoint-1 trigger and completed, upTo sqn=1
 # materialization-1 triggered, upTo sqn=2. ChangeSet[sqn=1] will be added to {*}_notUploaded_{*}.
 # materialization-1 completed, upTo sqn=3
 # checkpoint-2 trigger and completed, upTo sqn=3. Since materialization-1 has been completed when checkpoint-2 is triggered, checkpoint-2 is from sqn=2. ChangeSet[sqn=1] stays in notUploaded.
 # checkpoint-3 trigger and complated, upTo sqn=4.
 # before chk-2 subsumed, ChangeSet[sqn=4] trigger a pre-emptively upload, ChangeSet[sqn=1] and ChangeSet[sqn=4] are saved in the same file.
 # checkpoint-2 subsumed, FsStateChangelogWriter try to truncate to sqn=2. ChangeSet[sqn=1] will be delete, but also delete ChangeSet[sqn=4] which in the same file.
 # checkpoint-4 trigger and completed, it contains changelog [sqn=2 ~ sqn=4], but ChangeSet[sqn=4] already be deleted.

h2. Case-2 :

case-2 is provided by yanfei :

!image-2023-02-03-12-03-56-614.png|width=632,height=308!
 # checkpoint-1 trigger , upTo sqn=1. But BatchingStateChangeUploadScheduler is waiting for persist-delay
 # ChangeSet[sqn=1] trigger a pre-emptively upload, ChangeSet[sqn=0] and ChangeSet[sqn=1] are saved in the same file, and checkpoint-1 completed.
 # the task was canceled for other reasons before checkpoint-1 was confirmed (JobManager completed checkpoint-1 , but the confirm message has not yet reached the task). FsStateChangelogWriter try to truncateAndClose from sqn=1, ChangeSet[sqn=1] will be delete, but also delete ChangeSet[sqn=0] which in the same file.

h2. How to reproduce :

FsStateChangelogWriterTest#testChangelogFileAvailable and FsStateChangelogWriterTest#testChangelogFileAvailableAgain [in this PR|https://github.com/apache/flink/pull/21812] correspond to case-1 and case-2 above respectively.
h2. Problem analysis :

Like I mentioned above, I think the source of the problem is FsStateChangelogWriter#notifyStateNotUsed. A SteamStateHandle may correspond to multiple UploadResults. We cannot assume that the entire StreamStateHandle is no longer needed by this backend when a certain UploadResult is not used.

!image-2023-02-01-01-19-12-182.png|width=652,height=268!
h2. Proposal :

I propose to record the reference count of the StreamStateHandle in the TaskChangelogRegistry instead of the backend collection. TaskChangelogRegistry#notUsed(streamStateHandle, uploadResult) only decrements the reference count by one, and deletes the steamStateHandle when the reference count reaches zero.

What do you think  [~ym] , [~roman]  ?;;;","08/Feb/23 07:48;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45857&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10292;;;","13/Feb/23 09:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46029&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=7818;;;","13/Feb/23 10:10;roman;Thanks Feifan Wang and Yanfei Lei for the investigation and the fix, 

merged into master as c0aa73df4df4e39c138f2cddaeb8efad6c831d03

into 1.17 as c612575ed339b319d9822dd7cbc59e3d972fe5ed,

into 1.16 as 700f8839126ec362898936630571bf023735651e.

 

 ;;;","06/Mar/23 09:44;mapohl;Looks like it's still appearing? This one happened on master:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8215;;;","07/Mar/23 07:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46869&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10381;;;","08/Mar/23 08:39;Yanfei Lei;Aha, it's appearing in `ChangelogRecoveryITCase` again, maybe our fixes missed some cases, I'll look into it later.;;;","23/Mar/23 07:17;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47476&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=11177;;;","24/Mar/23 18:24;mapohl;I try to finalize FLINK-31593 but ran into an issue with the {{StatefulJobSavepointMigrationITCase}} for Flink version 1.17 with RocksDB state backend and {{SnapshotType.CHECKPOINT}}. The following exception is exposed through the logs:
{code}
34632 [Flat Map -> Sink: Unnamed (2/4)#30] WARN  org.apache.flink.runtime.taskmanager.Task [] - Flat Map -> Sink: Unnamed (2/4)#30 (98755c0995b5d43eae13ca81da224424_d1392353922252257afa15351a98bae9_1_30) switched from INITIALIZING to FAILED with failure cause:
java.lang.Exception: Exception while creating StreamOperatorStateContext.
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:258) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:256) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:747) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:722) ~[classes/:?]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:688) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [classes/:?]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [classes/:?]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_345]
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for StreamFlatMap_d1392353922252257afa15351a98bae9_(2/4) from any of the 1 provided restore options.
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:355) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:166) ~[classes/:?]
	... 11 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:405) ~[classes/:?]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:510) ~[classes/:?]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:99) ~[classes/:?]
	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.lambda$createKeyedStateBackend$1(AbstractChangelogStateBackend.java:145) ~[classes/:?]
	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:72) ~[classes/:?]
	at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94) ~[classes/:?]
	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:338) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:355) ~[classes/:?]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:166) ~[classes/:?]
	... 11 more
Caused by: java.io.FileNotFoundException: /tmp/junit5080387696602759609/checkpoints_9efe7b0c-8da8-4bef-9020-57abf2e29ac9/5fe4979761499f945215ac42815e65d4/taskowned/61058282-6146-48c0-9ece-b5d6bdd44234 (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[?:1.8.0_345]
	at java.io.FileInputStream.open(FileInputStream.java:195) ~[?:1.8.0_345]
	at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[?:1.8.0_345]
	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50) ~[classes/:?]
	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134) ~[classes/:?]
	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69) ~[classes/:?]
	at org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.downloadDataForStateHandle(RocksDBStateDownloader.java:127) ~[classes/:?]
	at org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.lambda$createDownloadRunnables$0(RocksDBStateDownloader.java:110) ~[classes/:?]
	at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:49) ~[classes/:?]
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640) ~[?:1.8.0_345]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_345]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_345]
	... 1 more
{code}

which looks like the same issue we're having here in FLINK-28440. [~Yanfei Lei] can you give some guidance here? I created the reference test data for 1.17 based on {{release-1.17.0}} and ran the test based on {{release-1.17}}. Is this a false flag?;;;","28/Mar/23 11:45;Yanfei Lei;[~mapohl] It seems that these are two problems. FLINK-28440 is the DSTL file of the changelog that can‘t be found, and FLINK-31593 is the rocksdb's file of the changelog state backend  that can‘t be found. 
Did the job restored from a native savepoint? currently, changelog does not support native savepoint( https://issues.apache.org/jira/browse/FLINK-29802).

I found some log in [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47590&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47590&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba,], and found the following log, it seems that job was restored from native savepoint. 
{code:java}
11:35:10,088 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Reset the checkpoint ID of job 12f8ba2f80aca97baf6f50f5117c8128 to 3.11:35:10,088 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 12f8ba2f80aca97baf6f50f5117c8128 from Savepoint 2 @ 0 for 12f8ba2f80aca97baf6f50f5117c8128 located at file:/__w/1/s/flink-tests/target/test-classes/stateful-scala-udf-migration-itcase-flink1.17-rocksdb-checkpoint. {code}
We can see the snapshot type in  CheckpointCoordinator is savepoint, but the snapshot type in filePath ""file:/__w/1/s/flink-tests/target/test-classes/stateful-scala-udf-migration-itcase-flink1.17-rocksdb-checkpoint"" is checkpoint, this is a bit strange.;;;","28/Mar/23 12:56;mapohl;FYI: The discussion on the FLINK-31593 issue continues in FLINK-31593.;;;","02/Apr/23 15:33;renqs;Redirected to here from FLINK-30107:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47803&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=9691;;;","06/Jun/23 13:17;Sergey Nuyanzin;Redirected to here from FLINK-30107
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47803&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=9652;;;","12/Jun/23 10:39;Sergey Nuyanzin;Redirected to here from FLINK-30107
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49701&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8186;;;","12/Jun/23 11:21;Sergey Nuyanzin;Redirected to here from FLINK-30107
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49750&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8702;;;","13/Jun/23 08:07;renqs;[~Yanfei Lei] Any progress on this one? Thanks;;;","20/Jun/23 10:39;Sergey Nuyanzin;Redirected to here from FLINK-30107

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50071&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=10737;;;","05/Jul/23 06:35;Sergey Nuyanzin;Redirected to here from FLINK-30107
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50954&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8565;;;","07/Aug/23 09:52;Sergey Nuyanzin;Redirected to here from FLINK-30107
1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52016&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","16/Aug/23 09:56;Sergey Nuyanzin;Redirected to here from FLINK-30107
1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52295&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=11371;;;","24/Aug/23 12:57;Sergey Nuyanzin;Redirected to here from FLINK-30107
1.17: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52480&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8866|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52295&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=11371];;;","29/Aug/23 08:13;renqs;[~Yanfei Lei] Could you take a look at this one? Thanks;;;","29/Aug/23 08:43;Yanfei Lei;[~renqs] Sorry for the late reply, I found that the recent reports are all from FLINK-30107,  I will check FLINK-30107 first.;;;","31/Aug/23 09:53;Sergey Nuyanzin;Redirected to here from FLINK-30107
1.16: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52880&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8648|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52295&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=11371];;;","30/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","04/Oct/23 17:36;Sergey Nuyanzin;Redirected to here from FLINK-30107
1.19: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53408&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8727;;;","05/Oct/23 10:09;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53560&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=7926;;;","09/Oct/23 07:57;Sergey Nuyanzin;Redirected to here from FLINK-30107
1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53603&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9047;;;","16/Oct/23 07:41;Sergey Nuyanzin;Redirected to here from FLINK-30107
1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53741&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8866;;;","16/Nov/23 12:55;mapohl;master (1.19): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54516&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9033;;;","01/Dec/23 07:32;mapohl;master (1.19): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55084&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8024;;;","07/Dec/23 14:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55224&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8482;;;","07/Dec/23 14:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55253&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8872;;;","07/Dec/23 14:48;mapohl;Build failure happened in the context of the FLINK-27075 GHA migration efforts:
 * [https://github.com/XComp/flink/actions/runs/7064675697/job/19233524911#step:12:8017]
 * [https://github.com/XComp/flink/actions/runs/7067421801/job/19240914912#step:12:7882];;;","16/Dec/23 13:42;Sergey Nuyanzin;Redirected to here from FLINK-30107
1.17: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55582&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10717];;;","20/Dec/23 11:48;mapohl;https://github.com/XComp/flink/actions/runs/7271736522/job/19813058211#step:12:8117;;;","20/Dec/23 11:51;mapohl;https://github.com/XComp/flink/actions/runs/7260428960/job/19779937286#step:12:8108;;;","07/Jan/24 23:55;Sergey Nuyanzin;Redirected to here from FLINK-30107
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55948&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9018;;;","08/Jan/24 00:22;Sergey Nuyanzin;Redirected to here from FLINK-30107
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55965&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8565;;;","25/Jan/24 10:58;mapohl;1.18: https://github.com/XComp/flink/actions/runs/7646851520/job/20836884690#step:10:7650;;;","25/Jan/24 11:05;mapohl;1.19: https://github.com/XComp/flink/actions/runs/7640007894/job/20815285699#step:10:8297;;;","05/Feb/24 07:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57274&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8885

{code}
Feb 05 03:47:04 03:47:04.282 [ERROR] Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 29.92 s <<< FAILURE! -- in org.apache.flink.test.checkpointing.ChangelogRecoveryITCase
Feb 05 03:47:04 03:47:04.282 [ERROR] org.apache.flink.test.checkpointing.ChangelogRecoveryITCase.testMaterialization[delegated state backend type = EmbeddedRocksDBStateBackend{, localRocksDbDirectories=null, enableIncrementalCheckpointing=TRUE, numberOfTransferThreads=-1, writeBatchSize=-1}] -- Time elapsed: 5.361 s <<< ERROR!
Feb 05 03:47:04 org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=0)
Feb 05 03:47:04 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:180)
Feb 05 03:47:04 	at org.apache.flink.runtime.executiongraph.failover.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
Feb 05 03:47:04 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:277)
Feb 05 03:47:04 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:268)
Feb 05 03:47:04 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:261)
Feb 05 03:47:04 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:787)
Feb 05 03:47:04 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:764)
Feb 05 03:47:04 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
Feb 05 03:47:04 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)
Feb 05 03:47:04 	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
[...]
Feb 05 03:47:04 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Feb 05 03:47:04 Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
Feb 05 03:47:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:294)
Feb 05 03:47:04 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:266)
Feb 05 03:47:04 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
Feb 05 03:47:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreStateAndGates(StreamTask.java:799)
Feb 05 03:47:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$restoreInternal$3(StreamTask.java:753)
Feb 05 03:47:04 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
Feb 05 03:47:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:753)
Feb 05 03:47:04 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:712)
Feb 05 03:47:04 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
Feb 05 03:47:04 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
Feb 05 03:47:04 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:751)
Feb 05 03:47:04 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
Feb 05 03:47:04 	at java.lang.Thread.run(Thread.java:748)
Feb 05 03:47:04 Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_08a489791a4e7fcd83ae029ef13928c6_(4/4) from any of the 1 provided restore options.
Feb 05 03:47:04 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:165)
Feb 05 03:47:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:399)
Feb 05 03:47:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:180)
Feb 05 03:47:04 	... 12 more
Feb 05 03:47:04 Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit71073126596521536/junit200918151019600319/3d58091e2f5857dd929374a673752361/dstl/ed44af84-4e46-4527-a2f6-e102fc710043 (No such file or directory)
Feb 05 03:47:04 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
Feb 05 03:47:04 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
Feb 05 03:47:04 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
Feb 05 03:47:04 	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107)
Feb 05 03:47:04 	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78)
Feb 05 03:47:04 	at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94)
Feb 05 03:47:04 	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:81)
Feb 05 03:47:04 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$3(StreamTaskStateInitializerImpl.java:393)
Feb 05 03:47:04 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:173)
Feb 05 03:47:04 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:137)
Feb 05 03:47:04 	... 14 more
Feb 05 03:47:04 Caused by: java.io.FileNotFoundException: /tmp/junit71073126596521536/junit200918151019600319/3d58091e2f5857dd929374a673752361/dstl/ed44af84-4e46-4527-a2f6-e102fc710043 (No such file or directory)
Feb 05 03:47:04 	at java.io.FileInputStream.open0(Native Method)
Feb 05 03:47:04 	at java.io.FileInputStream.open(FileInputStream.java:195)
Feb 05 03:47:04 	at java.io.FileInputStream.<init>(FileInputStream.java:138)
Feb 05 03:47:04 	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
Feb 05 03:47:04 	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
Feb 05 03:47:04 	at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87)
Feb 05 03:47:04 	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:72)
Feb 05 03:47:04 	at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89)
Feb 05 03:47:04 	at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
Feb 05 03:47:04 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
Feb 05 03:47:04 	... 22 more
{code};;;","12/Feb/24 07:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57440&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8528;;;","13/Feb/24 09:28;mapohl;https://github.com/apache/flink/actions/runs/7880739609/job/21503465125#step:10:7557;;;","14/Feb/24 10:02;mapohl;https://github.com/apache/flink/actions/runs/7895502334/job/21548198516#step:10:7557;;;","16/Mar/24 10:38;lincoln.86xy;1.19: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58323&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8960;;;","21/Mar/24 10:47;mapohl;https://github.com/apache/flink/actions/runs/8290287716/job/22688395896#step:10:8229;;;","21/Mar/24 11:12;mapohl;https://github.com/apache/flink/actions/runs/8360441603/job/22886656534#step:10:7536;;;","22/Mar/24 11:13;rskraba;1.20 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58481&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8875];;;","27/Mar/24 10:05;rskraba;1.19 https://github.com/apache/flink/actions/runs/8445595235/job/23133368149#step:10:8001 ;;;","03/Apr/24 15:08;rskraba;1.20 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8750
;;;","04/Apr/24 16:12;rskraba;1.20 hadoop313 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58717&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=9298;;;","05/Apr/24 14:54;rskraba;1.20 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58740&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8906
1.19 hadoop313 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58742&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8950
;;;","18/Apr/24 09:06;rskraba;1.19 Java 8 / Test (module: tests) [https://github.com/apache/flink/actions/runs/8731358696/job/23956855275#step:10:8099]
1.19 Java 11 / Test (module: tests) [https://github.com/apache/flink/actions/runs/8731358696/job/23956873835#step:10:7968];;;","03/May/24 15:54;rskraba;* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/8901164251/job/24444807095#step:10:7971
* 1.20 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/8887882381/job/24404087819#step:10:8262;;;","13/May/24 13:58;rskraba;* 1.18 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9029200531/job/24811449545#step:10:8625
* 1.19 test_cron_azure tests https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59499&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8120
;;;","23/May/24 12:58;rskraba;* 1.19 Java 21 / Test (module: tests) https://github.com/apache/flink/actions/runs/9201159696/job/25309170552#step:10:8003;;;","24/May/24 12:36;rskraba;* 1.19 Hadoop 3.1.3 / Test (module: tests) https://github.com/apache/flink/actions/runs/9217608890/job/25360146799#step:10:8157;;;","27/May/24 15:15;rskraba;* 1.19 Java 21 / Test (module: tests) https://github.com/apache/flink/actions/runs/9232147048/job/25403143624#step:10:8022;;;","31/May/24 13:20;rskraba;* 1.18 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9314898957/job/25640372228#step:10:8136;;;"
Support SUBSTR and REGEXP built-in function in Table API ,FLINK-28439,13470493,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,07/Jul/22 09:09,22/Aug/22 06:18,04/Jun/24 20:42,08/Jul/22 01:40,,,,,,1.16.0,,,API / Python,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28444,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 01:40:21 UTC 2022,,,,,,,,,,"0|z16odk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 01:40;dianfu;Merged to master via 90c8284b10ecad84d2c954efe0a98e8fe87cf1dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should clear the error message after the session job is running/finished,FLINK-28438,13470475,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,haoxin,haoxin,haoxin,07/Jul/22 08:01,11/Jul/22 07:45,04/Jun/24 20:42,11/Jul/22 07:22,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Once an error occurs, the error filed will contain the message even if the job is running successfully at the end.

We should clear the error message when the session job is running or finished.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/300,,,,,,,,,,9223372036854775807,,,,,,2022-07-07 08:01:54.0,,,,,,,,,,"0|z16o9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Engine Matrix and Build page,FLINK-28437,13470474,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,07/Jul/22 07:52,08/Jul/22 07:28,04/Jun/24 20:42,08/Jul/22 07:28,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 07:28:07 UTC 2022,,,,,,,,,,"0|z16o9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 07:28;lzljs3620320;master: 4010fae81eb12e98df216b74b7dff3ea74139a62;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_multi_sessionjob.sh is failing intermittently,FLINK-28436,13470463,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,morhidi,,07/Jul/22 06:35,24/Nov/22 01:01,04/Jun/24 20:42,25/Aug/22 09:45,kubernetes-operator-1.1.0,,,,,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,,,,,https://github.com/apache/flink-kubernetes-operator/runs/7222745771?check_suite_focus=true,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-07 06:35:53.0,,,,,,,,,,"0|z16o6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink version 1.15.0  consumption Kafka has backpressure.,FLINK-28435,13470459,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,HunterHunter,HunterHunter,07/Jul/22 06:04,07/Jul/22 10:18,04/Jun/24 20:42,,1.15.0,,,,,,,,Connectors / Kafka,,,,,,,0,,,,,"A simple SQL program
{code:java}
CREATE temporary TABLE `print_sink` (
  `id` VARCHAR(2147483647)
)WITH (
  'connector' = 'print'
);

insert into print_sink
select id
from kafka-source
/*+ OPTIONS('properties.bootstrap.servers'='brokers',
  'topic' = 'topicname',
  'properties.group.id'='test',
  'scan.startup.mode'='latest-offset')
  */ {code}
Back pressure occurs when I use `flink 1.15.0`,

When I use `flink 1.13/ flink 1.14`, it works normally.

I think this is caused by the version of Kafka.

I have tried to consume versions `Kafka 1.1` and `Kafka 2.7.1`, and `Flink 1.13/1.14` is normal （`Kafka 2.4.1` is used in `Flink 1.13/1.14`）;

But `Flink 1.15` has back pressure. (I had try to change Kafka version to `Kafka 2.7.1`, which is still back pressure).

 

 

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 10:18:05 UTC 2022,,,,,,,,,,"0|z16o60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 06:05;HunterHunter;[~jark] 

Can you help me find someone to have a look

 ;;;","07/Jul/22 10:18;martijnvisser;[~HunterHunter] There have been more changes than just the Kafka upgrade in Flink 1.15, so I don't think we can easily pin this on just the Kafka upgrade. I think we need more information from logs and metrics in order to pinpoint if there's something wrong in Flink or in your local setup. Are you constantly generating data on Kafka? What format are you using? Which Kafka broker version are you using etc. At first I'm suspecting something local, because we haven't had additional reports on backpressure while Flink 1.15 has been in use for quite some time already. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Extend ""NestedProjectionUtil"" to unify support for projection pushdown for physical fields or metadata only",FLINK-28434,13470438,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,fsk119,lincoln.86xy,lincoln.86xy,07/Jul/22 01:59,13/Jul/22 07:45,04/Jun/24 20:42,,,,,,,,,,Table SQL / Planner,,,,,,,0,,,,,"Extend `NestedProjectionUtil` to unify support for projection pushdown for physical fields or metadata only. 

Currently, the `NestedProjectionUtil` relies on rewritten of the input row type, this not works on the case that a table source only [SupportsReadingMetadata|https://github.com/apache/flink/pull/20118#] and not [SupportsProjectionPushDown (which was fixed by |https://github.com/apache/flink/pull/20118#] [FLINK-28334|https://github.com/apache/flink/pull/20118#] [)|https://github.com/apache/flink/pull/20118#]

Some refactor work is needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 07:07:33 UTC 2022,,,,,,,,,,"0|z16o1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 02:01;lincoln.86xy;cc [~fsk119] would you like to look at this issue when you have time later? ;;;","07/Jul/22 07:07;fsk119;Sure. I will take a look when I am free.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow connection to mysql through mariadb driver,FLINK-28433,13470436,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,bzhaoop,xiaopenglei,xiaopenglei,07/Jul/22 01:30,01/Nov/23 09:57,04/Jun/24 20:42,,,,,,,,,,Connectors / JDBC,,,,,,,1,pull-request-available,stale-assigned,,,"Flink connector support connection to mysql. But the url must be started with ""jdbc:mysql"". 

Some user need to use mariadb dirver to connect to mysql. It can be achieved by setting the driver parameter in jdbcOptions. Unfortunately， the url verification fails.

 

as follows:

!image-2022-07-07-09-29-06-834.png!

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/22 01:29;xiaopenglei;image-2022-07-07-09-29-06-834.png;https://issues.apache.org/jira/secure/attachment/13046385/image-2022-07-07-09-29-06-834.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Nov 01 09:50:38 UTC 2023,,,,,,,,,,"0|z16o0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 03:36;bzhaoop;Hi guys, 

I'm working on this now. Thanks;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","01/Nov/23 09:50;davidradl;hi [~bzhaoop] ,   

are you still looking at this?  It looks like there was an agreed pr, but then the JDBC connector got moved out of the core repository. The fix was not ported over.

I wondered if the pr is basically the same as mySQL apart from the prefix (they are supposed to be the same). If so I wonder if we could just change the mysql dialect library to accept mariadb url prefixes as well as mysql - or is more required? 

Change MySqlDialectFactory like this:

 
public boolean acceptsURL(String url) {
return url.startsWith(""jdbc:mysql:"") || url.startsWith(""jdbc:mariadb:"");
}
 
This change works for me with my limited testing.
 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalogTable only support using the latest columns as partition keys,FLINK-28432,13470416,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,06/Jul/22 19:46,06/Jul/22 19:46,04/Jun/24 20:42,,1.16.0,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"CatalogTable.of(
schema,
TEST_COMMENT,
PartitionKeys,
BatchTableProperties);

For example the schema contains 5 columns: first, second, third, fourth, fifth. It is fine if  partition keys are fourth and fifth. But in case they are second and third, the created ResolvedCatalogTable will use fourth and fifth as the partition keys.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-06 19:46:53.0,,,,,,,,,,"0|z16nwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompletedCheckPoints stored on ZooKeeper is not up-to-date, when JobManager is restarted it fails to recover the job due to ""checkpoint FileNotFound exception""",FLINK-28431,13470366,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,aresyhzhang,aresyhzhang,06/Jul/22 14:19,13/Jan/23 03:49,04/Jun/24 20:42,18/Jul/22 12:18,1.13.2,,,,,,,,,,,,,,,0,checkpoint,ha,native-kubernetes,,"We have built a lot of flink clusters in native Kubernetes session mode, flink version 1.13.2, some clusters can run normally for 180 days and some can run for 30 days.
The following takes an abnormal flink cluster flink-k8s-session-opd-public-1132 as an example.

Problem Description:
Appears when jobmanager restarts
File does not exist: /home/flink/recovery/flink/flink-k8s-session-opd-public-1132/completedCheckpoint86fce98d7e4a
The result of this is that the entire flink cluster cannot be started. Because other tasks in session mode are also affected by the inability to start, the impact is very serious.

Some auxiliary information:
1. flink cluster id: flink-k8s-session-opd-public-1132
2. High-availability.storageDir of cluster configuration: hdfs://neophdfsv2flink/home/flink/recovery/
3.error job id: 18193cde2c359f492f76c8ce4cd20271
4. There was a similar issue before: FLINK-8770, but I saw that it was closed without being resolved.
5. The complete jommanager log I have uploaded to the attachment

My investigation ideas:

1. View the node information on the zookeeper corresponding to the jobid 18193cde2c359f492f76c8ce4cd20271:

[zk: localhost:2181(CONNECTED) 17] ls /flink/flink/flink-k8s-session-opd-public-1132/checkpoints/18193cde2c359f492f76c8ce4cd20271
[0000000000000025852, 0000000000000025851]


[zk: localhost:2181(CONNECTED) 14] get /flink/flink/flink-k8s-session-opd-public-1132/checkpoints/18193cde2c359f492f76c8ce4cd20271/0000000000000025852

??sr;org.apache.flink.runtime.state.RetrievableStreamStateHandle?U?+LwrappedStreamStateHandlet2Lorg/apache/flink/runtime/state/StreamStateHandle;xpsr9org.apache.flink.runtime.state.filesystem.FileStateHandle?u?b?J stateSizefilePathtLorg/apache/flink/core/fs/Path;xp??srorg.apache.flink.core.fs.PathLuritLjava/net/URI;xpsr
java.net.URI?x.C?I?LstringtLjava/lang/String;xptrhdfs://neophdfsv2flink/home/flink/recovery/flink/flink-k8s-session-opd-public-1132/completedCheckpoint86fce98d7e4ax
cZxid = 0x1070932e2
ctime = Wed Jul 06 02:28:51 UTC 2022
mZxid = 0x1070932e2
mtime = Wed Jul 06 02:28:51 UTC 2022
pZxid = 0x30001c957
cversion=222
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 545
numChildren = 0.

I am sure that my zk node is normal, because there are 10+ flink clusters using the same zk node, but only this cluster has problems, other clusters are normal

2. View the hdfs edits modification log of the directory corresponding to hdfs:
./hdfs-audit.log.1:2022-07-06 10:28:51,752 INFO FSNamesystem.audit: allowed=true ugi=flinkuser@HADOOP.163.GZ (auth:KERBEROS) ip=/10.91.136.213 cmd= create src=/home/flink/recovery/flink/flink-k8s-session-opd-public-1132/completedCheckpoint86fce98d7e4a dst=null perm=flinkuser:flinkuser:rw-r--r-- proto=rpc
./hdfs-audit.log.1:2022-07-06 10:29:26,588 INFO FSNamesystem.audit: allowed=true ugi=flinkuser@HADOOP.163.GZ (auth:KERBEROS) ip=/10.91.136.213 cmd= delete src=/home/flink/recovery/flink/flink-k8s-session-opd-public-1132/completedCheckpoint86fce98d7e4a dst=null perm=null proto=rpc

I don't know why flink created the directory and then deleted it, and did not update the metadata information to zookeeper, which caused the jobmanager to restart without getting the correct directory and keep restarting.","flink:1.13.2
java:1.8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jan 13 03:49:31 UTC 2023,,,,,,,,,,"0|z16nlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 14:27;martijnvisser;Have you been able to confirm that this problem also occurs with newer (still supported) versions of Flink? ;;;","07/Jul/22 02:32;aresyhzhang;Not tried with latest flink 1.15, but flink 1.14.2 will also appear;;;","07/Jul/22 05:47;yunta;[~aresyhzhang] Could you check the original JM logs to see whether checkpoint-1132 is subsumed by a newer completed checkpoint. I think that might be why checkpoint-1132 is deleted on HDFS.;;;","13/Jan/23 03:49;aresyhzhang;Upgrade the flink version to 1.16.0 to solve;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveFunctionWrapper should use UserClassLoader to load class,FLINK-28430,13470350,13430553,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,luoyuxia,luoyuxia,06/Jul/22 12:58,21/Jul/22 13:09,04/Jun/24 20:42,21/Jul/22 13:09,1.16.0,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"Now, HiveFunctionWrapper will use current thread's ClassLoader to load the Hive UDFs class. But the class loader may differ with the class loader that loads the Hive UDFs, which will cause class not found issuse. 

To solve it, we need to use UserClassLoader to load the Hive UDFs class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28451,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 21 13:09:33 UTC 2022,,,,,,,,,,"0|z16nhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 13:09;luoyuxia;Will be fixed in https://issues.apache.org/jira/browse/FLINK-28451;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce pyflink tests time,FLINK-28429,13470341,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,06/Jul/22 12:36,26/Sep/22 06:56,04/Jun/24 20:42,05/Sep/22 07:04,1.16.0,,,,,1.16.0,1.17.0,,API / Python,,,,,,,0,pull-request-available,,,,"Currently, it costs about 1 hour 30mins in pyflink tests. We need to optimize it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 05 07:04:30 UTC 2022,,,,,,,,,,"0|z16nfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 08:43;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38226&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=28867;;;","05/Sep/22 07:04;hxb;merged into master:

    f52b65fa0719de10b553fc52b39431f714d7ec66

    10663811ffc30e449a194af79c55a3148d93f410

    3929eb207b95224b357a385edff32a29bd0d31b9

    9a725746b0abdb00d3ad7135ae39bf02e8fd3abe

Merged into release-1.16:

    6191a0488b50cc63042c4784e2d4742e28f10dfe

    79c0b1f9b1a778d80a6afcfbf1e5a186e1b3dcec

    5737b45ea1b6e9c00ceb8c6dbbe536e3ec7d969c

    6c3de7628c586f6861871a8323817051e11f6feb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Example in the Elasticsearch doc of fault tolerance section is missing,FLINK-28428,13470333,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ana4,ana4,06/Jul/22 12:18,22/May/24 04:20,04/Jun/24 20:42,,1.15.1,,,,,1.20.0,,,Connectors / ElasticSearch,Documentation,,,,,,0,pull-request-available,,,,"The example in English doc 
{code:java}
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.enableCheckpointing(5000); // checkpoint every 5000 msecs{code}
 

The example in Chinese doc
{code:java}
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.enableCheckpointing(5000); // 每 5000 毫秒执行一次 checkpoint

Elasticsearch6SinkBuilder sinkBuilder = new Elasticsearch6SinkBuilder<String>()
    .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
    .setHosts(new HttpHost(""127.0.0.1"", 9200, ""http""))
    .setEmitter(
    (element, context, indexer) -> 
    indexer.add(createIndexRequest(element))); {code}
IMO, the example in Chinese doc is correct.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 14:44:17 UTC 2022,,,,,,,,,,"0|z16ne0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 12:18;ana4;I will fix this.;;;","06/Jul/22 14:44;ana4;Whether this [commit|https://github.com/apache/flink-connector-elasticsearch/commit/3c9430593d9d61254b63a66234d8dba9c269060b] changes an error doc file? Should we change the English doc but change the Chinese doc?  The only one doc is correct, either Chinese or English.
 
I found the Chinese translations of _+IMPORTANT: Checkpointing is not enabled by default but ....... +_ was deleted and the example of the fault tolerance section in the English doc wasn't updated in this former commit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniClusterResource should not override rest port settings,FLINK-28427,13470303,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,06/Jul/22 09:09,11/Nov/22 15:35,04/Jun/24 20:42,11/Nov/22 15:35,,,,,,1.17.0,,,Tests,,,,,,,0,pull-request-available,,,,"For development purposes it is useful to be able to start the MiniClusterResource with a fixed port. This would allow you to run a single test with the UI running on a well-known port (8081, or whatever the user has configured).

We can't expect users to dig through the log files to figure out what port is used, nor should encourage them to directly use a MiniCluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 11 15:35:32 UTC 2022,,,,,,,,,,"0|z16n7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/22 15:35;chesnay;master: 74569dc137038dfdb27b4216ee79cf36bbd789c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink provides M1 wheel package,FLINK-28426,13470288,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,06/Jul/22 08:40,08/Jul/22 09:32,04/Jun/24 20:42,08/Jul/22 09:32,1.16.0,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,"In [FLINK-25188|https://issues.apache.org/jira/browse/FLINK-25188], pyflink has provided the support of M1 on MacOS. We also need to provide M1 wheel package.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 09:32:22 UTC 2022,,,,,,,,,,"0|z16n48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 09:32;hxbks2ks;Merged into master via 79293510debc96bf907c7902168c2b4f932ed13c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E2E tests run out of disk space on Azure,FLINK-28425,13470280,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,06/Jul/22 08:09,30/Aug/22 02:37,04/Jun/24 20:42,30/Aug/22 02:37,1.14.6,,,,,,,,Tests,,,,,,,0,stale-assigned,test-stability,,,All builds on the {{release-1.14}} branch fail on the E2E step due to running out of disk space. Running out of disk space causes FLINK-28305,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28305,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 30 02:37:45 UTC 2022,,,,,,,,,,"0|z16n2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Aug/22 02:37;hxb;Fixed in FLINK-28680;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcExactlyOnceSinkE2eTest hangs on Azure,FLINK-28424,13470278,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,martijnvisser,martijnvisser,06/Jul/22 08:01,21/Aug/23 22:35,04/Jun/24 20:42,,1.16.0,1.17.0,,,,,,,Connectors / JDBC,,,,,,,0,auto-deprioritized-critical,auto-deprioritized-major,test-stability,,"{code:java}
2022-07-06T07:10:57.8133295Z ==========================================================================================
2022-07-06T07:10:57.8137200Z === WARNING: This task took already 95% of the available time budget of 232 minutes ===
2022-07-06T07:10:57.8140723Z ==========================================================================================
2022-07-06T07:10:57.8186584Z ==============================================================================
2022-07-06T07:10:57.8187530Z The following Java processes are running (JPS)
2022-07-06T07:10:57.8188571Z ==============================================================================
2022-07-06T07:10:58.2136012Z 825016 Jps
2022-07-06T07:10:58.2136438Z 34359 surefirebooter8568713056714319310.jar
2022-07-06T07:10:58.2136774Z 525 Launcher
2022-07-06T07:10:58.2240260Z ==============================================================================
2022-07-06T07:10:58.2240814Z Printing stack trace of Java process 825016
2022-07-06T07:10:58.2241256Z ==============================================================================
2022-07-06T07:10:58.4498109Z 825016: No such process
2022-07-06T07:10:58.4524779Z ==============================================================================
2022-07-06T07:10:58.4525272Z Printing stack trace of Java process 34359
2022-07-06T07:10:58.4525713Z ==============================================================================
2022-07-06T07:10:58.6399085Z 2022-07-06 07:10:58
2022-07-06T07:10:58.6400425Z Full thread dump OpenJDK 64-Bit Server VM (25.292-b10 mixed mode):

2022-07-06T07:10:58.7332738Z ""Legacy Source Thread - Source: Custom Source -> Map -> Sink: Unnamed (1/4)#44585"" #870775 prio=5 os_prio=0 tid=0x00007fca5c06f800 nid=0xc3c26 waiting on condition [0x00007fca503b1000]
2022-07-06T07:10:58.7333386Z    java.lang.Thread.State: WAITING (parking)
2022-07-06T07:10:58.7333759Z 	at sun.misc.Unsafe.park(Native Method)
2022-07-06T07:10:58.7334404Z 	- parking to wait for  <0x00000000d5998448> (a java.util.concurrent.CountDownLatch$Sync)
2022-07-06T07:10:58.7334943Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2022-07-06T07:10:58.7335605Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
2022-07-06T07:10:58.7336392Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
2022-07-06T07:10:58.7337195Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
2022-07-06T07:10:58.7337966Z 	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
2022-07-06T07:10:58.7338677Z 	at org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest$TestEntrySource.waitForConsumers(JdbcExactlyOnceSinkE2eTest.java:314)
2022-07-06T07:10:58.7339566Z 	at org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest$TestEntrySource.run(JdbcExactlyOnceSinkE2eTest.java:300)
2022-07-06T07:10:58.7340281Z 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-07-06T07:10:58.7340883Z 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-07-06T07:10:58.7341583Z 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37685&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=14871",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31525,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 21 22:35:21 UTC 2023,,,,,,,,,,"0|z16n20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","28/Jul/22 22:37;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","03/Aug/22 09:11;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39162&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","17/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","25/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","18/Oct/22 08:20;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42115&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=17787;;;","19/Oct/22 08:36;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42176&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=17367;;;","02/Nov/22 15:25;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42726&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=18960;;;","22/Nov/22 10:06;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43183&view=logs&j=e1276d0f-df12-55ec-86b5-c0ad597d83c9&t=66648bdf-9af9-503d-c9a7-11f783a19935&l=17552;;;","02/Jan/23 10:25;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44336&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=17041;;;","09/Jan/23 07:52;mapohl;https://issues.apache.org/jira/browse/FLINK-28424?filter=-1&jql=project%20%3D%20FLINK%20AND%20text%20~%20%22JdbcExactlyOnceSinkE2eTest%22%20AND%20status%20NOT%20IN%20(Closed%2C%20Resolved);;;","12/Jan/23 09:05;martijnvisser;release-1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44733&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=14867

;;;","13/Feb/23 09:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46031&view=logs&j=075127ba-54d5-54b0-cccf-6a36778b332d&t=c35a13eb-0df9-505f-29ac-8097029d4d79&l=18949;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionVertexCancelTest crashed with exit code 239,FLINK-28423,13470262,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,martijnvisser,martijnvisser,06/Jul/22 07:10,12/Jul/22 08:06,04/Jun/24 20:42,12/Jul/22 08:05,1.16.0,,,,,,,,Runtime / Coordination,,,,,,,0,test-stability,,,,"{code:java}
Jul 06 03:40:19 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter4834330438302305324.jar /__w/1/s/flink-runtime/target/surefire 2022-07-06T03-33-45_475-jvmRun2 surefire1590202131814132315tmp surefire_28620311342464803109tmp
Jul 06 03:40:19 [ERROR] Error occurred in starting fork, check output in log
Jul 06 03:40:19 [ERROR] Process Exit Code: 239
Jul 06 03:40:19 [ERROR] Crashed tests:
Jul 06 03:40:19 [ERROR] org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest
Jul 06 03:40:19 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Jul 06 03:40:19 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter4834330438302305324.jar /__w/1/s/flink-runtime/target/surefire 2022-07-06T03-33-45_475-jvmRun2 surefire1590202131814132315tmp surefire_28620311342464803109tmp
Jul 06 03:40:19 [ERROR] Error occurred in starting fork, check output in log
Jul 06 03:40:19 [ERROR] Process Exit Code: 239
Jul 06 03:40:19 [ERROR] Crashed tests:
Jul 06 03:40:19 [ERROR] org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest
Jul 06 03:40:19 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Jul 06 03:40:19 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
Jul 06 03:40:19 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
Jul 06 03:40:19 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Jul 06 03:40:19 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Jul 06 03:40:19 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Jul 06 03:40:19 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
Jul 06 03:40:19 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
Jul 06 03:40:19 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
Jul 06 03:40:19 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
Jul 06 03:40:19 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
Jul 06 03:40:19 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
Jul 06 03:40:19 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
Jul 06 03:40:19 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37685&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8128",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28392,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 12 08:06:48 UTC 2022,,,,,,,,,,"0|z16myg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 07:21;rmetzger;This error usually indicates that the FatalExecptionHandler has been called. But I can't see any evidence from the logs. It seems to be caused by the {{ExecutionVertexCancelTest}}, here are the last lines:

{code}
================================================================================
Test testCancelFromScheduled(org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest) is running.
--------------------------------------------------------------------------------
03:36:05,646 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Created execution graph 2d39dc1365af352d59f55034de0a5618 for job f81f60a2092cbe3286458b04211040ea.
03:36:05,646 [                main] INFO  org.apache.flink.runtime.scheduler.SchedulerTestingUtils     [] - Running initialization on master for job Unnamed job (f81f60a2092cbe3286458b04211040ea).
03:36:05,646 [                main] INFO  org.apache.flink.runtime.scheduler.SchedulerTestingUtils     [] - Successfully ran initialization on master in 0 ms.
03:36:05,647 [                main] INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 new pipelined regions in 0 ms, total 1 pipelined regions currently.
03:36:05,647 [                main] INFO  org.apache.flink.runtime.scheduler.SchedulerTestingUtils     [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@7a977d23 for Unnamed job (f81f60a2092cbe3286458b04211040ea).
03:36:05,647 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - TestVertex (1/1) (2d39dc1365af352d59f55034de0a5618_403847e7f01320ff1624af5085616f1f_0_0) switched from SCHEDULED to CANCELING.
03:36:05,647 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - TestVertex (1/1) (2d39dc1365af352d59f55034de0a5618_403847e7f01320ff1624af5085616f1f_0_0) switched from CANCELING to CANCELED.
03:36:05,647 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 2d39dc1365af352d59f55034de0a5618_403847e7f01320ff1624af5085616f1f_0_0.
03:36:05,647 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest [] - 
--------------------------------------------------------------------------------
Test testCancelFromScheduled(org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest) successfully run.
================================================================================
03:36:05,648 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest [] - 
================================================================================
Test testRepeatedCancelFromRunning(org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest) is running.
--------------------------------------------------------------------------------
03:36:05,648 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Created execution graph e43f14294e9e15f2495185182b3cc6e1 for job 7dee3fa6bd5706737478f5c7257fd2fb.
03:36:05,648 [                main] INFO  org.apache.flink.runtime.scheduler.SchedulerTestingUtils     [] - Running initialization on master for job Unnamed job (7dee3fa6bd5706737478f5c7257fd2fb).
03:36:05,648 [                main] INFO  org.apache.flink.runtime.scheduler.SchedulerTestingUtils     [] - Successfully ran initialization on master in 0 ms.
03:36:05,649 [                main] INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 new pipelined regions in 0 ms, total 1 pipelined regions currently.
03:36:05,653 [                main] INFO  org.apache.flink.runtime.scheduler.SchedulerTestingUtils     [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@267e36f5 for Unnamed job (7dee3fa6bd5706737478f5c7257fd2fb).
03:36:05,653 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - TestVertex (1/1) (e43f14294e9e15f2495185182b3cc6e1_22419611cc620103f97874a5f4f35f01_0_0) switched from RUNNING to CANCELING.
03:36:05,654 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - TestVertex (1/1) (e43f14294e9e15f2495185182b3cc6e1_22419611cc620103f97874a5f4f35f01_0_0) switched from CANCELING to CANCELED.
03:36:05,654 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution e43f14294e9e15f2495185182b3cc6e1_22419611cc620103f97874a5f4f35f01_0_0.
03:36:05,654 [                main] INFO  org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest [] - 
--------------------------------------------------------------------------------
Test testRepeatedCancelFromRunning(org.apache.flink.runtime.executiongraph.ExecutionVertexCancelTest) successfully run.
================================================================================

{code};;;","12/Jul/22 07:23;rmetzger;I'd observe this issue and see if it is only this test going out with the {{Process Exit Code: 239}}, or different tests.;;;","12/Jul/22 08:06;chesnay;The error is incorrectly attributed because it's thrown from a thread pool after a test has finished.

If you trace back the fatal error from the logs using the thread info (pool-469-thread-1), you'll find it originates from {{{}RemoveCachedShuffleDescriptorTest#testRemoveOffloadedCacheForPointwiseEdgeAfterFailover{}}}, so it's a duplicate of FLINK-28392.
{code:java}
03:36:01,124 [   pool-469-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'pool-469-thread-1' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.NoSuchElementException: No value present
    at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:848) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2168) ~[?:1.8.0_292]
    at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$deployAll$4(DefaultExecutionDeployer.java:193) ~[classes/:?]
    at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:848) ~[?:1.8.0_292]
    at java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2168) ~[?:1.8.0_292]
    at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.waitForAllSlotsAndDeploy(DefaultExecutionDeployer.java:156) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.allocateSlotsAndDeploy(DefaultExecutionDeployer.java:108) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlotsAndDeploy(DefaultScheduler.java:423) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.maybeScheduleRegion(PipelinedRegionSchedulingStrategy.java:227) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.maybeScheduleRegions(PipelinedRegionSchedulingStrategy.java:212) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.restartTasks(PipelinedRegionSchedulingStrategy.java:166) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.DefaultScheduler.restartTasks(DefaultScheduler.java:378) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$null$3(DefaultScheduler.java:342) ~[classes/:?]
    at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:719) [?:1.8.0_292]
    at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:701) [?:1.8.0_292]
    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_292]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.util.NoSuchElementException: No value present
    at java.util.Optional.get(Optional.java:135) ~[?:1.8.0_292]
    at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.getExecutionOrThrow(DefaultExecutionDeployer.java:343) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.handleTaskDeploymentFailure(DefaultExecutionDeployer.java:338) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.deployTaskSafe(DefaultExecutionDeployer.java:331) ~[classes/:?]
    at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$deployOrHandleError$7(DefaultExecutionDeployer.java:319) ~[classes/:?]
    at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
    ... 24 more{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fall back to lastest supported HiveShim when user use a higher hive version,FLINK-28422,13470256,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,06/Jul/22 06:55,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,,1.20.0,,,Connectors / Hive,,,,,,,0,,,,,"Flink provides flink-connector-hive.jar, so that user can bundle it with a specific hive-exec.jar to make Flink read / write their Hive.

The flink-connector-hive mantains the Hive versions that Flink supports.  But when Hive releases a higher version, which may well be not in the maintain list of Flink for the delay of Flink's release .

Then user will find the unsupported version exception with such higher version hive-exec.jar.

The better way is to fall back to a nearest Hive version directly, with a warning message.  It usually works fine unless some big changes in Hive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 11:29:21 UTC 2022,,,,,,,,,,"0|z16mx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 11:29;martijnvisser;I'm wondering if this is a good idea. If we do this, then we'll easily forget to update the documentation with the versions of Hive that are supported. I think it's better to explicitly add support for specific versions. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SourceFunction as full caching provider in lookup table source,FLINK-28421,13470254,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,smiralex,renqs,renqs,06/Jul/22 06:44,09/Aug/22 23:16,04/Jun/24 20:42,,,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,Support full caching in lookup table source,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-06 06:44:15.0,,,,,,,,,,"0|z16mwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support partial lookup caching in lookup join runners,FLINK-28420,13470252,13470246,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,06/Jul/22 06:42,22/Jun/23 06:15,04/Jun/24 20:42,08/Aug/22 02:33,,,,,,1.16.0,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,"Support partial lookup caching in lookup join runners, including {{LookupJoinRunner}}, {{LookupJoinWithCalcRunner}}, {{AsyncLookupJoinRunner}} and {{AsyncLookupJoinWithCalcRunner}}",,,,,,,,,,,,,FLINK-28849,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27411,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 08 02:33:24 UTC 2022,,,,,,,,,,"0|z16mw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 02:33;jark;Fixed in master: bc76a93239e159fba7277da96336c3796887e6c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add runtime provider interface for full caching lookup,FLINK-28419,13470251,13470246,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,smiralex,renqs,renqs,06/Jul/22 06:38,08/Dec/23 10:04,04/Jun/24 20:42,04/Aug/22 14:56,,,,,,1.16.0,,,,,,,,,,0,pull-request-available,,,,Add runtime provider interface for full caching lookup,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 04 14:57:08 UTC 2022,,,,,,,,,,"0|z16mw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 14:57;renqs;Merged on master 5405239dec0884dff746129c73955c90f455c465;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add runtime provider interface for partial caching lookup,FLINK-28418,13470250,13470246,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,renqs,renqs,renqs,06/Jul/22 06:37,03/Aug/22 09:55,04/Jun/24 20:42,03/Aug/22 09:54,,,,,,1.16.0,,,Table SQL / API,,,,,,,0,pull-request-available,,,,Add runtime provider interface for partial caching lookup,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 09:55:04 UTC 2022,,,,,,,,,,"0|z16mvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 09:55;renqs;Merged on master: 8c6a9e58f1ebd9f1804498df490e5c9bd343c304;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add interface and default implementation for cache in lookup table,FLINK-28417,13470249,13470246,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,renqs,renqs,renqs,06/Jul/22 06:36,03/Aug/22 02:21,04/Jun/24 20:42,03/Aug/22 02:21,,,,,,1.16.0,,,Table SQL / API,,,,,,,0,pull-request-available,,,,"Add interfaces for cache in lookup table, including {{{}LookupCache{}}}, a default implementation {{DefaultLookupCache}}, and cache related metric group",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 02:21:30 UTC 2022,,,,,,,,,,"0|z16mvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 02:21;renqs;Merged to master: ed8870e602d2ee91e57dec5b9b2ba90bf2617070;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the new LookupFunction interface for lookup table source,FLINK-28416,13470247,13470246,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,renqs,renqs,renqs,06/Jul/22 06:30,02/Aug/22 07:02,04/Jun/24 20:42,02/Aug/22 07:02,,,,,,1.16.0,,,Table SQL / API,,,,,,,0,pull-request-available,,,,"Add the new {{{}LookupFunction{}}}, {{AsyncLookupFunction}} and their related providers in place of the top-level {{TableFunction}} as the API for lookup table",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 02 07:02:56 UTC 2022,,,,,,,,,,"0|z16mv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 07:02;renqs;Merged to master 3e2620bc785b3b4d82f1f188eb7b1e0e129b14d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-221: Support Partial and Full Caching in Lookup Table Source,FLINK-28415,13470246,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,renqs,renqs,renqs,06/Jul/22 06:28,12/Oct/22 04:26,04/Jun/24 20:42,09/Aug/22 23:17,1.16.0,,,,,1.16.0,,,Table SQL / API,Table SQL / Runtime,,,,,,0,,,,,Please see the FLIP for any details: https://cwiki.apache.org/confluence/display/FLINK/FLIP-221%3A+Abstraction+for+lookup+source+cache+and+metric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25409,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-06 06:28:34.0,,,,,,,,,,"0|z16muw:",9223372036854775807,"FLIP-221 provides a unified caching framework for lookup tables, including partial caching which loads entries into cache on a just-in-time basis, and full caching which loads all entries from the external system into the cache. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
" Using flink to query the hive table, an exception occurred, SQL validation failed. Failed to get table schema from deserializer",FLINK-28414,13470236,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,xcrossed,xcrossed,06/Jul/22 05:23,07/Jul/22 01:38,04/Jun/24 20:42,06/Jul/22 11:30,1.14.2,,,,,,,,Connectors / Hive,,,,,,,0,flink,hive-connector,serializable,sql,"org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: SQL validation failed. Failed to get table schema from deserializer
at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. Failed to get table schema from deserializer
at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:164)
at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:107)
at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:215)
at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:716)
at com.zhhainiao.wp.stat.PaidConversationRate$.main(PaidConversationRate.scala:153)
at com.zhhainiao.wp.stat.PaidConversationRate.main(PaidConversationRate.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
... 11 more
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to get table schema from deserializer
at org.apache.flink.table.catalog.hive.client.HiveShimV110.getFieldsFromDeserializer(HiveShimV110.java:116)
at org.apache.flink.table.catalog.hive.util.HiveTableUtil.getNonPartitionFields(HiveTableUtil.java:610)
at org.apache.flink.table.catalog.hive.util.HiveTableUtil.createTableSchema(HiveTableUtil.java:103)
at org.apache.flink.table.catalog.hive.HiveCatalog.instantiateCatalogTable(HiveCatalog.java:744)
at org.apache.flink.table.catalog.hive.HiveCatalog.getTable(HiveCatalog.java:461)
at org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:425)
at org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:395)
at org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:73)
at org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)
at org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)
at org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)
at org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)
at org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)
at org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)
at org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)
at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3085)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3085)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3132)
at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:117)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3076)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3085)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3070)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3335)
at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:997)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:975)
at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:952)
at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:159)
... 22 more
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.flink.table.catalog.hive.client.HiveShimV110.getFieldsFromDeserializer(HiveShimV110.java:109)
... 67 more
Caused by: MetaException(message:java.lang.ClassNotFoundException Class org.openx.data.jsonserde.JsonSerDe not found)
at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:432)
at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:411)
... 72 more","hive version:2.1.1

flink version:1.14.2

hive table info
CREATE EXTERNAL TABLE `xxx_events`(
`ver` string COMMENT 'from deserializer',
`recv_time`bigint COMMENT 'from deserializer',
`device_id` string COMMENT 'from deserializer',
`app_version` string COMMENT 'from deserializer',
`item_id` string COMMENT 'from deserializer',
`is_new`int COMMENT 'from deserializer',
`ptype`int COMMENT 'from deserializer',
`platform` string COMMENT 'from deserializer',
`scene` string COMMENT 'from deserializer',
`event_type` string COMMENT 'from deserializer')
PARTITIONED BY (
`dt` string)
ROW FORMAT SERDE
'org.openx.data.jsonserde.JsonSerDe'
STORED AS INPUTFORMAT
'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
's3://xxxx.com/user/hive/warehouse/xxx/events'
TBLPROPERTIES (
'transient_lastDdlTime'='1623066811')
Time taken: 0.087 seconds, Fetched: 41 row(s)
 
flink table code
 
val hive = new HiveCatalog(name, defaultDatabase, hiveConfDir)
tableEnv.registerCatalog(name, hive)
//
//// set the HiveCatalog as the current catalog of the session
tableEnv.useCatalog(name)
 
//query sql
statSql=""select * from mobile_events where dt = '20220706' ""
 
//get result
 
val rs = tableEnv.sqlQuery(statSql)
 
and then occurred, SQL validation failed. Failed to get table schema from deserializer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,scala,,,Thu Jul 07 01:38:26 UTC 2022,,,,,,,,,,"0|z16mso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 06:15;zoucao;Hi, could you check whether the dependency of 'org.openx.data.jsonserde.JsonSerDe' has been added into you jar? I think may be you lost some dependencies about hive serde.;;;","06/Jul/22 11:30;martijnvisser;Please ask these type of questions on the user mailing list, Stackoverflow or Slack. See https://flink.apache.org/gettinghelp.html;;;","07/Jul/22 01:38;xcrossed;ok;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support ParquetColumnarRowInputFormat in PyFlink,FLINK-28413,13470228,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,06/Jul/22 03:50,08/Jul/22 07:11,04/Jun/24 20:42,08/Jul/22 07:11,,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 07:11:20 UTC 2022,,,,,,,,,,"0|z16mqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 07:11;dianfu;Merged to master via e164e22fc837af365299bccd0f978a4ebd84b8ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kubernetes-client  doesn't infer kubeconfig encryption algorithm and fails on k3s,FLINK-28412,13470221,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kalencaya,kalencaya,06/Jul/22 00:42,25/Aug/22 06:24,04/Jun/24 20:42,,1.13.6,1.14.2,1.14.3,1.14.4,1.15.0,,,,Deployment / Kubernetes,,,,,,,0,,,,,"[k3s|https://github.com/k3s-io/k3s] is a lightweight and has fully conformant production-ready Kubernetes distribution.
k3s kube config use a different encryption algorithm which not same with Kubernetes config, and kubernetes-client also doesn't infer kube config encryption algorithm until 5.10.0 version.

kubernetes-client fixes k3s kube config compatibility through [issue|https://github.com/fabric8io/kubernetes-client/issues/3535] and works from 5.10.0 version.

flink-kubernetes module now uses 5.5.0 version and submitting to k3s would fail
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 25 06:24:12 UTC 2022,,,,,,,,,,"0|z16mpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 00:44;kalencaya;{code:java}
// kubernetes-client:5.10.0
// io.fabric8.kubernetes.client.Config.java
public static void configFromSysPropsOrEnvVars(Config config) {
    // ...
    config.setClientKeyAlgo(getKeyAlgorithm(config.getClientKeyFile(), config.getClientKeyData()));
    // ...
}
{code};;;","25/Aug/22 06:24;bzhaoop;For Flink 1.16.0, fabric8 client had been bumped to 5.12.3 via https://issues.apache.org/jira/browse/FLINK-28481;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinator exception may fail Session Cluster,FLINK-28411,13470170,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,darenwkt,darenwkt,darenwkt,05/Jul/22 16:51,07/Sep/22 14:43,04/Jun/24 20:42,07/Sep/22 14:30,,,,,,,,,Connectors / Common,,,,,,,0,pull-request-available,,,,"Part of Scheduler's startScheduling procedure involves starting all OperatorCoordinatorHolder, and when one of the OperatorCoordinator fails to start, the exception is forwarded up the stack triggering a JobMaster failover. However, JobMaster failover only works if HA is enabled[1]. If HA is not enabled the fatal error handler will simply exit the JM process killing the entire cluster. This is problematic in the case of a session cluster where there may be multiple jobs running. It also does not play well with external tooling that does not expect job failure to cause a full cluster failure. 

 

It would be preferable if failure to start an OperatorCoordinator did not take down the entire cluster, but instead failed that particular job. 

 

This issue is similar to https://issues.apache.org/jira/browse/FLINK-24303 which fix this issue for a SourceCoordinator specifically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 15:27:23 UTC 2022,,,,,,,,,,"0|z16mgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 17:38;martijnvisser;[~jqin] what do you think?;;;","06/Jul/22 00:49;becket_qin;[~martijnvisser] Yes, I think this is a problem. I need to check a bit more on the JM initialization logic to see what is the best fix. As Stephan mentioned in the other ticket, putting the initialization in the enumerator thread could be an option. It postpones the exception handling to after the JM initialization finishes, at which point the JM will be able to handle the per job global failure correctly.;;;","06/Jul/22 15:27;darenwkt;Hi [~becket_qin], [~martijnvisser], I have attempted to give this fix a go and submitted a PR: [https://github.com/apache/flink/pull/20186.]

Can I ask for your help to review the PR please? Thank you very much.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Elasticsearch E2E test cases to the external repository,FLINK-28410,13470159,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,afedulov,afedulov,05/Jul/22 15:00,29/Jun/23 14:22,04/Jun/24 20:42,13/Sep/22 14:01,,,,,,elasticsearch-3.0.0,,,Connectors / ElasticSearch,,,,,,,0,pull-request-available,,,,Depends on FLINK-28409,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Sep 13 14:01:04 UTC 2022,,,,,,,,,,"0|z16meo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 14:01;chesnay;main: a9328dfcd7793b73cddc428d4c4a0aa827d220ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move FlinkContainerTestEnvironment and its dependencied into flink-connector-test-utils,FLINK-28409,13470156,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,05/Jul/22 14:37,08/Jul/22 09:07,04/Jun/24 20:42,08/Jul/22 07:30,,,,,,1.16.0,,,Connectors / ElasticSearch,,,,,,,0,pull-request-available,,,,"As was discussed in this [1] thread, FlinkContainerTestEnvironment needs to become available via Maven central to the externalized connector repositories. For this, it has to be moved into flink-connector-test-utils, which is already being published.


[1] [https://lists.apache.org/thread/1otr8wjwp5oyhk2b481bvrdsx9906wzy] 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 07:30:00 UTC 2022,,,,,,,,,,"0|z16me0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 07:30;chesnay;master: 6c0dba21623f13c944def027cea10b245e844131;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FAQ / best practices page for Kubernetes Operator,FLINK-28408,13470151,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Do,morhidi,mbalassi,mbalassi,05/Jul/22 13:31,28/Nov/22 09:40,04/Jun/24 20:42,28/Nov/22 09:40,,,,,,,,,Documentation,Kubernetes Operator,,,,,,0,,,,,"Based on our recent interactions with the users we should gather some frequently recurring topics in the docs. The following initial ones come to my mind:
 # Enabling plugins,[especially built-in ones|https://stackoverflow.com/questions/72826712/apache-flink-operator-enable-azure-fs-hadoop/72844654]
 # Running SQL jobs on the operator
 # Templating FlinkDeployment CRs for multiple environments (kustomize)
 # Packaging the user jar into a custom container based on the flink base image
 # Logging / metrics integration
 # Using env variables",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 31 09:37:09 UTC 2022,,,,,,,,,,"0|z16mcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/22 09:37;morhidi;[~pvary] can you take this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Jenkins benchmark script more autonomous,FLINK-28407,13470150,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,akalashnikov,akalashnikov,akalashnikov,05/Jul/22 13:19,22/May/23 16:25,04/Jun/24 20:42,,,,,,,,,,Benchmarks,,,,,,,0,pull-request-available,stale-assigned,,,"Right now, we have Jenkins scripts in the benchmarks repository which contains all logic for launching benchmarks and sending data to Codespeed. But unfortunately, it is not clear to external observers how to use that in case of current Jenkins will be lost.
The suggestion is:
* add information to Readme file about Jenkins and Codespeed.
* add default values to the script rather than keeping them in only Jenkins.

After this task, anybody should be able easily to configure these benchmarks on their own Jenkins + Codespeed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 04 22:37:44 UTC 2022,,,,,,,,,,"0|z16mco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build fails on retrieving org.pentaho:pentaho-aggdesigner-algorithm,FLINK-28406,13470148,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,prabhujoseph,prabhujoseph,05/Jul/22 13:10,05/Jul/22 13:47,04/Jun/24 20:42,05/Jul/22 13:47,1.15.0,,,,,,,,,,,,,,,0,,,,,"Build fails at flink-connector-hive_2.12 and flink-sql-client on retrieving org.pentaho:pentaho-aggdesigner-algorithm from the dependency hive-exec.

Adding same fix as [FLINK-16432|https://github.com/apache/flink/pull/11316#issuecomment-600969333] to both flink-connector-hive_2.12 and flink-sql-client fixes the issue.


*Build Failure at flink-connector-hive_2.12*

{code}

[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  04:22 min (Wall Clock)
[INFO] Finished at: 2022-07-05T12:26:17Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.16-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-exec:jar:2.3.9 -> org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [repository.jboss.org (http://repository.jboss.org/nexus/content/groups/public/, default, disabled), conjars (http://conjars.org/repo, default, releases+snapshots), apache.snapshots (http://repository.apache.org/snapshots, default, snapshots)] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :flink-connector-hive_2.12

{code}

*Build Failure at flink-sql-client*

{code}

[ERROR] Failed to execute goal on project flink-sql-client: Could not resolve dependencies for project org.apache.flink:flink-sql-client:jar:1.16-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-exec:jar:2.3.9 -> org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [repository.jboss.org (http://repository.jboss.org/nexus/content/groups/public/, default, disabled), conjars (http://conjars.org/repo, default, releases+snapshots), apache.snapshots (http://repository.apache.org/snapshots, default, snapshots)] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :flink-sql-client

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27640,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-05 13:10:33.0,,,,,,,,,,"0|z16mc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Confluent Platform images to v7.2.2,FLINK-28405,13470135,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,05/Jul/22 12:00,18/Oct/22 13:35,04/Jun/24 20:42,18/Oct/22 13:35,1.16.0,,,,,1.17.0,,,Connectors / Kafka,Tests,,,,,,0,pull-request-available,stale-assigned,,,"We have updated the used Kafka Clients to v3.1.1 via FLINK-28060 and then to v3.2.1 via FLINK-28060 and finally to v3.2.3 via FLINK-29513, but we are using Confluent Platform 6.2.2 which supports up to Kafka 2.8.0.

We should update to Confluent Platform v7.2.2 (latest version of 7.2), which includes support for Kafka 3.2.3. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 18 13:35:24 UTC 2022,,,,,,,,,,"0|z16m9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","18/Oct/22 13:35;martijnvisser;Fixed in master: 2ec9117d46756960ba6a8d9bc2cd774a018a637e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Annotation @InjectClusterClient does not work correctly with RestClusterClient,FLINK-28404,13470121,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,Leo Zhou,Leo Zhou,05/Jul/22 11:28,06/Jul/22 08:56,04/Jun/24 20:42,06/Jul/22 08:54,,,,,,1.15.2,1.16.0,,Tests,,,,,,,0,pull-request-available,,,,"*test code:*
{code:java}
public class Test {

    @RegisterExtension
    private static final MiniClusterExtension MINI_CLUSTER_RESOURCE =
            new MiniClusterExtension(
                    new MiniClusterResourceConfiguration.Builder()
                            .setNumberTaskManagers(1)
                            .setNumberSlotsPerTaskManager(4)
                            .build());

    @org.junit.jupiter.api.Test
    void test(@InjectClusterClient RestClusterClient<?> restClusterClient) throws Exception {
        Object clusterId = restClusterClient.getClusterId();
    }
} {code}
*error info:*
{code:java}
org.junit.jupiter.api.extension.ParameterResolutionException: No ParameterResolver registered for parameter [org.apache.flink.client.program.rest.RestClusterClient<?> arg0] in method... {code}
this problem occurs because [MiniClusterExtension#supportsParameter|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/junit5/MiniClusterExtension.java#L168] does not support *_RestClusterClient_* parameterType. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 08:54:58 UTC 2022,,,,,,,,,,"0|z16m68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 11:33;Leo Zhou;Hi [~rmetzger] Can you confirm this problem ？;;;","05/Jul/22 12:03;rmetzger;What's the benefit of injecting {{RestClusterClient}} instead of just {{ClusterClient}}?;;;","05/Jul/22 12:25;Leo Zhou;For example, ReactiveModeITCase use {{RestClusterClient}} to get job details, See [ReactiveModeITCase.java#L202|https://github.com/apache/flink/blob/a89152713aa58647841c46ed2335b45d24c553f9/flink-tests/src/test/java/org/apache/flink/test/scheduling/ReactiveModeITCase.java#L202]. when we port ReactiveModeITCase to junit5, we would need inject {{RestClusterClient.}}

And [MiniClusterExtension.java#L107 |https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-test-utils/src/main/java/org/apache/flink/test/junit5/MiniClusterExtension.java#L107]provides an example about how to inject RestClusterClient, but it does not work well.;;;","05/Jul/22 12:41;chesnay;This indeed doesn't work. We need to invert the {{isAssignableFrom}} check in \{{MiniClusterExtension#supportsParameter}}.;;;","05/Jul/22 12:50;chesnay;[~rmetzger] The plain cluster client is quite limited, while the rest client can be used to query the entire rest api.;;;","05/Jul/22 14:16;rmetzger;Thanks!;;;","06/Jul/22 08:54;chesnay;master: 85ba36fc0123852a9dbc8663ea786f96682f1128

1.15: ef84a060b4e4b82c209080ac0963cacb0be9f05f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlParserPos value for SqlCall is ambiguous,FLINK-28403,13470113,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,suheng.cloud,suheng.cloud,05/Jul/22 11:16,05/Jul/22 13:42,04/Jun/24 20:42,05/Jul/22 12:39,1.15.0,,,,,,,,Table SQL / API,,,,,,,0,,,,,"Dear Community:

For a long time I was puzzled by how SqlParserPos actually means in SqlCall, because I found the definitions for different statements in parserImpls.ftl are not unified.

For example,  in SqlCreateTable, we use `startPos.plus(getPos())` which means if I want to find out the end position of the last token, calling `SqlCreateTable::getParserPosition::getEndxx` is ok, but in some other statement like SqlRichDescribeTable, we just use the intermediate `pos` as the final position, so I must call `SqlRichDescribeTable.getParserPosition().plusAll(SqlRichDescribeTable::getOperandList())` to merge the result pos. Even worse in SqlStatementSet, which wrapped in SqlExecuate,  both above methods not work because no child operand covered `END` syntax.

My question is
 # Does the SqlParserPosition mean entire span for a sqlCall or not ?
 # What is the easyest and right way to get the ""final position"" of a SqlCall/SqlNode in fink sql?

Correct me if I miss some point, thanks for any help.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 13:42:23 UTC 2022,,,,,,,,,,"0|z16m4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 12:39;martijnvisser;[~suheng.cloud] Please reach out to the Flink User mailing list or Slack channel for help, see https://flink.apache.org/gettinghelp.html. Jira is meant for bugs or new features. ;;;","05/Jul/22 13:03;suheng.cloud;[~martijnvisser] thanks, actually I wonder whether it is a bug, so I think maybe reported in Jira more suitable :)

cc [~jark]  sorry to ping you in,  can you help to have a look at this issue, thanks~;;;","05/Jul/22 13:42;martijnvisser;[~suheng.cloud] Please don't start pinging people. As mentioned in the link I've pasted, if in doubt if this is a bug and therefore needs to end up in Jira, it's better to first ask on the mailing list. If the conclusion is that this is indeed a bug, then we can re-open the ticket. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create FailureHandlingResultSnapshot with the truly failed execution,FLINK-28402,13470104,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,05/Jul/22 09:46,11/Jul/22 16:22,04/Jun/24 20:42,11/Jul/22 16:22,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"Previously, FailureHandlingResultSnapshot was always created to treat the only current attempt of an execution vertex as the failed execution. This is no longer right in speculative execution cases, in which an execution vertex can have multiple current executions, and any of them may fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 16:22:39 UTC 2022,,,,,,,,,,"0|z16m2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 16:22;zhuzh;Done via edd10a1d8fd09f8a5e296ecdf0201945d77d5ff2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove snapshot repo from quickstarts for released versions,FLINK-28401,13470087,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,05/Jul/22 09:06,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,,1.20.0,,,Quickstarts,,,,,,,0,,,,,"The quickstarts enable the apache snapshot repository, even when they are for fully released versions.
We should cut the snapshot repository from the quickstarts before the release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-05 09:06:19.0,,,,,,,,,,"0|z16lyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Customize table source. After inheriting Richparallelsourcefunction class, data disorder under Multiple Parallelism",FLINK-28400,13470085,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,thecai,thecai,05/Jul/22 09:00,05/Jul/22 12:41,04/Jun/24 20:42,,1.12.0,,,,,,,,Table SQL / API,,,,,,,0,,,,,"[^ESSourceFunction.java]

^The cycle of life and death occurs on line 118；^

^However, when inheriting the Richsourcefunction class, there will be no life and death cycle,but the speed of reading data is flow；^

^maybe the Richsourcefunction thread unsafe suspexted;^","<properties>
<java.version>1.8</java.version>
<maven.compiler.source>${java.version}</maven.compiler.source>
<maven.compiler.target>${java.version}</maven.compiler.target>
<flink.version>1.12.0</flink.version>
<scala.binary.version>2.12</scala.binary.version>
<hadoop.version>3.1.3</hadoop.version>
</properties>",43200,43200,,0%,43200,43200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/22 08:53;thecai;ESSourceFunction-1.java;https://issues.apache.org/jira/secure/attachment/13046276/ESSourceFunction-1.java","05/Jul/22 08:53;thecai;ESSourceFunction.java;https://issues.apache.org/jira/secure/attachment/13046277/ESSourceFunction.java","05/Jul/22 08:49;thecai;image-2022-07-05-16-49-41-235.png;https://issues.apache.org/jira/secure/attachment/13046280/image-2022-07-05-16-49-41-235.png","05/Jul/22 08:50;thecai;image-2022-07-05-16-50-20-394.png;https://issues.apache.org/jira/secure/attachment/13046279/image-2022-07-05-16-50-20-394.png","05/Jul/22 08:50;thecai;image-2022-07-05-16-50-30-563.png;https://issues.apache.org/jira/secure/attachment/13046278/image-2022-07-05-16-50-30-563.png",,,,,,5.0,,,,,,,,,,,,,,,,,,,,,暂无,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,,,Tue Jul 05 12:41:16 UTC 2022,,,,,,,,,,"0|z16ly8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 12:41;martijnvisser;[~thecai] Please validate your problem with Flink 1.15 since Flink 1.12 is no longer supported by the community. I also don't fully understand the problem that you're trying to explain. Can you elaborate? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor CheckedThread to accept runnable via constructor,FLINK-28399,13470083,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,chesnay,chesnay,05/Jul/22 08:41,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,,1.20.0,,,API / Core,,,,,,,0,pull-request-available,stale-assigned,,,Make the CheckedThread an easier drop-in replacements for threads by passing the runnable via the constructor.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Aug 21 22:37:56 UTC 2022,,,,,,,,,,"0|z16lxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 20:07;Weijie Guo;Hi [~chesnay]

Thanks for open this ticket, I think this idea is very reasonable. We should use `CheckedThread` just like java's `Thread`. The only difference is that `CheckedThread` will help us take care of exceptions. I have prepared a PR for this ticket, can you help assign this ticket to me and take a look.;;;","21/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointCoordinatorTriggeringTest.discardingTriggeringCheckpointWillExecuteNextCheckpointRequest( gets stuck,FLINK-28398,13470082,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,05/Jul/22 08:39,11/Mar/24 12:44,04/Jun/24 20:42,,1.16.0,,,,,1.20.0,,,Runtime / Checkpointing,,,,,,,0,stale-assigned,test-stability,,,"{code:java}
Jul 01 02:16:55 ""main"" #1 prio=5 os_prio=0 tid=0x00007fe41000b800 nid=0x5ca2 in Object.wait() [0x00007fe41a429000]
Jul 01 02:16:55    java.lang.Thread.State: WAITING (on object monitor)
Jul 01 02:16:55 	at java.lang.Object.wait(Native Method)
Jul 01 02:16:55 	at java.lang.Object.wait(Object.java:502)
Jul 01 02:16:55 	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:61)
Jul 01 02:16:55 	- locked <0x00000000f096ab58> (a java.lang.Object)
Jul 01 02:16:55 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.discardingTriggeringCheckpointWillExecuteNextCheckpointRequest(CheckpointCoordinatorTriggeringTest.java:731)
Jul 01 02:16:55 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 01 02:16:55 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 01 02:16:55 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 01 02:16:55 	at java.lang.reflect.Method.invoke(Method.java:498)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37433&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=15207",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28557,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Sep 13 03:28:08 UTC 2022,,,,,,,,,,"0|z16lxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 08:39;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37609&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8620;;;","06/Jul/22 13:25;chesnay;Well this test is a bit off; there are main thread violations happening on successful runs...;;;","06/Jul/22 13:30;chesnay;The only difference in the logs between a failed and successful run is that for the failed run the first checkpoint expires immediately.
{code:java}
03:31:12,570 [   pool-202-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1656991872568 for job c0aded0fd4229f9872c04f6ff331e89c.
03:31:12,589 [   pool-202-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint 1 of job c0aded0fd4229f9872c04f6ff331e89c expired before completing. {code};;;","06/Jul/22 13:34;chesnay;The test seems to be using a checkpoint timeout of 10ms (default in the CheckpointCoordinatorConfigurationBuilder).

Maybe that value is so low that it can trigger immediate failures. If I set the timeout 1 I can reproduce the issue easily.;;;","06/Jul/22 13:34;chesnay;[~pnowojski] thoughts?;;;","07/Jul/22 08:08;chesnay;[~gaoyunhaii] Do you have some thoughts on this?;;;","15/Jul/22 03:16;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38187&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","14/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","13/Sep/22 03:28;hxb;Given that it hasn't appeared for two months, I will downgrade the priority to major.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-245] Source Supports Speculative Execution For Batch Job,FLINK-28397,13470079,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,jingzhang,jingzhang,jingzhang,05/Jul/22 08:30,16/Aug/22 03:56,04/Jun/24 20:42,16/Aug/22 03:56,1.16.0,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,,,,,"This is the umbrella ticket of [FLIP-245|https://cwiki.apache.org/confluence/display/FLINK/FLIP-245%3A+Source+Supports+Speculative+Execution+For+Batch+Job].

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28131,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 13:50:25 UTC 2022,,,,,,,,,,"0|z16lww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 08:55;martijnvisser;[~jingzhang] I've converted this into it's own issue because it has its own FLIP. ;;;","07/Jul/22 13:50;jingzhang;[~martijnvisser] Make sense. Thanks for the updating.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc versions to k8s operator docs,FLINK-28396,13470078,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dmarsal,mbalassi,mbalassi,05/Jul/22 08:29,06/Jul/22 13:49,04/Jun/24 20:42,06/Jul/22 13:49,kubernetes-operator-1.0.0,kubernetes-operator-1.1.0,,,,,,,Documentation,Kubernetes Operator,,,,,,0,pull-request-available,,,,"We should populate this page for both 1.1 and 1.0:

https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/versions/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 13:49:10 UTC 2022,,,,,,,,,,"0|z16lwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 13:49;mbalassi;cb8b83cec7dd1df373b993f6e192fa6568685df0 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Elasticsearch6DynamicSinkITCase.testWritingDocumentsFromTableApi fails with Elasticsearch exception [type=index_not_found_exception, reason=no such index]",FLINK-28395,13470077,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,martijnvisser,martijnvisser,05/Jul/22 08:29,21/Aug/23 22:35,04/Jun/24 20:42,,1.15.3,,,,,,,,Connectors / ElasticSearch,,,,,,,0,auto-deprioritized-critical,auto-deprioritized-major,pull-request-available,test-stability,"{code:java}
Jul 05 02:49:58 Caused by: [table-api] ElasticsearchException[Elasticsearch exception [type=index_not_found_exception, reason=no such index]]
Jul 05 02:49:58 	at org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:509)
Jul 05 02:49:58 	at org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:420)
Jul 05 02:49:58 	at org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:139)
Jul 05 02:49:58 	at org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:199)
Jul 05 02:49:58 	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2061)
Jul 05 02:49:58 	at org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$15(RestHighLevelClient.java:1825)
Jul 05 02:49:58 	at org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1925)
Jul 05 02:49:58 	at org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:849)
Jul 05 02:49:58 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:549)
Jul 05 02:49:58 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:537)
Jul 05 02:49:58 	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122)
Jul 05 02:49:58 	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(Default
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37604&view=logs&j=e9af9cde-9a65-5281-a58e-2c8511d36983&t=c520d2c3-4d17-51f1-813b-4b0b74a0c307&l=13541",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/22 08:01;kurt.ding;image-2022-07-20-16-01-45-775.png;https://issues.apache.org/jira/secure/attachment/13047006/image-2022-07-20-16-01-45-775.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 21 22:35:21 UTC 2023,,,,,,,,,,"0|z16lwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Jul/22 07:38;kurt.ding;I have run several times  test for this case ,but I never get failure . So  I guess this issue can be seldomly repreduced  when flink execute query sql first , after that then execute  create table sql . Thus , elasticsearch high level client got  index not found exception.;;;","20/Jul/22 07:46;martijnvisser;[~kurt.ding] Thanks for that, I'm closing the ticket with that info. We can always re-open it in case it appears again;;;","20/Jul/22 08:01;kurt.ding;!image-2022-07-20-16-01-45-775.png!;;;","15/Aug/22 06:16;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39963&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=2e426bf0-b717-56bb-ab62-d63086457354 new instance;;;","15/Aug/22 06:28;kurt.ding;pr https://github.com/apache/flink/pull/20318;;;","24/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","26/Aug/22 00:15;kurt.ding;Pr is avaliable, need a reviewer;;;","04/Sep/22 02:12;kurt.ding;Pr is avaliable  [https://github.com/apache/flink/pull/20608]  and any response would be great . [~hxbks2ks]  ;;;","26/Sep/22 08:36;martijnvisser;[~kurt.ding] Sorry for the late reply; could you move this PR to https://github.com/apache/flink-connector-elasticsearch ?;;;","09/Nov/22 08:14;mapohl;Was this actually fixed in the end? I couldn't find a PR in the {{flink-connector-elasticsearch}} repo. If we fixed it, fyi: moving it to the external connector repo is not enough. We would need backports 1.16 and 1.15, wouldn't we?;;;","09/Nov/22 08:39;kurt.ding;I am going to work with this issue today later;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python py36-cython: InvocationError for command install_command.sh fails with exit code 1,FLINK-28394,13470075,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,hxbks2ks,martijnvisser,martijnvisser,05/Jul/22 08:25,24/Nov/22 02:15,04/Jun/24 20:42,24/Nov/22 02:15,1.15.3,1.16.0,,,,,,,API / Python,,,,,,,0,stale-assigned,test-stability,,,"{code:java}
Jul 05 03:47:22 Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Jul 05 03:47:32 Using Python version 3.8.13 (default, Mar 28 2022 11:38:47)
Jul 05 03:47:32 pip_test_code.py success!
Jul 05 03:47:32 py38-cython finish: run-test  after 1658.14 seconds
Jul 05 03:47:32 py38-cython start: run-test-post 
Jul 05 03:47:32 py38-cython finish: run-test-post  after 0.00 seconds
Jul 05 03:47:32 ___________________________________ summary ____________________________________
Jul 05 03:47:32 ERROR:   py36-cython: InvocationError for command /__w/3/s/flink-python/dev/install_command.sh --exists-action w .tox/.tmp/package/1/apache-flink-1.15.dev0.zip (exited with code 1)
Jul 05 03:47:32   py37-cython: commands succeeded
Jul 05 03:47:32   py38-cython: commands succeeded
Jul 05 03:47:32 cleanup /__w/3/s/flink-python/.tox/.tmp/package/1/apache-flink-1.15.dev0.zip
Jul 05 03:47:33 ============tox checks... [FAILED]============
Jul 05 03:47:33 Process exited with EXIT CODE: 1.
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37604&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27789",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Nov 24 02:15:20 UTC 2022,,,,,,,,,,"0|z16lw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 08:25;martijnvisser;CC [~dianfu] [~hxbks2ks];;;","05/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","07/Nov/22 08:18;mapohl;I'm reopening this issue because we saw a similar error with Python 3.7 now in [this 1.16 build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42858&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=28369]:
{code:java}
Nov 06 03:05:13 ERROR:   py37-cython: commands failed
Nov 06 03:05:13   py38-cython: commands succeeded
Nov 06 03:05:13   py39-cython: commands succeeded
Nov 06 03:05:13 cleanup /__w/2/s/flink-python/.tox/.tmp/package/1/apache-flink-1.16.dev0.zip
Nov 06 03:05:14 ============tox checks... [FAILED]============
Nov 06 03:05:14 Process exited with EXIT CODE: 1.
[...]
The STDIO streams did not close within 10 seconds of the exit event from process '/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.{code};;;","09/Nov/22 08:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42956&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=29157;;;","09/Nov/22 08:09;mapohl;[~hxbks2ks] can you give some guidance here on what the actual issue is? I'm not happy with what we extracted from the failed builds so far to give this Jira issue a meaningful purpose. The only thing I found is the following stacktrace (but that might be unrelated):
{code}
02:56:45,358 [Source: PythonInputFormatTableSource(a, b, c) -> SourceConversion[610] -> PythonCorrelate[611] -> Calc[612] -> SinkConversion[613] -> Map -> Sink: Unnamed (2/2)#0] WARN  org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory [] - Error cleaning up server>
payload: ""\0327/__w/1/s/flink-python/pyflink/bin/pyflink-udf-runner.sh\""\312\002\n\004PATH\022\301\002/__w/1/s/flink-python/.tox/py38-cython/bin:/__w/1/s/flink-python/dev/.conda/envs/3.8/bin:/__w/1/s/flink-python/dev/.conda/envs/3.7/bin:/__w/1/s/flink-python/dev/.conda/envs/3.6/bi>

java.lang.IllegalStateException: call already closed
        at org.apache.beam.vendor.grpc.v1p26p0.com.google.common.base.Preconditions.checkState(Preconditions.java:511) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl.closeInternal(ServerCallImpl.java:209) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl.close(ServerCallImpl.java:202) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onCompleted(ServerCalls.java:371) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.runners.fnexecution.state.GrpcStateService.close(GrpcStateService.java:64) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.runners.fnexecution.GrpcFnServer.close(GrpcFnServer.java:156) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$WrappedSdkHarnessClient.$closeResource(DefaultJobBundleFactory.java:642) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$WrappedSdkHarnessClient.close(DefaultJobBundleFactory.java:642) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$WrappedSdkHarnessClient.unref(DefaultJobBundleFactory.java:658) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$WrappedSdkHarnessClient.access$400(DefaultJobBundleFactory.java:589) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.lambda$createEnvironmentCaches$3(DefaultJobBundleFactory.java:212) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.processPendingNotifications(LocalCache.java:1809) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.runUnlockedCleanup(LocalCache.java:3462) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.postWriteCleanup(LocalCache.java:3438) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$Segment.clear(LocalCache.java:3215) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache.clear(LocalCache.java:4270) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LocalCache$LocalManualCache.invalidateAll(LocalCache.java:4909) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory.close(DefaultJobBundleFactory.java:319) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.runners.python.beam.PythonSharedResources.close(PythonSharedResources.java:67) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.memory.SharedResources$LeasedResource.dispose(SharedResources.java:180) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.memory.SharedResources.release(SharedResources.java:108) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.memory.MemoryManager.lambda$getSharedMemoryResourceForManagedMemory$7(MemoryManager.java:565) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.memory.OpaqueMemoryResource.close(OpaqueMemoryResource.java:65) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.close(BeamPythonFunctionRunner.java:284) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.python.AbstractExternalPythonFunctionOperator.close(AbstractExternalPythonFunctionOperator.java:64) ~[flink-python_2.12-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1000) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:879) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:762) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code};;;","09/Nov/22 08:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42957&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=28792;;;","14/Nov/22 03:37;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43087&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901;;;","14/Nov/22 04:02;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43104&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2;;;","15/Nov/22 06:19;hxbks2ks;`Python py36-cython` is not the cause of the actual error. Any test running under Python 3.6 fails, and then `Python py36-cython` will be displayed at the end. You need to scroll up in the log to find out the actual failed test. I looked at the links posted and they are all about https://issues.apache.org/jira/browse/FLINK-29461. ;;;","24/Nov/22 02:15;mapohl;[~hxbks2ks] is right. I verified the reported build failures most of them are related to FLINK-29461. Only that build failure of that comment refers to another test failure ({{{}StreamPandasUDFITTests.test_basic_functionality{}}}). I created FLINK-30163 to cover it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support AvroInputFormat in PyFlink,FLINK-28393,13470074,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,05/Jul/22 08:22,07/Jul/22 04:44,04/Jun/24 20:42,07/Jul/22 04:44,,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 04:44:32 UTC 2022,,,,,,,,,,"0|z16lvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 04:44;dianfu;Merged to master via bbd5a813e9c6a17b0c3bb31ee27d92d00448c956;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoveCachedShuffleDescriptorTest#testRemoveOffloadedCacheForPointwiseEdgeAfterFailover causes fatal error on CI,FLINK-28392,13470071,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zhuzh,martijnvisser,martijnvisser,05/Jul/22 08:16,12/Jul/22 08:07,04/Jun/24 20:42,06/Jul/22 13:47,1.16.0,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"{code:java}
Jul 05 03:30:03 [ERROR] Error occurred in starting fork, check output in log
Jul 05 03:30:03 [ERROR] Process Exit Code: 239
Jul 05 03:30:03 [ERROR] Crashed tests:
Jul 05 03:30:03 [ERROR] org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTest
Jul 05 03:30:03 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Jul 05 03:30:03 [ERROR] Command was /bin/sh -c cd /__w/1/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/1/s/flink-runtime/target/surefire/surefirebooter4932865857415988980.jar /__w/1/s/flink-runtime/target/surefire 2022-07-05T03-23-25_404-jvmRun1 surefire8916732512419442726tmp surefire_2130262314165063415tmp
Jul 05 03:30:03 [ERROR] Error occurred in starting fork, check output in log
Jul 05 03:30:03 [ERROR] Process Exit Code: 239
Jul 05 03:30:03 [ERROR] Crashed tests:
Jul 05 03:30:03 [ERROR] org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategyTest
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:405)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:321)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Jul 05 03:30:03 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8147",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28423,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 13:47:23 UTC 2022,,,,,,,,,,"0|z16lv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 09:58;chesnay;{code}
03:25:58,660 [   pool-212-thread-1] INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 1 tasks should be restarted to recover the failed task 00c6884391ca3083329c741d7b7c6bd6_0. 
03:25:58,673 [   pool-212-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'pool-212-thread-1' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.NoSuchElementException: No value present
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:848) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2168) ~[?:1.8.0_292]
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$deployAll$4(DefaultExecutionDeployer.java:193) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:848) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2168) ~[?:1.8.0_292]
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.waitForAllSlotsAndDeploy(DefaultExecutionDeployer.java:156) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.allocateSlotsAndDeploy(DefaultExecutionDeployer.java:108) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.allocateSlotsAndDeploy(DefaultScheduler.java:423) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.maybeScheduleRegion(PipelinedRegionSchedulingStrategy.java:227) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.maybeScheduleRegions(PipelinedRegionSchedulingStrategy.java:212) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy.restartTasks(PipelinedRegionSchedulingStrategy.java:166) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.restartTasks(DefaultScheduler.java:378) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$null$3(DefaultScheduler.java:342) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:719) [?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$UniRun.tryFire(CompletableFuture.java:701) [?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) [?:1.8.0_292]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.util.NoSuchElementException: No value present
	at java.util.Optional.get(Optional.java:135) ~[?:1.8.0_292]
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.getExecutionOrThrow(DefaultExecutionDeployer.java:343) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.handleTaskDeploymentFailure(DefaultExecutionDeployer.java:338) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.deployTaskSafe(DefaultExecutionDeployer.java:331) ~[classes/:?]
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$deployOrHandleError$7(DefaultExecutionDeployer.java:319) ~[classes/:?]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
	... 24 more
{code};;;","05/Jul/22 10:08;chesnay;-My guess is that this fails because the PartitionNotFoundException that we generate contains a random execution attempt id, which naturally will never be found in the execution graph when the restart strategy tries to rebuild that partition.-;;;","05/Jul/22 10:41;chesnay;Seems to only fail when the executor service is shut down while the deployment is still happening in the background. The re-deployment of the failed tasks then fails again because the executor rejects the request, and then we run into the fatal error for some reason.;;;","05/Jul/22 11:18;chesnay;I was able to somewhat reliably reproduce the issue locally.

If the scheduled executor service that backs the main thread is shut down before the deployment has started (Execution#deploy), then the deployment fails, causing the execution to transition to FAILED and it being deregistered from the set of active executions, which are the only ones visible to the restart strategy / ExecutionDeployer.

Putting aside that the test shouldn't just close the executor, it could be that the PipelinedRestartStrategy just doesn't work if a task fails on the JM side.

[~zhuzh] any thoughts?

 ;;;","06/Jul/22 07:15;zhuzh;Thanks for reporting this issue and investigating it. [~martijnvisser] [~chesnay]
This issue happens because the future executor was shutdown when the test is done, while the scheduler was still working and trying to deploy tasks. There were 2 exceptions caused by schedule tasks via the failed executor, 
 - CompletableFuture.supplyAsync(...) in Execution#deploy()
 - delayExecutor.schedule(...) in DefaultScheduler#restartTasksWithDelay(...)

They together make the error to be a fatal one, because an error was thrown in Execution#markFailed() and lead to fail an Execution twice. The ExecutionDeployer encounters this problem because it retrieves execution from ExecutionGraph#currentExecutions but the execution was unregistered during the first round failing the Execution. Previously this problem did not happen because it failed the Execution via ExecutionVertex, so it will not affected by the execution unregistering.

Theoretically, we should fix all the tests which shuts down the executor before terminating the job/scheduler. But I'm afraid there are many such kind of tests and we may easily miss to fix some of them. Therefore, I'm thinking to changing the ExecutionDeployer to not retrieve executions via ExecutionGraph#currentExecutions, to tolerate this case as before, and to also avoid similar problems that may happen in production.;;;","06/Jul/22 07:20;zhuzh;#20178 is created to fix this problem as proposed above.;;;","06/Jul/22 13:47;zhuzh;Fixed via e5c4e3f519f364b5951e7cac331eb8af48f0ed84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultBlocklistHandlerTest.testRemoveTimeoutNodes fails,FLINK-28391,13470067,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wanglijie,Paul Lin,Paul Lin,05/Jul/22 07:44,06/Jul/22 12:31,04/Jun/24 20:42,06/Jul/22 12:31,1.16.0,,,,,1.16.0,,,Tests,,,,,,,0,pull-request-available,test-stability,,,"Test org.apache.flink.runtime.blocklist.DefaultBlocklistHandlerTest.testRemoveTimeoutNodes unstable.

see https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/37624/logs/111

{code:java}
Jul 05 01:23:40 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.572 s - in org.apache.flink.runtime.shuffle.ShuffleMasterTest
Jul 05 01:23:41 [INFO] 
Jul 05 01:23:41 [INFO] Results:
Jul 05 01:23:41 [INFO] 
Jul 05 01:23:41 [ERROR] Failures: 
Jul 05 01:23:41 [ERROR]   DefaultBlocklistHandlerTest.testRemoveTimeoutNodes:93 
Jul 05 01:23:41 Expecting actual:
Jul 05 01:23:41   [BlockedNode{id:node1,cause:cause,endTimestamp:1656984203692}]
Jul 05 01:23:41 to contain exactly (and in same order):
Jul 05 01:23:41   [BlockedNode{id:node1,cause:cause,endTimestamp:1656984203692}]
Jul 05 01:23:41 but could not find the following elements:
Jul 05 01:23:41   [BlockedNode{id:node1,cause:cause,endTimestamp:1656984203692}]
Jul 05 01:23:41 
Jul 05 01:23:41 [INFO] 
Jul 05 01:23:41 [ERROR] Tests run: 6453, Failures: 1, Errors: 0, Skipped: 26
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 12:31:34 UTC 2022,,,,,,,,,,"0|z16lu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 08:08;wanglijie;I will take a look.;;;","05/Jul/22 08:17;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=8344;;;","06/Jul/22 08:25;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37722&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8 ;;;","06/Jul/22 12:31;zhuzh;Fixed via 2bd8c209df72679684aeb493c6df03a167fbd2f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allows RocksDB to configure FIFO Compaction to reduce CPU overhead.,FLINK-28390,13470065,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Ming Li,Ming Li,05/Jul/22 07:29,31/Oct/22 03:52,04/Jun/24 20:42,,,,,,,,,,Runtime / State Backends,,,,,,,0,,,,,"We know that the fifo compaction strategy may silently delete data and may lose data for the business. But in some scenarios, FIFO compaction can be a very effective way to reduce CPU usage.

 

Flink's Taskmanager is usually some small-scale processes, such as allocating 4 CPUs and 16G memory. When the state size is small, the CPU overhead occupied by RocksDB is not high, and as the state increases, RocksDB may frequently be in the compaction operation, which will occupy a large amount of CPU and affect the computing operation.

 

We usually configure a TTL for the state, so when using FIFO we can configure it to be slightly longer than the TTL, so that the upper layer is the same as before. 

 

Although the FIFO Compaction strategy may bring space amplification, the disk is cheaper than the CPU after all, so the overall cost is reduced.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 31 03:52:51 UTC 2022,,,,,,,,,,"0|z16lts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 02:53;masteryhx;Hi, [~Ming Li] .
I have also seen some user question like : [https://lists.apache.org/thread/n8q75r226byj7790m08nlkdpfj1mfsjo]

Currently IIUC, Flink should have supported to configure FIFO compaction and it should work.

So you mean we need some improvements for this feature ?;;;","11/Jul/22 03:03;Zhanghao Chen;Option ""state.backend.rocksdb.compaction.style"" already supports compaction style FIFO;;;","11/Jul/22 09:18;Ming Li;Hi, [~masteryhx], [~Zhanghao Chen]

Yes, although we currently have the compaction configuration of FIFO, it is actually unusable (the TTL and MAX_SIZE of FIFO cannot be configured). In addition, we do not recommend users to use it, and there is potential data loss. So I think we have the following work to do:
1.  Add FIFO related JNI, we can refer to https://github.com/facebook/rocksdb/wiki/FIFO-compaction-style;
2. Add the documentation and precautions for using FIFO.

In addition, when we used the FIFO of RocksDB internally, we also found a potential bug, which also needs to be fixed on the RocksDB branch of Flink. We can refer to https://github.com/facebook/rocksdb/issues/10133;;;","11/Jul/22 10:08;masteryhx;Okay, There are some users who don't care the data loss using it. Supporting TTL and MAX_SIZE can make users balance between performance and data loss. It makes sense to me.
[~yunta] WDYT ?;;;","11/Jul/22 11:17;martijnvisser;I do think that we need to be very, very careful with ""who don't care for data loss"". Perhaps it's the choice of words, but I don't think ""data loss"" is ever acceptable. It implies that some data got lost without that being the intention of the user.;;;","11/Jul/22 11:39;Ming Li;[~martijnvisser] we generally set the TTL for the state, assuming that the MAX_SIZE of the FIFO is configured to be infinite, and the TTL of the file is set to the TTL of the state, which may not cause data loss. At the same time, each KV does not need to set the TTL field, thereby reducing the state size and serialization overhead.


I think FIFO may not be suitable for scenarios where TTL is not set for the state. If all states are configured with TTL, then FIFO looks like a good choice.;;;","11/Jul/22 12:00;yunta;[~Ming Li] I think you can make your changes merged in the RocksDB community source code, and we can later leverage or cherry-pick to our forked version.

And I am against adding such documentation in Flink as this might lead users to make mistakes.;;;","11/Jul/22 12:01;Zhanghao Chen;""assuming that the MAX_SIZE of the FIFO is configured to be infinite, and the TTL of the file is set to the TTL of the state"". Sounds like a good idea (y);;;","11/Jul/22 14:23;Ming Li;[~yunta] the {{RocksDB}} version used by Flink already contains the required JNI, we need to add related configuration to {{{}RocksDBConfigurableOptions{}}}. But we'd better wait for the {{RocksDB}} community to fix the above issue before doing this upgrade.;;;","18/Jul/22 03:06;yunta;[~Ming Li] I think you can create the issue or even the PR directly in the RocksDB community and share the related link here.;;;","22/Jul/22 05:05;Ming Li;[~yunta] sorry, maybe I didn't describe clearly.

In Flink we only need to provide FIFO related configuration (like {{{}state.backend.rocksdb.fifo.max-size{}}}).

I have already reported the bug of FIFO TTL in RocksDB not taking effect to the RockDB community (https://github.com/facebook/rocksdb/issues/10133).;;;","22/Jul/22 13:11;yunta;[~Ming Li] I don't think we should add such configurations in Flink to avoid user loss data unexpectedly, you can still set this option via RocksDBOptionsFactory.;;;","23/Jul/22 17:35;martijnvisser;+1 to [~yunta] his comment;;;","25/Jul/22 03:21;Ming Li;[~yunta] ok thanks, I got it.

This commit has fixed the problem that the FIFO TTL does not take effect. Is it possible to cherry-pick to the fork version of flink?  https://github.com/facebook/rocksdb/pull/10386;;;","31/Oct/22 03:52;Yanfei Lei;[~Ming Li] As the bug has been fixed and the changes are minor, I don't think it will conflict with the [fork version of flink.|https://github.com/ververica/frocksdb] I'd like to cherry-pick this to frocksdb.

And I'm +1 to [~yunta]'s comments. Rocksdb Configuration documentation has already been complicated, introducing new TTL setting maybe increase the burden on users to use TTL.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct spec and status updates in FlinkDeploymentControllerTest,FLINK-28389,13470061,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,05/Jul/22 07:14,24/Nov/22 01:02,04/Jun/24 20:42,05/Jul/22 18:48,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The testing 

`FlinkDeploymentController` we use in the FlinkDeploymentControllerTest mutates the FlinkDeployment object. This behaviour is different how it works in real environments causing inconsistent behaviour in some tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 18:48:49 UTC 2022,,,,,,,,,,"0|z16lsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 18:48;gyfora;merged to main d75e45e45600f3c550b671d4883fb5ed9dcd96dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python doc build breaking nightly docs,FLINK-28388,13470060,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dianfu,mbalassi,mbalassi,05/Jul/22 07:10,12/Aug/22 12:29,04/Jun/24 20:42,08/Jul/22 02:35,1.16.0,,,,,1.16.0,,,API / Python,Documentation,,,,,,0,pull-request-available,,,,"For the past 5 days the nightly doc builds via GHA are broken:

https://github.com/apache/flink/actions/workflows/docs.yml

{noformat}
Exception occurred:
  File ""/root/flink/flink-python/pyflink/java_gateway.py"", line 86, in launch_gateway
    raise Exception(""It's launching the PythonGatewayServer during Python UDF execution ""
Exception: It's launching the PythonGatewayServer during Python UDF execution which is unexpected. It usually happens when the job codes are in the top level of the Python script file and are not enclosed in a `if name == 'main'` statement.
The full traceback has been saved in /tmp/sphinx-err-3thh_wi2.log, if you want to report the issue to the developers.
Please also report this if it was a user error, so that a better error message can be provided next time.
A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!
Makefile:76: recipe for target 'html' failed
make: *** [html] Error 2
==========sphinx checks... [FAILED]===========
Error: Process completed with exit code 1.
{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27586,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 13:16:55 UTC 2022,,,,,,,,,,"0|z16lso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 07:12;mbalassi;cc [~dianfu] [~ana4];;;","05/Jul/22 13:01;dianfu;[~mbalassi]  I'm looking into this. Have not reproduced it in my local environment. Besides, it seems that the tests also runs normally in Azure. I will try to reproduce it with the same environment & command as the GHA.;;;","05/Jul/22 13:34;mbalassi;Thanks for the update [~dianfu] . You can also run it on your own fork of Flink on Github if you comment out this line (for your fork):

[https://github.com/apache/flink/blob/master/.github/workflows/docs.yml#L24]

Probably that is the simplest way to repro.;;;","07/Jul/22 01:02;dianfu;[~mbalassi] Thanks for suggestions. Will try it.;;;","07/Jul/22 11:35;dianfu;Update: Have figured it out that it should be caused by this commit: [https://github.com/apache/flink/commit/dfdf7afb047d1b8af581883bf208e4d8a30a116f] where we bumped the versions of some dependencies. ;;;","08/Jul/22 02:35;dianfu;Fixed in master via 974fea145cc6189eae3410203339aa2950dbbd4d;;;","08/Jul/22 13:16;mbalassi;Thanks [~dianfu]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Organizing Options in table store,FLINK-28387,13470050,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Jul/22 06:25,05/Jul/22 06:43,04/Jun/24 20:42,05/Jul/22 06:43,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Introduce 4 options:
- CoreOptions: Core options for table store.
- CatalogOptions: Options for table store catalog.
- FlinkConnectorOptions: Flink connector options for table store.
- KafkaLogOptions: Kafka log options provided after configuring log.system for kafka.

Some adjustments to KafkaLogOptions:
- log.kafka.bootstrap.servers => kafka.bootstrap.servers
- log.topic => kafka.topic",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 06:43:54 UTC 2022,,,,,,,,,,"0|z16lqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 06:43;lzljs3620320;master: c233679e5e0a3dea7d98c4d3487466d1139da6b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trigger an immediate checkpoint after all tasks has reached end-of-data,FLINK-28386,13470049,13428259,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,gaoyunhaii,gaoyunhaii,05/Jul/22 06:25,23/Aug/23 09:41,04/Jun/24 20:42,04/Jul/23 14:16,,,,,,1.18.0,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,"Currently for bounded job in streaming mode, by default it will wait for one more checkpoint to commit the last piece of data. If the checkpoint period is long, the waiting time might also be long. to optimize this situation, we could eagerly trigger a checkpoint after tasks have reached end-of-data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31846,,,,,,,FLINK-32663,FLINK-32911,FLINK-32945,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 04 14:16:22 UTC 2023,,,,,,,,,,"0|z16lq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 03:51;zlzhang0122;For bounded job in this scene, IMO, maybe savepoint is better?Why should we use checkpoint instead of savepoint?;;;","21/Apr/23 11:38;pnowojski;[~zlzhang0122] Checkpoint could be used just to make the side effects visible (committing results in two phase commit operators/sinks). On the other hand, why savepoint makes any sense? There is no point in recovering  from such snapshot anyway.

About the ticket. Taking into account unaligned checkpoints, I think a better condition would be to trigger a checkpoint once all tasks are finished. With unaligned checkpoints, downstream tasks can be still processing in-flight data, while upstream sources are finished, so triggering checkpoint on finished sources wouldn't achieve the desired goal of stopping the job faster.;;;","19/May/23 03:37;Jiang Xin;Hi [~pnowojski], I'm going to work on this. I agree with you that only checking sources finished is not enough, but would it be simpler to just check all sink operators are finished?;;;","22/May/23 08:06;pnowojski;Hi [~Jiang Xin], thanks! I guess it depends what you mean by ""sin operators"". Just checking tail/the most downstream operators - yes. Literally {{SinkOperator}} probably not, as that could be a fragile check.;;;","04/Jul/23 14:16;lindong;Merged to the apache/flink master branch c6f443c92880f9c4afc8b30c886e2fba523c564a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the validator to return an error if the Jar URI is an empty string,FLINK-28385,13470044,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,haoxin,haoxin,haoxin,05/Jul/22 05:47,05/Jul/22 15:01,04/Jun/24 20:42,05/Jul/22 15:01,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/292,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 15:01:16 UTC 2022,,,,,,,,,,"0|z16lp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 15:01;mbalassi;[e88424e|https://github.com/apache/flink-kubernetes-operator/commit/e88424e57073905fadf2f0687aaa0441ee3ead5c] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add CatalogLock support for HiveCatalog in table store,FLINK-28384,13470016,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Jul/22 02:03,07/Jul/22 07:56,04/Jun/24 20:42,07/Jul/22 07:56,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"In HiveCatalog, we can set CatalogLock.Factory to TableStoreSink to support multiple concurrency writers for object storage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 07:56:11 UTC 2022,,,,,,,,,,"0|z16liw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 07:56;lzljs3620320;master: 7c6ddd6ec19a1a232450a2f604fd6df8d1144f82;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDFS source: Trying to access closed classloader,FLINK-28383,13469972,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tashoyan,tashoyan,04/Jul/22 14:25,17/Nov/23 13:17,04/Jun/24 20:42,,1.15.0,,,,,,,,Connectors / FileSystem,,,,,,,0,,,,,"This problem occurs when reading Avro files from HDFS. Flink is not able to restart a failed task because of the exception mentioned below. As a result, the entire application fails.

{code:none}
Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:172) ~[flink-dist-1.15.0.jar:1.15.0]
	at java.lang.Class.forName0(Native Method) ~[?:1.8.0_252]
	at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_252]
	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2247) ~[hadoop-common-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2212) ~[hadoop-common-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.crypto.CryptoCodec.getCodecClasses(CryptoCodec.java:125) ~[hadoop-common-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.crypto.CryptoCodec.getInstance(CryptoCodec.java:60) ~[hadoop-common-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.createStreamPair(DataTransferSaslUtil.java:339) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:511) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:304) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:245) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:215) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.net.TcpPeerServer.peerFromSocketAndKey(TcpPeerServer.java:93) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3568) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:779) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:696) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:359) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:669) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:888) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:945) ~[hadoop-hdfs-2.7.3.2.6.5.0-292.jar:?]
	at java.io.DataInputStream.read(DataInputStream.java:149) ~[?:1.8.0_252]
	at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:96) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.formats.avro.utils.FSDataInputStreamWrapper.read(FSDataInputStreamWrapper.java:50) ~[flink-avro-1.15.0.jar:1.15.0]
	at org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:65) ~[avro-1.10.2.jar:1.10.2]
	at org.apache.flink.formats.avro.AvroInputFormat.initReader(AvroInputFormat.java:135) ~[flink-avro-1.15.0.jar:1.15.0]
	at org.apache.flink.formats.avro.AvroInputFormat.open(AvroInputFormat.java:109) ~[flink-avro-1.15.0.jar:1.15.0]
{code}

The Hadoop class org.apache.hadoop.conf.Configuration has a member classLoader:

{code:java}
private ClassLoader classLoader;
  {
    classLoader = Thread.currentThread().getContextClassLoader();
    if (classLoader == null) {
      classLoader = Configuration.class.getClassLoader();
    }
  }
{code}

It seems that threads are reused despite the classloader is already closed.

",Flink 1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19916,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 09 06:07:13 UTC 2023,,,,,,,,,,"0|z16l94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 06:07;gabry.wu;any updates here?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce new compression algorithms of higher compression ratio,FLINK-28382,13469960,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,kevin.cyj,kevin.cyj,04/Jul/22 12:50,02/Aug/22 06:51,04/Jun/24 20:42,02/Aug/22 06:51,,,,,,1.16.0,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"Currently, we use lz4 for shuffle data compression which is a good balance between IO optimization and CPU consumption. But for some scenarios, the IO becomes bottleneck and the storage space is limited (especially for k8s environment). For these cases, we need compression algorithms of higher compression ratio to further reduce IO.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 02 06:51:19 UTC 2022,,,,,,,,,,"0|z16l6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 14:36;Weijie Guo;[~kevin.cyj] Maybe i can help introduce some compression algorithms such as z-std, and the default lz4 compressor should support configurable compression levels.;;;","08/Jul/22 07:18;kevin.cyj;[~Weijie Guo] Thanks for your interests. I have assigned this Jira to you.;;;","02/Aug/22 06:51;kevin.cyj;Merged into master via 47d0b6d26c052a817a66f7b719eecf01387cb0d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Spark Reader documentation,FLINK-28381,13469958,13449930,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Jul/22 12:47,05/Jul/22 07:19,04/Jun/24 20:42,05/Jul/22 07:19,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 07:19:15 UTC 2022,,,,,,,,,,"0|z16l60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 07:19;lzljs3620320;master: e2612c0cfb8e14e5d1c33df280e827f3f67c0426;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Produce one intermediate dataset for multiple consumers consuming the same data,FLINK-28380,13469957,13469906,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,04/Jul/22 12:42,09/Aug/22 01:59,04/Jun/24 20:42,09/Aug/22 01:59,,,,,,1.16.0,,,Client / Job Submission,Runtime / Coordination,Runtime / Network,,,,,0,pull-request-available,,,,"Currently, if one output of an upstream job vertex is consumed by multiple downstream job vertices, the upstream vertex will produce multiple dataset. For blocking shuffle, it means serialize and persist the same data multiple times. This ticket aims to optimize this behavior and make the upstream job vertex produce one dataset which will be read by multiple downstream vertex.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 09 01:59:08 UTC 2022,,,,,,,,,,"0|z16l5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 01:59;kevin.cyj;Merged into master via 60bc87c0b83149c4f19d7e54af2d967087a277fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KerberosLoginProviderTest static UGI mock leaks into other tests,FLINK-28379,13469946,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,martijnvisser,martijnvisser,04/Jul/22 10:59,01/Sep/22 11:18,04/Jun/24 20:42,11/Jul/22 14:11,1.16.0,,,,,1.16.0,,,Runtime / Network,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
Jul 04 03:46:37 [WARNING] Tests run: 18, Failures: 0, Errors: 0, Skipped: 3, Time elapsed: 14.738 s - in org.apache.flink.runtime.blob.BlobClientSslTest
Jul 04 04:12:06 ==============================================================================
Jul 04 04:12:06 Process produced no output for 900 seconds.
Jul 04 04:12:06 ==============================================================================
Jul 04 04:12:06 ==============================================================================
Jul 04 04:12:06 The following Java processes are running (JPS)
Jul 04 04:12:06 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Jul 04 04:12:06 531 surefirebooter600226482914290216.jar
Jul 04 04:12:06 467 Launcher
Jul 04 04:12:06 17752 Jps
Jul 04 04:12:06 ==============================================================================
Jul 04 04:12:06 Printing stack trace of Java process 531
Jul 04 04:12:06 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37535&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8298",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28320,FLINK-27745,,,,,,,,,,,,FLINK-27791,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 14:11:21 UTC 2022,,,,,,,,,,"0|z16l3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 11:20;chesnay;The ShuffleMasterTest gets stuck.

{code}
2022-07-04T04:12:07.3957972Z Jul 04 04:12:06 	at org.apache.flink.util.AutoCloseableAsync.close(AutoCloseableAsync.java:36)
2022-07-04T04:12:07.3958770Z Jul 04 04:12:06 	at org.apache.flink.runtime.shuffle.ShuffleMasterTest.testShuffleMasterLifeCycle(ShuffleMasterTest.java:72)
{code};;;","04/Jul/22 11:26;chesnay;Cluster is crashing; looks similar to FLINK-27745.

{code}
Could not initialize class org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration
{code};;;","04/Jul/22 11:30;chesnay;Probably caused by the KerberosLoginProviderTest, similar to FLINK-27791.;;;","05/Jul/22 08:40;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37387&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10108;;;","11/Jul/22 14:11;chesnay;master: 1ef0e7c34f594d9fa34f34477dfdb691d2836841;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use larger data reading buffer size for sort-shuffle,FLINK-28378,13469936,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,kevin.cyj,kevin.cyj,04/Jul/22 09:24,09/Feb/23 09:51,04/Jun/24 20:42,09/Feb/23 09:51,,,,,,,,,Runtime / Network,,,,,,,0,,,,,"Currently, for sort shuffle, we always use the network buffer size as the data reading buffer size which is 32K by default. We can increase this buffer size for better performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-04 09:24:04.0,,,,,,,,,,"0|z16l14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decrease the memory size per request for sort-shuffle data read from 8M to 4M,FLINK-28377,13469927,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,04/Jul/22 08:54,22/Jul/22 03:35,04/Jun/24 20:42,22/Jul/22 03:35,,,,,,1.16.0,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"Currently, for sort blocking shuffle, the corresponding data reader always allocate a fixed size of 8M buffers for shuffle data reading. FLINK-28373 can increase buffer utilization, after which, we can reduce the buffer size per request to reduce the time waiting for buffers. (This change is guarded by TPC-DS test that there is no performance regression)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 22 03:35:08 UTC 2022,,,,,,,,,,"0|z16kz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 03:35;kevin.cyj;Merged into master via cfc5d724217cda4140d7efda552e83b8ba36ffb6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restrict the number of threads for sort-shuffle data read,FLINK-28376,13469914,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,04/Jul/22 08:31,21/Jul/22 03:26,04/Jun/24 20:42,21/Jul/22 03:26,,,,,,1.16.0,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"Currently, the number of IO threads for shuffle data reading is relevant to the size of reading memory and the number of CPU cores. We should also consider the number of slots and the number of disks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 21 03:26:21 UTC 2022,,,,,,,,,,"0|z16kw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 03:26;kevin.cyj;Merged into master via 2f76a30569332728def4df0c316ee1a4f3409aa9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Whether to consider adding other data type to support for last_value function,FLINK-28375,13469910,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hehuiyuan,hehuiyuan,04/Jul/22 08:21,17/Feb/23 03:09,04/Jun/24 20:42,,,,,,,,,,Table SQL / API,,,,,,,0,,,,," 
{code:java}
CREATE TABLE kafkaTableSource (
 keyField INTEGER,
 timestampField INTEGER,
 arrayField ARRAY<String>,
 ws as TO_TIMESTAMP(FROM_UNIXTIME(timestampField)),
 WATERMARK FOR ws AS ws
) WITH (
    'connector' = 'kafka',
    'topic' = 'hehuiyuan1',
    'scan.startup.mode' = 'latest-offset',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.client.id' = 'test-consumer-group',
    'properties.group.id' = 'test-consumer-group',
    'format' = 'json'
);

CREATE TABLE kafkaTableSink(
    keyField INTEGER,
    timestampField INTEGER,
    arrayLastValue ARRAY<String>
)
WITH (
    'connector' = 'print'
);

insert into kafkaTableSink
select keyField,timestampField, last_value(arrayField) over (partition by keyField order by ws) from kafkaTableSource;
{code}
{color:#ff0000}Exception in thread ""main"" org.apache.flink.table.api.TableException: LAST_VALUE aggregate function does not support type: ''ARRAY''.{color}
{color:#ff0000}Please re-check the data type.{color}

 

I have a  modification to support this, but why does the community not support it? 

Is there any special reason that i do not considered?

 

The test the array data type can run:

mock data:

!image-2022-07-04-16-21-28-198.png!

result is right:

!image-2022-07-04-16-20-08-661.png|width=626,height=146!

 

 

non over window last value funciton test:
{code:java}
CREATE TABLE kafkaTableSource (
 keyField INTEGER,
 timestampField INTEGER,
 arrayField ARRAY<String>,
 ws as TO_TIMESTAMP(FROM_UNIXTIME(timestampField)),
 WATERMARK FOR ws AS ws - INTERVAL '2' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'hehuiyuan1',
    'scan.startup.mode' = 'latest-offset',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.client.id' = 'test-consumer-group',
    'properties.group.id' = 'test-consumer-group',
    'format' = 'json'
);

CREATE TABLE kafkaTableSink(
    keyField INTEGER,
    arrayLastValue ARRAY<String>
)
WITH (
    'connector' = 'print'
);

insert into kafkaTableSink
select keyField, last_value(arrayField) from kafkaTableSource GROUP BY TUMBLE(ws,INTERVAL '5' SECOND),keyField;
 {code}
The mock data:

!image-2022-07-04-16-42-31-399.png|width=705,height=183!

The result is right

!image-2022-07-04-16-44-02-720.png|width=687,height=155!

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/22 08:20;hehuiyuan;image-2022-07-04-16-20-08-661.png;https://issues.apache.org/jira/secure/attachment/13046206/image-2022-07-04-16-20-08-661.png","04/Jul/22 08:21;hehuiyuan;image-2022-07-04-16-21-28-198.png;https://issues.apache.org/jira/secure/attachment/13046205/image-2022-07-04-16-21-28-198.png","04/Jul/22 08:42;hehuiyuan;image-2022-07-04-16-42-31-399.png;https://issues.apache.org/jira/secure/attachment/13046207/image-2022-07-04-16-42-31-399.png","04/Jul/22 08:44;hehuiyuan;image-2022-07-04-16-44-02-720.png;https://issues.apache.org/jira/secure/attachment/13046208/image-2022-07-04-16-44-02-720.png","05/Jul/22 08:56;hehuiyuan;image-2022-07-05-16-56-36-721.png;https://issues.apache.org/jira/secure/attachment/13046275/image-2022-07-05-16-56-36-721.png",,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Feb 17 03:09:23 UTC 2023,,,,,,,,,,"0|z16kvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 11:21;martijnvisser;Looking at other SQL implementations like Postgres or MySQL, they all expect a scalar and not a compound data type. If you're interested in the entire array, why not cast it to a string? ;;;","04/Jul/22 12:37;hehuiyuan;Hi [~martijnvisser]  , we have some business  which needs  to use array data type.

That's why we have this problem;;;","04/Jul/22 13:52;martijnvisser;[~hehuiyuan] There is support in the Flink Type System for arrays https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/types/ but it's just the return type of this function is expecting to be a scalar (like a string). I don't see that we'll add support for this in Flink. ;;;","05/Jul/22 08:43;hehuiyuan;[~martijnvisser]  ok , thanks.

 

Are Timestamp and Char type  not support ?

!image-2022-07-05-16-56-36-721.png|width=539,height=312!;;;","05/Jul/22 10:55;martijnvisser;[~hehuiyuan] As you can see in the link I've provided earlier, both CHAR and TIMESTAMP are supported data types in Flink's type system. ;;;","17/Feb/23 03:09;hehuiyuan;Hi [~martijnvisser]  , both CHAR and TIMESTAMP are supported data types in Flink's type system.

As shown above, the last_value function not support .

Does this need support?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some further improvements of blocking shuffle,FLINK-28374,13469906,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,kevin.cyj,kevin.cyj,04/Jul/22 08:16,14/Oct/22 10:09,04/Jun/24 20:42,09/Aug/22 06:51,,,,,,1.16.0,,,Runtime / Network,,,,,,,0,,,,,This is an umbrella issue for sort-shuffle Improvements.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29641,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-04 08:16:28.0,,,,,,,,,,"0|z16kug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read a full buffer of data per file IO read request for sort-shuffle,FLINK-28373,13469905,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,04/Jul/22 08:13,12/Aug/22 04:31,04/Jun/24 20:42,09/Aug/22 06:50,,,,,,1.16.0,,,Runtime / Network,,,,,,,0,pull-request-available,,,,"Currently, for sort blocking shuffle, the corresponding data readers read shuffle data in buffer granularity. Before compression, each buffer is 32K by default, after compression the size will become smaller (may less than 10K). For file IO, this is pretty smaller. To achieve better performance and reduce IOPS, we can read more data per IO read request and parse buffer header and data in memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28789,FLINK-28942,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 09 06:50:48 UTC 2022,,,,,,,,,,"0|z16ku8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 06:50;kevin.cyj;Merged into master via d6a47d897a9a4753c800b39adb17c06e154422cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate Akka Artery,FLINK-28372,13469894,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,chesnay,chesnay,04/Jul/22 07:44,04/Sep/23 22:35,04/Jun/24 20:42,,,,,,,,,,Runtime / RPC,,,,,,,2,pull-request-available,stale-assigned,,,Our current Akka setup uses the deprecated netty-based stack. We need to eventually migrate to Akka Artery.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29065,,,,,,FLINK-24736,,,,,,,,,,FLINK-24743,,,FLINK-30772,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 04 22:35:10 UTC 2023,,,,,,,,,,"0|z16krs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade netty version in flink-akka-rpc,FLINK-28371,13469885,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mishagr,mishagr,04/Jul/22 07:15,04/Jul/22 07:44,04/Jun/24 20:42,04/Jul/22 07:44,1.14.5,1.15.0,1.16.0,,,,,,Runtime / RPC,,,,,,,0,,,,,"flink-akka-rpc contains dependency on io.netty:netty:3.10.6.Final

which is very old version with multiple known critical vulnerabilities.

Please upgrade this version to at least 4.1.70 as in shaded netty dependency.

Thanks,

Michael",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 07:44:48 UTC 2022,,,,,,,,,,"0|z16kps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 07:44;chesnay;There is no version of Akka that supports Netty 4.

Closing this in favor of FLINK-28372.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add close method for KafkaRecordSerializationSchema,FLINK-28370,13469881,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ruanhang1993,leonard,leonard,04/Jul/22 07:02,11/Oct/23 18:54,04/Jun/24 20:42,,1.14.5,1.15.0,,,,,,,Connectors / Kafka,,,,,,,0,pull-request-available,stale-assigned,,,"KafkaRecordSerializationSchema only offers open() method, we should offer close() method to release the opened resources.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22902,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Aug 20 22:37:48 UTC 2022,,,,,,,,,,"0|z16kow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 07:05;ruanhang1993;I am interested in this ticket and I could help to resolve this issue.;;;","20/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support PARSE_URL bulit-in function in Table API,FLINK-28369,13469857,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,04/Jul/22 05:09,07/Jul/22 01:13,04/Jun/24 20:42,07/Jul/22 01:13,,,,,,1.16.0,,,API / Python,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 01:13:14 UTC 2022,,,,,,,,,,"0|z16kjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 01:13;dianfu;Merged to master via a5b088da231be0d844ef2bc74c148e3bafffb2e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get field type return result problem,FLINK-28368,13469850,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,hunterLiu,hunterLiu,04/Jul/22 02:51,04/Jul/22 03:16,04/Jun/24 20:42,04/Jul/22 03:16,,,,,,,,,Table SQL / API,,,,,,,0,,,,,"When I use resolved Schema.get Column Data Types(), I expect it to return the types of all fields, but it returns the nullable of the field type. I don't know if it's a design flaw, or I Problems caused during use",flink1.14.x+iceberg0.13.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/22 02:51;hunterLiu;1.png;https://issues.apache.org/jira/secure/attachment/13046195/1.png","04/Jul/22 02:51;hunterLiu;2.png;https://issues.apache.org/jira/secure/attachment/13046194/2.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 03:16:03 UTC 2022,,,,,,,,,,"0|z16ki0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 02:52;hunterLiu;If this is a bug, send it to me to fix it;;;","04/Jul/22 03:16;jark;It's by design. Nullability is a part of data type in Flink. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffsetDateTime does not work with keyBy,FLINK-28367,13469824,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,_hl_,_hl_,03/Jul/22 15:49,05/Jul/22 11:14,04/Jun/24 20:42,,1.15.0,,,,,,,,API / DataStream,API / Type Serialization System,,,,,,0,,,,,"Using keyBy incorrectly (de-)serializes java.time.OffsetDateTime types - the offset gets lost and becomes null.

Here's a minimal non-working example:

 
{code:java}
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment

import java.time.OffsetDateTime

object MWE {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    env
      .fromElements(""2022-07-03T15:35:48.142Z"", ""2022-07-03T15:35:48.438Z"")
      .map(OffsetDateTime.parse(_))
      .keyBy((t: OffsetDateTime) => t)
      .print()

    env.execute()
  }
} {code}
 

Expected Output:
{code:java}
2022-07-03T15:35:48.438Z
2022-07-03T15:35:48.142Z{code}
Actual Output:
{code:java}
2022-07-03T15:35:48.438null
2022-07-03T15:35:48.142null{code}
The issue arises whenever keyBy and OffsetDateTime are involved; I believe it could have something to do with the way that flink serializes the state.","* Java 1.8 (openjdk 1.8.0_322)
 * Scala 2.12.15
 * Flink 1.15.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 11:14:53 UTC 2022,,,,,,,,,,"0|z16kc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/22 17:12;_hl_;A workaround is to register a custom Kryo serializer that simply uses toString and parse:
{code:java}
import java.time.OffsetDateTime

import com.esotericsoftware.kryo.io.{Input, Output}
import com.esotericsoftware.kryo.{Kryo, Serializer}

class FixedOffsetDateTimeSerializer
    extends Serializer[OffsetDateTime]
    with Serializable {
  override def write(kryo: Kryo, output: Output, obj: OffsetDateTime): Unit =
    output.writeString(obj.toString)

  override def read(
      kryo: Kryo,
      input: Input,
      `type`: Class[OffsetDateTime]
  ): OffsetDateTime =
    OffsetDateTime.parse(input.readString())
}

object MWE {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.getConfig.registerTypeWithKryoSerializer[FixedOffsetDateTimeSerializer](
      classOf[OffsetDateTime],
      new FixedOffsetDateTimeSerializer()
    )

    env
      .fromElements(""2022-07-03T15:35:48.142Z"", ""2022-07-03T15:35:48.438Z"")
      .map(OffsetDateTime.parse(_))
      .keyBy((t: OffsetDateTime) => t)
      .print()

    env.execute()
  }
}  {code}
But I feel like this should be working out of the box.;;;","03/Jul/22 21:29;qingwei91;I had a look, I think part of it is that we are using a very old version of Kryo, this issue is fixed in the newer version.

 

One way to fix it easily is to add new kryo version to your deps, as documented here: [https://github.com/EsotericSoftware/kryo#with-maven]

If you're using sbt, just add the following to your deps should suffice, it fixes the issue for me (using your example, thanks for that)
{code:java}
""com.esotericsoftware"" % ""kryo"" % ""5.3.0""{code}
 

I think it might be a good idea to bump Flink's Kryo, but I dont know what it entails, is it going to be a pain to upgrade because we depends on many obselete API? Might be good to get contributor's input here, and have it tracked in dedicated ticket;;;","04/Jul/22 11:24;martijnvisser;We ""can't"" bump Flink's Kryo version, see FLINK-3154;;;","04/Jul/22 12:18;_hl_;Thank's Lim for identifying the root cause!

Without having a deep understanding of flink, wouldn't it be possible to run multiple versions of Kryo in parallel, and default to the newer one (with proper versioning in savepoints) for new projects?;;;","05/Jul/22 11:14;martijnvisser;[~_hl_] Unfortunately not. It would be a welcome addition though :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for custom query for JDBC Table API source connector,FLINK-28366,13469815,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,JP_Thanigaivel,JP_Thanigaivel,03/Jul/22 13:05,04/Jul/22 15:46,04/Jun/24 20:42,,1.15.0,,,,,,,,Connectors / JDBC,,,,,,,0,,,,,"Support for custom query for JDBC Table API source connector.

Eg : Select a,b from TABLE_A a, TABLE_B b where a.id=b.id and a.id > 1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16024,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 15:46:57 UTC 2022,,,,,,,,,,"0|z16ka8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 11:26;martijnvisser;[~JP_Thanigaivel] Can you elaborate a bit on your problem? Are you trying to run a Batch job, do you want to perform a Lookup, which JDBC database do you want to call, have you implemented a Dialect for this? ;;;","04/Jul/22 13:04;JP_Thanigaivel;Hi [~martijnvisser], Yes I want to run flink on bounded mysql database using Table API connector.

Lets consider : 
I have two tables TABLE_A and TABLE_B.
I need to join both tables and get the records whose id column matches.
But currently flink selects all the records from both table and perform joins in-memory. But I want to push this join to underlying SQL Engine. Is this possible ? ;;;","04/Jul/22 13:37;martijnvisser;It's not currently possible but it is being worked on via FLINK-16024;;;","04/Jul/22 15:46;JP_Thanigaivel;Thanks [~martijnvisser] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TO_TIMESTAMP lose precision beyond seconds,FLINK-28365,13469797,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,qingwei91,qingwei91,03/Jul/22 08:04,05/Jul/22 14:11,04/Jun/24 20:42,04/Jul/22 20:49,1.16.0,shaded-7.0,,,,,,,Table SQL / Planner,,,,,,,0,,,,,"The TO_TIMESTAMP function in Flink SQL seems to have a regression.

 

It loses the precision after seconds, for example

 
{code:java}
TO_TIMESTAMP('2020-01-01 15:35:00.123456'){code}
returns 2020-01-01 15:35:00.123 in the resulting SQL AST, note that the last 3 digit are missing.

 

I think this started in 1.15.x, because I have code in 1.14 that preserve the precision as I expect.

 

Do you agree this is a bug?

PS: The documentation isn't very comprehensive: [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/]

 

I've added a few assertions to illustrate the issue: [https://github.com/apache/flink/pull/20139]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 20:49:02 UTC 2022,,,,,,,,,,"0|z16k68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 07:14;paul8263;Hi [~qingwei91] ,

Which version of Flink did you use? I tested it with Flink 1.14.3 and the result was cast to Timestamp(3).

The generated execution plan is as below:
{code:java}
== Abstract Syntax Tree ==
LogicalProject(EXPR$0=[TO_TIMESTAMP(_UTF-16LE'2020-01-01 15:35:00.123456', _UTF-16LE'yyyy-MM-dd HH:mm:ss.SSSSSS')])
+- LogicalValues(tuples=[[{ 0 }]])

== Optimized Physical Plan ==
Calc(select=[CAST(2020-01-01 15:35:00.123:TIMESTAMP(3)) AS EXPR$0])
+- Values(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])

== Optimized Execution Plan ==
Calc(select=[CAST(2020-01-01 15:35:00.123:TIMESTAMP(3)) AS EXPR$0])
+- Values(tuples=[[{ 0 }]]) {code}
We can see a cast to Timestamp(3) was added.

Also we can find some clues in flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java:
{code:java}
// TODO: the return type of TO_TIMESTAMP should be TIMESTAMP(9),
//  but we keep TIMESTAMP(3) now because we did not support TIMESTAMP(9) as time attribute.
//  See: https://issues.apache.org/jira/browse/FLINK-14925
public static final SqlFunction TO_TIMESTAMP =
        new SqlFunction(
                ""TO_TIMESTAMP"",
                SqlKind.OTHER_FUNCTION,
                ReturnTypes.cascade(
                        ReturnTypes.explicit(SqlTypeName.TIMESTAMP, 3),
                        SqlTypeTransforms.FORCE_NULLABLE),
                null,
                OperandTypes.or(
                        OperandTypes.family(SqlTypeFamily.CHARACTER),
                        OperandTypes.family(SqlTypeFamily.CHARACTER, SqlTypeFamily.CHARACTER)),
                SqlFunctionCategory.TIMEDATE); {code}
The return type of TO_TIMESTAMP function is always TIMESTAMP(3).;;;","04/Jul/22 11:29;martijnvisser;Like [~paul8263] says, the return type of TO_TIMESTAMP is always TIMESTAMP(3). I don't think there's a problem here;;;","04/Jul/22 13:10;qingwei91;Hi, sorry for creating PR for it.

 

-It returns something more precise in Flink 1.14.-

We are using TO_TIMESTAMP in 1.14 where it returns TIMESTAMP with up to 9 precision. I will try to provide a small reproduction and get back to you.

-I guess we can argue it was an unspecified behavior though. I am happy if you think we should close this ticket.-

 

Edit: I misread and wrote something useless...;;;","04/Jul/22 20:49;qingwei91;Hi, I am not able to reproduce this easily with a small example, sorry.

 

It might be tied to Derby, I will close this for now and come back if I manage to produce a tiny reprod;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Job support for Kubernetes Operator,FLINK-28364,13469794,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,wu3396,wu3396,03/Jul/22 07:50,13/Jul/22 15:27,04/Jun/24 20:42,13/Jul/22 15:27,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"*Describe the solution*

Job types that I want to support pyflink for

*Describe alternatives*
like [here|https://github.com/spotify/flink-on-k8s-operator/pull/165]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 15:27:11 UTC 2022,,,,,,,,,,"0|z16k5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/22 16:05;gyfora;I think this would be relatively easy to add for somebody who has experience using/developing the Python API.

We could either introduce a new field in the JobSpec specific to python (instead of jarUri) or create a more generic field that could be used for both such as jobArtifact (which would be either a jar or a python file).;;;","06/Jul/22 13:08;gyfora;Based on some feedback from [~thw] for SQL support, we should rethink whether we need direct support for this in the Operator CRDs.

Instead we can start with simply providing an example for using the current CRs to submit python jobs as we did for SQL (https://github.com/apache/flink-kubernetes-operator/tree/main/examples/flink-sql-runner-example);;;","06/Jul/22 15:36;thw;Thanks for the ping [~gyfora]. This is something that can be solved nicely with a custom application image and does not require any change to the operator. I had done something similar in the past: https://github.com/lyft/flinkk8soperator/tree/master/examples/beam-python;;;","07/Jul/22 01:21;dianfu;[~gyfora] [~thw] +1 to start with a simple example.;;;","08/Jul/22 04:21;nicholasjiang;[~dianfu], [~gyfora], [~thw], IMO, after [FLINK-28443|https://issues.apache.org/jira/projects/FLINK/issues/FLINK-28443] is completed, this feature could be more easily worked.;;;","08/Jul/22 12:50;bgeng777;Following the pattern we have adopted for sql job example, I have created a simple [example|https://github.com/bgeng777/flink-kubernetes-operator/blob/python_example/examples/flink-python-example/python-example.yaml] to run pyflink jobs. As [~nicholasjiang] said, the python demo is trival but we may need more document about building pyflink image in the operator repo.
[~gyfora] [~dianfu] I can take this ticket to add the pyflink example. ;;;","08/Jul/22 15:33;nicholasjiang;[~bgeng777], the users build a custom image which has Python and PyFlink prepared by following the document [using-flink-python-on-docker|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker], no need to more document about building PyFlink image. Hence you could firstly publish the official PyFlink image, then provide the PyFlink example.
[~wu3396], WDYT?;;;","09/Jul/22 04:23;wu3396;  My thought is that it is easy for users to use, and there are documentation examples to demonstrate. It would be great to have an official pyflink image.;;;","13/Jul/22 15:27;gyfora;Closing this as we have added the example for python submission so the operator essentially supports this.

Down the road if we see problems with the current basic approach we can reopen this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support drop savepoint statement in SQL client,FLINK-28363,13469761,13441036,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,Paul Lin,Paul Lin,02/Jul/22 11:15,01/Feb/23 06:36,04/Jun/24 20:42,01/Feb/23 06:36,,,,,,,,,Table SQL / Client,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 02 18:18:34 UTC 2022,,,,,,,,,,"0|z16jy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jul/22 18:18;Weijie Guo;Hi [~Paul Lin] , I'm very interested in this ticket. If you don't mind, can I take care of it~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support savepoint statements in SQL client,FLINK-28362,13469760,13441036,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,Paul Lin,Paul Lin,02/Jul/22 11:14,01/Feb/23 06:36,04/Jun/24 20:42,01/Feb/23 06:36,,,,,,,,,Table SQL / Client,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-02 11:14:03.0,,,,,,,,,,"0|z16jy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support show jobs statement in SQL client,FLINK-28361,13469759,13441036,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,Paul Lin,Paul Lin,02/Jul/22 11:04,22/Feb/23 20:38,04/Jun/24 20:42,01/Feb/23 06:36,,,,,,,,,Table SQL / Client,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Feb 22 20:38:33 UTC 2023,,,,,,,,,,"0|z16jxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/22 10:45;louzhengyu;can you assign this ticket to me? thx! [~Paul Lin];;;","22/Feb/23 20:38;AlexeyLV;What are the PRs that can be associated with this ticket? 
It would be useful to help with understanding of the scope of the changes for this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support stop job statement in SQL client,FLINK-28360,13469758,13441036,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,Paul Lin,Paul Lin,Paul Lin,02/Jul/22 11:03,10/Oct/22 11:22,04/Jun/24 20:42,10/Oct/22 11:22,,,,,,,,,Table SQL / Client,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 10 11:22:15 UTC 2022,,,,,,,,,,"0|z16jxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 11:22;fsk119;Merged into master: 7bce4854faccc0914951832d1dda101afce0105a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Microsoft SQL Server JDBC Dialect,FLINK-28359,13469748,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,qingwei91,qingwei91,qingwei91,02/Jul/22 08:13,04/Jul/22 11:32,04/Jun/24 20:42,04/Jul/22 11:32,1.14.5,1.15.0,,,,,,,Connectors / JDBC,,,,,,,0,pull-request-available,,,,"Hi, I wish to propose supporting SQL Server JDBC Dialect in jdbc connector.

In my workplace, we already have the code working, thought it might be a good idea to contribute it back.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Jul 03 07:45:36 UTC 2022,,,,,,,,,,"0|z16jvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/22 07:45;qingwei91;I am willing to work on this if project maintainer(s) agree this should be included in Flink.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when debug in local ,throw out ""The system time period specification expects Timestamp type but is 'TIMESTAMP_WITH_LOCAL_TIME_ZONE' "" exception",FLINK-28358,13469728,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,v_changpengfei,v_changpengfei,02/Jul/22 00:42,18/Aug/23 22:35,04/Jun/24 20:42,,1.14.4,,,,,,,,Connectors / JDBC,,,,,,,0,auto-deprioritized-minor,debug,pull-request-available,,"h1. subject

when i debug in local to see the jdbcconnector lookup mechanism and run org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.testLookup,throw out a exception ,detail as follow:
{code:java}
org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 106 to line 1, column 120: The system time period specification expects Timestamp type but is 'TIMESTAMP_WITH_LOCAL_TIME_ZONE'

	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:164)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:107)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:215)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:101)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:736)
	at org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.useDynamicTableFactory(JdbcLookupTableITCase.java:195)
	at org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.testLookup(JdbcLookupTableITCase.java:81)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 106 to line 1, column 120: The system time period specification expects Timestamp type but is 'TIMESTAMP_WITH_LOCAL_TIME_ZONE'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5043)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSnapshot(SqlValidatorImpl.java:4886)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1055)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3205)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3187)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3252)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:117)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3196)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3461)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1067)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1041)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1016)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:724)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:159)
	... 46 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: The system time period specification expects Timestamp type but is 'TIMESTAMP_WITH_LOCAL_TIME_ZONE'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
	... 65 more


Process finished with exit code -1 {code}
but run test used maven is ok,command as follow:
{code:java}
cd flink-connector-jdbc
mvn test -Dtest=org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase{code}
log as follow:
{code:java}
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.111 s - in org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 34.326 s
[INFO] Finished at: 2022-07-02T02:31:51+08:00
[INFO] Final Memory: 54M/706M
[INFO] ------------------------------------------------------------------------ {code}
 

 

I have commit a pr,feel free to review,link as follows:

[https://github.com/apache/flink/pull/20135]","maven:3.2.5 maven:3.6.1  maven:3.3.9 

openjdk:1.8.0_333

idea:IntelliJ IDEA 2021.3 (Ultimate Edition)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/22 12:31;v_changpengfei;image-2022-07-06-20-31-29-743.png;https://issues.apache.org/jira/secure/attachment/13046358/image-2022-07-06-20-31-29-743.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 18 22:35:05 UTC 2023,,,,,,,,,,"0|z16jqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 11:37;martijnvisser;[~v_changpengfei] I'm suspecting a local issue because all the CI and locally this is working properly. Can you provide a reproducable setup where this error keeps occurring? ;;;","04/Jul/22 12:20;v_changpengfei;I have commit a pr, you can see the root reason,feel free to review,link as follows:

[https://github.com/apache/flink/pull/20135];;;","05/Jul/22 12:36;martijnvisser;[~v_changpengfei] Can you confirm that this issue also occurs for Flink 1.15? ;;;","05/Jul/22 13:10;v_changpengfei;ok;;;","05/Jul/22 17:53;v_changpengfei;[~martijnvisser] hi,i take a look,1.15 do not have the java file of ""org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase""

but if migrate the test code to 1.15,there are still the exception mentioned above.

Need  I to migrate the test code to 1.15?

 ;;;","06/Jul/22 09:25;v_changpengfei;[~martijnvisser] i maybe found  the reason why the test was removed,have two points:
h2. one

run sql  “SELECT id1, id2, comment1, comment2 FROM T, LATERAL TABLE(jdbcLookup(id1, id2)) AS S(l_id1, comment1, comment2, l_id2)”

jdbcLookup is used as udf like 1.14 ,will throw exception ,as follows:
{code:java}
org.apache.flink.table.api.ValidationException: SQL validation failed. An error occurred in the type inference logic of function 'jdbcLookup'.
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:184)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:109)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:237)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:105)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:695)
    at org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.useLegacyTableFactory(JdbcLookupTableITCase.java:174)
    at org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.testLookup(JdbcLookupTableITCase.java:92)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runners.Suite.runChild(Suite.java:128)
    at org.junit.runners.Suite.runChild(Suite.java:27)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.flink.table.api.ValidationException: An error occurred in the type inference logic of function 'jdbcLookup'.
    at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.convertToBridgingSqlFunction(FunctionCatalogOperatorTable.java:158)
    at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.convertToSqlFunction(FunctionCatalogOperatorTable.java:142)
    at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lambda$lookupOperatorOverloads$0(FunctionCatalogOperatorTable.java:99)
    at java.util.Optional.flatMap(Optional.java:241)
    at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:99)
    at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:67)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1183)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1169)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1169)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1169)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1169)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1169)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:945)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:704)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:180)
    ... 48 more
Caused by: org.apache.flink.table.api.ValidationException: Could not extract a valid type inference for function class 'org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction'. Please check for implementation mistakes and/or provide a corresponding hint.
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:362)
    at org.apache.flink.table.types.extraction.TypeInferenceExtractor.extractTypeInference(TypeInferenceExtractor.java:150)
    at org.apache.flink.table.types.extraction.TypeInferenceExtractor.forTableFunction(TypeInferenceExtractor.java:113)
    at org.apache.flink.table.functions.TableFunction.getTypeInference(TableFunction.java:208)
    at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.convertToBridgingSqlFunction(FunctionCatalogOperatorTable.java:155)
    ... 62 more
Caused by: org.apache.flink.table.api.ValidationException: Error in extracting a signature to output mapping.
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:362)
    at org.apache.flink.table.types.extraction.FunctionMappingExtractor.extractOutputMapping(FunctionMappingExtractor.java:117)
    at org.apache.flink.table.types.extraction.TypeInferenceExtractor.extractTypeInferenceOrError(TypeInferenceExtractor.java:161)
    at org.apache.flink.table.types.extraction.TypeInferenceExtractor.extractTypeInference(TypeInferenceExtractor.java:148)
    ... 65 more
Caused by: org.apache.flink.table.api.ValidationException: Unable to extract a type inference from method:
public void org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction.eval(java.lang.Object[])
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:362)
    at org.apache.flink.table.types.extraction.FunctionMappingExtractor.extractResultMappings(FunctionMappingExtractor.java:183)
    at org.apache.flink.table.types.extraction.FunctionMappingExtractor.extractOutputMapping(FunctionMappingExtractor.java:114)
    ... 67 more
Caused by: org.apache.flink.table.api.ValidationException: Could not extract a data type from 'interface org.apache.flink.table.data.RowData' in generic class 'org.apache.flink.table.functions.TableFunction' in class org.apache.flink.connector.jdbc.table.JdbcRowDataLookupFunction. Please pass the required data type manually or allow RAW types.
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:362)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractDataTypeOrRawWithTemplate(DataTypeExtractor.java:240)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractDataTypeOrRaw(DataTypeExtractor.java:218)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractDataTypeWithClassContext(DataTypeExtractor.java:194)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractFromGeneric(DataTypeExtractor.java:124)
    at org.apache.flink.table.types.extraction.FunctionMappingExtractor.lambda$createGenericResultExtraction$13(FunctionMappingExtractor.java:478)
    at org.apache.flink.table.types.extraction.FunctionMappingExtractor.putExtractedResultMappings(FunctionMappingExtractor.java:319)
    at org.apache.flink.table.types.extraction.FunctionMappingExtractor.collectMethodMappings(FunctionMappingExtractor.java:269)
    at org.apache.flink.table.types.extraction.FunctionMappingExtractor.extractResultMappings(FunctionMappingExtractor.java:169)
    ... 68 more
Caused by: org.apache.flink.table.api.ValidationException: Cannot extract a data type from an internal 'org.apache.flink.table.data.RowData' class without further information. Please use annotations to define the full logical type.
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:362)
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:357)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.checkForCommonErrors(DataTypeExtractor.java:367)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractDataTypeOrError(DataTypeExtractor.java:272)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractDataTypeOrRawWithTemplate(DataTypeExtractor.java:232)
    ... 75 more {code}
 

!image-2022-07-06-20-31-29-743.png!

_1.15 uses JdbcRowDataLookupFunction to_ replace {_}of '{_}JdbcTableSource.Builder'{_}, whether it is not recommended to use JdbcRowDataLookupFunction as udf directly in 1.15 later,{_}I find this figure that may prove my conjecture
h2. two

run sql ""

SELECT source.id1, source.id2, L.comment1, L.comment2 FROM T AS source 

JOIN lookup for system_time as of source.proctime AS L 

ON source.id1 = L.id1 and source.id2 = L.id2

""

when debug in local ,throw out ""The system time period specification expects Timestamp type but is 'TIMESTAMP_WITH_LOCAL_TIME_ZONE' "" exception

 

finally,lead to this test of ""org.apache.flink.connector.jdbc.table.JdbcLookupTableITCase.testLookup"" all case is failed，so this test is removed

 

i have commit a new pull request in 1.15 with test which remove the useLegacyTableFactory testcase,feel free to review thanks

 

[https://github.com/apache/flink/pull/20185];;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Watermark issue when recovering Finished sources,FLINK-28357,13469687,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,Sandys-Lumsdaine,Sandys-Lumsdaine,01/Jul/22 15:21,09/Jul/22 15:34,04/Jun/24 20:42,07/Jul/22 12:31,1.15.0,,,,,1.14.6,1.15.2,1.16.0,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,"Copied mostly from email trail on the flink user mailing list:

I done a lot of experimentation and I’m convinced there is a problem with Flink handling Finished sources and recovery. 

The program consists of:
 * Two sources:
 ** One “Long Running Source” – stays alive and emits a watermark of DateTime.now() every 10 seconds.
 *** Prints the console a message saying the watermark has been emitted.
 *** *Throws an exception every 5 or 10 iterations to force a recovery.*
 ** One “Short Lived Source” – emits a Long.MAX_VALUE watermark, prints a message to the console and returns.
 * The “Short Live Source” feeds into a map() and then it joins with the “Long Running Source” with a KeyedCoProcessFunction. Moves to “FINISHED” state by Flink.

The problem here is that the “Join” receives no Long.MAX_VALUE watermark from the map() in some situations after a recovery. The dashboard goes from showing this:

!https://attachment.outlook.live.net/owa/MSA%3Ajas_sl%40hotmail.com/service.svc/s/GetAttachmentThumbnail?id=AQMkADAwATEyMTk3LTZiMDQtODBkMi0wMAItMDAKAEYAAAOeUdiydD9QS6CQDK1Dg0olBwACkmHn2W1HRKQHhbPYmGe%2BAASF%2B488AAAAApJh59ltR0SkB4Wz2JhnvgAFXJ9puQAAAAESABAAyemY6ar4b0GAFLHn3hpyCw%3D%3D&thumbnailType=2&isc=1&token=eyJhbGciOiJSUzI1NiIsImtpZCI6IkZBRDY1NDI2MkM2QUYyOTYxQUExRThDQUI3OEZGMUIyNzBFNzA3RTkiLCJ0eXAiOiJKV1QiLCJ4NXQiOiItdFpVSml4cThwWWFvZWpLdDRfeHNuRG5CLWsifQ.eyJvcmlnaW4iOiJodHRwczovL291dGxvb2subGl2ZS5jb20iLCJ1YyI6IjMwMDU4MTIzODAzMzRlMmZhNzE5ZGUxOTNjNjA4NjQ3IiwidmVyIjoiRXhjaGFuZ2UuQ2FsbGJhY2suVjEiLCJhcHBjdHhzZW5kZXIiOiJPd2FEb3dubG9hZEA4NGRmOWU3Zi1lOWY2LTQwYWYtYjQzNS1hYWFhYWFhYWFhYWEiLCJpc3NyaW5nIjoiV1ciLCJhcHBjdHgiOiJ7XCJtc2V4Y2hwcm90XCI6XCJvd2FcIixcInB1aWRcIjpcIjMxODQwOTE5NTk0NjE5NFwiLFwic2NvcGVcIjpcIk93YURvd25sb2FkXCIsXCJvaWRcIjpcIjAwMDEyMTk3LTZiMDQtODBkMi0wMDAwLTAwMDAwMDAwMDAwMFwiLFwicHJpbWFyeXNpZFwiOlwiUy0xLTI4MjctNzQxMzUtMTc5NTQ1NzIzNFwifSIsIm5iZiI6MTY1NjY4ODI3OCwiZXhwIjoxNjU2Njg4ODc4LCJpc3MiOiIwMDAwMDAwMi0wMDAwLTBmZjEtY2UwMC0wMDAwMDAwMDAwMDBAODRkZjllN2YtZTlmNi00MGFmLWI0MzUtYWFhYWFhYWFhYWFhIiwiYXVkIjoiMDAwMDAwMDItMDAwMC0wZmYxLWNlMDAtMDAwMDAwMDAwMDAwL2F0dGFjaG1lbnQub3V0bG9vay5saXZlLm5ldEA4NGRmOWU3Zi1lOWY2LTQwYWYtYjQzNS1hYWFhYWFhYWFhYWEiLCJoYXBwIjoib3dhIn0.KI4I55ycdP1duIwxyYZstLCtnNOwEkyTxfEwK_5a35-ZLMrKd8zHCB5Elw-9-A9UHIxFGSYOlwnHXRvDT0xa6FqFIlO8cnebBRLKv9DhxHwfZqdKWIeF2EcUqwH0ejeA3RvD3-dR95iHPTf52-tuKi27nclPUUEJgbfRWQY3wHMDAFLLaLvKM6AV5S1IhGjBmy3MF_1oulTXbqRZx0ar3L8YQiHEGnfKGjFO2zSxQcTZXAp_rch4HIrVv9GSEcQnD7nBhWPBuuzuvXOvJiUzg0u_e9CUuf1-OcQwhUV3cf7cvme8JadfliY6ywkOne1OZsclQeDFc8EnGZke3l2V_Q&X-OWA-CANARY=Es9QgEoDXEyksG3kZxXeMGC1LvlzW9oYJ-lyWNl-xblWQjmqz5FH_a2-eHuR6Zr51XNjigQpQDs.&owa=outlook.live.com&scriptVer=20220617005.11&animation=true!

To the below after a recovery (with the currentInput1/2Watermark metrics showing input 2 having not received a watermark from the map, saying –Long.MAX_VALUE):

!image-2022-07-01-16-18-14-768.png!

The program is currently set to checkpoint every 5 seconds. By experimenting with 70 seconds, it seems that if only one checkpoint has been taken with the “Short Lived Source” in a FINISHED state since the last recovery then everything works fine and the restarted “Short Lived Source” emits its watermark and I see the “ShortedLivedEmptySource emitting Long.MAX_VALUE watermark” message on the console meaning the run() definitely executed. However, I found that if 2 or more checkpoints are taken since the last recovery with the source in a FINISHED state then the console message does not appear and the watermark is not emitted.

To repeat – the Join does not get a Long.MAX_VALUE watermark from my source or Flink if I see two or more checkpoints logged in between recoveries. If zero or checkpoints are made, everything is fine – the join gets the watermark and I see my console message. You can play with the checkpointing frequency as per the code comments:

        // Useful checkpoint interval options:

        //    5 - see the problem after the first recovery

        //   70 - useful to see bad behaviour kick in after a recovery or two

        //  120 - won't see the problem as we don't have 2 checkpoints within a single recovery session

If I merge the Triggering/Completed checkpoint messages in the log with my console output I see something like this clearly showing the “Short Lived Source” run() method is not executed after 2 checkpoints with the operators marked as FINISHED:

 

2022-06-29T11:52:31.268Z: *ShortLivedEmptySource* emitting Long.MAX_VALUE watermark.

2022-06-29T11:52:31.293Z: LongRunningSource emitting initial watermark=1656503551268

2022-06-29T11:52:41.302Z: LongRunningSource emitting loop watermark=1656503561302

2022-06-29T11:52:51.302Z: LongRunningSource emitting loop watermark=1656503571302

2022-06-29T11:53:01.303Z: LongRunningSource emitting loop watermark=1656503581303

2022-06-29 11:53:02.772 INFO  [Checkpoint Timer] o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 1 (type=CheckpointType\{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})

2022-06-29 11:53:02.870 INFO  [jobmanager-io-thread-10] o.a.f.r.c.CheckpointCoordinator    Completed checkpoint 1 for job 877656d7752bc1304c2cb92790e6aefb

2022-06-29T11:53:11.303Z: LongRunningSource emitting loop watermark=1656503591303

2022-06-29T11:53:21.304Z: LongRunningSource emitting loop watermark=1656503601304

2022-06-29T11:53:21.304Z: ------------------ Recovery ------------------

2022-06-29T11:53:22.405Z: LongRunningSource emitting initial watermark=1656503602405

2022-06-29T11:53:22.408Z: *ShortLivedEmptySource* emitting Long.MAX_VALUE watermark.

2022-06-29T11:53:32.406Z: LongRunningSource emitting loop watermark=1656503612406

2022-06-29T11:53:42.406Z: LongRunningSource emitting loop watermark=1656503622406

2022-06-29 11:53:51.048 INFO  [Checkpoint Timer] o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 2 (type=CheckpointType\{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})

2022-06-29 11:53:51.067 INFO  [jobmanager-io-thread-4] o.a.f.r.c.CheckpointCoordinator     Completed checkpoint 2 for job 877656d7752bc1304c2cb92790e6aefb

2022-06-29T11:53:52.407Z: LongRunningSource emitting loop watermark=1656503632407

2022-06-29T11:54:02.407Z: LongRunningSource emitting loop watermark=1656503642407

2022-06-29T11:54:12.408Z: LongRunningSource emitting loop watermark=1656503652408

2022-06-29T11:54:22.408Z: LongRunningSource emitting loop watermark=1656503662408

2022-06-29T11:54:32.409Z: LongRunningSource emitting loop watermark=1656503672409

2022-06-29T11:54:42.409Z: LongRunningSource emitting loop watermark=1656503682409

2022-06-29T11:54:52.410Z: LongRunningSource emitting loop watermark=1656503692410

2022-06-29 11:55:01.048 INFO  [Checkpoint Timer] o.a.f.r.c.CheckpointCoordinator           Triggering checkpoint 3 (type=CheckpointType\{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD})

2022-06-29 11:55:01.057 INFO  [jobmanager-io-thread-10] o.a.f.r.c.CheckpointCoordinator    Completed checkpoint 3 for job 877656d7752bc1304c2cb92790e6aefb

2022-06-29T11:55:02.410Z: LongRunningSource emitting loop watermark=1656503702410

2022-06-29T11:55:02.411Z: ------------------ Recovery ------------------

2022-06-29T11:55:03.445Z: LongRunningSource emitting initial watermark=1656503703444       <<<<< NO “ShortLivedEmptySource” message after recovery

2022-06-29T11:55:13.446Z: LongRunningSource emitting loop watermark=1656503713445

2022-06-29T11:55:23.446Z: LongRunningSource emitting loop watermark=1656503723446

2022-06-29T11:55:33.446Z: LongRunningSource emitting loop watermark=1656503733446

 

I have also attached a longer example with shows everything working fine after 5 recoveries, and then breaking after the 6{^}th{^}.

I am guessing here it has something to do with the checkpointing and recovery of a FINISHED source.

Finally, here are some ways that allows the code to work:
 * Change the code so the “Short Lived Source” doesn’t return from run() and stays RUNNING (uncomment the Thread.sleep)
 * As I mentioned before, if I remove the map() operator the problem in the join also goes away. (I don’t see the console output but the join is happy)
 * Use a long enough checkpoint interval (e.g. 120 seconds) so we don’t have two checkpoints with FINISHED state per recovery.

The fact these changes prevent the issue means I really think there’s some bug or inconsistency here – if somebody could explain I would really appreciate it.

 ",This can be reproduced in an IDE with the attached sample program.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/22 15:21;Sandys-Lumsdaine;WatermarkDemoMain.java;https://issues.apache.org/jira/secure/attachment/13046159/WatermarkDemoMain.java","01/Jul/22 15:18;Sandys-Lumsdaine;image-2022-07-01-16-18-14-768.png;https://issues.apache.org/jira/secure/attachment/13046161/image-2022-07-01-16-18-14-768.png","08/Jul/22 16:06;Sandys-Lumsdaine;image-2022-07-08-17-06-01-256.png;https://issues.apache.org/jira/secure/attachment/13046462/image-2022-07-08-17-06-01-256.png","01/Jul/22 15:21;Sandys-Lumsdaine;longExample.txt;https://issues.apache.org/jira/secure/attachment/13046160/longExample.txt",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 09 15:34:45 UTC 2022,,,,,,,,,,"0|z16jhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 16:38;pnowojski;Thanks for the bug report. Indeed there was a bug where max watermark was being swallowed by this non chained map.

The problem was that {{FinishedOnRestoreInput#FinishedOnRestoreInput}} was being constructed with wrong number of inputs, because of some accidental {{null}} passed from the {{StreamGraphGenerator}}.

Just note that even with the bug fix (please see my PR), you will still not see the printed message from the short lived source:
{noformat}
System.out.println(String.format(""%s: ShortLivedEmptySource emitting Long.MAX_VALUE watermark."", DateTime.now()));
{noformat}
as after recovery this source is never started, so the code never reaches the run method. And that's fine, MAX_WATERMARK is emitted by the framework.

;;;","05/Jul/22 07:09;Sandys-Lumsdaine;Great - thanks for the feedback. Understood about the source not starting again after being marked as Finished and the framework emitting a MAX_WATERMARK - this is the behaviour I was expecting.;;;","05/Jul/22 07:11;Sandys-Lumsdaine;Also - you say ""non-chained map"" but {+}I believe this problem also occured with operator chaining{+}. I disabled operator chaining in my test program to make it clearer what was going on.;;;","05/Jul/22 08:44;pnowojski;{quote}
Also - you say ""non-chained map"" but I believe this problem also occurred with operator chaining. I disabled operator chaining in my test program to make it clearer what was going on.
{quote}
Are you sure? I can not reproduce this problem with or without my fix when chaining is enabled. There might be another issue lurking somewhere around, but I can not reproduce it at the moment. If you could confirm that there was no problem with enabled chaining, or if you could provide some reproduction steps (ideally modify [the ITCase that I'm adding in my PR|https://github.com/apache/flink/pull/20158/commits/2db367614bb16c6af714cb8c0cfefbb4ace272f3#diff-24e86c59d8547cbd9a95f798d3e0c300daa99dae9fba5be017ed4b2143bd5787]. This ITCase would livelock/never finish if the watermarks stagnate after recovery), that would be great. Just note what I wrote before, that verifying this issue based on messages printed from the sources won't work. ;;;","07/Jul/22 12:31;pnowojski;I've merged the non-chained tasks fix. [~Sandys-Lumsdaine] if you think there is still another bug present, please feel free to re-open the ticket.

merged commit 574ffa4 into apache:master
merged commit fa4b327 into apache:release-1.14
merged commit 5c1d412 into apache:release-1.15;;;","08/Jul/22 16:08;Sandys-Lumsdaine;Hello - sorry been busy and just got round to rechecking my test program. 

If I comment out the line ""env.disableOperatorChaining();"" to allow operator chaining the issue also occurs. The screenshot of the dashboard after a recovery and the watermark not working properly is shown below:

!image-2022-07-08-17-06-01-256.png!

As you can see input2 for the join is saying Long.MIN_VALUE meaning the framework didn't trasnmit the watermark on behalf of my finished source (I didn't see the console message as you describe because it is finished).

If you're having trouble reproducing on 1.15.0 before your fix, try changing the checkpointing frequency to 5 * 1000 instead of 70 as per my comments. For me that cause the issue immediately after the first recovery.

 

BTW, I can't re-open this issue as you describe - not sure if I have perms etc.;;;","09/Jul/22 14:26;Sandys-Lumsdaine;Maybe my example above is actually the same scenario as before.... if you're happy the chaining above isn't relevant for this bug I'm happy too.;;;","09/Jul/22 15:34;pnowojski;Hi [~Sandys-Lumsdaine]
{quote}
Maybe my example above is actually the same scenario as before....
{quote}
Yes, it looks like that. On the screen shot above for whatever a reason the {{Empty Stream Map}} is not chained with the {{Short Running Source}}. I'm not sure why, maybe you have slot sharing disabled, or maybe there is a keyed exchange before the {{Empty Stream Map}} (note that in my ITCase I've removed keyed exchanges). You can see that for example {{Join}} and {{Sink: Join Sink}} operators are chained together in one single task. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move operator metric recording logic into statusrecorder ,FLINK-28356,13469676,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,gyfora,gyfora,01/Jul/22 14:43,24/Nov/22 01:03,04/Jun/24 20:42,07/Jul/22 08:18,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently status and metric reporting happen independently which can cause some inconsistency in the metrics . We should move the metrics inside the status recorder 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 08:18:17 UTC 2022,,,,,,,,,,"0|z16jfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 14:44;gyfora;Cc[~matyas] ;;;","01/Jul/22 14:59;morhidi;you can assign this to me [~gyfora] ;;;","07/Jul/22 08:18;gyfora;merged to main 2adf8f6188c86759caa27d1c10ed9d74ed3a1006;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python Bash e2e tests don't clean-up after they've ran, causing disk space issues",FLINK-28355,13469672,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,martijnvisser,martijnvisser,01/Jul/22 14:20,06/Jul/22 06:31,04/Jun/24 20:42,06/Jul/22 02:02,1.14.6,1.15.2,1.16.0,,,1.14.6,1.15.2,1.16.0,API / Python,Test Infrastructure,,,,,,0,pull-request-available,,,,"The Bash based E2E tests that are used in Python aren't cleaned-up after they've ran. These cause disk space issues further downstream.

See the CI run from https://github.com/apache/flink/pull/20114 for results, for example:

-- When starting with the Bash e2e tests
{code:java}
08:47:10 ##[group]Top 15 biggest directories in terms of used disk space
Jul 01 08:47:12 3983560	.
Jul 01 08:47:12 1266692	./flink-end-to-end-tests
Jul 01 08:47:12 624568	./flink-dist
Jul 01 08:47:12 624180	./flink-dist/target
Jul 01 08:47:12 500076	./flink-dist/target/flink-1.16-SNAPSHOT-bin
Jul 01 08:47:12 500072	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
Jul 01 08:47:12 460812	./flink-connectors
Jul 01 08:47:12 392588	./.git
Jul 01 08:47:12 366396	./.git/objects
Jul 01 08:47:12 366388	./.git/objects/pack
Jul 01 08:47:12 349272	./flink-table
Jul 01 08:47:12 335592	./.git/objects/pack/pack-38d46915823ebec2bc660fd160e5cfca5bc3e567.pack
Jul 01 08:47:12 293044	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/opt
Jul 01 08:47:12 251272	./flink-filesystems
Jul 01 08:47:12 246596	./flink-end-to-end-tests/flink-streaming-kinesis-test
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37425&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=860bfb5d-81b0-5968-f128-2a8b5362110d&l=664

-- After completing all Bash bashed e2e tests:
{code:java}
2022-07-01T10:20:17.3594718Z Jul 01 10:20:17 ##[group]Top 15 biggest directories in terms of used disk space
2022-07-01T10:20:18.7520631Z Jul 01 10:20:18 5425892	.
2022-07-01T10:20:18.7521823Z Jul 01 10:20:18 1521472	./flink-end-to-end-tests
2022-07-01T10:20:18.7522566Z Jul 01 10:20:18 1242528	./flink-python
2022-07-01T10:20:18.7523244Z Jul 01 10:20:18 952336	./flink-python/dev
2022-07-01T10:20:18.7524159Z Jul 01 10:20:18 878764	./flink-python/dev/.conda
2022-07-01T10:20:18.7524870Z Jul 01 10:20:18 834200	./flink-python/dev/.conda/lib
2022-07-01T10:20:18.7525619Z Jul 01 10:20:18 726528	./flink-python/dev/.conda/lib/python3.7
2022-07-01T10:20:18.7526397Z Jul 01 10:20:18 683256	./flink-python/dev/.conda/lib/python3.7/site-packages
2022-07-01T10:20:18.7527101Z Jul 01 10:20:18 624568	./flink-dist
2022-07-01T10:20:18.7527768Z Jul 01 10:20:18 624180	./flink-dist/target
2022-07-01T10:20:18.7528494Z Jul 01 10:20:18 500076	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-07-01T10:20:18.7529298Z Jul 01 10:20:18 500072	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-07-01T10:20:18.7530046Z Jul 01 10:20:18 460812	./flink-connectors
2022-07-01T10:20:18.7530546Z Jul 01 10:20:18 392588	./.git
2022-07-01T10:20:18.7531014Z Jul 01 10:20:18 366396	./.git/objects
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37425&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=860bfb5d-81b0-5968-f128-2a8b5362110d&l=9631",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28305,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 02:02:16 UTC 2022,,,,,,,,,,"0|z16jeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 14:20;martijnvisser;[~hxbks2ks] [~dianfu] Can either of you help with this?;;;","05/Jul/22 17:34;martijnvisser;Fixed in

master: b0f0144b287e95a1189098d164cc02b9a6fa42aa;;;","06/Jul/22 02:02;hxbks2ks;Merged into release-1.15 via 230706488987d9ae96154ddf95247fc293e2bc6c
Merged into release-1.14 via 1e1935e12f641cdbb4fccbbd19d75c49a05f8600;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support instr and locate bulit-in function in Table API,FLINK-28354,13469660,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,01/Jul/22 12:57,05/Jul/22 01:00,04/Jun/24 20:42,05/Jul/22 01:00,,,,,,1.16.0,,,API / Python,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 01:00:47 UTC 2022,,,,,,,,,,"0|z16jbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 01:00;dianfu;Merged to master via 5a13c939bc26f38d98f76592b69db81a13de083e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude unschedulable nodes using IP addresses of kubernetes nodes,FLINK-28353,13469654,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Suxing Lee,Suxing Lee,Suxing Lee,01/Jul/22 11:57,26/Jul/22 01:53,04/Jun/24 20:42,26/Jul/22 01:52,1.14.4,1.15.0,,,,1.16.0,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,,,,"when the job is submitted to the k8s cluster and the parameter -Dkubernetes.rest-service.exposed.type=NodePort is used, the web ui address of the job obtained at this time is the IP of any machine in the k8s cluster.
but client will throw connect refuse exception when the node's schedule status is unschedulable. We should exclude those node IPs to choose web ui address",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 26 01:53:03 UTC 2022,,,,,,,,,,"0|z16jag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 01:52;wangyang0918;Fixed via:

master: 48acc1c138309df2f72533777d463fd2225e6fba;;;","26/Jul/22 01:53;wangyang0918;Thanks [~Suxing Lee] for your contribution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Make Pulsar connector stable,FLINK-28352,13469647,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,martijnvisser,martijnvisser,01/Jul/22 11:16,13/Apr/23 14:32,04/Jun/24 20:42,13/Apr/23 08:32,1.14.6,1.15.3,1.16.0,,,pulsar-3.0.1,pulsar-4.0.0,,Connectors / Pulsar,,,,,,,1,pull-request-available,test-stability,,,This ticket is an umbrella ticket to keep track of all currently known Pulsar connector test instabilities. These need to be resolved as soon as possible and before other new Pulsar features can be added. ,,,,,,,,,,,,,FLINK-25686,FLINK-26202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Apr 13 14:32:27 UTC 2023,,,,,,,,,,"0|z16j8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 18:39;syhily;After detailed debugging and logging. We can confirm this is a race condition on pulsar-client-all. It's reported as a bug on https://github.com/apache/pulsar/pull/16171. We will have a quick workaround for fixing this issue. Sorry for anyone who was bothered by Pulsar's test failure. 🧎;;;","22/Dec/22 08:27;mapohl;I disabled {{PulsarSourceUnorderedE2ECase}}, {{PulsarUnorderedPartitionSplitReaderTest}} and {{PulsarUnorderedSourceITCase}} in {{release-1.15}} and {{release-1.16}} due to the instabilities caused by these tests. The tests are not disabled in the externalized connector repository for Pulsar. See FLINK-30351 subtasks for details.

1.16: 9215ef5bbd76e11dd5b373cce4acdbbc9f1207fa
1.15: aa2c0395e8a368d7dd19605b94a0af4d77c519db;;;","04/Jan/23 06:03;syhily;The only issue we have now is the Direct buffer memory leak with Java 11. We will try to get it resolved in Pulsar 2.11.0 release.;;;","13/Apr/23 08:32;martijnvisser;I've converted FLINK-24302 to its own issue, so that we can close this umbrella ticket since all other subtickets are closed;;;","13/Apr/23 14:32;syhily;Thanks. The FLINK-24302 will be closed in Pulsar itself 3.0.0 release. I think.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Sink should support dynamic generated topic from record,FLINK-28351,13469641,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,01/Jul/22 10:52,13/Feb/23 13:52,04/Jun/24 20:42,08/Feb/23 14:59,1.15.0,,,,,pulsar-4.0.0,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,Some people would like to use dynamically-generated topics from messages and use the key hash range policy. This is not supported by the Pulsar sink currently. We would introduce a new interface named TopicExacter and add a new setTopics(TopicExacter) in PulsarSinkBuilder.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28505,,,,,,FLINK-28505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Feb 13 02:25:42 UTC 2023,,,,,,,,,,"0|z16j7k:",9223372036854775807,The default value of `pulsar.source.partitionDiscoveryIntervalMs` is changed to 300000 (5 minutes) for less burden.,,,,,,,,,,,,,,,,,,,"08/Feb/23 14:59;tison;Master via https://github.com/apache/flink-connector-pulsar/pull/14;;;","08/Feb/23 15:00;tison;[~syhily] I've written a release note here. Please check and comment (or if you have the permission, update directly) if changes are needed.;;;","13/Feb/23 02:25;syhily;[~tison] Where is the release note? I can't see the link.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not allow last-state upgrades across Flink minor versions,FLINK-28350,13469626,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,01/Jul/22 09:53,06/Jul/22 12:10,04/Jun/24 20:42,06/Jul/22 12:10,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Since the operator relies on the HA metadata for last-state upgrades we should make sure that it is possible to upgrade between Flink versions using that.

In some cases due to the HA format changes it might not be possible, if so we should force SAVEPOINT upgrade in those cases

cc [~wangyang0918] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 12:10:47 UTC 2022,,,,,,,,,,"0|z16j48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 09:57;gyfora;This can also have the outcome that we should not allow rollbacks during flink version upgrades which should also be treated as part of this ticket;;;","04/Jul/22 02:43;wangyang0918;AFAIK, we changed the HA format in the release-1.15.

BTW, I strongly do not recommend to use the last-state upgrade across Flink major version changes(e.g. 1.14 -> 1.15). This also means that we could always trigger the savepoint in this case.;;;","04/Jul/22 07:32;gyfora;I agree, let's simply not allow last-state upgrades/rollbacks between versions.;;;","06/Jul/22 12:10;gyfora;Merged to main f4b196f334f8adcaa3fb5d3d41a5b079675ef610;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider using resource uid as fixed jobid for operator deployed jobs,FLINK-28349,13469624,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,gyfora,gyfora,01/Jul/22 09:50,06/Jul/22 12:35,04/Jun/24 20:42,06/Jul/22 12:35,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,"There are certain connectors that rely on jobid uniqueness to a certain degree (such as the iceberg sink connector) so it might make sense to use the FlinkDeployment uid as the fixed job id instead of the default 0000000000000000.

cc [~matyas] [~wangyang0918] 

If we decide to do so we have to be mindful of the migration path for already running jobs with HA/last-state upgrade mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 12:35:22 UTC 2022,,,,,,,,,,"0|z16j3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 03:00;wangyang0918;The upstream project Flink has supported to generate a static job id based on the cluster-id instead of ZERO.

 

https://github.com/apache/flink/commit/e70fe68dea764606180ca3728184c00fc63ea0ff;;;","06/Jul/22 12:35;gyfora;Closing this as it will be supported by Flink itself from 1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add configurable flag to disable last-state fallback for savepoint upgrade,FLINK-28348,13469619,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,morhidi,gyfora,gyfora,01/Jul/22 09:33,24/Nov/22 01:03,04/Jun/24 20:42,05/Jul/22 07:50,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently if the job is not running and savepoint upgrade mode is configured, the ApplicationReconciler can fall back to last-state upgrade mode if HA was enabled.

While in most cases this is fine, in some situation the user might only want to allow savepoint ugrades. We should add a flag:



kubernetes.operator.job.upgrade.last-state-fallback.enabled : true

Default should be true to match the current behaviour.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 07:50:00 UTC 2022,,,,,,,,,,"0|z16j2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 13:32;morhidi;fyi [~gyfora] I've started working on this.;;;","05/Jul/22 07:50;gyfora;merged to main 17723ffc09b6813006044f50165bb6cf15427df7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update testcontainers dependency to v1.17.6,FLINK-28347,13469613,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,01/Jul/22 09:21,17/Jul/23 11:13,04/Jun/24 20:42,30/Mar/23 10:55,,,,,,1.18.0,,,Test Infrastructure,,,,,,,0,pull-request-available,stale-assigned,,,"Changelog: https://github.com/testcontainers/testcontainers-java/releases/tag/1.17.6

Main benefits for Flink: Elasticsearch and Pulsar improvements",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Mar 30 10:55:57 UTC 2023,,,,,,,,,,"0|z16j1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Mar/23 10:55;martijnvisser;Fixed in master: c2541cd1a56abdea7cee3ff06446de60a868afae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong plan when selects metadata in a different order against ddl,FLINK-28346,13469609,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,lincoln.86xy,lincoln.86xy,01/Jul/22 09:00,01/Jul/22 12:49,04/Jun/24 20:42,01/Jul/22 12:49,1.15.0,,,,,1.16.0,,,Table SQL / Planner,,,,,,,0,,,,,"The following query will get a wrong plan:

{code}

@Test
def testReadsMetaDataWithDifferentOrder(): Unit = {
val ddl =
s""""""
|CREATE TABLE src (
| id int,
| name varchar,
| tags varchar METADATA VIRTUAL,
| op varchar METADATA VIRTUAL,
| ts timestamp(3) METADATA VIRTUAL
|) WITH (
| 'connector' = 'values',
| 'readable-metadata'='tags:varchar,op:varchar,ts:timestamp(3)',
| 'enable-projection-push-down' = 'false'
|)"""""".stripMargin
util.tableEnv.executeSql(ddl)

util.verifyExecPlan(""SELECT id, name, ts, tags, op FROM src"")
}

{code}

 

{code}

Calc(select=[id, name, ts, CAST(tags AS VARCHAR(2147483647)) AS tags, CAST(op AS VARCHAR(2147483647)) AS op])
+- TableSourceScan(table=[[default_catalog, default_database, src]], fields=[id, name, op, tags, ts])

{code}

 ",,,,,,,,,,,,,FLINK-28334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 12:49:05 UTC 2022,,,,,,,,,,"0|z16j0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 12:49;lincoln.86xy;This is my misunderstanding, the extra cast operation is expected and does not map to the error type (though meta data keys were reordered, the execution plan was equivalent), so close this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink JDBC connector should check batch count before flush,FLINK-28345,13469601,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,hackergin,hackergin,01/Jul/22 08:37,05/Jul/22 11:25,04/Jun/24 20:42,05/Jul/22 11:25,1.14.5,1.15.0,,,,,,,Connectors / JDBC,,,,,,,0,,,,,"org.apache.flink.connector.jdbc.internal.JdbcOutputFormat#flush
{code:java}
//代码占位符
@Override
public synchronized void flush() throws IOException {
    checkFlushException();

    for (int i = 0; i <= executionOptions.getMaxRetries(); i++) {
        try {
            attemptFlush();
            batchCount = 0;
            break; 
   ....{code}
When flush the batch,  we should check batchCount  is grater than 0. Other wise it would cause some problem with some drivers that do not support empty batches, like clickhouse jdbc driver. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 02 04:09:23 UTC 2022,,,,,,,,,,"0|z16iyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 09:07;martijnvisser;Can't this be solved in the Dialect implementation instead of the generic implementation?;;;","01/Jul/22 10:10;hackergin;I think we can't solve this in the Dialect. Here is the error message,  The error message comes from inside the Driver.  I think the better way is to skip the empty batch statement before flush 

 
{code:java}
//代码占位符
Caused by: java.lang.RuntimeException: Writing records to JDBC failed.
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.checkFlushException(JdbcBatchingOutputFormat.java:154)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.writeRecord(JdbcBatchingOutputFormat.java:160)
	at org.apache.flink.streaming.api.functions.sink.OutputFormatSinkFunction.invoke(OutputFormatSinkFunction.java:87)
	at org.apache.flink.streaming.api.functions.sink.SinkFunction.invoke(SinkFunction.java:49)
	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:73)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:113)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:94)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:40)
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:163)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101)
	at org.apache.flink.connector.file.src.impl.FileSourceRecordEmitter.emitRecord(FileSourceRecordEmitter.java:45)
	at org.apache.flink.connector.file.src.impl.FileSourceRecordEmitter.emitRecord(FileSourceRecordEmitter.java:35)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:128)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:275)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:67)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:398)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:619)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:583)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:758)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:573)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: java.sql.SQLException: Please call addBatch method at least once before batch execution
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:190)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.lambda$open$0(JdbcBatchingOutputFormat.java:128)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.sql.SQLException: Please call addBatch method at least once before batch execution
	at com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:43)
	at com.clickhouse.jdbc.SqlExceptionUtils.emptyBatchError(SqlExceptionUtils.java:111)
	at com.clickhouse.jdbc.internal.InputBasedPreparedStatement.executeAny(InputBasedPreparedStatement.java:95)
	at com.clickhouse.jdbc.internal.AbstractPreparedStatement.executeLargeBatch(AbstractPreparedStatement.java:85)
	at com.clickhouse.jdbc.internal.ClickHouseStatementImpl.executeBatch(ClickHouseStatementImpl.java:568)
	at org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImpl.executeBatch(FieldNamedPreparedStatementImpl.java:65)
	at org.apache.flink.connector.jdbc.internal.executor.TableSimpleStatementExecutor.executeBatch(TableSimpleStatementExecutor.java:64)
	at org.apache.flink.connector.jdbc.internal.executor.TableBufferedStatementExecutor.executeBatch(TableBufferedStatementExecutor.java:64)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:216)
	at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:184)
	... 8 more{code}
 

 ;;;","01/Jul/22 10:28;martijnvisser;My concern is that we will have to get more and more database specific implementations done in the generic JDBC implementation.

Perhaps it's good to give this as input also with regards to https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217386271 since that should replace the current JDBC implementation. ;;;","01/Jul/22 11:05;hackergin;Whether it will increase the complexity of generic JDBC implementation is worth considering, but I don't think adding this judgment is a special implementation, or maybe a general optimization ? 

Maybe I have some aspects I didn't take into account, please correct me if I'm wrong.

In view of the fact that the community officials have not added support for the clickhouse, most users may not encounter this problem, but at least I hope this scenario can be considered in the new version of the connector.;;;","01/Jul/22 11:40;martijnvisser;The challenge of course is that this problem might not cause a problem now, but if we have 10 similar sized problems in the future we will have a problem. 

We can also flip it around: shouldn't Clickhouse add support for empty batches? Especially since the Flink JDBC connector doesn't support Clickhouse at this moment (there's no Dialect implementation, nor any test for it). ;;;","02/Jul/22 04:09;hackergin;[~martijnvisser]  Thanks for the suggestion, I reported it to the clickhouse community, they will fix it in the next version [https://github.com/ClickHouse/clickhouse-jdbc/issues/977]  .  Maybe we can close this jira now. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to connect flink and elasticsearch in pyflink 1.15.0?,FLINK-28344,13469596,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,elademir,elademir,01/Jul/22 08:17,04/Jul/22 01:49,04/Jun/24 20:42,01/Jul/22 08:22,1.15.0,,,,,,,,API / DataSet,API / DataStream,API / Python,Connectors / ElasticSearch,Connectors / Kafka,Table SQL / API,,0,,,,,"hello, I'm really interested in Pyflink and aim to create a project related to Kafka > Flink > ElasticSearch > Kibana.

I can consume messages from Kafka in Flink but can not find any source to connect Flink and ElasticSearch. How can I send kafka messages Flink consumed to  ElasticSearch?

My python 3.8 environment includes: apache-flink=1.15.0","apache-flink==1.15.0

python==3.8.0

java11

scala 2.11

elasticsearch==7.4.0

kibana==7.4.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,english,English,Python,Mon Jul 04 01:49:07 UTC 2022,,,,,,,,,,"0|z16ixk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 08:22;martijnvisser;[~elademir] Please use the User mailinglist, Stackoverflow or Slack for these type of questions, see https://flink.apache.org/gettinghelp.html. Jira is meant for tickets regarding new features, improvements or bugs. ;;;","01/Jul/22 08:28;elademir;oh thank you for information(y);;;","04/Jul/22 01:49;dianfu;Hi, [~elademir] 
There are two choices:
1) Use [ES Table API & SQL connector.|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/elasticsearch/] If you want to use it in DataStream API, see [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/datastream/intro_to_datastream_api/#create-using-table--sql-connectors] on how to use Table API & SQL connectors in DataStream jobs.
2) Use [ES DataStream API connector|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/elasticsearch/], it is supported since 1.16, see [https://github.com/apache/flink/blob/master/flink-python/pyflink/datastream/connectors/elasticsearch.py] for more details. For 1.15, you need to copy the implementation to your own project.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect fails using union map type,FLINK-28343,13469581,13421719,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,tartarus,tartarus,01/Jul/22 07:16,08/Dec/22 08:29,04/Jun/24 20:42,,,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"We can reproduce it with the following example
{code:java}
@Test
public void testUnionMapType() {
    // automatically load hive module in hive-compatible mode
    HiveModule hiveModule = new HiveModule(hiveCatalog.getHiveVersion());
    CoreModule coreModule = CoreModule.INSTANCE;
    for (String loaded : tableEnv.listModules()) {
        tableEnv.unloadModule(loaded);
    }
    tableEnv.loadModule(""hive"", hiveModule);
    tableEnv.loadModule(""core"", coreModule);
    tableEnv.executeSql(
            ""CREATE TABLE test_map_table (params string) PARTITIONED BY (`p_date` string)"");
    tableEnv.executeSql(""select map(\""\"",\""\"") as params from test_map_table union select map(\""\"",\""\"") as params from test_map_table"");
} {code}
Because union semantics need to be de-duplicated, So flink will introduce an Aggregate,

An exception will be thrown
{code:java}
Unsupported type(MAP) to generate hash code, the type(MAP) is not supported as a GROUP_BY/PARTITION_BY/JOIN_EQUAL/UNION field {code}
We can see the Aggregate operator in the execution plan
{code:java}
optimize subquery_rewrite cost 33 ms.
optimize result: 
LogicalSink(table=[*anonymous_collect$1*], fields=[params])
+- LogicalProject(inputs=[0])
   +- LogicalAggregate(group=[{0}])
      +- LogicalProject(inputs=[0])
         +- LogicalUnion(all=[true])
            :- LogicalProject(exprs=[[map(_UTF-16LE'':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", _UTF-16LE'':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")]])
            :  +- LogicalTableScan(table=[[test-catalog, default, test_map_table]])
            +- LogicalProject(exprs=[[map(_UTF-16LE'':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", _UTF-16LE'':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")]])
               +- LogicalTableScan(table=[[test-catalog, default, test_map_table]]) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 08 08:29:02 UTC 2022,,,,,,,,,,"0|z16iu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 07:32;luoyuxia;[~tartarus] Thanks for reporting. Should be fixed after https://issues.apache.org/jira/browse/FLINK-25645;;;","08/Dec/22 08:29;luoyuxia;Update: map type is still un-hashable.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink batch support for Hive StorageHandlers,FLINK-28342,13469577,13421719,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,tartarus,tartarus,01/Jul/22 07:04,11/Oct/22 02:54,04/Jun/24 20:42,,,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"Hive introduced StorageHandlers when integrating Hbase, we can refer to the documentation:

[https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration]

[https://cwiki.apache.org/confluence/display/Hive/StorageHandlers]

Usually a Hive table does not set InputFormat if it uses StorageHandler, but currently Flink's MRSplitsGetter does not consider this case. 

When accessing an external Hbase table mapped by Hive using the Flink dialect, an NPE is thrown.
{code:java}
2022-07-01 15:03:09,240 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Failed to create Source Enumerator for source Source: dp_workflow_dag_run_hbase[1]
org.apache.flink.util.FlinkRuntimeException: Could not enumerate file splits
	at org.apache.flink.connector.file.src.AbstractFileSource.createEnumerator(AbstractFileSource.java:143) ~[flink-connector-files-1.15.0.jar:1.15.0]
	at org.apache.flink.connectors.hive.HiveSource.createEnumerator(HiveSource.java:124) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:197) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.applyCall(RecreateOnResetOperatorCoordinator.java:318) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.start(RecreateOnResetOperatorCoordinator.java:71) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:196) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:165) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.registerAndStartNewCoordinators(DefaultOperatorCoordinatorHandler.java:159) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.initializeOperatorCoordinatorsFor(AdaptiveBatchScheduler.java:295) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.updateTopology(AdaptiveBatchScheduler.java:287) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.initializeVerticesIfPossible(AdaptiveBatchScheduler.java:181) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.startSchedulingInternal(AdaptiveBatchScheduler.java:147) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.scheduler.SpeculativeScheduler.startSchedulingInternal(SpeculativeScheduler.java:162) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:626) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:1092) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:965) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:406) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:612) ~[flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:611) ~[flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_745a2262-f519-4375-8b64-8bf2f805f55a.jar:1.15.0]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_102]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_102]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_102]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) [?:1.8.0_102]
Caused by: java.io.IOException: Fail to create input splits.
	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.enumerateSplits(HiveSourceFileEnumerator.java:60) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.file.src.AbstractFileSource.createEnumerator(AbstractFileSource.java:141) ~[flink-connector-files-1.15.0.jar:1.15.0]
	... 40 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.connectors.hive.FlinkHiveException: Unable to instantiate the hadoop input format
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_102]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_102]
	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.enumerateSplits(HiveSourceFileEnumerator.java:60) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.file.src.AbstractFileSource.createEnumerator(AbstractFileSource.java:141) ~[flink-connector-files-1.15.0.jar:1.15.0]
	... 40 more
Caused by: org.apache.flink.connectors.hive.FlinkHiveException: Unable to instantiate the hadoop input format
	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:126) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_102]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_102]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_102]
	at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_102]
Caused by: java.lang.NullPointerException
	at java.lang.Class.forName0(Native Method) ~[?:1.8.0_102]
	at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_102]
	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:120) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_102]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_102]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_102]
	at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_102] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27604,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-07-01 07:04:20.0,,,,,,,,,,"0|z16itc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix comment in BytesKeyNormalizationUtil.java,FLINK-28341,13469576,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,shenjiaqi,shenjiaqi,01/Jul/22 07:01,11/Nov/22 15:45,04/Jun/24 20:42,15/Sep/22 11:36,1.15.1,,,,,1.17.0,,,,,,,,,,0,comment,pull-request-available,starter,,"The comment [here|https://github.com/apache/flink/blob/release-1.15.1-rc1/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/sort/BytesKeyNormalizationUtil.java#L74] is not correct since [Byte.MIN_VALUE|https://docs.oracle.com/javase/7/docs/api/java/lang/Byte.html#MIN_VALUE] = -128, [Byte.MAX_VALUE|https://docs.oracle.com/javase/7/docs/api/java/lang/Byte.html#MAX_VALUE] = 127.

And I think [code below|https://github.com/apache/flink/blob/release-1.15.1-rc1/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/sort/BytesKeyNormalizationUtil.java#L77-L79] can be simplified as:

{code:java}
import org.junit.Assert;
import org.junit.Test;

public class TestIntegerConvertion {
  @Test
  public void testConvertByteInteger() {
    for (byte i = Byte.MIN_VALUE; ; ++i) {
      Assert.assertEquals(convertByFlink(i), convertSimplified(i));
      if (i == Byte.MAX_VALUE)
        break;
    }
  }

  private byte convertByFlink(byte originValue) {
    int highByte = originValue & 0xff;
    highByte -= Byte.MIN_VALUE;
    return (byte)highByte;
  }

  private byte convertSimplified(byte originValue) {
    return (byte) (originValue - Byte.MIN_VALUE); // no need to byte and 0xFF.
  }
}

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Nov 11 15:45:23 UTC 2022,,,,,,,,,,"0|z16it4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 14:25;rakeshv;[~shenjiaqi] can I work on this?;;;","15/Sep/22 11:34;shenjiaqi;problem solved, close this issue;;;","11/Nov/22 15:45;chesnay;master: 2454dfa3aa27006198cd969840b06fe7313b56d0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support using system conda env for PyFlink tests,FLINK-28340,13469571,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,01/Jul/22 06:35,04/Jul/22 07:28,04/Jun/24 20:42,04/Jul/22 07:28,,,,,,1.16.0,,,API / Python,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 07:28:44 UTC 2022,,,,,,,,,,"0|z16is0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 07:28;dianfu;Merged to master via 060fc5d268ae15ed84aa0f3a75c93e8328be87e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce SparkCatalog in table store,FLINK-28339,13469555,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,01/Jul/22 03:59,01/Jul/22 07:45,04/Jun/24 20:42,01/Jul/22 07:45,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 07:45:24 UTC 2022,,,,,,,,,,"0|z16iog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 07:45;lzljs3620320;master: e303ec7a913601389dc323d9dc719f9c937f6ee2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable the possibility to allow leading zeros for number in JSON,FLINK-28338,13469552,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,igniz,igniz,01/Jul/22 03:33,07/Jul/22 17:56,04/Jun/24 20:42,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,,,,,"Caused by: org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParseException: Invalid numeric value: Leading zeroes not allowed
 at [Source: (byte[])""\{""uuid"":""1285"",""name"":""杨YP"",""age"":01,""ts"":""2022-07-01 10:33:09.553"",""partition"":""part8""}""; line: 1, column: 38]
    at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2337)

!image-2022-07-01-11-31-32-561.png!",flinksql 1.13.6 ，解析kafka里面的json数据含有 0 开头的变量值,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/22 03:31;igniz;image-2022-07-01-11-31-32-561.png;https://issues.apache.org/jira/secure/attachment/13046119/image-2022-07-01-11-31-32-561.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 08:48:44 UTC 2022,,,,,,,,,,"0|z16ins:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 03:35;igniz;解决方法是添加一个类似 spring.jackson.parser.allow-numeric-leading-zeros=true 这种参数。也可以不用flinksql去解析，用datastream去解析后强转数据类型。

The solution is to add a spring jackson. parser. Allow numeric leading zeros = true. You can also use datastream to resolve data types without using flinksql.;;;","01/Jul/22 06:35;martijnvisser;[~igniz] Please make sure that the information and comments you're providing are in English;;;","01/Jul/22 06:35;martijnvisser;Last but not least: please verify this behaviour with the latest version of Flink;;;","01/Jul/22 07:27;igniz;The solution is to add a spring jackson. parser. Allow numeric leading zeros = true. You can also use datastream to resolve data types without using flinksql。[~martijnvisser] ;;;","01/Jul/22 09:06;martijnvisser;I think what you're asking for is to enable the ability to override the JSON standard (where leading zeros are not allowed) by adding a configuration option in Flink to override that, which would use Jackson's {{ALLOW_LEADING_ZEROS_FOR_NUMBERS}} to be used. Are you using DataStream, Table, SQL or Python? ;;;","07/Jul/22 08:48;igniz;filnkSql.I'm really sorry for my slow reply~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IllegalArgumentException: Table identifier not set,FLINK-28337,13469550,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,migowei,migowei,01/Jul/22 03:20,04/Jul/22 01:53,04/Jun/24 20:42,01/Jul/22 08:24,1.14.2,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"I use Flink Table SDK to select iceberg table. Set hivecatalog to usercatalog, but looks like the default_catalog is still used.

The error message is as flollows:
{code:java}
0:42:41,886 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started
10:42:44,392 INFO  org.apache.iceberg.BaseMetastoreCatalog                      [] - Table loaded by catalog: default_iceberg.s3a_flink.icebergtbcloudtrackingtest
10:42:44,397 INFO  org.apache.iceberg.mr.hive.HiveIcebergSerDe                  [] - Using schema from existing table {""type"":""struct"",""schema-id"":0,""fields"":[{""id"":1,""name"":""vin"",""required"":true,""type"":""string""},{""id"":2,""name"":""name"",""required"":true,""type"":""string""},{""id"":3,""name"":""uuid"",""required"":false,""type"":""string""},{""id"":4,""name"":""channel"",""required"":true,""type"":""string""},{""id"":5,""name"":""run_scene"",""required"":true,""type"":""string""},{""id"":6,""name"":""timestamp"",""required"":true,""type"":""timestamp""},{""id"":7,""name"":""rcv_timestamp"",""required"":true,""type"":""timestamp""},{""id"":8,""name"":""raw"",""required"":true,""type"":""string""}]}
10:42:44,832 INFO  org.apache.iceberg.BaseMetastoreTableOperations              [] - Refreshing table metadata from new version: s3a://warehouse/s3a_flink.db/icebergTBCloudTrackingTest/metadata/00011-8d1ef9f1-8172-49fd-b0de-58196642b662.metadata.json
10:42:44,866 INFO  org.apache.iceberg.BaseMetastoreCatalog                      [] - Table loaded by catalog: default_iceberg.s3a_flink.icebergtbcloudtrackingtest
10:42:44,867 INFO  org.apache.iceberg.mr.hive.HiveIcebergSerDe                  [] - Using schema from existing table {""type"":""struct"",""schema-id"":0,""fields"":[{""id"":1,""name"":""vin"",""required"":true,""type"":""string""},{""id"":2,""name"":""name"",""required"":true,""type"":""string""},{""id"":3,""name"":""uuid"",""required"":false,""type"":""string""},{""id"":4,""name"":""channel"",""required"":true,""type"":""string""},{""id"":5,""name"":""run_scene"",""required"":true,""type"":""string""},{""id"":6,""name"":""timestamp"",""required"":true,""type"":""timestamp""},{""id"":7,""name"":""rcv_timestamp"",""required"":true,""type"":""timestamp""},{""id"":8,""name"":""raw"",""required"":true,""type"":""string""}]}
10:42:48,079 INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient         [] - Trying to connect to metastore with URI thrift://hiveserver:9083
10:42:48,079 INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient         [] - Opened a connection to metastore, current connections: 3
10:42:48,081 INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient         [] - Connected to metastore.
10:42:48,081 INFO  org.apache.hadoop.hive.metastore.RetryingMetaStoreClient     [] - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.metastore.HiveMetaStoreClient ugi=root (auth:SIMPLE) retries=1 delay=1 lifetime=0
10:42:48,132 INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient         [] - Closed a connection to metastore, current connections: 2
10:42:48,308 INFO  org.apache.flink.connectors.hive.HiveParallelismInference    [] - Hive source(s3a_flink.icebergTBCloudTrackingTest}) getNumFiles use time: 171 ms, result: 2
Exception in thread ""main"" java.lang.IllegalArgumentException: Table identifier not set
    at org.apache.iceberg.relocated.com.google.common.base.Preconditions.checkArgument(Preconditions.java:142)
    at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:114)
    at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:89)
    at org.apache.iceberg.mr.mapreduce.IcebergInputFormat.lambda$getSplits$0(IcebergInputFormat.java:102)
    at java.util.Optional.orElseGet(Optional.java:267)
    at org.apache.iceberg.mr.mapreduce.IcebergInputFormat.getSplits(IcebergInputFormat.java:102)
    at org.apache.iceberg.mr.mapred.MapredIcebergInputFormat.getSplits(MapredIcebergInputFormat.java:69)
    at org.apache.iceberg.mr.hive.HiveIcebergInputFormat.getSplits(HiveIcebergInputFormat.java:98)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:107)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:71)
    at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:149)
    at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
    at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
    at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:144)
    at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:114)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:106)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:49)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:82)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104) {code}
code is :
{code:java}
        EnvironmentSettings settings = EnvironmentSettings.newInstance()
                .inBatchMode()
                .build();
        TableEnvironment tableEnv = TableEnvironment.create(settings);

        String catalogName = ""s3IcebergCatalog"";
        String defaultDatabase = ""s3a_flink"";
        String hiveConfDir = ""flink-cloud/src/main/resources"";
        HiveCatalog hive = new HiveCatalog(catalogName, defaultDatabase, hiveConfDir);

        tableEnv.registerCatalog(catalogName, hive);
        tableEnv.useCatalog(catalogName);
        tableEnv.useDatabase(defaultDatabase);
        System.out.println(tableEnv.getCurrentCatalog());
        String tableName = ""icebergTBCloudTrackingTest"";
        tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);

        String sql = ""select uuid from "" + tableName;
        System.out.println(sql);
        tableEnv.executeSql(sql).print();
 {code}
The output of `tableEnv.getCurrentCatalog()` is `s3IcebergCatalog`. But it reports `10:42:44,866 INFO  org.apache.iceberg.BaseMetastoreCatalog    [] - Table loaded by catalog: default_iceberg.s3a_flink.icebergtbcloudtrackingtest `,  and shows `java.lang.IllegalArgumentException: Table identifier not set`.

Does anyone know the reason please?

 ","Flink 1.14.2

Hive 3.1.2

Iceberg 0.12.1

Hadoop 3.2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 01:53:05 UTC 2022,,,,,,,,,,"0|z16inc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 06:36;martijnvisser;[~migowei] The Flink project doesn't support Iceberg; are you using https://github.com/apache/iceberg/tree/master/flink ?;;;","01/Jul/22 07:54;migowei;You mean I can't use Flink Table API to select an Iceberg table by HiveCatalog, even though I set  engine.hive.enabled = true? 

Part of my pom.xml:
{code:java}
<dependency>
    <groupId>org.apache.iceberg</groupId>
    <artifactId>iceberg-flink</artifactId>
</dependency>

<dependency>
    <groupId>org.apache.iceberg</groupId>
    <artifactId>iceberg-flink-runtime</artifactId>
    <version>${iceberg.version}</version>
</dependency> {code}
 ;;;","01/Jul/22 08:24;martijnvisser;[~migowei] I mean that the Flink community is not involved with integrating with Apache Iceberg. Iceberg has created an implementation for that, but for questions regarding integrating Iceberg with Flink, you should reach out to the Iceberg project. ;;;","04/Jul/22 01:53;migowei;Ok, thanks~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support parquet-avro format in PyFlink DataStream,FLINK-28336,13469547,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,01/Jul/22 03:01,04/Jul/22 07:14,04/Jun/24 20:42,04/Jul/22 07:14,,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,"Parquet-avro has three interfaces, ReflectData, SpecificData and GenericData, considered that the first two interface need cooresponding Java class, we just support GenericData in PyFlink, where users use strings to define Avro schema.",,,,,,,,,,,FLINK-28043,FLINK-28182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 07:14:32 UTC 2022,,,,,,,,,,"0|z16imo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 07:14;dianfu;Merged to master via 75024ceba42faef164325ed583c84c08a0e869ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete topic after tests,FLINK-28335,13469539,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,01/Jul/22 00:48,04/Jul/22 04:04,04/Jun/24 20:42,04/Jul/22 04:04,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Currently our test does not remove the topic, kafka local cluster may reuse the data inside the topic, resulting in a test error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 04:04:28 UTC 2022,,,,,,,,,,"0|z16ikw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 04:04;lzljs3620320;master: c24d3d244b4ae00e599924fa8814498f98f74aab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushProjectIntoTableSourceScanRule should cover the case when table source SupportsReadingMetadata and not SupportsProjectionPushDown,FLINK-28334,13469490,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Jun/22 15:12,07/Jul/22 06:55,04/Jun/24 20:42,07/Jul/22 06:28,1.15.0,,,,,1.16.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"""SELECT id, ts FROM src"" query on such a table:

{code}

CREATE TABLE src (
  id int,
  name varchar,
  tags varchar METADATA VIRTUAL,
  ts timestamp(3) METADATA VIRTUAL
) WITH (
  'connector' = 'values',
  'readable-metadata' = 'tags:varchar,ts:timestamp(3)',
  'enable-projection-push-down' = 'false'
)

{code}

 

error occurs

{code}

java.lang.AssertionError: Sql optimization: Assertion error: RexInputRef index 3 out of range 0..2

    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:84)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:62)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:58)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.optimizeTree(StreamCommonSubGraphBasedOptimizerV2.scala:209)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.optimizeBlock(StreamCommonSubGraphBasedOptimizerV2.scala:156)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.$anonfun$doOptimize$1(StreamCommonSubGraphBasedOptimizerV2.scala:79)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.$anonfun$doOptimize$1$adapted(StreamCommonSubGraphBasedOptimizerV2.scala:78)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizerV2.doOptimize(StreamCommonSubGraphBasedOptimizerV2.scala:78)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:94)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:389)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:1199)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan2(TableTestBase.scala:1109)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:1066)
    at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:687)
    at org.apache.flink.table.planner.plan.nodes.exec.stream.TableSourceJsonPlanTest.testReuseSourceWithoutProjectionPushDown(TableSourceJsonPlanTest.java:308)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.AssertionError: RexInputRef index 3 out of range 0..2
    at org.apache.calcite.util.Litmus$1.fail(Litmus.java:31)
    at org.apache.calcite.rex.RexChecker.visitInputRef(RexChecker.java:121)
    at org.apache.calcite.rex.RexChecker.visitInputRef(RexChecker.java:57)
    at org.apache.calcite.rex.RexInputRef.accept(RexInputRef.java:112)
    at org.apache.calcite.rel.core.Project.isValid(Project.java:214)
    at org.apache.calcite.rel.core.Project.<init>(Project.java:93)
    at org.apache.calcite.rel.logical.LogicalProject.<init>(LogicalProject.java:67)
    at org.apache.calcite.rel.logical.LogicalProject.copy(LogicalProject.java:128)
    at org.apache.flink.table.planner.plan.rules.logical.PushProjectIntoTableSourceScanRule.onMatch(PushProjectIntoTableSourceScanRule.java:167)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
    at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
    at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:69)
    ... 60 more

{code}

 ",,,,,,,,,,,FLINK-28346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 06:28:45 UTC 2022,,,,,,,,,,"0|z16ia0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 06:28;godfreyhe;Fix in
master: fb9843af5ffeb6d7561876704d463dea1fcdc153
1.15.2: 1dc4c5ba71e2c920ee94bf4274a850db6bc870d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlueSchemaRegistryAvroKinesisITCase is being Ignored due to `Access key not configured`,FLINK-28333,13469489,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,dannycranmer,chalixar,chalixar,30/Jun/22 15:11,08/Aug/22 12:37,04/Jun/24 20:42,08/Aug/22 12:37,1.15.0,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,,,,,0,,,,,"h1. Description
{{GlueSchemaRegistryAvroKinesisITCase}} test is not being run on CI and is skipped due to {{Access key not configured}}. 
Access Key and Secret Key should be added to test environment variables to enable test.

Currently on adding these keys to environment variables the test fails with 
{quote}AWSSchemaRegistryException: Exception occurred while fetching or registering schema definition = {""type"":""record"",""name"":""User"",""namespace"":""org.apache.flink.glue.schema.registry.test"",""fields"":[{""name"":""name"",""type"":""string""},{""name"":""favorite_number"",""type"":[""int"",""null""]},{""name"":""favorite_color"",""type"":[""string"",""null""]}]}, schema name = gsr_avro_input_stream 
	at com.amazonaws.services.schemaregistry.common.AWSSchemaRegistryClient.getORRegisterSchemaVersionId(AWSSchemaRegistryClient.java:202){quote}

These tests should be enabled and fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 08 12:18:42 UTC 2022,,,,,,,,,,"0|z16i9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 12:18;dannycranmer;This is by design, we do not provide AWS keys to feature branch CI builds since they could be stolen. The GSR keys were supplied to master builds but it appears they are no longer. I ran the end to end tests locally and provided keys and they passed (as part of https://issues.apache.org/jira/browse/FLINK-28094).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlueSchemaRegistryJsonKinesisITCase is being Ignored due to `Access key not configured`,FLINK-28332,13469488,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,dannycranmer,chalixar,chalixar,30/Jun/22 15:10,08/Aug/22 15:21,04/Jun/24 20:42,08/Aug/22 15:21,1.15.0,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,,,,,0,,,,,"h1. Description
{{GlueSchemaRegistryJsonKinesisITCase}} test is not being run on CI and is skipped due to {{Access key not configured}}. 
Access Key and Secret Key should be added to test environment variables to enable test.

Currently on adding these keys to environment variables the test fails with 
{quote}AWSSchemaRegistryException: Exception occurred while fetching or registering schema definition = {""$id"":""https://example.com/address.schema.json"",""$schema"":""http://json-schema.org/draft-07/schema#"",""type"":""object"",""properties"":{""f1"":{""type"":""string""},""f2"":{""type"":""integer"",""maximum"":10000}}}, schema name = gsr_json_input_stream 
	at com.amazonaws.services.schemaregistry.common.AWSSchemaRegistryClient.getORRegisterSchemaVersionId(AWSSchemaRegistryClient.java:202)
{quote}

These tests should be enabled and fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 08 12:18:50 UTC 2022,,,,,,,,,,"0|z16i9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 12:18;dannycranmer;This is by design, we do not provide AWS keys to feature branch CI builds since they could be stolen. The GSR keys were supplied to master builds but it appears they are no longer. I ran the end to end tests locally and provided keys and they passed (as part of https://issues.apache.org/jira/browse/FLINK-28094).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Persist status after every observe loop,FLINK-28331,13469479,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,30/Jun/22 14:15,24/Nov/22 01:02,04/Jun/24 20:42,03/Jul/22 10:32,kubernetes-operator-1.1.0,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,Make sure we don't loose any status information because of the reconcile logic.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Jul 03 10:32:27 UTC 2022,,,,,,,,,,"0|z16i7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jul/22 10:32;gyfora;Merged to main a2db6c31e21d79715324ed45d7138cb8f6d6149e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove old delegation token framework code when new is working fine,FLINK-28330,13469460,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,30/Jun/22 12:24,14/Nov/22 16:17,04/Jun/24 20:42,14/Nov/22 16:17,1.16.0,,,,,1.17.0,,,Connectors / Common,Deployment / Kubernetes,Deployment / YARN,,,,,0,pull-request-available,security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 14 16:17:06 UTC 2022,,,,,,,,,,"0|z16i3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 16:17;mbalassi; [{{480e6ed}}|https://github.com/apache/flink/commit/480e6edf9732f8334ef7576080fdbfc98051cb28] in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
List top 15 biggest directories in terms of used disk space,FLINK-28329,13469457,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,30/Jun/22 12:17,17/Aug/22 10:40,04/Jun/24 20:42,06/Jul/22 14:13,,,,,,1.14.6,1.15.2,1.16.0,Test Infrastructure,Tests,,,,,,0,pull-request-available,,,,"We are having the situation where a lot of disk space gets used by both Bash and Java E2E tests. In order to identify which tests aren't properly cleaning up, it would be good if we output the top 15 directories which are the biggest in used disk space",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 14:13:12 UTC 2022,,,,,,,,,,"0|z16i2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 08:16;martijnvisser;Fixed in master: a33970455712b9d5a67cc66bb049d94ad4c4b543;;;","06/Jul/22 08:07;martijnvisser;Re-opening to also add backports to {{release-1.14}} and {{release-1.15}};;;","06/Jul/22 14:13;martijnvisser;Fixed in release-1.15: 0c4ea69bbe2ebf06f74ede15ae7f7d3e10a876c7
Fixed in release-1.14: 35e43a97c80965f4db6436b0b70958c83688c1c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState failed with IllegalStateException,FLINK-28328,13469455,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hxbks2ks,hxbks2ks,30/Jun/22 12:08,01/Jul/22 10:00,04/Jun/24 20:42,01/Jul/22 10:00,1.16.0,,,,,,,,Runtime / Checkpointing,,,,,,,0,test-stability,,,,"
{code:java}
2022-06-30T10:24:44.5149015Z Jun 30 10:24:44 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Trigger checkpoint failure.
2022-06-30T10:24:44.5165889Z Jun 30 10:24:44 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-06-30T10:24:44.5174822Z Jun 30 10:24:44 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-30T10:24:44.5176702Z Jun 30 10:24:44 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.runJobAndGetCheckpoint(RescaleCheckpointManuallyITCase.java:196)
2022-06-30T10:24:44.5178545Z Jun 30 10:24:44 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingKeyedState(RescaleCheckpointManuallyITCase.java:137)
2022-06-30T10:24:44.5180318Z Jun 30 10:24:44 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState(RescaleCheckpointManuallyITCase.java:115)
2022-06-30T10:24:44.5181746Z Jun 30 10:24:44 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-30T10:24:44.5183196Z Jun 30 10:24:44 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-30T10:24:44.5184703Z Jun 30 10:24:44 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-30T10:24:44.5185708Z Jun 30 10:24:44 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-30T10:24:44.5186854Z Jun 30 10:24:44 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-30T10:24:44.5188130Z Jun 30 10:24:44 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-30T10:24:44.5189317Z Jun 30 10:24:44 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-30T10:24:44.5190508Z Jun 30 10:24:44 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-06-30T10:24:44.5191745Z Jun 30 10:24:44 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-06-30T10:24:44.5193308Z Jun 30 10:24:44 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-06-30T10:24:44.5194728Z Jun 30 10:24:44 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-06-30T10:24:44.5195872Z Jun 30 10:24:44 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-06-30T10:24:44.5196823Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-30T10:24:44.5197864Z Jun 30 10:24:44 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-06-30T10:24:44.5198838Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-06-30T10:24:44.5199856Z Jun 30 10:24:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-06-30T10:24:44.5201014Z Jun 30 10:24:44 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-06-30T10:24:44.5202053Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-30T10:24:44.5203015Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-30T10:24:44.5204282Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-30T10:24:44.5205225Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-30T10:24:44.5206196Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-30T10:24:44.5207234Z Jun 30 10:24:44 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-06-30T10:24:44.5208253Z Jun 30 10:24:44 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-06-30T10:24:44.5209332Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-30T10:24:44.5210340Z Jun 30 10:24:44 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-30T10:24:44.5211276Z Jun 30 10:24:44 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-30T10:24:44.5212212Z Jun 30 10:24:44 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-06-30T10:24:44.5213266Z Jun 30 10:24:44 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-06-30T10:24:44.5214841Z Jun 30 10:24:44 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-06-30T10:24:44.5216182Z Jun 30 10:24:44 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-06-30T10:24:44.5217485Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-06-30T10:24:44.5218864Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-06-30T10:24:44.5219921Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-06-30T10:24:44.5226496Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-06-30T10:24:44.5227579Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-06-30T10:24:44.5228338Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-06-30T10:24:44.5229066Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-06-30T10:24:44.5229867Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-06-30T10:24:44.5230722Z Jun 30 10:24:44 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-06-30T10:24:44.5231544Z Jun 30 10:24:44 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-06-30T10:24:44.5232523Z Jun 30 10:24:44 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-06-30T10:24:44.5233485Z Jun 30 10:24:44 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-06-30T10:24:44.5234349Z Jun 30 10:24:44 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-06-30T10:24:44.5235057Z Jun 30 10:24:44 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-06-30T10:24:44.5235730Z Jun 30 10:24:44 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-06-30T10:24:44.5236387Z Jun 30 10:24:44 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-06-30T10:24:44.5237048Z Jun 30 10:24:44 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Trigger checkpoint failure.
2022-06-30T10:24:44.5237954Z Jun 30 10:24:44 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$getCheckpointException$19(CheckpointCoordinator.java:2177)
2022-06-30T10:24:44.5238712Z Jun 30 10:24:44 	at java.util.Optional.orElseGet(Optional.java:267)
2022-06-30T10:24:44.5239435Z Jun 30 10:24:44 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.getCheckpointException(CheckpointCoordinator.java:2176)
2022-06-30T10:24:44.5240301Z Jun 30 10:24:44 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:917)
2022-06-30T10:24:44.5241134Z Jun 30 10:24:44 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.startTriggeringCheckpoint(CheckpointCoordinator.java:675)
2022-06-30T10:24:44.5241855Z Jun 30 10:24:44 	at java.util.Optional.ifPresent(Optional.java:159)
2022-06-30T10:24:44.5242560Z Jun 30 10:24:44 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint(CheckpointCoordinator.java:517)
2022-06-30T10:24:44.5243474Z Jun 30 10:24:44 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.triggerCheckpoint(CheckpointCoordinator.java:506)
2022-06-30T10:24:44.5244391Z Jun 30 10:24:44 	at org.apache.flink.runtime.scheduler.SchedulerBase.triggerCheckpoint(SchedulerBase.java:899)
2022-06-30T10:24:44.5245145Z Jun 30 10:24:44 	at org.apache.flink.runtime.jobmaster.JobMaster.triggerCheckpoint(JobMaster.java:832)
2022-06-30T10:24:44.5245799Z Jun 30 10:24:44 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-30T10:24:44.5246669Z Jun 30 10:24:44 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-30T10:24:44.5247407Z Jun 30 10:24:44 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-30T10:24:44.5248057Z Jun 30 10:24:44 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-30T10:24:44.5248743Z Jun 30 10:24:44 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-06-30T10:24:44.5249590Z Jun 30 10:24:44 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-06-30T10:24:44.5250528Z Jun 30 10:24:44 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-06-30T10:24:44.5251282Z Jun 30 10:24:44 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-06-30T10:24:44.5252067Z Jun 30 10:24:44 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-06-30T10:24:44.5252850Z Jun 30 10:24:44 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-06-30T10:24:44.5253633Z Jun 30 10:24:44 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-06-30T10:24:44.5254360Z Jun 30 10:24:44 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-06-30T10:24:44.5255233Z Jun 30 10:24:44 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-06-30T10:24:44.5256172Z Jun 30 10:24:44 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-06-30T10:24:44.5257300Z Jun 30 10:24:44 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-06-30T10:24:44.5258044Z Jun 30 10:24:44 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-06-30T10:24:44.5258698Z Jun 30 10:24:44 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-30T10:24:44.5259362Z Jun 30 10:24:44 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-30T10:24:44.5259981Z Jun 30 10:24:44 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-06-30T10:24:44.5260541Z Jun 30 10:24:44 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-06-30T10:24:44.5261126Z Jun 30 10:24:44 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-06-30T10:24:44.5261752Z Jun 30 10:24:44 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-06-30T10:24:44.5262349Z Jun 30 10:24:44 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-06-30T10:24:44.5262949Z Jun 30 10:24:44 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-06-30T10:24:44.5263646Z Jun 30 10:24:44 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-06-30T10:24:44.5264279Z Jun 30 10:24:44 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-06-30T10:24:44.5264834Z Jun 30 10:24:44 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-30T10:24:44.5265481Z Jun 30 10:24:44 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-30T10:24:44.5266124Z Jun 30 10:24:44 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-30T10:24:44.5266775Z Jun 30 10:24:44 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-30T10:24:44.5267336Z Jun 30 10:24:44 Caused by: java.lang.IllegalStateException
2022-06-30T10:24:44.5267889Z Jun 30 10:24:44 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177)
2022-06-30T10:24:44.5268633Z Jun 30 10:24:44 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.startTriggeringCheckpoint(CheckpointCoordinator.java:528)
2022-06-30T10:24:44.5269240Z Jun 30 10:24:44 	... 35 more
2022-06-30T10:24:44.5269571Z Jun 30 10:24:44 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37410&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 10:00:57 UTC 2022,,,,,,,,,,"0|z16i28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 09:29;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37433&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9225;;;","01/Jul/22 09:30;martijnvisser;[~akalashnikov] I saw that you also picked up a ticket for the same test, can you also look into this one? Or is it the same issue?;;;","01/Jul/22 10:00;akalashnikov;[~martijnvisser] thanks to ask me. It is indeed the same problem as FLINK-27162. I have fixed that already but it has not been reviewed yet. In fact, if you have knowledge of CheckpointCoordinator, I will appreciate it if you take a look at it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make table store src codes compiled with Flink 1.14 ,FLINK-28327,13469445,13450148,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,30/Jun/22 11:34,04/Jul/22 04:02,04/Jun/24 20:42,04/Jul/22 04:02,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"- Copy incompatible codes from Flink to Table Store
- Provide different versions for 1.15 and 1.14
- Add action: Build Flink 1.14",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 04:02:06 UTC 2022,,,,,,,,,,"0|z16i00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 04:02;lzljs3620320;master: d29816af772a3507008815931a6afc3c72afe5a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResultPartitionTest.testIdleAndBackPressuredTime failed with AssertError,FLINK-28326,13469444,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,hxbks2ks,hxbks2ks,30/Jun/22 11:26,08/Feb/23 04:13,04/Jun/24 20:42,08/Feb/23 04:13,1.16.0,1.17.0,,,,1.16.2,1.17.0,,Runtime / Network,,,,,,,0,pull-request-available,stale-assigned,test-stability,,"
{code:java}
2022-06-30T09:23:24.0469768Z Jun 30 09:23:24 [INFO] 
2022-06-30T09:23:24.0470382Z Jun 30 09:23:24 [ERROR] Failures: 
2022-06-30T09:23:24.0471581Z Jun 30 09:23:24 [ERROR]   ResultPartitionTest.testIdleAndBackPressuredTime:414 
2022-06-30T09:23:24.0472898Z Jun 30 09:23:24 Expected: a value greater than <0L>
2022-06-30T09:23:24.0474090Z Jun 30 09:23:24      but: <0L> was equal to <0L>
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37406&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Feb 08 04:11:31 UTC 2023,,,,,,,,,,"0|z16hzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 07:49;Weijie Guo;Thanks for reporting the issue, it's an unstable test, I'll fix it.

[~kevin.cyj] [~martijnvisser] can you help assign this ticket to me.;;;","04/Jul/22 06:59;martijnvisser;[~Weijie Guo] Thanks, I've assigned it to you;;;","07/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Sep/22 08:45;chesnay;master: f543b8ac690b1dee58bc3cb345a1c8ad0db0941e;;;","21/Nov/22 04:15;renqs;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43136&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8114]

Looks like the issue happens again. [~Weijie Guo] any thought?;;;","22/Nov/22 13:27;Weijie Guo;[~renqs] Sorry for the reply. I checked the code path and reproduced it locally. Through the previous fix, we can ensure that the request thread must be blocked before the main thread recycle the buffer. However, there is still an extreme case where the request thread recovers from blocking very quickly, resulting in a calculated back-pressure time is equal to 0. I propose that we can add a very short waiting time before `buffer.recycleBuffer` and I tested it thousands of times locally, it seems that the problem did not recur.

If there are no other suggestions, I will open a pull request based on this approach.;;;","18/Jan/23 07:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9464;;;","03/Feb/23 04:26;kevin.cyj;master: 7980d4b7956bbb5952933d6f4c49e27f59c96834;;;","03/Feb/23 05:54;Weijie Guo;Closed it, feel free to reopen this if it appears again.;;;","03/Feb/23 10:18;mapohl;Reopening: Shouldn't we provide a 1.16 backport for this issue as well, [~Weijie Guo]?;;;","03/Feb/23 10:48;Weijie Guo;[~mapohl]Thank you for your reminder :), and I found that the original fix commit was also not picked up to 1.16 branch. I have opened the back-port pull request to handle this.;;;","08/Feb/23 04:11;kevin.cyj;1.16: 096c4a5e29cdffc8b4ed72f7bc6fc7b42dab2e9b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataOutputSerializer#writeBytes increase position twice,FLINK-28325,13469429,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,huwh,huwh,30/Jun/22 10:15,11/Jul/22 11:39,04/Jun/24 20:42,11/Jul/22 11:39,,,,,,,,,Runtime / Task,,,,,,,0,,,,,"Hi, I was looking at the code and found that DataOutputSerializer.writeBytes increases the position twice, I feel it is a problem, please let me know if it is for a special purpose

org.apache.flink.core.memory.DataOutputSerializer#writeBytes

 

!image-2022-06-30-18-14-50-827.png!!image-2022-06-30-18-15-18-590.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23598,,,,,,,,,,,,,,,,,,,,"30/Jun/22 10:14;huwh;image-2022-06-30-18-14-50-827.png;https://issues.apache.org/jira/secure/attachment/13046083/image-2022-06-30-18-14-50-827.png","30/Jun/22 10:15;huwh;image-2022-06-30-18-15-18-590.png;https://issues.apache.org/jira/secure/attachment/13046082/image-2022-06-30-18-15-18-590.png","08/Jul/22 02:52;image.png;https://issues.apache.org/jira/secure/attachment/13046428/image.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 15:49:03 UTC 2022,,,,,,,,,,"0|z16hwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 15:49;Weijie Guo;duplicate as FLINK-23598;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JUnit5 Migration] Module: flink-sql-client,FLINK-28324,13469421,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Leo Zhou,Leo Zhou,Leo Zhou,30/Jun/22 09:43,20/Oct/22 06:12,04/Jun/24 20:42,20/Oct/22 06:12,,,,,,1.17.0,,,Table SQL / Client,,,,,,,0,pull-request-available,starter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Oct 20 06:12:35 UTC 2022,,,,,,,,,,"0|z16huo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 09:45;Leo Zhou;Hi [~zhuzh] , could you assign this ticket to me ?;;;","01/Jul/22 02:36;zhuzh;Thanks for volunteering to take it. [~Leo Zhou]
I have assigned you the ticket.;;;","20/Oct/22 06:12;zhuzh;master: 7f639a8ce4741f072621ac87976aa2589e9cfae0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support using new KafkaSource in PyFlink,FLINK-28323,13469405,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,30/Jun/22 08:39,17/Oct/22 01:49,04/Jun/24 20:42,01/Jul/22 05:09,,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,"KafkaSource implements new FileSource API, which should also be introduced to Python API, thus some other API e.g. HybridSource can use it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26385,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 05:09:16 UTC 2022,,,,,,,,,,"0|z16hr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 05:09;dianfu;Merged to master via ea0368e7ee0c56e0c9ad45233cb1e12ce458be1b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataStreamScanProvider's new method is not compatible,FLINK-28322,13469402,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,30/Jun/22 08:27,01/Sep/22 11:11,04/Jun/24 20:42,06/Jul/22 02:25,1.15.0,,,,,1.15.2,,,Table SQL / API,,,,,,,0,pull-request-available,,,,"In FLINK-25990 , 
Add a method ""DataStream<RowData> produceDataStream(ProviderContext providerContext, StreamExecutionEnvironment execEnv)"" in DataStreamScanProvider.
But this method has no default implementation, this is not compatible when users upgrade their DataStreamScanProvider implementation from 1.14 to 1.15. (The old method will not be called)
This method should be:
{code:java}
    default DataStream<RowData> produceDataStream(
            ProviderContext providerContext, StreamExecutionEnvironment execEnv) {
        return produceDataStream(execEnv);
    }
{code}

DataStreamSinkProvider is the same.

Impact on DataStreamSinkProvider:
DataStreamSinkProvider is no longer a functional interface and requires the use of an anonymous internal class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 02:25:52 UTC 2022,,,,,,,,,,"0|z16hqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 02:25;lzljs3620320;master: 03a1a42d210bf30751a8a5fed295cd56b0ff34df
release-1.15: 57c48a95a7f3cf952e1531fb763431c4284001d0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveDialectQueryITCase fails with error code 137,FLINK-28321,13469400,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,martijnvisser,martijnvisser,30/Jun/22 08:00,27/Aug/23 22:35,04/Jun/24 20:42,,1.15.0,1.17.0,,,,,,,Connectors / Hive,,,,,,,0,auto-deprioritized-critical,auto-deprioritized-major,test-stability,,"{code:java}
Moving data to directory file:/tmp/junit6349996144152770842/warehouse/db1.db/src1/.hive-staging_hive_2022-06-30_03-47-28_878_1781340705558822791-1/-ext-10000
Loading data to table db1.src1
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
OK
OK
OK
OK
OK
OK
OK
OK
OK
OK
OK
##[error]Exit code 137 returned from process: file name '/bin/docker', arguments 'exec -i -u 1001  -w /home/agent02_azpcontainer 8f23cd917ec9d96c13789dabcaafe59398053d00ecf042a5426f9d1588ade349 /__a/externals/node/bin/node /__w/_temp/containerHandlerInvoker.js'.
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37387&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24786",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Aug 27 22:35:10 UTC 2023,,,,,,,,,,"0|z16hq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Mar/23 09:05;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46767&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=22411

{code}
Mar 03 08:10:14 [ERROR] Error occurred in starting fork, check output in log
Mar 03 08:10:14 [ERROR] Process Exit Code: 137
Mar 03 08:10:14 [ERROR] Crashed tests:
Mar 03 08:10:14 [ERROR] org.apache.flink.connectors.hive.HiveDialectQueryITCase
Mar 03 08:10:14 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
[...]
{code};;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","27/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShuffleMasterTest hangs and doesn't produce any output for 900 seconds	,FLINK-28320,13469399,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,martijnvisser,martijnvisser,30/Jun/22 07:56,05/Jul/22 08:41,04/Jun/24 20:42,05/Jul/22 08:41,1.14.6,,,,,,,,Runtime / Coordination,,,,,,,0,test-stability,,,,"{code:java}
Jun 30 03:36:13 [INFO] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.057 s - in org.apache.flink.runtime.rpc.RpcEndpointTest
Jun 30 04:06:22 ==============================================================================
Jun 30 04:06:22 Process produced no output for 900 seconds.
Jun 30 04:06:22 ==============================================================================
Jun 30 04:06:22 ==============================================================================
Jun 30 04:06:22 The following Java processes are running (JPS)
Jun 30 04:06:22 ==============================================================================
Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
Jun 30 04:06:23 630 Launcher
Jun 30 04:06:23 23159 surefirebooter6827274093814314206.jar
Jun 30 04:06:23 11959 Jps
Jun 30 04:06:23 ==============================================================================
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37387&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8099

The ShuffleMasterTest comes after RpcEndpointTest, which is why this test must be the one that's hanging",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28379,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-30 07:56:20.0,,,,,,,,,,"0|z16hps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResumeCheckpointManuallyITCase gets stuck,FLINK-28319,13469390,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,martijnvisser,martijnvisser,30/Jun/22 07:20,16/May/24 14:52,04/Jun/24 20:42,,1.15.3,,,,,,,,,,,,,,,0,auto-deprioritized-major,test-stability,,,"{code:java}
Jun 30 03:16:25 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.109 s - in org.apache.flink.test.streaming.experimental.CollectITCase
==========================================================================================
=== WARNING: This task took already 95% of the available time budget of 237 minutes ===
==========================================================================================
==============================================================================
The following Java processes are running (JPS)
==============================================================================
932 Launcher
20281 Jps
17930 surefirebooter3147893032508885212.jar
==============================================================================
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37384&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=6280",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35380,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 21 22:35:22 UTC 2023,,,,,,,,,,"0|z16hns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 08:22;martijnvisser;What's interesting is that this is actually the latest running test, looking at a successful run:

{code:java}
2022-06-29T18:10:52.1705829Z Jun 29 18:10:52 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.637 s - in org.apache.flink.test.streaming.experimental.CollectITCase
2022-06-29T18:10:52.5971519Z Jun 29 18:10:52 [INFO] 
2022-06-29T18:10:52.5973079Z Jun 29 18:10:52 [INFO] Results:
2022-06-29T18:10:52.5975131Z Jun 29 18:10:52 [INFO] 
2022-06-29T18:10:52.5975676Z Jun 29 18:10:52 [WARNING] Tests run: 1887, Failures: 0, Errors: 0, Skipped: 4
2022-06-29T18:10:52.5976131Z Jun 29 18:10:52 [INFO] 
2022-06-29T18:10:52.6488474Z Jun 29 18:10:52 [INFO] ------------------------------------------------------------------------
2022-06-29T18:10:52.6489444Z Jun 29 18:10:52 [INFO] BUILD SUCCESS
2022-06-29T18:10:52.6490452Z Jun 29 18:10:52 [INFO] ------------------------------------------------------------------------
2022-06-29T18:10:52.6491317Z Jun 29 18:10:52 [INFO] Total time: 33:43 min
2022-06-29T18:10:52.6492224Z Jun 29 18:10:52 [INFO] Finished at: 2022-06-29T18:10:52+00:00
2022-06-29T18:10:53.2311913Z Jun 29 18:10:53 [INFO] Final Memory: 237M/4504M
2022-06-29T18:10:53.2313739Z Jun 29 18:10:53 [INFO] ------------------------------------------------------------------------
2022-06-29T18:10:53.2314958Z Jun 29 18:10:53 [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
{code};;;","11/Jul/22 12:55;chesnay;[~martijnvisser] The test execution order isn't _really_ deterministic, because 1 (of 2 or 4) test threads can get stuck while the other one continues with the remaining tests.

 

From the thread dump we can see that the ResumeCheckpointManuallyITCase is stuck:
{code:java}
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.test.util.TestUtils.waitUntilJobCanceled(TestUtils.java:174)
	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.runJobAndGetExternalizedCheckpoint(ResumeCheckpointManuallyITCase.java:351)
	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedCheckpoints(ResumeCheckpointManuallyITCase.java:314)
	at org.apache.flink.test.checkpointing.ResumeCheckpointManuallyITCase.testExternalizedFSCheckpointsZookeeper(ResumeCheckpointManuallyITCase.java:228) {code};;;","01/Dec/22 09:19;martijnvisser;1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43637&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=21940;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table idle  state ttl configure does not have check when window  size bigger than table state ttl,FLINK-28318,13469378,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,chaojipaopao,chaojipaopao,30/Jun/22 06:41,01/Jul/22 06:45,04/Jun/24 20:42,01/Jul/22 06:45,1.12.4,1.13.5,1.14.4,,,,,,Table SQL / Planner,Table SQL / Runtime,,,,,,0,,,,,"example: my tumbling window size is 1 day ,but  i config  table.exec.state.ttl is 1 hour, if count(1) group by somthing  and  some key more than a hour comming element ,we can get a error vlaue.",flinksql 1.12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,,Thu Jun 30 06:58:51 UTC 2022,,,,,,,,,,"0|z16hl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 06:58;martijnvisser;Can you please provide more information, like the used code and the stacktrace/error? It would also be necessary to test this on a still supported version of Flink. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw unsupport exception when RexSubQuery contains LogicalUnion and correlate variables,FLINK-28317,13469377,13510724,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,luoyuxia,luoyuxia,30/Jun/22 06:40,19/Dec/22 07:32,04/Jun/24 20:42,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-30 06:40:39.0,,,,,,,,,,"0|z16hkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[UI] add external JM and TM log links under history server,FLINK-28316,13469375,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,junhan,junhan,junhan,30/Jun/22 06:32,15/Jul/22 03:33,04/Jun/24 20:42,15/Jul/22 03:33,,,,,,1.16.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 15 03:33:08 UTC 2022,,,,,,,,,,"0|z16hkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 03:33;xtsong;master (1.16): 4e7594a62cc4712fa88d9f8c884bc4e3be2b5705;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[UI] Introduce aggregate stats in tables of the subtasks and taskmanagers,FLINK-28315,13469374,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,junhan,junhan,junhan,30/Jun/22 06:30,14/Jul/22 04:12,04/Jun/24 20:42,14/Jul/22 04:12,1.16.0,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 04:12:14 UTC 2022,,,,,,,,,,"0|z16hk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 04:12;xtsong;master (1.16): 35e557afe9e8392b1b1ebb115f0c7134504b584d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[UI] Introduce ""Cluster Environment"" tab under history server",FLINK-28314,13469373,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,junhan,junhan,junhan,30/Jun/22 06:27,12/Jul/22 10:32,04/Jun/24 20:42,12/Jul/22 10:32,1.16.0,,,,,1.16.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 12 10:32:10 UTC 2022,,,,,,,,,,"0|z16hk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 10:32;xtsong;master (1.16): 3ec376601f836df6314e771b243ca6f896a7f642;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add history server flag to DashboardConfiguration,FLINK-28313,13469368,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,30/Jun/22 06:16,06/Jul/22 02:57,04/Jun/24 20:42,06/Jul/22 02:57,,,,,,1.16.0,,,Runtime / REST,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 02:57:33 UTC 2022,,,,,,,,,,"0|z16hiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 02:57;guoyangze;master a4540fea6bc2ef4ded6007563480820fcc12ecc0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce REST APIs for log URL retrieval,FLINK-28312,13469367,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,30/Jun/22 06:15,07/Jul/22 05:24,04/Jun/24 20:42,07/Jul/22 05:24,,,,,,1.16.0,,,Runtime / REST,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 05:23:57 UTC 2022,,,,,,,,,,"0|z16hio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 05:23;guoyangze;master a99fc3ffbf95910dd8fad6833852ca5eed3f1896;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce REST APIs for the environmental information,FLINK-28311,13469366,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,30/Jun/22 06:15,07/Dec/22 08:31,04/Jun/24 20:42,06/Jul/22 09:10,,,,,,1.16.0,,,Runtime / REST,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30116,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 09:10:51 UTC 2022,,,,,,,,,,"0|z16hig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 09:10;guoyangze;master: ebdde651edae8db6b2ac740f07d97124dc01fea4
1449e8da48b5cf798fad32a71d9bbf7c927c5acf
4ab2536b968ad26aa0b672217e68a0450a47f5fd
adfe97deb5cea83c9c0b2af6dc295c6aa33ab96c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce aggregating task metrics,FLINK-28310,13469365,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,30/Jun/22 06:14,04/Jul/22 06:46,04/Jun/24 20:42,04/Jul/22 06:46,,,,,,1.16.0,,,Runtime / Metrics,Runtime / REST,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 06:45:57 UTC 2022,,,,,,,,,,"0|z16hi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 06:45;guoyangze;master 94915d6c9e1b3a4e5f419d405a7d60c19b6bd483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce metrics of the duration that a task stays in each status,FLINK-28309,13469363,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,30/Jun/22 06:12,01/Jul/22 03:36,04/Jun/24 20:42,01/Jul/22 03:36,,,,,,1.16.0,,,Runtime / Metrics,Runtime / REST,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 03:36:23 UTC 2022,,,,,,,,,,"0|z16hhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 03:36;guoyangze;master: bb8e1d14f05aca186ec874437eba3d44fbb3bd97;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce accumulated time metrics that a running task is busy / idle / back-pressured,FLINK-28308,13469362,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,30/Jun/22 06:11,01/Jul/22 03:36,04/Jun/24 20:42,01/Jul/22 03:36,,,,,,1.16.0,,,Runtime / Metrics,Runtime / REST,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 03:35:59 UTC 2022,,,,,,,,,,"0|z16hhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 03:35;guoyangze;master: 6b8230cf83fd33c4fdd3d7cc99e6a90d9839a350
bc2a86e5bb5f048fb5e5007f916405773a88b5cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-241: Completed Jobs Information Enhancement,FLINK-28307,13469358,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,guoyangze,guoyangze,30/Jun/22 06:08,19/Jul/22 05:21,04/Jun/24 20:42,19/Jul/22 05:21,,,,,,1.16.0,,,Runtime / REST,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,"Streaming and Batch users have different interests in probing a job. While streaming users mainly care about the instant status of a running job (tps, delay, backpressure, etc.), batch users care more about the overall job status during the entire execution (queueing / execution time, total data amount, etc.).

As Flink grows into a unified streaming & batch processor and is adopted by more and more batch users, the experiences in inspecting completed jobs has become more important than ever.

We compared Flink with other popular batch processors, and spotted several potential improvements. Most of these changes involves WebUI & REST API changes, which should be discussed and voted on as FLIPs. However, creating separated FLIPs for each of the improvement might be overkill, because changes needed by each improvement are quite small. Thus, we include all these potential improvements in this one FLIP.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 18 11:50:00 UTC 2022,,,,,,,,,,"0|z16hgo:",9223372036854775807,"We have enhanced the experiences of viewing completed jobs’ information in this release.
- JobManager / HistoryServer WebUI now provides detailed execution time metrics, including durations tasks spend in each execution state and the accumulated busy / idle / back-pressured time during running.
- JobManager / HistoryServer WebUI now provides aggregation of major SubTask metrics, grouped by Task or TaskManager.
- JobManager / HistoryServer WebUI now provides more environmental information, including environment variables, JVM options and classpath.
- HistoryServer now supports browsing logs from external log archiving services. For more details: https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/advanced/historyserver#log-integration",,,,,,,,,,,,,,,,,,,"18/Jul/22 11:50;xtsong;I've drafted a release note for this feature.

{code}
We have enhanced the experiences of viewing completed jobs’ information in this release.
- JobManager / HistoryServer WebUI now provides detailed execution time metrics, including durations tasks spend in each execution state and the accumulated busy / idle / back-pressured time during running.
- JobManager / HistoryServer WebUI now provides aggregation of major SubTask metrics, grouped by Task or TaskManager.
- JobManager / HistoryServer WebUI now provides more environmental information, including environment variables, JVM options and classpath.
- HistoryServer now supports browsing logs from external log archiving services. For more details: https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/advanced/historyserver#log-integration
{code}

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support get table statistic for Hive partition table,FLINK-28306,13469344,13444738,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,30/Jun/22 03:26,29/Aug/22 07:51,04/Jun/24 20:42,09/Aug/22 06:15,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-30 03:26:43.0,,,,,,,,,,"0|z16hdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client end-to-end test failed with ElasticsearchException,FLINK-28305,13469331,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,30/Jun/22 02:01,26/Jul/22 02:30,04/Jun/24 20:42,20/Jul/22 06:21,1.14.5,,,,,1.14.6,,,Table SQL / Client,,,,,,,0,test-stability,,,,"
{code:java}
2022-06-29T19:14:39.0384925Z Jun 29 19:14:38 java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0385547Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0386327Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkAsyncErrorsAndRequests(ElasticsearchSinkBase.java:432) ~[?:?]
2022-06-29T19:14:39.0387078Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:329) ~[?:?]
2022-06-29T19:14:39.0388071Z Jun 29 19:14:38 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:65) ~[flink-table_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0389189Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0390354Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0391515Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0392712Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0393822Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:495) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0394861Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0395885Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:806) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0396858Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0397824Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0398882Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0400103Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0401000Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0401575Z Jun 29 19:14:38 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0402051Z Jun 29 19:14:38 	Suppressed: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0402682Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0403421Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.close(ElasticsearchSinkBase.java:366) ~[?:?]
2022-06-29T19:14:39.0404395Z Jun 29 19:14:38 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0405470Z Jun 29 19:14:38 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0406549Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0407648Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0408717Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1032) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0409755Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1018) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0410752Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:925) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0411732Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0412711Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0413665Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0414726Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0415607Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0416179Z Jun 29 19:14:38 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0417155Z Jun 29 19:14:38 	Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0418020Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0418792Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0419560Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0420302Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0421041Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0421853Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0422676Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0423465Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0424215Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0424907Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0425605Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0426411Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0427309Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) ~[?:?]
2022-06-29T19:14:39.0428140Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) ~[?:?]
2022-06-29T19:14:39.0428965Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) ~[?:?]
2022-06-29T19:14:39.0429771Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) ~[?:?]
2022-06-29T19:14:39.0430553Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) ~[?:?]
2022-06-29T19:14:39.0431331Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114) ~[?:?]
2022-06-29T19:14:39.0432205Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) ~[?:?]
2022-06-29T19:14:39.0432961Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) ~[?:?]
2022-06-29T19:14:39.0433744Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) ~[?:?]
2022-06-29T19:14:39.0434792Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) ~[?:?]
2022-06-29T19:14:39.0435534Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) ~[?:?]
2022-06-29T19:14:39.0459721Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) ~[?:?]
2022-06-29T19:14:39.0460527Z Jun 29 19:14:38 		... 1 more
2022-06-29T19:14:39.0461628Z Jun 29 19:14:38 Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0462516Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0463295Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0464052Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0464810Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0465563Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0466397Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0467225Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0468016Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0468773Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0469464Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0470155Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0470967Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0471854Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) ~[?:?]
2022-06-29T19:14:39.0472688Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) ~[?:?]
2022-06-29T19:14:39.0473770Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) ~[?:?]
2022-06-29T19:14:39.0474587Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) ~[?:?]
2022-06-29T19:14:39.0475633Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) ~[?:?]
2022-06-29T19:14:39.0476411Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114) ~[?:?]
2022-06-29T19:14:39.0477277Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) ~[?:?]
2022-06-29T19:14:39.0478054Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) ~[?:?]
2022-06-29T19:14:39.0478967Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) ~[?:?]
2022-06-29T19:14:39.0479884Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) ~[?:?]
2022-06-29T19:14:39.0480637Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) ~[?:?]
2022-06-29T19:14:39.0481424Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) ~[?:?]
2022-06-29T19:14:39.0481994Z Jun 29 19:14:38 	... 1 more
2022-06-29T19:14:39.0482845Z Jun 29 19:14:38 2022-06-29 19:14:37,877 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 5ef97af7fdd0c4b551549dfceac18104
2022-06-29T19:14:39.0484012Z Jun 29 19:14:38 2022-06-29 19:14:37,879 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task df8d2cd231aa2f7a73136c030b1a8da4_0.
2022-06-29T19:14:39.0485681Z Jun 29 19:14:38 2022-06-29 19:14:37,880 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 2 tasks should be restarted to recover the failed task df8d2cd231aa2f7a73136c030b1a8da4_0. 
2022-06-29T19:14:39.0486975Z Jun 29 19:14:38 2022-06-29 19:14:37,881 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.ElasticsearchAppendSinkTable (5ef97af7fdd0c4b551549dfceac18104) switched from state RUNNING to FAILING.
2022-06-29T19:14:39.0487770Z Jun 29 19:14:38 org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-06-29T19:14:39.0488762Z Jun 29 19:14:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0489968Z Jun 29 19:14:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0491102Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0492170Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0493277Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0494715Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0495748Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0496761Z Jun 29 19:14:38 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0497414Z Jun 29 19:14:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_332]
2022-06-29T19:14:39.0497973Z Jun 29 19:14:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_332]
2022-06-29T19:14:39.0498613Z Jun 29 19:14:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_332]
2022-06-29T19:14:39.0499182Z Jun 29 19:14:38 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_332]
2022-06-29T19:14:39.0500434Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0501693Z Jun 29 19:14:38 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0502871Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0504007Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0505167Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0506301Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0507347Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0508336Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0509323Z Jun 29 19:14:38 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0510309Z Jun 29 19:14:38 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0511318Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0512335Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0513356Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0514378Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0515347Z Jun 29 19:14:38 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0516410Z Jun 29 19:14:38 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0517381Z Jun 29 19:14:38 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0518558Z Jun 29 19:14:38 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0519542Z Jun 29 19:14:38 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0520481Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0521421Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0522345Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0522979Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_332]
2022-06-29T19:14:39.0523555Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_332]
2022-06-29T19:14:39.0524137Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_332]
2022-06-29T19:14:39.0524708Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_332]
2022-06-29T19:14:39.0525253Z Jun 29 19:14:38 Caused by: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0525874Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0526657Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkAsyncErrorsAndRequests(ElasticsearchSinkBase.java:432) ~[?:?]
2022-06-29T19:14:39.0527410Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:329) ~[?:?]
2022-06-29T19:14:39.0528401Z Jun 29 19:14:38 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:65) ~[flink-table_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0529506Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0530680Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0531847Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0532961Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0534005Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:495) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0535053Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0536089Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:806) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0537211Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0538184Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0539134Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0540046Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0540924Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0541485Z Jun 29 19:14:38 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0541967Z Jun 29 19:14:38 	Suppressed: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0542603Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0543337Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.close(ElasticsearchSinkBase.java:366) ~[?:?]
2022-06-29T19:14:39.0544308Z Jun 29 19:14:38 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0545383Z Jun 29 19:14:38 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0546464Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0547559Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0548606Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1032) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0549639Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1018) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0550635Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:925) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0551615Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0552611Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0553575Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0554892Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0555837Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0556411Z Jun 29 19:14:38 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0557388Z Jun 29 19:14:38 	Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0558455Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0559232Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0559999Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0560747Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0561492Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0562322Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0563270Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0576211Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0577089Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0577791Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0578496Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0579331Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0580224Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) ~[?:?]
2022-06-29T19:14:39.0581044Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) ~[?:?]
2022-06-29T19:14:39.0581870Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) ~[?:?]
2022-06-29T19:14:39.0582689Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) ~[?:?]
2022-06-29T19:14:39.0583493Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) ~[?:?]
2022-06-29T19:14:39.0584262Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114) ~[?:?]
2022-06-29T19:14:39.0585017Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) ~[?:?]
2022-06-29T19:14:39.0585771Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) ~[?:?]
2022-06-29T19:14:39.0586546Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) ~[?:?]
2022-06-29T19:14:39.0587322Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) ~[?:?]
2022-06-29T19:14:39.0588299Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) ~[?:?]
2022-06-29T19:14:39.0589098Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) ~[?:?]
2022-06-29T19:14:39.0589669Z Jun 29 19:14:38 		... 1 more
2022-06-29T19:14:39.0590752Z Jun 29 19:14:38 Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0591642Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0592434Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0593196Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0593935Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0594676Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0595509Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0596352Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0597144Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0597889Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0598697Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0599402Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0600208Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0601451Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) ~[?:?]
2022-06-29T19:14:39.0602310Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) ~[?:?]
2022-06-29T19:14:39.0603127Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) ~[?:?]
2022-06-29T19:14:39.0603927Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) ~[?:?]
2022-06-29T19:14:39.0604709Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) ~[?:?]
2022-06-29T19:14:39.0605477Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114) ~[?:?]
2022-06-29T19:14:39.0606364Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) ~[?:?]
2022-06-29T19:14:39.0607123Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) ~[?:?]
2022-06-29T19:14:39.0607908Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) ~[?:?]
2022-06-29T19:14:39.0608673Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) ~[?:?]
2022-06-29T19:14:39.0609399Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) ~[?:?]
2022-06-29T19:14:39.0610193Z Jun 29 19:14:38 	at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) ~[?:?]
2022-06-29T19:14:39.0610854Z Jun 29 19:14:38 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0611795Z Jun 29 19:14:38 2022-06-29 19:14:37,890 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 505af083caeb63496e86df5e472f21d0.
2022-06-29T19:14:39.0612982Z Jun 29 19:14:38 2022-06-29 19:14:37,895 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job insert-into_default_catalog.default_database.ElasticsearchAppendSinkTable (5ef97af7fdd0c4b551549dfceac18104) switched from state FAILING to FAILED.
2022-06-29T19:14:39.0613767Z Jun 29 19:14:38 org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-06-29T19:14:39.0614755Z Jun 29 19:14:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0615982Z Jun 29 19:14:38 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0617122Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0618187Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0619282Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0620362Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0621419Z Jun 29 19:14:38 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0622423Z Jun 29 19:14:38 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0623077Z Jun 29 19:14:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_332]
2022-06-29T19:14:39.0623635Z Jun 29 19:14:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_332]
2022-06-29T19:14:39.0624269Z Jun 29 19:14:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_332]
2022-06-29T19:14:39.0624899Z Jun 29 19:14:38 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_332]
2022-06-29T19:14:39.0625955Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0627169Z Jun 29 19:14:38 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0628340Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0629460Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0630623Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0631770Z Jun 29 19:14:38 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0632819Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0633812Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0634812Z Jun 29 19:14:38 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0635805Z Jun 29 19:14:38 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0636829Z Jun 29 19:14:38 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0637846Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0638969Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0639983Z Jun 29 19:14:38 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0640959Z Jun 29 19:14:38 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0641899Z Jun 29 19:14:38 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0642882Z Jun 29 19:14:38 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0643869Z Jun 29 19:14:38 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0644826Z Jun 29 19:14:38 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0645787Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0646718Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0647709Z Jun 29 19:14:38 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_7fea9212-b1dc-4182-8bf4-548cb601f5cb.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0648410Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_332]
2022-06-29T19:14:39.0648988Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_332]
2022-06-29T19:14:39.0649573Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_332]
2022-06-29T19:14:39.0650154Z Jun 29 19:14:38 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_332]
2022-06-29T19:14:39.0650700Z Jun 29 19:14:38 Caused by: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0651335Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0652104Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkAsyncErrorsAndRequests(ElasticsearchSinkBase.java:432) ~[?:?]
2022-06-29T19:14:39.0652859Z Jun 29 19:14:38 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:329) ~[?:?]
2022-06-29T19:14:39.0653852Z Jun 29 19:14:38 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.processElement(SinkOperator.java:65) ~[flink-table_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0654973Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0656147Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0657309Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0658450Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0659486Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:495) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0660536Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0661583Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:806) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0662547Z Jun 29 19:14:38 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0663531Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0664492Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0665410Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0666300Z Jun 29 19:14:38 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0666878Z Jun 29 19:14:38 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0667356Z Jun 29 19:14:38 	Suppressed: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
2022-06-29T19:14:39.0668101Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427) ~[?:?]
2022-06-29T19:14:39.0668829Z Jun 29 19:14:38 		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.close(ElasticsearchSinkBase.java:366) ~[?:?]
2022-06-29T19:14:39.0669807Z Jun 29 19:14:38 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0670882Z Jun 29 19:14:38 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0671966Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0673057Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0674126Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1032) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0675169Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1018) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0676176Z Jun 29 19:14:38 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:925) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0677190Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0678175Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0679223Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0680132Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0681028Z Jun 29 19:14:38 		at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
2022-06-29T19:14:39.0681602Z Jun 29 19:14:38 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_332]
2022-06-29T19:14:39.0682580Z Jun 29 19:14:38 	Caused by: org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException: Elasticsearch exception [type=cluster_block_exception, reason=index [my_users] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];]
2022-06-29T19:14:39.0683468Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:496) ~[?:?]
2022-06-29T19:14:39.0684252Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:407) ~[?:?]
2022-06-29T19:14:39.0685017Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:138) ~[?:?]
2022-06-29T19:14:39.0685770Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:196) ~[?:?]
2022-06-29T19:14:39.0686513Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793) ~[?:?]
2022-06-29T19:14:39.0687329Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$10(RestHighLevelClient.java:1581) ~[?:?]
2022-06-29T19:14:39.0688293Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1663) ~[?:?]
2022-06-29T19:14:39.0689078Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:590) ~[?:?]
2022-06-29T19:14:39.0689831Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:333) ~[?:?]
2022-06-29T19:14:39.0690515Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.elasticsearch.client.RestClient$1.completed(RestClient.java:327) ~[?:?]
2022-06-29T19:14:39.0691211Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) ~[?:?]
2022-06-29T19:14:39.0692018Z Jun 29 19:14:38 		at org.apache.flink.elasticsearch7.shaded.org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) ~[?:?]
2022-06-29T19:14:39.0693088Z Jun 29 19:14:38 2022-06-29 19:14:09,618 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - --------------------------------------------------------------------------------
2022-06-29T19:14:39.0693967Z Jun 29 19:14:38 2022-06-29 19:14:09,620 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] -  Preconfiguration: 
2022-06-29T19:14:39.0694727Z Jun 29 19:14:38 2022-06-29 19:14:09,620 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37377&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28355,FLINK-28425,FLINK-28680,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 20 06:21:41 UTC 2022,,,,,,,,,,"0|z16hao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 07:24;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=2a8cc459-df7a-5e6f-12bf-96efcc369aa9&l=20541;;;","30/Jun/22 07:26;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=89ed5489-a970-5ff2-67f7-d7391de0165f&l=21631

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=cc5499f8-bdde-5157-0d76-b6528ecd808e&l=21262

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37383&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=75f4c82e-ad02-5844-81c9-d16399e3372d&l=21540;;;","04/Jul/22 11:10;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37536&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=20973

This is most likely caused because there's no more diskspace available, which causes Elasticsearch to go into read-only mode. Most likely caused by something like FLINK-28355;;;","05/Jul/22 08:30;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37603&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=21293;;;","06/Jul/22 08:03;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37690&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=20822;;;","06/Jul/22 08:26;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37722&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","13/Jul/22 02:29;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38058&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c;;;","20/Jul/22 06:21;hxbks2ks;I have merged the commit 6ab358e87525d41526648fd6e802179e9ceed2f3 in release-1.14 to clean up the python environment.

I will continue to pay attention to the CI to see whether this problem have been solved
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Rework Kubernetes tests to no longer use driver version ""none""",FLINK-28304,13469244,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,29/Jun/22 19:09,11/Nov/22 08:39,04/Jun/24 20:42,,1.14.6,1.15.3,1.16.0,,,,,,Deployment / Kubernetes,Tests,,,,,,0,,,,,"As a follow-up of FLINK-28269 we should reconsider moving away from the current implementation which uses driver version {{none}} to the default driver {{docker}}. That's currently also done for flink-kubernetes-operator. However, when trying that for Flink, the tests timed out. 

Even better would be to get completely rid of the Bash based Kubernetes tests, see FLINK-28277. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-29 19:09:37.0,,,,,,,,,,"0|z16h4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka SQL Connector loses data when restoring from a savepoint with a topic with empty partitions,FLINK-28303,13469231,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tanjialiang,rmetzger,rmetzger,29/Jun/22 17:27,24/Oct/23 18:14,04/Jun/24 20:42,23/Oct/23 18:30,1.14.4,1.15.4,1.16.2,1.17.1,kafka-3.0.0,kafka-3.0.1,kafka-3.1.0,,Connectors / Kafka,,,,,,,0,pull-request-available,,,,"Steps to reproduce:
- Set up a Kafka topic with 10 partitions
- produce records 0-9 into the topic
- take a savepoint and stop the job
- produce records 10-19 into the topic
- restore the job from the savepoint.

The job will be missing usually 2-4 records from 10-19.

My assumption is that if a partition never had data (which is likely with 10 partitions and 10 records), the savepoint will only contain offsets for partitions with data. 
While the job was offline (and we've written record 10-19 into the topic), all partitions got filled. Now, when Kafka comes online again, it will use the ""latest"" offset for those partitions, skipping some data.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 23 18:30:17 UTC 2023,,,,,,,,,,"0|z16h1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 17:29;sharonxr55;I posted this with logs in this thread: https://lists.apache.org/thread/wbppox04rrsqsb1m0wy2nrgwzjsrtf24;;;","29/Jun/22 17:43;tzulitai;[~rmetzger] [~sharonxr55] have you tried setting the ""auto.offset.reset"" property to ""earliest""? The default of that value is ""latest"".
That config dictates the position to start from for a partition when 1) no records have been consumed yet from that partition, and 2) when an attempted offset to read from is out of bounds or invalid.

If all the records are consistently present after changing that config to ""earliest"", then it should prove your theory.

If that is indeed the case, we should fix the Kafka connector so that as soon as a partition is discovered, even with the absence of any records we should already write it into state, probably with a special ""earliest"" value/marker offset of some sort. When we restore from state that has a partition still associated with that marker value, then we should always attempt to resume reading from the earliest position. I would be surprised if that isn't already the case though (have not looked at the code).;;;","30/Jun/22 08:26;martijnvisser;[~renqs] WDYT?;;;","30/Jun/22 10:19;tanjialiang;I found this bug in flink-1.14.2 too. When some partition is empty, this bug will be appear.

[~tzulitai] And I'm sorry to tell you the ""auto.offset.reset"" is invalid in 1.14, because of this bug：https://issues.apache.org/jira/browse/FLINK-24697 ;;;","30/Jun/22 13:12;rmetzger;[~tzulitai] Thanks a lot for your quick response. We haven't tried setting the ""auto.offset.reset"" property yet (and it seems not possible in 1.14 according to [~tanjialiang] comment). We actually experienced this issue in an end to end test (that's why we have such low data volumes, actually triggering the issue). I will look into the E2E test again soon.
I had the same idea fixing the issue as you are proposing. Let's see what [~renqs]'s take on the issue is.;;;","02/Jul/22 02:46;libenchao;Besides this case, I would like to mention that when the Kafka Cluster is unhealthy, e.g. some partitions are in under replica status, the problem also arises.
In our internal use cases, we suffers this a lot, there will be two cases:
1. if we do not enable cp/sp, when Flink job starts with 'group-offsets', we will omit the 'under replica' partition. when the partition recovers, we'll treat it as a new added partition, and we'll consume from earliest. This will lead to repeated consuming.
2. if we enabled cp/sp, we'll use the offsets stored in state, however, it's not consumable for now. When the partition recovers, we'll again add it as a new partition, and consume this partition twice. This will also lead to repeated consuming.

PS: We are using 1.11 now, I didn't checked the master's code whether this still exist.;;;","04/Jul/22 07:43;martijnvisser;[~libenchao] There have been quite some changes between 1.11 and the current versions (including a migration from FlinkKafkaSource to KafkaSource and a Kafka version bump). ;;;","04/Jul/22 13:05;libenchao;[~martijnvisser] Yes, it's a quite old version. I just want to remind the one who will take this issue to take this into consideration if possible.;;;","25/Sep/23 11:00;tanjialiang;Maybe this this is the reason? [FLINK-33153|https://issues.apache.org/jira/browse/FLINK-33153];;;","25/Sep/23 12:58;martijnvisser;[~tanjialiang] It would be good to double check this in the latest Kafka connector code, perhaps it's already addressed since the work was done on https://cwiki.apache.org/confluence/display/FLINK/FLIP-288%3A+Enable+Dynamic+Partition+Discovery+by+Default+in+Kafka+Source;;;","26/Sep/23 05:45;tanjialiang;[~martijnvisser] I had already check the latest kafka connector code, this problem still exists.;;;","26/Sep/23 08:25;tanjialiang;Maybe we should implement the LatestOffsetsInitializer to look up the end offset and pass it to the reader, instead of pass KafkaPartitionSplit#LATEST_OFFSET(-1).;;;","05/Oct/23 04:02;tzulitai;Making this a blocker as it's confirmed to be a real issue data loss issue with the newer {{{}KafkaSource{}}}.;;;","23/Oct/23 18:30;tzulitai;Merged.

apache/flink-connector-kafka:main - 54e3b70deb349538edba1ec2b051ed9d9f79b563
apache/flink-connector-kafka:v3.0 538e9c10463dbdf0942c8858678e98bf3522d566;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve documentation on how a user should migrate from FlinkKafkaProducer to KafkaSink,FLINK-28302,13469120,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,martijnvisser,martijnvisser,martijnvisser,29/Jun/22 14:07,11/Nov/22 12:49,04/Jun/24 20:42,,1.14.6,1.15.3,1.16.0,,,,,,Connectors / Kafka,,,,,,,0,,,,,We have documented the migration process from FlinkKafkaProducer to KafkaSink in the release notes of 1.14 https://nightlies.apache.org/flink/flink-docs-release-1.14/release-notes/flink-1.14/#port-kafkasink-to-new-unified-sink-api-flip-143 and it's missed often by users. We should just include this in the overall documentation of the Kafka connector ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29999,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-29 14:07:32.0,,,,,,,,,,"0|z16gvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When will flink-kubernetes-operator support pyflink,FLINK-28301,13469116,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,1053032898@qq.com,1053032898@qq.com,29/Jun/22 13:46,29/Jun/22 14:07,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-29 13:46:55.0,,,,,,,,,,"0|z16guo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Helm/Helmfile lint fails with ""invalid Yaml document separator""",FLINK-28300,13469106,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,maksimaniskov,maksimaniskov,maksimaniskov,29/Jun/22 13:03,07/Jul/22 14:48,04/Jun/24 20:42,07/Jul/22 14:48,kubernetes-operator-1.0.0,kubernetes-operator-1.0.1,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"helmfile lint output:
{quote}{{STDERR:}}
{{    Error: 1 chart(s) linted, 1 chart(s) failed}}
{{  }}
{{  COMBINED OUTPUT:}}
{{    Error: 1 chart(s) linted, 1 chart(s) failed}}
{{    ==> Linting /var/folders/yb/r7jlgg613z5dxhpf83zxbxlh0000gn/T/helmfile1348989835/flink-operator-jobs/namespaces/apache-flink-kubernetes-operator/flink-kubernetes-operator/latest/flink-kubernetes-operator}}
{{    [INFO] Chart.yaml: icon is recommended}}
{{    [ERROR] templates/serviceaccount.yaml: unable to parse YAML: invalid Yaml document separator: apiVersion: v1}}
{quote}
 

Problematic is trimming whitespaces, including the line feeds, in [helm/flink-kubernetes-operator/templates/serviceaccount.yaml|https://github.com/apache/flink-kubernetes-operator/blob/97883eb8f586ccd703952efc59a43f019ef83fa7/helm/flink-kubernetes-operator/templates/serviceaccount.yaml#L20]
{quote}{{---}}
{{ {{{}- if .Values.operatorServiceAccount.create -{}}}}}
{{apiVersion: v1}}{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 14:48:45 UTC 2022,,,,,,,,,,"0|z16gsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 14:02;maksimaniskov;Here is fix for it [https://github.com/MaksimAniskov/flink-kubernetes-operator/commit/565050a9b050e6fb72545e77feab48bf267a6701|https://github.com/MaksimAniskov/flink-kubernetes-operator/commit/565050a9b050e6fb72545e77feab48bf267a6701];;;","29/Jun/22 15:14;gyfora;Hi [~maksimaniskov] , thank you for reporting this issue. Could you please open a PR to the apache repo?;;;","30/Jun/22 06:42;maksimaniskov;https://github.com/apache/flink-kubernetes-operator/pull/286;;;","07/Jul/22 14:48;mbalassi;[4c8875a|https://github.com/apache/flink-kubernetes-operator/commit/4c8875aba1104ab376120829099aa1f3f092a7dd] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get rid of Sink v2,FLINK-28299,13469105,13450148,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,29/Jun/22 13:02,01/Jul/22 01:15,04/Jun/24 20:42,01/Jul/22 01:15,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,Sink v2 makes hard to be compatible with Flink 1.14.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 01:15:54 UTC 2022,,,,,,,,,,"0|z16gs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 01:15;lzljs3620320;master: e5a37fd07aa7996b037f88e48b1a19ba5b6ab89d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support left and right built-in function in the Table API,FLINK-28298,13469104,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,29/Jun/22 13:00,04/Jul/22 02:15,04/Jun/24 20:42,04/Jul/22 02:15,1.15.0,,,,,1.16.0,,,API / Python,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 02:15:10 UTC 2022,,,,,,,,,,"0|z16gs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 02:15;dianfu;Merged to master via da720c3b97d5c3e781abb1e910a8337f6a5ee0e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect metric group order for namespaced operator metrics,FLINK-28297,13469069,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,29/Jun/22 09:45,08/Jul/22 12:41,04/Jun/24 20:42,08/Jul/22 12:41,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The metric group for the resource namespace should follow the main metric group for the metric itself.

So instead of 
{noformat}
flink-kubernetes-operator-64d8cc77c4-w49nj.k8soperator.default.flink-kubernetes-operator.resourcens.default.FlinkDeployment.READY.Count: 1
{noformat}
we should have
{noformat}
flink-kubernetes-operator-64d8cc77c4-w49nj.k8soperator.default.flink-kubernetes-operator.FlinkDeployment.READY.resourcens.default.Count: 1{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 12:41:24 UTC 2022,,,,,,,,,,"0|z16gk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 05:11;gyfora;On a second thought we might simply want to get the resourcens tagname and keep the ns always front. Let's discuss offiline first;;;","08/Jul/22 12:41;gyfora;merged to main 1b3acc23868f9ad09ea73e7556a7b31176496783;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Hive's UDAF which implement GenericUDAFResolver,FLINK-28296,13469056,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,29/Jun/22 08:40,05/Aug/22 13:17,04/Jun/24 20:42,05/Aug/22 13:17,,,,,,1.16.0,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"Some Hive's udaf implementt GenericUDAFResolver, but Flink only support the function implement UDAF and GenericUDAFResolver2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 05 13:17:07 UTC 2022,,,,,,,,,,"0|z16ghc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 13:17;jark;Fixed in master: f9bd7d2f553174c2ae25da63e3d5be156649cf27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error deserializing kafka records,FLINK-28295,13469047,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Later,,pedromazala,pedromazala,29/Jun/22 07:30,29/Jun/22 14:43,04/Jun/24 20:42,29/Jun/22 09:57,1.15.0,,,,,,,,API / DataStream,Connectors / Kafka,,,,,,0,,,,,"*It only happens if I'm using newer java versions than 11*

It is not possible to deserialize Kafka messages using `KafkaDeserializationSchema`. An exception is thrown (`java.lang.reflect.InaccessibleObjectException`) when forwarding (`out.collect(deserialized)`) the deserialized object.

It happens on `org.apache.flink.runtime.io.network.api.writer.RecordWriter#serializeRecord` at `record.write(serializer)`. It seems it is not being possible to perform the write of `DataOutputSerializer` because the exception happens on this object and not on my domain object for deserialization.

--------

*Build.gradle*

{code:groovy}
ext {
    flinkVersion = ""1.15.0""
}

dependencies {
    implementation ""org.apache.flink:flink-core:${flinkVersion}""
    implementation ""org.apache.flink:flink-clients:${flinkVersion}""
    implementation ""org.apache.flink:flink-avro:${flinkVersion}""

    implementation ""org.apache.flink:flink-connector-kafka:${flinkVersion}""

    implementation ""org.apache.flink:flink-connector-elasticsearch7:${flinkVersion}""
}
{code}

*Deserializer*

{code:java}
class ApicurioKafkaDeserializationSchema implements KafkaDeserializationSchema<MyEvent> {
        private MyDeserializer deserializer;

        @Override
        public void open(DeserializationSchema.InitializationContext context) throws Exception {
            KafkaDeserializationSchema.super.open(context);

            deserializer = new MyDeserializer();
        }

        @Override
        public boolean isEndOfStream(MyEvent nextElement) {
            return false;
        }

        @Override
        public MyEvent deserialize(ConsumerRecord<byte[], byte[]> record) {
            String key = new String(record.key());
            String value = deserializer.deserialize(record.value());

            return MyEvent.of(key, value);
        }

        @Override
        public TypeInformation<MyEvent> getProducedType() {
            return Types.GENERIC(MyEvent.class);
        }
    }
{code}

*Stacktrace*

{code:java}
java.io.IOException: Failed to deserialize consumer record due to
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:56)
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:33)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:143)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.io.IOException: Failed to deserialize consumer record ConsumerRecord(topic = customer.cdc.snapshot.aggregate, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1656327039514, serialized key size = 39, serialized value size = 268, headers = RecordHeaders(headers = [], isReadOnly = false), key = [B@758cd630, value = [B@1cfd2da5).
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:57)
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter.emitRecord(KafkaRecordEmitter.java:53)
	... 14 common frames omitted
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:115)
	at org.apache.flink.connector.kafka.source.reader.KafkaRecordEmitter$SourceOutputWrapper.collect(KafkaRecordEmitter.java:67)
	at org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema.deserialize(KafkaDeserializationSchema.java:81)
	at org.apache.flink.connector.kafka.source.reader.deserializer.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:54)
	... 15 common frames omitted
Caused by: java.lang.RuntimeException: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @1e92bd61
	at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:69)
	at org.apache.flink.api.java.typeutils.runtime.kryo.FlinkChillPackageRegistrar.registerSerializers(FlinkChillPackageRegistrar.java:67)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance(KryoSerializer.java:512)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized(KryoSerializer.java:521)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(KryoSerializer.java:347)
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:168)
	at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:46)
	at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:132)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:106)
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:54)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:104)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:77)
	at org.apache.flink.streaming.runtime.tasks.BroadcastingOutputCollector.collect(BroadcastingOutputCollector.java:32)
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:313)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)
	... 18 common frames omitted
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @1e92bd61
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:67)
	... 35 common frames omitted
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15736,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 29 14:43:21 UTC 2022,,,,,,,,,,"0|z16gfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 09:57;martijnvisser;[~pedromazala] Thanks for opening the ticket, but Flink currently doesn't support a JDK newer than 11. This is something that will be picked up when working on the Java 17 support FLINK-15736;;;","29/Jun/22 14:43;pedromazala;Thank you for the reply [~martijnvisser].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some metric reporters don't allow register same metric with different names,FLINK-28294,13469033,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xuannan,xuannan,29/Jun/22 06:50,23/Feb/23 08:54,04/Jun/24 20:42,23/Feb/23 08:54,1.15.0,,,,,,,,Runtime / Metrics,,,,,,,1,,,,,"Currently, some metric reporters keep an internal Map keyed by the metric object to keep track of the registered metric. The problem with the map with metric object as the key is that when the same metric object is registered with different names or metric groups, only the last registered metric will be reported. 

If I understand correctly, we do not forbid registering the same metric with different names or metric groups. For example, in `SchedulerBase#registerJobMetrics`, we register `numberOfRestarts` with two names, ""numRestarts"" and ""fullRestarts"". Unfortunately, in this case, the metric reporter will only report the fullRestarts metric, which is deprecated.

I found that the following metric reporters have the problem, Influxdb metric reporter, Datadog metric reporter, and Dropwizard metric reporter.  

One possible fix is to swap the key and value of the internal map in the metric reporter. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Feb 23 08:54:12 UTC 2023,,,,,,,,,,"0|z16gc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 08:54;zhuzh;It is a duplicate issue of FLINK-30246.
The specific problem of `fullRestarts`&`numRestarts` is fixed via FLINK-30558.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayServiceITCase.testCloseOperationAndFetchResultInParallel failed,FLINK-28293,13469022,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hxbks2ks,hxbks2ks,29/Jun/22 06:14,29/Jun/22 06:20,04/Jun/24 20:42,29/Jun/22 06:20,1.16.0,,,,,,,,Table SQL / Gateway,,,,,,,0,test-stability,,,,"
{code:java}
2022-06-29T03:17:48.4575178Z Jun 29 03:17:48 [INFO] Running org.apache.flink.table.gateway.service.SqlGatewayServiceITCase
2022-06-29T03:17:49.1369728Z Exception in thread ""Thread-137"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1370535Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1371467Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1372442Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1373501Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 19fe6b8c-ec82-48ec-8bce-251b71d62fcd.
2022-06-29T03:17:49.1378602Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1379559Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1380241Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1380942Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1381653Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1382139Z 	... 2 more
2022-06-29T03:17:49.1382966Z Exception in thread ""Thread-139"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1383651Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1384444Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1385081Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1385981Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 34e57e3f-5498-4c7c-b614-c942e5d2883a.
2022-06-29T03:17:49.1386782Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1387490Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1388184Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1388893Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1389596Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1390062Z 	... 2 more
2022-06-29T03:17:49.1390720Z Exception in thread ""Thread-187"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1391400Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1392190Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1392831Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1393724Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 1e8d017a-924e-4d44-bf12-910668400ab7.
2022-06-29T03:17:49.1394502Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1395215Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1396206Z Exception in thread ""Thread-185"" 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1396988Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1397696Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1398175Z 	... 2 more
2022-06-29T03:17:49.1398825Z Exception in thread ""Thread-189"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1399594Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1400377Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1401077Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1402059Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 41bac683-8c98-4ec4-b551-fbe4166f0c26.
2022-06-29T03:17:49.1402984Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1403693Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1404385Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1405086Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1405776Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1406260Z 	... 2 more
2022-06-29T03:17:49.1453180Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1453871Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1454686Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1455329Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1456434Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the ca5b2be5-60af-4162-87d9-68f010bfa816.
2022-06-29T03:17:49.1457255Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1457955Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1458651Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1459352Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1460059Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1460542Z 	... 2 more
2022-06-29T03:17:49.1461211Z Exception in thread ""Thread-213"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1461903Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1462678Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1463315Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1464207Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 12b429f9-bd9c-4ad3-841c-3916ee14261e.
2022-06-29T03:17:49.1465005Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1465715Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1466402Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1467297Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1467989Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1468464Z 	... 2 more
2022-06-29T03:17:49.1469245Z Exception in thread ""Thread-215"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1469931Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1470720Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1471359Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1472232Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 4242a9c9-da7b-4fb5-8986-d5178f1b2a28.
2022-06-29T03:17:49.1473032Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1473734Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1474422Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1475122Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1475826Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1476301Z 	... 2 more
2022-06-29T03:17:49.1476945Z Exception in thread ""Thread-271"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1477625Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1478422Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1479058Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1479945Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the a4e5326c-d342-42e1-8b14-1a21a70a6a81.
2022-06-29T03:17:49.1480743Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1481450Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1482113Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1482949Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1483661Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1484139Z 	... 2 more
2022-06-29T03:17:49.1484808Z Exception in thread ""Thread-269"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1485494Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1486277Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1486894Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1487781Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 48677a5c-7fec-4de9-8b63-876a334cd2f4.
2022-06-29T03:17:49.1488669Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1489369Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1490050Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1490812Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1491514Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1491976Z 	... 2 more
2022-06-29T03:17:49.1492642Z Exception in thread ""Thread-267"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1493324Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1494116Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1494756Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1495637Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 506cc4be-d989-453d-a100-526421e37b6c.
2022-06-29T03:17:49.1496421Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1497123Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1497805Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1498509Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1499222Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1499697Z 	... 2 more
2022-06-29T03:17:49.1500354Z Exception in thread ""Thread-265"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1501023Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1501809Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1502443Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1503323Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 0050ce01-9705-4602-b884-e346b46e97b9.
2022-06-29T03:17:49.1504115Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1504826Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1505516Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1506203Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1506904Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1507381Z 	... 2 more
2022-06-29T03:17:49.1508037Z Exception in thread ""Thread-275"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1508720Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1509509Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1510230Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1511109Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the ce15fbec-050a-4ace-ace0-2e3166571cd9.
2022-06-29T03:17:49.1511975Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1512690Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1513375Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1514075Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1514779Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1515259Z 	... 2 more
2022-06-29T03:17:49.1515911Z Exception in thread ""Thread-259"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1516589Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1517380Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1518016Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1518899Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 9b2d548e-411b-4297-8dae-534f6347ebe9.
2022-06-29T03:17:49.1519694Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1520387Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1521076Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1521774Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1522575Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1523059Z 	... 2 more
2022-06-29T03:17:49.1523724Z Exception in thread ""Thread-255"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1553817Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1554939Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1555634Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1556755Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the aae0e620-9c3a-4f4b-a610-bd29e1143171.
2022-06-29T03:17:49.1557556Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1558279Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1558970Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1559674Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1560362Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1561082Z 	... 2 more
2022-06-29T03:17:49.1561770Z Exception in thread ""Thread-277"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1562609Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1563409Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1564157Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1589568Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 2388ea56-5355-4adc-b6d2-8e051953c660.
2022-06-29T03:17:49.1590907Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1591845Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1595842Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1596874Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1607783Z Exception in thread ""Thread-253"" Exception in thread ""Thread-251"" Exception in thread ""Thread-247"" 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1608700Z 	... 2 more
2022-06-29T03:17:49.1609203Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1609831Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1610667Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1616263Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1617359Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the ac7b7650-adeb-48d4-af57-a73388d230d0.
2022-06-29T03:17:49.1618211Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1618971Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1622817Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1625112Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1625921Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1626392Z 	... 2 more
2022-06-29T03:17:49.1630695Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1631366Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1632462Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1633138Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1638815Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 6dbe48e0-1f6b-45c6-9ade-9486b9f90793.
2022-06-29T03:17:49.1639791Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1640501Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1641497Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1642237Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1648224Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1649002Z 	... 2 more
2022-06-29T03:17:49.1665021Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to cancelOperation.
2022-06-29T03:17:49.1665740Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:99)
2022-06-29T03:17:49.1666537Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testCancelAndCloseOperationInParallel$12(SqlGatewayServiceITCase.java:303)
2022-06-29T03:17:49.1671277Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.1672431Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the cc9faf3d-90bd-484a-8f06-d4d7ad6984c8.
2022-06-29T03:17:49.1673295Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:168)
2022-06-29T03:17:49.1674041Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:194)
2022-06-29T03:17:49.1679948Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:163)
2022-06-29T03:17:49.1680702Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.cancelOperation(OperationManager.java:100)
2022-06-29T03:17:49.1681398Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.cancelOperation(SqlGatewayServiceImpl.java:96)
2022-06-29T03:17:49.1681917Z 	... 2 more
2022-06-29T03:17:49.2997089Z Exception in thread ""Thread-565"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3002691Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3003491Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3004392Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3008009Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3008725Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3009412Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3010146Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3010886Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3015003Z 	... 3 more
2022-06-29T03:17:49.3016004Z Exception in thread ""Thread-422"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3016738Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3017490Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3021430Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3022216Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3022747Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3023419Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3024441Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3028138Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3028889Z 	... 3 more
2022-06-29T03:17:49.3029721Z Exception in thread ""Thread-556"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3030450Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3031308Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3036058Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3036944Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3037475Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3038145Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3042259Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3065166Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3065711Z 	... 3 more
2022-06-29T03:17:49.3066637Z Exception in thread ""Thread-423"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3067366Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3070935Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3071930Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3072645Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3073174Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3073844Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3077036Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3077818Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3078331Z 	... 3 more
2022-06-29T03:17:49.3079334Z Exception in thread ""Thread-424"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3080074Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3083292Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3084342Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3087675Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3088242Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3091225Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3091957Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3093007Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3096370Z 	... 3 more
2022-06-29T03:17:49.3097256Z Exception in thread ""Thread-425"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3106376Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3107205Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3108055Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3108762Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3109283Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3114363Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3115138Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3115898Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3116483Z 	... 3 more
2022-06-29T03:17:49.3117293Z Exception in thread ""Thread-426"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3121757Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3123165Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3124072Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3124799Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3128273Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3129048Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3129792Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3130536Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3131051Z 	... 3 more
2022-06-29T03:17:49.3136888Z Exception in thread ""Thread-427"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3137669Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3138464Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3139319Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3140036Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3144464Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3147404Z Exception in thread ""Thread-432"" 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3151171Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3151966Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3159771Z 	... 3 more
2022-06-29T03:17:49.3160709Z Exception in thread ""Thread-434"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3161456Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3166785Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3167719Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3172571Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3173151Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3173783Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3174540Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3175287Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3179588Z 	... 3 more
2022-06-29T03:17:49.3180483Z Exception in thread ""Thread-435"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3181207Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3181979Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3182823Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3186773Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3187423Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3188097Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3188834Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3192890Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3193537Z 	... 3 more
2022-06-29T03:17:49.3194349Z Exception in thread ""Thread-437"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3195077Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3195853Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3200130Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3200895Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3201433Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3202059Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3202954Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3207494Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3208033Z 	... 3 more
2022-06-29T03:17:49.3208880Z Exception in thread ""Thread-438"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3209845Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3267386Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3268712Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3269446Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3272818Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3273525Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3274391Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3329125Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3329861Z 	... 3 more
2022-06-29T03:17:49.3330938Z Exception in thread ""Thread-430"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3331774Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3340695Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3341650Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3342371Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3342898Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3343661Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3348121Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3348875Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3349406Z 	... 3 more
2022-06-29T03:17:49.3350342Z Exception in thread ""Thread-431"" org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3355051Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3355857Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3356708Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3365378Z Exception in thread ""Thread-474"" Exception in thread ""Thread-436"" Exception in thread ""Thread-476"" Exception in thread ""Thread-520"" Exception in thread ""Thread-522"" Exception in thread ""Thread-523"" Exception in thread ""Thread-524"" Exception in thread ""Thread-526"" Exception in thread ""Thread-525"" Exception in thread ""Thread-527"" Exception in thread ""Thread-529"" Exception in thread ""Thread-533"" Exception in thread ""Thread-534"" Exception in thread ""Thread-535"" Exception in thread ""Thread-536"" Exception in thread ""Thread-537"" Exception in thread ""Thread-538"" Exception in thread ""Thread-539"" Exception in thread ""Thread-540"" Exception in thread ""Thread-542"" Exception in thread ""Thread-541"" Exception in thread ""Thread-543"" Exception in thread ""Thread-544"" Exception in thread ""Thread-545"" Exception in thread ""Thread-546"" 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3371930Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3375204Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3375991Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3380046Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3380637Z 	... 3 more
2022-06-29T03:17:49.3381074Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3381734Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3382509Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3386515Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3387290Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3387819Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3388446Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3389183Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3392828Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3393373Z 	... 3 more
2022-06-29T03:17:49.3393803Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3394462Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3398579Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3399567Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3400303Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3400827Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3401571Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3405221Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3406011Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3406530Z 	... 3 more
2022-06-29T03:17:49.3406930Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3407585Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3411644Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3412592Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3413300Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3413823Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3414445Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3418031Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3419012Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3419530Z 	... 3 more
2022-06-29T03:17:49.3419957Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3426826Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3427730Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3428646Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3429361Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3429895Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3433423Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3434190Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3434935Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3435418Z 	... 3 more
2022-06-29T03:17:49.3435841Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3440016Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3440817Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3441711Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3442564Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3443047Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3447172Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3447936Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3448675Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3449190Z 	... 3 more
2022-06-29T03:17:49.3453500Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3454254Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3455038Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3455944Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3456717Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3460706Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3461423Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3462155Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3462852Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3463369Z 	... 3 more
2022-06-29T03:17:49.3466946Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3467856Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3468626Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3469617Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3474051Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3474674Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3475360Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3476098Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3476852Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3481471Z 	... 3 more
2022-06-29T03:17:49.3481956Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3482727Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3483534Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3484661Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3488369Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3488927Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3489618Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3490349Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3491040Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3495450Z 	... 3 more
2022-06-29T03:17:49.3495906Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3496568Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3497341Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3502462Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3503292Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3503832Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3504509Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3505248Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3509371Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3509938Z 	... 3 more
2022-06-29T03:17:49.3510322Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3510982Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3511813Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3516325Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3517073Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3517808Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.3518476Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.3519162Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.3523568Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.3524104Z 	... 3 more
2022-06-29T03:17:49.3524535Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.3525209Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.3528497Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.3529457Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.3530181Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.3530706Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4197903Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4198765Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4199527Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4199991Z 	... 3 more
2022-06-29T03:17:49.4200392Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4201018Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4201826Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4202833Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4203508Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4203982Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4204619Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4205324Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4206032Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4206519Z 	... 3 more
2022-06-29T03:17:49.4206911Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4207538Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4208255Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4209112Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4210114Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4210608Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4211242Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4212047Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4212755Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4213219Z 	... 3 more
2022-06-29T03:17:49.4213613Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4214291Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4215023Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4215891Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4216813Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4217288Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4217929Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4218623Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4219326Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4219810Z 	... 3 more
2022-06-29T03:17:49.4220203Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4220807Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4221534Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4222383Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4223184Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4223672Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4224303Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4224997Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4225682Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4226171Z 	... 3 more
2022-06-29T03:17:49.4226564Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4227182Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4227914Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4228768Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4229439Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4229909Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4230537Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4231361Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4232061Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4232609Z 	... 3 more
2022-06-29T03:17:49.4232999Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4233601Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4234330Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4235181Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4235854Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4236347Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4236982Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4237657Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4238363Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4238842Z 	... 3 more
2022-06-29T03:17:49.4239233Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4239851Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4240576Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4241430Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4242081Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4242688Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4243441Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4244146Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4244851Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4245330Z 	... 3 more
2022-06-29T03:17:49.4245706Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4246321Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4247046Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4247896Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4248578Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4249063Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4249696Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4250372Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4251075Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4251738Z 	... 3 more
2022-06-29T03:17:49.4252165Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4252783Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4253610Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4254449Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4255120Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4255607Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4256239Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4257006Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4257708Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4258187Z 	... 3 more
2022-06-29T03:17:49.4258572Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4259188Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4259919Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4261076Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4261757Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4262251Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4262871Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4263732Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4264443Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4264923Z 	... 3 more
2022-06-29T03:17:49.4265306Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4265915Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4266630Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4267485Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4268161Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4268648Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4269463Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4270166Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4270866Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4271328Z 	... 3 more
2022-06-29T03:17:49.4271722Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4272341Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4273377Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4274234Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4274964Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4275772Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4276436Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4277131Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4277958Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4278444Z 	... 3 more
2022-06-29T03:17:49.4278838Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4279460Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4280319Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4281224Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4281925Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4282577Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4283223Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4284002Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4284718Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4285180Z 	... 3 more
2022-06-29T03:17:49.4285573Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4286192Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4286920Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4287773Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4288443Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4288915Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4289576Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4290269Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4290978Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4291459Z 	... 3 more
2022-06-29T03:17:49.4291853Z org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to submitOperation.
2022-06-29T03:17:49.4292457Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:89)
2022-06-29T03:17:49.4293190Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.submitDefaultOperation(SqlGatewayServiceITCase.java:392)
2022-06-29T03:17:49.4294041Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.lambda$testSubmitOperationAndCloseOperationManagerInParallel$16(SqlGatewayServiceITCase.java:329)
2022-06-29T03:17:49.4294900Z 	at java.lang.Thread.run(Thread.java:748)
2022-06-29T03:17:49.4295390Z Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: The OperationManager is closed.
2022-06-29T03:17:49.4296145Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.writeLock(OperationManager.java:180)
2022-06-29T03:17:49.4296840Z 	at org.apache.flink.table.gateway.service.operation.OperationManager.submitOperation(OperationManager.java:86)
2022-06-29T03:17:49.4297528Z 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.submitOperation(SqlGatewayServiceImpl.java:86)
2022-06-29T03:17:49.4298004Z 	... 3 more
2022-06-29T03:17:49.4299408Z Jun 29 03:17:49 [ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.91 s <<< FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceITCase
2022-06-29T03:17:49.4300229Z Jun 29 03:17:49 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCloseOperationAndFetchResultInParallel  Time elapsed: 0.021 s  <<< FAILURE!
2022-06-29T03:17:49.4300829Z Jun 29 03:17:49 java.lang.AssertionError: 
2022-06-29T03:17:49.4301161Z Jun 29 03:17:49 
2022-06-29T03:17:49.4301511Z Jun 29 03:17:49 Expecting code to raise a throwable.
2022-06-29T03:17:49.4302180Z Jun 29 03:17:49 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.runCancelOrCloseOperationWhenFetchResults(SqlGatewayServiceITCase.java:435)
2022-06-29T03:17:49.4303083Z Jun 29 03:17:49 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCloseOperationAndFetchResultInParallel(SqlGatewayServiceITCase.java:272)
2022-06-29T03:17:49.4303804Z Jun 29 03:17:49 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-29T03:17:49.4304387Z Jun 29 03:17:49 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-29T03:17:49.4305052Z Jun 29 03:17:49 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-29T03:17:49.4305654Z Jun 29 03:17:49 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-29T03:17:49.4306255Z Jun 29 03:17:49 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-06-29T03:17:49.4306927Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-06-29T03:17:49.4307687Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-06-29T03:17:49.4308464Z Jun 29 03:17:49 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-06-29T03:17:49.4309175Z Jun 29 03:17:49 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-06-29T03:17:49.4309911Z Jun 29 03:17:49 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-06-29T03:17:49.4310710Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-06-29T03:17:49.4311509Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-06-29T03:17:49.4312305Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-06-29T03:17:49.4313118Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-06-29T03:17:49.4313887Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-06-29T03:17:49.4314918Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-06-29T03:17:49.4315763Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-06-29T03:17:49.4316446Z Jun 29 03:17:49 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-06-29T03:17:49.4317202Z Jun 29 03:17:49 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-06-29T03:17:49.4318193Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-29T03:17:49.4318969Z Jun 29 03:17:49 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-06-29T03:17:49.4319731Z Jun 29 03:17:49 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-06-29T03:17:49.4320478Z Jun 29 03:17:49 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-06-29T03:17:49.4321248Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-06-29T03:17:49.4322016Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-29T03:17:49.4322903Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-29T03:17:49.4323610Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-29T03:17:49.4324310Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-29T03:17:49.4325053Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-29T03:17:49.4325930Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-29T03:17:49.4326758Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-29T03:17:49.4327628Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-29T03:17:49.4328682Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-06-29T03:17:49.4329699Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-06-29T03:17:49.4330578Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-29T03:17:49.4331465Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-29T03:17:49.4332224Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-29T03:17:49.4332914Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-29T03:17:49.4333623Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-29T03:17:49.4334390Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-29T03:17:49.4335131Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-29T03:17:49.4335852Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-29T03:17:49.4336951Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-29T03:17:49.4338086Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-06-29T03:17:49.4339173Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-29T03:17:49.4339923Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-29T03:17:49.4340782Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-29T03:17:49.4341485Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-29T03:17:49.4342190Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-29T03:17:49.4342950Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-29T03:17:49.4343693Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-29T03:17:49.4344410Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-29T03:17:49.4345253Z Jun 29 03:17:49 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-29T03:17:49.4346075Z Jun 29 03:17:49 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-06-29T03:17:49.4346667Z Jun 29 03:17:49 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-29T03:17:49.4347279Z Jun 29 03:17:49 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-29T03:17:49.4347893Z Jun 29 03:17:49 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-29T03:17:49.4348506Z Jun 29 03:17:49 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-29T03:17:49.4348965Z Jun 29 03:17:49 
2022-06-29T03:17:49.7164363Z Jun 29 03:17:49 [INFO] 
2022-06-29T03:17:49.7164814Z Jun 29 03:17:49 [INFO] Results:
2022-06-29T03:17:49.7165149Z Jun 29 03:17:49 [INFO] 
2022-06-29T03:17:49.7165479Z Jun 29 03:17:49 [ERROR] Failures: 
2022-06-29T03:17:49.7166802Z Jun 29 03:17:49 [ERROR]   SqlGatewayServiceITCase.testCloseOperationAndFetchResultInParallel:272->runCancelOrCloseOperationWhenFetchResults:435 
2022-06-29T03:17:49.7167451Z Jun 29 03:17:49 Expecting code to raise a throwable.
2022-06-29T03:17:49.7167815Z Jun 29 03:17:49 [INFO] 
2022-06-29T03:17:49.7168204Z Jun 29 03:17:49 [ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37332&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 29 06:15:31 UTC 2022,,,,,,,,,,"0|z16g9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 06:15;hxbks2ks;cc [~fsk119];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get rid of BulkReaderFormatFactory and BulkWriterFormatFactory,FLINK-28292,13469018,13450148,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,29/Jun/22 05:48,30/Jun/22 06:42,04/Jun/24 20:42,30/Jun/22 06:42,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,These APIs were changed significantly in 1.14 and 1.15 and are not compatible,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 06:42:10 UTC 2022,,,,,,,,,,"0|z16g8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 06:42;lzljs3620320;master: c8c3cd1d93012d6eba6a7f7639435d489e0cffd8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add kerberos delegation token renewer feature instead of logged from keytab individually,FLINK-28291,13469005,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,jiulong.zhu1026,jiulong.zhu1026,29/Jun/22 02:52,18/Aug/23 22:35,04/Jun/24 20:42,,1.13.5,,,,,,,,Deployment / YARN,,,,,,,0,auto-deprioritized-minor,PatchAvailable,patch-available,,"h2. 1. Design

LifeCycle of delegation token in RM:
 # Container starts with DT given by client.
 # Enable delegation token renewer by:
 ## set {{security.kerberos.token.renew.enabled}} true, default false. And
 ## specify {{security.kerberos.login.keytab}} and {{security.kerberos.login.principal}}
 # When enabled delegation token renewer, the renewer thread will re-obtain tokens from DelegationTokenProvider(only HadoopFSDelegationTokenProvider now). Then the renewer thread will broadcast new tokens to RM locally, all JMs and all TMs by RPCGateway.
 # RM process adds new tokens in context by UserGroupInformation.

LifeCycle of delegation token in JM / TM:
 # TaskManager starts with keytab stored in remote hdfs.
 # When registered successfully, JM / TM get the current tokens of RM boxed by {{JobMasterRegistrationSuccess}} / {{{}TaskExecutorRegistrationSuccess{}}}.
 # JM / TM process add new tokens in context by UserGroupInformation.

It’s too heavy and unnecessary to retrieval leader of ResourceManager by HAService, so DelegationTokenManager is instanced by ResourceManager. So DelegationToken can hold the reference of ResourceManager, instead of RM RPCGateway or self gateway.
h2. 2. Test
 # No local junit test. It’s too heavy to build junit environments including KDC and local hadoop.

 # Cluster test

step 1: Specify krb5.conf with short token lifetime(ticket_lifetime, renew_lifetime) when submitting flink application.

```
{{flink run .... -yD security.kerberos.token.renew.enabled=true -yD security.kerberos.krb5-conf.path= /home/work/krb5.conf -yD security.kerberos.login.use-ticket-cache=false ...}}

```
step 2: Watch token identifier changelog and synchronizer between rm and worker.
>> 
In RM / JM log, 
2022-06-28 15:13:03,509 INFO org.apache.flink.runtime.util.HadoopUtils [] - New token (HDFS_DELEGATION_TOKEN token 52101 for work on ha-hdfs:newfyyy) created in KerberosDelegationToken, and next schedule delay is 64799880 ms. 
2022-06-28 15:13:03,529 INFO org.apache.flink.runtime.util.HadoopUtils [] - Updating delegation tokens for current user. 2022-06-28 15:13:04,729 INFO org.apache.flink.runtime.util.HadoopUtils [] - JobMaster receives new token (HDFS_DELEGATION_TOKEN token 52101 for work on ha-hdfs:newfyyy) from RM.

… 
2022-06-29 09:13:03,732 INFO org.apache.flink.runtime.util.HadoopUtils [] - New token (HDFS_DELEGATION_TOKEN token 52310 for work on ha-hdfs:newfyyy) created in KerberosDelegationToken, and next schedule delay is 64800045 ms.

2022-06-29 09:13:03,805 INFO org.apache.flink.runtime.util.HadoopUtils [] - Updating delegation tokens for current user. 
2022-06-29 09:13:03,806 INFO org.apache.flink.runtime.util.HadoopUtils [] - JobMaster receives new token (HDFS_DELEGATION_TOKEN token 52310 for work on ha-hdfs:newfyyy) from RM.

>> 
In TM log, 

2022-06-28 15:13:17,983 INFO org.apache.flink.runtime.util.HadoopUtils [] - TaskManager receives new token (HDFS_DELEGATION_TOKEN token 52101 for work on ha-hdfs:newfyyy) from RM. 
2022-06-28 15:13:18,016 INFO org.apache.flink.runtime.util.HadoopUtils [] - Updating delegation tokens for current user. 
… 
2022-06-29 09:13:03,809 INFO org.apache.flink.runtime.util.HadoopUtils [] - TaskManager receives new token (HDFS_DELEGATION_TOKEN token 52310 for work on ha-hdfs:newfyyy) from RM.

2022-06-29 09:13:03,836 INFO org.apache.flink.runtime.util.HadoopUtils [] - Updating delegation tokens for current user.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/22 02:29;jiulong.zhu1026;FLINK-28291.0001.patch;https://issues.apache.org/jira/secure/attachment/13046193/FLINK-28291.0001.patch",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 18 22:35:05 UTC 2023,,,,,,,,,,"0|z16g60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionManagerTest.testSessionNumberLimit failed with AssertionFailedError,FLINK-28290,13469003,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,hxbks2ks,hxbks2ks,29/Jun/22 02:42,30/Jun/22 12:10,04/Jun/24 20:42,30/Jun/22 12:06,1.16.0,,,,,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-06-28T11:40:17.0099766Z Jun 28 11:40:17 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.192 s <<< FAILURE! - in org.apache.flink.table.gateway.service.session.SessionManagerTest
2022-06-28T11:40:17.0101180Z Jun 28 11:40:17 [ERROR] org.apache.flink.table.gateway.service.session.SessionManagerTest.testSessionNumberLimit  Time elapsed: 0.115 s  <<< FAILURE!
2022-06-28T11:40:17.0103132Z Jun 28 11:40:17 org.opentest4j.AssertionFailedError: Failed to create session, the count of active sessions exceeds the max count: 3 ==> Expected org.apache.flink.table.gateway.api.utils.SqlGatewayException to be thrown, but nothing was thrown.
2022-06-28T11:40:17.0104111Z Jun 28 11:40:17 	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:71)
2022-06-28T11:40:17.0105112Z Jun 28 11:40:17 	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:41)
2022-06-28T11:40:17.0105830Z Jun 28 11:40:17 	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3098)
2022-06-28T11:40:17.0106714Z Jun 28 11:40:17 	at org.apache.flink.table.gateway.service.session.SessionManagerTest.testSessionNumberLimit(SessionManagerTest.java:103)
2022-06-28T11:40:17.0108109Z Jun 28 11:40:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-28T11:40:17.0108750Z Jun 28 11:40:17 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-28T11:40:17.0109463Z Jun 28 11:40:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-28T11:40:17.0110099Z Jun 28 11:40:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-28T11:40:17.0110738Z Jun 28 11:40:17 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-06-28T11:40:17.0111459Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-06-28T11:40:17.0112413Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-06-28T11:40:17.0113461Z Jun 28 11:40:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-06-28T11:40:17.0114613Z Jun 28 11:40:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-06-28T11:40:17.0115411Z Jun 28 11:40:17 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-06-28T11:40:17.0116262Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-06-28T11:40:17.0117111Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-06-28T11:40:17.0117957Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-06-28T11:40:17.0118829Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-06-28T11:40:17.0119660Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-06-28T11:40:17.0120492Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-06-28T11:40:17.0121260Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-06-28T11:40:17.0122126Z Jun 28 11:40:17 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-06-28T11:40:17.0122934Z Jun 28 11:40:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-06-28T11:40:17.0123774Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0124659Z Jun 28 11:40:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-06-28T11:40:17.0125490Z Jun 28 11:40:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-06-28T11:40:17.0126280Z Jun 28 11:40:17 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-06-28T11:40:17.0127081Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-06-28T11:40:17.0127896Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0128712Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T11:40:17.0129463Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T11:40:17.0130210Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T11:40:17.0131123Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0132179Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T11:40:17.0132934Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T11:40:17.0133847Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T11:40:17.0134965Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-06-28T11:40:17.0136044Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-06-28T11:40:17.0137068Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-28T11:40:17.0137881Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0138696Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T11:40:17.0139446Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T11:40:17.0140191Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T11:40:17.0140994Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0141922Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T11:40:17.0142691Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T11:40:17.0143607Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T11:40:17.0144669Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-06-28T11:40:17.0145606Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-28T11:40:17.0146415Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0147235Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T11:40:17.0147978Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T11:40:17.0148707Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T11:40:17.0149506Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T11:40:17.0150300Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T11:40:17.0151059Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T11:40:17.0152134Z Jun 28 11:40:17 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T11:40:17.0153104Z Jun 28 11:40:17 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-06-28T11:40:17.0153730Z Jun 28 11:40:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-28T11:40:17.0154361Z Jun 28 11:40:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-28T11:40:17.0155016Z Jun 28 11:40:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-28T11:40:17.0155670Z Jun 28 11:40:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-28T11:40:17.0156155Z Jun 28 11:40:17 
2022-06-28T11:40:17.8466649Z Jun 28 11:40:17 [INFO] 
2022-06-28T11:40:17.8467245Z Jun 28 11:40:17 [INFO] Results:
2022-06-28T11:40:17.8467619Z Jun 28 11:40:17 [INFO] 
2022-06-28T11:40:17.8467995Z Jun 28 11:40:17 [ERROR] Failures: 
2022-06-28T11:40:17.8468867Z Jun 28 11:40:17 [ERROR]   SessionManagerTest.testSessionNumberLimit:103 Failed to create session, the count of active sessions exceeds the max count: 3 ==> Expected org.apache.flink.table.gateway.api.utils.SqlGatewayException to be thrown, but nothing was thrown.
2022-06-28T11:40:17.8470031Z Jun 28 11:40:17 [INFO] 
2022-06-28T11:40:17.8470490Z Jun 28 11:40:17 [ERROR] Tests run: 10, Failures: 1, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37302&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28270,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 12:06:34 UTC 2022,,,,,,,,,,"0|z16g5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 02:42;hxbks2ks;cc [~fsk119];;;","30/Jun/22 12:06;fsk119;Fixed in master ac94fa1e9f32ef25c665bc167146bede73b46afd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Spark2 Reader for table store,FLINK-28289,13469000,13449930,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,29/Jun/22 02:20,14/Jul/22 07:18,04/Jun/24 20:42,14/Jul/22 07:18,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 07:18:49 UTC 2022,,,,,,,,,,"0|z16g4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 07:18;lzljs3620320;master: 9844ed49085c6729e34403050d4e857ea0b6a9aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support decode and encode built-in function in the Table API,FLINK-28288,13468999,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,29/Jun/22 02:12,01/Jul/22 05:16,04/Jun/24 20:42,01/Jul/22 05:16,,,,,,1.16.0,,,API / Python,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 05:16:08 UTC 2022,,,,,,,,,,"0|z16g4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 05:16;dianfu;Merged to master via 2f4f46219d09f261539a0aa47f915ee74c4721db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should TaskManagerRunner need a ShutdownHook,FLINK-28287,13468991,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hejiefang,hejiefang,29/Jun/22 00:06,07/Jul/22 00:37,04/Jun/24 20:42,,1.14.0,1.14.5,1.15.1,,,,,,,,,,,,,0,,,,,"TaskManagerRunner  has a close method，but did not call when it stop.

Some resources in TaskManagerRunner come with ShutdownHook, but some resources do not, such as rpcSystem, which causes the temporary file flink-rpc-akka_*.jar to not be deleted when stop.

Should TaskManagerRunner need a ShutdownHook to call the close method to release all resources

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-29 00:06:21.0,,,,,,,,,,"0|z16g2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move “enablechangelog” constant out of flink-streaming-java module,FLINK-28286,13468878,13425108,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,28/Jun/22 13:18,14/Jul/22 23:15,04/Jun/24 20:42,13/Jul/22 12:16,1.16.0,,,,,1.16.0,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,"Some methods in the flink-runtime module want to use {{StreamConfig.ENABLE_CHANGE_LOG_STATE_BACKEND}} constant(in flink-streaming-java module), but flink-runtime should not depend on flink-streaming-java. We should move {{ENABLE_CHANGE_LOG_STATE_BACKEND to a right place.}}

See this [discussion|https://github.com/apache/flink/pull/19907#discussion_r902485780] for more details. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26372,FLINK-27155,FLINK-25458,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 12:16:19 UTC 2022,,,,,,,,,,"0|z16fds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 12:16;roman;Merged into master as f1d894d1293e966dbdca06a48e4281d2daf63978.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Push filter into orc reader,FLINK-28285,13468871,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,28/Jun/22 12:31,14/Jul/22 02:58,04/Jun/24 20:42,14/Jul/22 02:58,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 02:58:57 UTC 2022,,,,,,,,,,"0|z16fc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 02:58;lzljs3620320;master: 088955f72bddf8134934dfd21152e1933c0cab77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add JdbcSink based on new Sink (sink2),FLINK-28284,13468865,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,eskabetxe,eskabetxe,28/Jun/22 11:47,06/Jan/23 09:01,04/Jun/24 20:42,06/Jan/23 09:01,,,,,,,,,Connectors / JDBC,,,,,,,0,pull-request-available,stale-major,,,Allow to use the new sink with jdbc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25421,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jan 06 09:01:12 UTC 2023,,,,,,,,,,"0|z16faw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 09:24;martijnvisser;[~eskabetxe] I think this is a duplicate of FLINK-25421 - For that ticket, [~RocMarshal] has just posted a FLIP https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217386271 on the Dev mailing list. ;;;","29/Jun/22 10:58;eskabetxe;[~MartijnVisser]  yes its duplicated (Im realy bad in searching :()
You could close this..

[~RocMarshal]  if you need help with the sink tell me. I will leave the code there as a draft if it could help in any way. ;;;","28/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","06/Jan/23 09:01;wanglijie; This is a duplicate of FLINK-25421, let me close this ticket so that we have all of the discussions in one place. cc [~eskabetxe] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improving the log of flink when job start and deploy,FLINK-28283,13468851,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,zlzhang0122,zlzhang0122,28/Jun/22 10:17,11/Feb/23 01:41,04/Jun/24 20:42,11/Feb/23 01:41,1.14.2,,,,,,,,Runtime / Task,,,,,,,0,,,,,"When running a large job with many operators and subtasks on flink, the JobManager and TaskManager will have a huge logs about the subtask executing msg such as ""XXX switched from CREATED to SCHEDULED、XXX switched from SCHEDULED to DEPLOYING 、XXX switched from DEPLOYING to RUNNING 、XXX switched from RUNNING to CANCELING、XXX switched from CANCELING to CANCELED"", etc. .

Maybe we can do some improvement about this, such as aggregate these msg to reduce the log, or change the log level and only logs the failure msg and subtask, etc. Not so sure about the solution, but these msg is really too much. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Feb 11 01:41:49 UTC 2023,,,,,,,,,,"0|z16f7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 10:04;xtsong;I think logging all the task state changes might be necessary.

- We cannot aggregate these logs, because state changing of tasks are independent from each other.
- We cannot only log failures, because tasks may stuck in some state and never reaches fail / running state. In such cases, we need to know what state the task is in. Same for changing log level.;;;","09/Feb/23 12:35;chesnay;I'm inclined to close this ticket; aggregations won't work, and hiding the log message can already be achieved via log4j filters.;;;","10/Feb/23 01:47;xtsong;I'm also inclined to close the ticket. But let's wait a bit longer for the response from the reporter.;;;","10/Feb/23 09:40;zlzhang0122;[~xtsong] Right you are, actually what I think is cache theses logs for some short time and then aggregate and print it, but indeed this may cause some problem such as timeliness and lose of logs, so this is fine with me to close this ticket.

[~chesnay] Sure, we can hiding the log message via log4j filters.;;;","11/Feb/23 01:41;xtsong;[~zlzhang0122], thanks for your reply. Closing this as won't do.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Planner free in flink-table-store-codegen,FLINK-28282,13468839,13450148,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,28/Jun/22 09:41,30/Jun/22 06:42,04/Jun/24 20:42,30/Jun/22 06:42,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"We currently have the table-planner bundled into flink-table-store-codegen, which causes:

* bundle jar is too big, 20+MB
* Dependence on planner code will make it difficult to be compatible with multiple versions of Flink",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 06:42:51 UTC 2022,,,,,,,,,,"0|z16f54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 06:42;lzljs3620320;master: 910b8f3e2bcbb4883b6b3794c01858a8097d0d65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate test_kubernetes_itcases.sh,FLINK-28281,13468837,13342988,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,28/Jun/22 09:19,28/Jun/22 09:19,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,We should migrate {{test_kubernetes_itcases.sh}} from a Bash e2e test to a proper Java e2e test. This would probably also have helped preventing tickets like FLINK-28269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-28 09:19:54.0,,,,,,,,,,"0|z16f4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate test_kubernetes_application_ha.sh,FLINK-28280,13468836,13342988,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ConradJam,martijnvisser,martijnvisser,28/Jun/22 09:19,29/Jul/22 09:42,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,We should migrate {{test_kubernetes_application_ha.sh}} from a Bash e2e test to a proper Java/Go e2e test. This would probably also have helped preventing tickets like FLINK-28269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 29 09:42:33 UTC 2022,,,,,,,,,,"0|z16f4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 13:52;ConradJam;H [~martijnvisser] I want to try this ticket,can you assign to me?;;;","27/Jul/22 13:56;ConradJam;Can I understand that the test_kubernetes_application_ha.sh related tests are turned into java code?{*}{*};;;","29/Jul/22 09:42;martijnvisser;I have no strong opinion on that, but I don't think they should be depending on Bash. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate test_kubernetes_application.sh,FLINK-28279,13468834,13342988,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,28/Jun/22 09:18,28/Jun/22 09:18,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,We should migrate {{test_kubernetes_application.sh}} from a Bash e2e test to a proper Java e2e test. This would probably also have helped preventing tickets like FLINK-28269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-28 09:18:53.0,,,,,,,,,,"0|z16f40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate the Kubernetes Session Tests test_kubernetes_session.sh,FLINK-28278,13468833,13342988,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,28/Jun/22 09:17,28/Jun/22 09:17,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,We should migrate both {{kubernetes session tests}} test_kubernetes_session.sh from a Bash e2e test to a proper Java e2e test. This would probably also have helped preventing tickets like FLINK-28269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-28 09:17:44.0,,,,,,,,,,"0|z16f3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate test_kubernetes_embedded_job.sh,FLINK-28277,13468832,13342988,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,28/Jun/22 09:16,28/Jun/22 09:16,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,We should migrate {{test_kubernetes_embedded_job.sh}} from a Bash e2e test to a proper Java e2e test. This would probably also have helped preventing tickets like FLINK-28269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-28 09:16:02.0,,,,,,,,,,"0|z16f3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.connectors.hive.FlinkHiveException: Unable to instantiate the hadoop input format,FLINK-28276,13468830,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,migowei,migowei,28/Jun/22 09:03,01/Jul/22 03:08,04/Jun/24 20:42,01/Jul/22 03:08,1.14.2,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"When I read Iceberg tables using Flink HiveCatalog, based on S3A,  I got this error:

 
{code:java}
//代码占位符
Exception in thread ""main"" org.apache.flink.connectors.hive.FlinkHiveException: Unable to instantiate the hadoop input format
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:100)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:71)
    at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:149)
    at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
    at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
    at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:144)
    at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:114)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:106)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:49)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:58)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:82)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:81)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:185)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:805)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1274)
    at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:601)
    at loshu.flink.hive.FlinkSQLHiveWriter.main(FlinkSQLHiveWriter.java:69)
Caused by: java.lang.InstantiationException
    at sun.reflect.InstantiationExceptionConstructorAccessorImpl.newInstance(InstantiationExceptionConstructorAccessorImpl.java:48)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at java.lang.Class.newInstance(Class.java:442)
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:98)
    ... 30 more
16:33:36,767 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Stopping s3a-file-system metrics system...
16:33:36,767 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system stopped.
16:33:36,768 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system shutdown complete.Process finished with exit code 1
 {code}
My code is:
{code:java}
//代码占位符
public class FlinkSQLHiveWriter {
    private static org.apache.log4j.Logger log = Logger.getLogger(FlinkSQLHiveWriter.class);
    public static void main(String[] args) throws Exception {
        System.setProperty(""HADOOP_USER_NAME"", ""root"");
        System.setProperty(""hadoop.home.dir"", ""/opt/hadoop-3.2.1/"");
        EnvironmentSettings settings = EnvironmentSettings.newInstance()
                .inBatchMode()
                .build();
        TableEnvironment tableEnv = TableEnvironment.create(settings);

        String catalogName = ""s3IcebergCatalog"";
        String defaultDatabase = ""s3a_flink"";
        String hiveConfDir = ""flink-cloud/src/main/resources""; 
        HiveCatalog hive = new HiveCatalog(catalogName, defaultDatabase, hiveConfDir);

        tableEnv.registerCatalog(catalogName, hive);
        tableEnv.useCatalog(catalogName);
        tableEnv.useDatabase(defaultDatabase);

        System.out.println(hive.listDatabases());
        System.out.println(hive.listTables(defaultDatabase));
        String tableName = ""icebergTBCloudTracking"";
        // set sql dialect as default, means using flink sql.
        tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);

        String sql = ""select vin from "" + tableName;
//        String sql = ""DESC "" + tableName;
        System.out.println(sql);
        Table table = tableEnv.sqlQuery(sql);
        table.execute();
    }
} {code}
I can ""show tables"" or ""describe tables"", but when using ""select * from table"" the error occurs.

 ","Flink 1.14.2

Hive 3.1.2

Scala 2.12

Iceberg 0.12.1

Hadoop 3.2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/22 02:05;luoyuxia;BA9CEEA0-BF38-4568-A7AD-66C68B19CF14.png;https://issues.apache.org/jira/secure/attachment/13046065/BA9CEEA0-BF38-4568-A7AD-66C68B19CF14.png","30/Jun/22 01:37;migowei;image-2022-06-30-09-37-44-705.png;https://issues.apache.org/jira/secure/attachment/13045972/image-2022-06-30-09-37-44-705.png","30/Jun/22 02:36;migowei;image-2022-06-30-10-36-17-075.png;https://issues.apache.org/jira/secure/attachment/13046068/image-2022-06-30-10-36-17-075.png","01/Jul/22 03:05;migowei;image-2022-07-01-11-05-01-238.png;https://issues.apache.org/jira/secure/attachment/13046116/image-2022-07-01-11-05-01-238.png",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,,,Fri Jul 01 03:07:31 UTC 2022,,,,,,,,,,"0|z16f34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 08:35;luoyuxia;[~migowei] What's the table's input format? You can use `describe extended xxx` to check it. And please make sure the class of table's inputFormat exists in your classpath.

 ;;;","30/Jun/22 01:46;migowei;The table input format displayed by `describe extended xxx` is as follows. 

!image-2022-06-30-09-37-44-705.png!

 

Debugging in debug mode shows this input format is used:

org.apache.hadoop.mapred.FileInputFormat

org.apache.hadoop.mapred.FileOutputFormat

 

And part of my pom.xml is:
{code:java}
<!-- mapreduce -->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-core</artifactId>
    <version>${hadoop.version}</version>
    </exclusions>
</dependency>

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-common</artifactId>
    <version>${hadoop.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
    <version>${hadoop.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-hdfs</artifactId>
</dependency>

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>${hadoop.version}</version>
</dependency> 

<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapred</artifactId>
    <version>0.22.0</version>
</dependency>{code}
This issue still exists.  T T;;;","30/Jun/22 02:05;luoyuxia;FileInputFormat is just an abstract class. I mean the what is the real inputformat? E.g. OrcInputFormat, MapredParquetInputFormat, TextInputFormat.

!BA9CEEA0-BF38-4568-A7AD-66C68B19CF14.png|width=500,height=204!;;;","30/Jun/22 02:37;migowei;I didn't specify the input format when creating the table, so the result is as follows.  Which input format should I use when using iceberg table? Thanks a lot!!!

!image-2022-06-30-10-36-17-075.png!;;;","30/Jun/22 03:12;luoyuxia;The picture isn't completed so that I can't see the full inputFormt. It'll be better if I can know the full class name of inputFormt.

But I try to google it for the exception. Seems the inputFormt class is an abtract class so that it can'be Instantiated. [https://stackoverflow.com/questions/7896773/instantiationexception-on-simple-reflective-call-to-newinstance-on-a-class]

Any input format you can use in iceberg table. But the column based format, like OrcInputFormat,ParquetFormat is more common for data warehouse.;;;","30/Jun/22 03:46;migowei;Thanks! I think this is the cause of the problem. I create table by flink sdk as follow:
{code:java}
Map<String, String> properties = new HashMap<>();
        properties.put(""type"", ""iceberg"");
        properties.put(""clients"", ""5"");
        properties.put(""property-version"", ""1"");
        properties.put(""warehouse"", ""s3a://warehouse/"");
        properties.put(""catalog-type"", ""hive"");
        properties.put(""uri"", ""thrift://hiveserver:9083"");


        Configuration conf = new Configuration();
        conf.set(""fs.s3a.connection.ssl.enabled"", ""false"");
        conf.set(""fs.s3a.endpoint"", ""http://mys3"");
        conf.set(""fs.s3a.region"", ""us-east-1"");
        conf.set(""fs.s3a.path.style.access"", ""true"");
        conf.set(""fs.s3a.impl"", ""org.apache.hadoop.fs.s3a.S3AFileSystem"");
        conf.set(""fs.s3a.fast.upload"", ""true"");
        conf.set(""execution.checkpointing.checkpoints-after-tasks-finish.enabled"", ""true"");

        String HIVE_CATALOG = ""s3IcebergCatalog"";
        CatalogLoader catalogLoader = CatalogLoader.hive(HIVE_CATALOG, conf, properties);
        HiveCatalog catalog = (HiveCatalog) catalogLoader.loadCatalog();

        Namespace namespace = Namespace.of(""s3a_flink"");
        if (!catalog.namespaceExists(namespace))
            catalog.createNamespace(namespace);

        TableIdentifier name =
                TableIdentifier.of(namespace, ""icebergTBCloudTracking"");

        Schema schema = new Schema(0,
                Types.NestedField.required(1, ""vin"", Types.StringType.get()),
                Types.NestedField.required(2, ""name"", Types.StringType.get()),
                Types.NestedField.optional(3, ""uuid"", Types.StringType.get()),
                Types.NestedField.required(4, ""channel"", Types.StringType.get()),
                Types.NestedField.required(5, ""run_scene"", Types.StringType.get()),
                Types.NestedField.required(6, ""timestamp"", Types.TimestampType.withoutZone()),
                Types.NestedField.required(7, ""rcv_timestamp"", Types.TimestampType.withoutZone()),
                Types.NestedField.required(8, ""raw"", Types.StringType.get())
                );

         PartitionSpec spec = PartitionSpec.unpartitioned();

        Map<String, String> props =
                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, FileFormat.ORC.name());

        Table table = null;
 
        if (!catalog.tableExists(name))
            table = catalog.createTable(name, schema, spec, props);
        else
            table = catalog.loadTable(name); {code}
May I ask how to specify inputformat when creating table by sdk?  Thanks!;;;","30/Jun/22 04:23;luoyuxia;It's about iceberg sdk. I'm not familar with it. And I'm not sure wheher we can specific the inputfomat using sdk.

If not, I think you may need to use Hive SQL in HiveCli or switch to [hive dialect  |https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/hive/hive_dialect]in Flink.;;;","01/Jul/22 03:07;migowei;I add  `table.updateProperties().set(""engine.hive.enabled"", ""true"").commit();` in code and it works.

!image-2022-07-01-11-05-01-238.png!

 

But there are other errors. I will open another question. Thanks~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table.execute().print() prints SqlRawValue,FLINK-28275,13468813,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,28/Jun/22 08:18,06/Jul/22 10:18,04/Jun/24 20:42,,1.15.0,,,,,,,,Table SQL / API,,,,,,,0,,,,,"If the following code is executed with Flink 1.15.0 and Flink ML 2.1

 
{code:java}
import org.apache.flink.ml.linalg.Vectors;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

public class SqlRawValueExample {
public static void main(String[] args) {
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

DataStream<Row> dataStream = env.fromElements(Row.of(Vectors.dense(1.0, 2.0)));
Table table = tEnv.fromDataStream(dataStream);

table.execute().print();
}
}{code}
 

The following result would be printed out

 
{code:java}
+----+--------------------------------+
| op |                             f0 |
+----+--------------------------------+
| +I |                 SqlRawValue{?} |
+----+--------------------------------+{code}
 

while the expected result is 
{code:java}
+----+--------------------------------+
| op |                             f0 |
+----+--------------------------------+
| +I |                     [1.0, 2.0] |
+----+--------------------------------+{code}
This behavior affects the readability of the generated results.

 ","Flink 1.15

Flink ML 2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 10:18:59 UTC 2022,,,,,,,,,,"0|z16ezc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jul/22 08:22;paul8263;Hi all,

I tested in both Flink 1.14.3 and 1.15.0. It came out that the problem only exists with Flink 1.15.x.

I investigated the code in org/apache/flink/table/planner/functions/casting/RowDataToStringConverterImpl.java::init(), and found:
{code:java}
    this.columnConverters[index] =
            row -> {
                if (row.isNullAt(index)) {
                    return PrintStyle.NULL_VALUE;
                }
                return castExecutor.cast(getter.getFieldOrNull(row)).toString();
            }; {code}


 
The generated castExecutor here calls org/apache/flink/table/data/binary/BinaryRawValueData.java::toObject() method, with the serializer of RawValueDataSerializer. 

The generated code for castExecutor is as below:
{code:java}
public final class GeneratedCastExecutor$0 implements org.apache.flink.table.data.utils.CastExecutor {
    private final org.apache.flink.table.runtime.typeutils.RawValueDataSerializer typeSerializer$2;
    public GeneratedCastExecutor$0(org.apache.flink.table.runtime.typeutils.RawValueDataSerializer typeSerializer$2) {
        this.typeSerializer$2 = typeSerializer$2;
    }
    @Override public Object cast(Object _myInputObj) throws org.apache.flink.table.api.TableException {
        org.apache.flink.table.data.binary.BinaryRawValueData _myInput = ((org.apache.flink.table.data.binary.BinaryRawValueData)(_myInputObj));
        boolean _myInputIsNull = _myInputObj == null;
        boolean isNull$0;
        org.apache.flink.table.data.binary.BinaryStringData result$1;
        isNull$0 = _myInputIsNull;
        if (!isNull$0) {
            java.lang.Object deserializedObj$0 = _myInput.toObject(typeSerializer$2);
            if (deserializedObj$0 != null) {
                java.lang.String resultString$1;
                resultString$1 = deserializedObj$0.toString().toString();
                result$1 = org.apache.flink.table.data.binary.BinaryStringData.fromString(resultString$1);
            } else {
                result$1 = null;
            }
            isNull$0 = result$1 == null;
        } else {
            result$1 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
        }
        return result$1;
    }
} {code}


Via debug I found that the contructor arg is a RowValueDataserializer instance holding a DenseVectorSerializer.

In RowValueDataserializer::deserialize, it does not use the wrapped serializer to deserialize the object. This is where I think the bug is.

I would like to fix this. Could anyone assign me this ticket please?;;;","06/Jul/22 10:18;paul8263;Hi all,

It might not be a good practice to deserialize the data to javaObject in RowValueDataserializer::deserialize.

The root cause is in this case Flink tries to convert BinaryRawValueData to the backed object using RawValueDataSerializer. It should use RawValueDataSerializer.getInnerSerializer() instead.

However, if we introduced the dependency flink-table-runtime(where RawValueDataSerializer belongs to) to flink-table-common(BinaryRawValueData), we would get a circular dependency. We cannot do this.

Does anyone have any better ideas? Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContinuousFileMonitoringFunction doesn't work with reactive mode,FLINK-28274,13468793,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Leo Zhou,rmetzger,rmetzger,28/Jun/22 06:34,06/Jul/22 06:12,04/Jun/24 20:42,06/Jul/22 06:12,1.16.0,,,,,1.16.0,,,API / DataStream,Runtime / Coordination,,,,,,0,pull-request-available,,,,"This issue was first reported in the Flink Slack: https://apache-flink.slack.com/archives/C03G7LJTS2G/p1656257678477659

It seems that reactive mode is changing the parallelism of the `ContinuousFileMonitoringFunction`, which is supposed to always run with a parallelism of 1.

This is the error
{code}
INITIALIZING to FAILED with failure cause: java.lang.IllegalArgumentException: ContinuousFileMonitoringFunction retrieved invalid state.
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
{code}

You can see from the logs that the parallelism is changing on a rescale event:
{code}
2022-06-27 13:38:54,979 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/1) (cbaad20beee908b95c9fe5c34ba76bfa) switched from RUNNING to CANCELING.
2022-06-27 13:38:55,254 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/1) (cbaad20beee908b95c9fe5c34ba76bfa) switched from CANCELING to CANCELED.
2022-06-27 13:38:55,657 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (2/2) (6ceaacbe8d9aa507b0a56c850082da8c) switched from DEPLOYING to INITIALIZING.
2022-06-27 13:38:55,722 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (2/2) (6ceaacbe8d9aa507b0a56c850082da8c) switched from INITIALIZING to RUNNING.
2022-06-27 13:44:54,058 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/2) (665b12194741744d6bba4408a252fa45) switched from RUNNING to CANCELING.
2022-06-27 13:45:00,825 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/2) (3cc408fd0eb9ddfa97b22f4dfc09d8dc) switched from DEPLOYING to INITIALIZING.
2022-06-27 13:45:00,826 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (1/2) (3cc408fd0eb9ddfa97b22f4dfc09d8dc) switched from INITIALIZING to RUNNING.
2022-06-27 13:45:01,434 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (2/2) (79338042d84b6458c34760bc85145512) switched from DEPLOYING to INITIALIZING.
2022-06-27 13:45:02,427 | INFO  | .executiongraph.ExecutionGraph  | Source: Custom File Source (2/2) (79338042d84b6458c34760bc85145512) switched from INITIALIZING to RUNNING.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 06:12:33 UTC 2022,,,,,,,,,,"0|z16euw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 06:40;rmetzger;I currently don't have the capacity to look into fixing this, but if somebody is interested, this is what I would do:
1. Add a new test using the file monitoring source to the ReactiveModeITCase and verify that the test really fails. Maybe this is not the best way to test the fix, but its a starting point for locally reproducing the issue.
2. I guess that the “computeVertexParallelismStoreForExecution” method is where you’ll find the logic where reactive mode sets the parallelism
3. The fix is either ensuring that maxParallelism for the monitoring function is set correctly, or somehow telling the adaptive scheduler to not change the parallelism of that function (in a generic way);;;","30/Jun/22 13:06;Leo Zhou;---The fix is either ensuring that maxParallelism for the monitoring function is set correctly, or somehow telling the adaptive scheduler to not change the parallelism of that function (in a generic way)

I prefer to set the max parallelism for the monitoring function correctly. Since ContinuousFileMonitoringFunction is a legacy source function, we'd better not change the adaptive scheduler for  this issue.

[AdaptiveScheduler.java#L344|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/AdaptiveScheduler.java#L344] will calculate the maxParallelism for vertex only if no max parallelism was configured, so if we can ensure that maxParallelism for the monitoring function is 1, then reactive mode would not change the parallelism of the `ContinuousFileMonitoringFunction`.

Since `ContinuousFileMonitoringFunction` is only used in [StreamExecutionEnvironment.java#L1855|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java#L1855] , It's safe to explicitly set the max parallelism here.;;;","30/Jun/22 13:15;rmetzger;Thanks for your research. I agree that we shouldn't include source-specific logic into the adaptive scheduler for this.

I like the idea of setting maxParallelism in the SEE (I currently can not imagine any issues with that, but we'll see if all the tests are passing properly once you've done the change ;) );;;","30/Jun/22 13:59;Leo Zhou;HI [~rmetzger] , could you assign this ticket to me before I open a PR ?;;;","04/Jul/22 14:44;martijnvisser;Does this problem only occur for {ContinuousFileMonitoringFunction} or also for the new {FileSource}? ;;;","05/Jul/22 02:37;wanglijie;I think it only occur for {{ContinuousFileMonitoringFunction (legacy file source)}}. From what I understand, the {{ContinuousFileMonitoringFunction}} plays the role of SplitEnumerator in legacy file source (parallelism must be 1). The new FileSource does not need it, so new FileSource shouldn't have this problem.;;;","05/Jul/22 02:42;wanglijie;I have one more question, does it also occur with other operators whose parallelism must be 1 (For example, {{GlobalAgg}} in Table/SQL)?;;;","05/Jul/22 06:52;martijnvisser;Honest question: why fix it if it's only occurring in a legacy component and there is a solution for it, using the target approach? Especially since {ContinuousFileMonitoringFunction} is internal as well. ;;;","05/Jul/22 07:35;Leo Zhou;-- Does this problem only occur for \{ContinuousFileMonitoringFunction} or also for the new \{FileSource}

As [~wanglijie95] said, with FLIP-27, FileSource does not need {{ContinuousFileMonitoringFunction}}  to plays the role of SplitEnumerator, so this problem won't occur for new FileSource.

 

-- does it also occur with other operators whose parallelism must be 1 ?

From what I understand, if the max parallelism is not set to 1, it's possible that this problem may occur for these operators whose parallelism must be 1.

 

-- why fix it if it's only occurring in a legacy component ?

The legacy source are still widely used, for example, When reading files with datastream api, the first thought is to use readFile/readTextFile() methods, especially for starters. Since the fix work won't take much effort, may be it's worthy.;;;","05/Jul/22 07:41;martijnvisser;-- The legacy source are still widely used, for example, When reading files with datastream api, the first thought is to use readFile/readTextFile() methods, especially for starters. Since the fix work won't take much effort, may be it's worthy.

I don't think that the usage is a good reason, because that's always the case when something new is introduced. Without an incentive for users to migrate, they will never do so. It probably makes sense to fix it in this case, but I still think that we are doing something wrong as a Flink community since people shouldn't use {Internal} interfaces at all. ;;;","05/Jul/22 07:48;Leo Zhou;`Without an incentive for users to migrate, they will never do so. `  cannot agree more :);;;","06/Jul/22 06:10;rmetzger;Merged to master in https://github.com/apache/flink/commit/c71762686b1ef9c8fbc1cdb58bc0c9c2dbaa2da9;;;","06/Jul/22 06:12;rmetzger;Thanks [~Leo Zhou] for the fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Send information on Flink Mirror build pipelines to dedicated Slack channel,FLINK-28273,13468792,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,martijnvisser,martijnvisser,28/Jun/22 06:31,01/Jul/22 19:01,04/Jun/24 20:42,01/Jul/22 19:01,,,,,,,,,Build System / Azure Pipelines,,,,,,,0,,,,,We would like to publish the status for each pipeline run in https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1 to the dedicated #builds Slack channel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-28 06:31:45.0,,,,,,,,,,"0|z16euo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle TLS Certificate Renewal in Webhook,FLINK-28272,13468784,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,morhidi,,28/Jun/22 05:22,24/Nov/22 01:03,04/Jun/24 20:42,12/Sep/22 09:42,kubernetes-operator-1.0.0,,,,,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"We found that flink-kubernetes-operator v1.0.0 does not reload new certificate when updated by cert-manager, and it causes the following error when updating FlinkDeployment


{{Failed sync attempt to 597d35a7434bede526f526852c33a65262765219: one or more objects failed to apply, reason: Internal error occurred: failed calling webhook ""flinkoperator.flink.apache.org"": Post ""}}
{{[https://flink-operator-webhook-service.flink-operator.svc:443/validate?timeout=10s|https://flink-operator-webhook-service.flink-operator.svc/validate?timeout=10s]}}
{{"": x509: certificate signed by unknown authority (possibly because of ""x509: invalid signature: parent certificate cannot sign this kind of certificate"" while trying to verify candidate authority certificate ""FlinkDeployment Validator"") (retried 3 times).}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 12 09:42:17 UTC 2022,,,,,,,,,,"0|z16esw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 05:25;morhidi;We can do something similar:

https://goldius.medium.com/netty-reloading-ssl-tls-certificate-2078cdf6bfc1;;;","17/Aug/22 06:37;wu3396;Hello,
I've just hit the same issue , my env is:
kubernetes-operator:1.1.0
 
flink-webhook logs
{code:java}
{ ""@timestamp"": ""2022-08-17T05:58:52.578Z"", 
   ""ecs.version"": ""1.2.0"", 
   ""log.level"": ""WARN"", 
   ""message"": ""An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception."", 
   ""process.thread.name"": ""nioEventLoopGroup-3-2"",
   ""log.logger"": ""org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline"", 
   ""error.type"": ""org.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException"", 
   ""error.message"": ""javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate"",
   ""error.stack_trace"": ""org.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate 
at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:477) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Unknown Source) Caused by: javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate at java.base/sun.security.ssl.Alert.createSSLException(Unknown Source) at java.base/sun.security.ssl.Alert.createSSLException(Unknown Source) at java.base/sun.security.ssl.TransportContext.fatal(Unknown Source) at java.base/sun.security.ssl.Alert$AlertConsumer.consume(Unknown Source) at java.base/sun.security.ssl.TransportContext.dispatch(Unknown Source) at java.base/sun.security.ssl.SSLTransport.decode(Unknown Source) at java.base/sun.security.ssl.SSLEngineImpl.decode(Unknown Source) at java.base/sun.security.ssl.SSLEngineImpl.readRecord(Unknown Source) at java.base/sun.security.ssl.SSLEngineImpl.unwrap(Unknown Source) at java.base/sun.security.ssl.SSLEngineImpl.unwrap(Unknown Source) at java.base/javax.net.ssl.SSLEngine.unwrap(Unknown Source) at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:296) at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1342) at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1235) at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1284) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507) at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446) ... 17 more"" 
}{code}
 ;;;","12/Sep/22 09:42;mbalassi;[e5a325c|https://github.com/apache/flink-kubernetes-operator/commit/e5a325c48965a50d61d0aa29e61ba79e97f27082] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add configuration options for speculative scheduler,FLINK-28271,13468779,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,28/Jun/22 03:55,01/Jul/22 13:32,04/Jun/24 20:42,01/Jul/22 13:32,,,,,,1.16.0,,,Runtime / Configuration,,,,,,,0,pull-request-available,,,,"Following config options are needed for speculative scheduler:
* jobmanager.adaptive-batch-scheduler.speculative.enabled, default to ""false"".  It controls whether to enable speculative execution. Note that speculative execution only works with AdaptiveBatchScheduler, which requires ""jobmanager.scheduler"" to be set to ""AdaptiveBatch"".
* jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions, default to ""2"". It controls how many executions (including the original one and speculative ones) of an ExecutionVertex can execute at the same time.
* jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration, default to ""1 min"". It controls how long an identified slow node should be blocked for.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 13:32:01 UTC 2022,,,,,,,,,,"0|z16ers:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 13:32;zhuzh;Done via 2aae49748350806a8523b3b0d95b6f795a9327f0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionManagerTest.testIdleSessionCleanup failed with Session doesn't exist,FLINK-28270,13468776,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hxbks2ks,hxbks2ks,28/Jun/22 03:24,30/Jun/22 12:10,04/Jun/24 20:42,30/Jun/22 12:10,1.16.0,,,,,,,,Table SQL / Gateway,,,,,,,0,test-stability,,,,"
{code:java}
2022-06-28T02:21:32.7045686Z Jun 28 02:21:32 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.367 s <<< FAILURE! - in org.apache.flink.table.gateway.service.session.SessionManagerTest
2022-06-28T02:21:32.7046900Z Jun 28 02:21:32 [ERROR] org.apache.flink.table.gateway.service.session.SessionManagerTest.testIdleSessionCleanup  Time elapsed: 2.328 s  <<< ERROR!
2022-06-28T02:21:32.7048127Z Jun 28 02:21:32 org.apache.flink.table.gateway.api.utils.SqlGatewayException: Session 'b15bff8b-33b8-4784-a53e-d3c900f64137' does not exist.
2022-06-28T02:21:32.7048951Z Jun 28 02:21:32 	at org.apache.flink.table.gateway.service.session.SessionManager.getSession(SessionManager.java:128)
2022-06-28T02:21:32.7049799Z Jun 28 02:21:32 	at org.apache.flink.table.gateway.service.session.SessionManagerTest.testIdleSessionCleanup(SessionManagerTest.java:80)
2022-06-28T02:21:32.7050524Z Jun 28 02:21:32 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-28T02:21:32.7051281Z Jun 28 02:21:32 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-28T02:21:32.7052008Z Jun 28 02:21:32 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-28T02:21:32.7052830Z Jun 28 02:21:32 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-28T02:21:32.7053710Z Jun 28 02:21:32 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-06-28T02:21:32.7054472Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-06-28T02:21:32.7055316Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-06-28T02:21:32.7056162Z Jun 28 02:21:32 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-06-28T02:21:32.7056948Z Jun 28 02:21:32 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-06-28T02:21:32.7057926Z Jun 28 02:21:32 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-06-28T02:21:32.7059622Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-06-28T02:21:32.7060852Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-06-28T02:21:32.7061721Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-06-28T02:21:32.7062621Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-06-28T02:21:32.7063480Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-06-28T02:21:32.7064327Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-06-28T02:21:32.7065312Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-06-28T02:21:32.7066107Z Jun 28 02:21:32 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-06-28T02:21:32.7066925Z Jun 28 02:21:32 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-06-28T02:21:32.7067849Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7068646Z Jun 28 02:21:32 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-06-28T02:21:32.7069491Z Jun 28 02:21:32 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-06-28T02:21:32.7070317Z Jun 28 02:21:32 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-06-28T02:21:32.7071342Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-06-28T02:21:32.7072450Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7073289Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T02:21:32.7074056Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T02:21:32.7074809Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T02:21:32.7075646Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7076663Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T02:21:32.7077614Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T02:21:32.7078568Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T02:21:32.7079717Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-06-28T02:21:32.7080884Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-06-28T02:21:32.7081851Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-28T02:21:32.7082666Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7083639Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T02:21:32.7084768Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T02:21:32.7085594Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T02:21:32.7086429Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7087311Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T02:21:32.7088100Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T02:21:32.7089056Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T02:21:32.7090142Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-06-28T02:21:32.7091175Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-28T02:21:32.7092010Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7092848Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-28T02:21:32.7093615Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-28T02:21:32.7094391Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-28T02:21:32.7095218Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-28T02:21:32.7096026Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-28T02:21:32.7096788Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-28T02:21:32.7098317Z Jun 28 02:21:32 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-28T02:21:32.7099619Z Jun 28 02:21:32 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-06-28T02:21:32.7100266Z Jun 28 02:21:32 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-28T02:21:32.7101011Z Jun 28 02:21:32 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-28T02:21:32.7101729Z Jun 28 02:21:32 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-28T02:21:32.7102408Z Jun 28 02:21:32 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-28T02:21:32.7102891Z Jun 28 02:21:32 
2022-06-28T02:21:33.1062704Z Jun 28 02:21:33 [INFO] 
2022-06-28T02:21:33.1063557Z Jun 28 02:21:33 [INFO] Results:
2022-06-28T02:21:33.1064328Z Jun 28 02:21:33 [INFO] 
2022-06-28T02:21:33.1064943Z Jun 28 02:21:33 [ERROR] Errors: 
2022-06-28T02:21:33.1066905Z Jun 28 02:21:33 [ERROR]   SessionManagerTest.testIdleSessionCleanup:80 » SqlGateway Session 'b15bff8b-33...
2022-06-28T02:21:33.1068294Z Jun 28 02:21:33 [INFO] 
2022-06-28T02:21:33.1069001Z Jun 28 02:21:33 [ERROR] Tests run: 10, Failures: 0, Errors: 1, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28290,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 12:10:04 UTC 2022,,,,,,,,,,"0|z16er4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 03:25;hxbks2ks;cc [~fsk119];;;","30/Jun/22 12:10;fsk119;This has a similar reason as FLINK-28290. The check thread may touch the session before the subsequent request, which causes the thread is expired.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes test failed with permission denied,FLINK-28269,13468774,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,hxbks2ks,hxbks2ks,28/Jun/22 03:20,17/Oct/22 15:53,04/Jun/24 20:42,29/Jun/22 16:55,1.15.0,1.16.0,,,,1.14.6,1.15.2,1.16.0,Deployment / Kubernetes,Tests,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-06-28T00:57:41.2724102Z Jun 28 00:57:41 Preparing to unpack .../conntrack_1%3a1.4.5-2_amd64.deb ...
2022-06-28T00:57:41.2747620Z Jun 28 00:57:41 Unpacking conntrack (1:1.4.5-2) ...
2022-06-28T00:57:41.3287046Z Jun 28 00:57:41 Setting up conntrack (1:1.4.5-2) ...
2022-06-28T00:57:41.3316320Z Jun 28 00:57:41 Processing triggers for man-db (2.9.1-1) ...
2022-06-28T00:57:43.0246060Z Jun 28 00:57:43 fs.protected_regular = 0
2022-06-28T00:57:46.5039126Z Jun 28 00:57:46 * Profile ""minikube"" not found. Run ""minikube profile list"" to view all profiles.
2022-06-28T00:57:46.5047104Z Jun 28 00:57:46   To start a cluster, run: ""minikube start""
2022-06-28T00:57:46.5088285Z Jun 28 00:57:46 Starting minikube ...
2022-06-28T00:57:46.6801624Z Jun 28 00:57:46 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:57:46.7004754Z Jun 28 00:57:46 * Using the none driver based on user configuration
2022-06-28T00:57:46.7128484Z Jun 28 00:57:46 * Starting control plane node minikube in cluster minikube
2022-06-28T00:57:46.7178545Z Jun 28 00:57:46 * Running on localhost (CPUs=2, Memory=6946MB, Disk=85173MB) ...
2022-06-28T00:57:47.0620600Z Jun 28 00:57:47 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:57:49.3027006Z Jun 28 00:57:49 
2022-06-28T00:57:49.3040058Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:57:49.3040497Z stdout:
2022-06-28T00:57:49.3040596Z 
2022-06-28T00:57:49.3040809Z stderr:
2022-06-28T00:57:49.3041270Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:57:49.3041461Z 
2022-06-28T00:57:49.3041663Z * 
2022-06-28T00:57:49.3063195Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:57:49.3063744Z │                                                                                             │
2022-06-28T00:57:49.3064375Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:57:49.3064975Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:57:49.3065484Z │                                                                                             │
2022-06-28T00:57:49.3066019Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:57:49.3066543Z │                                                                                             │
2022-06-28T00:57:49.3067048Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:57:49.3067360Z Jun 28 00:57:49 
2022-06-28T00:57:49.3922883Z E0628 00:57:49.392012  194614 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:57:49.3948489Z Jun 28 00:57:49 
2022-06-28T00:57:49.3957501Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:49.3960093Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:49.3960924Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:49.3966499Z Jun 28 00:57:49 
2022-06-28T00:57:49.4693197Z Jun 28 00:57:49 
2022-06-28T00:57:49.4700763Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:49.4702406Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:49.4703168Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:49.4731042Z Jun 28 00:57:49 
2022-06-28T00:57:49.4733005Z Jun 28 00:57:49 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:57:54.5441686Z Jun 28 00:57:54 
2022-06-28T00:57:54.5443916Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:54.5445961Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:54.5446526Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:54.5450376Z Jun 28 00:57:54 
2022-06-28T00:57:54.5469377Z Jun 28 00:57:54 Starting minikube ...
2022-06-28T00:57:54.6254675Z Jun 28 00:57:54 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:57:54.6299271Z Jun 28 00:57:54 * Using the none driver based on existing profile
2022-06-28T00:57:54.6324036Z Jun 28 00:57:54 * Starting control plane node minikube in cluster minikube
2022-06-28T00:57:54.6867587Z Jun 28 00:57:54 * Restarting existing none bare metal machine for ""minikube"" ...
2022-06-28T00:57:54.6954113Z Jun 28 00:57:54 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:57:55.7715259Z Jun 28 00:57:55 
2022-06-28T00:57:55.7730526Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:57:55.7731022Z stdout:
2022-06-28T00:57:55.7731134Z 
2022-06-28T00:57:55.7731336Z stderr:
2022-06-28T00:57:55.7731822Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:57:55.7732019Z 
2022-06-28T00:57:55.7732211Z * 
2022-06-28T00:57:55.7732693Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:57:55.7733157Z │                                                                                             │
2022-06-28T00:57:55.7733672Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:57:55.7734360Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:57:55.7734861Z │                                                                                             │
2022-06-28T00:57:55.7735418Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:57:55.7735959Z │                                                                                             │
2022-06-28T00:57:55.7736589Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:57:55.7740218Z Jun 28 00:57:55 
2022-06-28T00:57:55.8458528Z E0628 00:57:55.845551  194918 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:57:55.8471612Z Jun 28 00:57:55 
2022-06-28T00:57:55.8477089Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:55.8479071Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:55.8480223Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:55.8482785Z Jun 28 00:57:55 
2022-06-28T00:57:55.9175143Z Jun 28 00:57:55 
2022-06-28T00:57:55.9189731Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:57:55.9192377Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:57:55.9194955Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:57:55.9207163Z Jun 28 00:57:55 
2022-06-28T00:57:55.9233298Z Jun 28 00:57:55 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:58:00.9957446Z Jun 28 00:58:00 
2022-06-28T00:58:00.9963674Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:00.9964966Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:00.9965541Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:00.9973430Z Jun 28 00:58:00 
2022-06-28T00:58:01.0002581Z Jun 28 00:58:00 Starting minikube ...
2022-06-28T00:58:01.0729479Z Jun 28 00:58:01 * minikube v1.26.0 on Ubuntu 20.04
2022-06-28T00:58:01.0790218Z Jun 28 00:58:01 * Using the none driver based on existing profile
2022-06-28T00:58:01.0808505Z Jun 28 00:58:01 * Starting control plane node minikube in cluster minikube
2022-06-28T00:58:01.1067402Z Jun 28 00:58:01 * Restarting existing none bare metal machine for ""minikube"" ...
2022-06-28T00:58:01.1177185Z Jun 28 00:58:01 * OS release is Ubuntu 20.04.4 LTS
2022-06-28T00:58:02.2118290Z Jun 28 00:58:02 
2022-06-28T00:58:02.2134209Z X Exiting due to RUNTIME_ENABLE: sudo systemctl enable cri-docker.socket: exit status 1
2022-06-28T00:58:02.2136970Z stdout:
2022-06-28T00:58:02.2137355Z 
2022-06-28T00:58:02.2137725Z stderr:
2022-06-28T00:58:02.2138686Z Failed to enable unit: Unit file cri-docker.socket does not exist.
2022-06-28T00:58:02.2139016Z 
2022-06-28T00:58:02.2139342Z * 
2022-06-28T00:58:02.2139953Z ╭─────────────────────────────────────────────────────────────────────────────────────────────╮
2022-06-28T00:58:02.2140560Z │                                                                                             │
2022-06-28T00:58:02.2141218Z │    * If the above advice does not help, please let us know:                                 │
2022-06-28T00:58:02.2141916Z │      https://github.com/kubernetes/minikube/issues/new/choose                               │
2022-06-28T00:58:02.2142571Z │                                                                                             │
2022-06-28T00:58:02.2143272Z │    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    │
2022-06-28T00:58:02.2143943Z │                                                                                             │
2022-06-28T00:58:02.2144601Z ╰─────────────────────────────────────────────────────────────────────────────────────────────╯
2022-06-28T00:58:02.2145064Z Jun 28 00:58:02 
2022-06-28T00:58:02.2880448Z E0628 00:58:02.287766  195222 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:02.2892504Z Jun 28 00:58:02 
2022-06-28T00:58:02.2896830Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:02.2899275Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:02.2900544Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:02.2901362Z Jun 28 00:58:02 
2022-06-28T00:58:02.3582770Z Jun 28 00:58:02 
2022-06-28T00:58:02.3587835Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:02.3589055Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:02.3589630Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:02.3599452Z Jun 28 00:58:02 
2022-06-28T00:58:02.3621002Z Jun 28 00:58:02 Command: start_kubernetes_if_not_running failed. Retrying...
2022-06-28T00:58:07.3638880Z Jun 28 00:58:07 Command: start_kubernetes_if_not_running failed 3 times.
2022-06-28T00:58:07.3639385Z Jun 28 00:58:07 Could not start minikube. Aborting...
2022-06-28T00:58:07.3639759Z Jun 28 00:58:07 Debugging failed Kubernetes test:
2022-06-28T00:58:07.3640148Z Jun 28 00:58:07 Currently existing Kubernetes resources
2022-06-28T00:58:07.5490786Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.5922448Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.5945237Z Jun 28 00:58:07 Flink logs:
2022-06-28T00:58:07.6330601Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.6707241Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7097992Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7506609Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7886315Z The connection to the server localhost:8080 was refused - did you specify the right host or port?
2022-06-28T00:58:07.7896794Z Jun 28 00:58:07 Stopping minikube ...
2022-06-28T00:58:07.8560167Z E0628 00:58:07.855722  195270 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:07.8564021Z E0628 00:58:07.856252  195270 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:07.8583780Z Jun 28 00:58:07 
2022-06-28T00:58:07.8595235Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:07.8596811Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:07.8597431Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:07.8605881Z Jun 28 00:58:07 
2022-06-28T00:58:07.8632037Z Jun 28 00:58:07 Command: minikube stop failed. Retrying...
2022-06-28T00:58:12.9320992Z E0628 00:58:12.931814  195281 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:12.9323097Z E0628 00:58:12.932141  195281 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:12.9346464Z Jun 28 00:58:12 
2022-06-28T00:58:12.9356598Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:12.9359608Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:12.9361856Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:12.9370436Z Jun 28 00:58:12 
2022-06-28T00:58:12.9401892Z Jun 28 00:58:12 Command: minikube stop failed. Retrying...
2022-06-28T00:58:18.0093241Z E0628 00:58:18.009042  195290 root.go:88] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
2022-06-28T00:58:18.0094620Z E0628 00:58:18.009278  195290 cloud_events.go:60] unable to write to /home/vsts/.minikube/profiles/minikube/events.json: open /home/vsts/.minikube/profiles/minikube/events.json: permission denied
2022-06-28T00:58:18.0105801Z Jun 28 00:58:18 
2022-06-28T00:58:18.0109189Z X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
2022-06-28T00:58:18.0112493Z * Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
2022-06-28T00:58:18.0114979Z * Related issue: https://github.com/kubernetes/minikube/issues/9165
2022-06-28T00:58:18.0119419Z Jun 28 00:58:18 
2022-06-28T00:58:18.0139916Z Jun 28 00:58:18 Command: minikube stop failed. Retrying...
2022-06-28T00:58:23.0155447Z Jun 28 00:58:23 Command: minikube stop failed 3 times.
2022-06-28T00:58:23.0156414Z Jun 28 00:58:23 Could not stop minikube. Aborting...
2022-06-28T00:58:23.0159072Z Jun 28 00:58:23 [FAIL] Test script contains errors.
2022-06-28T00:58:23.0166657Z Jun 28 00:58:23 Checking for errors...
2022-06-28T00:58:23.0402412Z Jun 28 00:58:23 No errors in log files.
2022-06-28T00:58:23.0404271Z Jun 28 00:58:23 Checking for exceptions...
2022-06-28T00:58:23.0675070Z Jun 28 00:58:23 No exceptions in log files.
2022-06-28T00:58:23.0677372Z Jun 28 00:58:23 Checking for non-empty .out files...
2022-06-28T00:58:23.0700681Z grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
2022-06-28T00:58:23.0706900Z Jun 28 00:58:23 No non-empty .out files.
2022-06-28T00:58:23.0707516Z Jun 28 00:58:23 
2022-06-28T00:58:23.0708309Z Jun 28 00:58:23 [FAIL] 'Run Kubernetes test' failed after 0 minutes and 44 seconds! Test exited with exit code 1
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4975",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29671,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 29 16:55:40 UTC 2022,,,,,,,,,,"0|z16eqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 03:26;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559;;;","28/Jun/22 09:03;martijnvisser;A quick Google search turned me to https://github.com/kubernetes/minikube/issues/14403 - My assumption is that Azure has had an update which is now causing this error;;;","28/Jun/22 17:38;martijnvisser;[~yangwang166] [~rmetzger] I've been doing some investigation on this but I do have one question: Do you know if it was it a deliberate decision to use the none driver over the other existing ones?;;;","29/Jun/22 02:39;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37302&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 02:43;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37306&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 02:44;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37313&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 02:46;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37327&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c;;;","29/Jun/22 02:47;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37328&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 02:47;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37329&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 06:08;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37332&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 06:26;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37334&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","29/Jun/22 07:31;rmetzger;[~martijnvisser] afaik [~yangwang166] introduced this code originally.
I assume the ""none"" driver was used for performance / efficiency reasons. Most of the disadvantages listed https://minikube.sigs.k8s.io/docs/drivers/none/ don't apply to CI environments.;;;","29/Jun/22 09:13;martijnvisser;Thanks for the info [~rmetzger];;;","29/Jun/22 16:55;martijnvisser;Fixed in
master: aa26f46bf3908d7db58e3a822246b54f2bd9eece
release-1.15: 82953560041636f397bc30d47d2312a00349b2b5
release-1.14: d101a8a755db611213eff77db9ea385cae636387;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch6SinkE2ECase failed with no space left on device,FLINK-28268,13468770,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Problem,martijnvisser,hxbks2ks,hxbks2ks,28/Jun/22 02:53,28/Jun/22 19:06,04/Jun/24 20:42,28/Jun/22 19:06,1.16.0,,,,,,,,Connectors / ElasticSearch,,,,,,,0,test-stability,,,,"
{code:java}
2022-06-27T14:35:05.6850998Z Jun 27 14:35:05 [ERROR] org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase  Time elapsed: 54.613 s  <<< ERROR!
2022-06-27T14:35:05.6851516Z Jun 27 14:35:05 java.lang.RuntimeException: Failed to build JobManager image
2022-06-27T14:35:05.6852121Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.buildJobManagerContainer(FlinkContainersBuilder.java:215)
2022-06-27T14:35:05.6852837Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.build(FlinkContainersBuilder.java:181)
2022-06-27T14:35:05.6853675Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.<init>(FlinkContainerTestEnvironment.java:83)
2022-06-27T14:35:05.6854487Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.FlinkContainerTestEnvironment.<init>(FlinkContainerTestEnvironment.java:62)
2022-06-27T14:35:05.6855176Z Jun 27 14:35:05 	at org.apache.flink.streaming.tests.ElasticsearchSinkE2ECaseBase.<init>(ElasticsearchSinkE2ECaseBase.java:58)
2022-06-27T14:35:05.6855857Z Jun 27 14:35:05 	at org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase.<init>(Elasticsearch6SinkE2ECase.java:36)
2022-06-27T14:35:05.6856431Z Jun 27 14:35:05 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-06-27T14:35:05.6856998Z Jun 27 14:35:05 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-06-27T14:35:05.6857653Z Jun 27 14:35:05 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-06-27T14:35:05.6858250Z Jun 27 14:35:05 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2022-06-27T14:35:05.6858817Z Jun 27 14:35:05 	at org.junit.platform.commons.util.ReflectionUtils.newInstance(ReflectionUtils.java:550)
2022-06-27T14:35:05.6859429Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.ConstructorInvocation.proceed(ConstructorInvocation.java:56)
2022-06-27T14:35:05.6860144Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-06-27T14:35:05.6860896Z Jun 27 14:35:05 	at org.junit.jupiter.api.extension.InvocationInterceptor.interceptTestClassConstructor(InvocationInterceptor.java:73)
2022-06-27T14:35:05.6861586Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-06-27T14:35:05.6862300Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-06-27T14:35:05.6863038Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-06-27T14:35:05.6863743Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-06-27T14:35:05.6864429Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-06-27T14:35:05.6865061Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-06-27T14:35:05.6865667Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:77)
2022-06-27T14:35:05.6866345Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestClassConstructor(ClassBasedTestDescriptor.java:355)
2022-06-27T14:35:05.6867072Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateTestClass(ClassBasedTestDescriptor.java:302)
2022-06-27T14:35:05.6867776Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassTestDescriptor.instantiateTestClass(ClassTestDescriptor.java:79)
2022-06-27T14:35:05.6868508Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:280)
2022-06-27T14:35:05.6869344Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:272)
2022-06-27T14:35:05.6869953Z Jun 27 14:35:05 	at java.util.Optional.orElseGet(Optional.java:267)
2022-06-27T14:35:05.6870551Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:271)
2022-06-27T14:35:05.6871271Z Jun 27 14:35:05 	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
2022-06-27T14:35:05.6871960Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$before$2(ClassBasedTestDescriptor.java:197)
2022-06-27T14:35:05.6872701Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6873374Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:196)
2022-06-27T14:35:05.6874033Z Jun 27 14:35:05 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)
2022-06-27T14:35:05.6874720Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
2022-06-27T14:35:05.6875392Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6876068Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-27T14:35:05.6876708Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-27T14:35:05.6877331Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-27T14:35:05.6878016Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6878803Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-27T14:35:05.6879443Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-27T14:35:05.6880209Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-27T14:35:05.6881108Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-06-27T14:35:05.6881906Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-27T14:35:05.6882578Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6883261Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-27T14:35:05.6883890Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-27T14:35:05.6884511Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-27T14:35:05.6885186Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-27T14:35:05.6885841Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-27T14:35:05.6886477Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-27T14:35:05.6887415Z Jun 27 14:35:05 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-27T14:35:05.6888135Z Jun 27 14:35:05 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-06-27T14:35:05.6888663Z Jun 27 14:35:05 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-27T14:35:05.6889201Z Jun 27 14:35:05 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-27T14:35:05.6889747Z Jun 27 14:35:05 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-27T14:35:05.6890288Z Jun 27 14:35:05 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-27T14:35:05.6891296Z Jun 27 14:35:05 Caused by: org.apache.flink.tests.util.flink.container.ImageBuildException: Failed to build image ""flink-dist-configured-jobmanager""
2022-06-27T14:35:05.6891969Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkImageBuilder.build(FlinkImageBuilder.java:204)
2022-06-27T14:35:05.6892675Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkContainersBuilder.buildJobManagerContainer(FlinkContainersBuilder.java:213)
2022-06-27T14:35:05.6893195Z Jun 27 14:35:05 	... 56 more
2022-06-27T14:35:05.6894128Z Jun 27 14:35:05 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout:  stderr: write /flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device
2022-06-27T14:35:05.6894894Z Jun 27 14:35:05 	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
2022-06-27T14:35:05.6895444Z Jun 27 14:35:05 	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
2022-06-27T14:35:05.6895980Z Jun 27 14:35:05 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)
2022-06-27T14:35:05.6896566Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:222)
2022-06-27T14:35:05.6897235Z Jun 27 14:35:05 	at org.apache.flink.tests.util.flink.container.FlinkImageBuilder.build(FlinkImageBuilder.java:179)
2022-06-27T14:35:05.6897697Z Jun 27 14:35:05 	... 57 more
2022-06-27T14:35:05.6898551Z Jun 27 14:35:05 Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout:  stderr: write /flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device
2022-06-27T14:35:05.6899343Z Jun 27 14:35:05 	at com.github.dockerjava.api.command.BuildImageResultCallback.getImageId(BuildImageResultCallback.java:78)
2022-06-27T14:35:05.6900029Z Jun 27 14:35:05 	at com.github.dockerjava.api.command.BuildImageResultCallback.awaitImageId(BuildImageResultCallback.java:50)
2022-06-27T14:35:05.6900686Z Jun 27 14:35:05 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:147)
2022-06-27T14:35:05.6901316Z Jun 27 14:35:05 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:40)
2022-06-27T14:35:05.6901907Z Jun 27 14:35:05 	at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17)
2022-06-27T14:35:05.6902431Z Jun 27 14:35:05 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39)
2022-06-27T14:35:05.6902929Z Jun 27 14:35:05 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-06-27T14:35:05.6903465Z Jun 27 14:35:05 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-06-27T14:35:05.6904049Z Jun 27 14:35:05 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-06-27T14:35:05.6904549Z Jun 27 14:35:05 	at java.lang.Thread.run(Thread.java:750)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37250&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28263,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 03:04:19 UTC 2022,,,,,,,,,,"0|z16eps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 02:54;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37249&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","28/Jun/22 03:04;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37240&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSourceLegacyITCase.testBrokerFailure hangs,FLINK-28267,13468767,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,renqs,zhuzh,zhuzh,28/Jun/22 02:45,16/Oct/23 07:20,04/Jun/24 20:42,,1.16.0,,,,,,,,Connectors / Kafka,,,,,,,0,,,,,"""main"" #1 prio=5 os_prio=0 tid=0x00007f5ae000b800 nid=0x22c2 waiting on condition [0x00007f5ae8398000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000a5565010> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:67)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runBrokerFailureTest(KafkaConsumerTestBase.java:1506)
	at org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase.testBrokerFailure(KafkaSourceLegacyITCase.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37247&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=41526",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28453,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 16 07:20:12 UTC 2023,,,,,,,,,,"0|z16ep4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 02:46;zhuzh;Similar to FLINK-26387 which was fixed in 1.15.0.
[~renqs] would you take a look?;;;","05/Jul/22 02:07;lincoln.86xy;It seems to appear frequently:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37431&view=logs&j=219f6d90-20a2-5863-7c1b-c80377a1018f&t=20186858-1485-5059-c9c6-446952519524]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37579&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203

 

 ;;;","05/Jul/22 18:39;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37653&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&s=ae4f8708-9994-57d3-c2d7-b892156e7812;;;","06/Jul/22 02:02;fsk119;One more case: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37665&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","07/Jul/22 12:06;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37772&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=38945

CC [~renqs];;;","12/Jul/22 06:35;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=41593;;;","13/Jul/22 02:05;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","15/Jul/22 03:20;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38194&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","19/Jul/22 02:56;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38347&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","19/Jul/22 02:56;hxbks2ks;Hi [~renqs] Any updates on the progress？;;;","19/Jul/22 02:57;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38349&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","19/Jul/22 06:26;renqs;There aren't many clues from the log... I'm trying to reproduce locally and will dig deeper these days.;;;","25/Jul/22 16:13;chesnay;Anyone opposed to disabling this test temporarily? It unnecessarily slows down CI for unrelated PRs.;;;","26/Jul/22 10:50;chesnay;Disabled in a87808108614ef7b1d40d0e994c2683b272e41d5.;;;","16/Oct/23 07:20;martijnvisser;Seems to be activated again the apache/flink-kafka-connector repo, see https://github.com/apache/flink-connector-kafka/actions/runs/6520764104/job/17708646991;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka connector fails: Invalid negative offset,FLINK-28266,13468640,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tashoyan,tashoyan,27/Jun/22 15:36,28/Jun/22 08:24,04/Jun/24 20:42,28/Jun/22 08:24,1.15.0,,,,,,,,Connectors / Kafka,,,,,,,0,,,,,"The failure occurs when reading from a Kafka topic using new KafkaSource API.

Configure KafkaSource with starting offsets set to a timestamp. This timestamp should be big enough, so all records in the Kafka topic have their timestamps smaller.
{code:scala}
val kafkaSource = KafkaSource.builder()
        .setTopics(topic)
        .setBootstrapServers(bootstrapServers)
        .setStartingOffsets(OffsetsInitializer.timestamp(VERY_BIG_VALUE))
{code}
KafkaSource finds none records in the Kafka topic that have timestamps bigger than this configured timestamp. In this case we observe the failure:

*IllegalArgumentException: Invalid negative offset*

Full stack trace:
{code:none}
org.apache.flink.util.FlinkRuntimeException: Failed to initialize partition splits due to
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.handlePartitionSplitChanges(KafkaSourceEnumerator.java:299) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83) ~[flink-runtime-1.15.0.jar:1.15.0]
	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40) [flink-core-1.15.0.jar:1.15.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_332]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_332]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_332]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_332]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_332]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_332]
Caused by: java.lang.IllegalArgumentException: Invalid negative offset
	at org.apache.kafka.clients.consumer.OffsetAndTimestamp.<init>(OffsetAndTimestamp.java:36) ~[kafka-clients-2.8.1.jar:?]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.lambda$offsetsForTimes$8(KafkaSourceEnumerator.java:622) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ~[?:1.8.0_332]
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ~[?:1.8.0_332]
	at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1723) ~[?:1.8.0_332]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_332]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_332]
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_332]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_332]
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566) ~[?:1.8.0_332]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.offsetsForTimes(KafkaSourceEnumerator.java:615) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.kafka.source.enumerator.initializer.TimestampOffsetsInitializer.getPartitionOffsets(TimestampOffsetsInitializer.java:57) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.initializePartitionSplits(KafkaSourceEnumerator.java:272) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.lambda$checkPartitionChanges$0(KafkaSourceEnumerator.java:242) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80) ~[flink-runtime-1.15.0.jar:1.15.0]
{code}

A simple reproducer based on  [embedded-kafka|https://github.com/embeddedkafka/embedded-kafka] is attached.

Note: the bug FLINK-20114 is similar but different.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28185,,,,,,,,,,,,,FLINK-20114,,,,,,,"27/Jun/22 15:36;tashoyan;NegativeOffsetSpec.scala;https://issues.apache.org/jira/secure/attachment/13045675/NegativeOffsetSpec.scala",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 08:24:53 UTC 2022,,,,,,,,,,"0|z16eb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 07:38;mason6345;[~tashoyan], it seems that the issue you face is a duplicate of https://issues.apache.org/jira/browse/FLINK-28185;;;","28/Jun/22 08:24;tashoyan;Duplicate of FLINK-28185;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistency in Kubernetes HA service: broken state handle,FLINK-28265,13468587,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,rmetzger,rmetzger,27/Jun/22 12:29,27/Aug/22 06:58,04/Jun/24 20:42,27/Aug/22 06:56,1.14.4,,,,,1.15.3,1.16.0,,Deployment / Kubernetes,Runtime / Coordination,,,,,,0,pull-request-available,,,,"I have a JobManager, which at some point failed to acknowledge a checkpoint:

{code}
Error while processing AcknowledgeCheckpoint message
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete the pending checkpoint 193393. Failure reason: Failure to finalize checkpoint.
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1255)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1100)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.runtime.persistence.StateHandleStore$AlreadyExistException: checkpointID-0000000000000193393 already exists in ConfigMap cm-00000000000000000000000000000000-jobmanager-leader
	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.getKeyAlreadyExistException(KubernetesStateHandleStore.java:534)
	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.lambda$addAndLock$0(KubernetesStateHandleStore.java:155)
	at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$attemptCheckAndUpdateConfigMap$11(Fabric8FlinkKubeClient.java:316)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source)
	... 3 common frames omitted
{code}

the JobManager creates subsequent checkpoints successfully.
Upon failure, it tries to recover this checkpoint (0000000000000193393), but fails to do so because of:
{code}
Caused by: org.apache.flink.util.FlinkException: Could not retrieve checkpoint 193393 from state handle under checkpointID-0000000000000193393. This indicates that the retrieved state handle is broken. Try cleaning the state handle store ... Caused by: java.io.FileNotFoundException: No such file or directory: s3://xxx/flink-ha/xxx/completedCheckpoint72e30229420c
{code}

I'm running Flink 1.14.4.

Note: This issue has been first discussed here: https://github.com/apache/flink/pull/15832#pullrequestreview-1005973050 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25098,FLINK-29105,,,,,,"30/Jun/22 11:03;cymau;flink_checkpoint_issue.txt;https://issues.apache.org/jira/secure/attachment/13046087/flink_checkpoint_issue.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Aug 27 06:56:34 UTC 2022,,,,,,,,,,"0|z16dzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 12:31;rmetzger;[~mapohl] I'm trying to get the full JobManager logs ... but they might have been gone by now, due to the retention of our log management system.

UPDATE: the log is indeed gone. I will properly collect the logs next time this happens.;;;","27/Jun/22 12:32;rmetzger;We don't have a reliable way of reproducing this issue yet.
But another user reported something similar in: https://github.com/apache/flink/pull/15832#issuecomment-1167167537;;;","29/Jun/22 07:26;wangyang0918;Thanks [~rmetzger] for sharing this issue.

 

I am curious why Flink still recovers from the checkpoint (0000000000000193393) if JobManager creates subsequent checkpoints successfully. It should pick the latest the successful checkpoint.;;;","29/Jun/22 07:36;rmetzger;Yes, I was wondering exactly the same. I've setup some internal monitoring to catch if this situation occurs again. I'll then share the logs.;;;","30/Jun/22 11:03;cymau;[^flink_checkpoint_issue.txt];;;","30/Jun/22 11:14;martijnvisser;[~cymau] That log doesn't relates to this issue, since this issue is strictly about the Kubernetes tests that we are running. Your log are Flink logs;;;","01/Jul/22 02:48;cymau;[~martijnvisser] We had another incident where we hit this issue and uploaded the full JobManager log: 

Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Could not retrieve checkpoint 9702 from state handle under checkpointID-0000000000000009702. This indicates that the retrieved state handle is broken. Try cleaning the state handle store.

Please ignore if not related to this issue.;;;","01/Jul/22 07:17;martijnvisser;[~cymau] Like I said, this can't be related to this ticket. This ticket is about a test instability on the Flink source code. This test isn't included in the actual released Flink version. If you have a problem, I recommend to post this to the User mailing list, Stackoverflow or Slack per https://flink.apache.org/gettinghelp.html;;;","11/Jul/22 10:18;EnriqueL8;Hi [~martijnvisser] , 

I understand that the original ticket was created due to a test instability but the test is highlighting the fact that there is a problem in the flow that results in a broken state. What [~cymau] is saying and that I original flagged is that we are seeing the same type of behaviour in the field. I'm happy to raise a separate ticket to track the same problem occurring in the field but it's the exact same behaviour. All the test is doing which is great is highlighting that there is a problem.

Thanks,
Enrique;;;","11/Jul/22 11:12;martijnvisser;[~EnriqueL8] It feels like my comment is misplaced on this Jira ticket, because I believe I made that comment when I was talking about FLINK-28269. I've made a mistake or some comments were removed. 

Regarding the provided logs, are there also logs available of the previous runs? Are there also taskmanager logs available? CC [~cymau]

[~wangyang0918] [~mapohl] What are your thoughts on this? ;;;","13/Jul/22 02:44;wangyang0918;Since we always add the new checkpoint first and then subsume the oldest one, I am curious how it could happen we only have one checkpoint which is invalid. If adding the new checkpoint failed, we should have the old successful checkpoint. On the contrary, if subsuming the oldest one failed, we should still have the newly added checkpoint.

 

Could you please verify the checkpoint 9701 or 9703 exists on the S3?

 

I believe the the logs of previous run(e.g. kubectl logs <pod-name> --previous) and the Kubernetes APIServer audit log will help a lot to debug the root cause.;;;","13/Jul/22 10:27;mapohl;I looked into the {{EOFException}} generation for [~cymau] case. Based on the stacktrace, we can assume that the file was empty:
{code}
Caused by: java.io.EOFException
	at java.io.ObjectInputStream$PeekInputStream.readFully(Unknown Source) ~[?:?]
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(Unknown Source) ~[?:?]
	at java.io.ObjectInputStream.readStreamHeader(Unknown Source) ~[?:?]
	at java.io.ObjectInputStream.<init>(Unknown Source) ~[?:?]
	at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.<init>(InstantiationUtil.java:66) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:613) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:593) ~[flink-dist_2.11-1.13.2.jar:1.13.2]
[...]
{code}
I'm wondering whether there is some external process that ran that cleaned up files? Because usually, we only delete files but don't change the content. Another issue I could think of is that some network issue (since we're accessing some (maybe distributed) FileSystem) resulted in the InputStream for the deserialization not to contain any data. But I would have thought that this error case would result in a different {{IOException}}.

In contrast, the other appearances were caused by the RetrievableStateHandle file being entirely deleted (i.e. discarded) which is a behavior I would rather expect if we have a bug in Flink.;;;","25/Jul/22 02:24;wangyang0918;I want to share some progress about this ticket. The root cause might be we should not discard the state when coming across {{AlreadyExistException}} in {{{}KubernetesStateHandleStore#addAndLock{}}}.

If something is temporarily wrong with the JobManager network, {{Fabric8FlinkKubeClient#checkAndUpdateConfigMap}} failed with {{KubernetesException}} in the first run and retried again. However, the http request is actually sent successfully and handled by the K8s APIServer, which means the entry was added to the ConfigMap. This will cause the second retry fails with {{AlreadyExistException}} and then discard the state. If the JobManager crashed exactly, it will throw the {{FileNotFoundException: No such file or directory: s3://xxx/flink-ha/xxx/completedCheckpoint72e30229420c}} in the following attempts since added entry is not cleaned up.

 

 ;;;","16/Aug/22 08:32;wangyang0918;I will work on this ticket.;;;","27/Aug/22 06:56;wangyang0918;Fixed via:

master(1.16): aae96d0c9d1768c396bdf2ee6510677fbb8f317a

release-1.15: fcff4903c8d625edb8f4e33b03bfded52c3deba8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor split and read in table store,FLINK-28264,13468576,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/Jun/22 12:00,28/Jun/22 05:32,04/Jun/24 20:42,28/Jun/22 05:32,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Refactor split and read, for better Hybrid reading. The full and incremental read paths should be different, otherwise it is easy to get bugs.

* Add boolean isIncremental to Split
* Use Split in Flink Source Split and Hive Split
* FileStoreRead.createReader(Split)
* Create ConcatRecordReader or MergeTreeReader by Split.isIncremental
* Do key projection in KeyValueFileStoreRead.createReader",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28244,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 05:32:58 UTC 2022,,,,,,,,,,"0|z16dww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 05:32;lzljs3620320;master: 08f8d9bf664fbfa7a1f0239ccce6163d88361c0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPC-DS Bash e2e tests don't clean-up after completing,FLINK-28263,13468574,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,martijnvisser,martijnvisser,27/Jun/22 11:56,12/Aug/22 12:29,04/Jun/24 20:42,28/Jun/22 19:05,1.16.0,,,,,1.14.6,1.15.2,1.16.0,Tests,,,,,,,0,pull-request-available,test-stability,,,"When debugging the disk space usage for the e2e tests, the top 20 folders with the largest file size are:

{code:java}
2022-06-27T09:32:59.8000587Z Jun 27 09:32:59 List top 20 directories with largest file size
2022-06-27T09:33:00.9811803Z Jun 27 09:33:00 4088524	.
2022-06-27T09:33:00.9813428Z Jun 27 09:33:00 1277080	./flink-end-to-end-tests
2022-06-27T09:33:00.9814324Z Jun 27 09:33:00 624512	./flink-dist
2022-06-27T09:33:00.9815152Z Jun 27 09:33:00 624124	./flink-dist/target
2022-06-27T09:33:00.9816093Z Jun 27 09:33:00 500032	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-06-27T09:33:00.9817429Z Jun 27 09:33:00 500028	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-06-27T09:33:00.9818167Z Jun 27 09:33:00 486412	./.git
2022-06-27T09:33:00.9819096Z Jun 27 09:33:00 479416	./.git/objects
2022-06-27T09:33:00.9819512Z Jun 27 09:33:00 479408	./.git/objects/pack
2022-06-27T09:33:00.9820584Z Jun 27 09:33:00 461456	./flink-connectors
2022-06-27T09:33:00.9821403Z Jun 27 09:33:00 449832	./.git/objects/pack/pack-0bdd9e3186d0cb404910c5843d19b5cb80b84fe0.pack
2022-06-27T09:33:00.9821992Z Jun 27 09:33:00 349236	./flink-table
2022-06-27T09:33:00.9822631Z Jun 27 09:33:00 293008	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/opt
2022-06-27T09:33:00.9823233Z Jun 27 09:33:00 251272	./flink-filesystems
2022-06-27T09:33:00.9823818Z Jun 27 09:33:00 246588	./flink-end-to-end-tests/flink-streaming-kinesis-test
2022-06-27T09:33:00.9824502Z Jun 27 09:33:00 246464	./flink-end-to-end-tests/flink-streaming-kinesis-test/target
2022-06-27T09:33:00.9825210Z Jun 27 09:33:00 196656	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/lib
2022-06-27T09:33:00.9825966Z Jun 27 09:33:00 184364	./flink-end-to-end-tests/flink-streaming-kinesis-test/target/KinesisExample.jar
2022-06-27T09:33:00.9826652Z Jun 27 09:33:00 156136	./flink-end-to-end-tests/flink-tpcds-test
2022-06-27T09:33:00.9827284Z Jun 27 09:33:00 151180	./flink-end-to-end-tests/flink-tpcds-test/target
{code}

See https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2732&view=logs&j=0e31ee24-31a6-528c-a4bf-45cde9b2a14e&t=ff03a8fa-e84e-5199-efb2-5433077ce8e2&l=5093

After running {{TPC-DS end-to-end test}} and after the clean-up, the following directories are listed in the top 20:

{code:java}
2022-06-27T09:49:51.7694429Z Jun 27 09:49:51 List top 20 directories with largest file size AFTER cleaning temorary folders and files
2022-06-27T09:49:52.9617221Z Jun 27 09:49:52 5315996	.
2022-06-27T09:49:52.9618830Z Jun 27 09:49:52 2504556	./flink-end-to-end-tests
2022-06-27T09:49:52.9619848Z Jun 27 09:49:52 1383612	./flink-end-to-end-tests/flink-tpcds-test
2022-06-27T09:49:52.9620796Z Jun 27 09:49:52 1378656	./flink-end-to-end-tests/flink-tpcds-test/target
2022-06-27T09:49:52.9621730Z Jun 27 09:49:52 1223944	./flink-end-to-end-tests/flink-tpcds-test/target/table
2022-06-27T09:49:52.9622844Z Jun 27 09:49:52 624508	./flink-dist
2022-06-27T09:49:52.9623585Z Jun 27 09:49:52 624120	./flink-dist/target
2022-06-27T09:49:52.9624398Z Jun 27 09:49:52 500028	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-06-27T09:49:52.9625366Z Jun 27 09:49:52 500024	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-06-27T09:49:52.9625994Z Jun 27 09:49:52 486412	./.git
2022-06-27T09:49:52.9626514Z Jun 27 09:49:52 479416	./.git/objects
2022-06-27T09:49:52.9631740Z Jun 27 09:49:52 479408	./.git/objects/pack
2022-06-27T09:49:52.9632755Z Jun 27 09:49:52 461456	./flink-connectors
2022-06-27T09:49:52.9633717Z Jun 27 09:49:52 449832	./.git/objects/pack/pack-0bdd9e3186d0cb404910c5843d19b5cb80b84fe0.pack
2022-06-27T09:49:52.9634769Z Jun 27 09:49:52 379348	./flink-end-to-end-tests/flink-tpcds-test/target/table/store_sales.dat
2022-06-27T09:49:52.9635596Z Jun 27 09:49:52 349236	./flink-table
2022-06-27T09:49:52.9636489Z Jun 27 09:49:52 293008	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/opt
2022-06-27T09:49:52.9637526Z Jun 27 09:49:52 288980	./flink-end-to-end-tests/flink-tpcds-test/target/table/catalog_sales.dat
2022-06-27T09:49:52.9638378Z Jun 27 09:49:52 251272	./flink-filesystems
2022-06-27T09:49:52.9639238Z Jun 27 09:49:52 246588	./flink-end-to-end-tests/flink-streaming-kinesis-test
{code}

See https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2732&view=logs&j=0e31ee24-31a6-528c-a4bf-45cde9b2a14e&t=ff03a8fa-e84e-5199-efb2-5433077ce8e2&l=5708

This results in not enough disk space errors during various runs further downstream. This test should also properly clean-up its files",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28268,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 19:05:04 UTC 2022,,,,,,,,,,"0|z16dwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 19:05;martijnvisser;Merged in:
master: 5d94fe6c128f2fa9e3531ab8b169e8a2e21bc391
release-1.15: 7557a282314b29808ea5552aaf69134a7ebf9d08
release-1.14: 852713c52e1c4ce8d27905f5d3dedaed2015d4eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The select result does not match the sink table,FLINK-28262,13468566,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,simen,simen,27/Jun/22 11:05,14/Jul/22 08:47,04/Jun/24 20:42,,1.14.0,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,,,,0,,,,,"sql is as follows:
{code:java}
CREATE TABLE source
(
    int_one INT,
    int_two INT
) WITH (
      'connector' = 'datagen'
      ,'rows-per-second' = '1'
      ,'fields.int_one.min' = '1'
      ,'fields.int_one.max' = '1'
      ,'fields.int_two.min' = '2'
      ,'fields.int_two.max' = '2'
      );
CREATE TABLE sink
(
    int_one0 INT,
    int_two0 INT
) WITH (
      'connector' = 'print'
      );
insert into sink
select
       int_two as int_two0,  -- The result I want is to write to the int_two0 field of the sink table
       int_one as int_one0   -- The result I want is to write to the int_one0 field of the sink table
from source;
{code}
 

The result is as follows:
{code:java}
+I[2, 1]
+I[2, 1]
+I[2, 1]{code}
expected outcome:
{code:java}
+I[1, 2] 
+I[1, 2] 
+I[1, 2]{code}
The problem seems to be here, I want query result and result table to be mapped by field name instead of location:
{code:java}
select
       int_two as int_two0, 
       int_one as int_one0 
from source;{code}
Will this issue be fixed in a later version?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 08:47:16 UTC 2022,,,,,,,,,,"0|z16duo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 11:06;simen;[~jark] can you take a look at it？;;;","28/Jun/22 03:06;Leo Zhou;From what I understand, `int_two as int_two0` in Select clause has nothing to do with field mapping. If you want specify the mapping by name instead of location, you can do it like that: `insert into sink (...) select ...`;;;","14/Jul/22 08:47;Zsigner;[~simen] ，I agree with [~Leo Zhou] 's point of view, the grammar of this piece can be referred to here（[INSERT Statement|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/sql/insert/]）
{code:java}
[EXECUTE] INSERT { INTO | OVERWRITE } [catalog_name.][db_name.]table_name [PARTITION part_spec] [column_list] select_statement

part_spec:
  (part_col_name1=val1 [, part_col_name2=val2, ...])

column_list:
  (col_name1 [, column_name2, ...]) {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider Using Dependent Resources for Ingress,FLINK-28261,13468562,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,morhidi,,27/Jun/22 10:41,31/Aug/23 15:44,04/Jun/24 20:42,,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,"JOSDK 3 introduced the concept of dependent resources, which would allow us to handle Ingress a more JOSDK native way: see [https://javaoperatorsdk.io/docs/dependent-resources#standalone-dependent-resources.] This functionality could be a good fit for standalone mode implementation too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-27 10:41:54.0,,,,,,,,,,"0|z16dts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-runtime-web fails to execute ""npm ci"" on Apple Silicon (arm64) without rosetta",FLINK-28260,13468528,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rmetzger,rmetzger,27/Jun/22 08:39,31/May/24 08:09,04/Jun/24 20:42,,,,,,,,,,Build System,Runtime / Web Frontend,,,,,,0,,,,,"Flink 1.16-SNAPSHOT fails to build in the flink-runtime-web project because we are using an outdated frontend-maven-plugin (v 1.11.3).
This is the error:
{code}
[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.11.3:npm (npm install) on project flink-runtime-web: Failed to run task: 'npm ci --cache-max=0 --no-save ${npm.proxy}' failed. java.io.IOException: Cannot run program ""/Users/rmetzger/Projects/flink/flink-runtime-web/web-dashboard/node/node"" (in directory ""/Users/rmetzger/Projects/flink/flink-runtime-web/web-dashboard""): error=86, Bad CPU type in executable -> [Help 1]
{code}

Using the latest frontend-maven-plugin (v. 1.12.1) properly passes the build, because this version downloads the proper arm64 npm version. However, frontend-maven-plugin 1.12.1 requires Maven 3.6.0, which is too high for Flink (highest mvn version for Flink is 3.2.5).

The best workaround is using rosetta on M1 Macs.",,,,,,,,,,,FLINK-28016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-27 08:39:31.0,,,,,,,,,,"0|z16dm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-parquet doesn't compile on M1 mac without rosetta,FLINK-28259,13468525,13437160,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,27/Jun/22 08:22,30/Jun/22 08:23,04/Jun/24 20:42,30/Jun/22 08:23,1.16.0,,,,,1.16.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,"Compiling Flink 1.16-SNAPSHOT fails on an M1 Mac (apple silicon) without the rosetta translation layer, because the automatically downloaded ""protoc-3.17.3-osx-aarch_64.exe"" file is actually just a copy of ""protoc-3.17.3-osx-x86_64.exe"". (as you can read here: https://github.com/os72/protoc-jar/issues/93)

This is the error:
{code}
[ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.1:test-compile (default) on project flink-parquet: An error occurred while invoking protoc. Error while executing process. Cannot run program ""/Users/rmetzger/Projects/flink/flink-formats/flink-parquet/target/protoc-plugins/protoc-3.17.3-osx-aarch_64.exe"": error=86, Bad CPU type in executable -> [Help 1]
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 08:23:34 UTC 2022,,,,,,,,,,"0|z16dlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 08:22;rmetzger;Prototc 3.21.2 provides a proper aarch64 binary. I'll raise a PR to bump the version.;;;","30/Jun/22 08:23;martijnvisser;Fixed in master: f916bcf82a437740ec96e6bfddfd609a0d8fc31b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ExecutionHistory to host historical executions for each execution vertex,FLINK-28258,13468516,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,27/Jun/22 07:51,28/Jun/22 04:07,04/Jun/24 20:42,28/Jun/22 04:07,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"With speculative execution, tracking prior executions in an {{EvictingBoundedList}} does not work. This is because when using {{EvictingBoundedList}} relies on the assumption that the historical executions are added in ascending order of attempt number successively. This is no longer true if speculative execution is enabled. e.g. 3 speculative execution attempts #1, #2, #3 are running concurrently, later #3 failed, and then #1 failed, and execution attempt #2 keeps running.
The broken assumption may result in exceptions in REST, job archiving and so on.
We propose to introduce an {{ExecutionHistory}} to replace {{EvictingBoundedList}}. It hosts the historical executions in a {{LinkedHashMap}} with a size bound. When the map grows beyond the size bound, elements are dropped from the head of the map (FIFO order).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 04:07:14 UTC 2022,,,,,,,,,,"0|z16djk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 04:07;zhuzh;Done via 8540b8d351025ba773efe0015968fed12a3673a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace ValueKind to RowKind in table store,FLINK-28257,13468498,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/Jun/22 06:56,28/Jun/22 03:01,04/Jun/24 20:42,28/Jun/22 03:01,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"In FLINK-28244
We need store changelog of a table, the row kind should contains UPDATE_BEFORE and UPDATE_AFTER, but if we use ValueKind, it dose not contain UPDATE kinds.
We can just use RowKind.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28244,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 03:01:19 UTC 2022,,,,,,,,,,"0|z16dfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 03:01;lzljs3620320;master: 515d1f99b27b1ec9569c025ca59d91bdf8545b54;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move the write and prepareCommit logic of AbstractTableWrite to FileStoreWrite,FLINK-28256,13468482,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,lzljs3620320,lzljs3620320,27/Jun/22 04:13,24/Oct/22 02:11,04/Jun/24 20:42,24/Oct/22 02:11,,,,,,table-store-0.3.0,,,Table Store,,,,,,,0,pull-request-available,,,,TableWrite can be a simple key value converter wrapper build on FileStoreWrite.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 24 02:11:08 UTC 2022,,,,,,,,,,"0|z16dc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/22 02:11;lzljs3620320;master:
9cf4f76b736000a2ccd31a937551c0bca1c2e557
2d0562329615727cb0406cb9afd18cc4e76018fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add extraFiles to DataFileMeta,FLINK-28255,13468477,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/Jun/22 03:32,28/Jun/22 03:00,04/Jun/24 20:42,28/Jun/22 03:00,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"See FLINK-28244
 {code:java}
class DataFileMeta {
    String fileName;
    .....
    // store the name of extra files, extra files including changelog_file, primary_key_index_file, secondary_index_file, and etc...
    List<String> extraFiles;
}
{code}
Extra files can help us for many uses, including index files, changelog files and etc...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28244,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 03:00:15 UTC 2022,,,,,,,,,,"0|z16daw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 03:00;lzljs3620320;master: 0643906175416c2e5132fea7d3cacc934927b645;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add benchmark of vectorAssembler and minMaxScaler,FLINK-28254,13468475,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,27/Jun/22 03:00,19/Apr/23 01:58,04/Jun/24 20:42,19/Apr/23 01:58,,,,,,ml-2.2.0,,,Library / Machine Learning,,,,,,,0,pull-request-available,stale-major,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Apr 19 01:58:16 UTC 2023,,,,,,,,,,"0|z16dag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Apr/23 01:58;lindong;Merge to apache/flink-ml master branch 68aab8ad67888c3b7ea77ba47996e0a366b19466.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"It reports ""LocalDateTime is not supported in PyFlink currently"" when converting between Table and DataStream",FLINK-28253,13468473,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,dianfu,dianfu,27/Jun/22 02:38,24/Aug/22 06:25,04/Jun/24 20:42,24/Aug/22 06:25,1.14.0,1.15.0,,,,,,,API / Python,,,,,,,0,,,,,"For the following job:
{code}
from pyflink.datastream.stream_execution_environment import StreamExecutionEnvironment
from pyflink.table import EnvironmentSettings
from pyflink.table.table_environment import StreamTableEnvironment


if __name__ == '__main__':
    env = StreamExecutionEnvironment.get_execution_environment()
    settings = EnvironmentSettings.new_instance() \
        .in_streaming_mode() \
        .build()
    t_env = StreamTableEnvironment.create(stream_execution_environment=env, environment_settings=settings)

    t_env.execute_sql(""""""
            CREATE TABLE events (
                 `id` VARCHAR,
                 `source` VARCHAR,
                 `resources` VARCHAR,
                 `time` TIMESTAMP(3),
                 WATERMARK FOR `time` as `time` - INTERVAL '30' SECOND,
                 PRIMARY KEY (`id`) NOT ENFORCED
            ) WITH (
                'connector' = 'filesystem',
                 'path' = 'file:///path/to/input',
                 'format' = 'csv'
            )
        """""")

    events_stream_table = t_env.from_path('events')

    events_stream = t_env.to_data_stream(events_stream_table)
    #  Types.ROW([Types.STRING(), Types.STRING(), Types.STRING(), Types.SQL_TIMESTAMP()])

    # now do some processing - let's filter by the type of event we get

    codebuild_stream = events_stream.filter(
        lambda event: event['source'] == 'aws.codebuild'
    )

    codebuild_stream.print()
    env.execute()
{code}

It will reports the following exception:
{code}
Traceback (most recent call last):
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/tests/test_2.py"", line 47, in <module>
    process()
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/tests/test_2.py"", line 39, in process
    lambda event: event['source'] == 'aws.codebuild'
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/data_stream.py"", line 432, in filter
    self._j_data_stream.getTransformation().getOutputType())
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/common/typeinfo.py"", line 1070, in _from_java_type
    TypeInfoDataTypeConverter.toLegacyTypeInfo(j_type_info.getDataType())))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/common/typeinfo.py"", line 1042, in _from_java_type
    j_row_field_types]
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/common/typeinfo.py"", line 1041, in <listcomp>
    row_field_types = [_from_java_type(j_row_field_type) for j_row_field_type in
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/common/typeinfo.py"", line 1072, in _from_java_type
    raise TypeError(""The java type info: %s is not supported in PyFlink currently."" % j_type_info)
TypeError: The java type info: LocalDateTime is not supported in PyFlink currently.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 24 06:25:13 UTC 2022,,,,,,,,,,"0|z16da0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 06:25;dianfu;Fixed in FLINK-28827 via 1c40dc15fbc29e8e5e514565109a3fb05b47a83f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential Refactoring,FLINK-28252,13468461,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,tanjianjia@gmail.com,tanjianjia@gmail.com,26/Jun/22 21:28,29/Jun/22 10:10,04/Jun/24 20:42,29/Jun/22 10:10,,,,,,,,,,,,,,,,0,,,,,"*Summary*

I have identified a few classes with refactoring potential, specifically on subtask. The issue was refactored once on May 25, 2022 along with other classes in commit e68d679e8aca9b8b17e1b667188545c915a137d0.  Is it possible to improve the overall package of subtasks further? I am probably not qualified and experienced enough to point out the exact code needs to be refactored, given my limited knowledge of the project and codebase.

*Urgency*

Low priority. For maintenance of code in the long run.

Hi,

I am currently doing some research on Estimating Time Taken for Software Refactoring. Flink is one of my case studies being a highly popular project. Would appreciate your kind feedback on the appropriateness of the estimated time to refactor.

I am specifically looking at potential for refactoring, such as breaking functions that are too long into shorter functions, or to remove code duplication. Would you agree that there could be some modification to the code to make it better in the listed classes? And if so, is the estimated time appropriate to carry out the improvement? I will be grateful for any help you can provide.
 # StreamTask.java - 4 hours
 # SubtaskCheckpointCoordinator.java - 3.5 hours
 # SubtaskCheckpointCoordinatorImpl.java - 4.5 hours

 

The attached csv file contains the other recommendations from my research. Thank you once again for any help you can provide.

 

Regards,

Alvin",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/22 21:23;tanjianjia@gmail.com;Flink_release-1.12.5.csv;https://issues.apache.org/jira/secure/attachment/13045639/Flink_release-1.12.5.csv",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 29 10:10:08 UTC 2022,,,,,,,,,,"0|z16d7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 10:10;martijnvisser;[~tanjianjia@gmail.com] Thanks for opening a ticket. If you would like to improve on improving and simplifying our codebase, we'd love that. But we don't use Jira to give out these type of estimates, because all of our time is scarce and we have enough (concrete) problems that we would like to work on. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-avro format reader  may thrown ’java.util.NoSuchElementException' Exception  in some cases  ,FLINK-28251,13468409,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jinshuangxian,jinshuangxian,25/Jun/22 14:28,30/Jun/22 13:29,04/Jun/24 20:42,,1.15.0,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,,,,,"my source table is :

CREATE TABLE fileTable (
  data bytes
) 
WITH (
  'connector' = 'filesystem',
  'source.monitor-interval' = '60s',
  'path' = '${bucket}/${dirname}',
  'format' = 'avro'
);

After running for a period of time, it is possible to throw the following Exception:

!image-2022-06-25-22-14-31-301.png!

 

I found that the following two codes may have bugs. After modification, they can run stably Online.

!image-2022-06-25-22-25-17-507.png!

 

!image-2022-06-25-22-27-06-418.png!","flink 1.15.0

centos7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/22 14:14;jinshuangxian;image-2022-06-25-22-14-31-301.png;https://issues.apache.org/jira/secure/attachment/13045615/image-2022-06-25-22-14-31-301.png","25/Jun/22 14:25;jinshuangxian;image-2022-06-25-22-25-17-507.png;https://issues.apache.org/jira/secure/attachment/13045614/image-2022-06-25-22-25-17-507.png","25/Jun/22 14:27;jinshuangxian;image-2022-06-25-22-27-06-418.png;https://issues.apache.org/jira/secure/attachment/13045613/image-2022-06-25-22-27-06-418.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 13:29:55 UTC 2022,,,,,,,,,,"0|z16cw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 13:29;martijnvisser;[~jinshuangxian] Do you want to open a PR for a fix? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exactly-once sink kafka cause out of memory,FLINK-28250,13468405,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,charles-tan,jinshuangxian,jinshuangxian,25/Jun/22 14:10,01/Oct/22 06:16,04/Jun/24 20:42,18/Jul/22 08:44,1.15.0,,,,,1.15.2,1.16.0,,Connectors / Kafka,,,,,,,0,pull-request-available,,,,"*my sql code:*

CREATE TABLE sourceTable (

data bytes

)WITH(

'connector'='kafka',

'topic'='topic1',

'properties.bootstrap.servers'='host1',

'properties.group.id'='gorup1',

'scan.startup.mode'='latest-offset',

'format'='raw'

);

 

CREATE TABLE sinkTable (

data bytes

)

WITH (

'connector'='kafka',

'topic'='topic2',

'properties.bootstrap.servers'='host2',

'properties.transaction.timeout.ms'='30000',

'sink.semantic'='exactly-once',

'sink.transactional-id-prefix'='xx-kafka-sink-a',

'format'='raw'

);

insert into sinkTable

select data

from sourceTable;

 

*problem:*

After the program runs online for about half an hour, full gc frequently appears

 

{*}Troubleshoot{*}：

I use command 'jmap -dump:live,format=b,file=/tmp/dump2.hprof' dump the problem tm memory. It is found that there are 115 FlinkKafkaInternalProducers, which is not normal.

!image-2022-06-25-22-07-54-649.png!!image-2022-06-25-22-07-35-686.png!

After reading the code of KafkaCommitter, it is found that after the commit is successful, the producer is not recycled, only abnormal situations are recycled.

!image-2022-06-25-22-08-04-891.png!

I added a few lines of code. After the online test, the program works normally, and the problem of oom memory is solved.

!image-2022-06-25-22-08-15-024.png!","*flink version: flink-1.15.0*

*tm: 8* parallelism, 1 slot, 2g

centos7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28962,,,,,,,,,,,,,,,,,,,,"25/Jun/22 14:07;jinshuangxian;image-2022-06-25-22-07-35-686.png;https://issues.apache.org/jira/secure/attachment/13045612/image-2022-06-25-22-07-35-686.png","25/Jun/22 14:08;jinshuangxian;image-2022-06-25-22-07-54-649.png;https://issues.apache.org/jira/secure/attachment/13045611/image-2022-06-25-22-07-54-649.png","25/Jun/22 14:08;jinshuangxian;image-2022-06-25-22-08-04-891.png;https://issues.apache.org/jira/secure/attachment/13045610/image-2022-06-25-22-08-04-891.png","25/Jun/22 14:08;jinshuangxian;image-2022-06-25-22-08-15-024.png;https://issues.apache.org/jira/secure/attachment/13045609/image-2022-06-25-22-08-15-024.png",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Oct 01 06:16:44 UTC 2022,,,,,,,,,,"0|z16cv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 10:10;martijnvisser;[~renqs] Any thoughts?;;;","05/Jul/22 03:12;renqs;Thanks for the detailed investigation [~jinshuangxian] ! I think this explains the OOM issue. Would you like to create a patch? ;;;","08/Jul/22 16:37;charles-tan;Hi all, I've gone ahead and opened a PR with [~jinshuangxian]'s suggested patch, as I'm running into the same issues with my application ([mailing list thread|https://lists.apache.org/thread/c86cd8qyqb6qxy639hkzbozkwv2qxk84]). Would appreciate feedback on the [PR|https://github.com/apache/flink/pull/20205], thanks!;;;","11/Jul/22 02:01;renqs;Thanks for the contribution [~charles-tan]! I've assigned the ticket to you. ;;;","18/Jul/22 08:45;renqs;Merged on master: 74f90d722f7be5db5298b84626935a585391f0df
release-1.15: adbf09fb941c8f793df6d322ed95df87bc4254f3;;;","01/Oct/22 06:16;rmetzger;Thanks a lot for the fix.
I've opened a related ticket here: https://issues.apache.org/jira/browse/FLINK-29492;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink can not delete older checkpoint folders from google storage bucket,FLINK-28249,13468362,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jiezhang,jiezhang,24/Jun/22 21:08,29/Jun/22 10:12,04/Jun/24 20:42,,1.12.0,,,,,,,,FileSystems,,,,,,,0,,,,,"We are running flink 1.12 with this config: https://nightlies.apache.org/flink/flink-docs-release-1.12/deployment/filesystems/gcs.html#libraries

 

It is able to checkpoint to google storage bucket, but it can NOT delete older checkpoint folders from google storage bucket.

 

logs:
{code:java}
2022-06-22 23:14:28,477 WARN  org.apache.flink.runtime.checkpoint.CheckpointSubsumeHelper  [] - Fail to subsume the old checkpoint.
java.io.IOException: Error deleting 'gs://flink-checkpoint-dev-bucket/device-logs/flink-checkpoint/822475f36e7a9a4f4048cb82791c55e2/chk-1/_metadata', stage 2 with generation 1655939488397964
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl$4.onFailure(GoogleCloudStorageImpl.java:937) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.BatchHelper.execute(BatchHelper.java:184) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.BatchHelper.lambda$queue$0(BatchHelper.java:164) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:323) ~[gcs-connector-latest-hadoop2.jar:?]
    at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134) ~[?:1.8.0_332]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:69) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:36) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.BatchHelper.queue(BatchHelper.java:162) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.queueSingleObjectDelete(GoogleCloudStorageImpl.java:960) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.deleteObjects(GoogleCloudStorageImpl.java:891) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.deleteInternal(GoogleCloudStorageFileSystem.java:432) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.delete(GoogleCloudStorageFileSystem.java:398) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.delete(GoogleHadoopFileSystemBase.java:821) ~[gcs-connector-latest-hadoop2.jar:?]
    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.delete(HadoopFileSystem.java:160) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at org.apache.flink.runtime.state.filesystem.FileStateHandle.discardState(FileStateHandle.java:85) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discard(CompletedCheckpoint.java:249) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:220) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at org.apache.flink.runtime.checkpoint.CheckpointSubsumeHelper.subsume(CheckpointSubsumeHelper.java:63) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:73) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1211) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1082) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$acknowledgeCheckpoint$7(SchedulerBase.java:1042) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_332]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_332]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_332]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_332]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_332]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_332]
    at java.lang.Thread.run(Thread.java:750) [?:1.8.0_332]
    Suppressed: java.nio.file.DirectoryNotEmptyException: Cannot delete a non-empty directory.
        at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.delete(GoogleCloudStorageFileSystem.java:387) ~[gcs-connector-latest-hadoop2.jar:?]
        at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.delete(GoogleHadoopFileSystemBase.java:821) ~[gcs-connector-latest-hadoop2.jar:?]
        at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.delete(HadoopFileSystem.java:160) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at org.apache.flink.runtime.state.filesystem.FsCompletedCheckpointStorageLocation.disposeStorageLocation(FsCompletedCheckpointStorageLocation.java:74) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discard(CompletedCheckpoint.java:263) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at org.apache.flink.runtime.checkpoint.CompletedCheckpoint.discardOnSubsume(CompletedCheckpoint.java:220) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at org.apache.flink.runtime.checkpoint.CheckpointSubsumeHelper.subsume(CheckpointSubsumeHelper.java:63) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore.addCheckpoint(StandaloneCompletedCheckpointStore.java:73) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1211) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1082) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$acknowledgeCheckpoint$7(SchedulerBase.java:1042) ~[flink-dist_2.11-1.12.7.jar:1.12.7]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_332]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_332]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_332]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_332]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_332]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_332]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_332]
Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageExceptions.createJsonResponseException(GoogleCloudStorageExceptions.java:89) ~[gcs-connector-latest-hadoop2.jar:?]
    at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl$4.onFailure(GoogleCloudStorageImpl.java:917) ~[gcs-connector-latest-hadoop2.jar:?]
    ... 31 more
2022-06-22 23:14:28,487 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 4 for job 822475f36e7a9a4f4048cb82791c55e2 (46652 bytes in 527 ms). {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 29 10:12:31 UTC 2022,,,,,,,,,,"0|z16clk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jun/22 04:58;Yanfei Lei;Hi Jie, this log is *just a warning instead of a bug* and does not affect usage.
{code:java}
2022-06-22 23:14:28,477 WARN  org.apache.flink.runtime.checkpoint.CheckpointSubsumeHelper  [] - Fail to subsume the old checkpoint.{code}
Can you check if 
{noformat}
gs://flink-checkpoint-dev-bucket/device-logs/flink-checkpoint/822475f36e7a9a4f4048cb82791c55e2/chk-1/_metadata{noformat}
 exists in your google storage bucket?  If yes, you can delete it manually.

 

And it is better to ask on the mailing list or Slack, more people will see and help.;;;","29/Jun/22 10:12;martijnvisser;I would recommend upgrading to a later Flink version, especially Flink 1.15 since that contains recoverable writer support for GCS.

I don't think this is a bug, but just not supported in this version of Flink. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metaspace memory is leaking when repeatedly submitting Beam batch pipelines via the REST API,FLINK-28248,13468294,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jigga,jigga,24/Jun/22 12:49,06/Jul/22 06:55,04/Jun/24 20:42,,1.14.4,,,,,,,,API / Core,,,,,,,0,,,,,"We have a Flink cluster running on k8s/OpenShift in session mode running our Apache Beam pipelines. Some of these pipelines are streaming pipelines and run continuously; some are batch pipelines submitted periodically whenever there is a load to be processed.

We believe that the batch pipelines cause the issue. We submit 1 to several batch jobs every 5 minutes. For each job, a new instance of the ChildFirstClassLoader is instantiated and it looks like they are not closed properly after the job finishes.

Attached is the screenshot from the Eclipse memory analyzer - from the Leak Suspects report. When the heap dump was captured, there were 2 streaming and several batch jobs running plus over 100 finished batch jobs.

!image-2022-06-24-14-45-51-689.png!

In our current setup, we allocate 8GB for the metaspace:

!image-2022-06-24-14-51-47-909.png!

 

And the top components from the mem analyzer:

!image-2022-06-24-15-07-43-035.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/22 12:45;jigga;image-2022-06-24-14-45-51-689.png;https://issues.apache.org/jira/secure/attachment/13045570/image-2022-06-24-14-45-51-689.png","24/Jun/22 12:51;jigga;image-2022-06-24-14-51-47-909.png;https://issues.apache.org/jira/secure/attachment/13045571/image-2022-06-24-14-51-47-909.png","24/Jun/22 13:07;jigga;image-2022-06-24-15-07-43-035.png;https://issues.apache.org/jira/secure/attachment/13045573/image-2022-06-24-15-07-43-035.png","05/Jul/22 13:47;jigga;image-2022-07-05-15-47-45-038.png;https://issues.apache.org/jira/secure/attachment/13046311/image-2022-07-05-15-47-45-038.png","05/Jul/22 13:51;jigga;image-2022-07-05-15-51-05-840.png;https://issues.apache.org/jira/secure/attachment/13046313/image-2022-07-05-15-51-05-840.png","05/Jul/22 13:58;jigga;image-2022-07-05-15-58-43-448.png;https://issues.apache.org/jira/secure/attachment/13046314/image-2022-07-05-15-58-43-448.png",,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 06:55:45 UTC 2022,,,,,,,,,,"0|z16c6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 13:44;chesnay;Please attach the heap dump, or if that is not possible investigate the gc root that prevents the classloaders from being garbage collected.

At first glance this is likely caused by a thread being leaked.;;;","27/Jun/22 14:14;jigga;Unfortunately, I don't think I'll be able to attach the heap dump as it may contain the client's sensitive data that I'd rather not leak. 

""investigate the gc root that prevents the classloaders from being garbage collected"" - any hint on how to do this in the Eclipse memory analyzer?;;;","05/Jul/22 13:59;jigga;So after spending the past few days in the Eclipse memory analyzer and testing various dependencies setups, I can safely say that the references to the ChildFirstClassLoader leak through thread locals containing Jackson configurations.

Here's one example, where the jobmanager-io-thread-1 holds a reference to the ChildFirstClasLoader instance through some thread-local entry. And essentially each job submission is a new thread-local entry that holds the reference to the new class loader instance used to submit the job.

!image-2022-07-05-15-47-45-038.png!

Here an example where the RMI Connection thread holds a reference to ChildFirstClassLoader via its contextClassLoader instance:

!image-2022-07-05-15-51-05-840.png!

Again, it's Jackson that's present somewhere in between.

In the next screenshot, it's some flink-akka actor thread that indirectly stores the reference to ChildFirstClassLoader instance and again, it's Jackson that is somewhere in the middle.

!image-2022-07-05-15-58-43-448.png!

I think the important bit of information is that I moved Jackson libraries to the Flink's lib folder as I've also moved some other common libs there that depend on Jackson and if Jackson is not there, job submission fails with ClassNotFound exception, even if Jackson is packaged in the job's jar.;;;","06/Jul/22 06:55;chesnay;Where is that ThreadLocal coming from? Do you use them in your user code?

ThreadLocals are highly problematic in systems where long-running threads (like those from Flink) cross classloader boundaries, because usaers typically don't bother removing entries from the map again.

I'm not aware of us using thread locals in the runtime, so I'd think it's either in your code, beam or some other library. In this case there's nothing we could on our end.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception will be thrown when over window contains grouping in Hive Dialect,FLINK-28247,13468287,13430553,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,24/Jun/22 12:10,08/Jul/22 04:11,04/Jun/24 20:42,08/Jul/22 04:11,,,,,,1.16.0,,,,,,,,,,0,pull-request-available,,,,"The exception will be reprodued by the following sql when using Hive Dialect:
{code:java}
create table t(category int, live int, comments int)
SELECT grouping(category), lag(live) over(partition by grouping(category)) FROM t GROUP BY category, live; {code}
The reson is it will first call `HiveParserCalcitePlanner#genSelectForWindowing` to generate the window, which will then call `HiveParserUtils#rewriteGroupingFunctionAST` to rewrite the group function in the over window :

 
{code:java}
// rewrite grouping function
if (current.getType() == HiveASTParser.TOK_FUNCTION
        && current.getChildCount() >= 2) {
    HiveParserASTNode func = (HiveParserASTNode) current.getChild(0);
    if (func.getText().equals(""grouping"")) {
        visited.setValue(true);
        convertGrouping(
                current, grpByAstExprs, noneSet, legacyGrouping, found);
    }
} 


{code}
 

So `grouping(category)` will be converted to `grouping(0, 1)`. 

After `HiveParserCalcitePlanner#genSelectForWindowing`, it will try to rewrite it again:

 
{code:java}
if (!qbp.getDestToGroupBy().isEmpty()) {
    // Special handling of grouping function
    expr =
            rewriteGroupingFunctionAST(
                    getGroupByForClause(qbp, selClauseName),
                    expr,
                    !cubeRollupGrpSetPresent);
} {code}
And it will also fall back to `convertGrouping` again as `current.getChildCount() >= 2` will be true. But then, it can't find any field 

presented  in group by  for it's `grouping(0, 1)` now.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 04:10:54 UTC 2022,,,,,,,,,,"0|z16c4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 12:53;luoyuxia;The issue is introduced in this [change|[https://github.com/apache/flink/pull/15983/files#diff-ea0bbb90359ad01a52dbb18628362f6714a6c7817ca2367e7fd2c4257e153d1dR734]], seems `current.getChildCount() == 2` is changed to  `current.getChildCount() >= 2` by mistake. 

To fix it may be simple, we only need to revert such modification for this line . In hive 2.x, it's fine for it only supports one parameter in grouping function. 

But in Hive 3.x, Hive support multiargument grouping function[HIVE-15996|https://issues.apache.org/jira/browse/HIVE-15996]  . So such change won't work when  grouping function contains more than one paramters.

For better compatibility, we may need to follow the behavior in Hive 3.x;;;","08/Jul/22 04:10;lirui;Fixed in master: a069a306a0c95ba62bbc63f6afc995fd3216a326;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store classifier in dependency,FLINK-28246,13467898,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/Jun/22 10:57,14/Nov/22 08:32,04/Jun/24 20:42,14/Nov/22 08:32,,,,,,1.17.0,,,Build System / CI,,,,,,,0,pull-request-available,stale-assigned,,,"The classifier is required to properly differentiate between jars and test-jars.

In the future we could also improve the accuracy of the notice checker (which currently doesn't care about classifiers).",,,,,,,,,,,,,FLINK-29915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 14 08:32:41 UTC 2022,,,,,,,,,,"0|z169qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Nov/22 08:32;chesnay;master: befb3b9dbe7f5e6d122673af15208cadaa71ce2c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor DependencyParser#parseDependencyTreeOutput to return a Tree,FLINK-28245,13467746,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/Jun/22 10:38,14/Nov/22 11:23,04/Jun/24 20:42,14/Nov/22 11:23,,,,,,1.17.0,,,Build System / CI,,,,,,,0,pull-request-available,stale-assigned,,,"Returning a tree structure makes it easier to infer transitive properties based on the tree structure, which will be used by a new safeguard for FLINK-28016.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 14 11:23:07 UTC 2022,,,,,,,,,,"0|z168so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Nov/22 11:23;chesnay;master: 1d28a978b6ba52850bdb320b661a61b584627f6a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce changelog file for DataFile,FLINK-28244,13466980,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/Jun/22 09:00,30/Jun/22 06:43,04/Jun/24 20:42,30/Jun/22 06:43,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"When using TableStore to support stream consumption, there are two requirements.
 * Downstream gets all changelogs
 * The order of stream consumption is the order of input

For append only table, it is easy to meet both.

But for the primary key table, Its files are all sorted and de-duplicated by pk, making it impossible to meet the above expectations.

We can output another ChangelogFile when the DataFile flush, and the stream reads it directly.

We can modify DataFileMeta:
{code:java}
class DataFileMeta {
    String fileName;
    .....
    // store the name of extra files, extra files including changelog_file, primary_key_index_file, secondary_index_file, and etc...
    List<String> extraFiles;
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28257,FLINK-28255,FLINK-28264,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 06:43:27 UTC 2022,,,,,,,,,,"0|z1642g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 06:43;lzljs3620320;master: 83a321fa116f9dc520d7ea936ce6103e1cb45869;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Considering only time of targeted operation for calculation benchmarks performance,FLINK-28243,13466771,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,akalashnikov,akalashnikov,24/Jun/22 08:30,24/Jun/22 13:31,04/Jun/24 20:42,,,,,,,,,,Benchmarks,,,,,,,0,,,,,"Right now, in many benchmarks, the iteration time consists of many parts which are not always connected to the target of the particular benchmark. For example, if we want to benchmark async operator, we want to take into account only the time which was spent on this operator but currently, the time of one iteration of asyncOperatorBenchmark is `job/tasks initialization phase` + `operator execution` + `shutdown phase`.

Ideally, we want to isolate somehow `operator execution` time and consider only it in this benchmark but `job/tasks initialization phase` should be benchmarked separately.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28241,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 13:31:15 UTC 2022,,,,,,,,,,"0|z162s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 13:31;akalashnikov;FLINK-28241should be reverted as soon as this task will be done;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC source with meta columns may cause error result on downstream stateful operators,FLINK-28242,13466731,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,lincoln.86xy,lincoln.86xy,24/Jun/22 08:25,13/Jul/22 09:20,04/Jun/24 20:42,13/Jul/22 09:20,1.15.0,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,"The intermediate result of current test case temporalJoinITCase#testEventTimeMultiTemporalJoin is wrong:

{code}

+I,    5,RMB,40,2020-08-16T00:03,null,null,null,null
+I,    2,US Dollar,1,2020-08-15T00:02,102,2020-08-15T00:00:02,102,2020-08-15T00:00:02
+I,    3,RMB,40,2020-08-15T00:03,702,2020-08-15T00:00:04,702,2020-08-15T00:00:04
-U,   2,US Dollar,1,2020-08-16T00:03,106,2020-08-16T00:02,106,2020-08-16T00:02

...

{code}

because the ""-U,   2,US Dollar,1,2020-08-16T00:03..."" has a different 'order_time' column against ""+I,    2,US Dollar,1,2020-08-15T00:02..."", and after join there's no upsert key, so downstream operator can only do retract by the complete row, and will fail at this case.

The root cause is when cdc source carries meta data column (e.g., operation time in binlog or operation type, which will make the delete|update_before message not exactly the same as the previous version), and after some operations like join (not on the primary key of cdc source, the output will have no upsert key anymore), then downstream operator can not do retract correctly.

This is obscure to users, but we should think of a way to at least report the error to users (during compiling), or other solution eliminate the problem completely.",,,,,,,,,,,,,FLINK-24666,,,,,,,,,,,,,,,,,,,,,,FLINK-27849,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 09:17:45 UTC 2022,,,,,,,,,,"0|z162j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 09:17;lincoln.86xy;This can be unified as one case of non-deterministic update which will be solved by FLINK-27849, so mark this as duplicated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Massive regression on 20.06.2021,FLINK-28241,13466637,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,akalashnikov,akalashnikov,24/Jun/22 08:14,27/Jun/22 06:17,04/Jun/24 20:42,27/Jun/22 06:17,,,,,,,,,Benchmarks,,,,,,,0,pull-request-available,,,,"In FLINK-27921 added property `REQUIREMENTS_CHECK_DELAY` that optimizes the number of checks of resources but it also leads to a little delay during the init phase which is totally not critical for the normal scenario. But unfortunately, our benchmarks can not exclude the time of the initial phase(or shutdown phase) from the total time of benchmarks. As result, all benchmarks, that have a relatively short execution time(compare to the initial phase time), have regression(up to 50%). As a quick solution, it makes sense just to set `REQUIREMENTS_CHECK_DELAY` to zero for all benchmarks.
Not full list of affected benchmarks:
 * asyncWait
 * globalWindow
 * mapSink
 * readFileSplit
 * remoteRebalance
 * serializer*
 * sorted*
 * twoInputOneIdleMapSink",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27921,,,,,,FLINK-28243,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 27 06:17:17 UTC 2022,,,,,,,,,,"0|z161y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 06:17;roman;Fix merged as 5d2f5a96b4bcb6f2e96db0a16a7ccea6280c91e2
(disable the new feature in benchmarks).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyShuffleMetricFactory#RequestedMemoryUsageMetric#getValue may throw ArithmeticException when the total segments of NetworkBufferPool is 0,FLINK-28240,13466581,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,pltbkd,pltbkd,24/Jun/22 07:59,05/Jul/22 07:12,04/Jun/24 20:42,05/Jul/22 07:12,1.15.0,,,,,1.15.2,1.16.0,,Runtime / Network,,,,,,,0,pull-request-available,,,,"In a single vertex job, the network memory can be set to 0 since the job doesn't need it, and in this case the totalNumberOfMemorySegments of the NetworkBufferPool will also be 0.

While the NettyShuffleMetricFactory#RequestedMemoryUsageMetric#getValue uses the totalNumberOfMemorySegments of NetworkBufferPool as divisor without validating, so an ArithmeticException will be thrown when the totalNumberOfMemorySegments is 0.

Since 0 network memory is in fact valid for a single vertex job, I suppose the RequestedMemoryUsageMetric#getValue should check if the devisor is 0, and return 0 as the usage directly in such cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 07:12:13 UTC 2022,,,,,,,,,,"0|z161ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 07:12;gaoyunhaii;Merged on master via c6b3a8aa25607fb62d51b92f11f72778e5d618b2

Merged on 1.15 via 95f4b6fc7fd4a713d8d6348d32a121498ffce1e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table-Planner-Loader lacks access to commons-math3,FLINK-28239,13466580,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,24/Jun/22 07:54,24/Jun/22 13:48,04/Jun/24 20:42,24/Jun/22 13:47,1.15.0,,,,,1.15.2,1.16.0,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"{{FlinkRelOptUtil}} requires {{commons-math3}}, but this dependency is neither bundled by {{table-planner}} nor in the owner classpath of the {{PlannerModule}}.

{code}
{""errors"":[""Internal server error."",""<Exception on server side:\norg.apache.flink.client.program.ProgramInvocationException: The program caused an error: \n\nClasspath: [file:/tmp/flink-web-8bc46ccd-f545-474c-8605-d084950afed1/flink-web-upload/38118da6-41f1-4e0b-9bb1-c69ee9662f3a_data-streamverse-core-1.0-SNAPSHOT-jar-with-dependencies.jar]\n\nSystem.out: (none)\n\nSystem.err: (none)
at org.apache.flink.client.program.PackagedProgramUtils.generateException(PackagedProgramUtils.java:264)
at org.apache.flink.client.program.PackagedProgramUtils.getPipelineFromProgram(PackagedProgramUtils.java:172)
at org.apache.flink.client.program.PackagedProgramUtils.createJobGraph(PackagedProgramUtils.java:82)
at org.apache.flink.runtime.webmonitor.handlers.utils.JarHandlerUtils$JarHandlerContext.toJobGraph(JarHandlerUtils.java:159)
at org.apache.flink.runtime.webmonitor.handlers.JarPlanHandler.lambda$handleRequest$1(JarPlanHandler.java:107)
at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: org/apache/commons/math3/util/ArithmeticUtils
at org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil$.mergeMiniBatchInterval(FlinkRelOptUtil.scala:439)
at org.apache.flink.table.planner.plan.rules.physical.stream.MiniBatchIntervalInferRule.onMatch(MiniBatchIntervalInferRule.scala:81)
at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$2(FlinkGroupProgram.scala:63)
at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
at scala.collection.Iterator.foreach(Iterator.scala:937)
at scala.collection.Iterator.foreach$(Iterator.scala:937)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 13:47:11 UTC 2022,,,,,,,,,,"0|z161lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 13:47;chesnay;master: ec1adccac3ff63a9b13dab8fad8e23b3746b9dca
1.15: 5b60be4791ac7442ceab011256609d4544eaf240 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unstable testCancelOperationAndFetchResultInParallel,FLINK-28238,13465112,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,24/Jun/22 04:32,04/Jul/22 02:14,04/Jun/24 20:42,04/Jul/22 02:14,1.16.0,,,,,1.16.0,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,"The failed test in https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2711&view=logs&j=43a[…]cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f&l=11611

It's possible the fetcher fetches the results from the closed operation.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28293,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 02:14:49 UTC 2022,,,,,,,,,,"0|z15sjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 12:00;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37410&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","04/Jul/22 02:14;fsk119;Fix in master: 7974e81ec51a2071b9658f768f651ffc371b15b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the flink-ml python examples in doc,FLINK-28237,13464351,13429597,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,24/Jun/22 03:07,27/Jun/22 09:29,04/Jun/24 20:42,27/Jun/22 03:54,ml-2.1.0,,,,,ml-2.1.0,,,API / Python,Library / Machine Learning,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 27 03:54:38 UTC 2022,,,,,,,,,,"0|z15nu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 03:54;hxbks2ks;Merged into master via 58dbc3813e37085ef67c05f31a44ef11482ecd65
Merged into release-2.1 via 58dbc3813e37085ef67c05f31a44ef11482ecd65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"managed memory  no  matter how much I configure, it will be exhausted",FLINK-28236,13464265,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,LCER,LCER,24/Jun/22 02:58,24/Jun/22 07:05,04/Jun/24 20:42,24/Jun/22 07:05,1.15.0,,,,,,,,,,,,,,,0,,,,,"managed memory  no  matter how much I configure, it will be exhausted, resulting in taskmanager crash。

!image-2022-06-24-10-55-07-475.png|width=565,height=330!

 

I have a question, managed memory What resources of the physical machine are used? I check the memory occupied by the process through the top command. The actual RES usage is only about 10g

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/22 02:57;LCER;flink-conf.yaml;https://issues.apache.org/jira/secure/attachment/13045533/flink-conf.yaml","24/Jun/22 02:55;LCER;image-2022-06-24-10-55-07-475.png;https://issues.apache.org/jira/secure/attachment/13045534/image-2022-06-24-10-55-07-475.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 07:05:24 UTC 2022,,,,,,,,,,"0|z15nb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 05:53;JasonLee;hi [~LCER] ,This error is not necessarily a managed memory problem, you need to look at the JM log, which will have detailed error information;;;","24/Jun/22 07:05;martijnvisser;[~LCER] I'm closing this ticket since Jira is for bugs, not necessary for user questions. Those are better asked on the mailing list, Stackoverflow or Slack. See https://flink.apache.org/gettinghelp.html#having-a-question

I can also recommend going through the documentation at https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/memory/mem_setup/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Min aggregate function support type: ''ARRAY''.,FLINK-28235,13464224,13421719,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tartarus,tartarus,24/Jun/22 02:53,24/Jun/22 07:15,04/Jun/24 20:42,24/Jun/22 07:15,1.15.0,,,,,,,,Table SQL / Planner,,,,,,,0,,,,,"Min aggregate function does not support type: ''ARRAY''.

We can reproduce it with the following SQL
{code:java}
select app, concat_ws(',', collect_set(cast(user_id as string))) as user_list, count(distinct user_id) as uv from test_table group by app {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 07:14:55 UTC 2022,,,,,,,,,,"0|z15n20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 07:14;luoyuxia;Duplicated to [FLINK-19004|https://issues.apache.org/jira/browse/FLINK-19004];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
" Infinite or NaN exception for power(-1, 0.5)",FLINK-28234,13464043,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,24/Jun/22 02:32,08/Aug/22 02:16,04/Jun/24 20:42,08/Aug/22 02:16,,,,,,1.16.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,stale-assigned,,,"When using the follwiing sql  , it will throw the ""java.lang.NumberFormatException: Infinite or NaN""

 
{code:java}
SELECT power(-1, 0.5) {code}
It happen in ExpressionReducer, and power(-1, 0.5)  is NaN, which can't be used to construct BigDecimal, so the exception throws.

But for the sql,

 
{code:java}
create table src(key int);
insert into src values (-1);
SELECT power(key, 0.5) from src;{code}
 

it will run normally.

I think we  should keep same behavror for these two case, instead of throw exception for one case, and run normally for another case.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 08 02:16:33 UTC 2022,,,,,,,,,,"0|z15lxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 02:41;luoyuxia;I will fix it.;;;","28/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","08/Aug/22 02:16;godfreyhe;Fixed in master: 437978fdf8a67926827fd137341302e53a618203;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Successful observe doesn't clear errors due to patching,FLINK-28233,13463009,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,23/Jun/22 15:35,23/Jun/22 17:57,04/Jun/24 20:42,23/Jun/22 17:57,kubernetes-operator-1.0.0,kubernetes-operator-1.1.0,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,We set the error to `null` when trying to clear it but due to the patching mechanism this does nothing. It should be set to an empty string instead.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 23 17:57:44 UTC 2022,,,,,,,,,,"0|z15fk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 17:57;gyfora;merged to main 5e03a618b22ac8d77fe3e7886fd20be8ab518f75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow for custom pre-flight checks for SQL UDFs,FLINK-28232,13463000,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rmetzger,rmetzger,23/Jun/22 14:56,23/Jun/22 14:56,04/Jun/24 20:42,,,,,,,,,,Table SQL / API,,,,,,,0,,,,,"Currently, implementors of SQL UDFs [1] can not validate the UDF input before submitting a SQL query to the runtime. 
Take for example a UDF that computes a regex based on user input. Ideally there's a callback for the UDF implementor to check if the user-provided regex is valid and compiles, to avoid errors during the execution of the SQL query.

It would be ideal to get access to the schema information resolved by the SQL planner in that pre-flight validation to also allow for schema related checks pre-flight.


[1] https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/udfs/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-23 14:56:25.0,,,,,,,,,,"0|z15fi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Apache Ozone FileSystem,FLINK-28231,13462995,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bianqi,bianqi,23/Jun/22 14:38,03/Aug/22 12:06,04/Jun/24 20:42,,1.12.1,,,,,,,,fs,,,,,,,0,,,,,"h3. Background：

Apache Ozone has become a new generation object storage system. The future is promising.

Currently, Flink already supports S3, aliyunOSS, and Google Cloud Storage. But Apache Ozone is not supported.
h3. Purpose：

 

Flink supports the Apache Ozone file system
h3. work content：

We need to develop Flink to support Apache Ozone FileSystem. Also improve the documentation.

[https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/filesystems/overview/]","* Flink1.12.1
 * Hadoop3.2.2
 * Ozone1.2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/22 01:07;bianqi;abc.png;https://issues.apache.org/jira/secure/attachment/13045523/abc.png","24/Jun/22 02:05;bianqi;image-2022-06-24-10-05-42-193.png;https://issues.apache.org/jira/secure/attachment/13045528/image-2022-06-24-10-05-42-193.png","24/Jun/22 02:08;bianqi;image-2022-06-24-10-08-26-010.png;https://issues.apache.org/jira/secure/attachment/13045527/image-2022-06-24-10-08-26-010.png","24/Jun/22 01:11;bianqi;mapreduce-webui.png;https://issues.apache.org/jira/secure/attachment/13045524/mapreduce-webui.png","23/Jun/22 14:43;bianqi;微信图片_20220623224142.png;https://issues.apache.org/jira/secure/attachment/13045513/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20220623224142.png",,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 12:06:44 UTC 2022,,,,,,,,,,"0|z15fgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 16:05;prabhujoseph;[~bianqi] The client fails to invoke the remote server (Ozone Manager) method. Could you check if any error in the Ozone Manager which runs at jykj0.yarn.com:9862. ;;;","24/Jun/22 01:07;bianqi;[~prabhujoseph]  thank you relpay.

ozone om doesn't have any error log, I submit MapReduce job without any problem, I can read and write files normally.

If I use hdfs scheme to read and write HDFS data no problem

 

Flink wordcount use HDFS scheme:

 

!image-2022-06-24-10-05-42-193.png!

 MapReduce: WordCount use ozone  ofs scheme

!abc.png!

Flink wordcount use ofs scheme ozone

!image-2022-06-24-10-08-26-010.png!

 ;;;","24/Jun/22 02:34;bianqi;this is debug log 
{code:java}
2022-06-24 10:23:06,547 DEBUG org.apache.flink.runtime.util.HadoopUtils                    [] - Adding /soft/hadoop/etc/hadoop/hdfs-site.xml to hadoop configuration
2022-06-24 10:23:06,568 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender http://jykj3.yarn.com:37478 with session ID 34137e8d-3a8c-4788-b50d-d991a9d9b157.
2022-06-24 10:23:06,568 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://jykj3.yarn.com:37478 was granted leadership with leaderSessionID=34137e8d-3a8c-4788-b50d-d991a9d9b157
2022-06-24 10:23:06,568 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Confirm leader session ID 34137e8d-3a8c-4788-b50d-d991a9d9b157 for leader http://jykj3.yarn.com:37478.
2022-06-24 10:23:06,569 DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Write leader information: LeaderInformation{leaderSessionID='34137e8d-3a8c-4788-b50d-d991a9d9b157', leaderAddress=http://jykj3.yarn.com:37478}.
2022-06-24 10:23:06,593 DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Successfully wrote leader information: LeaderInformation{leaderSessionID='34137e8d-3a8c-4788-b50d-d991a9d9b157', leaderAddress=http://jykj3.yarn.com:37478}.
2022-06-24 10:23:06,595 DEBUG org.apache.flink.runtime.rpc.akka.SupervisorActor            [] - Starting FencedAkkaRpcActor with name resourcemanager_0.
2022-06-24 10:23:06,596 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager at akka://flink/user/rpc/resourcemanager_0 .
2022-06-24 10:23:06,619 DEBUG org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory [] - Starting Dispatcher.
2022-06-24 10:23:06,624 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Starting DefaultLeaderElectionService with ZooKeeperLeaderElectionDriver{leaderPath='/leader/dispatcher_lock'}.
2022-06-24 10:23:06,624 DEBUG org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory [] - Starting ResourceManager.
2022-06-24 10:23:06,624 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Starting the resource manager.
2022-06-24 10:23:06,631 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Starting DefaultLeaderRetrievalService with ZookeeperLeaderRetrievalDriver{retrievalPath='/leader/resource_manager_lock'}.
2022-06-24 10:23:06,631 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Starting DefaultLeaderRetrievalService with ZookeeperLeaderRetrievalDriver{retrievalPath='/leader/dispatcher_lock'}.
2022-06-24 10:23:06,668 INFO  org.apache.hadoop.yarn.client.RMProxy                        [] - Connecting to ResourceManager at jykj0.yarn.com/192.168.149.101:8030
2022-06-24 10:23:06,681 DEBUG org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CreateBuilderImpl [] - Protected mode findNode result: null
2022-06-24 10:23:06,687 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: DefaultDispatcherRunner with session ID 5180000f-64dd-41cd-9a5f-40485d5fcf3c.
2022-06-24 10:23:06,688 INFO  org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner [] - DefaultDispatcherRunner was granted leadership with leader id 5180000f-64dd-41cd-9a5f-40485d5fcf3c. Creating new DispatcherLeaderProcess.
2022-06-24 10:23:06,746 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
2022-06-24 10:23:06,759 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.
2022-06-24 10:23:06,759 DEBUG org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Retrieving all stored job ids from ZooKeeperStateHandleStore{namespace='flink/application_1655993758285_0008/jobgraphs'}.
2022-06-24 10:23:06,782 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Retrieved job ids [] from ZooKeeperStateHandleStore{namespace='flink/application_1655993758285_0008/jobgraphs'}
2022-06-24 10:23:06,783 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
2022-06-24 10:23:06,798 DEBUG org.apache.flink.runtime.rpc.akka.SupervisorActor            [] - Starting FencedAkkaRpcActor with name dispatcher_1.
2022-06-24 10:23:06,804 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .
2022-06-24 10:23:06,828 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Confirm leader session ID 5180000f-64dd-41cd-9a5f-40485d5fcf3c for leader akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/dispatcher_1.
2022-06-24 10:23:06,828 DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Write leader information: LeaderInformation{leaderSessionID='5180000f-64dd-41cd-9a5f-40485d5fcf3c', leaderAddress=akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/dispatcher_1}.
2022-06-24 10:23:06,840 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: false)
2022-06-24 10:23:06,845 DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Successfully wrote leader information: LeaderInformation{leaderSessionID='5180000f-64dd-41cd-9a5f-40485d5fcf3c', leaderAddress=akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/dispatcher_1}.
2022-06-24 10:23:06,854 INFO  org.apache.flink.yarn.YarnResourceManagerDriver              [] - Recovered 0 containers from previous attempts ([]).
2022-06-24 10:23:06,855 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Recovered 0 workers from previous attempt.
2022-06-24 10:23:06,871 DEBUG org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Leader node has changed.
2022-06-24 10:23:06,873 DEBUG org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - New leader information: Leader=akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/dispatcher_1, session ID=5180000f-64dd-41cd-9a5f-40485d5fcf3c.
2022-06-24 10:23:06,879 DEBUG org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Try to connect to remote RPC endpoint with address akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/dispatcher_1. Returning a org.apache.flink.runtime.dispatcher.DispatcherGateway gateway.
2022-06-24 10:23:06,923 INFO  org.apache.hadoop.conf.Configuration                         [] - resource-types.xml not found
2022-06-24 10:23:06,924 INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils           [] - Unable to find 'resource-types.xml'.
2022-06-24 10:23:06,952 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
2022-06-24 10:23:06,963 INFO  org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl [] - Upper bound of the thread pool size is 500
2022-06-24 10:23:06,968 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Starting DefaultLeaderElectionService with ZooKeeperLeaderElectionDriver{leaderPath='/leader/resource_manager_lock'}.
2022-06-24 10:23:06,993 DEBUG org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CreateBuilderImpl [] - Protected mode findNode result: null
2022-06-24 10:23:06,997 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Grant leadership to contender LeaderContender: ActiveResourceManager with session ID d72df1be-a8e7-4817-8d4c-828871abd3f1.
2022-06-24 10:23:06,999 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - ResourceManager akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/resourcemanager_0 was granted leadership with fencing token 8d4c828871abd3f1d72df1bea8e74817
2022-06-24 10:23:07,004 DEBUG org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Starting the slot manager.
2022-06-24 10:23:07,015 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2022-06-24 10:23:07,015 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2022-06-24 10:23:07,016 DEBUG org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Confirm leader session ID d72df1be-a8e7-4817-8d4c-828871abd3f1 for leader akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/resourcemanager_0.
2022-06-24 10:23:07,016 DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Write leader information: LeaderInformation{leaderSessionID='d72df1be-a8e7-4817-8d4c-828871abd3f1', leaderAddress=akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/resourcemanager_0}.
2022-06-24 10:23:07,024 DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver [] - Successfully wrote leader information: LeaderInformation{leaderSessionID='d72df1be-a8e7-4817-8d4c-828871abd3f1', leaderAddress=akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/resourcemanager_0}.
2022-06-24 10:23:07,057 DEBUG org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver [] - Leader node has changed.
2022-06-24 10:23:07,058 DEBUG org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - New leader information: Leader=akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/resourcemanager_0, session ID=d72df1be-a8e7-4817-8d4c-828871abd3f1.
2022-06-24 10:23:07,058 DEBUG org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Try to connect to remote RPC endpoint with address akka.tcp://flink@jykj3.yarn.com:38636/user/rpc/resourcemanager_0. Returning a org.apache.flink.runtime.resourcemanager.ResourceManagerGateway gateway.
2022-06-24 10:23:07,085 INFO  org.apache.flink.api.java.utils.PlanGenerator                [] - The job has 0 registered types and 0 default Kryo serializers
2022-06-24 10:23:07,085 DEBUG org.apache.flink.api.java.utils.PlanGenerator                [] - Registered Kryo types: []
2022-06-24 10:23:07,086 DEBUG org.apache.flink.api.java.utils.PlanGenerator                [] - Registered Kryo with Serializers types: []
2022-06-24 10:23:07,086 DEBUG org.apache.flink.api.java.utils.PlanGenerator                [] - Registered Kryo with Serializer Classes types: []
2022-06-24 10:23:07,086 DEBUG org.apache.flink.api.java.utils.PlanGenerator                [] - Registered Kryo default Serializers: []
2022-06-24 10:23:07,086 DEBUG org.apache.flink.api.java.utils.PlanGenerator                [] - Registered Kryo default Serializers Classes []
2022-06-24 10:23:07,086 DEBUG org.apache.flink.api.java.utils.PlanGenerator                [] - Registered POJO types: []
2022-06-24 10:23:07,093 DEBUG org.apache.flink.client.PlanTranslator                       [] - Set parallelism 1, plan default parallelism 1
2022-06-24 10:23:07,101 DEBUG org.apache.flink.optimizer.Optimizer                         [] - Beginning compilation of program 'WordCount Example'
2022-06-24 10:23:07,101 DEBUG org.apache.flink.optimizer.Optimizer                         [] - Using a default parallelism of 1
2022-06-24 10:23:07,101 DEBUG org.apache.flink.optimizer.Optimizer                         [] - Using default data exchange mode PIPELINED
2022-06-24 10:23:07,137 DEBUG org.apache.flink.runtime.fs.hdfs.HadoopFsFactory             [] - Instantiating for file system scheme ofs Hadoop File System org.apache.hadoop.fs.ozone.RootedOzoneFileSystem
2022-06-24 10:23:10,201 INFO  org.apache.hadoop.io.retry.RetryInvocationHandler            [] - java.lang.IllegalStateException, while invoking $Proxy43.submitRequest over nodeId=null,nodeAddress=jykj0.yarn.com:9862 after 1 failover attempts. Trying to failover after sleeping for 4000ms.
2022-06-24 10:23:14,205 INFO  org.apache.hadoop.io.retry.RetryInvocationHandler            [] - java.lang.IllegalStateException, while invoking $Proxy43.submitRequest over nodeId=null,nodeAddress=jykj0.yarn.com:9862 after 2 failover attempts. Trying to failover after sleeping for 6000ms.
2022-06-24 10:23:17,033 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2022-06-24 10:23:17,034 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2022-06-24 10:23:20,209 INFO  org.apache.hadoop.io.retry.RetryInvocationHandler            [] - java.lang.IllegalStateException, while invoking $Proxy43.submitRequest over nodeId=null,nodeAddress=jykj0.yarn.com:9862 after 3 failover attempts. Trying to failover after sleeping for 8000ms.
2022-06-24 10:23:27,052 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2022-06-24 10:23:27,052 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2022-06-24 10:23:28,210 INFO  org.apache.hadoop.io.retry.RetryInvocationHandler            [] - java.lang.IllegalStateException, while invoking $Proxy43.submitRequest over nodeId=null,nodeAddress=jykj0.yarn.com:9862 after 4 failover attempts. Trying to failover after sleeping for 10000ms {code};;;","24/Jun/22 07:12;martijnvisser;I'm not sure what is expected from this ticket. Flink currently doesn't support Ozone, which is why the 'New feature' type makes sense. If you want to work on adding support for that, do let me know so I can assign the ticket to you. I don't really see how adding the logs from Ozone in this ticket help since they are Ozone/Hadoop exceptions, not Flink. ;;;","24/Jun/22 08:22;prabhujoseph;[~martijnvisser] Could you please make me a contributor. 

[~bianqi] Do you want to work on this. If not, i would like to take this up.;;;","24/Jun/22 09:33;martijnvisser;[~prabhujoseph] I'll assign it to you, you can create a PR. See https://flink.apache.org/contributing/contribute-code.html for all contribution guides.;;;","24/Jun/22 09:38;prabhujoseph;Thanks [~martijnvisser].;;;","30/Jun/22 11:57;bianqi;I have successfully integrated Flink1.16.0 and Ozone.Below is the sample output of the test
{code:java}
[root@jykj0 bin]# ./flink run-application -t yarn-application /soft/flink-1.16.0/flink-1.16-SNAPSHOT/examples/batch/WordCount.jar --input ofs://jykj0.yarn.com/volume/bucket/warehouse/input --output ofs://jykj0.yarn.com/volume/bucket/warehouse/output21
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/soft/flink-1.16.0/flink-1.16-SNAPSHOT/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/soft/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2022-06-30 19:50:04,390 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /tmp/flink-1.13.5/.yarn-properties-root.
2022-06-30 19:50:04,390 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                [] - Found Yarn properties file under /tmp/flink-1.13.5/.yarn-properties-root.
2022-06-30 19:50:04,546 WARN  org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The configuration directory ('/soft/flink-1.16.0/flink-1.16-SNAPSHOT/conf') already contains a LOG4J config file.If you want to use logback, then please delete or rename the log configuration file.
2022-06-30 19:50:04,609 INFO  org.apache.hadoop.yarn.client.RMProxy                        [] - Connecting to ResourceManager at jykj0.yarn.com/192.168.149.101:8050
2022-06-30 19:50:04,918 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar
2022-06-30 19:50:05,044 INFO  org.apache.hadoop.conf.Configuration                         [] - resource-types.xml not found
2022-06-30 19:50:05,045 INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils           [] - Unable to find 'resource-types.xml'.
2022-06-30 19:50:05,081 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - The configured JobManager memory is 782 MB. YARN will allocate 1024 MB to make up an integer multiple of its minimum allocation memory (1024 MB, configured via 'yarn.scheduler.minimum-allocation-mb'). The extra 242 MB may not be used by Flink.
2022-06-30 19:50:05,081 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - The configured TaskManager memory is 1653 MB. YARN will allocate 2048 MB to make up an integer multiple of its minimum allocation memory (1024 MB, configured via 'yarn.scheduler.minimum-allocation-mb'). The extra 395 MB may not be used by Flink.
2022-06-30 19:50:05,081 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Cluster specification: ClusterSpecification{masterMemoryMB=1024, taskManagerMemoryMB=1653, slotsPerTaskManager=4}
2022-06-30 19:50:09,632 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Adding delegation tokens to the AM container.
2022-06-30 19:50:09,636 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Delegation tokens added to the AM container.
2022-06-30 19:50:09,640 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Submitting application master application_1656324008705_0012
2022-06-30 19:50:09,814 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted application application_1656324008705_0012
2022-06-30 19:50:09,814 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Waiting for the cluster to be allocated
2022-06-30 19:50:09,816 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Deploying cluster, current state ACCEPTED
2022-06-30 19:50:16,619 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - YARN application has been deployed successfully.
2022-06-30 19:50:16,620 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface jykj1.yarn.com:38881 of application 'application_1656324008705_0012'.
[root@jykj0 bin]# hdfs dfs -cat ofs://jykj0.yarn.com/volume/bucket/warehouse/output21
2022-06-30 19:51:09,831 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2022-06-30 19:51:09,885 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2022-06-30 19:51:09,885 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
22 2
232 1
323 1
332 2
abc 2
bb 1
bbb 2
cdd 1 {code}
I think it would be nice to just write the Ozone integration documentation. No need to develop code to integrate Ozone。

[~martijnvisser] 

What do you think about just updating the documentation? Because I think there will be more and more companies using Ozone in the future。

 ;;;","22/Jul/22 14:50;XinWen;Hi [~bianqi] , can you share the steps to substitute Hadoop to Ozone? I am facing the same problem. Maybe write a blog before official document is updated!;;;","03/Aug/22 12:06;martijnvisser;[~bianqi] Updating the documentation is fine!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deduplicate Dependency classes across checks,FLINK-28230,13462973,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Jun/22 12:31,08/Jul/22 08:20,04/Jun/24 20:42,08/Jul/22 08:20,,,,,,1.16.0,,,Build System / CI,,,,,,,0,pull-request-available,,,,"The new Dependency class introduced in FLINK-28201 can also be used for the NoticeFileChecker, or after FLINK-28202, the shade-plugin parser utils.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 08:20:53 UTC 2022,,,,,,,,,,"0|z15fc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 08:20;chesnay;master: c26094e72e555f867308a7e29eb98a365030e6f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Source API alternatives for StreamExecutionEnvironment#fromCollection() methods,FLINK-28229,13462968,13449984,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,23/Jun/22 11:57,12/Feb/24 20:46,04/Jun/24 20:42,05/Dec/23 09:38,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,"* FromElementsFunction
 * FromIteratorFunction

are based on SourceFunction API",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Dec 05 09:38:31 UTC 2023,,,,,,,,,,"0|z15faw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 08:13;vivacell;Hello [~afedulov] ! I would like to start working on this issue.;;;","12/Oct/22 05:46;vivacell;Hello, [~martijnvisser] ! I have implemented FLIP-27 source API alternatives for FromElementsFunction and FromIteratorFunction, but as I can see in the Contribution Guidelines, I can't open PR, because this task is unassigned. Can this task be assigned to me?;;;","12/Oct/22 07:06;martijnvisser;[~vivacell] You can open a PR without being assigned, but it's definitely a good practice ask to get assigned a ticket :) I've done so now;;;","12/Oct/22 09:03;vivacell;[~martijnvisser] thanks! Opened PR - https://github.com/apache/flink/pull/21028;;;","29/Jun/23 09:57;afedulov;[~vivacell] we are currently pursuing an alternative implementation based on the DataGeneratorSource, as proposed by Chesnay [here|https://github.com/apache/flink/pull/21028#pullrequestreview-1138915753]. I am going to assign the ticket to myself and make sure to attribute co-authorship to you if we end up reusing some parts of the original PR.;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","05/Dec/23 09:38;chesnay;master:
e44efbff8070dca3489550fdeadc5e1ce31e68c1
18c03f2e6c593a772f64cdb5c089e2911d3cbc89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Target generation logic might skip over specs when observing already upgraded clusters,FLINK-28228,13462966,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,23/Jun/22 11:42,01/Jul/22 09:20,04/Jun/24 20:42,01/Jul/22 09:20,kubernetes-operator-1.1.0,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"There are currently 2 problems with detecting already upgraded clusters in the AbstractDeploymentObserver:

1. Detecting the initial deployments is not reliable, because the user sends in an upgrade before the observer runs the logic will fail because we have a new higher generation.

2. When an upgrade was detected we should not simply use ReconciliationUtils.updateForSpecReconciliationSuccess because this will mark the current spec as reconciled which is not necessarily true",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 09:20:19 UTC 2022,,,,,,,,,,"0|z15fag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 09:20;gyfora;merged to main 92979998a9342b225b24e04aeb7ed1daebcf730e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate o.a.f.streaming.examples to the new Source API,FLINK-28227,13462963,13449984,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,afedulov,afedulov,afedulov,23/Jun/22 11:31,11/Aug/23 10:24,04/Jun/24 20:42,28/Jun/23 02:15,,,,,,1.18.0,,,Examples,,,,,,,0,pull-request-available,,,,"Reimplement 
 * CarSource
 * SimpleSource
 * SessionWindowing
 * IterateExample
 *  

with the new Source API.

Blocks: https://issues.apache.org/jira/browse/FLINK-28046",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32702,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 28 02:15:20 UTC 2023,,,,,,,,,,"0|z15f9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 01:47;Leo Zhou;Hi [~afedulov] ，if you don't mind, I'd like to take this ticket.  ;;;","11/Jul/22 06:44;martijnvisser;[~Leo Zhou] I've assigned it to you;;;","11/Jul/22 08:13;Leo Zhou;Thanks, [~martijnvisser] .;;;","11/Jul/22 10:45;afedulov;[~Leo Zhou] This is ticket cannot be worked on before FLIP-238 is implemented. These examples will be migrated as part of the implementation of the named FLIP. Thanks for the understanding.;;;","11/Jul/22 11:06;Leo Zhou;[~afedulov] It's ok, looking forward to your nice work. And if there is anything about source migration I can participate in, it would be nice to let me know, thanks.;;;","28/Jun/23 02:15;Weijie Guo;master(1.18) via 3f5428ff1092ac0332c6712318f18a1483aed797.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Run kubernetes pyflink application test' fails while pulling image,FLINK-28226,13462753,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,martijnvisser,martijnvisser,23/Jun/22 10:53,05/Jul/22 14:08,04/Jun/24 20:42,27/Jun/22 02:37,1.14.6,,,,,1.14.6,1.15.2,1.16.0,API / Python,Deployment / Kubernetes,,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37103&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=6592

{code:java}
Jun 23 10:40:35 Flink logs:
Error from server (BadRequest): container ""flink-main-container"" in pod ""flink-native-k8s-pyflink-application-1-5d87889db9-rm8mm"" is waiting to start: image can't be pulled
Jun 23 10:40:35 deployment.apps ""flink-native-k8s-pyflink-application-1"" deleted
Jun 23 10:40:35 clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
Jun 23 10:40:36 pod/flink-native-k8s-pyflink-application-1-5d87889db9-rm8mm condition met
Jun 23 10:40:36 Stopping minikube ...
Jun 23 10:40:36 * Stopping node ""minikube""  ...
Jun 23 10:40:46 * 1 node stopped.
Jun 23 10:40:46 [FAIL] Test script contains errors.
Jun 23 10:40:46 Checking for errors...
Jun 23 10:40:46 No errors in log files.
Jun 23 10:40:46 Checking for exceptions...
Jun 23 10:40:46 No exceptions in log files.
Jun 23 10:40:46 Checking for non-empty .out files...
grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
Jun 23 10:40:46 No non-empty .out files.
Jun 23 10:40:46 

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 27 02:37:37 UTC 2022,,,,,,,,,,"0|z15dz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 10:54;martijnvisser;CC [~hxbks2ks]  [~dianfu] ;;;","27/Jun/22 02:37;hxbks2ks;Merged into master via e52484a6b55147d8d4aa32bb51dc46aadb3acdf0
Merged into release-1.15 via af0a415024881851fd4919cb2efd87edaab7c121
Merged into release-1.14 via a473be67375578ebbd330c98dbdd330dbd34dd66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports custom ENVs in the Helm chart,FLINK-28225,13462442,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,haoxin,haoxin,haoxin,23/Jun/22 10:21,28/Jun/22 13:45,04/Jun/24 20:42,28/Jun/22 13:45,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Can we add custom ENVs supports in the operator Helm?

Such as:
{code:java}
# In the values.yaml

operatorEnvs:
# - name: """"
#   value: """"
webhookEnvs:
# - name: """"
#   value: """"{code}
{code:java}
# In the deployment.yaml
env:
- name: name1
  value: value1
{{- range $k, $v := .Values.operatorEnvs }}
- name: {{ $v.name }}            
  value: {{ $v.value }}          
{{- end }}{code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/276,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 13:45:57 UTC 2022,,,,,,,,,,"0|z15c20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 10:51;gyfora;Makes sense but we should move the suggested configs under the `operatorPod` section of the values.yaml where we have labels and annotations already;;;","23/Jun/22 11:00;haoxin;got it;;;","23/Jun/22 11:08;haoxin;https://github.com/apache/flink-kubernetes-operator/pull/276/files;;;","28/Jun/22 13:45;mbalassi;1937653 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for algorithms and features in Flink ML 2.1,FLINK-28224,13462196,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,23/Jun/22 09:56,19/Apr/23 02:16,04/Jun/24 20:42,19/Apr/23 02:16,,,,,,ml-2.2.0,,,Library / Machine Learning,,,,,,,0,pull-request-available,stale-major,,,The algorithms and new features introduced in Flink ML 2.1 needs to be documented and displayed on Flink ML's document website.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Apr 19 02:15:36 UTC 2023,,,,,,,,,,"0|z15ajc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Apr/23 02:15;lindong;Merge to apache/flink-ml master branch ee53ada6651719af96f758ca48250902fb726709;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add artifact-fetcher to the pod-template.yaml example,FLINK-28223,13462163,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ConradJam,morhidi,,23/Jun/22 09:52,24/Nov/22 01:03,04/Jun/24 20:42,27/Jul/22 09:38,,,,,,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,We could improve the pod template example to have an artifact fetcher.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 27 09:38:19 UTC 2022,,,,,,,,,,"0|z15ac0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 09:19;ConradJam;Hi,[~matyas] I want to try this ticket,can you assignee to me ?;;;","27/Jun/22 10:29;morhidi;[~gyfora] can you reassign it to [~ConradJam] pls?;;;","27/Jul/22 09:38;mbalassi;[https://github.com/apache/flink-kubernetes-operator/commit/792350e8722a3cbe9b32b5bd65983a5e14541933] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sql-csv/json modules,FLINK-28222,13462083,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Jun/22 09:44,05/Jul/22 08:17,04/Jun/24 20:42,05/Jul/22 08:17,,,,,,1.16.0,,,Build System,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,,For consistency and maintainability the csv/json formats should have a dedicated sql-jar module.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 08:17:45 UTC 2022,,,,,,,,,,"0|z159u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 08:17;chesnay;master: 
e413aa494e7b3cb7555e0622d0cc9cb473b1ba50
ee0534200ee25b390a611dfad6d503b698e7a295;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint may corrupt file metas by repeat commit,FLINK-28221,13462031,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,23/Jun/22 09:38,06/Jul/22 09:13,04/Jun/24 20:42,06/Jul/22 09:13,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"[https://github.com/apache/flink-table-store/runs/7020439369?check_suite_focus=true]
Error:  Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 46.953 s <<< FAILURE! - in org.apache.flink.table.store.connector.RescaleBucketITCase 
[32285|https://github.com/apache/flink-table-store/runs/7020439369?check_suite_focus=true#step:4:32286]Error:  testSuspendAndRecoverAfterRescaleOverwrite Time elapsed: 25.545 s <<< ERROR!
{code:java}
Caused by: java.lang.IllegalStateException: Trying to add file {org.apache.flink.table.data.binary.BinaryRowData@9c67b85d, 0, 0, data-4756dfaf-e14e-440e-b211-df2b25f2537a-0.orc} which is already added. Manifest might be corrupted.
32416	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
32417	at org.apache.flink.table.store.file.operation.AbstractFileStoreScan.plan(AbstractFileStoreScan.java:189)
32418	at org.apache.flink.table.store.table.source.TableScan.plan(TableScan.java:99)
32419	at org.apache.flink.table.store.connector.source.FileStoreSource.restoreEnumerator(FileStoreSource.java:117)
32420	at org.apache.flink.table.store.connector.source.FileStoreSource.createEnumerator(FileStoreSource.java:93)
32421	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:197)
32422	... 33 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 09:13:36 UTC 2022,,,,,,,,,,"0|z159io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 09:40;lzljs3620320;We should store FileStoreCommitImpl's commitUser into state.;;;","06/Jul/22 09:13;lzljs3620320;master: dc38c6c8279fa1582ab38c34e34d37aaa09edea2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Table Like support excluding physical columns,FLINK-28220,13461905,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,JustinLee,JustinLee,23/Jun/22 09:25,16/May/24 14:16,04/Jun/24 20:42,,1.15.0,,,,,,,,Table SQL / API,,,,,,,0,,,,,"when users want to Create Table A Like B , they can choose to include or exclude options, computed columns ,etc.  But it's mandatory that table A should inherit all physical columns of table B, which may cause inconvenience in some scenes , such as table A has its own schema and just want to inherit the options of table B.

In our production case, sometimes we need to consume kafka table as RAW format which has an original JSON format , so we would like to Create Table A (data string ) with ('format'='raw') Like B, but it encounters an error saying table A has more than one physical columns because of inheriting these columns from table B. 

so I think it would be more flexible to provide the option to include or exclude physical columns when Using Create Table .. Like .. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-23 09:25:00.0,,,,,,,,,,"0|z158qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor TableStoreCatalog: Introduce a dedicated Catalog for table store,FLINK-28219,13461398,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/Jun/22 08:25,24/Jun/22 09:01,04/Jun/24 20:42,24/Jun/22 09:01,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"We currently have developed a Flink's Catalog.
If we expose this Catalog directly to other connector developers, it is not good and there will be many unsupported interfaces and capabilities.

So here we create a tablestore dedicated catalog.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 09:01:04 UTC 2022,,,,,,,,,,"0|z155m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 09:01;lzljs3620320;master: bdcf482531ac33b77476b7af651ef216e53525de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add readiness and liveness probe to k8s operator,FLINK-28218,13461206,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,mbalassi,mbalassi,23/Jun/22 08:03,18/Aug/22 13:28,04/Jun/24 20:42,18/Aug/22 13:28,,,,,,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Let us add readiness and liveness probes to the operator, we could expose relevant HTTP endpoints for this.

I have observed cases recently where even though the operator pod was running it could not do its job due to missing rolebindings or misconfigured dynamic namespaces.

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 18 13:28:04 UTC 2022,,,,,,,,,,"0|z154fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 12:45;ConradJam;Hi [~mbalassi] , I have idea for readiness and liveliness probe to k8s operator

We can judge the job Rest address of SessionCluster or JobCluster and test its connectivity by requesting its overview, such as localhost:8081/overview 

What do you think ？

 ;;;","23/Jun/22 14:25;gyfora;[~ConradJam] this probe is not about the deployments/jobs but the operator itself. ;;;","28/Jun/22 06:42;ConradJam;Ok, I misunderstood before, can we expose an endpoint in Flink Operator and add probes to helm in this part?;;;","27/Jul/22 13:46;ConradJam;I want to tick this ticket,can [~mbalassi] assign to me ?;;;","27/Jul/22 14:08;gyfora;I think [~pvary] is already working on this;;;","27/Jul/22 14:57;ConradJam;👌 ;;;","18/Aug/22 13:28;gyfora;merged to main 199ed194f81502d3e146265c1694eddaf07e838e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump mysql-connector-java from 8.0.27 to 8.0.28,FLINK-28217,13460973,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Jun/22 07:26,28/Jun/22 07:39,04/Jun/24 20:42,28/Jun/22 07:39,1.16.0,,,,,1.16.0,,,Connectors / JDBC,,,,,,,0,,,,,"We should bump our test dependency for {{mysql-connector-java}} to make sure that we support the latest version of MySQL

This will also address CVE-2022-21363 and CVE-2021-22569",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 07:39:16 UTC 2022,,,,,,,,,,"0|z152zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 07:39;martijnvisser;Fixed in master: c28c70af5a295a487467bf2ae789735831bdd804;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop S3FileSystemFactory does not honor fs.s3.impl,FLINK-28216,13460366,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,prabhujoseph,prabhujoseph,prabhujoseph,23/Jun/22 05:59,29/Jun/22 08:25,04/Jun/24 20:42,,1.15.0,,,,,,,,FileSystems,,,,,,,0,,,,,"Currently Hadoop S3FileSystemFactory has hardcoded the S3 FileSystem implementation to S3AFileSystem. It does not allow to configure any other implementation specified in fs.s3.impl. Suggest to read the fs.s3.impl from Hadoop Config loaded and use the same.

 
{code:java}
@Override
protected org.apache.hadoop.fs.FileSystem createHadoopFileSystem() {
    return new S3AFileSystem();
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 29 08:25:28 UTC 2022,,,,,,,,,,"0|z14z8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 06:04;prabhujoseph;[~martijnvisser] Could you please make me a contributor. Would like to provide a patch for this Jira.;;;","23/Jun/22 06:41;martijnvisser;[~prabhujoseph] Can you elaborate a bit on what you're trying to achieve? Is it not already possible, as documented on https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/s3/ ?;;;","23/Jun/22 07:13;prabhujoseph;[~martijnvisser] EMR has their own [EMRFS|https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-fs.html] to access Amazon S3. Currently, EMR Hive, Spark and Presto uses EMRFS. But Flink has hardcoded to Hadoop S3AFileSystem. It does not check the fs.s3a.impl config from core-site.xml

{code}
<property>
  <name>fs.s3.impl</name>
  <value>com.amazon.ws.emr.hadoop.fs.EmrFileSystem</value>
</property>
{code}

bq. Is it not already possible, as documented on https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/s3/ ?

Yes it is possible to access S3 from Flink using S3AFileSystem as per above document but not using EMRFS or any other implementation.
;;;","28/Jun/22 13:42;martijnvisser;The current Hadoop S3AFileSystem implementation is specific to the Amazon S3 Hadoop plugin. When reading on EMRFS I think this should only work via the Hadoop fallback https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/overview/#hadoop-file-system-hdfs-and-its-other-implementations;;;","28/Jun/22 15:17;prabhujoseph;[~martijnvisser] I was checking the fallback option but it doesn't have the features like Entropy Injection and RecoverableWriter which FlinkS3FileSystem (directly supported FileSystem on top of S3AFileSystem) provides. If FlinkS3FileSystem would be on top of configured fs.s3.impl (EMRFS), could get both Flink and EMRFS provided features.;;;","28/Jun/22 17:24;martijnvisser;The S3 Hadoop plugin is deliberately a small footprint plugin. I wouldn't be in favour of expanding it for EMRFS. I think it would be a better idea to create a separate plugin/implementation, specifically for EMRFS only. ;;;","29/Jun/22 08:19;prabhujoseph;I think it is good idea as this will also provide isolation of dependencies of EMRFS and Hadoop S3 FileSystem. Will work on this and update you. ;;;","29/Jun/22 08:25;martijnvisser;We might need to have a discussion on the mailing list if we want to have this as a Flink community. I'm particularly thinking about maintenance and testing. It's great that you want to make a contribution, but we should see if there are more who are interested in this feature and could help maintain it. 

Regarding testing, if we want to know that this works, we need to have some type of testing with EMRFS. However, that's an Amazon specific thing so how could we test that properly? 

Could you open a discussion thread on the Dev mailing list for this? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Maven Surefire plugin to 3.2.2,FLINK-28215,13460276,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Jun/22 05:48,11/Dec/23 15:37,04/Jun/24 20:42,11/Dec/23 15:37,,,,,,1.19.0,,,Build System,,,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Dec 11 15:37:14 UTC 2023,,,,,,,,,,"0|z14yoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Dec/23 15:37;martijnvisser;Fixed in apache/flink:master ea4cdc28651ad91defd4fc7b371a1f520ea7a262;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayDataSerializer can not be reused to copy customized type of array data ,FLINK-28214,13460173,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yittg,yittg,23/Jun/22 05:32,24/Jun/22 06:04,04/Jun/24 20:42,,,,,,,,,,,,,,,,,0,,,,,"In FLINK-25238, we fix the ArrayDataSerializer to support copying customized type of array data with similar way in MapDataSerializer.

The MapDataSerializer#toBinaryMap always contains copy semantics implicitly
but ArrayDataSerializer#toBinaryArray not.
So the returned value of ArrayDataSerializer#toBinaryArray will be covered by new copied data.

We should always copy from the returned value of ArrayDataSerializer#toBinaryArray in ArrayDataSerializer#copy explicitly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25238,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 06:04:45 UTC 2022,,,,,,,,,,"0|z14y1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 05:34;yittg;[~gyfora] Would you mind checking this issue?;;;","23/Jun/22 06:12;Weijie Guo;Hi [~yittg], I'm not particularly sure why the extra copy is needed here. IMHO, BinaryMapData needs a continuous memory space to store key and value, but reuseKeyWriter and reuseValueWriter have two independent memory spaces, so copying needs to occur.For BinaryArrayData, no special handling should be required.Let me know if I missed something.;;;","24/Jun/22 06:04;yittg;Thanks [~Weijie Guo], let me describe it with some pseudocode.
{code:java}
serializer = ArrayDataSerializer()
arr_a = CustomizedArrayData()
bi_arr_a = serializer.copy(arr_a) // (1)
arr_b = CustomizedArrayData()
bi_arr_b = serializer.copy(arr_b) // (2)
{code}
For now, {{bi_arr_a}} and {{bi_arr_b}} will point to the same object, which is {{{}serializer.reuseArray{}}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamExecutionEnvironment configure method support override pipeline.jars option,FLINK-28213,13459649,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,lsy,lsy,lsy,23/Jun/22 04:25,17/Aug/22 12:45,04/Jun/24 20:42,30/Jun/22 08:49,1.16.0,,,,,1.16.0,,,Runtime / Configuration,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 17 12:45:57 UTC 2022,,,,,,,,,,"0|z14utc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 08:49;zhuzh;Done via 065018dde64a10e8a4cf0ba296e2851a6867871c;;;","15/Aug/22 12:01;twalthr;[~lsy] I'm not super happy about the changes in StreamExecutionEnvironment:
{code}
// merge PipelineOptions.JARS, user maybe set this option in high level such as table
        // module, so here need to merge the jars from both configuration object
        configuration
                .getOptional(PipelineOptions.JARS)
                .ifPresent(
                        jars ->
                                ConfigUtils.mergeCollectionsToConfig(
{code}

This kind of violates the contract of config options. Setting a config option suddenly becomes a merging operation. Can't we deal with this problem in the table module directly? We have similar logic in {{TableConfig#addJobParameter}} for this. Since Flink 1.15, the configuration story in the table module has improved significantly, so TableConfig has access to global configuration.;;;","17/Aug/22 09:03;zhuzh;I agree that we should not change the semantics of the {{PublicEvolving}} method {{StreamExecutionEnvironment#configure()}}, from setting to merging.

Maybe we can add the user set {{PipelineOptions.JARS}} to {{TableConfig.configuration}} on table environment initialization. Later the table module can add new jars into {{PipelineOptions.JARS}} in {{TableConfig.configuration}}. And finally the {{PipelineOptions.JARS}} in {{TableConfig.configuration}} can just override that in the {{configuration}} in {{StreamExecutionEnvironment}} via {{StreamExecutionEnvironment#configure()}}.

This behavior can be explained as the table module is enriching/modifying the user set {{PipelineOptions.JARS}} with program options or job configs, which I think is acceptable, because it's already happening (e.g. ExecutionConfigAccessor#fromProgramOptions(...)).;;;","17/Aug/22 11:26;twalthr;I created FLINK-29014 and will come up with a PR today that should make all parties happy.;;;","17/Aug/22 12:45;lsy;[~twalthr] Thanks for your review, I think your idea maybe good, I didn't notice that TableConfig has provide similar function.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException is thrown when project contains window which dosen't refer all fields of input when using Hive dialect,FLINK-28212,13459568,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,23/Jun/22 04:15,04/Aug/22 08:41,04/Jun/24 20:42,04/Aug/22 08:41,,,,,,1.16.0,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"Can be reproduced by following sql when using Hive dialect:
{code:java}
CREATE TABLE alltypesorc(
                            ctinyint TINYINT,
                            csmallint SMALLINT,
                            cint INT,
                            cbigint BIGINT,
                            cfloat FLOAT,
                            cdouble DOUBLE,
                            cstring1 STRING,
                            cstring2 STRING,
                            ctimestamp1 TIMESTAMP,
                            ctimestamp2 TIMESTAMP,
                            cboolean1 BOOLEAN,
                            cboolean2 BOOLEAN);

select a.ctinyint, a.cint, count(a.cdouble)
  over(partition by a.ctinyint order by a.cint desc
    rows between 1 preceding and 1 following)
from alltypesorc {code}
Then it will throw the exception ""caused by: java.lang.IndexOutOfBoundsException: index (7) must be less than size (1)"".

 

The reson is for such sql, Hive dialect will generate a RelNode:
{code:java}
LogicalSink(table=[*anonymous_collect$1*], fields=[ctinyint, cint, _o__c2])
  LogicalProject(ctinyint=[$0], cint=[$2], _o__c2=[$12])
    LogicalProject(ctinyint=[$0], csmallint=[$1], cint=[$2], cbigint=[$3], cfloat=[$4], cdouble=[$5], cstring1=[$6], cstring2=[$7], ctimestamp1=[$8], ctimestamp2=[$9], cboolean1=[$10], cboolean2=[$11], _o__col13=[COUNT($5) OVER (PARTITION BY $0 ORDER BY $2 DESC NULLS LAST ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)])
      LogicalTableScan(table=[[test-catalog, default, alltypesorc]]) {code}
 Note: the first ProjectNode  from down to top conatins all fields.

And as the  ""{*}1{*} PRECEDING AND *1* FOLLOWING""  in the window whose input will also contains all fields in the project node  will be converted to RexInputRef in Calcite. So, the window will be like 
{code:java}
COUNT($5) OVER (PARTITION BY $0 ORDER BY $2 DESC NULLS LAST ROWS BETWEEN $11 PRECEDING AND $11 FOLLOWING{code}
{color:#172b4d}Note: `$11` is a special field for windows, which is actually recorded as window's constants.{color}

 

But the in rule ""ProjectWindowTransposeRule"", the uncesscassy field(not refered by the top project and window) will be removed,

so the the input of the window will only contains 4 fields (ctinyint, cint, cdouble, count(cdouble)).

Finally, in RelExplainUtil, when explain boundString, it won't find {*}$11{*}, so the exception ""Caused by: java.lang.IndexOutOfBoundsException: index (8) must be less than size (1)"" throws.
{code:java}
val ref = bound.getOffset.asInstanceOf[RexInputRef]
// ref.getIndex will be 11 but origin input size of the window is 3
val boundIndex = ref.getIndex - calcOriginInputRows(window)
// offset = 8, but the window's constants only contains one single element ""1""
val offset = window.constants.get(boundIndex).getValue2
val offsetKind = if (bound.isPreceding) ""PRECEDING"" else ""FOLLOWING""
s""$offset $offsetKind"" {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 04 08:41:36 UTC 2022,,,,,,,,,,"0|z14ubc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 06:09;luoyuxia;It seems a bug of Calcite for it hasn't adjust the index of window's lowerBound/upperBound.

But to fix it in Flink, the idea is straghtforward, trim the produced project node in HiveParser. The logical plan produced by Hive parser should looks like:
{code:java}
LogicalSink(table=[*anonymous_collect$1*], fields=[ctinyint, cint, EXPR$2])
  LogicalProject(ctinyint=[$0], cint=[$2], EXPR$2=[COUNT($5) OVER (PARTITION BY $0 ORDER BY $2 DESC NULLS LAST ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)])
    LogicalTableScan(table=[[test-catalog, default, alltypesorc]]) {code}
 The project node only contains needed field.;;;","04/Aug/22 08:41;jark;Fixed in master: 42287b399f7ca74a419cee0385a84e7859fc7628
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename Schema to TableSchema,FLINK-28211,13459243,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/Jun/22 03:38,23/Jun/22 03:59,04/Jun/24 20:42,23/Jun/22 03:59,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"There are some systems that use schema as a concept of database, so the Schema class will be very confuse in this case, it is better to rename it as TableSchema.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 23 03:59:00 UTC 2022,,,,,,,,,,"0|z14sb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 03:59;lzljs3620320;master: 5a34efce8fadf96fc05194b28f62f0680a5afa62;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkSessionJob fails after FlinkDeployment is updated,FLINK-28210,13459185,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,Daniel Crowe,Daniel Crowe,23/Jun/22 03:31,24/Jun/22 00:57,04/Jun/24 20:42,23/Jun/22 08:11,kubernetes-operator-1.0.0,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,"I created a flink deployment using this example:
{code}
curl https://raw.githubusercontent.com/apache/flink-kubernetes-operator/main/examples/basic-session-job.yaml -o basic-session-job.yaml 

kubectl create -f basic-session-job.yaml 
{code}

Then, I modified the memory allocated to the jobManager and applied the change
{code}
kubectl apply -f basic-session-job.yaml 
{code}

The job manager is restarted to apply the change, but the jobs are not. 

Looking at the operator logs, it appears that something is failing during job status observation:

{noformat}
2022-06-23 03:29:51,189 o.a.f.k.o.c.FlinkSessionJobController [INFO ][default/basic-session-job-example2] Starting reconciliation
2022-06-23 03:29:51,190 o.a.f.k.o.o.JobStatusObserver  [INFO ][default/basic-session-job-example2] Observing job status
2022-06-23 03:29:51,205 o.a.f.k.o.c.FlinkSessionJobController [INFO ][default/basic-session-job-example] Starting reconciliation
2022-06-23 03:29:51,206 o.a.f.k.o.o.JobStatusObserver  [INFO ][default/basic-session-job-example] Observing job status
2022-06-23 03:29:51,208 o.a.f.k.o.c.FlinkDeploymentController [INFO ][default/basic-session-cluster] Starting reconciliation
2022-06-23 03:29:51,227 o.a.f.k.o.c.FlinkDeploymentController [INFO ][default/basic-session-cluster] End of reconciliation
{noformat}
","The [quick start|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start/] was followed to install minikube and the flink operator. 

 

minikube 1.24.1

kubectl 1.24.2

flink operator: 1.0.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 00:57:56 UTC 2022,,,,,,,,,,"0|z14ry8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 04:39;gyfora;This is expected if HA is not configured for the session FlinkDeploymemt. Can you share your session yaml?;;;","23/Jun/22 07:34;Daniel Crowe;Is this the file you are after?

{noformat}
################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  ""License""); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################

apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: basic-session-cluster
spec:
  image: flink:1.15
  flinkVersion: v1_15
  jobManager:
    resource:
      memory: ""2048m""
      cpu: 1
  taskManager:
    resource:
      memory: ""2048m""
      cpu: 1
  serviceAccount: flink
---
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
  name: basic-session-job-example
spec:
  deploymentName: basic-session-cluster
  job:
    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.0/flink-examples-streaming_2.12-1.15.0-TopSpeedWindowing.jar
    parallelism: 4
    upgradeMode: stateless

---
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
  name: basic-session-job-example2
spec:
  deploymentName: basic-session-cluster
  job:
    jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.0/flink-examples-streaming_2.12-1.15.0.jar
    parallelism: 2
    upgradeMode: stateless
    entryClass: org.apache.flink.streaming.examples.statemachine.StateMachineExample
{noformat}
;;;","23/Jun/22 08:11;gyfora;Yes. At this point in 1.0.0 this is an expected limitation of the session mode. 
If you enable HA like in [https://github.com/apache/flink-kubernetes-operator/blob/main/examples/basic-checkpoint-ha.yaml#L30-L31 |https://github.com/apache/flink-kubernetes-operator/blob/main/examples/basic-checkpoint-ha.yaml#L30-L31]

that would hopefully make it work.

We will try to improve this behaviour in later versions, this is related to https://issues.apache.org/jira/browse/FLINK-27979

cc [~aitozi] ;;;","24/Jun/22 00:57;Daniel Crowe;Thank you. I'll give it a go.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSink with EXACTLY_ONCE  produce reduplicate data(flink kafka connector1.14.4),FLINK-28209,13458941,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanyao,tanyao,23/Jun/22 03:01,23/Jun/22 05:55,04/Jun/24 20:42,,1.14.4,,,,,,,,Connectors / Kafka,,,,,,,0,,,,,"I'm trying to read mysql binlog and transport it to kafka;

here is what i'm using :

*Flink: 1.14.4*

*Flink-CDC : 2.2*

*Kafka: CDH6.2(2.1)*

 

*Stage-1:*

mysql-cdc-connector was used to consume mysql binlog data . about 40W rows changed when i executed some sql in mysql, and i can get those 40W rows without any data lose or reduplicate, just the some number as mysql changed . So, i don't think cdc is the problem.

 

Stage-2:

when i got binlog data, first i deserialized it to type of Tuple2<String,String>, which tuple2.f0 has the format  ""db.table"" and i intend to use it as kafka topic for every different db.table, tuple2.f1 contains binlog value only.

 

*Stage-3:*

then, i used KafkaSink (which was introduced in flink 1.14) to write binlog to different kafka topic as tuple2.f0 indicated. 

Here is the code like :

!image-2022-06-23-10-49-01-213.png!

 

As u can see, I just want to use EXACTLY_ONCE semantics，but here is the problem:

after about 10mins waiting for all binlog consumed, i checked all data in a single kafka topic   (just one topic ), the total number of rows is bigger than the number of binlog rows from mysql data changed, because too many reduplicated data sink to kafka. For example

!image-2022-06-23-10-58-15-141.png!

 

Stage-4:

however, when i changed  EXACTLY_ONCE. to.  AT_LEAST_ONCE, everything worked very well, no more reduplicated data in kafka.

 

 

So i'm wonderring , is there any bug in KafkaSink when EXACTLY_ONCE is configured.

 

Can anybody help ? Hope for your answer sincerely.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/22 02:49;tanyao;image-2022-06-23-10-49-01-213.png;https://issues.apache.org/jira/secure/attachment/13045485/image-2022-06-23-10-49-01-213.png","23/Jun/22 02:58;tanyao;image-2022-06-23-10-58-15-141.png;https://issues.apache.org/jira/secure/attachment/13045484/image-2022-06-23-10-58-15-141.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-23 03:01:45.0,,,,,,,,,,"0|z14qg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The method createBatchSink in class HiveTableSink should setParallelism for map operator,FLINK-28208,13458669,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiangang,Jiangang,Jiangang,23/Jun/22 01:35,15/Aug/22 03:10,04/Jun/24 20:42,15/Aug/22 03:10,1.16.0,,,,,1.16.0,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"The problem is found when using Adaptive Batch Scheduler. In these, a simple SQL like ""select * from * where *"" would generate three operators including source, map and sink. The map's parallelism is set to -1 by default and is not the same with source and sink. As a result, the three operators can not be chained together.

 The reason is that we add map operator in method createBatchSink but not setParallelism. The changed code is as following:
{code:java}
private DataStreamSink<Row> createBatchSink(
        DataStream<RowData> dataStream,
        DataStructureConverter converter,
        StorageDescriptor sd,
        HiveWriterFactory recordWriterFactory,
        OutputFileConfig fileNaming,
        final int parallelism)
        throws IOException {

    ...

    return dataStream
            .map((MapFunction<RowData, Row>) value -> (Row) converter.toExternal(value))
            .setParallelism(parallelism) // New added to ensure the right parallelism             .writeUsingOutputFormat(builder.build())
            .setParallelism(parallelism);
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 15 03:10:47 UTC 2022,,,,,,,,,,"0|z14ork:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 04:07;luoyuxia;[~Jiangang] Thanks for reporting it. I think the mapper parallelism should be set. Would you like to take it since the fix is quite simple?

If you have no time, I'll fix it.;;;","05/Aug/22 11:54;Jiangang;[~luoyuxia] I would like to fix it. You can review the code. Thanks.;;;","15/Aug/22 03:10;jark;Fixed in master: 15c422b483ee0c5084eb243edad6de3aad65e6a1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disabling webhook should also disable mutator,FLINK-28207,13458645,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,mbalassi,mbalassi,22/Jun/22 20:11,19/Jul/22 15:32,04/Jun/24 20:42,19/Jul/22 15:32,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The configuration for the mutating webhook suggests that it is nested inside the (validating) webhook:
https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/values.yaml#L73-L76

Based on this I would expect that if I disable the top level webhook it also disables the mutator, however this is not the case:
https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/templates/webhook.yaml#L19-L79
https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/templates/webhook.yaml#L115-L148

I do not see a use case currently where we would want the mutating webhook without having the validating one, so I suggest following the hierarchy that the helm configs imply. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 19 15:32:21 UTC 2022,,,,,,,,,,"0|z14om8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 20:11;mbalassi;[~aitozi] do you have any concerns?;;;","23/Jun/22 02:05;aitozi;The option is introduced to let user can choose to enable/disable the validator and mutator individually. But, for compatibility, the validator option keep with the {{webhook.create}}, It should be called {{webhook.validator.create}} better, I think.  The context is [here|https://github.com/apache/flink-kubernetes-operator/pull/265#discussion_r895715818]

One another solution: we can add the {{webhook.validator.create}} and let the {{webhook.create}} control both, WDYT ?;;;","23/Jun/22 07:57;mbalassi;Thanks for the clarification [~aitozi]. I will go with your suggestion.;;;","19/Jul/22 15:32;gyfora;merged to main a2908f5b773573fe2b76dd6061512a27f698ec4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException on Checkpoint Recovery,FLINK-28206,13458639,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,uharaqo,uharaqo,22/Jun/22 19:27,29/Jun/22 03:09,04/Jun/24 20:42,,1.14.4,,,,,,,,Runtime / Checkpointing,,,,,,,1,,,,," 

We have only one Job Manager in Kubernetes and it suddenly got killed without any logs. A new Job Manager process could not recover from a checkpoint due to an EOFException. 
Task Managers killed themselves since they could not find any Job Manager. There were no error logs other than that on the Task Manager side.

It looks to me that the checkpoint is corrupted. Is there a way to identify the cause? What would you recommend us to do to mitigate this problem?

Here's the logs during the recovery phase. (Removed the stacktrace. Please find that at the bottom.)
{noformat}
{""timestamp"":""2022-06-22T17:21:25.870Z"",""level"":""INFO"",""logger"":""org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils"",""message"":""Recovering checkpoints from KubernetesStateHandleStore{configMapName='univex-flink-record-collector-46071c6a64e47d1ce828dfe032f943a6-jobmanager-leader'}.""}
{""timestamp"":""2022-06-22T17:21:25.875Z"",""level"":""INFO"",""logger"":""org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils"",""message"":""Found 1 checkpoints in KubernetesStateHandleStore{configMapName='univex-flink-record-collector-46071c6a64e47d1ce828dfe032f943a6-jobmanager-leader'}.""}
{""timestamp"":""2022-06-22T17:21:25.876Z"",""level"":""INFO"",""logger"":""org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils"",""message"":""Trying to fetch 1 checkpoints from storage.""}
{""timestamp"":""2022-06-22T17:21:25.876Z"",""level"":""INFO"",""logger"":""org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils"",""message"":""Trying to retrieve checkpoint 58130.""}
{""timestamp"":""2022-06-22T17:21:25.901Z"",""level"":""ERROR"",""logger"":""org.apache.flink.runtime.entrypoint.ClusterEntrypoint"",""message"":""Fatal error occurred in the cluster entrypoint."",""level"":""INFO"",""logger"":""org.apache.flink.runtime.entrypoint.ClusterEntrypoint"",""message"":""Shutting StandaloneSessionClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..""}
{""timestamp"":""2022-06-22T17:21:25.921Z"",""level"":""INFO"",""logger"":""org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint"",""message"":""Shutting down rest endpoint.""}
{""timestamp"":""2022-06-22T17:21:25.922Z"",""level"":""INFO"",""logger"":""org.apache.flink.runtime.blob.BlobServer"",""message"":""Stopped BLOB server at 0.0.0.0:6124""}
{noformat}

The stacktrace of the ERROR:
{noformat}
org.apache.flink.util.FlinkException: JobMaster for job 46071c6a64e47d1ce828dfe032f943a6 failed.
    at org.apache.flink.runtime.dispatcher.Dispatcher.jobMasterFailed(Dispatcher.java:913)
    at org.apache.flink.runtime.dispatcher.Dispatcher.jobManagerRunnerFailed(Dispatcher.java:473)
    at org.apache.flink.runtime.dispatcher.Dispatcher.handleJobManagerRunnerResult(Dispatcher.java:450)
    at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$runJob$3(Dispatcher.java:427)
    at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
    at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
    at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
    at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
    at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
    at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
    at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
    at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
    at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
    at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
    at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Failed to initialize high-availability completed checkpoint store
    at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
    at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702)
    ... 3 more
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException: Failed to initialize high-availability completed checkpoint store
    at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:316)
    at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:114)
    at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
    ... 3 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Failed to initialize high-availability completed checkpoint store
    at org.apache.flink.runtime.scheduler.SchedulerUtils.createCompletedCheckpointStoreIfCheckpointingIsEnabled(SchedulerUtils.java:57)
    at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:180)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:140)
    at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:134)
    at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110)
    at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:346)
    at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:323)
    at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:106)
    at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:94)
    at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
    ... 4 more
Caused by: org.apache.flink.util.FlinkException: Could not retrieve checkpoint 58130 from state handle under checkpointID-0000000000000058130. This indicates that the retrieved state handle is broken. Try cleaning the state handle store.
    at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils.retrieveCompletedCheckpoint(DefaultCompletedCheckpointStoreUtils.java:111)
    at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils.retrieveCompletedCheckpoints(DefaultCompletedCheckpointStoreUtils.java:89)
    at org.apache.flink.kubernetes.utils.KubernetesUtils.createCompletedCheckpointStore(KubernetesUtils.java:314)
    at org.apache.flink.kubernetes.highavailability.KubernetesCheckpointRecoveryFactory.createRecoveredCompletedCheckpointStore(KubernetesCheckpointRecoveryFactory.java:78)
    at org.apache.flink.runtime.scheduler.SchedulerUtils.createCompletedCheckpointStore(SchedulerUtils.java:91)
    at org.apache.flink.runtime.scheduler.SchedulerUtils.createCompletedCheckpointStoreIfCheckpointingIsEnabled(SchedulerUtils.java:54)
    ... 13 more
Caused by: java.io.EOFException
    at java.base/java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2872)
    at java.base/java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3367)
    at java.base/java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:936)
    at java.base/java.io.ObjectInputStream.<init>(ObjectInputStream.java:379)
    at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.<init>(InstantiationUtil.java:68)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:612)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:595)
    at org.apache.flink.runtime.state.RetrievableStreamStateHandle.retrieveState(RetrievableStreamStateHandle.java:59)
    at org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils.retrieveCompletedCheckpoint(DefaultCompletedCheckpointStoreUtils.java:102)
    ... 18 more
{noformat}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Kotlin,,Wed Jun 29 03:09:52 UTC 2022,,,,,,,,,,"0|z14ol4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 02:04;chenya_zhang;+1 that we are seeing similar exceptions today when running with Flink 1.14 to read/deserialize from checkpoint states:
{code:java}
org.apache.flink.util.FlinkRuntimeException: Unexpected list element deserialization failure at org.apache.flink.runtime.state.ListDelimitedSerializer.deserializeNextElement(ListDelimitedSerializer.java:89) at org.apache.flink.runtime.state.ListDelimitedSerializer.deserializeList(ListDelimitedSerializer.java:51) at org.apache.flink.contrib.streaming.state.RocksDBListState.getInternal(RocksDBListState.java:120) at org.apache.flink.contrib.streaming.state.RocksDBListState.get(RocksDBListState.java:112) at org.apache.flink.contrib.streaming.state.RocksDBListState.get(RocksDBListState.java:61) at org.apache.flink.runtime.state.metrics.LatencyTrackingListState.get(LatencyTrackingListState.java:63) at org.apache.flink.runtime.state.metrics.LatencyTrackingListState.get(LatencyTrackingListState.java:34) at org.apache.flink.streaming.runtime.operators.windowing.WindowOperator.onEventTime(WindowOperator.java:475) at org.apache.flink.streaming.api.operators.InternalTimerServiceImpl.advanceWatermark(InternalTimerServiceImpl.java:302) at org.apache.flink.streaming.api.operators.InternalTimeServiceManagerImpl.advanceWatermark(InternalTimeServiceManagerImpl.java:180) at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:603) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitWatermark(OneInputStreamTask.java:239) at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:200) at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:136) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) at java.base/java.lang.Thread.run(Thread.java:829) 

Caused by: java.io.EOFException at org.apache.flink.core.memory.DataInputDeserializer.readFully(DataInputDeserializer.java:172) at org.apache.flink.formats.avro.utils.DataInputDecoder.readBytes(DataInputDecoder.java:95) at org.apache.avro.io.ResolvingDecoder.readBytes(ResolvingDecoder.java:243) at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:543) at org.apache.avro.generic.GenericDatumReader.readBytes(GenericDatumReader.java:534) at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:193) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:160) at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:259) at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:247) at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:179) at org.apache.avro.generic.GenericDatumReader.readArray(GenericDatumReader.java:298) at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:183) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:160) at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:259) at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:247) at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:179) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:160) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153) at 
... 25 more{code}
 ;;;","24/Jun/22 02:28;yunta;[~chenya_zhang] I don't think your problem is the same as this reported one, and you problem mainly happens during deserializing avro record of RocksDB list state. I think you can create another ticket with more details.

[~uharaqo] Your problem might be related to the corrupted {{_metadata}} file under remote {{chk-x}} folder, could you check whether the remote checkpoint meta file is complete?;;;","27/Jun/22 18:23;uharaqo;Hi [~yunta] , thanks for the advice. I have two metadata files: corrupted and invalid files. Do you know which serializer I should use to deserialize the binaries?;;;","27/Jun/22 19:52;uharaqo;RetrievableStreamStateHandle#retrieveState() would be the code that failed on production. But this code failed on my local machine even with a valid metadata file downloaded from S3:
{noformat}
new RetrievableStreamStateHandle(org.apache.flink.core.fs.Path.fromLocalFile(localFile), localFile.length()).retrieveState();{noformat}
Error:
{noformat}
Exception in thread ""main"" java.io.StreamCorruptedException: invalid stream header: 4960672D
    at java.base/java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:940)
    at java.base/java.io.ObjectInputStream.<init>(ObjectInputStream.java:379)
    at org.apache.flink.util.InstantiationUtil$ClassLoaderObjectInputStream.<init>(InstantiationUtil.java:68)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:615)
    at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:595)
    at org.apache.flink.runtime.state.RetrievableStreamStateHandle.retrieveState(RetrievableStreamStateHandle.java:59)
    at DeserializationTest.main(DeserializationTest.java:35){noformat}
Any thoughts?;;;","28/Jun/22 11:26;yunta;You can try to use [Checkpoints#loadCheckpointMetadata |https://github.com/apache/flink/blob/99ccebf7d908c77ccca97ca9d66330474182d36d/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/Checkpoints.java#L100] to load the _metadata file to see what's wrong.;;;","28/Jun/22 17:18;uharaqo;Thanks [~yunta] . I can read the metadata file (checkpoint: 58130) which caused the EOF error:
{noformat}
result = {CheckpointMetadata@955} ""Checkpoint Metadata""
 checkpointId = 58130
 operatorStates = {ArrayList@957}  size = 11
  0 = {OperatorState@960} ""OperatorState(operatorID: 5323682bad0857bbc5dc34e8a0198f36, parallelism: 64, maxParallelism: 128, coordinatorState: (none), sub task states: 64, total size (bytes): 3607732189)""
  1 = {OperatorState@961} ""OperatorState(operatorID: b0d7d72b41ddffa0e746c98bfade0740, parallelism: 64, maxParallelism: 128, coordinatorState: (none), sub task states: 64, total size (bytes): 11901811)""
  2 = {OperatorState@962} ""OperatorState(operatorID: 799506d4ab267dabfd7e5e151c32b771, parallelism: 64, maxParallelism: 128, coordinatorState: (none), sub task states: 0, total size (bytes): 0)""
  3 = {OperatorState@963} ""OperatorState(operatorID: bc764cd8ddf7a0cff126f51c16239658, parallelism: 64, maxParallelism: 128, coordinatorState: 528 bytes, sub task states: 64, total size (bytes): 16400)""
  4 = {OperatorState@964} ""OperatorState(operatorID: 6ac701675b3507fc60576d5eb1d2a02a, parallelism: 64, maxParallelism: 128, coordinatorState: (none), sub task states: 0, total size (bytes): 0)""
  5 = {OperatorState@965} ""OperatorState(operatorID: 4f12c5e521389e5e6c62d38d49148502, parallelism: 64, maxParallelism: 128, coordinatorState: (none), sub task states: 64, total size (bytes): 0)""
  6 = {OperatorState@966} ""OperatorState(operatorID: 2963852293169ba90d9d1e7d6308db5c, parallelism: 64, maxParallelism: 128, coordinatorState: 144 bytes, sub task states: 64, total size (bytes): 15296)""
  7 = {OperatorState@967} ""OperatorState(operatorID: 30ddfdac31a80b264a11886ca4e04ea8, parallelism: 64, maxParallelism: 128, coordinatorState: (none), sub task states: 64, total size (bytes): 15868786)""
  8 = {OperatorState@968} ""OperatorState(operatorID: ee7af715df0b4d5489c7e945244bab03, parallelism: 64, maxParallelism: 128, coordinatorState: (none), sub task states: 64, total size (bytes): 646402101)""
  9 = {OperatorState@969} ""OperatorState(operatorID: feca28aff5a3958840bee985ee7de4d3, parallelism: 64, maxParallelism: 128, coordinatorState: 128 bytes, sub task states: 64, total size (bytes): 15264)""
  10 = {OperatorState@970} ""OperatorState(operatorID: 99f06a2f6abbb4a5ff119455ad01a44b, parallelism: 64, maxParallelism: 128, coordinatorState: (none), sub task states: 64, total size (bytes): 0)""
 masterStates = {Collections$EmptyList@958}  size = 0
{noformat}
We use Kubernetes and flink-s3-fs-presto. The network was stable at the time and we were able to restart the job by ignoring the savepoint and checkpoint.

Do you think the EOF error was caused by the Presto plugin?;;;","29/Jun/22 03:09;yunta;[~uharaqo] After you checked the {{_metadata}}, I think the problem is related with the serialized RetrievableStateHandle stored in {{high-availability.storageDir}}. Since the store path is named as [random string|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/persistence/filesystem/FileSystemStateStorageHelper.java#L75-L77], it might not be easy to find which one points to your checkpoint-58310.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memory leak in the timing flush of the jdbc-connector,FLINK-28205,13458585,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,michael201827,michael201827,22/Jun/22 13:58,03/Aug/22 07:10,04/Jun/24 20:42,27/Jun/22 07:50,1.13.6,1.14.5,1.15.0,,,,,,Connectors / JDBC,,,,,,,0,,,,,"Bug position: org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.scheduler

When writing with the jdbc-connector, the RuntimeException thrown by the scheduled thread to process the flush record is caught, this will cause the flink task to not fail out until new data arrives. So, during this time, the scheduled thread will continue to wrap the previous flushException by creating a RuntimeException. For each flushException, the object reference cannot be released and cannot be reclaimed by the GC, resulting in a memory leak.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24677,,,FLINK-24677,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,,,Thu Jun 23 11:44:26 UTC 2022,,,,,,,,,,"0|z14o94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 14:06;martijnvisser;[~michael201827] Please use English for your ticket;;;","23/Jun/22 02:47;michael201827;I changed the language;;;","23/Jun/22 11:44;wanglijie;I think this is the same issue with FLINK-24677;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleting a FlinkDeployment results in an error on the pod,FLINK-28204,13458580,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mcasters,mcasters,22/Jun/22 13:35,24/Nov/22 01:03,04/Jun/24 20:42,22/Jun/22 14:34,kubernetes-operator-1.0.0,,,,,kubernetes-operator-1.0.1,,,Kubernetes Operator,,,,,,,0,,,,,"I didn't configure the memory settings of my Flink cluster correctly in the Flink deployment Yaml.

So I thought I would delete the deployment but I'm getting this error in the log of the f-k-o pod:
{code:java}
2022-06-22 13:19:13,521 o.a.f.k.o.c.FlinkDeploymentController [INFO ][default/apache-hop-flink] Deleting FlinkDeployment
2022-06-22 13:19:13,521 i.j.o.p.e.ReconciliationDispatcher [ERROR][default/apache-hop-flink] Error during event processing ExecutionScope{ resource id: CustomResourceID{name='apache-hop-flink', namespace='default'}, version: 23709} failed.
java.lang.RuntimeException: Cannot create observe config before first deployment, this indicates a bug.
at org.apache.flink.kubernetes.operator.config.FlinkConfigManager.getObserveConfig(FlinkConfigManager.java:137)
at org.apache.flink.kubernetes.operator.service.FlinkService.cancelJob(FlinkService.java:357)
at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.shutdown(ApplicationReconciler.java:327)
at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractDeploymentReconciler.cleanup(AbstractDeploymentReconciler.java:56)
at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractDeploymentReconciler.cleanup(AbstractDeploymentReconciler.java:37)
at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:107)
at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:59)
at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:68)
at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:50)
at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)
at io.javaoperatorsdk.operator.processing.Controller.cleanup(Controller.java:49)
at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleCleanup(ReconciliationDispatcher.java:252)
at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:72)
at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)
at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.base/java.lang.Thread.run(Unknown Source) {code}
So in essence this leaves me in a state between not deployed and not able to delete the flinkdeployment.

 ","AWS EKS

 
{code:java}
kubectl version                                                                                                                                                
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.8"", GitCommit:""a12b886b1da059e0190c54d09c5eab5219dd7acf"", GitTreeState:""clean"", BuildDate:""2022-06-17T22:27:29
Z"", GoVersion:""go1.17.10"", Compiler:""gc"", Platform:""linux/amd64""}

Server Version: version.Info{Major:""1"", Minor:""22+"", GitVersion:""v1.22.9-eks-a64ea69"", GitCommit:""540410f9a2e24b7a2a870ebfacb3212744b5f878"", GitTreeState:""clean"", BuildDate:""2022-0
5-12T19:15:31Z"", GoVersion:""go1.16.15"", Compiler:""gc"", Platform:""linux/amd64""}


 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 13:56:29 UTC 2022,,,,,,,,,,"0|z14o80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 14:18;morhidi;Hi Matt, this is a know issue in v1.0.0. You cannot delete a deployment that has never run. You can use kubectl apply... to correct the CR, then you can delete it. There is a fix for it already but it has not been released yet.;;;","22/Jun/22 14:33;gyfora;We will probably have a patch release (1.0.1) in a few days but if you want to try the unreleased fix you can use one of our automatically built images:
[https://github.com/apache/flink-kubernetes-operator/pkgs/container/flink-kubernetes-operator/25573191?tag=ac50e32]

 ;;;","24/Jun/22 11:18;mcasters;These pods aren't doing anything so I can just create new ones as well with a different name ;)

I just created the case because of the bug message in the log.  Thanks for the quick response folks. You've been incredibly helpful!;;;","24/Jun/22 11:35;morhidi;[~mcasters] the release candidate is available at: [https://dist.apache.org/repos/dist/dev/flink/flink-kubernetes-operator-1.0.1-rc1] 

 

You can verify if it solves your problem

helm repo add flink-kubernetes-operator-rc1 https://dist.apache.org/repos/dist/dev/flink/flink-kubernetes-operator-1.0.1-rc1

helm install flink-kubernetes-operator flink-kubernetes-operator-rc1/flink-kubernetes-operator --set webhook.create=false;;;","24/Jun/22 13:53;mcasters;Hi Matyas,

That works like charm.  I still had a few of these broken pods around and they deleted immediately with 1.0.1-rc1.

Great work!
Matt;;;","24/Jun/22 13:56;morhidi;Great! The official patch release will be out too soon (when the voting period ends).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mark all bundled dependencies as optional,FLINK-28203,13458571,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Jun/22 12:58,12/May/23 13:44,04/Jun/24 20:42,12/May/23 13:44,,,,,,1.18.0,,,Build System,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri May 12 13:44:32 UTC 2023,,,,,,,,,,"0|z14o60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 13:44;chesnay;master:
edf5666224f12e4b345ecafc6ae2fe5e3ecea393
49dcdf0fc2a5f113c0c1558b65df4ffa587ff72a
4fa80bf27b5501e87c9abcb2004fac8083c6d660;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generalize utils around shade-plugin,FLINK-28202,13458570,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Jun/22 12:57,16/Nov/22 13:57,04/Jun/24 20:42,16/Nov/22 13:57,,,,,,1.17.0,,,Build System,Build System / CI,,,,,,0,pull-request-available,stale-assigned,,,"We'll be adding another safeguard against developer mistakes which also parses the output of the shade plugin, like the license checker.

We should generalize this parsing such that both checks can use the same code.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Nov 16 13:57:30 UTC 2022,,,,,,,,,,"0|z14o5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/Nov/22 13:57;chesnay;master: 5b32b9defcbb2adf7a0ad0898a67c40e0e012ebb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generalize utils around dependency-plugin,FLINK-28201,13458566,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Jun/22 12:39,07/Jul/22 13:22,04/Jun/24 20:42,07/Jul/22 13:22,,,,,,1.16.0,,,Build System,Build System / CI,,,,,,0,pull-request-available,,,,"We'll be adding another safeguard against developer mistakes which also parses the output of the dependency plugin, like the scala suffix checker.

We should generalize this parsing such that both checks can use the same code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 13:22:38 UTC 2022,,,,,,,,,,"0|z14o4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 13:22;chesnay;master: 81c1cb82afc7c1ef66a5dd21555997abaf4a7817;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Table Store Hive reader documentation,FLINK-28200,13458545,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,lzljs3620320,lzljs3620320,22/Jun/22 11:51,19/Jul/22 09:13,04/Jun/24 20:42,19/Jul/22 09:13,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 19 09:13:27 UTC 2022,,,,,,,,,,"0|z14o08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 09:13;lzljs3620320;master: d8bce17793451b19ef10766cb9b813dd728d0ebd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failures on YARNHighAvailabilityITCase.testClusterClientRetrieval and YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint,FLINK-28199,13458540,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,22/Jun/22 11:38,16/Nov/23 13:27,04/Jun/24 20:42,,1.16.0,1.17.1,,,,,,,Deployment / YARN,,,,,,,0,test-stability,,,,"{code:java}
Jun 22 08:57:50 [ERROR] Errors: 
Jun 22 08:57:50 [ERROR]   YARNHighAvailabilityITCase.testClusterClientRetrieval » Timeout testClusterCli...
Jun 22 08:57:50 [ERROR]   YARNHighAvailabilityITCase.testKillYarnSessionClusterEntrypoint:156->YarnTestBase.runTest:288->lambda$testKillYarnSessionClusterEntrypoint$0:182->waitForJobTermination:325 » Execution
Jun 22 08:57:50 [INFO] 
Jun 22 08:57:50 [ERROR] Tests run: 27, Failures: 0, Errors: 2, Skipped: 0
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37037&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=29523",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Nov 16 13:27:34 UTC 2023,,,,,,,,,,"0|z14nz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 11:40;martijnvisser;[~ferenc-csaky] [~bgeng777] This error occurred after we've merged FLINK-27667. Any thoughts on this? ;;;","22/Jun/22 14:56;ferenc-csaky;Hm, this is weird. As I see this happened only 1 time since the fix for FLINK-27667 got merged, so this thing can be unrelated.

testKillYarnSessionClusterEntrypoint() timed out after 60s:
{code:java}
java.lang.AssertionError: There is at least one application on the cluster that is not finished.[App application_1655885546027_0002 is in state RUNNING.]
Jun 22 08:44:47 		at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:325)
Jun 22 08:44:47 		at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:289)
Jun 22 08:44:47 		... 60 more {code}
testClusterClientRetrieval() timed out after 30 minutes, cause:
{code:java}
java.lang.AssertionError: There is at least one application on the cluster that is not finished.[App application_1655885546027_0003 is in state ACCEPTED., App application_1655885546027_0002 is in state RUNNING.]
Jun 22 08:44:47 			at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:325)
Jun 22 08:44:47 			at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:289)
Jun 22 08:44:47 			... 60 more {code}
It failed to shutdown the 0002 app.;;;","22/Jun/22 15:01;bgeng777;According to the log, the exception is thrown as the yarn application for

testKillYarnSessionClusterEntrypoint is not stopped as expected, which also leads to the failure of 

testClusterClientRetrieval furthermore. 

Similary as [~ferenc-csaky]'s analysis, IIUC, the PR for FLINK-27677 may not be relevant to this failure as FLINK-27677 's codes only change the @AfterAll method, which should be executed after all tests finished while this failure happens after a single test.

It looks that {{killApplicationAndWait()}} may wrongly return due to the side effect of the previous test.;;;","22/Jun/22 18:07;martijnvisser;It was indeed only once that this happened. Do we want to keep the ticket open to see if it was just a fluke or if this is going to re-appear more frequently? ;;;","23/Jun/22 17:16;ferenc-csaky;I would say to keep it open for now, it is possible it will happen again.;;;","11/Jul/22 02:22;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37975&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","07/Sep/22 06:04;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40763&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347;;;","22/Nov/22 11:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43216&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30889;;;","02/Aug/23 11:11;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51893&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=27086

This time, only {{testClusterClientRetrieval}} failed. But the JM process finished without any issues at {{2023-08-02 03:21:25,901}}. The cleanup is triggered in the test. But the application wasn't cleared:
{code}
Aug 02 03:22:02 [ERROR] org.apache.flink.yarn.YARNHighAvailabilityITCase.testClusterClientRetrieval  Time elapsed: 29.494 s  <<< FAILURE!
Aug 02 03:22:02 java.lang.AssertionError: There is at least one application on the cluster that is not finished.[App application_1690946369165_0003 is in state RUNNING.]
Aug 02 03:22:02         at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:336)
Aug 02 03:22:02         at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:300)
Aug 02 03:22:02         at org.apache.flink.yarn.YARNHighAvailabilityITCase.testClusterClientRetrieval(YARNHighAvailabilityITCase.java:221)
Aug 02 03:22:02         at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

What about increasing the deadline for shutting down the YARN applications? It's currently set to 10s (see [apache/flink:org.apache.flink.yarn.YarnTestBase:310|https://github.com/apache/flink/blob/c8ae39d4ac73f81873e1d8ac37e17c29ae330b23/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YarnTestBase.java#L310];;;","30/Aug/23 09:13;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52808&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30468;;;","16/Nov/23 13:27;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54348&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=28130;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraConnectorITCase fails with timeout,FLINK-28198,13458539,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,echauchot,martijnvisser,martijnvisser,22/Jun/22 11:33,14/Apr/23 10:25,04/Jun/24 20:42,14/Apr/23 08:18,1.16.0,,,,,cassandra-3.1.0,,,Connectors / Cassandra,,,,,,,0,pull-request-available,test-stability,,,"{code:java}
Jun 22 07:57:37 [ERROR] org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.testRaiseCassandraRequestsTimeouts  Time elapsed: 12.067 s  <<< ERROR!
Jun 22 07:57:37 com.datastax.driver.core.exceptions.OperationTimedOutException: [/172.17.0.1:59915] Timed out waiting for server response
Jun 22 07:57:37 	at com.datastax.driver.core.exceptions.OperationTimedOutException.copy(OperationTimedOutException.java:43)
Jun 22 07:57:37 	at com.datastax.driver.core.exceptions.OperationTimedOutException.copy(OperationTimedOutException.java:25)
Jun 22 07:57:37 	at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:35)
Jun 22 07:57:37 	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:293)
Jun 22 07:57:37 	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:58)
Jun 22 07:57:37 	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:39)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37037&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=13736",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Apr 14 10:25:27 UTC 2023,,,,,,,,,,"0|z14nyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 11:34;martijnvisser;[~echauchot] I'm seeing this failure quite frequently, do you want me to assign this to you? I can also share more examples where this has failed. ;;;","27/Jun/22 13:37;echauchot;[~martijnvisser] thanks for raising this. Yes please do so.;;;","29/Jun/22 02:45;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37313&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","30/Jun/22 07:12;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37382&view=logs&j=a1ac4ce4-9a4f-5fdb-3290-7e163fba19dc&t=3a8f44aa-4415-5b14-37d5-5fecc568b139&l=14109

[~echauchot] These tests are failing on createTable but its the same error, so adding them to this ticket

One more note: it's happening for both JDK8 and JDK11;;;","01/Jul/22 09:28;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37433&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=13906;;;","05/Jul/22 03:45;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37607&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","05/Jul/22 09:24;martijnvisser;[~echauchot] Any update from your end?;;;","05/Jul/22 10:14;echauchot;Hi [~martijnvisser] I was busy on an urgent matter, I just started to take a look. Seems like createTable fails with timeout but not the first create keyspace request. Raising once again the timeouts in the cassandra cluster does not seem to be the correct move here. All the CQL requests are launched synchronously but there might be residual load on the cluster. I'll try to find a cluster internal metric and block on its status to ensure the load is ok before starting new tests 
;;;","06/Jul/22 08:17;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37694&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=13869

Could it be an issue with the Cassandra setup in Testcontainers? Perhaps we need to activate some extra logging for these tests? ;;;","06/Jul/22 09:02;echauchot;[~martijnvisser] I could also raise the driver timeout that should be higher than the server side timeout (10s raised to 30s): https://docs.datastax.com/en/developer/java-driver/3.11/manual/socket_options/#driver-read-timeout.
I also figured out that the driver timeouts were set on the clusterBuilder because previous timeout errors were only on test cases. Here, the request timeouts are on _session.execute_ that do not use the ClusterBuilder. Thus, they use the default 12s timeout. I'll raise to 36s per session request.;;;","13/Jul/22 02:25;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=dbe51908-4958-5c8c-9557-e10952d4259d&t=55d11a16-067d-538d-76a3-4c096a3a8e24;;;","14/Jul/22 02:37;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=dbe51908-4958-5c8c-9557-e10952d4259d&t=55d11a16-067d-538d-76a3-4c096a3a8e24;;;","14/Jul/22 02:55;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38146&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","18/Jul/22 07:59;echauchot;[~hxbks2ks]I know the build is flaky, I'm waiting for the merge of the above PR to fix it. [~chesnay] is busy or maybe on vacation, [~hxbks2ks], [~martijnvisser] maybe you can review/merge the PR?;;;","18/Jul/22 08:20;martijnvisser;Merged in master: 7cdf0e68b7228c239348e8d623083736ec9ddc4b;;;","06/Apr/23 04:59;Sergey Nuyanzin;still reproduced for 1.16
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47891&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=15459;;;","06/Apr/23 07:45;echauchot;Will take a look;;;","14/Apr/23 10:25;dannycranmer;Given this is backwards compatible I am moving this to v3.1.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink didn't deletes the YARN application files when the submit is failed in application mode,FLINK-28197,13458536,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zlzhang0122,zlzhang0122,22/Jun/22 11:27,22/Jun/22 11:48,04/Jun/24 20:42,,1.14.2,,,,,,,,Deployment / YARN,,,,,,,0,,,,,"When users submit a Flink job to YARN and the submit is failed in yarn Application Mode, the YARN application files such as Flink binaries, libraries, etc. won't be delete and will exists permanently unless users delete manually.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-22 11:27:43.0,,,,,,,,,,"0|z14ny8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename hadoop.version property,FLINK-28196,13458299,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Jun/22 10:22,22/Jun/22 17:16,04/Jun/24 20:42,22/Jun/22 17:16,,,,,,1.16.0,,,Build System,,,,,,,0,pull-request-available,,,,"Maven 3.8.5 had a change (as I understand it for consistency purposes) where properties set on the command-line are also applied to upstream dependencies.

See https://issues.apache.org/jira/browse/MNG-7417

In other words, since Hadoop has a {{hadoop.version}} property in their parent pom, when we set this CI it not only sets _our_ property, but also the one from Hadoop.

We should prefix our property with ""flink"" to prevent such conflicts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 17:16:24 UTC 2022,,,,,,,,,,"0|z14mhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 17:16;chesnay;master: 3b50f19ad27a49c5b804e8e811cbb2062dcff003;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Annotate Python3.6 as deprecated in PyFlink 1.16,FLINK-28195,13458114,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,22/Jun/22 09:58,28/Jun/22 06:01,04/Jun/24 20:42,27/Jun/22 09:01,1.16.0,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,Python 3.6 extended support end on 23 December 2021. We plan that PyFlink 1.16 will be the last version support Python3.6.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 27 09:01:05 UTC 2022,,,,,,,,,,"0|z14lcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 09:01;hxbks2ks;Merged into master via 459711bfd0efac85761b008031801857f60bf479;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove workaround around avro sql jar,FLINK-28194,13457887,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Jun/22 09:32,23/Jun/22 01:06,04/Jun/24 20:42,23/Jun/22 01:06,,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,"Because of FLINK-17417 flink-python contains a workaround that manually assembles a sort-of avro sql jar.
Rely on the sql-avro jar instead and remove the workaround.",,,,,,,,,,,,,FLINK-28016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17417,FLINK-28183,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 23 01:06:49 UTC 2022,,,,,,,,,,"0|z14jy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 01:06;dianfu;Merged to master via 40efeb9314fe70bbc600a9131a7031ac193f246d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable to identify whether a job vertex contains source/sink operators,FLINK-28193,13457204,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,22/Jun/22 08:07,24/Jun/22 12:03,04/Jun/24 20:42,24/Jun/22 12:03,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"Speculative execution does not support sources/sinks in the first version. Therefore, it will not create speculation instances for vertices which contains source/sink operators.

Note that a job vertex with no input/output does not mean it is a source/sink vertex. Multi-input sources can have input. And it's possible that the vertex with no output edge does not contain any sink operator. Besides that, a new sink with topology can spread the sink logic into multiple job vertices connected with job edges.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 12:03:54 UTC 2022,,,,,,,,,,"0|z14fq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 12:03;zhuzh;Done via cef491eab34f904cf67f8a0392d48c03def249ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescaleBucketITCase is not stable,FLINK-28192,13456714,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,22/Jun/22 07:06,22/Jun/22 08:59,04/Jun/24 20:42,22/Jun/22 08:59,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"[https://github.com/apache/flink-table-store/runs/6996213019?check_suite_focus=true]

The job's status is not stable

!image-2022-06-22-15-06-14-271.png|width=760,height=88!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/22 07:06;qingyue;image-2022-06-22-15-06-14-271.png;https://issues.apache.org/jira/secure/attachment/13045431/image-2022-06-22-15-06-14-271.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 08:59:54 UTC 2022,,,,,,,,,,"0|z14cpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 07:19;qingyue;Maybe we don't need this assertion, let me remove it. cc [~lzljs3620320] ;;;","22/Jun/22 08:59;lzljs3620320;master: d8e415ae88677698903d0445cbca19dd12270a75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
【flink-runtime】Achieve metrics summary and unified active reporting,FLINK-28191,13455303,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,jkf6160@163.com,jkf6160@163.com,22/Jun/22 03:27,22/Jun/22 06:51,04/Jun/24 20:42,,,,,,,,,,Runtime / Metrics,,,,,,,0,,,,,"Currently we use flink-metrics-http plugins to report metrics , have the following problems：
 # Different components invoke the reporting interface independently and in parallel, frequently invoking the interface.
 # Data can only be aggregated downstream .
 # The number of downstream data rows is large, consuming storage resources

 

Plan：Metrics data for all components are summarized and sent downstream","Flink version : 1.11.1、1.14.3 

Java version : 8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-22 03:27:25.0,,,,,,,,,,"0|z143zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException is thrown if the intermediate result of nesting UDFs is used,FLINK-28190,13455293,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,TsReaper,TsReaper,22/Jun/22 03:26,22/Jun/22 07:48,04/Jun/24 20:42,22/Jun/22 07:48,1.14.5,1.15.0,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,"Add the following test case to {{TableEnvironmentITCase}} to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  tEnv.executeSql(""create temporary function myfun1 as 'MyFun1'"")
  tEnv.executeSql(""create temporary function myfun2 as 'MyFun2'"")

  val data: Seq[Row] = Seq(
    Row.of(""Hi"", ""Hello"")
  )
  tEnv.executeSql(
    s""""""
      |create table T (
      |  a string,
      |  b string
      |) with (
      |  'connector' = 'values',
      |  'data-id' = '${TestValuesTableFactory.registerData(data)}',
      |  'bounded' = 'true'
      |)
      |"""""".stripMargin)
  tEnv.executeSql(""create temporary view my_view as select myfun1(a, b) as mp from T"")
  tEnv.executeSql(""select myfun2(mp), mp['Hi'] from my_view"").print()
}
{code}

UDF classes are
{code:java}
import org.apache.flink.table.functions.ScalarFunction;

import java.util.HashMap;
import java.util.Map;

public class MyFun1 extends ScalarFunction {

    public Map<String, String> eval(String k, String v) {
        Map<String, String> returnMap = new HashMap<>();
        returnMap.put(k, v);
        return returnMap;
    }
}
{code}

{code:java}
import org.apache.flink.table.functions.ScalarFunction;

import java.util.Map;

public class MyFun2 extends ScalarFunction {

    public String eval(Map<String, String> input) {
        return String.valueOf(input);
    }
}
{code}

The exception stack is
{code}
Caused by: java.lang.NullPointerException
	at StreamExecCalc$25.processElement_split1(Unknown Source)
	at StreamExecCalc$25.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions$FromElementSourceFunction.run(TestValuesRuntimeFunctions.java:530)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
{code}

The generated code is
{code}
public class ToBinary$0 implements org.apache.flink.table.runtime.generated.Projection<org.apache.flink.table.data.RowData, org.apache.flink.table.data.binary.BinaryRowData> {

  org.apache.flink.table.data.binary.BinaryRowData out = new org.apache.flink.table.data.binary.BinaryRowData(2);
org.apache.flink.table.data.writer.BinaryRowWriter outWriter = new org.apache.flink.table.data.writer.BinaryRowWriter(out);

  public ToBinary$0(Object[] references) throws Exception {
    
  }

  @Override
  public org.apache.flink.table.data.binary.BinaryRowData apply(org.apache.flink.table.data.RowData in1) {
    
if (in1 instanceof org.apache.flink.table.data.binary.BinaryRowData) {
  return ((org.apache.flink.table.data.binary.BinaryRowData) in1);
}

    innerApply(in1);
    return out;
  }

  /* Fit into JavaCodeSplitter's void function limitation. */
  private void innerApply(org.apache.flink.table.data.RowData in1) {
    
    
outWriter.reset();


if (in1.isNullAt(0)) {
  outWriter.setNullAt(0);
} else {
  outWriter.writeString(0, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0)));
}
             


if (in1.isNullAt(1)) {
  outWriter.setNullAt(1);
} else {
  outWriter.writeString(1, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)));
}
             
outWriter.complete();
        out.setRowKind(in1.getRowKind());
  }
}
        

public class ToBinary$1 implements org.apache.flink.table.runtime.generated.Projection<org.apache.flink.table.data.RowData, org.apache.flink.table.data.binary.BinaryRowData> {

  org.apache.flink.table.data.binary.BinaryRowData out = new org.apache.flink.table.data.binary.BinaryRowData(2);
org.apache.flink.table.data.writer.BinaryRowWriter outWriter = new org.apache.flink.table.data.writer.BinaryRowWriter(out);

  public ToBinary$1(Object[] references) throws Exception {
    
  }

  @Override
  public org.apache.flink.table.data.binary.BinaryRowData apply(org.apache.flink.table.data.RowData in1) {
    
if (in1 instanceof org.apache.flink.table.data.binary.BinaryRowData) {
  return ((org.apache.flink.table.data.binary.BinaryRowData) in1);
}

    innerApply(in1);
    return out;
  }

  /* Fit into JavaCodeSplitter's void function limitation. */
  private void innerApply(org.apache.flink.table.data.RowData in1) {
    
    
outWriter.reset();


if (in1.isNullAt(0)) {
  outWriter.setNullAt(0);
} else {
  outWriter.writeString(0, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0)));
}
             


if (in1.isNullAt(1)) {
  outWriter.setNullAt(1);
} else {
  outWriter.writeString(1, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)));
}
             
outWriter.complete();
        out.setRowKind(in1.getRowKind());
  }
}
        

public class ToBinary$2 implements org.apache.flink.table.runtime.generated.Projection<org.apache.flink.table.data.RowData, org.apache.flink.table.data.binary.BinaryRowData> {

  org.apache.flink.table.data.binary.BinaryRowData out = new org.apache.flink.table.data.binary.BinaryRowData(2);
org.apache.flink.table.data.writer.BinaryRowWriter outWriter = new org.apache.flink.table.data.writer.BinaryRowWriter(out);

  public ToBinary$2(Object[] references) throws Exception {
    
  }

  @Override
  public org.apache.flink.table.data.binary.BinaryRowData apply(org.apache.flink.table.data.RowData in1) {
    
if (in1 instanceof org.apache.flink.table.data.binary.BinaryRowData) {
  return ((org.apache.flink.table.data.binary.BinaryRowData) in1);
}

    innerApply(in1);
    return out;
  }

  /* Fit into JavaCodeSplitter's void function limitation. */
  private void innerApply(org.apache.flink.table.data.RowData in1) {
    
    
outWriter.reset();


if (in1.isNullAt(0)) {
  outWriter.setNullAt(0);
} else {
  outWriter.writeString(0, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0)));
}
             


if (in1.isNullAt(1)) {
  outWriter.setNullAt(1);
} else {
  outWriter.writeString(1, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)));
}
             
outWriter.complete();
        out.setRowKind(in1.getRowKind());
  }
}
        

public class ToBinary$3 implements org.apache.flink.table.runtime.generated.Projection<org.apache.flink.table.data.RowData, org.apache.flink.table.data.binary.BinaryRowData> {

  org.apache.flink.table.data.binary.BinaryRowData out = new org.apache.flink.table.data.binary.BinaryRowData(2);
org.apache.flink.table.data.writer.BinaryRowWriter outWriter = new org.apache.flink.table.data.writer.BinaryRowWriter(out);

  public ToBinary$3(Object[] references) throws Exception {
    
  }

  @Override
  public org.apache.flink.table.data.binary.BinaryRowData apply(org.apache.flink.table.data.RowData in1) {
    
if (in1 instanceof org.apache.flink.table.data.binary.BinaryRowData) {
  return ((org.apache.flink.table.data.binary.BinaryRowData) in1);
}

    innerApply(in1);
    return out;
  }

  /* Fit into JavaCodeSplitter's void function limitation. */
  private void innerApply(org.apache.flink.table.data.RowData in1) {
    
    
outWriter.reset();


if (in1.isNullAt(0)) {
  outWriter.setNullAt(0);
} else {
  outWriter.writeString(0, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0)));
}
             


if (in1.isNullAt(1)) {
  outWriter.setNullAt(1);
} else {
  outWriter.writeString(1, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)));
}
             
outWriter.complete();
        out.setRowKind(in1.getRowKind());
  }
}
        

      public class StreamExecCalc$25 extends org.apache.flink.table.runtime.operators.TableStreamOperator
          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {

        private final Object[] references;
        private transient MyFun1 function_MyFun1;
        private transient org.apache.flink.table.data.conversion.StringStringConverter converter$6;
        private transient org.apache.flink.table.data.conversion.MapMapConverter converter$8;
        private transient MyFun2 function_MyFun2;
        
        private final org.apache.flink.table.data.binary.BinaryStringData str$12 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""Hi"");
                   
        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(2);
        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);

        public StreamExecCalc$25(
            Object[] references,
            org.apache.flink.streaming.runtime.tasks.StreamTask task,
            org.apache.flink.streaming.api.graph.StreamConfig config,
            org.apache.flink.streaming.api.operators.Output output,
            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
          this.references = references;
          function_MyFun1 = (((MyFun1) references[0]));
          converter$6 = (((org.apache.flink.table.data.conversion.StringStringConverter) references[1]));
          converter$8 = (((org.apache.flink.table.data.conversion.MapMapConverter) references[2]));
          function_MyFun2 = (((MyFun2) references[3]));
          this.setup(task, config, output);
          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
              .setProcessingTimeService(processingTimeService);
          }
        }

        @Override
        public void open() throws Exception {
          super.open();
          
          function_MyFun1.open(new org.apache.flink.table.functions.FunctionContext(getRuntimeContext()));
                 
          
          converter$6.open(getRuntimeContext().getUserCodeClassLoader());
                     
          
          converter$8.open(getRuntimeContext().getUserCodeClassLoader());
                     
          
          function_MyFun2.open(new org.apache.flink.table.functions.FunctionContext(getRuntimeContext()));
                 
        }

        @Override
        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
          
          org.apache.flink.table.data.binary.BinaryStringData field$4;
          boolean isNull$4;
          org.apache.flink.table.data.binary.BinaryStringData field$5;
          boolean isNull$5;
          java.util.Map externalResult$7;
          org.apache.flink.table.data.MapData result$9;
          boolean isNull$9;
          java.lang.String externalResult$10;
          org.apache.flink.table.data.binary.BinaryStringData result$11;
          boolean isNull$11;
          boolean isNull$23 = false;
          boolean result$24 = false;
          
          
          isNull$4 = in1.isNullAt(0);
          field$4 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$4) {
            field$4 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0));
          }
          isNull$5 = in1.isNullAt(1);
          field$5 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$5) {
            field$5 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1));
          }
          
          out.setRowKind(in1.getRowKind());
          
          
          
          
          
          
          
          
          externalResult$7 = (java.util.Map) function_MyFun1
            .eval(isNull$4 ? null : ((java.lang.String) converter$6.toExternal((org.apache.flink.table.data.binary.BinaryStringData) field$4)), isNull$5 ? null : ((java.lang.String) converter$6.toExternal((org.apache.flink.table.data.binary.BinaryStringData) field$5)));
          
          externalResult$10 = (java.lang.String) function_MyFun2
            .eval(externalResult$7);
          
          isNull$11 = externalResult$10 == null;
          result$11 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$11) {
            result$11 = (org.apache.flink.table.data.binary.BinaryStringData) converter$6.toInternalOrNull((java.lang.String) externalResult$10);
          }
          
          if (isNull$11) {
            out.setNullAt(0);
          } else {
            out.setNonPrimitiveValue(0, result$11);
          }
                    
          
          
          
          
          boolean isNull$13 = (isNull$9 || false);
          org.apache.flink.table.data.binary.BinaryStringData result$13 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$13) {
           
          if (result$9 instanceof org.apache.flink.table.data.binary.BinaryMapData) {
            org.apache.flink.table.data.binary.BinaryMapData binaryMap$21 = (org.apache.flink.table.data.binary.BinaryMapData) result$9;
            final int length$15 = binaryMap$21.size();
            final org.apache.flink.table.data.binary.BinaryArrayData keys$16 = binaryMap$21.keyArray();
            final org.apache.flink.table.data.binary.BinaryArrayData values$17 = binaryMap$21.valueArray();
          
            int index$18 = 0;
            boolean found$19 = false;
            if (false) {
              while (index$18 < length$15 && !found$19) {
                if (keys$16.isNullAt(index$18)) {
                  found$19 = true;
                } else {
                  index$18++;
                }
              }
            } else {
              while (index$18 < length$15 && !found$19) {
                final org.apache.flink.table.data.binary.BinaryStringData key$14 = ((org.apache.flink.table.data.binary.BinaryStringData) keys$16.getString(index$18));
                
          
          
          isNull$23 = false || false;
          result$24 = false;
          if (!isNull$23) {
            
          
          result$24 = ((org.apache.flink.table.data.binary.BinaryStringData) str$12).equals(key$14);
          
            
          }
          
                if (result$24) {
                  found$19 = true;
                } else {
                  index$18++;
                }
              }
            }
          
            if (!found$19 || values$17.isNullAt(index$18)) {
              isNull$13 = true;
            } else {
              result$13 = ((org.apache.flink.table.data.binary.BinaryStringData) values$17.getString(index$18));
            }
          } else {
            org.apache.flink.table.data.GenericMapData genericMap$22 = (org.apache.flink.table.data.GenericMapData) result$9;
            org.apache.flink.table.data.binary.BinaryStringData value$20 =
              (org.apache.flink.table.data.binary.BinaryStringData) genericMap$22.get((org.apache.flink.table.data.binary.BinaryStringData) ((org.apache.flink.table.data.binary.BinaryStringData) str$12));
            if (value$20 == null) {
              isNull$13 = true;
            } else {
              result$13 = value$20;
            }
          }
                  
          }
                  
          if (isNull$13) {
            out.setNullAt(1);
          } else {
            out.setNonPrimitiveValue(1, result$13);
          }
                    
                  
          output.collect(outElement.replace(out));
          
          
        }

        

        @Override
        public void close() throws Exception {
           super.close();
          
          function_MyFun1.close();
                 
          
          function_MyFun2.close();
                 
        }

        
      }
    

public class ToBinary$26 implements org.apache.flink.table.runtime.generated.Projection<org.apache.flink.table.data.RowData, org.apache.flink.table.data.binary.BinaryRowData> {

  org.apache.flink.table.data.binary.BinaryRowData out = new org.apache.flink.table.data.binary.BinaryRowData(2);
org.apache.flink.table.data.writer.BinaryRowWriter outWriter = new org.apache.flink.table.data.writer.BinaryRowWriter(out);

  public ToBinary$26(Object[] references) throws Exception {
    
  }

  @Override
  public org.apache.flink.table.data.binary.BinaryRowData apply(org.apache.flink.table.data.RowData in1) {
    
if (in1 instanceof org.apache.flink.table.data.binary.BinaryRowData) {
  return ((org.apache.flink.table.data.binary.BinaryRowData) in1);
}

    innerApply(in1);
    return out;
  }

  /* Fit into JavaCodeSplitter's void function limitation. */
  private void innerApply(org.apache.flink.table.data.RowData in1) {
    
    
outWriter.reset();


if (in1.isNullAt(0)) {
  outWriter.setNullAt(0);
} else {
  outWriter.writeString(0, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0)));
}
             


if (in1.isNullAt(1)) {
  outWriter.setNullAt(1);
} else {
  outWriter.writeString(1, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)));
}
             
outWriter.complete();
        out.setRowKind(in1.getRowKind());
  }
}
        

public class ToBinary$27 implements org.apache.flink.table.runtime.generated.Projection<org.apache.flink.table.data.RowData, org.apache.flink.table.data.binary.BinaryRowData> {

  org.apache.flink.table.data.binary.BinaryRowData out = new org.apache.flink.table.data.binary.BinaryRowData(2);
org.apache.flink.table.data.writer.BinaryRowWriter outWriter = new org.apache.flink.table.data.writer.BinaryRowWriter(out);

  public ToBinary$27(Object[] references) throws Exception {
    
  }

  @Override
  public org.apache.flink.table.data.binary.BinaryRowData apply(org.apache.flink.table.data.RowData in1) {
    
if (in1 instanceof org.apache.flink.table.data.binary.BinaryRowData) {
  return ((org.apache.flink.table.data.binary.BinaryRowData) in1);
}

    innerApply(in1);
    return out;
  }

  /* Fit into JavaCodeSplitter's void function limitation. */
  private void innerApply(org.apache.flink.table.data.RowData in1) {
    
    
outWriter.reset();


if (in1.isNullAt(0)) {
  outWriter.setNullAt(0);
} else {
  outWriter.writeString(0, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0)));
}
             


if (in1.isNullAt(1)) {
  outWriter.setNullAt(1);
} else {
  outWriter.writeString(1, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)));
}
             
outWriter.complete();
        out.setRowKind(in1.getRowKind());
  }
}
        

public class ToBinary$28 implements org.apache.flink.table.runtime.generated.Projection<org.apache.flink.table.data.RowData, org.apache.flink.table.data.binary.BinaryRowData> {

  org.apache.flink.table.data.binary.BinaryRowData out = new org.apache.flink.table.data.binary.BinaryRowData(2);
org.apache.flink.table.data.writer.BinaryRowWriter outWriter = new org.apache.flink.table.data.writer.BinaryRowWriter(out);

  public ToBinary$28(Object[] references) throws Exception {
    
  }

  @Override
  public org.apache.flink.table.data.binary.BinaryRowData apply(org.apache.flink.table.data.RowData in1) {
    
if (in1 instanceof org.apache.flink.table.data.binary.BinaryRowData) {
  return ((org.apache.flink.table.data.binary.BinaryRowData) in1);
}

    innerApply(in1);
    return out;
  }

  /* Fit into JavaCodeSplitter's void function limitation. */
  private void innerApply(org.apache.flink.table.data.RowData in1) {
    
    
outWriter.reset();


if (in1.isNullAt(0)) {
  outWriter.setNullAt(0);
} else {
  outWriter.writeString(0, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(0)));
}
             


if (in1.isNullAt(1)) {
  outWriter.setNullAt(1);
} else {
  outWriter.writeString(1, ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1)));
}
             
outWriter.complete();
        out.setRowKind(in1.getRowKind());
  }
}
{code}

You can see that {{result$9}} is never assigned a value, causing this bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-22 03:26:13.0,,,,,,,,,,"0|z143xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The taskmanager dynamically created by flink native k8s creates pods directly, why not create a deployment?",FLINK-28189,13454673,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,hob007,hob007,22/Jun/22 01:58,23/Feb/23 08:51,04/Jun/24 20:42,23/Feb/23 08:51,1.14.4,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,"I am using Flink native k8s deployment. The pods is dynamically created. But there's some problem with K8S cluster monitor. I can't monitor taskmanager pods status, because these pods without deployment.

 

So my question is:

The taskmanager dynamically created by flink native k8s creates pods directly, why not create a deployment?

 

For example:

Define a deployment and using replicates to control taskmanager's pods.

 

Chinese description reference:

https://issues.apache.org/jira/browse/FLINK-28167",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 08:05:32 UTC 2022,,,,,,,,,,"0|z1403s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 03:33;wangyang0918;First, I believe you could also monitor a pod status while it is created by Flink ResourceManager, not a deployment. The monitoring should be irrelevant with how the pods are created.

 

Using naked pod has more advantages than k8s deployment for TaskManager.
 * Flink could delete specific naked TaskManager pod easily. And it is more difficult to do this in k8s deployment. Imagine that some TaskManager pods in session cluster are idle and we want to release them.
 * Flink now could not work normally when we allow the TaskManager pod could be restarted. For example, TaskManager with same hostname:port will fail to register to the ResourceManager with more than once.
 * For fine-grained resource management, we might have various TaskManager pods with different spec(2G, 4G, etc.).;;;","22/Jun/22 03:35;wangyang0918;BTW, the JIRA ticket is not a good place to ask such questions. You could join the Flink slack workspace or post it to the user mail list.;;;","22/Jun/22 08:05;hob007;Got it, thanks very much!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation: Wrong default value for classloader.parent-first-patterns.default,FLINK-28188,13454645,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,tashoyan,tashoyan,21/Jun/22 20:03,21/Jun/22 20:16,04/Jun/24 20:42,21/Jun/22 20:16,1.15.0,,,,,,,,Documentation,,,,,,,0,,,,,"The documentation provides a wrong value:
[https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/deployment/config/#classloader-parent-first-patterns-default]

""java.;scala.;org.apache.flink.;com.esotericsoftware.kryo;org.apache.hadoop.;javax.annotation.;org.slf4j;org.apache.log4j;org.apache.logging;org.apache.commons.logging;ch.qos.logback;org.xml;javax.xml;org.apache.xerces;org.w3c""

The actual value is:

""java."", ""org.apache.flink."", ""javax.annotation."",
""org.slf4j"",
""org.apache.log4j"",
 ""org.apache.logging"",
 ""org.apache.commons.logging"",
 ""ch.qos.logback""
(from CoreOptions.java:187)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23131,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-21 20:03:25.0,,,,,,,,,,"0|z13zxk:",9223372036854775807,"This is applicable for plugin.classloader.parent-first-patterns.default, not for classloader.parent-first-patterns.default.
But plugin.classloader.parent-first-patterns.default is not documented at all.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate job submission for FlinkSessionJob,FLINK-28187,13454597,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,aitozi,jeesmon,jeesmon,21/Jun/22 15:16,11/Jul/22 07:30,04/Jun/24 20:42,11/Jul/22 07:30,kubernetes-operator-1.0.0,kubernetes-operator-1.1.0,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"During a session job submission if a deployment error (ex: concurrent.TimeoutException) is hit, operator will submit the job again. But first submission could have succeeded in jobManager side and second submission could result in duplicate job. Operator log attached.

Per [~gyfora]:

The problem is that in case a deployment error was hit, the SessionJobObserver will not be able to tell whether it has submitted the job or not. So it will simply try to submit it again. We have to find a mechanism to correlate Jobs on the cluster with the SessionJob CR itself. Maybe we could override the job name itself for this purpose or something like that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/22 15:15;jeesmon;flink-operator-log.txt;https://issues.apache.org/jira/secure/attachment/13045379/flink-operator-log.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 07:30:16 UTC 2022,,,,,,,,,,"0|z13zn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 15:18;gyfora;cc [~aitozi] ;;;","21/Jun/22 15:30;gyfora;The proper mechanism for this seems to be implemented already in https://issues.apache.org/jira/browse/FLINK-11544

We can set a custom fixed jobId for the deployment itself. We should use a combination of resource name, and generation similar to [https://github.com/apache/flink-kubernetes-operator/commit/ab59d6eb980512775590d0d01e697fe0c28d1b3b]

This way the observer can robustly detect already submitted jobs.;;;","22/Jun/22 11:13;aitozi;Currently, it generates the JobId in advance to help duplicate the job submission [link|https://github.com/apache/flink-kubernetes-operator/blob/91753ec5cef1aef85ff3884197e75fa25f7f6625/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/FlinkService.java#L215].
If we run the same jobID job, it will throw DuplicateJobSubmissionException 
I think the problem is caused by that if the job submitted failed, it will not store the reconcile spec, so the jobId is not stored. And it will regenerate a new one to submit;;;","22/Jun/22 11:21;gyfora;This is not what I am suggesting please check my other PR and other ticket to illustrate the concept;;;","22/Jun/22 11:23;gyfora;We need to deterministically generate the jobid from resource name + meta/generation, and add a logic to the observer to detect already submitted jobs;;;","22/Jun/22 11:29;aitozi;thanks, I will check it now ;;;","22/Jun/22 12:07;aitozi;I get your meaning now, but I think the case is a bit different from the flink deployment. In the FlinkDeployment, we could get the deployment first then compare the generation. 
In the session job mode, the jobID is the unique key, if the spec changed will generate a different jobID and it will make the old job orphaned. The spec change can happen during the job submission failure and the job observed I think (although it is small probability). 

Can we generate the JobID by the uid of the resource. In this way, one CR will have the same JobID through its lifetime. By this, we can always get the job by the same JobID ;;;","22/Jun/22 13:02;gyfora;I think the problem you highlight here could be solved.

Let me give you a hypothetical solution:
Generate the job id with the following function:
""namespace/name@GenerationId""

This way we can easily identify old jobs deployed for the current resource even if the generation changed in the meantime.;;;","22/Jun/22 13:03;gyfora;I know we can't use arbitrary strings as jobid so it is a bit more complicated but as long as we can get the resource name + generation from the jobid itself we can solve this properly I believe;;;","22/Jun/22 15:15;aitozi;I'm afraid of not clearly expressing my meaning. I will try to give an example about what I think:

1. Submit the job with {{Generation1}} , and JobID is generated {{ns/name@Generation1}}
2. The submission timeout but actually succeed and the last reconcile spec not updated
3. User change the spec and the generation become {{Generation2}} (Before the observer have sync the job status and update the last reconcile spec)
4. The observer observe the job with JobID {{ns/name@Generation2}} not match the first job 
5. The reconciler reconcile to submit the job with {{Generation2}}. 

In this sequence, the job {{ns/name@Generation1}} will be orphaned.;;;","22/Jun/22 15:21;aitozi;IMO, there is one  and only one job for one FlinkSessionJob, so I think the JobID associated with the resource UID will be enough here;;;","22/Jun/22 15:26;gyfora;We do upgrades in 2 steps, in the UPGRADING state the expected upgrade target generation is already in the status. 
We have to put the generation in the jobid otherwise we don't know if a job in upgrading state was already upgraded or not.

Please look at this commit: [https://github.com/apache/flink-kubernetes-operator/commit/ab59d6eb980512775590d0d01e697fe0c28d1b3b]

This is not so different how applications work also. You always have a single application cluster but still you need to attach the generation info otherwise you cannot deal with errors happening during or directly after submission.;;;","23/Jun/22 02:54;aitozi;[~gyfora] I add one comment here: [https://github.com/apache/flink-kubernetes-operator/commit/ab59d6eb980512775590d0d01e697fe0c28d1b3b#r76767242];;;","23/Jun/22 05:23;gyfora;Thanks for the comment. Identifying failed first deployments is slightly tricky I agree but this doesn't really affect the general requirement:

 1.  Have a way to detect in the FlinkService if a job for this resource is already running (throw an error) -> never allow double submission
 2.  Have a way to detect in the Observer if an upgrade already happened and update the lastReconciledSpec accordingly

For Deployments 1) is provided by Flink itself, 2) is basically covered in the commit I sent. For sessionjobs we need to cover both using the jobid magic somehow :) ;;;","23/Jun/22 05:44;aitozi;> For sessionjobs we need to cover both using the jobid magic somehow 

Can this done by generating JobID with the resource UID ?

1. Dispatcher will throw DuplicateJobSubmissionException if the same JobID submitted twice.

2. Upgrade happens with the following steps:

 

1) suspend the old job, reconcile status to upgrading

2) submit the job with new spec, same jobId

3) If job submitted succeed, but somehow throws timeout, then observer can detect the JobID has running , then update the reconcile status to deployed and update the lastReconciledSpec

Do you think this is a valid solution? [~gyfora] 

 ;;;","23/Jun/22 05:59;aitozi;I think it over again the generation will make check whether the upgrade already happen much easy and accurate. It will be better flink can have some meta for a session job. By this we could use uniq JobID + generation meta to get this done much beautiful;;;","23/Jun/22 06:01;gyfora;I think the resource UID would work for session jobs. We cannot use that easily for FlinkDeployments as we have a Deployment object after cancel (keep the cluster running) so there the generation makes for a simpler, more accurate check.

I think we can start with your proposal to use the resourceUID as jobId :) ;;;","23/Jun/22 06:08;aitozi;OK, I will work on it in this way;;;","27/Jun/22 13:37;aitozi;FYI, I'm working on this now, please help assign the ticket [~gyfora];;;","08/Jul/22 12:16;gyfora;[~aitozi] we have similar logic now in Flink for Applications: [https://github.com/apache/flink/commit/e70fe68dea764606180ca3728184c00fc63ea0ff];;;","10/Jul/22 02:26;aitozi;[~gyfora] Thanks for your hint, I take a look on it;;;","11/Jul/22 07:30;gyfora;merged to main 16c9f45061d0b6c2ca31f4f0ed98378e70a9f33b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trigger Operator Events on Configuration Changes,FLINK-28186,13454582,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,morhidi,morhidi,,21/Jun/22 13:52,01/Feb/23 23:03,04/Jun/24 20:42,01/Feb/23 23:03,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,"The Operator can already emit K8s Events related to CRs it manages, but it needs to emit events on important Operator related changes too, e.g. config updates, dynamic namespace changes, etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 29 15:09:41 UTC 2022,,,,,,,,,,"0|z13zjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 14:51;mbalassi;[~morhidi] could you give an update on this please? Do you think it is relevant for 1.3?;;;","29/Nov/22 15:09;morhidi;It's not urgent, removed the tag.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Invalid negative offset"" when using OffsetsInitializer.timestamp(.)",FLINK-28185,13454580,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mason6345,peter.schrott,peter.schrott,21/Jun/22 13:46,11/Oct/23 18:50,04/Jun/24 20:42,05/Jun/23 14:56,1.15.0,,,,,1.16.3,1.17.0,,Connectors / Kafka,,,,,,,2,pull-request-available,,,,"When using the {{OffsetsInitializer.timestamp(.)}} on a topic with empty partitions – little traffice + low retention – an {{IllegalArgumentException: Invalid negative offset}} occures. See stracktrace below.

The problem here is, that the admin client returns -1 as timestamps and offset for empty partitions in {{{}KafkaAdminClient.listOffsets(.){}}}. [1] Please compare the attached screenshot. When creating {{OffsetAndTimestamp}} object from the admin client response the exception is thrown.
{code:java}
org.apache.flink.util.FlinkRuntimeException: Failed to initialize partition splits due to 
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.handlePartitionSplitChanges(KafkaSourceEnumerator.java:299)
    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83)
    at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
    at java.util.concurrent.FutureTask.run(FutureTask.java)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.IllegalArgumentException: Invalid negative offset
    at org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.OffsetAndTimestamp.<init>(OffsetAndTimestamp.java:36)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.lambda$offsetsForTimes$8(KafkaSourceEnumerator.java:622)
    at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
    at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
    at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1723)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.offsetsForTimes(KafkaSourceEnumerator.java:615)
    at org.apache.flink.connector.kafka.source.enumerator.initializer.TimestampOffsetsInitializer.getPartitionOffsets(TimestampOffsetsInitializer.java:57)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.initializePartitionSplits(KafkaSourceEnumerator.java:272)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.lambda$checkPartitionChanges$0(KafkaSourceEnumerator.java:242)
    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80)
    ... 8 common frames omitted
15:25:58.025 INFO  [flink-akka.actor.default-dispatcher-11] o.a.f.runtime.jobmaster.JobMaster - Trying to recover from a global failure.
org.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for 'Source: XXX -> YYY -> Sink: ZZZ' (operator 351e440289835f2ff3e6fee31bf6e13c).
    at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:556)
    at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$QuiesceableContext.failJob(RecreateOnResetOperatorCoordinator.java:231)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.failJob(SourceCoordinatorContext.java:316)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext.handleUncaughtExceptionFromAsyncCall(SourceCoordinatorContext.java:329)
    at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:42)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
    at java.util.concurrent.FutureTask.run(FutureTask.java)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.flink.util.FlinkRuntimeException: Failed to initialize partition splits due to 
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.handlePartitionSplitChanges(KafkaSourceEnumerator.java:299)
    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83)
    at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40)
    ... 8 common frames omitted
Caused by: java.lang.IllegalArgumentException: Invalid negative offset
    at org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.OffsetAndTimestamp.<init>(OffsetAndTimestamp.java:36)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.lambda$offsetsForTimes$8(KafkaSourceEnumerator.java:622)
    at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321)
    at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
    at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1723)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.offsetsForTimes(KafkaSourceEnumerator.java:615)
    at org.apache.flink.connector.kafka.source.enumerator.initializer.TimestampOffsetsInitializer.getPartitionOffsets(TimestampOffsetsInitializer.java:57)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.initializePartitionSplits(KafkaSourceEnumerator.java:272)
    at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.lambda$checkPartitionChanges$0(KafkaSourceEnumerator.java:242)
    at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80)
    ... 8 common frames omitted {code}
*Expected Result:*
Consumer is initialized and records of partitions that contain data (> given timestamp) are consumed. Newly incomming data on ""empty"" partitions are also consumed.

*Actual Result:*
Consumer is not initizalied. No data are consumed.

 

[1] [https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/enumerator/KafkaSourceEnumerator.java#L604]

 ","Flink 1.15.0
Kafka 2.8.1",,,,,,,,,,FLINK-28537,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28266,,,FLINK-29032,,,,,,,,,,,,,,,,,,,,"21/Jun/22 13:35;peter.schrott;Bildschirmfoto 2022-06-21 um 15.24.58-1.png;https://issues.apache.org/jira/secure/attachment/13045373/Bildschirmfoto+2022-06-21+um+15.24.58-1.png","28/Jun/22 08:23;tashoyan;NegativeOffsetSpec.scala;https://issues.apache.org/jira/secure/attachment/13045784/NegativeOffsetSpec.scala",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,"// Start from the first record whose timestamp is greater than or equals a timestamp
.setStartingOffsets(OffsetsInitializer.timestamp(1592323200L))
",false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 05 14:56:20 UTC 2023,,,,,,,,,,"0|z13zjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 13:50;martijnvisser;[~renqs] Any thoughts on this? ;;;","22/Jun/22 08:45;renqs;I prefer to throw an exception with clearer message under this case. If we silently set the starting offset of empty partitions to earliest, there could be situation that the timestamp of newly incomming data are lower the specified starting timestamp, which is against the semantic of OffsetsInitializer.timestamp(). I'd like to leave the decision to users because they can always implement a custom {{{}OffsetsInitializer{}}}. ;;;","22/Jun/22 09:38;martijnvisser;Thanks for the explanation, that makes sense to me too;;;","28/Jun/22 07:49;mason6345;However, the proposal is a minor breaking change in behavior. From the source code: ""the timestamp does not exist in the partition yet, we will just consume from the latest"". So, the implementation is intended to return the latest offsets, if the timestamp cannot be found.

+1 for the proposal, the semantics are clearer to me. Additionally, it is a bit confusing in this case, because the auto offset strategy doesn't really play a role here.;;;","28/Jun/22 08:23;tashoyan;[~renqs] are you proposing to let user select a custom OffsetResetStrategy?
For example:

{code:java}
OffsetsInitializer.timestamp(timestamp, OffsetResetStrategy.LATEST)
{code}

This would be convenient. The default OffsetResetStrategy should be NONE - throw an Exception ;;;","28/Jun/22 08:34;mason6345;Yeah, it seems the implementation needs to account for the configured `OffsetResetStrategy`. Like `SpecifiedOffsetsInitializer` does;;;","14/Jul/22 22:09;mason6345;[~renqs] Mind if I take this one on? 

 

I know the code comment says it will reset to latest timestamp, but wouldn't it be clearer if the logic applied the `OffsetResetStrategy`? The default strategy for this can be latest, for backward compatibility purposes.;;;","15/Jul/22 02:57;renqs;Thanks for the feedback [~tashoyan] [~mason6345] and sorry for my late response. Yeah I think it makes sense to let users to specify the fallback strategy if the given timestamp could not be found. 

[~mason6345] Sure I'lll assign the ticket to you and thanks for the contribution!;;;","16/Aug/22 16:26;mason6345;[~renqs] gentle ping, do you have bandwidth to take a look at the PR?;;;","28/Sep/22 16:04;mason6345;cc: [~renqs] [~martijnvisser] would anyone have bandwidth to look at the PR?;;;","25/Nov/22 07:48;martijnvisser;Fixed in master: e44ecf0536589bf25c6e787d3c6307b30d19de67;;;","14/Dec/22 13:22;Erbureth;Hi,

could you please backport the fix to 1.15 & 1.16 series? As the 1.16.0 was released recently, 1.17 is a long way ahead, and the bug is a blocker for us.;;;","14/Dec/22 13:30;martijnvisser;[~Erbureth] Let's see. Especially given that Flink 1.15 and 1.16 are on different versions of the Kafka Client/Admin so I could imagine that this could be tricky. 

[~mason6345] [~renqs] I believe in the end you kept the original {{OffsetsInitializer}} so it could be backported to 1.16 and 1.15, right? Can it easily be backported?;;;","14/Dec/22 19:17;Erbureth;[~martijnvisser] For what it's worth, I have checked the code, and the relevant portions are the same across 1.15 - master. There was to the different API version happened in 1.15, which is where the bug was introduced.;;;","15/Dec/22 05:47;mason6345;Yup this is easily backported since we decided to defer the public interface changes. Let me finish off 1.16 externalization of the Kafka connector and I'll open some backport PRs;;;","20/Dec/22 15:04;martijnvisser;Re-opening so a backport to 1.16 can be added;;;","01/May/23 22:25;mason6345;[~martijnvisser] I forgot about your comment here. Backport: https://github.com/apache/flink/pull/22505;;;","05/Jun/23 14:56;martijnvisser;Fixed in release-1.16: 219f22e53d848b8055e22da3a8243e21c1dd4e31;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogStoreE2eTest is not stable,FLINK-28184,13454574,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,qingyue,qingyue,21/Jun/22 13:22,20/Jul/22 03:13,04/Jun/24 20:42,20/Jul/22 03:13,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,,,,,"[https://github.com/apache/flink-table-store/runs/6984608843?check_suite_focus=true]

!image-2022-06-21-21-23-16-325.png|width=1021,height=124!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/22 13:23;qingyue;image-2022-06-21-21-23-16-325.png;https://issues.apache.org/jira/secure/attachment/13045367/image-2022-06-21-21-23-16-325.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-21 13:22:47.0,,,,,,,,,,"0|z13zi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-python is lacking several test dependencies,FLINK-28183,13454572,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,chesnay,chesnay,21/Jun/22 13:12,09/Aug/22 02:24,04/Jun/24 20:42,09/Aug/22 02:24,1.16.0,,,,,1.16.0,,,API / Python,Build System,,,,,,0,pull-request-available,,,,"The pyflink_gateway_server searches the output directories of various modules to construct a test classpath.
Half of these are not declared as actual test dependencies in maven. Because of that there are no guarantees that these modules are actually built before flink-python.

Additionally there seem to be no safeguards in place to verify that these jars actually exist.

Considering that this is only required for testing most of this logic should also be moved into maven, copying these dependencies to some directory under flink-python/target, to make this de-facto build logic more discoverable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28194,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 09 02:24:19 UTC 2022,,,,,,,,,,"0|z13zhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 08:07;martijnvisser;[~hxbks2ks] Is this something that you can pick up? ;;;","09/Aug/22 02:24;dianfu;Merged to master via 5506930cc79eb131c66c5df0320045ad53437dce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Avro generic record decoder in PyFlink,FLINK-28182,13454570,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,21/Jun/22 13:03,01/Jul/22 03:23,04/Jun/24 20:42,01/Jul/22 01:41,,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,"Avro generic record decoder is useful for format like parquet-avro, which enables PyFlink users read parquet files into python native objects within a given avro schema.",,,,,,,,,,,,,FLINK-28336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 01:41:41 UTC 2022,,,,,,,,,,"0|z13zh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 01:41;dianfu;Merged to master via ae409d04e3b4398925e9bd53ef559ca81b7486c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support in FlinkSessionJob to submit job using jar available in jobManager's classpath,FLINK-28181,13454568,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeesmon,jeesmon,jeesmon,21/Jun/22 12:55,23/Sep/22 06:33,04/Jun/24 20:42,23/Sep/22 06:33,,,,,,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,,,,,"Currently FlinkSessionJob needs to download job jar from remote endpoint (http/s3/etc.) and submit it to jobManager for starting the job. There is no built-in support for starting a job using a jar that is already available in jobManager's docker image. This ticket is created to support submitting a job using a jar that is available in jobManager's classpath.

We have a need for team specific session cluster where all jobs are bundled in a docker image with many different configurations. For these job jars to be copied to a remote endpoint create additional maintenance and release overhead.

Slack discussion thread: https://apache-flink.slack.com/archives/C03G7LJTS2G/p1655495665285339?thread_ts=1655495313.473359&cid=C03G7LJTS2G",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 23 06:33:34 UTC 2022,,,,,,,,,,"0|z13zgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 12:50;jeesmon;[~gyfora] Could you please help me to get started on this? As I understood changes are:

1. Make job.jobUri an optional field
2. Add code to create a noop or empty jar
3. When jobUri is not defined and entryClass is defined, use noop/empty jar to submit job
4. Add validation to make sure either jobUri or entryClass is defined
5. Update documentation

Thanks;;;","24/Jun/22 12:55;gyfora;yes , i think what you wrote makes perfect sense. We can probably have some simple maven trick to create the empty jar or even include it simply in the repo for starter ;;;","23/Sep/22 06:33;gyfora;Resolved by 8f53441a4978eeb38dc5ef229c179cc60598ce87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify Application and SessionJob reconcilers,FLINK-28180,13454566,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,21/Jun/22 12:43,27/Jun/22 10:34,04/Jun/24 20:42,27/Jun/22 10:34,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Currently there is a lot of duplicate core logic in the application and session job reconcilers.

Also some features are not implemented consistently in both.

We should refactor and unify these as much as possible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 27 10:34:28 UTC 2022,,,,,,,,,,"0|z13zg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 10:34;gyfora;merged to main 3a49555e49df780c4efa8ecd4b679c2895766e29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeafPredicate accepts multiple literals,FLINK-28179,13454560,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,21/Jun/22 12:25,22/Jun/22 04:29,04/Jun/24 20:42,22/Jun/22 04:29,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"LeafPredicate should accept multiple literals for `IN` and other functions.

So in this Jira, do some refactor:
 * Remove Literal class (value is not null). But the literal can be null in `IN`.
 * Introduce LeafFunction and LeafBinaryFunction and LeafUnaryFunction.
 * PredicateBuilder should not be a singleton class.

 ",,,,,,,,,,,,,FLINK-28063,FLINK-28064,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 04:29:20 UTC 2022,,,,,,,,,,"0|z13zew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 04:29;lzljs3620320;master: cceca50c778a0c450f959d279c69f98a5960ad3f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show the delegated StateBackend and whether changelog is enabled in the UI,FLINK-28178,13454555,13425108,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,21/Jun/22 11:47,09/Aug/22 11:59,04/Jun/24 20:42,09/Aug/22 11:52,,,,,,1.16.0,,,Runtime / Checkpointing,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,"If changelog is enabled, StateBackend shown in Web UI is always 'ChangelogStateBackend'. I think ChangelogStateBackend should not expose to user, we should show the delegated StateBackend in this place. And We should add add a row to indicate whether changelog is enabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/22 12:48;Feifan Wang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13045864/screenshot-1.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 09 11:52:12 UTC 2022,,,,,,,,,,"0|z13zds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 03:17;Feifan Wang;Hi [~yunta], how do you think about this ?;;;","28/Jun/22 03:28;yunta;[~Feifan Wang] I think this deserves to be done and would help improve user experience. 

BTW, please fill the field of `Component/s`.

;;;","28/Jun/22 03:37;Feifan Wang;Thanks [~yunta], I just 'Runtime / Web Frontend' to Components, I'm not sure if I need to add other components.;;;","28/Jun/22 03:42;Feifan Wang;[~yunta], I want to try this work, can you assign this ticket to me ?;;;","28/Jun/22 06:04;yunta;[~Feifan Wang] already assigned to you, please go ahead.;;;","29/Jun/22 12:48;Feifan Wang;Hi [~yunta], I have submit a pr, can you help review it ?


!screenshot-1.png|width=370,height=176!;;;","12/Jul/22 02:35;Feifan Wang;Hi [~yunta] , can you help me review the [pr|https://github.com/apache/flink/pull/20103] ?;;;","12/Jul/22 03:27;yunta;[~Feifan Wang] sure, you can ping me on the github directly.;;;","12/Jul/22 03:51;Feifan Wang;Thanks [~yunta] , I got it.;;;","09/Aug/22 11:52;roman;Merged as 9ed70a1e8b5d59abdf9d7673bc5b44d421140ef0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch6DynamicSinkITCase.testWritingDocumentsNoPrimaryKey failed with 503 Service Unavailable,FLINK-28177,13454554,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,kurt.ding,hxbks2ks,hxbks2ks,21/Jun/22 11:44,13/Apr/23 09:00,04/Jun/24 20:42,,1.16.0,,,,,,,,Connectors / ElasticSearch,,,,,,,0,pull-request-available,stale-assigned,test-stability,,"
{code:java}
2022-06-21T07:39:23.9065585Z Jun 21 07:39:23 [ERROR] Tests run: 4, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 43.125 s <<< FAILURE! - in org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase
2022-06-21T07:39:23.9068457Z Jun 21 07:39:23 [ERROR] org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.testWritingDocumentsNoPrimaryKey  Time elapsed: 8.697 s  <<< ERROR!
2022-06-21T07:39:23.9069955Z Jun 21 07:39:23 java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
2022-06-21T07:39:23.9071135Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-06-21T07:39:23.9072225Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-21T07:39:23.9073408Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
2022-06-21T07:39:23.9075081Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
2022-06-21T07:39:23.9076560Z Jun 21 07:39:23 	at org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.testWritingDocumentsNoPrimaryKey(Elasticsearch6DynamicSinkITCase.java:286)
2022-06-21T07:39:23.9078535Z Jun 21 07:39:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-21T07:39:23.9079534Z Jun 21 07:39:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-21T07:39:23.9080702Z Jun 21 07:39:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-21T07:39:23.9081838Z Jun 21 07:39:23 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-21T07:39:23.9082942Z Jun 21 07:39:23 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-21T07:39:23.9084127Z Jun 21 07:39:23 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-21T07:39:23.9085246Z Jun 21 07:39:23 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-21T07:39:23.9086380Z Jun 21 07:39:23 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-06-21T07:39:23.9087812Z Jun 21 07:39:23 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-06-21T07:39:23.9088843Z Jun 21 07:39:23 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-06-21T07:39:23.9089823Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-21T07:39:23.9103797Z Jun 21 07:39:23 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-06-21T07:39:23.9105022Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-06-21T07:39:23.9106065Z Jun 21 07:39:23 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-06-21T07:39:23.9107500Z Jun 21 07:39:23 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-06-21T07:39:23.9108591Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-21T07:39:23.9109575Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-21T07:39:23.9110606Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-21T07:39:23.9111634Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-21T07:39:23.9112653Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-21T07:39:23.9113922Z Jun 21 07:39:23 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2022-06-21T07:39:23.9115083Z Jun 21 07:39:23 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-06-21T07:39:23.9116049Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-21T07:39:23.9117274Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-21T07:39:23.9118325Z Jun 21 07:39:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-21T07:39:23.9119209Z Jun 21 07:39:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-06-21T07:39:23.9120252Z Jun 21 07:39:23 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-06-21T07:39:23.9121429Z Jun 21 07:39:23 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-06-21T07:39:23.9122551Z Jun 21 07:39:23 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-06-21T07:39:23.9123804Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-06-21T07:39:23.9125115Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-06-21T07:39:23.9126752Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-06-21T07:39:23.9128514Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-06-21T07:39:23.9130205Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-06-21T07:39:23.9131495Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-06-21T07:39:23.9132711Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-06-21T07:39:23.9133994Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-06-21T07:39:23.9135312Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-06-21T07:39:23.9136669Z Jun 21 07:39:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-06-21T07:39:23.9138441Z Jun 21 07:39:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-06-21T07:39:23.9139779Z Jun 21 07:39:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-06-21T07:39:23.9141025Z Jun 21 07:39:23 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-06-21T07:39:23.9142515Z Jun 21 07:39:23 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-06-21T07:39:23.9143627Z Jun 21 07:39:23 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-06-21T07:39:23.9144607Z Jun 21 07:39:23 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-06-21T07:39:23.9145546Z Jun 21 07:39:23 Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
2022-06-21T07:39:23.9146964Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
2022-06-21T07:39:23.9148605Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
2022-06-21T07:39:23.9149875Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
2022-06-21T07:39:23.9151096Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-06-21T07:39:23.9152258Z Jun 21 07:39:23 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-06-21T07:39:23.9153435Z Jun 21 07:39:23 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-06-21T07:39:23.9154452Z Jun 21 07:39:23 	at java.lang.Thread.run(Thread.java:748)
2022-06-21T07:39:23.9155568Z Jun 21 07:39:23 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-06-21T07:39:23.9156812Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-06-21T07:39:23.9158282Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-21T07:39:23.9159528Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
2022-06-21T07:39:23.9160439Z Jun 21 07:39:23 	... 6 more
2022-06-21T07:39:23.9161272Z Jun 21 07:39:23 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-06-21T07:39:23.9162423Z Jun 21 07:39:23 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-06-21T07:39:23.9163729Z Jun 21 07:39:23 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-06-21T07:39:23.9165328Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-06-21T07:39:23.9166552Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-06-21T07:39:23.9168323Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-21T07:39:23.9169486Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-21T07:39:23.9170692Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-06-21T07:39:23.9171919Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-21T07:39:23.9173149Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-21T07:39:23.9174422Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-21T07:39:23.9175566Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-21T07:39:23.9176814Z Jun 21 07:39:23 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-06-21T07:39:23.9178449Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-06-21T07:39:23.9179849Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-06-21T07:39:23.9181289Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-06-21T07:39:23.9182745Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-21T07:39:23.9184021Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-21T07:39:23.9185327Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-21T07:39:23.9186488Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-21T07:39:23.9188035Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-06-21T07:39:23.9189136Z Jun 21 07:39:23 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-06-21T07:39:23.9189967Z Jun 21 07:39:23 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-06-21T07:39:23.9190831Z Jun 21 07:39:23 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-06-21T07:39:23.9191735Z Jun 21 07:39:23 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-06-21T07:39:23.9192690Z Jun 21 07:39:23 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-21T07:39:23.9193789Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-06-21T07:39:23.9194964Z Jun 21 07:39:23 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-06-21T07:39:23.9196026Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-06-21T07:39:23.9197515Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-06-21T07:39:23.9198640Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-06-21T07:39:23.9199605Z Jun 21 07:39:23 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-06-21T07:39:23.9200641Z Jun 21 07:39:23 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-06-21T07:39:23.9201874Z Jun 21 07:39:23 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-06-21T07:39:23.9203264Z Jun 21 07:39:23 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-06-21T07:39:23.9204252Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-06-21T07:39:23.9205243Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-06-21T07:39:23.9206434Z Jun 21 07:39:23 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-21T07:39:23.9207709Z Jun 21 07:39:23 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-06-21T07:39:23.9208893Z Jun 21 07:39:23 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-06-21T07:39:23.9209990Z Jun 21 07:39:23 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-06-21T07:39:23.9210973Z Jun 21 07:39:23 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-06-21T07:39:23.9212039Z Jun 21 07:39:23 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-06-21T07:39:23.9213096Z Jun 21 07:39:23 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-06-21T07:39:23.9214204Z Jun 21 07:39:23 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-06-21T07:39:23.9215415Z Jun 21 07:39:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-21T07:39:23.9216443Z Jun 21 07:39:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-21T07:39:23.9217806Z Jun 21 07:39:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-21T07:39:23.9218846Z Jun 21 07:39:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-21T07:39:23.9219945Z Jun 21 07:39:23 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-06-21T07:39:23.9221177Z Jun 21 07:39:23 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-06-21T07:39:23.9222644Z Jun 21 07:39:23 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-06-21T07:39:23.9223953Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:306)
2022-06-21T07:39:23.9225211Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:294)
2022-06-21T07:39:23.9226513Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:285)
2022-06-21T07:39:23.9228053Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-06-21T07:39:23.9229267Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-06-21T07:39:23.9230414Z Jun 21 07:39:23 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-06-21T07:39:23.9231443Z Jun 21 07:39:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-21T07:39:23.9232441Z Jun 21 07:39:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-21T07:39:23.9233528Z Jun 21 07:39:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-21T07:39:23.9234547Z Jun 21 07:39:23 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-21T07:39:23.9235592Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-06-21T07:39:23.9236899Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-06-21T07:39:23.9238893Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-06-21T07:39:23.9240094Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-06-21T07:39:23.9241308Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2022-06-21T07:39:23.9242831Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-06-21T07:39:23.9243901Z Jun 21 07:39:23 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-06-21T07:39:23.9244861Z Jun 21 07:39:23 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-06-21T07:39:23.9245873Z Jun 21 07:39:23 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-06-21T07:39:23.9246814Z Jun 21 07:39:23 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-06-21T07:39:23.9248051Z Jun 21 07:39:23 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-06-21T07:39:23.9249032Z Jun 21 07:39:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-06-21T07:39:23.9250006Z Jun 21 07:39:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-21T07:39:23.9251054Z Jun 21 07:39:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-21T07:39:23.9251981Z Jun 21 07:39:23 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-06-21T07:39:23.9252842Z Jun 21 07:39:23 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-06-21T07:39:23.9253792Z Jun 21 07:39:23 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-06-21T07:39:23.9254767Z Jun 21 07:39:23 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-06-21T07:39:23.9255671Z Jun 21 07:39:23 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-06-21T07:39:23.9256575Z Jun 21 07:39:23 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-06-21T07:39:23.9257800Z Jun 21 07:39:23 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-06-21T07:39:23.9258656Z Jun 21 07:39:23 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-06-21T07:39:23.9259384Z Jun 21 07:39:23 	... 4 more
2022-06-21T07:39:23.9260945Z Jun 21 07:39:23 Caused by: org.elasticsearch.ElasticsearchStatusException: org.elasticsearch.ElasticsearchStatusException: method [HEAD], host [http://172.17.0.1:53246], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2022-06-21T07:39:23.9262618Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2028)
2022-06-21T07:39:23.9263824Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1778)
2022-06-21T07:39:23.9265071Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1735)
2022-06-21T07:39:23.9266281Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1718)
2022-06-21T07:39:23.9267715Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.ping(RestHighLevelClient.java:705)
2022-06-21T07:39:23.9269055Z Jun 21 07:39:23 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:140)
2022-06-21T07:39:23.9270666Z Jun 21 07:39:23 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:45)
2022-06-21T07:39:23.9272139Z Jun 21 07:39:23 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.open(ElasticsearchSinkBase.java:317)
2022-06-21T07:39:23.9273433Z Jun 21 07:39:23 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
2022-06-21T07:39:23.9274713Z Jun 21 07:39:23 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
2022-06-21T07:39:23.9276289Z Jun 21 07:39:23 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.open(SinkOperator.java:58)
2022-06-21T07:39:23.9277934Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
2022-06-21T07:39:23.9279487Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:722)
2022-06-21T07:39:23.9280891Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
2022-06-21T07:39:23.9282338Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:698)
2022-06-21T07:39:23.9283489Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:665)
2022-06-21T07:39:23.9284539Z Jun 21 07:39:23 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-06-21T07:39:23.9285637Z Jun 21 07:39:23 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
2022-06-21T07:39:23.9286658Z Jun 21 07:39:23 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-06-21T07:39:23.9287956Z Jun 21 07:39:23 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-06-21T07:39:23.9288874Z Jun 21 07:39:23 	at java.lang.Thread.run(Thread.java:748)
2022-06-21T07:39:23.9290431Z Jun 21 07:39:23 Caused by: org.elasticsearch.client.ResponseException: org.elasticsearch.client.ResponseException: method [HEAD], host [http://172.17.0.1:53246], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2022-06-21T07:39:23.9292068Z Jun 21 07:39:23 	at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:936)
2022-06-21T07:39:23.9293173Z Jun 21 07:39:23 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:233)
2022-06-21T07:39:23.9294395Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1765)
2022-06-21T07:39:23.9295316Z Jun 21 07:39:23 	... 19 more
2022-06-21T07:39:23.9297237Z Jun 21 07:39:23 Caused by: org.elasticsearch.client.ResponseException: org.elasticsearch.client.ResponseException: method [HEAD], host [http://172.17.0.1:53246], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2022-06-21T07:39:23.9298987Z Jun 21 07:39:23 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:552)
2022-06-21T07:39:23.9300097Z Jun 21 07:39:23 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:537)
2022-06-21T07:39:23.9301150Z Jun 21 07:39:23 	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122)
2022-06-21T07:39:23.9302385Z Jun 21 07:39:23 	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
2022-06-21T07:39:23.9303793Z Jun 21 07:39:23 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
2022-06-21T07:39:23.9305195Z Jun 21 07:39:23 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.responseReceived(HttpAsyncRequestExecutor.java:309)
2022-06-21T07:39:23.9306731Z Jun 21 07:39:23 	at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:255)
2022-06-21T07:39:23.9308370Z Jun 21 07:39:23 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
2022-06-21T07:39:23.9309546Z Jun 21 07:39:23 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
2022-06-21T07:39:23.9310791Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
2022-06-21T07:39:23.9311965Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
2022-06-21T07:39:23.9313214Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
2022-06-21T07:39:23.9314834Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
2022-06-21T07:39:23.9316027Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
2022-06-21T07:39:23.9317681Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
2022-06-21T07:39:23.9318936Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
2022-06-21T07:39:23.9319925Z Jun 21 07:39:23 	... 1 more
2022-06-21T07:39:23.9320441Z Jun 21 07:39:23 
2022-06-21T07:39:23.9321427Z Jun 21 07:39:23 [ERROR] org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.testWritingDocumentsFromTableApi  Time elapsed: 2.068 s  <<< ERROR!
2022-06-21T07:39:23.9322817Z Jun 21 07:39:23 java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
2022-06-21T07:39:23.9323996Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-06-21T07:39:23.9325100Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-21T07:39:23.9326175Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
2022-06-21T07:39:23.9327643Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
2022-06-21T07:39:23.9328788Z Jun 21 07:39:23 	at org.apache.flink.streaming.connectors.elasticsearch.table.Elasticsearch6DynamicSinkITCase.testWritingDocumentsFromTableApi(Elasticsearch6DynamicSinkITCase.java:213)
2022-06-21T07:39:23.9329633Z Jun 21 07:39:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-21T07:39:23.9330267Z Jun 21 07:39:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-21T07:39:23.9330982Z Jun 21 07:39:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-21T07:39:23.9331623Z Jun 21 07:39:23 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-21T07:39:23.9332258Z Jun 21 07:39:23 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-21T07:39:23.9332982Z Jun 21 07:39:23 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-21T07:39:23.9333686Z Jun 21 07:39:23 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-21T07:39:23.9334391Z Jun 21 07:39:23 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-06-21T07:39:23.9335084Z Jun 21 07:39:23 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-06-21T07:39:23.9335705Z Jun 21 07:39:23 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-06-21T07:39:23.9336325Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-21T07:39:23.9337002Z Jun 21 07:39:23 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-06-21T07:39:23.9337983Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-06-21T07:39:23.9338658Z Jun 21 07:39:23 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-06-21T07:39:23.9339367Z Jun 21 07:39:23 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-06-21T07:39:23.9339997Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-21T07:39:23.9340618Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-21T07:39:23.9341241Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-21T07:39:23.9341874Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-21T07:39:23.9342685Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-21T07:39:23.9343430Z Jun 21 07:39:23 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2022-06-21T07:39:23.9344131Z Jun 21 07:39:23 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-06-21T07:39:23.9344813Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-21T07:39:23.9345422Z Jun 21 07:39:23 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-21T07:39:23.9345994Z Jun 21 07:39:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-21T07:39:23.9346550Z Jun 21 07:39:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-06-21T07:39:23.9347265Z Jun 21 07:39:23 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-06-21T07:39:23.9348144Z Jun 21 07:39:23 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-06-21T07:39:23.9348866Z Jun 21 07:39:23 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-06-21T07:39:23.9349622Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-06-21T07:39:23.9350446Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-06-21T07:39:23.9351290Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-06-21T07:39:23.9352164Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-06-21T07:39:23.9353015Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-06-21T07:39:23.9353789Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-06-21T07:39:23.9354473Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-06-21T07:39:23.9355264Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-06-21T07:39:23.9356105Z Jun 21 07:39:23 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-06-21T07:39:23.9356904Z Jun 21 07:39:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-06-21T07:39:23.9357933Z Jun 21 07:39:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-06-21T07:39:23.9358752Z Jun 21 07:39:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-06-21T07:39:23.9359525Z Jun 21 07:39:23 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-06-21T07:39:23.9360205Z Jun 21 07:39:23 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-06-21T07:39:23.9360877Z Jun 21 07:39:23 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-06-21T07:39:23.9361550Z Jun 21 07:39:23 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-06-21T07:39:23.9362177Z Jun 21 07:39:23 Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
2022-06-21T07:39:23.9362855Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
2022-06-21T07:39:23.9363636Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
2022-06-21T07:39:23.9364426Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
2022-06-21T07:39:23.9365254Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-06-21T07:39:23.9365955Z Jun 21 07:39:23 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-06-21T07:39:23.9366651Z Jun 21 07:39:23 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-06-21T07:39:23.9367501Z Jun 21 07:39:23 	at java.lang.Thread.run(Thread.java:748)
2022-06-21T07:39:23.9368180Z Jun 21 07:39:23 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-06-21T07:39:23.9368920Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-06-21T07:39:23.9369566Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-21T07:39:23.9370273Z Jun 21 07:39:23 	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
2022-06-21T07:39:23.9370837Z Jun 21 07:39:23 	... 6 more
2022-06-21T07:39:23.9371324Z Jun 21 07:39:23 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-06-21T07:39:23.9372011Z Jun 21 07:39:23 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-06-21T07:39:23.9372819Z Jun 21 07:39:23 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-06-21T07:39:23.9373600Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-06-21T07:39:23.9374279Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-06-21T07:39:23.9374996Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-21T07:39:23.9375690Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-21T07:39:23.9376451Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-06-21T07:39:23.9377293Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-21T07:39:23.9378273Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-21T07:39:23.9378979Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-21T07:39:23.9379668Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-21T07:39:23.9380348Z Jun 21 07:39:23 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-06-21T07:39:23.9381089Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-06-21T07:39:23.9381916Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-06-21T07:39:23.9382808Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-06-21T07:39:23.9383636Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-21T07:39:23.9384363Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-21T07:39:23.9385063Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-21T07:39:23.9385754Z Jun 21 07:39:23 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-21T07:39:23.9386483Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-06-21T07:39:23.9387436Z Jun 21 07:39:23 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-06-21T07:39:23.9388129Z Jun 21 07:39:23 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-06-21T07:39:23.9388716Z Jun 21 07:39:23 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-06-21T07:39:23.9389296Z Jun 21 07:39:23 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-06-21T07:39:23.9389989Z Jun 21 07:39:23 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-21T07:39:23.9390723Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-06-21T07:39:23.9391482Z Jun 21 07:39:23 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-06-21T07:39:23.9392184Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-06-21T07:39:23.9392935Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-06-21T07:39:23.9393646Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-06-21T07:39:23.9394287Z Jun 21 07:39:23 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-06-21T07:39:23.9394967Z Jun 21 07:39:23 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-06-21T07:39:23.9395747Z Jun 21 07:39:23 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-06-21T07:39:23.9396439Z Jun 21 07:39:23 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-06-21T07:39:23.9397162Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-06-21T07:39:23.9398011Z Jun 21 07:39:23 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-06-21T07:39:23.9398628Z Jun 21 07:39:23 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-21T07:39:23.9399299Z Jun 21 07:39:23 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-06-21T07:39:23.9400043Z Jun 21 07:39:23 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-06-21T07:39:23.9400745Z Jun 21 07:39:23 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-06-21T07:39:23.9401407Z Jun 21 07:39:23 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-06-21T07:39:23.9402080Z Jun 21 07:39:23 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-06-21T07:39:23.9402715Z Jun 21 07:39:23 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-06-21T07:39:23.9403433Z Jun 21 07:39:23 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-06-21T07:39:23.9404146Z Jun 21 07:39:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-21T07:39:23.9404799Z Jun 21 07:39:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-21T07:39:23.9405447Z Jun 21 07:39:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-21T07:39:23.9406113Z Jun 21 07:39:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-21T07:39:23.9406808Z Jun 21 07:39:23 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-06-21T07:39:23.9407872Z Jun 21 07:39:23 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-06-21T07:39:23.9408835Z Jun 21 07:39:23 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-06-21T07:39:23.9409707Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:306)
2022-06-21T07:39:23.9410617Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:294)
2022-06-21T07:39:23.9411435Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:285)
2022-06-21T07:39:23.9412265Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-06-21T07:39:23.9413123Z Jun 21 07:39:23 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-06-21T07:39:23.9413868Z Jun 21 07:39:23 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-06-21T07:39:23.9414521Z Jun 21 07:39:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-21T07:39:23.9415144Z Jun 21 07:39:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-21T07:39:23.9415860Z Jun 21 07:39:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-21T07:39:23.9416508Z Jun 21 07:39:23 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-21T07:39:23.9417257Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-06-21T07:39:23.9418395Z Jun 21 07:39:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-06-21T07:39:23.9419186Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-06-21T07:39:23.9419935Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-06-21T07:39:23.9420695Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2022-06-21T07:39:23.9421445Z Jun 21 07:39:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-06-21T07:39:23.9422116Z Jun 21 07:39:23 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-06-21T07:39:23.9422738Z Jun 21 07:39:23 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-06-21T07:39:23.9423333Z Jun 21 07:39:23 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-06-21T07:39:23.9423947Z Jun 21 07:39:23 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-06-21T07:39:23.9424575Z Jun 21 07:39:23 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-06-21T07:39:23.9425216Z Jun 21 07:39:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-06-21T07:39:23.9425853Z Jun 21 07:39:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-21T07:39:23.9426495Z Jun 21 07:39:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-21T07:39:23.9427148Z Jun 21 07:39:23 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-06-21T07:39:23.9427891Z Jun 21 07:39:23 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-06-21T07:39:23.9428487Z Jun 21 07:39:23 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-06-21T07:39:23.9429094Z Jun 21 07:39:23 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-06-21T07:39:23.9429662Z Jun 21 07:39:23 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-06-21T07:39:23.9430242Z Jun 21 07:39:23 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-06-21T07:39:23.9430784Z Jun 21 07:39:23 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-06-21T07:39:23.9431319Z Jun 21 07:39:23 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-06-21T07:39:23.9431758Z Jun 21 07:39:23 	... 4 more
2022-06-21T07:39:23.9432524Z Jun 21 07:39:23 Caused by: org.elasticsearch.ElasticsearchStatusException: org.elasticsearch.ElasticsearchStatusException: method [HEAD], host [http://172.17.0.1:53246], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2022-06-21T07:39:23.9433670Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:2028)
2022-06-21T07:39:23.9434456Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1778)
2022-06-21T07:39:23.9435218Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1735)
2022-06-21T07:39:23.9436068Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1718)
2022-06-21T07:39:23.9436792Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.ping(RestHighLevelClient.java:705)
2022-06-21T07:39:23.9437916Z Jun 21 07:39:23 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:140)
2022-06-21T07:39:23.9438928Z Jun 21 07:39:23 	at org.apache.flink.streaming.connectors.elasticsearch6.Elasticsearch6ApiCallBridge.verifyClientConnection(Elasticsearch6ApiCallBridge.java:45)
2022-06-21T07:39:23.9439860Z Jun 21 07:39:23 	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.open(ElasticsearchSinkBase.java:317)
2022-06-21T07:39:23.9440663Z Jun 21 07:39:23 	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
2022-06-21T07:39:23.9441468Z Jun 21 07:39:23 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
2022-06-21T07:39:23.9442246Z Jun 21 07:39:23 	at org.apache.flink.table.runtime.operators.sink.SinkOperator.open(SinkOperator.java:58)
2022-06-21T07:39:23.9443048Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
2022-06-21T07:39:23.9443869Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:722)
2022-06-21T07:39:23.9444733Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
2022-06-21T07:39:23.9445604Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:698)
2022-06-21T07:39:23.9446320Z Jun 21 07:39:23 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:665)
2022-06-21T07:39:23.9447276Z Jun 21 07:39:23 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-06-21T07:39:23.9448183Z Jun 21 07:39:23 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
2022-06-21T07:39:23.9448814Z Jun 21 07:39:23 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-06-21T07:39:23.9449428Z Jun 21 07:39:23 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-06-21T07:39:23.9449984Z Jun 21 07:39:23 	at java.lang.Thread.run(Thread.java:748)
2022-06-21T07:39:23.9450789Z Jun 21 07:39:23 Caused by: org.elasticsearch.client.ResponseException: org.elasticsearch.client.ResponseException: method [HEAD], host [http://172.17.0.1:53246], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2022-06-21T07:39:23.9451698Z Jun 21 07:39:23 	at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:936)
2022-06-21T07:39:23.9452379Z Jun 21 07:39:23 	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:233)
2022-06-21T07:39:23.9453102Z Jun 21 07:39:23 	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1765)
2022-06-21T07:39:23.9453647Z Jun 21 07:39:23 	... 19 more
2022-06-21T07:39:23.9454367Z Jun 21 07:39:23 Caused by: org.elasticsearch.client.ResponseException: org.elasticsearch.client.ResponseException: method [HEAD], host [http://172.17.0.1:53246], URI [/], status line [HTTP/1.1 503 Service Unavailable]
2022-06-21T07:39:23.9455236Z Jun 21 07:39:23 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:552)
2022-06-21T07:39:23.9456024Z Jun 21 07:39:23 	at org.elasticsearch.client.RestClient$1.completed(RestClient.java:537)
2022-06-21T07:39:23.9456672Z Jun 21 07:39:23 	at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122)
2022-06-21T07:39:23.9457735Z Jun 21 07:39:23 	at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
2022-06-21T07:39:23.9458724Z Jun 21 07:39:23 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
2022-06-21T07:39:23.9459529Z Jun 21 07:39:23 	at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.responseReceived(HttpAsyncRequestExecutor.java:309)
2022-06-21T07:39:23.9460352Z Jun 21 07:39:23 	at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:255)
2022-06-21T07:39:23.9461146Z Jun 21 07:39:23 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
2022-06-21T07:39:23.9461905Z Jun 21 07:39:23 	at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
2022-06-21T07:39:23.9462666Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
2022-06-21T07:39:23.9463399Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
2022-06-21T07:39:23.9464132Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
2022-06-21T07:39:23.9464872Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
2022-06-21T07:39:23.9465617Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
2022-06-21T07:39:23.9466324Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
2022-06-21T07:39:23.9467164Z Jun 21 07:39:23 	at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
2022-06-21T07:39:23.9467977Z Jun 21 07:39:23 	... 1 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36994&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/22 02:48;kurt.ding;image-2022-07-21-10-48-33-213.png;https://issues.apache.org/jira/secure/attachment/13047049/image-2022-07-21-10-48-33-213.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 22 08:42:03 UTC 2022,,,,,,,,,,"0|z13zdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 03:23;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37817&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","11/Jul/22 02:26;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","19/Jul/22 02:51;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38339&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","21/Jul/22 02:48;kurt.ding;I think this is because of the connection loss of elasticsearch in job execution stage with  no job retry config .;;;","21/Jul/22 02:48;kurt.ding;!image-2022-07-21-10-48-33-213.png!;;;","21/Jul/22 11:31;hxbks2ks;[~kurt.ding] Thanks for your investigation. Would you like to create a PR to fix it?;;;","22/Jul/22 01:13;kurt.ding;Yeah , I am  going to fix it .;;;","22/Jul/22 01:51;hxbks2ks;Thanks [~kurt.ding]. I have assigned it to you.;;;","25/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","26/Aug/22 00:14;kurt.ding;Pr is avaliable ,need a reviewer;;;","26/Aug/22 02:33;hxb;Merged into master via cdc4f4b100d3b2e66536a23685d7505c1f6670af
elasticsearch-main: 13c029f82c96b6600d29596d67d6bbe7b2061d88;;;","09/Nov/22 08:20;mapohl;I'm reopening the issue. We experienced this error again in a more recent 1.16 build that included the fix: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42957&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=15609

[~kurt.ding] may you have time to have another look what went wrong there?;;;","30/Nov/22 07:05;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43606&view=logs&j=a1ac4ce4-9a4f-5fdb-3290-7e163fba19dc&t=3a8f44aa-4415-5b14-37d5-5fecc568b139&l=16081;;;","02/Dec/22 02:45;kurt.ding;Sorry for a late response. I will check this issue today ;;;","09/Dec/22 03:17;kurt.ding;[~mapohl]  Would you like to  look at this PR ? https://github.com/apache/flink-connector-elasticsearch/pull/48;;;","21/Dec/22 08:37;martijnvisser;Fixed in flink-connector-elasticsearch: 79e7c96666c3518777a3914a7902764d98b038e7
[~mapohl] Do you think it should also be backported to 1.16? Given that Elasticsearch isn't in 1.17 anymore and these issues aren't regularly occurring, I'm leaning towards closing the ticket and only for it in the externalized version. WDYT?;;;","21/Dec/22 13:58;mapohl;Is it complicated to create a 1.16 backport here? I would still prefer to have it in {{release-1.16}} as well to be consistent. ...if I don't miss something and creating the backport PR for 1.16 is overly complicated;;;","22/Dec/22 08:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44162&view=logs&j=a1ac4ce4-9a4f-5fdb-3290-7e163fba19dc&t=3a8f44aa-4415-5b14-37d5-5fecc568b139&l=16447;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_es_sink_dynamic failed in jdk11,FLINK-28176,13454551,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,21/Jun/22 11:27,13/Jul/22 13:05,04/Jun/24 20:42,13/Jul/22 11:02,1.16.0,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-06-21T03:01:35.4707985Z Jun 21 03:01:35 _________________ FlinkElasticsearch7Test.test_es_sink_dynamic _________________
2022-06-21T03:01:35.4709206Z Jun 21 03:01:35 
2022-06-21T03:01:35.4710708Z Jun 21 03:01:35 self = <pyflink.datastream.tests.test_connectors.FlinkElasticsearch7Test testMethod=test_es_sink_dynamic>
2022-06-21T03:01:35.4711754Z Jun 21 03:01:35 
2022-06-21T03:01:35.4712481Z Jun 21 03:01:35     def test_es_sink_dynamic(self):
2022-06-21T03:01:35.4715653Z Jun 21 03:01:35         ds = self.env.from_collection(
2022-06-21T03:01:35.4718082Z Jun 21 03:01:35             [{'name': 'ada', 'id': '1'}, {'name': 'luna', 'id': '2'}],
2022-06-21T03:01:35.4719972Z Jun 21 03:01:35             type_info=Types.MAP(Types.STRING(), Types.STRING()))
2022-06-21T03:01:35.4721209Z Jun 21 03:01:35     
2022-06-21T03:01:35.4722120Z Jun 21 03:01:35 >       es_dynamic_index_sink = Elasticsearch7SinkBuilder() \
2022-06-21T03:01:35.4723876Z Jun 21 03:01:35             .set_emitter(ElasticsearchEmitter.dynamic_index('name', 'id')) \
2022-06-21T03:01:35.4725448Z Jun 21 03:01:35             .set_hosts(['localhost:9200']) \
2022-06-21T03:01:35.4726419Z Jun 21 03:01:35             .build()
2022-06-21T03:01:35.4727430Z Jun 21 03:01:35 
2022-06-21T03:01:35.4877335Z Jun 21 03:01:35 pyflink/datastream/tests/test_connectors.py:132: 
2022-06-21T03:01:35.4882723Z Jun 21 03:01:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-06-21T03:01:35.4884972Z Jun 21 03:01:35 pyflink/datastream/connectors/elasticsearch.py:130: in set_hosts
2022-06-21T03:01:35.4886124Z Jun 21 03:01:35     j_http_hosts_array = to_jarray(JHttpHost, j_http_hosts_list)
2022-06-21T03:01:35.4887527Z Jun 21 03:01:35 pyflink/util/java_utils.py:37: in to_jarray
2022-06-21T03:01:35.4888600Z Jun 21 03:01:35     j_arr[i] = arr[i]
2022-06-21T03:01:35.4890812Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_collections.py:238: in __setitem__
2022-06-21T03:01:35.4892201Z Jun 21 03:01:35     return self.__set_item(key, value)
2022-06-21T03:01:35.4893842Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/java_collections.py:221: in __set_item
2022-06-21T03:01:35.4895153Z Jun 21 03:01:35     return get_return_value(answer, self._gateway_client)
2022-06-21T03:01:35.4896282Z Jun 21 03:01:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-06-21T03:01:35.4897191Z Jun 21 03:01:35 
2022-06-21T03:01:35.4900656Z Jun 21 03:01:35 answer = 'zsorg.apache.flink.api.python.shaded.py4j.Py4JException: Cannot convert org.apache.flink.elasticsearch7.shaded.org.ap...haded.py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\tat java.base/java.lang.Thread.run(Thread.java:829)\\n'
2022-06-21T03:01:35.4903369Z Jun 21 03:01:35 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f7dd5b8b580>
2022-06-21T03:01:35.4904543Z Jun 21 03:01:35 target_id = None, name = None
2022-06-21T03:01:35.4905404Z Jun 21 03:01:35 
2022-06-21T03:01:35.4906381Z Jun 21 03:01:35     def get_return_value(answer, gateway_client, target_id=None, name=None):
2022-06-21T03:01:35.4908583Z Jun 21 03:01:35         """"""Converts an answer received from the Java gateway into a Python object.
2022-06-21T03:01:35.4909687Z Jun 21 03:01:35     
2022-06-21T03:01:35.4910838Z Jun 21 03:01:35         For example, string representation of integers are converted to Python
2022-06-21T03:01:35.4912061Z Jun 21 03:01:35         integer, string representation of objects are converted to JavaObject
2022-06-21T03:01:35.4913137Z Jun 21 03:01:35         instances, etc.
2022-06-21T03:01:35.4913921Z Jun 21 03:01:35     
2022-06-21T03:01:35.4914859Z Jun 21 03:01:35         :param answer: the string returned by the Java gateway
2022-06-21T03:01:35.4916648Z Jun 21 03:01:35         :param gateway_client: the gateway client used to communicate with the Java
2022-06-21T03:01:35.4918294Z Jun 21 03:01:35             Gateway. Only necessary if the answer is a reference (e.g., object,
2022-06-21T03:01:35.4919591Z Jun 21 03:01:35             list, map)
2022-06-21T03:01:35.4920758Z Jun 21 03:01:35         :param target_id: the name of the object from which the answer comes from
2022-06-21T03:01:35.4921963Z Jun 21 03:01:35             (e.g., *object1* in `object1.hello()`). Optional.
2022-06-21T03:01:35.4923122Z Jun 21 03:01:35         :param name: the name of the member from which the answer comes from
2022-06-21T03:01:35.4924246Z Jun 21 03:01:35             (e.g., *hello* in `object1.hello()`). Optional.
2022-06-21T03:01:35.4925140Z Jun 21 03:01:35         """"""
2022-06-21T03:01:35.4925981Z Jun 21 03:01:35         if is_error(answer)[0]:
2022-06-21T03:01:35.4926846Z Jun 21 03:01:35             if len(answer) > 1:
2022-06-21T03:01:35.4927828Z Jun 21 03:01:35                 type = answer[1]
2022-06-21T03:01:35.4928784Z Jun 21 03:01:35                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2022-06-21T03:01:35.4929792Z Jun 21 03:01:35                 if answer[1] == REFERENCE_TYPE:
2022-06-21T03:01:35.4930858Z Jun 21 03:01:35                     raise Py4JJavaError(
2022-06-21T03:01:35.4931806Z Jun 21 03:01:35                         ""An error occurred while calling {0}{1}{2}.\n"".
2022-06-21T03:01:35.4932792Z Jun 21 03:01:35                         format(target_id, ""."", name), value)
2022-06-21T03:01:35.4933346Z Jun 21 03:01:35                 else:
2022-06-21T03:01:35.4933841Z Jun 21 03:01:35 >                   raise Py4JError(
2022-06-21T03:01:35.4934829Z Jun 21 03:01:35                         ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".
2022-06-21T03:01:35.4935466Z Jun 21 03:01:35                         format(target_id, ""."", name, value))
2022-06-21T03:01:35.4936110Z Jun 21 03:01:35 E                   py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:
2022-06-21T03:01:35.4937114Z Jun 21 03:01:35 E                   org.apache.flink.api.python.shaded.py4j.Py4JException: Cannot convert org.apache.flink.elasticsearch7.shaded.org.apache.http.HttpHost to org.apache.flink.elasticsearch7.shaded.org.apache.http.HttpHost
2022-06-21T03:01:35.4938983Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:166)
2022-06-21T03:01:35.4940139Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144)
2022-06-21T03:01:35.4941251Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.commands.ArrayCommand.execute(ArrayCommand.java:97)
2022-06-21T03:01:35.4942313Z Jun 21 03:01:35 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2022-06-21T03:01:35.4943334Z Jun 21 03:01:35 E                   	at java.base/java.lang.Thread.run(Thread.java:829)
2022-06-21T03:01:35.4943905Z Jun 21 03:01:35 
2022-06-21T03:01:35.4945225Z Jun 21 03:01:35 .tox/py39-cython/lib/python3.9/site-packages/py4j/protocol.py:330: Py4JError
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 13:05:25 UTC 2022,,,,,,,,,,"0|z13zcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 07:11;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37382&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27400;;;","01/Jul/22 09:29;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37433&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27386;;;","04/Jul/22 10:59;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37535&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27068;;;","05/Jul/22 08:19;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=26897;;;","06/Jul/22 07:10;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37685&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27609;;;","07/Jul/22 06:17;hxbks2ks;Merged into master via fa67c3b7072fb8d80d05e10b1703cff5700fcb39;;;","12/Jul/22 06:38;martijnvisser;Re-opened, a new failed run:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37772&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27201;;;","13/Jul/22 10:07;hxbks2ks;Hi，[~martijnvisser] the commit in the failed case is e5c4e3f519f364b5951e7cac331eb8af48f0ed84 which is before the fixed commit fa67c3b7072fb8d80d05e10b1703cff5700fcb39.;;;","13/Jul/22 13:05;martijnvisser;Thanks for clarifying [~hxbks2ks]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve LicenseChecker output,FLINK-28175,13454543,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Jun/22 10:45,23/Jun/22 09:40,04/Jun/24 20:42,23/Jun/22 09:40,,,,,,1.16.0,,,Build System / CI,,,,,,,0,pull-request-available,,,,"The license checker output is difficult to parse for people who aren't too familiar with it. They just get bombarded with 200 log lines, that are too long, with too much redundant information, with no quick way to identify whether they are relevant for a particular change (e.g., module) without any guidance on whether something is critical or not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 23 09:40:49 UTC 2022,,,,,,,,,,"0|z13zb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 09:40;chesnay;master: a18a5a4c115b2095eb32d27bd4a4c937cccd5720;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamically resolve StatsD Reporter Host,FLINK-28174,13454536,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,emile.alberts,emile.alberts,21/Jun/22 10:32,21/Jun/22 12:13,04/Jun/24 20:42,21/Jun/22 11:35,,,,,,,,,Kubernetes Operator,,,,,,,0,question,,,,"I'm trying to use the StatsD metrics reporter with the Flink Kubernetes Operator. These are the default configurations that I'm using:

{code:yaml}
defaultConfiguration:
  create: true
  append: true
  flink-conf.yaml: |+
    kubernetes.operator.metrics.reporter.stsd.factory.class: org.apache.flink.metrics.statsd.StatsDReporterFactory
    kubernetes.operator.metrics.reporter.stsd.host: localhost
    kubernetes.operator.metrics.reporter.stsd.port: 8125
{code}

However, instead of using localhost, I want to dynamically resolve the host. Instead of _localhost_ as the host, I want to use the value from the field path _status.hostIP_. 

Is this currently possible? If so can you provide assistance, please?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 12:13:46 UTC 2022,,,,,,,,,,"0|z13z9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 11:35;martijnvisser;[~emile.alberts] Thanks for opening a ticket, but please ask this question to the Flink community via either mailing lists, Slack or Stackoverflow. See https://flink.apache.org/community.html#how-do-i-get-help-from-apache-flink

(Note: the priority can also not be a blocker, we follow https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process for determining the status);;;","21/Jun/22 11:53;emile.alberts;Thank you [~martijnvisser]

I'll ask the community and provide feedback on the ticket.;;;","21/Jun/22 12:13;martijnvisser;[~emile.alberts] That's great, thank you for that!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Parquet format tests are failing with NoSuchMethodError,FLINK-28173,13454533,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,sonice_lj,martijnvisser,martijnvisser,21/Jun/22 10:17,28/Jun/22 12:03,04/Jun/24 20:42,28/Jun/22 12:03,1.16.0,,,,,1.16.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,test-stability,,,"{code:java}
Jun 21 02:44:38 java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
Jun 21 02:44:38 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
Jun 21 02:44:38 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
Jun 21 02:44:38 	at org.apache.hadoop.conf.Configuration.readFields(Configuration.java:3798)
Jun 21 02:44:38 	at org.apache.flink.formats.parquet.utils.SerializableConfiguration.readObject(SerializableConfiguration.java:50)
Jun 21 02:44:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jun 21 02:44:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
{code}

{code:java}
Jun 21 02:44:42 [ERROR]   Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [ERROR]   Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [INFO] 
Jun 21 02:44:42 [ERROR] ParquetColumnarRowSplitReaderTest.testProject
Jun 21 02:44:42 [ERROR]   Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [ERROR]   Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [INFO] 
Jun 21 02:44:42 [ERROR] ParquetColumnarRowSplitReaderTest.testReachEnd
Jun 21 02:44:42 [ERROR]   Run 1: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [ERROR]   Run 2: com.google.common.base.Preconditions.checkState(ZLjava/lang/String;I)V
Jun 21 02:44:42 [INFO] 
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testCreateGenericReader:161->createReader:269 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testCreateReflectReader:133->createReader:269 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testCreateSpecificReader:118->createReader:269 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testReadWithRestoreGenericReader:203->restoreReader:293 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   AvroParquetRecordFormatTest.testReflectReadFromGenericRecords:147->createReader:269 » NoSuchMethod
Jun 21 02:44:42 [ERROR]   ParquetRowDataWriterTest.testCompression:126 » NoSuchMethod com.google.common....
Jun 21 02:44:42 [ERROR]   ParquetRowDataWriterTest.testTypes:117->innerTest:168 » NoSuchMethod com.googl...
Jun 21 02:44:42 [ERROR]   SerializableConfigurationTest.testResource:45 » NoSuchMethod com.google.common...
Jun 21 02:44:42 [INFO] 
Jun 21 02:44:42 [ERROR] Tests run: 31, Failures: 0, Errors: 24, Skipped: 0
Jun 21 02:44:42 [INFO] 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=16375",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22920,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 12:03:01 UTC 2022,,,,,,,,,,"0|z13z8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 10:50;chesnay;Maybe related to https://issues.apache.org/jira/browse/FLINK-22920?;;;","21/Jun/22 11:30;martijnvisser;Indeed since it's started failing since then. Thnx. ;;;","21/Jun/22 14:29;sonice_lj;[~martijnvisser] I'm wondering why my pr can pass all CI testcases in azure and then cause error above.;;;","21/Jun/22 14:48;sonice_lj;[~martijnvisser] If this commit is blocking other PRs, please revert it. I'll setup a brand new local environment to test my commit at tomorrow.;;;","21/Jun/22 14:51;chesnay;[~sonice_lj] This only fails on the cron builds where we use Hadoop 3.;;;","21/Jun/22 15:05;sonice_lj;[~chesnay] Thanks for your information. ;;;","22/Jun/22 02:06;sonice_lj;[~martijnvisser] Please assign this ticket to me.
I will add `hadoop3-tests` profile to cover default dependencies with guava exclusion.;;;","22/Jun/22 10:02;martijnvisser;[~sonice_lj] I've assigned it to you. ;;;","23/Jun/22 02:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37079&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=16746;;;","28/Jun/22 03:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37264&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0;;;","28/Jun/22 12:03;martijnvisser;Fixed in master: ce66d1998d21c89077248f5b41550db316382249;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scatter dstl files into separate directories by job id,FLINK-28172,13454531,13425108,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,21/Jun/22 10:16,15/Aug/22 14:06,04/Jun/24 20:42,08/Jul/22 15:36,1.15.0,,,,,1.16.0,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,"In the current implementation of {_}FsStateChangelogStorage{_}, dstl files from all jobs are put into the same directory (configured via {_}dstl.dfs.base-path{_}). Everything is fine if it's a filesystem like S3.But if it is a file system like hadoop, there will be some problems.

First, there may be an upper limit to the number of files in a single directory. Increasing this threshold will greatly reduce the performance of the distributed file system.

Second, dstl file management becomes difficult because the user cannot tell which job the dstl file belongs to, especially when the retained checkpoint is turned on.
h3. Propose
 # create a subdirectory named with the job id under the _dstl.dfs.base-path_ directory when the job starts
 # all dstl files upload to the subdirectory

( Going a step further, we can even create two levels of subdirectories under the _dstl.dfs.base-path_ directory, like _base-path/\{jobId}/dstl ._ This way, if the user configures the same dstl.dfs.base-path as state.checkpoints.dir, all files needed for job recovery will be in the same directory and well organized. )",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 08 15:36:40 UTC 2022,,,,,,,,,,"0|z13z8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 03:16;Feifan Wang;Hi [~yuanmei], [~roman], how do you think about this propose ?;;;","28/Jun/22 04:02;ym;I think this is a reasonable proposal, the directory at least should be organized under job (I do not think we share between jobs).

 

Is there any reason why we do not do this? [~roman] ;;;","28/Jun/22 08:52;roman;Good idea, thanks for the proposal [~Feifan Wang].
I don't see any reason not to do this [~ym].;;;","28/Jun/22 12:52;Feifan Wang;Thanks [~ym] and [~roman] for your reply, I have submit a pr for this ticket, can your help me review it ?;;;","28/Jun/22 13:29;roman;Thanks for the PR, [~Feifan Wang]!
I'll take a look ;;;","29/Jun/22 01:51;Feifan Wang;Thanks very much [~roman].;;;","08/Jul/22 15:36;roman;Merged as 135130379e592d9ae752bc4eea20b6fb222f8c15 into master (1.16).

Thanks [~Feifan Wang] for the contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adjust Job and Task manager port definitions to work with Istio+mTLS,FLINK-28171,13454530,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,elishamoshe,elishamoshe,elishamoshe,21/Jun/22 10:14,17/Aug/23 12:37,04/Jun/24 20:42,,1.14.4,,,,,,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,stale-assigned,,,"Hello,

 

We are launching Flink deployments using the [Flink Kubernetes Operator|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-stable/] on a Kubernetes cluster with Istio and mTLS enabled.

 

We found that the TaskManager is unable to communicate with the JobManager on the jobmanager-rpc port:

 

{{2022-06-15 15:25:40,508 WARN  akka.remote.ReliableDeliverySupervisor                       [] - Association with remote system [akka.tcp://[flink@amf-events-to-inference-and-central.nwdaf-edge|mailto:flink@amf-events-to-inference-and-central.nwdaf-edge]:6123] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://[flink@amf-events-to-inference-and-central.nwdaf-edge|mailto:flink@amf-events-to-inference-and-central.nwdaf-edge]:6123]] Caused by: [The remote system explicitly disassociated (reason unknown).]}}

 

The reason for the issue is that the JobManager service port definitions are not following the Istio guidelines [https://istio.io/latest/docs/ops/configuration/traffic-management/protocol-selection/] (see example below).

 

There was also an email discussion around this topic in the users mailing group under the subject ""Flink Kubernetes Operator with K8S + Istio + mTLS - port definitions"".

With the help of the community, we were able to work around the issue but it was very hard and forced us to skip Istio proxy which is not ideal.

 

We would like you to consider changing the default port definitions, either
 # Rename the ports – I understand it is Istio specific guideline but maybe it is better to at least be aligned with one (popular) vendor guideline instead of none at all.
 # Add the “appProtocol” property[1] that is not specific to any vendor but requires Kubernetes >= 1.19 where it was introduced as beta and moved to stable in >= 1.20. The option to add appProtocol property was added only in [https://github.com/fabric8io/kubernetes-client/releases/tag/v5.10.0] with [#3570|https://github.com/fabric8io/kubernetes-client/issues/3570].
 # Or allow a way to override the defaults.

 

[https://kubernetes.io/docs/concepts/services-networking/_print/#application-protocol]

 

 

{{# k get service inference-results-to-analytics-engine -o yaml}}

{{apiVersion: v1}}

{{kind: Service}}

{{...}}

{{spec:}}

{{  clusterIP: None}}

{{  ports:}}

{{  - name: jobmanager-rpc *# should start with “tcp-“ or add ""appProtocol"" property*}}

{{    port: 6123}}

{{    protocol: TCP}}

{{    targetPort: 6123}}

{{  - name: blobserver *# should start with ""tcp-"" or add ""appProtocol"" property*}}

{{    port: 6124}}

{{    protocol: TCP}}

{{    targetPort: 6124}}

{{...}}","flink-kubernetes-operator 1.0.0

Flink 1.14-java11

Kubernetes v1.19.5

Istio 1.7.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 17 12:37:00 UTC 2023,,,,,,,,,,"0|z13z88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 11:46;martijnvisser;Discussion from the user mailing list can be found at https://lists.apache.org/thread/yl40s9069wksz66qlf9t6jhmwsn59zft

My preferred solution would be to use the {appProtocol} solution, but only if that wouldn't hurt the current user base. If it would hurt them (because they are running on older version of Kubernetes which causes errors when adding this option), I think this feature would need to wait until impact would be less. Especially since I don't think we currently can/want to support Istio. ;;;","22/Jun/22 01:51;wangyang0918;I second with [~martijnvisser]'s suggestion that we should consider {{appProtocal}} first and make sure it does not break for old K8s versions.;;;","24/Jun/22 05:35;elishamoshe;Thanks [~martijnvisser] and [~wangyang0918] for sharing your view.

 

I assume that if we simply add `appProtocol ` and Kubernetes is < 1.19 will result in ValidationError ""unknown field"".

That said, I believe we can use the Kubernetes client in flink-kubernetes to check the Kubernetes server version and add `appProtocol` only if >= 1.19.

 ;;;","24/Jun/22 06:44;martijnvisser;I'm not sure it's that easy. You have to take updates, upgrades and downgrades into account. Would be great to get [~wangyang0918] his opinion since he's definitely more an expert on this topic;;;","18/Jul/22 13:00;elishamoshe;Hi,

 

We will appreciate a reply. Istio mTLS is a hard requirement for us. We will be happy to propose a PullRequest but we would like to implement a solution you think is best.

 

From what I saw, the ports are generated in these code segments [InitJobManagerDecorator.java#L166|https://github.com/apache/flink/blob/master/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/decorators/InitJobManagerDecorator.java#L166] or [HeadlessClusterIPService.java#L43|https://github.com/apache/flink/blob/master/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/services/HeadlessClusterIPService.java#L43] or [KubernetesResourceManagerDriver.java#L251|https://github.com/apache/flink/blob/189f88485d75821fe285e61bbf6623e88aec24d3/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManagerDriver.java#L251] or all.

 

One possible solution that is {*}very easy{*}, *still backward compatible* and can be *configured even when we use the Flink operator* is to have an environment variable - FLINK_ADD_APP_PROTOCOL_TO_PORTS and if true, the code will add ""appProtocol"" to the ports definition.

(Env var name is open for discussion :))


What do you think?;;;","20/Jul/22 11:17;wangyang0918;I am really sorry for the late response since I am occupied by some internal things. I am just thinking whether we could get this done by reusing pod template. If the container port with same name already be defined in the pod template, we will not override it in the decorators.;;;","20/Jul/22 18:15;elishamoshe;Thanks, [~wangyang0918] . Are you referring to the env var approach? If so, at least it will give a much needed solution for new deployments.
Deploying Flink on Istio+mTLS enabled system does not work so any such deployment will be new anyway.

Perhaps, if someone already have Flink deployed and they try to enable Istio+mTLS, this env var won't help.

Do you think we can progress with a PullRequest? Do you have a different solution? We need a solution that will work when using Flink Kubernetes Operator.;;;","22/Jul/22 03:45;wangyang0918;I do not mean the env var. What I mean is we could define the complete container port in the pod template[1], and then the decorator[2] could simply skip the same port if they have already be configured.

 
{code:java}
spec:
  containers:
  - name: flink-main-container
    ports:
    - containerPort: 8081
      name: rest
      protocol: TCP
      appProtocol: TCP
    - containerPort: 6123
      name: jobmanager-rpc
      protocol: TCP
      appProtocol: TCP
    - containerPort: 6124
      name: blobserver
      protocol: TCP
      appProtocol: TCP{code}
 

[1]. [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/native_kubernetes/#pod-template]

[2]. [https://github.com/apache/flink/blob/master/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/decorators/InitJobManagerDecorator.java#L166]

 ;;;","24/Jul/22 08:10;elishamoshe;Thanks, [~wangyang0918] . Sounds good.

This is quite urgent for us. Do you want me to take a shot with a PR or can the Flink team implement it soon?

 

Thanks.;;;","24/Jul/22 18:31;martijnvisser;[~elishamoshe] Feel free to open a PR. I understand that this is important for you, but we do have a number of priorities for the 1.16 release that have been committed to. This functionality doesn't have such commitment. It could very well be that this won't be able to make the 1.16 release and might get postponed to 1.17. There's no such thing as ""the Flink team""; Flink is open source and is operated via the community. All resources are scarce and everyone is doing the best they can. ;;;","25/Jul/22 02:57;wangyang0918;[~elishamoshe] You have got assigned. I am glad to help with code review and merging.;;;","26/Jul/22 08:49;elishamoshe;Great! Thank you both.;;;","07/Sep/22 12:41;johnboy74;Is there tested workaround for this?;;;","07/Sep/22 13:16;elishamoshe;[~johnboy74] 

You can configure Istio to allow Akka cluster communication to bypass the Istio sidecar proxy:

https://doc.akka.io/docs/akka-management/current/bootstrap/istio.html;;;","09/Apr/23 13:35;aliazovs;hello, [~wangyang0918]  and [~martijnvisser] we created PR for this based on flink 1.14.6, [https://github.com/apache/flink/pull/22371] we will appreciate your assistance in code review, merging and verifying it also goes to newer versions.;;;","11/Apr/23 02:41;sergiosp;Hello [~wangyang0918] ! also experiencing this issue with native kubernetes deployment with HA enabled (not in flink k8s operator).

In [https://lists.apache.org/thread/yl40s9069wksz66qlf9t6jhmwsn59zft] you mentioned that ""If HA enabled, the internal jobmanager rpc service will not be created. Instead, the TaskManager retrieves the JobManager address via HA services and connects to it via pod ip.""

Do you know whether we can change the way TaskManager connects to HA services: do not use ip address and instead use pod name?

I think we cannot add the akka workaround of bypassing the istio sidecar. Alternatively, do you know how to add TLS encryption to this channel (TaskManager->HA service) manually? Thanks for the info ~

 ;;;","11/Apr/23 08:08;martijnvisser;[~aliazovs] New features aren't added to already released versions, so this can only be added towards {{master}} and made available (if merged in time) with Flink 1.18. Keep in mind that Flink 1.14 is not supported anymore by the community too. ;;;","17/Apr/23 08:36;elishamoshe;[~martijnvisser] / [~wangyang0918]  can you please review the latest PR that [~aliazovs] created? It is for master as requested.;;;","03/Jul/23 14:00;tamirsagi;Hey [~elishamoshe] 

We recently encountered the same issue, when we changed Istio to STRICT mode to enforce mTLS. 

The clusters are deployed on AWS EKS , running with Flink 1.17.1 and HA enabled.

 

When we enabled debug logs it showed:

{{[Debug] [{}] [o.a.f.r.t.TaskExecutor]: Could not resolve ResourceManager address akka.tcp://flink@10.227.67.80:6123/user/rpc/resourcemanager_0, retrying in 10000 ms. org.apache.flink.runtime.rpc.exceptions.RpcConnectionException: Could not connect to rpc endpoint under address akka.tcp://flink@10.227.67.80:6123/user/rpc/resourcemanager_0.}}

{{Caused by: akka.actor.ActorNotFound: Actor not found for: ActorSelection[Anchor(akka.tcp://flink@10.227.67.80:6123/), Path(/user/rpc/resourcemanager_0)]}}

(I think first thing to do is to print the error as ""error"" not ""debug"")

 

When we enabled envoy logs on TM pod it showed

{{NR filter_chain_not_found - ""{-}"" 0 0 0 - ""{-}"" ""{-}"" ""{-}"" ""{-}"" ""{-}"" - - 10.227.67.80:6123}}

AKKA annotations[1]  worked fine along with Istio but did not work when mTLS was enabled. 

We also tried to enable mTLS and exclude certain ports [2]. that did not help as well.

In general, protocol selection (appProtocol property) refers to a service not a pod[3]. I'm not sure that adding app protocol to pod template will solve that.

[1] [https://doc.akka.io/docs/akka-management/current/bootstrap/istio.html] 

[2] [https://istio.io/latest/docs/reference/config/security/peer_authentication/]

[3] [https://istio.io/latest/docs/ops/configuration/traffic-management/protocol-selection/] 

 

 ;;;","17/Jul/23 07:34;elishamoshe;Hi [~tamirsagi] ,

Unfortunately, the appProtocol solution does not apply if you are running in HA.

For that, the only workaround we found is excluding the AKKA ports using Istio annotations.;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","17/Aug/23 12:37;aliazov;pull-request-available;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Python tests are taking more than 240 minutes to complete, causing timeouts",FLINK-28170,13454529,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,martijnvisser,martijnvisser,21/Jun/22 10:12,28/Jun/22 06:08,04/Jun/24 20:42,28/Jun/22 06:08,1.16.0,,,,,,,,API / Python,,,,,,,0,test-stability,,,,"{code:java}
Jun 21 05:16:50 pyflink/table/tests/test_join.py ......                                  [ 38%]
Jun 21 05:17:56 pyflink/table/tests/test_pandas_conversion.py ............               [ 42%]
Jun 21 05:18:57 pyflink/table/tests/test_pandas_udaf.py .......s........                 [ 46%]
Jun 21 05:19:39 pyflink/table/tests/test_pandas_udf.py .........                         [ 48%]
==========================================================================================
=== WARNING: This task took already 95% of the available time budget of 237 minutes ===
==========================================================================================
==============================================================================
The following Java processes are running (JPS)
==============================================================================
24464 PythonGatewayServer
11860 Jps
==============================================================================
Printing stack trace of Java process 24464
==============================================================================
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 06:08:07 UTC 2022,,,,,,,,,,"0|z13z80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 10:12;martijnvisser;CC [~dianfu] [~hxbks2ks];;;","28/Jun/22 06:08;hxbks2ks;Currently running 4 Python versions of the tests at the same time, the total time is likely to exceed 240min. Since Python 3.6 was declared as deprecate in release 1.16 https://issues.apache.org/jira/browse/FLINK-28195, I stopped the test of python 3.6 first.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GlueSchemaRegistryJsonKinesisITCase fails on JDK11 due to NoSuchMethodError,FLINK-28169,13454525,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,CrynetLogistics,martijnvisser,martijnvisser,21/Jun/22 10:01,01/Jul/22 12:05,04/Jun/24 20:42,01/Jul/22 10:30,1.16.0,,,,,1.16.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,test-stability,,,"{code:java}
Jun 21 03:06:27 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
Jun 21 03:06:27 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:537)
Jun 21 03:06:27 	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:340)
Jun 21 03:06:27 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
Jun 21 03:06:27 	... 8 more
Jun 21 03:06:27 Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: 'org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.SdkHttpClient org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.createHttpClient()'
Jun 21 03:06:27 	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
Jun 21 03:06:27 	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
Jun 21 03:06:27 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:40)
Jun 21 03:06:27 	at org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer$ListStreamsWaitStrategy.retryUntilSuccessRunner(KinesaliteContainer.java:150)
Jun 21 03:06:27 	at org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer$ListStreamsWaitStrategy.waitUntilReady(KinesaliteContainer.java:146)
Jun 21 03:06:27 	at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)
Jun 21 03:06:27 	at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:926)
Jun 21 03:06:27 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:480)
Jun 21 03:06:27 	... 10 more
Jun 21 03:06:27 Caused by: java.lang.NoSuchMethodError: 'org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.SdkHttpClient org.apache.flink.connector.aws.testutils.AWSServicesTestUtils.createHttpClient()'
Jun 21 03:06:27 	at org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer$ListStreamsWaitStrategy.list(KinesaliteContainer.java:157)
Jun 21 03:06:27 	at org.rnorth.ducttape.ratelimits.RateLimiter.getWhenReady(RateLimiter.java:51)
Jun 21 03:06:27 	at org.apache.flink.connectors.kinesis.testutils.KinesaliteContainer$ListStreamsWaitStrategy.lambda$retryUntilSuccessRunner$0(KinesaliteContainer.java:153)
Jun 21 03:06:27 	at org.rnorth.ducttape.unreliables.Unreliables.lambda$retryUntilSuccess$0(Unreliables.java:43)
Jun 21 03:06:27 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
Jun 21 03:06:27 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
Jun 21 03:06:27 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
Jun 21 03:06:27 	... 1 more
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=16659",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 12:05:51 UTC 2022,,,,,,,,,,"0|z13z74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 10:01;martijnvisser;CC [~dannycranmer];;;","21/Jun/22 10:20;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36926&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=15615;;;","23/Jun/22 19:18;chalixar;Hello [~martijnvisser]
Can you assign the issue to me.
Additionally a FYI, I will raise a PR to disable the test to unblock the build pipeline and proceed with a fix.
Since this is a packaging/shading issue and non of the dependencies has an ongoing work we shouldn't worry about the ignored test while being fixed.;;;","23/Jun/22 20:18;martijnvisser;Thanks [~chalixar] I've assigned it to you;;;","24/Jun/22 08:44;chalixar;[~martijnvisser] 
Disabling PR available to unblock CI.
I will follow with a fix PR;;;","24/Jun/22 17:45;martijnvisser;Disabled test via 67ff04698e6ec2340a32f1bd5c86255abc428831
Keeping the ticket open for the permanent fix;;;","30/Jun/22 13:10;CrynetLogistics;I have been coordinating with [~chalixar]  in the background and I have found a fix and can push a PR, please reassign to me ([~chalixar]  please confirm you are happy with this);;;","30/Jun/22 13:19;martijnvisser;[~CrynetLogistics] Done :);;;","30/Jun/22 16:38;CrynetLogistics;Thanks for assigning it to me [~martijnvisser] , I have pushed the PR and passed CI. Thanks [~chalixar] for approving and creating the related follow ups  FLINK-28332 and FLINK-28333. ;;;","01/Jul/22 10:30;martijnvisser;Fixed in master: 335c4049ed6ab47877b3ec64db220e5d5adbae04;;;","01/Jul/22 12:05;CrynetLogistics;Thanks [~martijnvisser] much appreciated.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.table.api.ValidationException: Data type 'ARRAY<STRING> NOT NULL' does not support null values.,FLINK-28168,13454519,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ongbo,ongbo,21/Jun/22 09:39,21/Jun/22 09:47,04/Jun/24 20:42,,1.12.7,,,,,,,,Table SQL / Planner,,,,,,,0,,,,,"org.apache.flink.table.api.ValidationException: Data type 'ARRAY<STRING> NOT NULL' does not support null values.
If the filter condition is placed in where, there will be an error, that is, the first element of CTRP is accessed, but if the filter condition is removed, there will be no problem with the query of the first element of CTRP.
This UDF return String[]
 
 
 
 
{code:java}
//代码占位符
SELECT
   *,
   ctrp[1] AS xxxx,
FROM
(
   SELECT
      CASE
         WHEN ...=... THEN UDF(...)
         WHEN ...=... THEN UDF(...)
       END AS ctrp
      FROM  table
) t
WHERE ctrp[2] <> 'xx'; {code}
To solve the above problem, I changed my SQL to this:
 
{code:java}
//代码占位符
CREATE VIEW V1 AS
SELECT
    *,
    ctrp[1] AS ctrp_resource_type,
FROM
(
    SELECT
        CASE
            WHEN ...=... THEN UDF(...)
            WHEN ...=... THEN UDF(...)
    END AS ctrp
    FROM table
) t;



select
*
from V1
where ctrp[1] <> 'xx'; {code}
{panel}
This will not happen, except when CTRP [1]<>'xx'"" is placed in the filter condition.
{panel}
 
 
{panel}
 
{panel}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 09:46:53 UTC 2022,,,,,,,,,,"0|z13z5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 09:46;martijnvisser;[~ongbo] Please verify if this is still the case for the latest version of Flink, since the community doesn't support Flink 1.12 anymore. I've also lowered the priority given Flink's Jira process https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink native k8s动态创建的taskmanager 直接创建pod，为啥不创建deployment？,FLINK-28167,13454514,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,hob007,hob007,21/Jun/22 09:13,22/Jun/22 01:43,04/Jun/24 20:42,21/Jun/22 09:23,,,,,,,,,,,,,,,,0,,,,,"flink native k8s动态创建的taskmanager 直接创建pod，为啥不创建deployment？

 

--原因：没有deployment，导致K8s默认监控的deployment看不到，taskmanager的状态。

只能看到一个jobmanager的deployment。

 

 ",Flink 14版本,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 01:43:30 UTC 2022,,,,,,,,,,"0|z13z4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 09:23;martijnvisser;[~hob007] Please make sure that you're using English for your tickets;;;","22/Jun/22 01:43;hob007;OK, I will create a new tickets.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configurable Automatic Retries on Error,FLINK-28166,13454512,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,21/Jun/22 09:09,24/Nov/22 01:02,04/Jun/24 20:42,22/Jun/22 14:17,kubernetes-operator-1.1.0,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,Make automatic reconciliation retries configurable. The current behaviour is the default defined in JOSDK: https://javaoperatorsdk.io/docs/features#automatic-retries-on-error,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 14:17:06 UTC 2022,,,,,,,,,,"0|z13z48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 14:17;gyfora;merged to main 39d9d105b0f77298252e1ff917a2b5e30538ee9d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove include_hadoop_aws profile,FLINK-28165,13454500,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Jun/22 08:43,14/Nov/22 14:28,04/Jun/24 20:42,14/Nov/22 14:28,,,,,,1.17.0,,,Build System,Deployment / YARN,,,,,,0,pull-request-available,,,,"The profile should be merged into the default configurations, because it was specific for Hadoop 2.6+ but we upgrade Hadoop to 2.8.5.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 14 14:28:49 UTC 2022,,,,,,,,,,"0|z13z1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 14:28;chesnay;master: 79870d10794bd3be384debcdecd2bf080c274ee8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce utilities API for REST endpint,FLINK-28164,13453709,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,fsk119,fsk119,21/Jun/22 07:05,09/Aug/22 02:12,04/Jun/24 20:42,09/Aug/22 02:12,1.16.0,,,,,1.16.0,,,Table SQL / Gateway,,,,,,,0,,,,,"It includes heartbeat, get_info, api_versions API in the REST endpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 09 02:12:37 UTC 2022,,,,,,,,,,"0|z13u5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 09:08;Wencong Liu;According to the discussion in https://issues.apache.org/jira/browse/FLINK-15787, all request/response body fields will use camelCase in the utilities related API.
cc [~fsk119] [~xtsong]  [~chesnay] ;;;","09/Aug/22 02:12;xtsong;master (1.16): 41bde65874c24ab036a016d17eb5f1ec09d6f819;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the statement related API for REST endpoint,FLINK-28163,13453683,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,fsk119,fsk119,21/Jun/22 07:02,09/Aug/22 02:12,04/Jun/24 20:42,09/Aug/22 02:12,1.16.0,,,,,1.16.0,,,Table SQL / Gateway,,,,,,,0,,,,,"It includes executeStatement, fetchResults API in the FLIP-91.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 09 02:12:18 UTC 2022,,,,,,,,,,"0|z13u00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 09:01;Wencong Liu;In the development of version 1.16, we will temporarily skip the development of statement completement API in sql gateway rest endpoint. Considering the workload and sql gateway temporarily does not need to be compatible with sql client, so the relevant development work will be carried out in the development of version 1.17. cc [~fsk119] [~xtsong] 



[FLINK-28796] Add Statement Completement API for sql gateway rest endpoint - ASF JIRA (apache.org);;;","08/Aug/22 07:00;Wencong Liu;Compared with the description of the statement api in [FLIP-91: Support SQL Gateway - Apache Flink - Apache Software Foundation|https://cwiki.apache.org/confluence/display/FLINK/FLIP-91%3A+Support+SQL+Gateway]], I have made the following modifications in the specific implementation:

*1. Execute a statement*

*1.1* The request body of the API will contain an object of type Map<String, String>, which will be converted to a Configuration and passed to the corresponding Operation. The json string converted from the request body will be like following:
{code:java}
{
""statement"": """", 

""executionTimeout"": """"

""executionConfig"":{
""k1"" : ""v1"",
""k2"" : ""v2""
}
}
{code}
*1.2* Both the has_result and operation_type attribute in the response body will be removed. The json string converted from response body will be like following:
{code:java}
{
""operationHandle"": ""xxxxxxxxxx""
}
{code}
*2. Fetch results*
The field {{next_result_uri}} will be null if there is no more data. And there will be no field {{{}exception{}}}.;;;","08/Aug/22 07:21;fsk119;Thanks for Wencong Liu's summary. The proposal LGTM.

I add some background for changes 1.2 and 1.3:

{*}For change 1.2{*}: we remove the {{opeation_type}} and {{has_results}} from the original proposal.

The motivation to introduce the {{OperaitonType}} is that HiveServer2 needs this to build Hive {{{}OperationHandle{}}}. However, during the implementation, we find we only need to build the Hive {{OperationHandle}} when submitting the Operation. At this moment, the {{HiveServer2Endpoint}} knows the type of Operation. So I think we can make the {{OperationType}} only visible on the HiveServer2 Endpoint side and make the {{SqlGatewayService}} expose the basic information.

 
We remove `has_results` because all operations have their results. Here we follow the behavior with the `{{{}TableResult{}}}`. 

{*}For change 1.3{*}: I think is unified to let the REST deal with all exception details rather than using Gateway to wrap all information into the {{{}ResultSet{}}}.

 

BTW, I think it should be {{operationHandle}} rather than {{{}operation_handle{}}}.;;;","08/Aug/22 08:14;Wencong Liu;operation_handle has been modified to operationHandle.;;;","09/Aug/22 02:12;xtsong;master (1.16): 47970b0435a3ae968c90922721d44fff8edb01a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the operation related API for REST endpoint,FLINK-28162,13453664,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,fsk119,fsk119,21/Jun/22 07:00,09/Aug/22 02:11,04/Jun/24 20:42,09/Aug/22 02:11,,,,,,1.16.0,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,"It includes getOperationStatus, cancelOperation, closeOperation.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 09 02:11:45 UTC 2022,,,,,,,,,,"0|z13tvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 09:07;Wencong Liu;According to the discussion in https://issues.apache.org/jira/browse/FLINK-15787, all request/response body fields will use camelCase in the operation related API.
cc [~fsk119] [~xtsong]  [~chesnay] ;;;","09/Aug/22 02:11;xtsong;master (1.16): 91eccea38c88abf38504094046aa99dbb1f9c06e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the session related API for REST endpoint,FLINK-28161,13453646,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,fsk119,fsk119,21/Jun/22 06:58,06/Aug/22 09:06,04/Jun/24 20:42,04/Aug/22 02:05,,,,,,1.16.0,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,"It includes openSession, closeSession and configure session. 

Please refer to FLIP-91 for API details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Aug 06 09:06:41 UTC 2022,,,,,,,,,,"0|z13trs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 10:15;Wencong Liu;In the development of version 1.16, we will temporarily skip the development of configure session api in sql gateway rest endpoint. Considering the workload and sql gateway temporarily does not need to be compatible with sql client, so the relevant development work will be carried out in the development of version 1.17. cc [~fsk119] [~xtsong] 

[FLINK-28777] Add configure session API for sql gateway rest endpoint - ASF JIRA (apache.org);;;","02/Aug/22 11:52;fsk119;[~Wencong Liu] Considering the time limited, I agree to move some unimportant features to the next version. Please just go ahead.;;;","04/Aug/22 02:05;xtsong;master (1.16): 687e0622221fd5d8cce6bf7258c2d18421bb02ce;;;","06/Aug/22 09:06;Wencong Liu;According to the discussion in https://issues.apache.org/jira/browse/FLINK-15787, all request/response body fields will use camelCase in the session related API.
cc [~fsk119] [~xtsong]  [~chesnay] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. ,FLINK-28160,13453208,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,cjh,cjh,21/Jun/22 06:01,22/Jun/22 06:09,04/Jun/24 20:42,,1.15.0,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,"2022-06-21 11:29:34,914 WARN  org.apache.flink.table.runtime.generated.GeneratedClass      [] - Failed to compile split code, falling back to original code
org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: {color:#FF0000}Table program cannot be compiled{color}. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94) ~[flink-table-runtime-1.15.0.jar:1.15.0]
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:97) ~[flink-table-runtime-1.15.0.jar:1.15.0]
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68) ~[flink-table-runtime-1.15.0.jar:1.15.0]
    at org.apache.flink.table.runtime.operators.aggregate.MiniBatchLocalGroupAggFunction.open(MiniBatchLocalGroupAggFunction.java:59) ~[flink-table-runtime-1.15.0.jar:1.15.0]
    at org.apache.flink.table.runtime.operators.bundle.AbstractMapBundleOperator.open(AbstractMapBundleOperator.java:82) ~[flink-table-runtime-1.15.0.jar:1.15.0]
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15.0.jar:1.15.0]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.15.0.jar:1.15.0]
    ... 14 more",flink 1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,sql,,,Wed Jun 22 06:07:07 UTC 2022,,,,,,,,,,"0|z13r2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 05:49;twalthr;[~cjh] please add a reproducible example. Otherwise exceptions of these kind are hard to debug without original SQL query/expression?;;;","22/Jun/22 06:07;cjh;[~twalthr] , OK . The original SQL query is as follows：

select
    dt_str,
    spcadmin.tenant_id as tenant_id,
    spcadmin.org_id as org_id,
    StringSortAgg(LISTAGG(spcadmin.admin_name,',')) as admin_name,
    spcadmin.room_id as rect_id,
    spcadmin.space_name as rect_name
from (
    select
        tenant_id,org_id,account_id,admin_name,
        room_id,space_name,
        ParseDebeziumTimestamp(start_date,'yyyy-MM-dd') as start_date,
        max(case when end_date is null then null else TO_DATE(ParseDebeziumTimestamp(end_date,'yyyy-MM-dd')) end) as end_date
    from admin
    where admin_name is not null
    and start_date is not null
    group by
        tenant_id,org_id,account_id,admin_name,
        room_id,space_name,
        ParseDebeziumTimestamp(start_date,'yyyy-MM-dd') as start_date
) spcadmin
join comm on IntToDate(comm.whole_date) >= spcadmin.start_date 
    and IntToDate(comm.whole_date) < (case when spcadmin.end_date is null then TO_DATE('2023-01-01') else spcadmin.end_date end)
group by
    dt_str,
    spcadmin.tenant_id,
    spcadmin.org_id,
    spcadmin.room_id,
    spcadmin.space_name;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store: Bucket pruning based on primary key filter,FLINK-28159,13452172,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,21/Jun/22 03:49,06/Jul/22 09:55,04/Jun/24 20:42,06/Jul/22 09:55,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"TABLE(a, b, c), pk is a

Query: SELECT * FROM T WHERE a = '...';

If a is a specific value, we can know which bucket is needed. We don't need to read other buckets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 06 09:55:17 UTC 2022,,,,,,,,,,"0|z13ko8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 06:59;liliwei;Not sure I can handle it, but I'd like to try, may i?;;;","05/Jul/22 07:47;lzljs3620320;[~liliwei] Thanks for your attention. This jira is still a bit uncertain, I'll explore it.

Can you take a look to FLINK-27103 , that one related to DataFileWriter and DataFileReader.;;;","06/Jul/22 09:55;lzljs3620320;master: 19f2973d6bc59b0d0310c48eae8bb97741fb31d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink supports all modes of Hive UDAF (PARTIAL1, PARTIAL2, FINAL, COMPLETE)",FLINK-28158,13452139,13421719,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,luoyuxia,tartarus,tartarus,21/Jun/22 03:45,08/Dec/22 08:33,04/Jun/24 20:42,08/Dec/22 08:33,1.15.0,,,,,,,,Connectors / Hive,,,,,,,0,,,,,"Currently Flink UDAF only supports Hive UDAF's PARTIAL_1 and FINAL mode.

When Flink uses Hive's UDAF percent_rank, it fails with the following exception message
{code:java}
org.apache.flink.table.api.TableException: Unexpected error in type inference logic of function 'percent_rank'. This is a bug.    at org.apache.flink.table.types.inference.TypeInferenceUtil.createUnexpectedException(TypeInferenceUtil.java:206)
    at org.apache.flink.table.planner.functions.inference.TypeInferenceReturnInference.inferReturnType(TypeInferenceReturnInference.java:80)
    at org.apache.calcite.sql.SqlOperator.inferReturnType(SqlOperator.java:482)
    at org.apache.calcite.rex.RexBuilder.deriveReturnType(RexBuilder.java:283)
    at org.apache.calcite.rex.RexBuilder.makeCall(RexBuilder.java:257)
    at org.apache.flink.table.planner.delegation.hive.SqlFunctionConverter.visitOver(SqlFunctionConverter.java:121)
    at org.apache.flink.table.planner.delegation.hive.SqlFunctionConverter.visitOver(SqlFunctionConverter.java:56)
    at org.apache.calcite.rex.RexOver.accept(RexOver.java:121)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.getWindowRexAndType(HiveParserCalcitePlanner.java:1859)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.genSelectForWindowing(HiveParserCalcitePlanner.java:1913)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.genSelectLogicalPlan(HiveParserCalcitePlanner.java:2002)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.genLogicalPlan(HiveParserCalcitePlanner.java:2751)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.logicalPlan(HiveParserCalcitePlanner.java:284)
    at org.apache.flink.table.planner.delegation.hive.HiveParserCalcitePlanner.genLogicalPlan(HiveParserCalcitePlanner.java:272)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.analyzeSql(HiveParser.java:303)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.processCmd(HiveParser.java:251)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:211)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:695)
    at org.apache.flink.connectors.hive.HiveDialectITCase.testPercent_rank(HiveDialectITCase.java:800)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.flink.table.functions.hive.FlinkHiveUDFException: Failed to get Hive result type from org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank
    at org.apache.flink.table.functions.hive.HiveGenericUDAF.inferReturnType(HiveGenericUDAF.java:249)
    at org.apache.flink.table.functions.hive.HiveFunction$HiveFunctionOutputStrategy.inferType(HiveFunction.java:122)
    at org.apache.flink.table.types.inference.TypeInferenceUtil.inferOutputType(TypeInferenceUtil.java:151)
    at org.apache.flink.table.planner.functions.inference.TypeInferenceReturnInference.inferReturnTypeOrError(TypeInferenceReturnInference.java:99)
    at org.apache.flink.table.planner.functions.inference.TypeInferenceReturnInference.inferReturnType(TypeInferenceReturnInference.java:76)
    ... 44 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Only COMPLETE mode supported for Rank function
    at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank$GenericUDAFAbstractRankEvaluator.init(GenericUDAFRank.java:124)
    at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank$GenericUDAFPercentRankEvaluator.init(GenericUDAFPercentRank.java:59)
    at org.apache.flink.table.functions.hive.HiveGenericUDAF.init(HiveGenericUDAF.java:99)
    at org.apache.flink.table.functions.hive.HiveGenericUDAF.inferReturnType(HiveGenericUDAF.java:243)
    ... 48 more {code}
According to the exception message, we can see that it is because the percentage_rank function requires COMPLETE Mode.

We can reproduce it with ITCase:
{code:java}
@Test
public void testPercent_rank() throws Exception {
    // automatically load hive module in hive-compatible mode
    HiveModule hiveModule = new HiveModule(hiveCatalog.getHiveVersion());
    CoreModule coreModule = CoreModule.INSTANCE;
    for (String loaded : tableEnv.listModules()) {
        tableEnv.unloadModule(loaded);
    }
    tableEnv.loadModule(""hive"", hiveModule);
    tableEnv.loadModule(""core"", coreModule);
    // Flink UDAF only supports Hive UDAF's PARTIAL_1 and FINAL mode.
    tableEnv.executeSql(
            ""create temporary function percent_rank as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank'"");

    tableEnv.executeSql(
            ""create table cbo_t1(key string, value string, c_int int, c_float float, c_boolean boolean)"");
    List<Row> results =
            CollectionUtil.iteratorToList(
                    tableEnv.executeSql(
                                    ""select percent_rank() over(partition by c_float order by key) from cbo_t1"")
                            .collect());
} {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 08 08:33:03 UTC 2022,,,,,,,,,,"0|z13kgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 08:33;luoyuxia;percent_rank is supported in FLINK-27620. I think it maynot a valid issue, I'll close it. Feel free to open it if you have other thoughts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive Reader supports Hive3,FLINK-28157,13452131,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,21/Jun/22 03:45,10/Nov/22 16:05,04/Jun/24 20:42,26/Oct/22 12:14,,,,,,table-store-0.3.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29983,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Oct 26 12:14:21 UTC 2022,,,,,,,,,,"0|z13kf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 12:14;lzljs3620320;master: f80659ba2219a4b70b83007f83fef71c304affa0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink KafkaSource with Bounded Throw Exception,FLINK-28156,13451808,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiangfei Liu,Jiangfei Liu,21/Jun/22 03:08,21/Jun/22 08:05,04/Jun/24 20:42,,1.14.3,,,,,,,,API / DataStream,Connectors / Kafka,,,,,,0,,,,,"I want to use KafkaSource consume topic between commited offset and last-offset,

but throw a exception

 KafkaSource.<String>builder()
.setBootstrapServers(""10.18.34.43:9092,10.18.34.44:9092,10.18.34.45:9092"")
.setTopics(topic)
.setGroupId(groupId)
// .setStartingOffsets(OffsetsInitializer.timestamp(1655717760000L))
.setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.EARLIEST))
.setValueOnlyDeserializer(new SimpleStringSchema())
.setBounded(OffsetsInitializer.latest())
.build();

 

Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:146)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:101)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    ... 1 more
Caused by: java.lang.IllegalStateException: Consumer is not subscribed to any topics or assigned any partitions
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1223)
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
    at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:99)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:56)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:138)
    ... 6 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 08:05:42 UTC 2022,,,,,,,,,,"0|z13ifc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 08:05;martijnvisser;[~Jiangfei Liu] I think is a duplicate of FLINK-27041;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add documentation for ""ALTER TABLE ... COMPACT""",FLINK-28155,13451800,13443507,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,qingyue,qingyue,21/Jun/22 03:08,15/Jul/22 02:46,04/Jun/24 20:42,15/Jul/22 02:46,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,,,,,It is notable that this feature is only supported for Flink's ManagedTable since version 1.15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-21 03:08:04.0,,,,,,,,,,"0|z13idk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the E2E test for HiveServer2 Endpoint,FLINK-28154,13451580,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,,fsk119,fsk119,21/Jun/22 02:37,03/Aug/22 07:11,04/Jun/24 20:42,03/Aug/22 07:11,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27770,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-21 02:37:16.0,,,,,,,,,,"0|z13h0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the catalog-related API for HiveServer2 Endpoint,FLINK-28153,13451565,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,fsk119,fsk119,21/Jun/22 02:35,21/Jul/22 12:58,04/Jun/24 20:42,21/Jul/22 12:58,1.16.0,,,,,,,,Connectors / Hive,Table SQL / Client,,,,,,0,,,,,Allow to fetch catalog object in the HiveServer2 Endpoint. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-21 02:35:32.0,,,,,,,,,,"0|z13gxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the statement-related API for HiveServer2 Endpoint,FLINK-28152,13451532,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,21/Jun/22 02:31,29/Jul/22 02:34,04/Jun/24 20:42,29/Jul/22 02:06,,,,,,,,,,,,,,,,0,pull-request-available,,,,Allow HiveServer2 Endpoint to submit sql and fetch results. It's better we can test under the YARN envrionment.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 29 02:06:51 UTC 2022,,,,,,,,,,"0|z13gq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 02:06;fsk119;Merged into master:

 

6a95672db9cf23c9110ec9b0d4701e9c1e3acfa1

f477a43ff23576cd2e1f8c632f78458948245df4

1fb5875ae6e2536266275c28e6260019706c28f2

27cecdebccc95e9acf6ba38616b14e531b35d1be

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow to cancel the Operation for the HiveServer2 Endpoint,FLINK-28151,13451519,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,ZhaoWeiNan,fsk119,fsk119,21/Jun/22 02:29,03/Aug/22 03:23,04/Jun/24 20:42,03/Aug/22 03:23,1.16.0,,,,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 03 03:23:52 UTC 2022,,,,,,,,,,"0|z13gn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 05:15;ZhaoWeiNan;hi, [~fsk119] . I want to contribute this issue. Please assign to me.

Thanks.

BR;;;","03/Aug/22 03:23;fsk119;Implemented in the master: 69df7a46bf439fc94ad1074b0d9ea9b3503632b1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the hiveserver2 endpoint and factory,FLINK-28150,13451486,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,21/Jun/22 02:26,26/Jul/22 02:10,04/Jun/24 20:42,24/Jul/22 04:31,,,,,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,Introduce the HiveServer2Endpoint and Factory and allow the gateway to load them using SPI mechanism.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28679,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-21 02:26:27.0,,,,,,,,,,"0|z13gfs:",9223372036854775807,Merged in the master: d067629d4d200f940d0b58759459d7ff5832b292,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-223] Support HiveServer2 Endpoint,FLINK-28149,13451461,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,21/Jun/22 02:23,30/Sep/22 08:55,04/Jun/24 20:42,30/Sep/22 08:55,1.16.0,,,,,1.16.0,,,Connectors / Hive,Table SQL / Client,,,,,,0,,,,,This is an umbrella issue for FLIP-223: https://cwiki.apache.org/confluence/display/FLINK/FLIP-223%3A+Support+HiveServer2+Endpoint,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-21 02:23:25.0,,,,,,,,,,"0|z13ga8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to load jar connector to a Python Table API app,FLINK-28148,13451374,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,CrynetLogistics,CrynetLogistics,20/Jun/22 16:25,27/Jun/22 09:12,04/Jun/24 20:42,22/Jun/22 05:48,1.16.0,,,,,,,,API / Python,Connectors / Common,Table SQL / API,,,,,0,connector,jar,python,table-api,"h2. Background

User currently unable to build & install the latest PyFlink and then load jars. The jar loading mechanism was introduced in FLINK-16943.
h2. Reproduction steps
 * Clone the latest Flink from the master branch.
 * Follow the Flink [recommended steps|https://nightlies.apache.org/flink/flink-docs-master/docs/flinkdev/building/] to build Flink & install PyFlink. Notes: Tutorial recommended Maven 3.2.x, Python 3.6-3.9, reproduced with: Maven 3.2.5, Python 3.7.
 * Create a new Python Table API app that loads in a jar, similar to:

{code:java}
from pyflink.table import TableEnvironment, StreamTableEnvironment, EnvironmentSettings
env_settings = EnvironmentSettings.in_streaming_mode()
t_env = StreamTableEnvironment.create(environment_settings=env_settings)
t_env.get_config().set(""pipeline.classpaths"", ""file:///path/to/your/jar.jar"") {code}
 
 * The following alternative way of loading jars produce a similar issue:

{code:java}
table_env.get_config().get_configuration().set_string(""pipeline.jars"", ""file:///path/to/your/jar.jar"") {code}
 
 * The jar loaded here can be any jar, and the following message will appear:

{code:java}
Traceback (most recent call last):
  File ""pyflink_table_api_firehose.py"", line 48, in <module>
    log_processing()
  File ""pyflink_table_api_firehose.py"", line 14, in log_processing
    t_env.get_config().set(""pipeline.classpaths"", ""file:///home/YOUR_USER/pyflink-table-api/flink/flink-connectors/flink-sql-connector-aws-kinesis-firehose/target/flink-sql-connector-aws-kinesis-firehose-1.16-SNAPSHOT.jar"")
  File ""/home/YOUR_USER/.local/lib/python3.7/site-packages/pyflink/table/table_config.py"", line 109, in set
    add_jars_to_context_class_loader(value.split("";""))
  File ""/home/YOUR_USER/.local/lib/python3.7/site-packages/pyflink/util/java_utils.py"", line 169, in add_jars_to_context_class_loader
    addURL.invoke(loader, to_jarray(get_gateway().jvm.Object, [url]))
  File ""/home/YOUR_USER/.local/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1322, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/home/YOUR_USER/.local/lib/python3.7/site-packages/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/home/YOUR_USER/.local/lib/python3.7/site-packages/py4j/protocol.py"", line 328, in get_return_value
    format(target_id, ""."", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o45.invoke.
: java.lang.IllegalArgumentException: object is not an instance of declaring class
   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.base/java.lang.reflect.Method.invoke(Method.java:566)
   at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
   at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
   at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
   at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
   at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
   at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
   at java.base/java.lang.Thread.run(Thread.java:829) {code}
 
 * Next do:

{code:java}
pip uninstall apache-flink
pip install apache-flink{code}
...to downgrade it to 1.15 release.

The loading of the jar should be successful. Even if you try to load the same connector built from master (reproduced with Kafka, Kinesis Firehose).

Reproduced on Mac and Amazon Linux 2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 27 09:12:18 UTC 2022,,,,,,,,,,"0|z13fqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 01:51;hxbks2ks;I have fixed it in https://issues.apache.org/jira/browse/FLINK-28002. You can fetch the latest master code and try again.;;;","22/Jun/22 05:48;dianfu;[~CrynetLogistics] Have closed this ticket as duplicate as it seems a bug and have been fixed in FLINK-28002. Feel free to reopen it if the fix in FLINK-28002 doesn't solve your issue.;;;","27/Jun/22 09:12;CrynetLogistics;[~dianfu] [~hxbks2ks] Apologies for the late reply, I was off for a few days. 

Thank you making the fix! This is a duplicate of FLINK-28002.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update httplib2 to at least 0.19.0 ,FLINK-28147,13451346,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,20/Jun/22 14:17,21/Jun/22 06:41,04/Jun/24 20:42,21/Jun/22 06:41,1.15.0,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,We should update httplib2 to at least 0.19.0 to address CVE-2021-21240 and avoid false flags about Flink being vulnerable. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 06:41:44 UTC 2022,,,,,,,,,,"0|z13fko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 06:41;martijnvisser;Fixed in master: dcb77049d21b586d4669229a8248da72ef6fdcf7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync blocklist information between JobMaster & ResourceManager,FLINK-28146,13451343,13450987,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,20/Jun/22 13:34,22/Jul/22 12:12,04/Jun/24 20:42,22/Jul/22 12:12,1.16.0,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,The newly added/updated blocked nodes should be synchronized between JM and RM.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 22 12:12:23 UTC 2022,,,,,,,,,,"0|z13fk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 12:12;zhuzh;Done via 7b05a1b4c9a4ae664fb6b7c4bb85fb3ea6281505;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let ResourceManager support blocklist mechanism,FLINK-28145,13451342,13450987,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,20/Jun/22 13:33,22/Jul/22 06:03,04/Jun/24 20:42,22/Jul/22 06:03,1.16.0,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"Let ResourceManager support blocklist mechanism:
1. SlotManager should filter out blocked resources when allocating registered resources.
2. ResourceManagerDriver should avoid allocating task managers from blocked nodes.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 22 06:03:42 UTC 2022,,,,,,,,,,"0|z13fjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 06:03;zhuzh;Done via
6f7455b078ba89302b8c05e9d39d3a1ca114700c
9815caad271a561640ffe0df7193c04270d53a25
2e5cac1f31aa571276df20e24889994672692a89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let JobMaster support blocklist mechanism,FLINK-28144,13451341,13450987,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,20/Jun/22 13:31,11/Jul/22 15:38,04/Jun/24 20:42,11/Jul/22 15:38,1.16.0,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"SlotPool should avoid allocating slots that located on blocked nodes. To do that, our core idea is to keep the SlotPool in such a state: there is no slot in SlotPool that is free (no task assigned) and located on blocked nodes. Details are as following:

1. When receiving slot offers from task managers located on blocked nodes, all offers should be rejected.
2. When a node is newly blocked, we should release all free(no task assigned) slots on it. We need to find all task managers on blocked nodes and release all free slots on them by SlotPoolService#releaseFreeSlotsOnTaskManager.
3. When a slot state changes from reserved(task assigned) to free(no task assigned), it will check whether the corresponding task manager is blocked. If yes, release the slot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 15:38:34 UTC 2022,,,,,,,,,,"0|z13fjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 15:38;zhuzh;Done via:
f2f83e1956eccecaa2371b21bddaf7778bb4f819
04f2f0c2660b312449419a3acb58a46a38d84f64
72ea8b5999bf36125aa5f1a38df4ec52c7a95702
387b2a473d0c0a8d58d1ca0401894dffc0527b31;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce BlocklistHandler,FLINK-28143,13451339,13450987,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,20/Jun/22 13:28,01/Jul/22 16:24,04/Jun/24 20:42,01/Jul/22 16:24,1.16.0,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,JobMasterBlocklistHandler/ResourceManagerBlocklistHandler is the component in JM/RM responsible for managing all blocked node information and performing them on resources.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28137,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 16:24:46 UTC 2022,,,,,,,,,,"0|z13fj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 16:24;zhuzh;Done via
57f1b6e508159cb40b60ef34d58f29044c88286c
b174deec01552f28b6ca1003d07831396ce48117;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enrich TaskManagerLocation with node information,FLINK-28142,13451338,13450987,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,20/Jun/22 13:24,01/Jul/22 03:54,04/Jun/24 20:42,01/Jul/22 03:54,1.16.0,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"In order to support blocking nodes, it is necessary to know the node where the task is located. To do that, we need to add a node identifier into TaskManagerLocation. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 03:54:35 UTC 2022,,,,,,,,,,"0|z13fiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 03:54;zhuzh;Done via:
51c139b601806cc4f4272fb678f4d1bed4cf06ab
eab0a1faf5e7ecf8da641880b8913d49ac19da2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Dynamic Namespaces,FLINK-28141,13451330,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,morhidi,,20/Jun/22 12:48,24/Nov/22 01:01,04/Jun/24 20:42,19/Jul/22 07:59,kubernetes-operator-1.1.0,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,Starter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27871,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 19 07:59:32 UTC 2022,,,,,,,,,,"0|z13fh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 07:59;gyfora;This is generally documented as part of the configs and helm chart;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the documentation by adding Python examples,FLINK-28140,13451328,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pemide,dianfu,dianfu,20/Jun/22 12:34,30/Jul/22 06:44,04/Jun/24 20:42,30/Jul/22 06:44,,,,,,1.15.2,1.16.0,,API / Python,Documentation,,,,,,0,pull-request-available,,,,"There are still quite a few documentations only having Java/Scala examples. The aim of this JIRA is to improve these kinds of documentation by adding Python examples.

Here is a list of documentations needed to improve:
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/common/]
 * Done [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/data_stream_api/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/time_attributes/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/temporal_table_function/]
 * Done [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/timezone/]
 * Done [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tableapi/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/modules/]
 * Done [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/sources/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/application_parameters/]
 * Done [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution/execution_configuration/]
 * Done [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution/parallel/]
 * Done [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/]
 * Done [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/metrics/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/formats/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/rabbitmq/]
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/]

Note: we could improve these documentation in separate PRs to ease contribute and review~",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 30 06:44:24 UTC 2022,,,,,,,,,,"0|z13fgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 12:42;pemide;[~dianfu] Could I take this ticket?;;;","20/Jun/22 12:43;dianfu;[~pemide] Thanks for taking this. Have assigned it to you~;;;","01/Jul/22 05:39;dianfu;Updated page: [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/data_stream_api/]
- master via ce11ce307e7c90ecbc92a42815d9e3cbae707675
- release-1.15 via 898dc4927608fee4df801e7abff879fb1010ddc5;;;","07/Jul/22 11:27;dianfu;Updated page https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/concepts/time_attributes/: 
- master via 645d83172468cd3000a262d6c012ac1d96b956c6
- release-1.15 via e75bfae36a1bd5752628353b95a86a7f665e9134;;;","18/Jul/22 06:34;dianfu;Updated the following pages:
 - https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/
 - https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution/execution_configuration/
 - https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution/parallel/
 - https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tableapi/
 - https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/timezone/
 - https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/
 - https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery/
via the following commits:
 - master via cf6e72e504a7a5849dd397f6f0db2d2ce2cf1710
 - release-1.15 via 54c8d5a64cc6a7ff875a26c6a4fe56a5d1198ac6;;;","30/Jul/22 06:44;dianfu;Updated the remaining documentation pages:
* master via 37039db5db89bce8039f3bd2625ac1091c6c3e97
* release-1.15 via 8db40cecadf48bde47e18d64d30651157ea52a48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation for speculative execution,FLINK-28139,13451186,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,20/Jun/22 11:44,23/Aug/22 05:39,04/Jun/24 20:42,23/Aug/22 05:39,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 23 05:39:06 UTC 2022,,,,,,,,,,"0|z13fdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 05:39;zhuzh;Done via 70d9f6c31b289b6ea284a02fdb1d8cfc1a1a5414;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics for speculative execution,FLINK-28138,13451181,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,20/Jun/22 11:43,22/Jul/22 08:03,04/Jun/24 20:42,22/Jul/22 08:03,,,,,,1.16.0,,,Documentation,,,,,,,0,pull-request-available,,,,"Following two metrics will be added to expose job problems and show the effectiveness of speculative execution:
 # {*}numSlowExecutionVertices{*}: Number of slow execution vertices at the moment.
 # {*}numEffectiveSpeculativeExecutions{*}: Number of speculative executions which finish before their corresponding original executions finish.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 22 08:03:54 UTC 2022,,,,,,,,,,"0|z13fc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 08:03;zhuzh;Done via
2173b45b570de8ff1507b8e29a884e2449ffea62
19b0a95c30afd9ed65252c49fb00cef882412553;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce SpeculativeScheduler,FLINK-28137,13451167,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,20/Jun/22 11:42,13/Jul/22 15:15,04/Jun/24 20:42,13/Jul/22 15:15,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"A SpeculativeScheduler will be used if speculative execution is enabled. It extends AdaptiveBatchScheduler so that speculative execution can work along with the feature to adaptively tuning parallelisms for batch jobs.

The major differences of SpeculativeScheduler are:
 * SpeculativeScheduler needs to be able to directly deploy an Execution, while AdaptiveBatchScheduler can only perform ExecutionVertex level deployment.
 * SpeculativeScheduler does not restart the ExecutionVertex if an execution fails when any other current execution is still making progress
 * SpeculativeScheduler listens on slow tasks. Once there are slow tasks, it will block the slow nodes and deploy speculative executions of the slow tasks on other nodes.
 * Once any execution finishes, SpeculativeScheduler will cancel all the remaining executions of the same execution vertex.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28143,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 13 15:15:26 UTC 2022,,,,,,,,,,"0|z13f94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 15:15;zhuzh;Done via
81c739ae462412e531216bb46bc567fce2355dd8
265612c2cf93a589d87d7fc8ca168bc19d838885;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement ExecutionTimeBasedSlowTaskDetector,FLINK-28136,13451160,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wangwj,zhuzh,zhuzh,20/Jun/22 11:41,01/Jul/22 03:45,04/Jun/24 20:42,01/Jul/22 03:45,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"In the first version of speculative execution, an ExecutionTimeBasedSlowTaskDetector will be used to detect slow tasks. For ExecutionTimeBasedSlowTaskDetector, if a task's execution time is much longer than that of most tasks of the same JobVertex, the task will be identified as slow. More specifically, it will compute an execution time baseline for each JobVertex. Tasks which execute longer than or equals to the baseline will be identified as slow tasks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 03:45:42 UTC 2022,,,,,,,,,,"0|z13f7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 03:45;zhuzh;Done via ef07590403ad7448a523ef5da78da0306cccdae7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce SlowTaskDetector,FLINK-28135,13451148,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wangwj,zhuzh,zhuzh,20/Jun/22 11:39,01/Jul/22 03:45,04/Jun/24 20:42,01/Jul/22 03:45,,,,,,1.16.0,,,,,,,,,,0,pull-request-available,,,,A SlowTaskDetector will periodically check all the current tasks/executions and notify the SlowTaskDetectorListener about the detected slow tasks. SpeculativeScheduler will register itself as the SlowTaskDetectorListener.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 03:45:19 UTC 2022,,,,,,,,,,"0|z13f4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 03:45;zhuzh;Done via a0e8818d6967e5b0d14367d2095c91a05cadfe1f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce SpeculativeExecutionVertex,FLINK-28134,13451140,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,20/Jun/22 11:38,01/Jul/22 13:32,04/Jun/24 20:42,01/Jul/22 13:32,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"SpeculativeExecutionVertex will be used if speculative execution is enabled, as a replacement of ExecutionVertex to form an ExecutionGraph. The core difference is that a SpeculativeExecutionVertex can have multiple current executions running at the same time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 13:32:38 UTC 2022,,,,,,,,,,"0|z13f34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 13:32;zhuzh;Done via
b96d476d4cd05de1e6c0b001f2477aaed98bd473
e73225e4071fb7ffe7c4d5cbfd89983129fe3312
67aeb8165c623e35f773de7e50d0f00c24cca539;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rework DefaultScheduler to directly deploy executions,FLINK-28133,13451131,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,20/Jun/22 11:37,24/Jun/22 07:26,04/Jun/24 20:42,24/Jun/22 07:26,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,"Currently, the DefaultScheduler(base of AdaptiveBatchScheduler) can only perform ExecutionVertex level deployment. However, in this case, the scheduler is actually deploying the current execution attempt of the ExecutionVertex.

Therefore, we need to rework the DefaultScheduler to directly deploy executions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 24 07:26:25 UTC 2022,,,,,,,,,,"0|z13f14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 07:26;zhuzh;Done via c66101d518615ec2d6a36c829568583a6a5d93f9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should we remove type validations when using lookup-key join ?,FLINK-28132,13451057,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zaobao,zaobao,20/Jun/22 11:28,16/Jun/23 06:47,04/Jun/24 20:42,16/Jun/23 06:46,1.15.0,,,,,,,,Connectors / JDBC,Table SQL / Planner,,,,,,0,,,,,"As described in https://issues.apache.org/jira/browse/FLINK-18234

 

Execute sql

{color:#6a8759}select * from t left join jdbc_source for system_time as of t.proctime AS j on t.id = j.id{color}

t.id(VARCHAR) j.id(INT) will throw exception 
org.apache.flink.table.api.TableException: VARCHAR(2147483647) and INTEGER does not have common type now

If I remove some type validation codes, the sql works well on MySQL

 

Is it necessary to check data types when we join stream data to dynamic tables",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 06:44:59 UTC 2022,,,,,,,,,,"0|z13eko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 12:07;martijnvisser;[~zaobao] I don't see why we should remove these validations, this might work in this specific case for MySQL but not in other cases. There's also a simple solution for your problem, because you can explicitly CAST the data type to the correct one. ;;;","20/Jun/22 13:19;zaobao;[~martijnvisser] All the RDBMs I've used support implicit type conversion, including MSSQL, MySQL, Oracle and Postgres

In addition, HQL supports comparison between INT and VARCHAR

With KV storages like Redis, it makes no difference whether the field type is INT or VARCHAR, as we convert keys to STRING to access Redis


If these validations are removed, user-friendliness and development efficiency will improve

I'm a user of flink-sql. It makes sense to me;;;","20/Jun/22 14:25;martijnvisser;But it's not only RDBMs; it's also Flink internally. I could see some danger. 

Either way, there was a FLIP drafted on implicit type coercion (https://cwiki.apache.org/confluence/display/FLINK/FLIP-154%3A+SQL+Implicit+Type+Coercion) but that discussion has never been completed and the FLIP hasn't been voted on. A change such as this one would definitely require this FLIP discussion to be restarted and voted on. ;;;","21/Jun/22 03:39;zaobao;I never thought about how to modify Flink's behavior of type conversions internally.

Could we pass the raw value of parameters to external storages (such as RDBMS) without any type conversion/validation, when using lookup-key join, and just leave these problems to the external storages themselves;;;","21/Jun/22 06:44;martijnvisser;We definitely can, but only after the FLIP has been discussed and accepted via a vote;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-168: Speculative Execution for Batch Job,FLINK-28131,13450988,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,20/Jun/22 11:17,26/Sep/22 06:22,04/Jun/24 20:42,26/Sep/22 06:22,,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,,,,,"Speculative executions is helpful to mitigate slow tasks caused by problematic nodes. The basic idea is to start mirror tasks on other nodes when a slow task is detected. The mirror task processes the same input data and produces the same data as the original task. 

More detailed can be found in [FLIP-168|[https://cwiki.apache.org/confluence/display/FLINK/FLIP-168%3A+Speculative+Execution+for+Batch+Job].]

 

This is the umbrella ticket to track all the changes of this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-10644,,,,,FLINK-28397,FLINK-28587,FLINK-28130,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-20 11:17:27.0,,,,,,,,,,"0|z13e5c:",9223372036854775807,"Speculative execution(FLIP-168) is introduced in Flink 1.16 to mitigate batch job slowness which is caused by problematic nodes. A problematic node may have hardware problems, accident I/O busy, or high CPU load. These problems may make the hosted tasks run much slower than tasks on other nodes, and affect the overall execution time of a batch job.

When speculative execution is enabled, Flink will keep detecting slow tasks. Once slow tasks are detected, the nodes that the slow tasks locate in will be identified as problematic nodes and get blocked via the blocklist mechanism(FLIP-224). The scheduler will create new attempts for the slow tasks and deploy them to nodes that are not blocked, while the existing attempts will keep running. The new attempts process the same input data and produce the same data as the original attempt. Once any attempt finishes first, it will be admitted as the only finished attempt of the task, and the remaining attempts of the task will be canceled.

Most existing sources can work with speculative execution(FLIP-245). Only if a source uses SourceEvent, it must implement SupportsHandleExecutionAttemptSourceEvent to support speculative execution. Sinks do not support speculative execution yet so that speculative execution will not happen on sinks at the moment.

The Web UI & REST API are also improved(FLIP-249) to display multiple concurrent attempts of tasks and blocked task managers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-224: Blocklist Mechanism,FLINK-28130,13450987,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,20/Jun/22 11:14,05/Jun/23 11:31,04/Jun/24 20:42,16/Aug/22 03:57,1.16.0,,,,,1.16.0,,,Runtime / Coordination,,,,,,,0,,,,,"In order to support speculative execution for batch jobs([FLIP-168|https://cwiki.apache.org/confluence/display/FLINK/FLIP-168%3A+Speculative+Execution+for+Batch+Job]), we need a mechanism to block resources on nodes where the slow tasks are located. We propose to introduce a blocklist mechanism as follows:  Once a node is marked as blocked, future slots should not be allocated from the blocked node, but the slots that are already allocated will not be affected.

More details see [FLIP-224|https://cwiki.apache.org/confluence/display/FLINK/FLIP-224%3A+Blocklist+Mechanism]

This is the umbrella ticket to track all the changes of this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32253,,,,,FLINK-28131,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-20 11:14:32.0,,,,,,,,,,"0|z13e54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation for rewrite data layout after scaling bucket number,FLINK-28129,13450936,13443540,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,20/Jun/22 09:13,23/Jun/22 02:36,04/Jun/24 20:42,23/Jun/22 02:36,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"We should add a thorough doc on
 * How to rescale data layout after changing bucket number. 
 * The current limitation on rescaling partitions for the log system.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 23 02:36:11 UTC 2022,,,,,,,,,,"0|z13dts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 02:54;qingyue;Hi [~lzljs3620320] please assign it to me;;;","21/Jun/22 03:36;lzljs3620320;Thanks [~qingyue] ;;;","23/Jun/22 02:36;lzljs3620320;master: df78da0e428467af3153984199300f049c22878f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create e2e test for spark reader,FLINK-28128,13450926,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,lzljs3620320,lzljs3620320,20/Jun/22 08:14,13/Oct/22 02:51,04/Jun/24 20:42,13/Oct/22 02:51,,,,,,table-store-0.3.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-20 08:14:17.0,,,,,,,,,,"0|z13drk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancel during stop-with-savepoint may still create savepoint,FLINK-28127,13450924,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,20/Jun/22 08:06,20/Jun/22 08:06,04/Jun/24 20:42,,1.15.0,,,,,,,,Runtime / Coordination,,,,,,,0,,,,,"If the job is cancelled while a stop-with-savepoint is happening, then the savepoint may complete at the same time the cancellation is triggered, without being reported to the user via the REST API.
This is because the checkpoint coordinator does not run in the main thread, so it might happen that the final task acknowledge is handed over, immediately followed by a cancellation.

This chances of this happening are very low, but nevertheless we should figure out a solution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-20 08:06:54.0,,,,,,,,,,"0|z13dr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Iteration gets stuck when replayable datastream and its downstream operator have different parallelism,FLINK-28126,13450901,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gaoyunhaii,xuannan,xuannan,20/Jun/22 05:54,23/Jun/22 03:38,04/Jun/24 20:42,,ml-2.0.0,,,,,,,,Library / Machine Learning,,,,,,,0,,,,,"Iteration gets stuck when replayable datastream and its downstream operator have different parallelism. It can be reproduced with the following code snippet.


{code:java}
    @Test
    public void testIteration() throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        final SingleOutputStreamOperator<Integer> variable = env.fromElements(0).name(""i"");
        final SingleOutputStreamOperator<Integer> data = env.fromElements(1, 2).name(""inc"")
                .map(x -> x).setParallelism(1); // test can pass if parallelism is 2.

        final IterationConfig config = IterationConfig.newBuilder().build();

        Iterations.iterateBoundedStreamsUntilTermination(
                DataStreamList.of(variable),
                ReplayableDataStreamList.replay(data),
                config,
                (IterationBody) (variableStreams, dataStreams) -> {
                    final DataStream<Integer> sample = dataStreams.get(0);
                    final SingleOutputStreamOperator<Integer> trainOutput =
                            sample
                                    .transform(
                                            ""iter"",
                                            TypeInformation.of(Integer.class),
                                            new IterTransform())
                                    .setParallelism(2)
                                    .map((MapFunction<Integer, Integer>) integer -> integer)
                                    .setParallelism(1);

                    return new IterationBodyResult(
                            DataStreamList.of(trainOutput), DataStreamList.of(trainOutput));
                });

        env.execute();
    }

    public static class IterTransform extends AbstractStreamOperator<Integer>
            implements OneInputStreamOperator<Integer, Integer>, IterationListener<Integer> {

        @Override
        public void processElement(StreamRecord<Integer> element) throws Exception {
            LOG.info(""Processing element: {}"", element);
        }

        @Override
        public void onEpochWatermarkIncremented(
                int epochWatermark, Context context, Collector<Integer> collector)
                throws Exception {
            LOG.info(""onEpochWatermarkIncremented: {}"", epochWatermark);
            if (epochWatermark >= 10) {
                return;
            }
            collector.collect(0);
        }

        @Override
        public void onIterationTerminated(Context context, Collector<Integer> collector)
                throws Exception {
            LOG.info(""onIterationTerminated"");
        }
    }
{code}

After digging into the code, I found that the `ReplayOperator` doesn't emit the epoch watermark with a broadcast output. [~gaoyunhaii], could you look to see if this is the case?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 23 03:38:59 UTC 2022,,,,,,,,,,"0|z13dm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 03:38;gaoyunhaii;Very thanks [~xuannan] for the investigation, I'll have a look~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote leadership changing logs to INFO level,FLINK-28125,13450898,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,Weijie Guo,xtsong,xtsong,20/Jun/22 05:16,29/Mar/23 09:40,04/Jun/24 20:42,29/Mar/23 09:40,,,,,,,,,Runtime / Coordination,,,,,,,0,pull-request-available,stale-assigned,starter,,"Currently, {{DefaultLeaderElectionService}} logs leadership changing events in DEBUG level, expecting the contender to log the changes in INFO level. This is fragile because there're many different {{LeaderContender}} implementations and not all of them print the logs properly. E.g., {{JobMasterServiceLeadershipRunner}} logs the leadership changes in TRACE level.

I'd suggest to revisit the logging of leadership changing, printing the INFO logs from {{LeaderElectionService}}. That probably also means removing the contender side logs to avoid redundancy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31651,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 20 22:37:40 UTC 2022,,,,,,,,,,"0|z13dlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 06:49;Weijie Guo;Hi [~xtsong] , I am interested in this ticket, would you like to assign it to me?;;;","20/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink on K8S in hosetNetwork mode, the CPU usage will spike",FLINK-28124,13450890,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,spoon-lz,spoon-lz,20/Jun/22 03:22,20/Jun/22 03:22,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,Runtime / Metrics,,,,,,0,,,,,"When using the flink on k8s, in the configuration 'kubernetes. Hostnetwork. Enabled = true' parameters, If a node is running too many pods
, there will be a node CPU surge problem, The metric module collected the indicator information of the network card

 
{code:java}
Class :SystemResourcesCounter
Line 109 :calculateNetworkUsage(hardwareAbstractionLayer.getNetworkIFs());{code}
The problem is with this method
{code:java}
hardwareAbstractionLayer.getNetworkIFs(){code}
this method returns the current all the  network card on the machine, each pod will create a virtual network card (ifr_name = ""kube - ipvs0""), In hostNetwork mode, the network card is visible to the public, resulting in too much network card information returned by the interface

For example, if a node runs 200 pods, then the method calling this interface in each POD will return at least 200 nic information. With the default execution once every 5 seconds, this machine needs to return 200*200=40000 network card information every 5 seconds, this results in a significant CPU consumption",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-20 03:22:21.0,,,,,,,,,,"0|z13djk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When a flink job is restarted, the metaspce size of the taskmanager does not decrease but keeps increasing. After several restarts, the flink job metaspce oom.",FLINK-28123,13450882,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,liu zhuang,liu zhuang,20/Jun/22 02:21,09/Feb/23 09:34,04/Jun/24 20:42,09/Feb/23 09:34,1.12.0,,,,,,,,Runtime / Task,,,,,,,0,,,,,"When I use the flink standalone deployment mode, when the flink job restarts, the metaspce size of the taskmanager does not decrease but keeps increasing. After restarting several times, the flink job metaspce oom.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 04:54:36 UTC 2022,,,,,,,,,,"0|z13dhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 02:34;liu zhuang;TaskManager log:

2022-06-17 16:35:42,720 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Fatal error occurred while executing the TaskManager. Shutting it down...
java.lang.OutOfMemoryError: Metaspace. The metaspace out-of-memory error has occurred. This can mean two things: either the job requires a larger size of JVM metaspace to load classes or there is a class loading leak. In the first case 'taskmanager.memory.jvm-metaspace.size' configuration option should be increased. If the error persists (usually in cluster after several job (re-)submissions) then there is probably a class loading leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
2022-06-17 16:36:21,653 ERROR org.apache.flink.runtime.taskmanager.Task                    [] - Error in the task canceler for task Co-Process-Broadcast (2/6)#0.
java.lang.OutOfMemoryError: Metaspace. The metaspace out-of-memory error has occurred. This can mean two things: either the job requires a larger size of JVM metaspace to load classes or there is a class loading leak. In the first case 'taskmanager.memory.jvm-metaspace.size' configuration option should be increased. If the error persists (usually in cluster after several job (re-)submissions) then there is probably a class loading leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
2022-06-17 16:35:46,965 ERROR org.apache.flink.runtime.taskmanager.Task                    [] - Error in the task canceler for task Source: Custom Source (2/2)#0.
java.lang.OutOfMemoryError: Metaspace. The metaspace out-of-memory error has occurred. This can mean two things: either the job requires a larger size of JVM metaspace to load classes or there is a class loading leak. In the first case 'taskmanager.memory.jvm-metaspace.size' configuration option should be increased. If the error persists (usually in cluster after several job (re-)submissions) then there is probably a class loading leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
2022-06-17 16:36:23,101 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Fatal error occurred while executing the TaskManager. Shutting it down...
java.lang.OutOfMemoryError: Metaspace. The metaspace out-of-memory error has occurred. This can mean two things: either the job requires a larger size of JVM metaspace to load classes or there is a class loading leak. In the first case 'taskmanager.memory.jvm-metaspace.size' configuration option should be increased. If the error persists (usually in cluster after several job (re-)submissions) then there is probably a class loading leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
2022-06-17 16:36:30,814 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Custom Source (2/2)#0 (2f7ed12b5dcad0ba93f9ccd9e2892f51).
2022-06-17 16:36:17,287 ERROR org.apache.kafka.common.utils.KafkaThread                    [] - Uncaught exception in thread 'kafka-producer-network-thread | producer-4':
java.lang.OutOfMemoryError: Metaspace. The metaspace out-of-memory error has occurred. This can mean two things: either the job requires a larger size of JVM metaspace to load classes or there is a class loading leak. In the first case 'taskmanager.memory.jvm-metaspace.size' configuration option should be increased. If the error persists (usually in cluster after several job (re-)submissions) then there is probably a class loading leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
2022-06-17 16:38:13,745 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Fatal error occurred while executing the TaskManager. Shutting it down...
java.lang.OutOfMemoryError: Metaspace. The metaspace out-of-memory error has occurred. This can mean two things: either the job requires a larger size of JVM metaspace to load classes or there is a class loading leak. In the first case 'taskmanager.memory.jvm-metaspace.size' configuration option should be increased. If the error persists (usually in cluster after several job (re-)submissions) then there is probably a class loading leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...;;;","20/Jun/22 04:47;martijnvisser;[~liu zhuang] Please try this with the latest supported Flink version, since 1.12 is no longer supported by the Flink community;;;","20/Jun/22 04:54;liu zhuang;[~martijnvisser],OK，thanks your reply.I will try the latest flink version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Overview "" and ""Project Configuration"" in ""User-defined Sources & Sinks"" page ",FLINK-28122,13450879,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hunterLiu,yckkcy,yckkcy,20/Jun/22 02:04,05/Sep/22 12:50,04/Jun/24 20:42,05/Sep/22 12:50,,,,,,1.17.0,,,chinese-translation,Documentation,,,,,,0,pull-request-available,stale-assigned,,,"The links are
https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#overview
and 
https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#project-configuration",,,,,,,,,,,,,,,FLINK-16105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 05 12:50:36 UTC 2022,,,,,,,,,,"0|z13dh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 10:03;chenzihao;hi, [~yckkcy] , I am willing to do it. Can you assign it to me? thank you.;;;","21/Jun/22 15:37;yckkcy;Sorry, [~chenzihao] I don't have the authority to assign it to you. Maybe you can ask someone who has the authority to do that.

But I'm willing to help review it after you pull the request on Github.;;;","22/Jun/22 01:57;chenzihao;[~martijnvisser] hi, Martijn. Can you help to confirm this ticket? I can do this work if needed.;;;","13/Jul/22 02:46;hunterLiu;[~jark] Hi,Jark,I am very interested to do it, I can do this work if needed.;;;","20/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","05/Sep/22 12:50;martijnvisser;Fixed in master: 2a3e2bb62df4fdb1e3914d4abc18e7a4b3f5a9dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Extension Points"" and ""Full Stack Example"" in ""User-defined Sources & Sinks"" page ",FLINK-28121,13450877,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yckkcy,yckkcy,yckkcy,20/Jun/22 02:00,30/Aug/22 11:22,04/Jun/24 20:42,30/Aug/22 11:22,,,,,,1.16.0,,,chinese-translation,Documentation,,,,,,0,pull-request-available,,,,"The links are https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#extension-points
and 
https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#full-stack-example",,,,,,,,,,,,,,,FLINK-16105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 30 11:22:09 UTC 2022,,,,,,,,,,"0|z13dgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 02:10;yckkcy;[~jark] hi,could you assign this to me?;;;","19/Jul/22 05:43;hunterLiu;[~yckkcy] Hi,Chengkai,I have completed the translation of the above part, you can see this pr (https://github.com/apache/flink/pull/20283), looking forward to your joining this pr;;;","20/Jul/22 14:43;yckkcy;[~hunterLiu] Hi, thanks for your contributions. I think it is better to submit my translations in my own pr of FLINK-28121 instead of joining your pr. 
Because it is recommended by the community that one pr should only solve one jira issue for the best.;;;","20/Jul/22 14:48;hunterLiu;[~yckkcy] alright，got it;;;","30/Aug/22 11:22;martijnvisser;Fixed in master: b3dcafa9db278fc02945c7bc5c32765c99d00bb1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Meet assert error: BatchPhysicalExchange.BATCH_PHYSICAL has lower cost then  best cost  of subset :RelSubset#15.BATCH_PHYSICAL.hash[0, 1]true.[]]",FLINK-28120,13450871,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,20/Jun/22 01:39,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,,1.20.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"When I run the following sql with Hive dialect,

 
{code:java}
create table src(key string, value string);

SELECT key, value FROM
(
  SELECT key, value FROM src
  UNION ALL
  SELECT key, key as value FROM ( 
    SELECT distinct key FROM (
      SELECT key, value FROM (
        SELECT key, value FROM src
        UNION ALL
        SELECT key, value FROM src
      )t1 
    group by key, value)t2
  )t3
)t4
group by key, value {code}
 

 

it'll throw the excpetion 

 
{code:java}
Caused by: java.lang.AssertionError: rel [rel#1507:BatchPhysicalExchange.BATCH_PHYSICAL.hash[0, 1]true.[](input=RelSubset#999,distribution=hash[key, value])] has lower cost {8.657154570189462E8 rows, 2.9568623376365746E10 cpu, 7.2E9 io, 3.394292742113678E9 network, 4.944093593596532E9 memory} than best cost {8.657154570189462E8 rows, 2.9568623376365746E10 cpu, 7.2E9 io, 3.3942927421136775E9 network, 4.944093593596532E9 memory} of subset [rel#1103:RelSubset#15.BATCH_PHYSICAL.hash[0, 1]true.[]] {code}
And then I check the Flink code in where it's thrown, I find it's in 

 
{code:java}
if (relCost.isLt(subset.bestCost)) {
  return litmus.fail(""rel [{}] has lower cost {} than ""
          + ""best cost {} of subset [{}]"",
          rel, relCost, subset.bestCost, subset);
} {code}
It seems the relCost is less than best cost, so the excpetion throw.

But the relCost is actually greater than the best cost, shown as follows:

!截屏2022-06-18 上午11.48.46.png|width=391,height=268!

 

It seems the logic in Flink cost comparison breaks.

Then, I find the method #isLt in FlinkCost, which depend on #isLe and #equals. But #isLe  use normalizeCost, #equals doesn't use normalizeCost, which bring such incosistent.

For such case, the normalizeCost if  relCost and bestCost will be same. Althogh the network isn't same,  they will end with be same when calculated as a normalizeCost, which seems like precison loss in double.

So #isLe will be true, but in method #equals, it will compare io, nework, memory separately, which result in false. Then #isLt  = #isLe(other) && !#equals(other) will be true, which bring such exceptioin.

To fix it, I think we should change the logic for #equals to make it consistent with what we use to compare in #isLe.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/22 01:45;luoyuxia;截屏2022-06-18 上午11.48.46.png;https://issues.apache.org/jira/secure/attachment/13045259/%E6%88%AA%E5%B1%8F2022-06-18+%E4%B8%8A%E5%8D%8811.48.46.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 01:58:29 UTC 2022,,,,,,,,,,"0|z13dfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 01:58;luoyuxia;[~godfrey] Could you please have a look. And I would like to take the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the document for Contribute Documentation,FLINK-28119,13450816,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,martijnvisser,csq,csq,19/Jun/22 14:20,19/Jun/23 12:27,04/Jun/24 20:42,19/Jun/23 12:27,1.16.0,,,,,,,,Documentation,,,,,,,0,,,,,"Following the [Contribute Documentation|https://flink.apache.org/contributing/contribute-documentation.html], it requires me to install Hugo when executing 
{code:sh}
cd docs
./build_docs.sh -p
{code}
.
And the web server is actually accessed by http://localhost:1313 

Maybe we should:
1. List the prerequisites before building the doc.
2. Correct the web server address.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 06:52:36 UTC 2022,,,,,,,,,,"0|z13d34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 06:52;martijnvisser;[~csq] Thanks! I'll update this. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TableStoreOptions to merge all options,FLINK-28118,13450800,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liliwei,lzljs3620320,lzljs3620320,19/Jun/22 10:04,05/Jul/22 06:29,04/Jun/24 20:42,05/Jul/22 06:23,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Currently we have OPTIONS in various places, which is harder to maintain, we could have just one class that contains all the OPTIONS, including
- MergeTreeOptions
- FileStoreOptions
- LogOptions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 06:23:44 UTC 2022,,,,,,,,,,"0|z13czk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 02:04;liliwei;I'd like to have a try, may i have the ticket?;;;","28/Jun/22 08:10;lzljs3620320;[~liliwei] Thanks~;;;","05/Jul/22 06:23;lzljs3620320;master: 80b69f77af418da669bbeb70ee7e7320aafdb945;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More metrics for FileSource,FLINK-28117,13450750,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,18/Jun/22 14:39,20/Jun/22 06:39,04/Jun/24 20:42,,1.16.0,,,,,,,,Connectors / FileSystem,Runtime / Metrics,,,,,,0,,,,,"According to user's requirement[1], following metrics are required:
1. No. of splits SplitAssigner is initialized with, number of splits re-added back to the SplitAssigner
2. Readers created per unit time
3. Time taken to create a reader
4. Time taken for the Reader to produce a single Row
5. Readers closed per unit time
 
Some further nice-to-have metrics:1. Number of rows emitted by the source per unit time
2. Time taken by the enumerator to discover the splits 
3. Total splits discovered
 
Please check FLIP-33 first and extend it if above mentioned metrics were not included in the FLIP.
 
[1] https://lists.apache.org/thread/t70hhss6d9s65y1vygyytbm6sgl05yrl",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28021,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 06:39:02 UTC 2022,,,,,,,,,,"0|z13cog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 06:39;martijnvisser;[~jingge] Why would these metrics only be interesting for FileSource? I also don't think we should list these as ""user requirement"". This is one user providing input, this should be part of a broader discussion imho. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch connector doesn't provide security through token,FLINK-28116,13450660,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,deepikasahu,deepikasahu,17/Jun/22 17:41,22/Jun/22 14:01,04/Jun/24 20:42,,1.15.0,,,,,,,,Connectors / ElasticSearch,,,,,,,0,,,,,Elasticsearchsink doesn’t have an option to pass the refreshing token in the header. It supports only the hardcoded user name and password. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 14:01:55 UTC 2022,,,,,,,,,,"0|z13c4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/22 12:36;jingge;Thanks for reporting this. Flink need a more generic way to handle credential when working with ecosystems. Do you want to contribute? [~deepikasahu] ;;;","22/Jun/22 14:01;deepikasahu;In earlier version of flink we had the provision to set the rest client factory at the ElasticsearchSink.Builder  level


        ElasticsearchSink.Builder<Map.Entry<String, String>> elasticsearchSinkBuilder = new ElasticsearchSink.Builder<>(.....);

// provide a RestClientFactory for custom configuration on the internally created REST client:-

 elasticsearchSinkBuilder.setRestClientFactory(new HighLevelRestClientFactory(adfsSecurityProvider));

 

Any specific reason this was removed , is there any other work around for the same?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.15.0 Parallelism Rebalance causes flink job failure,FLINK-28115,13450653,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gaoyunhaii,huameng,huameng,17/Jun/22 17:04,06/Jul/22 05:52,04/Jun/24 20:42,,1.15.0,,,,,,,,,,,,,,,0,,,,,"{color:#de350b}*Issue:*{color}

*Flink 1.15.0 Parallelism Rebalance causes flink job failure.* Same issue was not in flink 1.14.4.

{color:#de350b}*Exceptions:*{color}

*1 of the 8 re-balance parallelism task slots failed due to* 
*{color:#de350b}org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: finishConnect(..) failed: Connection refused: /127.0.0.1:43354{color}*
*{color:#de350b}Caused by: java.net.ConnectException: finishConnect(..) failed: Connection refused{color}*

 

*Job topology:*

*kafkaSource (parallelism 4) -> map/processer (parallelism 8) -> kafkaSink (4 parallelism)*

*Our dev flink cluster has 8 hosts,* *each host has 25 task managers alive.* *Each TM has 2 task slots*

*!image-2022-06-17-13-01-08-992.png!*

 
 
*Error stack trace:*
2022-06-17 12:54:38.563 WARN  [Framework] [Map (3/8)#5|#5] org.apache.flink.runtime.taskmanager.Task  - Map (3/8)#5 (69a82f741d68fd7161d7b13de48c6c4b) switched from RUNNING to FAILED with failure cause: org.apache.flink.runtime.io.network.partition.consumer.PartitionConnectionException: Connection for partition 2064424258b3b74fdc349607017f1029#1@a94744160d7d5e85101881e2d783dcd2 not reachable.
    at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:190)
    at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.internalRequestPartitions(SingleInputGate.java:342)
    at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.requestPartitions(SingleInputGate.java:312)
    at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.requestPartitions(InputGateWithMetrics.java:115)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connecting to remote task manager '/127.0.0.1:43354' has failed. This might indicate that the remote task manager has been lost.
    at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connect(PartitionRequestClientFactory.java:169)
    at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.connectWithRetries(PartitionRequestClientFactory.java:135)
    at org.apache.flink.runtime.io.network.netty.PartitionRequestClientFactory.createPartitionRequestClient(PartitionRequestClientFactory.java:96)
    at org.apache.flink.runtime.io.network.netty.NettyConnectionManager.createPartitionRequestClient(NettyConnectionManager.java:95)
    at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.requestSubpartition(RemoteInputChannel.java:186)
    ... 15 more
Caused by: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: finishConnect(..) failed: Connection refused: /127.0.0.1:43354
Caused by: java.net.ConnectException: finishConnect(..) failed: Connection refused
    at org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors.newConnectException0(Errors.java:155)
    at org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors.handleConnectErrno(Errors.java:128)
    at org.apache.flink.shaded.netty4.io.netty.channel.unix.Socket.finishConnect(Socket.java:320)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.doFinishConnect(AbstractEpollChannel.java:710)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.finishConnect(AbstractEpollChannel.java:687)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.epollOutReady(AbstractEpollChannel.java:567)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:470)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at java.lang.Thread.run(Thread.java:748)",Flink 1.15.0 session cluster with 8 hosts.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27341,,,,,,,,,,,,,,,,"17/Jun/22 17:01;huameng;image-2022-06-17-13-01-08-992.png;https://issues.apache.org/jira/secure/attachment/13045232/image-2022-06-17-13-01-08-992.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,English,,,Wed Jul 06 05:52:30 UTC 2022,,,,,,,,,,"0|z13c34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 22:05;jingge;[~renqs] Would you like to take a look at this?

[~huameng] did the issue only happen with Kafka connector? Have you tried with any other connectors?;;;","20/Jun/22 03:44;renqs;From the stack trace it looks like unrelated to Kafka connector but a TaskManager failure. [~huameng] could you check if the TaskManager is still alive while the error happens? Also some logs from that TM are helpful to locate the problem.;;;","21/Jun/22 14:16;huameng;Hi Jing and Qingsheng, thanks for looking into this parallelism re-balance issue issue.

The exception stack trace logging is from the task manager (TM).

This issue exists in only Flink 1.15.0, and occurs right after the job submission, and where the # of parallelism of map/process function is different from that of sink or source.

The issue is not related to Flink Kafka connector. The TaskManager is still alive when the issue occurs.

*Job topology:*

*kafkaSource (parallelism 4) -> map/processer (parallelism 8) -> kafkaSink (4 parallelism)*

*Our dev flink cluster has 8 hosts,* *each host has 25 task managers alive.* *Each TM has 2 task slots*

Interestingly, if a job uses same number of parallelism (No rebalance)  for *source* (parallelism {*}6{*}) -> *map* (parallelism {*}6{*}) -> *sink* (parallelism {*}6{*}), the job runs fine with no issue. ;;;","30/Jun/22 18:47;huameng;Hi Qingsheng and Flink team, 

After more experiments of flink 1.15.0 (Standlone cluster of 20 nodes)  job with parallelism rebalancing. Here is what we observed.

The parallelism rebalance issues occurs when Flink 1.15.0 uses a localhost taskmanager mix with task managers on other nodes, remote  taskmanagers connect to localhost:port is refused.  I feel like this is defect with Flink 1.15.0. JobManager DNS name , not localhost, should be used.

 

We are able to deploy flink 1.15.0 cluster to run only JobManager on zookeeper hosts, so the Jobmanager will not use localhost to assign taskmanager local to Jobmanager, which will make flink jobs with parallelism rebalance work.;;;","06/Jul/22 03:43;renqs;[~gaoyunhaii] Could you take a look at this issue? Thanks!;;;","06/Jul/22 05:52;gaoyunhaii;Hi [~huameng] sorry this seems to be a known issue: https://issues.apache.org/jira/browse/FLINK-27341 We'll try to fix this issue as soon as possible~ ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The path of the Python client interpreter could not point to an archive file in distributed file system,FLINK-28114,13450633,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,17/Jun/22 14:57,20/Jun/22 01:08,04/Jun/24 20:42,20/Jun/22 01:08,,,,,,1.15.1,1.16.0,,API / Python,,,,,,,0,,,,,"See https://github.com/apache/flink/blob/master/flink-python/src/main/java/org/apache/flink/client/python/PythonEnvUtils.java#L178 for more details about this limitation.

Users could execute PyFlink jobs in YARN application mode as following:
{code}
./bin/flink run-application -t yarn-application \
      -Djobmanager.memory.process.size=1024m \
      -Dtaskmanager.memory.process.size=1024m \
      -Dyarn.application.name=<ApplicationName> \
      -Dyarn.ship-files=/path/to/shipfiles \
      -pyarch shipfiles/venv.zip \
      -pyclientexec venv.zip/venv/bin/python3 \
      -pyexec venv.zip/venv/bin/python3 \
      -py shipfiles/word_count.py
{code}

In the above case, venv.zip will be distributed to the TMs via Flink blob server. However, blob server doesn't support files with size exceeding of 2GB. See https://github.com/apache/flink/blob/ea52732dc48a4f1c5be0925890cd8aa1ea2a11ed/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobServerConnection.java#L223 for more details. This is very serious problem as Python users usually tend to install a lot Python libraries inside the venv.zip and some Python libraries are very large.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 01:08:52 UTC 2022,,,,,,,,,,"0|z13byw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 01:08;dianfu;Fixed in:
- master via 6b04a50ae2182d4cdd8e44ea9a16171d1d2394ce
- release-1.15 via da05d0f3f6950dcf5e839bae0c396dbdf8a69e9e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document periodic savepointing,FLINK-28113,13450613,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,17/Jun/22 12:59,19/Jul/22 14:25,04/Jun/24 20:42,19/Jul/22 14:25,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,Starter,,,,"We should add a new section to the job management doc page about periodic savepoint triggering [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/job-management/#savepoint-management]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 18 05:49:37 UTC 2022,,,,,,,,,,"0|z13bug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 15:13;ConradJam;Hi [~gyfora] I want to tick this ticket;;;","15/Jul/22 06:45;gyfora;[~ConradJam] have you started working on this? It would be great to have it this week if possible.

I can take over if you dont have time;;;","18/Jul/22 05:49;gyfora;I have unassigned this ticket for now, please open a PR if you are still working on it, otherwise someone else can take it :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading error message when Hadoop S3FileSystem is used and not at classpath,FLINK-28112,13450608,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaborgsomogyi,prabhujoseph,prabhujoseph,17/Jun/22 12:30,22/Jun/22 14:18,04/Jun/24 20:42,22/Jun/22 14:05,1.15.0,1.16.0,,,,1.16.0,,,FileSystems,,,,,,,0,pull-request-available,,,,"When Using Hadoop S3FileSystem which is not at classpath, below error message is thrown *S3 scheme is not directly supported by Flink* which is misleading. Actually it is one of the directly supported filesystems.

{code}
Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 's3'. The scheme is not directly supported by Flink and no Hadoop file system to support this scheme could be loaded. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 14:05:31 UTC 2022,,,,,,,,,,"0|z13btc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 12:45;gaborgsomogyi;That message comes when ""fs.allowed-fallback-filesystems"" doesn't contain the target FS. When it's set the following message comes:
{code:java}
                        String.format(
                                ""Could not find a file system implementation for scheme '%s'. The scheme is ""
                                        + ""directly supported by Flink through the following plugin%s: %s. Please ensure that each ""
                                        + ""plugin resides within its own subfolder within the plugins directory. See https://ci.apache""
                                        + "".org/projects/flink/flink-docs-stable/ops/plugins.html for more information. If you want to ""
                                        + ""use a Hadoop file system for that scheme, please add the scheme to the configuration fs""
                                        + "".allowed-fallback-filesystems. For a full list of supported file systems, ""
                                        + ""please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/."",
{code}
;;;","17/Jun/22 13:00;prabhujoseph;Yes you are right. But *S3 scheme is not directly supported by Flink* log message is a wrong information to user. Flink directly supports S3 scheme. 

Suggest to check if the scheme is part of directly supported filesystem or not, based on that log the message in the final else

{code}
      } else {
                try {
                    fs = FALLBACK_FACTORY.create(uri);
                } catch (UnsupportedFileSystemSchemeException e) {
                    throw new UnsupportedFileSystemSchemeException(
                            ""Could not find a file system implementation for scheme '""
                                    + uri.getScheme()
                                    + ""'. The scheme is not directly supported by Flink and no Hadoop file system to ""
                                    + ""support this scheme could be loaded. For a full list of supported file systems, ""
                                    + ""please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/."",
                            e);
                }
            }
{code};;;","17/Jun/22 13:35;gaborgsomogyi;Now I see, one case is not covered, namely when directly supported and fallback failed also.;;;","17/Jun/22 14:47;gaborgsomogyi;Created a PR, plz have a look.;;;","22/Jun/22 14:05;martijnvisser;Fixed in master: 5bcef81356f965ee9e6a8ab54b5faca1e3979873;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flinksql use hivecatalog cause union all  operation lost  'eventTime attribute',FLINK-28111,13450585,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chaojipaopao,chaojipaopao,17/Jun/22 09:59,30/Jul/22 13:58,04/Jun/24 20:42,,1.12.4,1.13.5,1.14.4,,,,,,Table SQL / API,Table SQL / Planner,Table SQL / Runtime,,,,,0,,,,," In my scenario , i have 2 topics  have same schema ; i register them  to  table and define eventtime.

then create view use union all  2 table ,and use view  group by  tumble window ;

but when set hivecatalog ,sql can not run ;just like this:

Exception in thread ""main"" org.apache.flink.table.api.TableException: Window aggregate can only be defined over a time attribute column, but TIMESTAMP(3) encountered.

 

 *The complete code is as follows*
{code:java}
package com.unicom.test;

import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.SqlDialect;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.catalog.hive.HiveCatalog;

/**
 *
 * @author yt
 */
public class DataGenAndPrintSink {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);
        EnvironmentSettings envSetting = EnvironmentSettings
                .newInstance()
                .useBlinkPlanner()
                .inStreamingMode()
                .build();

        StreamTableEnvironment tableEnv =  StreamTableEnvironment.create(env, envSetting);

        String defaultDatabase = ""dc_dw"" ;
        String catalogName = ""dc_catalog"";

        HiveCatalog hive = new HiveCatalog(catalogName, defaultDatabase, ""hdfs://beh/flink/hive/conf"",""1.1.0"");

        tableEnv.registerCatalog(catalogName, hive);

        tableEnv.useCatalog(catalogName);

        tableEnv.useDatabase(defaultDatabase);

        tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);

        String sourceDDL = ""CREATE TABLE IF NOT EXISTS source_table (\n"" +
                ""    -- 维度数据\n"" +
                ""    order_id  STRING,\n"" +
                ""    -- 用户 id\n"" +
                ""    user_id BIGINT,\n"" +
                ""    -- 用户\n"" +
                ""    price BIGINT,\n"" +
                ""    -- 事件时间戳\n"" +
                ""    row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)),\n"" +
                ""    -- watermark 设置\n"" +
                ""    WATERMARK FOR row_time AS row_time - INTERVAL '5' SECOND\n"" +
                "") WITH (\n"" +
                ""  'connector' = 'datagen',\n"" +
                ""  'rows-per-second' = '10',\n"" +
                ""  'fields.order_id.length' = '1',\n"" +
                ""  'fields.user_id.min' = '1',\n"" +
                ""  'fields.user_id.max' = '100000',\n"" +
                ""  'fields.price.min' = '1',\n"" +
                ""  'fields.price.max' = '100000'\n"" +
                "")"";

        String sourceDDL_2 = ""CREATE TABLE IF NOT EXISTS source_table_2 (\n"" +
                ""    -- 维度数据\n"" +
                ""    order_id  STRING,\n"" +
                ""    -- 用户 id\n"" +
                ""    user_id BIGINT,\n"" +
                ""    -- 用户\n"" +
                ""    price BIGINT,\n"" +
                ""    -- 事件时间戳\n"" +
                ""    row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)),\n"" +
                ""    -- watermark 设置\n"" +
                ""    WATERMARK FOR row_time AS row_time - INTERVAL '5' SECOND\n"" +
                "") WITH (\n"" +
                ""  'connector' = 'datagen',\n"" +
                ""  'rows-per-second' = '10',\n"" +
                ""  'fields.order_id.length' = '1',\n"" +
                ""  'fields.user_id.min' = '1',\n"" +
                ""  'fields.user_id.max' = '100000',\n"" +
                ""  'fields.price.min' = '1',\n"" +
                ""  'fields.price.max' = '100000'\n"" +
                "")"";

        tableEnv.executeSql(sourceDDL);
        tableEnv.executeSql(sourceDDL_2);

        String view = ""create view IF NOT EXISTS test_view as select * from (select * from source_table union all select * from source_table_2) tb1"";

        tableEnv.executeSql(view);
        

        String sqlGroup = ""select count(*),UNIX_TIMESTAMP(CAST(tumble_start(row_time, interval '1' minute) AS STRING)) * 1000  as window_start from test_view group by order_id,tumble(row_time, interval '1' minute)"";


        tableEnv.executeSql(sqlGroup).print();
    }

}
{code}
 ","flink 1.12.4

hadoop 2.6.5

hive 1.1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jul 30 13:58:24 UTC 2022,,,,,,,,,,"0|z13bo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 11:40;martijnvisser;[~chaojipaopao] Can you please verify if this problem is occurring in Flink 1.15 or Flink 1.14? ;;;","17/Jun/22 13:19;chaojipaopao;[~martijnvisser] 1.12.4 but i try flink1.13.5 and flink1.14.4 also have  this problem;;;","17/Jun/22 22:09;jingge;[~chaojipaopao] how about Flink 1.15.0? Would you like to check it with the most up-to-date release? Thanks!;;;","20/Jun/22 01:54;chaojipaopao;[~jingge] [~martijnvisser]  flink 1.15.0 also have this problem.;;;","30/Jul/22 13:58;DavidLiu001;This is not an issue, but rather, by design.

The timestamp sequence can not be guranteed when there are two timestamps in ""union all"", which will lost the time attribute.

""join"" two stream tables with time attribute will also have the similar results.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive Reader supports projection pushdown,FLINK-28110,13450582,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,lzljs3620320,lzljs3620320,17/Jun/22 09:50,07/Jul/22 09:34,04/Jun/24 20:42,07/Jul/22 09:34,,,,,,table-store-0.2.0,,,Table Store,,,,,,,1,pull-request-available,,,,"When the user declares fields in the DDL, we may not report an error when the declared fields are incomplete, at this time we can assume that the user only wants to read these fields, in fact, it is projection pushdown",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 09:34:20 UTC 2022,,,,,,,,,,"0|z13bnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/22 10:11;complone;Maybe we need to use FilterUtils.isRetainedAfterApplyingFilterPredicates to filter out expressions of type List<ResolvedExpression> from DataFileReader's BinaryRowData data;

For example [ TestValuesTableFactory | https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory ] Conversion done by inner class TestValuesScanTableSourceWithoutProjectionPushDown;;;","18/Jun/22 10:14;complone;I will try to perfect this feature;;;","19/Jun/22 06:03;lzljs3620320;Hi [~complone] I see that you are already working on multiple JIRAs and they are not yet clearly progressing.
Maybe you can focus on a few problems instead of spreading out many aspects.;;;","07/Jul/22 09:34;lzljs3620320;master: 952b62a794928391f00f98218f5fab86b6bf5ee2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete useful code in the row emitter.,FLINK-28109,13450580,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ana4,ana4,17/Jun/22 09:27,22/May/24 04:23,04/Jun/24 20:42,,1.15.0,,,,,1.20.0,,,Connectors / ElasticSearch,,,,,,,0,pull-request-available,,,," 

The `.id(key)` in the RowElasticsearchEmitter make users get confused.

The following is the source code. key always null, we can never call the `id` method.
{code:java}
if (key != null) {
    final UpdateRequest updateRequest =
            new UpdateRequest(indexGenerator.generate(row), documentType, key)
                    .doc(document, contentType)
                    .upsert(document, contentType);
    indexer.add(updateRequest);
} else {
    final IndexRequest indexRequest =
            new IndexRequest(indexGenerator.generate(row), documentType)
                    .id(key)
                    .source(document, contentType);
    indexer.add(indexRequest);
}{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-17 09:27:51.0,,,,,,,,,,"0|z13bn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support compaction for append-only table,FLINK-28108,13450574,13443507,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,17/Jun/22 09:14,15/Jul/22 02:45,04/Jun/24 20:42,15/Jul/22 02:45,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,FLINK-27708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 15 02:45:47 UTC 2022,,,,,,,,,,"0|z13bls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 09:56;lzljs3620320;I think this is depends on FLINK-27708;;;","15/Jul/22 02:45;lzljs3620320;master: ec198499edcb5acaa804f21bd60022bdb633c85c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support id of document is null,FLINK-28107,13450569,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,17/Jun/22 08:47,04/Jul/22 02:19,04/Jun/24 20:42,20/Jun/22 03:17,1.16.0,,,,,1.16.0,,,API / Python,Connectors / ElasticSearch,,,,,,0,pull-request-available,,,," 
{code:java}
es7_sink = Elasticsearch7SinkBuilder() \
.set_emitter(ElasticsearchEmitter.static_index('foo')) \
.set_hosts(['localhost:9200'])  {code}
Caused by: java.lang.NullPointerException

at org.apache.flink.connector.elasticsearch.sink.SimpleElasticsearchEmitter$StaticIndexRequestGenerator.apply(SimpleElasticsearchEmitter.java:68)

at org.apache.flink.connector.elasticsearch.sink.SimpleElasticsearchEmitter$StaticIndexRequestGenerator.apply(SimpleElasticsearchEmitter.java:55)

at org.apache.flink.connector.elasticsearch.sink.SimpleElasticsearchEmitter.emit(SimpleElasticsearchEmitter.java:52)

at org.apache.flink.connector.elasticsearch.sink.SimpleElasticsearchEmitter.emit(SimpleElasticsearchEmitter.java:30)

at org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriter.write(ElasticsearchWriter.java:123)

at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:158)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 03:17:24 UTC 2022,,,,,,,,,,"0|z13bko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:56;martijnvisser;[~afedulov] [~alexanderpreuss] Should this PR also be directed towards the externalized Elasticsearch repo or will that be synced later? ;;;","17/Jun/22 09:08;ana4;[~martijnvisser] When this PR merge the main repo, I will create a new PR to Elasticsearch repo and add Python ES docs.;;;","18/Jun/22 12:41;jingge;[~ana4] please find the external ES connector repo: [https://github.com/apache/flink-connector-elasticsearch.|https://github.com/apache/flink-connector-elasticsearch]

It is recommended to work on the external ES connector repo for further development and bug fixs in the future. Thanks.;;;","19/Jun/22 08:58;ana4;[~jingge] Thanks, currently I will create the same two PR for main and external ES repos.;;;","20/Jun/22 03:17;dianfu;Merged to master via 11910d52cd1b948b21d0ea04263be689d2bd721e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create flink-table-store-connector-base to shade all flink dependencies,FLINK-28106,13450565,13449930,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,17/Jun/22 08:36,20/Jun/22 09:52,04/Jun/24 20:42,20/Jun/22 09:52,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"For Hive and other readers, they currently need to shade a bunch of dependencies, which is not very friendly, we can have a common module, and connector depends on this one module.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 09:52:37 UTC 2022,,,,,,,,,,"0|z13bjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 09:52;lzljs3620320;master: bab753f4d26d3479609632a38bdbb300a72d7ffd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should test the copied object in GlobFilePathFilterTest#testGlobFilterSerializable,FLINK-28105,13450555,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Leo Zhou,Leo Zhou,Leo Zhou,17/Jun/22 08:01,20/Jun/22 08:07,04/Jun/24 20:42,20/Jun/22 08:07,1.14.4,1.15.0,1.16.0,,,1.16.0,,,Tests,,,,,,,0,pull-request-available,,,,"Variable [matcherCopy|https://github.com/apache/flink/blob/master/flink-core/src/test/java/org/apache/flink/api/common/io/GlobFilePathFilterTest.java#L170] is created without testing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 08:07:51 UTC 2022,,,,,,,,,,"0|z13bhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:08;Leo Zhou;Hi [~zhuzh] ，can you take a look ?;;;","17/Jun/22 08:37;zhuzh;Thanks for reporting this problem! [~Leo Zhou] 

The ticket is assigned to you.;;;","20/Jun/22 08:07;zhuzh;Fixed via 44b941557e131ea03486ba5324232fe7b421a6c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop the unused order parameter in FirstValueFunction/LastValueFunction,FLINK-28104,13450553,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,17/Jun/22 07:56,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,,1.20.0,,,Table SQL / API,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jun 18 03:56:47 UTC 2022,,,,,,,,,,"0|z13bh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 11:45;libenchao;Although this is not documented, however we have used this in some cases.;;;","17/Jun/22 13:01;luoyuxia;[~libenchao] Appreciate for your inputs.  Seems it should be careful when try to drop it.

But for such case,  not only it's not documented, but also users have no way to call first_value(value, order) unless user modify Flink's source code for the function FIRST_VALUE actually only accpet 1 arguments currently.

The reason why I want to drop it is when I try to implement fist_value(value, [repect_null]) FLINK-26764, a sql startard for first_value function,  I find the current implemtation is more like fist_value(value, order).  Also, I found the order parameter actually isn't exposed to user, and I think it shouldn't exposed to user as it's not sql startard.

So, I want to drop it, make codebase clean and then move to implement fist_value(value, [repect_null]).

As you have said you used it, maybe we need still keep it.

But from myside, as it haven't exposed to user, and  won't be exposed to user in the future, I still think we should drop it.

 

 ;;;","18/Jun/22 03:56;libenchao;{quote}But for such case,  not only it's not documented, but also users have no way to call first_value(value, order) unless user modify Flink's source code for the function FIRST_VALUE actually only accpet 1 arguments currently.
{quote}
Yes, we modified the {{{}SqlFirstLastValueAggFunction{}}}'s definition to enable it. And I know there is no guarantee for non-documented or experimental features.

{{FIRST_VALUE}} and {{LAST_VALUE}} should be an {{OVER}} aggregate function instead of a general aggregate function actually. But in Flink, they are used as general aggregate function, that's why the {{ORDER}} parameter is important.
In {{OVER}} aggregate, rows are sorted, and the result for {{FIRST_VALUE}} and {{LAST_VALUE}} is deterministic. However, in general aggregation, there is no order guarantees for input rows, hence {{FIRST_VALUE}} and {{LAST_VALUE}} without {{ORDER}} will make the result non-deterministic.

In Calcite, the {{FIRST_VALUE}} and {{LAST_VALUE}} is implemented as {{OVER}} aggregate function. In Flink, we redefined it as a general aggregate function.
I'm not opposing to remove the unexposed {{ORDER}} parameter. But, we may need to think about the usage of {{LAST_VALUE}} and {{FIRST_VALUE}} in general aggregation cases.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST API does not support X-HTTP-Method-Override,FLINK-28103,13450536,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ldwnt,ldwnt,17/Jun/22 06:05,11/Jul/22 12:33,04/Jun/24 20:42,,1.13.5,,,,,,,,Runtime / REST,,,,,,,0,,,,,"The job is still running:

!image-2022-06-17-14-04-44-307.png!

 

but the cancelling api returns 404:

!image-2022-06-17-14-05-36-264.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/22 06:04;ldwnt;image-2022-06-17-14-04-44-307.png;https://issues.apache.org/jira/secure/attachment/13045194/image-2022-06-17-14-04-44-307.png","17/Jun/22 06:05;ldwnt;image-2022-06-17-14-05-36-264.png;https://issues.apache.org/jira/secure/attachment/13045193/image-2022-06-17-14-05-36-264.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jun 18 06:27:57 UTC 2022,,,,,,,,,,"0|z13bdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:58;martijnvisser;[~ldwnt] Thanks, can you verify this with the latest versions of Flink since 1.13 is not supported by the community anymore? ;;;","17/Jun/22 09:47;chesnay;Why are you doing a POST request with X-HTTP-Method-Override instead of a plain PATCH?;;;","18/Jun/22 06:22;ldwnt;[~martijnvisser] I'll try that next week;;;","18/Jun/22 06:27;ldwnt;[~chesnay] PATCH is supported neither in Java (HttpURLConnection) nor in http tools such as Insomnia. The header X-HTTP-Method-Override is a workaround but seems not working with the flink rest service.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink AkkaRpcSystemLoader fails when temporary directory is a symlink,FLINK-28102,13450529,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,prabhujoseph,prabhujoseph,17/Jun/22 04:37,22/Nov/22 09:44,04/Jun/24 20:42,31/Oct/22 01:34,1.15.2,1.16.0,,,,1.16.1,1.17.0,,Runtime / RPC,,,,,,,0,pull-request-available,,,,"Flink AkkaRpcSystemLoader fails when temporary directory is a symlink

*Error Message:*
{code}
Caused by: java.nio.file.FileAlreadyExistsException: /tmp
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88) ~[?:1.8.0_332]
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.8.0_332]
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.8.0_332]
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) ~[?:1.8.0_332]
        at java.nio.file.Files.createDirectory(Files.java:674) ~[?:1.8.0_332]
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) ~[?:1.8.0_332]
        at java.nio.file.Files.createDirectories(Files.java:727) ~[?:1.8.0_332]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcSystemLoader.loadRpcSystem(AkkaRpcSystemLoader.java:58) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.rpc.RpcSystem.load(RpcSystem.java:101) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.startTaskManagerRunnerServices(TaskManagerRunner.java:186) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.start(TaskManagerRunner.java:288) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:481) ~[flink-dist-1.15.0.jar:1.15.0]
{code}


*Repro:*

{code}
1. /tmp is a symlink points to actual directory /mnt/tmp

[root@prabhuHost log]# ls -lrt /tmp
lrwxrwxrwx 1 root root 8 Jun 15 07:51 /tmp -> /mnt/tmp

2. Start Cluster
./bin/start-cluster.sh

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29698,,,,,,,,,,,,,FLINK-29728,FLINK-30139,,FLINK-30143,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Oct 31 01:34:47 UTC 2022,,,,,,,,,,"0|z13bbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 07:19;Weijie Guo;You can set io.tmp.dirs yourself using -D

createDirectories will throw FileAlreadyExistsException if dir exists but is not a directory, such as symlink.;;;","17/Jun/22 08:07;prabhujoseph;Yes setting io.tmp.dirs to the actual directory pointed by symlink worked. Shall we improve the logic to handle the symlink which points to Actual Directory case as well.;;;","17/Jun/22 08:58;Weijie Guo; We can handle symlinks correctly before FLINK-23500, but now it's broken, from my personal point of view, we should allow symlinks as before, what do you think [~chesnay] [~prabhujoseph] ,I can try to fix this if you guys think so too.;;;","20/Oct/22 07:21;TsReaper;Mistakenly closed.;;;","20/Oct/22 07:48;Weijie Guo;_[~xtsong]_ I will fix this, could you help assign this ticket to me? Thanks~;;;","31/Oct/22 01:34;xtsong;- master (1.17): 2859196f9ab1d86a3d90e47a89cbd13be74741b9
- release-1.16: 3ae578e2233abd42f770d1bf395792c85698fd89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNHighAvailabilityITCase failed on azure,FLINK-28101,13450518,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,lsy,lsy,17/Jun/22 02:13,18/Jun/22 12:46,04/Jun/24 20:42,17/Jun/22 02:25,1.16.0,,,,,,,,Deployment / YARN,,,,,,,0,,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36789&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461]

 
{code:java}
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:98)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
	Suppressed: java.nio.file.NoSuchFileException: /tmp/junit9212264733683694168/.flink/application_1655366536423_0003
		at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
		at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
		at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
		at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
		at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
		at java.nio.file.Files.readAttributes(Files.java:1737)
		at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
		at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
		at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372)
		at java.nio.file.Files.walkFileTree(Files.java:2706)
		at java.nio.file.Files.walkFileTree(Files.java:2742)
		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.deleteAllFilesAndDirectories(TempDirectory.java:199)
		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:186)
		... 38 more
		Suppressed: java.nio.file.NoSuchFileException: /tmp/junit9212264733683694168/.flink/application_1655366536423_0003
			at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
			at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
			at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
			at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
			at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
			at java.nio.file.Files.delete(Files.java:1126)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.resetPermissionsAndTryToDeleteAgain(TempDirectory.java:250)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:212)
			at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.visitFileFailed(TempDirectory.java:199)
			at java.nio.file.Files.walkFileTree(Files.java:2672)
			... 41 more
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27667,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sat Jun 18 12:46:28 UTC 2022,,,,,,,,,,"0|z13b9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/22 12:46;jingge;closed because of duplication, please refer to FLINK-27667 for more information.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rocketmq-flink checkpoint,FLINK-28100,13450517,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,SOD_DOB,SOD_DOB,17/Jun/22 01:56,27/Jun/22 07:47,04/Jun/24 20:42,27/Jun/22 07:47,1.13.2,,,,,,,,Runtime / State Backends,,,,,,,0,checkpoint,RocketMQ,,,"When I using [ROCKETMQ-FLINK|https://github.com/apache/rocketmq-flink], but I don't know  How do I set up stateBackend and save checkpoints in HDFS?

Is that not supported?

can you help me?","flink version: flink-1.13.2

rocketmq version: 4.2.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,scala,,Mon Jun 27 07:47:27 UTC 2022,,,,,,,,,,"0|z13b94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 02:12;nicholasjiang;[~SOD_DOB], I'm the RocketMQ connector owner. What's the problem did you occur for checkpoint? ;;;","17/Jun/22 02:21;SOD_DOB;[~nicholasjiang] hi ~

Recently I am using RocketMQ-Flink, but I do not know how to set up SateBackend in Flink to save rocketMQ consumer consumption information.;;;","27/Jun/22 02:41;Yanfei Lei;hi [~SOD_DOB], you can set up SateBackend  by 
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStateBackend(new HashMapStateBackend()); {code}
in your code, or set up it in flink-conf.yaml. For details, please see [1]. 

And it is better to ask on the mailing list or Slack, more people will see and offer help.

 

[1]https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/#configuring-a-state-backend

 ;;;","27/Jun/22 07:47;martijnvisser;As [~Yanfei Lei] mentioned please ask these type of questions on the User mailing list, Slack or Stackoverflow. See https://flink.apache.org/gettinghelp.html for all details;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSink timeout exception,FLINK-28099,13450434,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,Krjk,Krjk,16/Jun/22 14:36,16/Jun/22 21:31,04/Jun/24 20:42,16/Jun/22 21:31,,,,,,,,,,,,,,,,0,,,,,"Hello!
I'm trying to replace FlinkKafkaProducer with KafkaSink, but jobs fail with this exception:
Caused by: org.apache.kafka.common.errors.TimeoutException: Expiring 10 record(s) for enriched_flow-21:120015 ms has passed since batch creation
so I have two questions:
1) Why jobs worked fine with FlinkKafkaProducer and fail with KafkaSink with the same configuration?
2) How to make that flink jobs won't fail with that exception? We use DeliveryGuarantee.NONE because we don't have checkpoints yet, so its ok if there will be some data loss
Thanks in advance!",Flink 1.14.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,,,Thu Jun 16 21:31:59 UTC 2022,,,,,,,,,,"0|z13aqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 21:31;martijnvisser;[~Krjk] Thanks for opening a ticket. The error is caused because there are records that are put faster in the queue then they can be sent from the client. You can find a detailed explanation and solution in https://stackoverflow.com/questions/56807188/how-to-fix-kafka-common-errors-timeoutexception-expiring-1-records-xxx-ms-has

The implementation details of the KafkaSink differs quite a lot from the FlinkKafkaProducer. 

For future questions, please check https://flink.apache.org/community.html on how to get help. Jira tickets are meant for bugs or new features, not for support questions. Those are better suited for the User mailing lists, Slack or Stackoverflow. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor table store compactor,FLINK-28098,13450409,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,16/Jun/22 11:46,17/Jun/22 06:36,04/Jun/24 20:42,17/Jun/22 06:36,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"There is currently some room for code optimization on the path of the compact.
 * It uses the CompactManager, which actually only uses the CompactTask.
 * It also doesn't use an asynchronous thread to complete the compaction.
 * There should be FileStoreTable to provide unified table-level compaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 17 06:36:30 UTC 2022,,,,,,,,,,"0|z13alc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 06:36;lzljs3620320;master: c44ed5bb50f06eb94f18e8ae5d8c09ee2b909d10;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove bundling of formats-common in json/csv format,FLINK-28097,13450404,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,16/Jun/22 11:25,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,,1.20.0,,,Build System,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,,,,,"For some reason the normal json/csv format jars bundle flink-formats-common.
This seems unnecessary; this bundling can either be relegated to the sql-jar (Which is already done!) or the user-jar packaging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-16 11:25:58.0,,,,,,,,,,"0|z13ak8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect support set variable,FLINK-28096,13450400,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,16/Jun/22 10:29,01/Aug/22 06:13,04/Jun/24 20:42,01/Aug/22 06:13,,,,,,1.16.0,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Aug 01 06:13:06 UTC 2022,,,,,,,,,,"0|z13ajc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 06:13;jark;Fixed in master: a6a7063cfafd8cd5c1b04169839b92373bf96fde;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace IOUtils dependency on oss filesystem,FLINK-28095,13450399,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Jun/22 10:16,17/Jun/22 07:16,04/Jun/24 20:42,17/Jun/22 07:16,,,,,,1.16.0,,,FileSystems,,,,,,,0,pull-request-available,,,,"The oss fs has an undeclared dependency on commons-io for a single call to IOUtils.
We can make our lives a little bit easier by using the Flink IOUtils instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 17 07:16:44 UTC 2022,,,,,,,,,,"0|z13aj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 07:16;chesnay;master: 9aaf09c3db753ef805e3c7e3889a1f919d6362a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade AWS SDK to support ap-southeast-3 ,FLINK-28094,13450398,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,16/Jun/22 10:16,20/Sep/22 17:13,04/Jun/24 20:42,16/Aug/22 07:45,,,,,,1.14.6,1.15.2,1.16.0,Connectors / Kinesis,,,,,,,0,pull-request-available,,,,The AWS base module pulls AWS SDK v2.17.52 which does not support {{ap-southeast-3}}. Update to the latest version. Ensure to cover connectors (KDS/KDF/DDB) and formats (avro-glue-schema-registry and json-glue-schema-registry),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26699,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 11 09:40:55 UTC 2022,,,,,,,,,,"0|z13aiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 13:30;martijnvisser;[~dannycranmer] Should this ticket also cover the upgrade of the SDK for {{avro-glue-schema-registry}} and {{avro-glue-schema-registry}} ? I don't think we should let the different AWS SDK diverge in the codebase? ;;;","16/Jun/22 14:39;dannycranmer;[~martijnvisser] yes will do;;;","09/Aug/22 07:07;dannycranmer;Merged commit [{{28e9526}}|https://github.com/apache/flink/commit/28e952631bbb1035231fa83b3c7bd8e60538d1df] and [{{d2704b1}}|https://github.com/apache/flink/commit/d2704b111a2160bee530673f1032fd1dcb7ebc67] into apache:master;;;","10/Aug/22 08:34;dannycranmer;Merged commit [{{c124872}}|https://github.com/apache/flink/commit/c124872de681ef3f0ea5e7431bb4d33368228f86] into apache:release-1.15 ;;;","11/Aug/22 09:40;dannycranmer;Merged commit [{{0e19d8e}}|https://github.com/apache/flink/commit/0e19d8eded511558d7f7653dc371cbea6e9472b0] into apache:release-1.14 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Kafka Source Can Not Support Recovery On Checkpoint,FLINK-28093,13450388,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiangfei Liu,Jiangfei Liu,16/Jun/22 09:29,16/Jun/22 09:30,04/Jun/24 20:42,,1.13.5,1.14.3,1.14.4,,,,,,Connectors / Kafka,Table SQL / API,,,,,,0,,,,,"1.Flink SQL Kafka Source Consumer Topic Msg，And Do Checkpoint

2.Stop Flink Task

3.Write Data to Topic

4.Recovery On Checkpoint

5.Flink Job Can Not Recovery On Checkpoint And Dont Continued consumption On Checkpoint State",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/22 09:28;Jiangfei Liu;s.png;https://issues.apache.org/jira/secure/attachment/13045157/s.png","16/Jun/22 09:30;Jiangfei Liu;是.png;https://issues.apache.org/jira/secure/attachment/13045158/%E6%98%AF.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-16 09:29:16.0,,,,,,,,,,"0|z13ago:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support ASCII and CHR built-in function in the Table API,FLINK-28092,13450385,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,16/Jun/22 09:25,29/Jun/22 01:45,04/Jun/24 20:42,29/Jun/22 01:44,,,,,,1.16.0,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,FLINK-9970,,,FLINK-9970,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 29 01:44:54 UTC 2022,,,,,,,,,,"0|z13ag0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 09:28;Sergey Nuyanzin;isn't it a duplicate for https://issues.apache.org/jira/browse/FLINK-9970 ?;;;","16/Jun/22 09:38;ana4;[~Sergey Nuyanzin] Thanks a lot. You are right.;;;","29/Jun/22 01:44;dianfu;Merged to master via e7e120acf9258ed8ef3c416fed6ed59cc0a06855;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread leak in CheckpointResourcesCleanupRunnerTest#testCancellationAfterStart,FLINK-28091,13450378,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,chesnay,chesnay,16/Jun/22 08:59,16/Jun/22 08:59,04/Jun/24 20:42,,1.16.0,,,,,,,,Runtime / Coordination,Tests,,,,,,0,,,,,"This test uses the HaltingCheckpointRecoveryFactory without every triggering the latch, causing one thread to get stuck.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-16 08:59:18.0,,,,,,,,,,"0|z13aeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support attachAsDatastream in Python Table API,FLINK-28090,13450370,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,billrao,billrao,billrao,16/Jun/22 08:09,25/Jul/22 08:25,04/Jun/24 20:42,25/Jul/22 08:25,,,,,,1.16.0,,,API / Python,,,,,,,0,pull-request-available,,,,Implement attachAsDatastream. A pull request is submitted as this issue is created. ,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 25 08:25:33 UTC 2022,,,,,,,,,,"0|z13aco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 08:25;dianfu;Merged to master via 1e7732f18a8c8d8691aef562a0e20bcc26afc134;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hive dialect support ""tablesample (xx rows)""",FLINK-28089,13450364,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,16/Jun/22 07:28,20/Jul/22 09:25,04/Jun/24 20:42,20/Jul/22 09:25,,,,,,1.16.0,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jul 20 09:25:44 UTC 2022,,,,,,,,,,"0|z13abc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 09:25;jark;Fixed in master: 97736fa1bab0a27b36ab25b5e38fbfe2ced5a550;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adjust table store document to catalog,FLINK-28088,13450354,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,16/Jun/22 06:30,17/Jun/22 03:01,04/Jun/24 20:42,16/Jun/22 09:40,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"After quite a bit of development, we needed to adjust the documentation, use the latest model, and tweak some details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 16 09:40:24 UTC 2022,,,,,,,,,,"0|z13a94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 09:40;lzljs3620320;master: 50e5093a23ff88f3d0741b8c423c040c0f2d9956;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add validation for the name of FlinkDeployment CR,FLINK-28087,13450342,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin_123,kevin_123,kevin_123,16/Jun/22 03:33,16/Jun/22 15:15,04/Jun/24 20:42,16/Jun/22 14:01,,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"The name of FlinkDeployment CR must be match the regex:
{code:java}
'[a-z]([-a-z0-9]{0,43}[a-z0-9])?'{code}
By RRC-1035, a DNS-1035 label must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name', or 'abc-123', regex used for validation is '[a-z|https://github.com/apache/flink-kubernetes-operator/pull/%5B-a-z0-9%5D*%5Ba-z0-9%5D]?').

The rest service name of Flink Cluster uses the meta.name of FlinkDeployment CR, So the value of meta.name must follow the convention of DNS-1035 label.

The length of the value of name must be no more than 45 characters. See [Limit the value of kubernetes.cluster-id to have no more than 45 characters |https://github.com/apache/flink/pull/11708]

To avoid operator repeatedly creating and destroying flink clusters due to invalid service name, So I suggest add validation for the name of FlinkDeployment CR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/22 03:38;kevin_123;operator-flink.jpg;https://issues.apache.org/jira/secure/attachment/13045150/operator-flink.jpg",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 16 14:01:25 UTC 2022,,,,,,,,,,"0|z13a6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 06:21;kevin_123;Please assign this issue to me. [~gyfora] ;;;","16/Jun/22 14:01;gyfora;merged to main 6472dcea93945760722ce5abbeaae0a63f87840a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Catalog supports partition methods,FLINK-28086,13450336,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,lzljs3620320,lzljs3620320,16/Jun/22 02:47,29/Mar/23 03:08,04/Jun/24 20:42,29/Mar/23 03:08,,,,,,table-store-0.4.0,,,Table Store,,,,,,,0,,,,,"Table Store Catalog can support:
 * listPartitions
 * listPartitionsByFilter
 * getPartition
 * partitionExists
 * dropPartition",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 25 02:38:07 UTC 2022,,,,,,,,,,"0|z13a54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 06:28;liliwei;may i have the ticket?;;;","22/Jul/22 08:34;nicholasjiang;[~lzljs3620320], could you please assign this ticket to me? I'm working for supporting partition methods.;;;","25/Jul/22 02:38;lzljs3620320;[~liliwei] Sorry for the late reply, If you are still interested in this, you can review the pr.
[~nicholasjiang] Assigned to u;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close all the pending Pulsar transactions when flink shutdown the pipeline.,FLINK-28085,13450311,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Jun/22 21:59,18/Oct/22 12:23,04/Jun/24 20:42,18/Oct/22 12:23,1.14.4,1.15.0,,,,1.17.0,,,Connectors / Pulsar,,,,,,,1,pull-request-available,stale-assigned,,,Currently transactionId is not persisted. After a job restart we lose handle to the transaction which is still not aborted in Pulsar broker. Pulsar broker will abort these hanging transactions after a timeout but this is not desirable. We need to close all the pending transactionId.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Oct 18 12:23:37 UTC 2022,,,,,,,,,,"0|z139zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","18/Oct/22 12:23;tison;master via 713b0b170bf3d8d13b1663e73c1e9d6f100da731;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar unordered reader should disable retry and delete reconsume logic.,FLINK-28084,13450307,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Jun/22 21:38,16/Sep/22 01:37,04/Jun/24 20:42,16/Sep/22 01:37,1.14.4,1.15.0,,,,1.14.6,1.15.3,1.16.0,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,"UnroderdPulsarSourceReader currently calls reconsume, but this feature relys on retry topic. But if retry topic is enabled the initial search will only support earliest and lates (because it will be a multiconsumer impl). We plan to delete the reconsume logic to get rid of dependency on retry topic and should disable retry.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Sep 16 01:37:24 UTC 2022,,,,,,,,,,"0|z139yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 01:37;tison;https://github.com/apache/flink/pull/20725;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSource cannot work with object-reusing DeserializationSchema.,FLINK-28083,13450295,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Jun/22 18:54,08/Nov/22 06:53,04/Jun/24 20:42,08/Nov/22 06:53,1.14.4,1.15.0,,,,1.17.0,,,Connectors / Pulsar,,,,,,,0,pull-request-available,stale-assigned,,,This issue is the same as Kafka's https://issues.apache.org/jira/browse/FLINK-25132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25132,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Nov 08 06:53:36 UTC 2022,,,,,,,,,,"0|z139w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","08/Nov/22 06:53;tison;master via 27d42b2e599d4fafc45698711167810407ea0fa2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support end to end encryption on Pulsar connector.,FLINK-28082,13450276,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Jun/22 17:13,10/Jan/23 04:00,04/Jun/24 20:42,10/Jan/23 04:00,1.16.0,,,,,pulsar-4.0.0,,,Connectors / Pulsar,,,,,,,0,pull-request-available,stale-assigned,,,"Add this Pulsar encryption support:

https://pulsar.apache.org/docs/security-encryption/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jan 10 04:00:04 UTC 2023,,,,,,,,,,"0|z139rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","10/Jan/23 04:00;tison;master via https://github.com/apache/flink-connector-pulsar/pull/13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated Hadoop specific Flink configuration options,FLINK-28081,13450263,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,15/Jun/22 15:42,15/Dec/22 10:52,04/Jun/24 20:42,15/Dec/22 10:52,1.16.0,,,,,,,,Connectors / Hadoop Compatibility,,,,,,,0,pull-request-available,stale-assigned,,,"FLINK-7967 deprecated the following Flink configuration options in 1.5:
{code:java}
fs.hdfs.hdfsdefault
fs.hdfs.hdfssite
fs.hdfs.hadoopconf
{code}
These are deprecated for 10 minor releases so I think it's time to remove them.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Dec 15 10:52:38 UTC 2022,,,,,,,,,,"0|z139ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","07/Nov/22 15:32;gaborgsomogyi;I've extracted it from the delegation token umbrella jira because this can be done only in 2.x and not hard requirement.;;;","15/Dec/22 10:52;gaborgsomogyi;I think this area needs a full refactor so closing it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce MutableURLClassLoader as parent class of FlinkUserClassLoader and SafetyNetWrapperClassLoader,FLINK-28080,13450254,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,15/Jun/22 15:26,30/Jun/22 07:29,04/Jun/24 20:42,30/Jun/22 07:29,1.16.0,,,,,1.16.0,,,Runtime / Task,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 07:29:39 UTC 2022,,,,,,,,,,"0|z139mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 07:29;martijnvisser;Fixed in master: 0e95f5a3f63fec54682b9ad6471ecbc7376837a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check Hive DDL against table store schema when creating table,FLINK-28079,13450194,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,15/Jun/22 09:58,16/Jun/22 11:52,04/Jun/24 20:42,16/Jun/22 11:52,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"As table store schema is supported, we should use this schema as the ground truth. Hive DDL should only be used for checking.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 16 11:52:09 UTC 2022,,,,,,,,,,"0|z1399k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 11:52;lzljs3620320;master: a3d92b20f21d265109b667291ffd62177d5e0a78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers runs into timeout,FLINK-28078,13450185,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,15/Jun/22 09:16,09/Mar/23 12:08,04/Jun/24 20:42,09/Mar/23 12:08,1.15.2,1.16.0,,,,1.15.3,1.16.0,,Runtime / Coordination,,,,,,,0,pull-request-available,stale-assigned,test-stability,,"[Build #36189|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36189&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10455] got stuck in {{ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers}}
{code}
""ForkJoinPool-45-worker-25"" #525 daemon prio=5 os_prio=0 tid=0x00007fc74d9e3800 nid=0x62c8 waiting on condition [0x00007fc6ff2f2000]
May 30 16:36:10    java.lang.Thread.State: WAITING (parking)
May 30 16:36:10 	at sun.misc.Unsafe.park(Native Method)
May 30 16:36:10 	- parking to wait for  <0x00000000c2571b80> (a java.util.concurrent.CompletableFuture$Signaller)
May 30 16:36:10 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
May 30 16:36:10 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
May 30 16:36:10 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
May 30 16:36:10 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
May 30 16:36:10 	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
May 30 16:36:10 	at org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers(ZooKeeperMultipleComponentLeaderElectionDriverTest.java:256)
May 30 16:36:10 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 30 16:36:10 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 30 16:36:10 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 30 16:36:10 	at java.lang.reflect.Method.invoke(Method.java:498)
[...]
{code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27880,FLINK-29173,,,,,,,,,,,,,,,,,,,,FLINK-30484,FLINK-31379,,,,,CURATOR-645,"31/Aug/22 08:32;mapohl;FLINK-28078-build-40525-20220830.14.tar.gz;https://issues.apache.org/jira/secure/attachment/13048798/FLINK-28078-build-40525-20220830.14.tar.gz",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Mar 09 12:08:12 UTC 2023,,,,,,,,,,"0|z1397k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 08:42;chesnay;We seem to be entering a strange loop. The ZK log show the following over and over again; the final number inf the getData calls is incremented on each loop.

{code}
FinalRequestProcessor - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_ddb7327c-1775-4084-9fbd-40e263758697-latch-0000000000
FinalRequestProcessor - sessionid:0x100cf6d9cf60000 type:getData cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_ddb7327c-1775-4084-9fbd-40e263758697-latch-0000000000
FinalRequestProcessor - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
FinalRequestProcessor - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
FinalRequestProcessor - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
FinalRequestProcessor - sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
FinalRequestProcessor - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
FinalRequestProcessor - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
{code};;;","21/Jun/22 12:05;mapohl;{code}
16:17:07,802 [ForkJoinPool-45-worker-25] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Starting
16:17:07,804 [ForkJoinPool-45-worker-25] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Default schema
16:17:07,814 [ForkJoinPool-45-worker-25-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: CONNECTED
16:17:07,817 [ForkJoinPool-45-worker-25-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker [] - New config event received: {}
16:17:07,824 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
16:17:07,824 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
16:17:07,826 [ForkJoinPool-45-worker-25-EventThread] INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker [] - New config event received: {}
16:17:07,848 [ForkJoinPool-45-worker-25-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
16:17:07,860 [ForkJoinPool-45-worker-25] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
{code}

The test itself usually creates three {{ElectionDriver}} instances and removes them one by one through a for loop. The logs of the failed test reveal that only two out of the three have the quorum connection established (i.e. the log message {{Connected to ZooKeeper quorum. Leader election can start.}} is printed). The first iteration picks the first instance, checks its leadership and closes it. 

The {{anyOf}} call in the next iteration should actually still succeed because there's one {{ElectionDriver}} that has an established connection. But the resulting {{anyOf}} composite future doesn't complete, i.e. non of the left Leadership futures completes resulting in the test getting stuck in the subsequent {{join}} call.;;;","21/Jun/22 15:02;mapohl;It's not clear to me, yet, why we're not seeing the second ElectionDriver taking over the leadership. I'd expect to get through the second iteration as well because the leadership should be obtainable for the second ElectionDriver as well. ;;;","24/Jun/22 06:37;mapohl;I did a comparison of a successful run and the failed run. I'm getting the feeling that we're running into some race condition on the {{LeaderLatch}} implementation side.

Here are the logs of a successful run starting from the log message where the latch zNode is checked for existence after being created:
{code:java}
1376 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x1b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1376 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x1b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1377 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:createContainer cxid:0x1c zxid:0xa txntype:19 reqpath:n/a
1378 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:createContainer cxid:0x1c zxid:0xa txntype:19 reqpath:n/a
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1f zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1380 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x1f zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1381 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1381 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1385 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:create2 cxid:0x21 zxid:0xb txntype:15 reqpath:n/a
1385 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:create2 cxid:0x21 zxid:0xb txntype:15 reqpath:n/a
1385 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x22 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink
1385 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x22 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink
1386 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x23 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1386 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x23 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1387 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1387 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1387 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x25 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1387 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x25 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1388 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren cxid:0x26 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1388 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren cxid:0x26 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1390 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:create2 cxid:0x27 zxid:0xc txntype:15 reqpath:n/a
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:create2 cxid:0x27 zxid:0xc txntype:15 reqpath:n/a
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x28 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x28 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x29 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1391 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x29 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1392 [ForkJoinPool-1-worker-9-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
1392 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x2a zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1392 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x2a zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1393 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:exists cxid:0x2b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1393 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:exists cxid:0x2b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1393 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren cxid:0x2c zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1393 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren cxid:0x2c zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1394 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:create2 cxid:0x2d zxid:0xd txntype:15 reqpath:n/a
1395 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:create2 cxid:0x2d zxid:0xd txntype:15 reqpath:n/a
1421 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getData cxid:0x2e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_2b57761c-31e0-458d-9466-b409010d3d14-latch-0000000000
1421 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getData cxid:0x2e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_2b57761c-31e0-458d-9466-b409010d3d14-latch-0000000000
1421 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x2f zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1421 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x2f zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1422 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getData cxid:0x30 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_1e836e28-a9d1-4bea-97f3-222034044247-latch-0000000001
1422 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getData cxid:0x30 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_1e836e28-a9d1-4bea-97f3-222034044247-latch-0000000001
1497 [ForkJoinPool-1-worker-9] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
1501 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:checkWatches cxid:0x31 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1501 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:checkWatches cxid:0x31 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1505 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:delete cxid:0x32 zxid:0xe txntype:2 reqpath:n/a
1506 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:delete cxid:0x32 zxid:0xe txntype:2 reqpath:n/a
1506 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x33 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1506 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x33 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1507 [ForkJoinPool-1-worker-9-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
1507 [ForkJoinPool-1-worker-9] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
1508 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:checkWatches cxid:0x34 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1508 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:checkWatches cxid:0x34 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1508 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:delete cxid:0x35 zxid:0xf txntype:2 reqpath:n/a
1508 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:delete cxid:0x35 zxid:0xf txntype:2 reqpath:n/a
1509 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x36 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1509 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:getChildren2 cxid:0x36 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
1509 [ForkJoinPool-1-worker-9-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
1510 [ForkJoinPool-1-worker-9] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
1510 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:checkWatches cxid:0x37 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1510 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:checkWatches cxid:0x37 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
1510 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:delete cxid:0x38 zxid:0x10 txntype:2 reqpath:n/a
1511 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:delete cxid:0x38 zxid:0x10 txntype:2 reqpath:n/a
1511 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:checkWatches cxid:0x39 zxid:0xfffffffffffffffe txntype:unknown reqpath:/zookeeper/config
1511 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:checkWatches cxid:0x39 zxid:0xfffffffffffffffe txntype:unknown reqpath:/zookeeper/config
1513 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - Processing request:: sessionid:0x100009ef1b30000 type:closeSession cxid:0x3a zxid:0x11 txntype:-11 reqpath:n/a
1513 [SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor [] - sessionid:0x100009ef1b30000 type:closeSession cxid:0x3a zxid:0x11 txntype:-11 reqpath:n/a
{code}
and here are the merged (i.e. I integrated the test-sides logs into the zookeeper-server logs based on the time) logs:
{code:java}
16:17:07,824 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
16:17:07,824 [Curator-ConnectionStateManager-0] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Connected to ZooKeeper quorum. Leader election can start.
16:17:07,829 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:exists cxid:0x10 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,829 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:exists cxid:0x10 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,831 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x11 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,832 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x11 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,837 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x12 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,837 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x12 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,838 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:createContainer cxid:0x13 zxid:0x7 txntype:19 reqpath:n/a
16:17:07,838 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:createContainer cxid:0x13 zxid:0x7 txntype:19 reqpath:n/a
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x14 zxid:0x8 txntype:15 reqpath:n/a
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x14 zxid:0x8 txntype:15 reqpath:n/a
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x15 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x15 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x16 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,839 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x16 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x17 zxid:0x9 txntype:15 reqpath:n/a
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x17 zxid:0x9 txntype:15 reqpath:n/a
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren cxid:0x18 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren cxid:0x18 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x19 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x19 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1a zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1a zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,846 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1b zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,848 [ForkJoinPool-45-worker-25-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
16:17:07,854 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1c zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,854 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x1c zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,848 [ForkJoinPool-45-worker-25-EventThread] DEBUG org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - ZooKeeperMultipleComponentLeaderElectionDriver obtained the leadership.
16:17:07,860 [ForkJoinPool-45-worker-25] INFO  org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriver [] - Closing ZooKeeperMultipleComponentLeaderElectionDriver.
16:17:07,860 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x1d zxid:0xa txntype:15 reqpath:n/a
16:17:07,860 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x1d zxid:0xa txntype:15 reqpath:n/a
16:17:07,862 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:checkWatches cxid:0x1e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,862 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:checkWatches cxid:0x1e zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default
16:17:07,863 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x1f zxid:0xb txntype:2 reqpath:n/a
16:17:07,863 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x1f zxid:0xb txntype:2 reqpath:n/a
16:17:07,863 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_ddb7327c-1775-4084-9fbd-40e263758697-latch-0000000000
16:17:07,863 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x20 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_ddb7327c-1775-4084-9fbd-40e263758697-latch-0000000000
16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_6eb174e9-bb77-4a73-9604-531242c11c0e-latch-0000000001
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_6eb174e9-bb77-4a73-9604-531242c11c0e-latch-0000000001
16:17:07,871 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x25 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,871 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x25 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,875 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x26 zxid:0xe txntype:2 reqpath:n/a
16:17:07,875 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x26 zxid:0xe txntype:2 reqpath:n/a
16:17:07,876 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x27 zxid:0xf txntype:15 reqpath:n/a
16:17:07,876 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x27 zxid:0xf txntype:15 reqpath:n/a
16:17:07,876 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x28 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_8008ceb9-e0b9-459b-8fa5-44428fa31e29-latch-0000000002
16:17:07,876 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getData cxid:0x28 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_8008ceb9-e0b9-459b-8fa5-44428fa31e29-latch-0000000002
[...]
{code}
The LeaderLatch implementation works like that (based on the code):
 # {{LeaderLatch}} is started pointing to a given zNode path ({{{}/flink/default/latch/{}}} in our case).
 # The instance will get {{reset()}} initially setting the leadership for this latch to {{false}} and triggering a child creation
 # Child creation is done ""withProtection"" (i.e. a random String will be prefixed to the child) in mode {{EPHEMERAL_SEQUENTIAL}} (i.e. the child will be deleted on connection-loss and a monotonically increasing number is added as a suffix) with a callback that triggers {{getChildren}} in case of success and selecting the leader based on the suffix of the child nodes names (lower ID wins). Hence, an order between the children is established based on the ID.
 # {{getChildren}} callback triggers leadership detection in the local {{LeaderLatch}} instance.
 ## For the child with the lowest ID, the leadership is triggered in the callback
 ## If the current {{LeaderLatch}} doesn't correspond to the child with the lowest ID, a watcher is set up watching the direct predecessor (based on the ID-based ordering I mentioned before) for deletion. If the deletion happens, the watcher is triggered (only once, i.e. not repeatedly) and reinitiates the leadership detection through {{getChildren}} (4.) if the {{LeaderLatch}} isn't closed, yet. This is what we observe in the successful test run with the watcher being triggered. 

There is another code path if the predecessor node doesn't exist in {{getChildren}} (4.). In that case, an entire {{reset()}} (2.) is triggered resulting in the deletion of the current's {{LeaderLatch}}'s zNode and the recreation of the child zNode. It appears that we're ending up in a this code path for the unsuccessful test run.;;;","24/Jun/22 07:11;mapohl;The loop consists of the following logs:
{code}
16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch
16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a
16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_6eb174e9-bb77-4a73-9604-531242c11c0e-latch-0000000001
{code}
# The {{reset()}} triggers [getChildren|https://github.com/apache/curator/blob/d1a9234ecae47e3704037c839e6041931c24d1f4/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L629] through the [LeaderLatch#getChildren|https://github.com/apache/curator/blob/d1a9234ecae47e3704037c839e6041931c24d1f4/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L525] after a new child is created (I would assume {{create2}} entry in the logs before {{getChildren}} entry which is not the case; so, I might be wrong in my observation)
# The callback of {{getChildren}} triggers [checkLeadership|https://github.com/apache/curator/blob/d1a9234ecae47e3704037c839e6041931c24d1f4/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L625].
# In the meantime, the predecessor gets deleted (I'd assume because of the deterministic ordering of the events in ZK). This causes the [callback in checkLeadership|https://github.com/apache/curator/blob/d1a9234ecae47e3704037c839e6041931c24d1f4/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L607] to fail with a {{NONODE}} event and triggering the reset of the current {{LeaderLatch}} instance which again triggers the deletion of the current's {{LeaderLatch}}'s child zNode and which is executed on the server later on.;;;","24/Jun/22 07:12;mapohl;I found [CURATOR-3|https://issues.apache.org/jira/browse/CURATOR-3] that might be of relevance here. In the unsuccessful test run, there is a {{checkWatch}} event triggered after the first {{LeaderLatch}} is closed. But the ticket refers to a single {{LeaderLatch}} not getting the leadership anymore. So, our observed behavior with the loop is different.;;;","01/Jul/22 06:43;mapohl;I created CURATOR-645 to cover the issue on the curator side. I'll leave this issue open because it might mean that curator needs to be updated to cover a fix.;;;","31/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/Aug/22 08:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40027&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10214;;;","17/Aug/22 07:50;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","29/Aug/22 02:35;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40433&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53;;;","29/Aug/22 16:21;mapohl;CURATOR-645 is probably caused by some issue where we revoke the leadership in the test ""too fast"" which makes the curator code end up in a different code path that contains a bug. There's already a fix for that in [a PR in the CURATOR project|https://github.com/apache/curator/pull/430]. But I'm not sure how fast we're going to get this merged.

I started experimenting with some temporary (dirty) workaround for our test to make it less likely to fail. I'm suspecting that we only need to ""add some workload on the leader's side"" to throttle the leadership revocation. Still, this issue can happen in production as well. There's a race condition between the leader losing its leadership and candidates going through an re-evaluation of the leadership on their end ({{LeaderLatch#getChildren}} is called before the leader's znode is deleted but {{LeaderLatch#checkLeadership}} is called after the leader's znode is deleted). We can only overcome this by fixing CURATOR-645 and upgrading to the corresponding Apache Curator version.

I would still leave the Jira issue as {{Major}} because of nobody having it reported by now. The test case made the issue only visible because the race condition because more likely in the test with no actual workload being processed by the leader process.;;;","31/Aug/22 08:31;mapohl;Ok, I tried to reproduce the issue in [build 20220830.14|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40525&view=results] (I let each of the test jobs run the unit test repeatedly except for the `core` stage due to some bug in the if statement :facepalm:). I attached the results of the run for reproducability reasons to this issue (see  [^FLINK-28078-build-40525-20220830.14.tar.gz]).

In total there were 6178 test execution over all participating modules (there were failed test runs due to the jobs being cancelled after ~4hours).
{code}
for f in $(ls *zip); do job_name=""${f%"".zip""}""; unzip -p $f ${job_name}/mvn-1.log | grep -c ""Test org.apache.flink.runtime.leaderelection.ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers successfully run.""; done  | paste -sd+ | bc
{code}

I couldn't find any evidence that any of the test runs ran into the issue by checking the size of the {{zookeeper-server-1.log}} for each of the jobs (we should have observed a peak in file size due to the infinite loop on the ZK side). All logs have similar sizes:
{code}
for f in $(ls *zip); do job_name=""${f%"".zip""}""; echo $job_name; unzip -p $f ${job_name}/zookeeper-server-1.log | wc --bytes | numfmt --to iec --format ""%8.4f""; done
logs-ci-test_ci_connect_1-1661858590
41,0784M
logs-ci-test_ci_connect_2-1661858534
43,2716M
logs-ci-test_ci_core-1661858500
caution: filename not matched:  logs-ci-test_ci_core-1661858500/zookeeper-server-1.log
  0,0000
logs-ci-test_ci_finegrained_resource_management-1661858511
42,9698M
logs-ci-test_ci_misc-1661858514
39,8520M
logs-ci-test_ci_python-1661858538
41,2648M
logs-ci-test_ci_table-1661858510
43,8197M
logs-ci-test_ci_tests-1661858630
40,8045M
{code}

I will proceed with implementing the (dirty temporary) workaround in the PR. Let's see whether that reduces the likelihood for this test failing again.;;;","01/Sep/22 12:41;mapohl;master: 655184cdb086ac2adec3e743701868f1a55b6129
1.15: ed8700d03cccc47bfa39da5e1e6611eb9be7d5a1;;;","01/Sep/22 14:25;mapohl;Created FLINK-29173 as a follow-up to revert the changes from this issue and upgrading curator.;;;","06/Mar/23 14:44;mapohl;Looks like this issue can still appear:
1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46843&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","06/Mar/23 14:46;mapohl;Another workaround to cover the issue which is closer to reality is that we use separate client for each LeaderElectionService. This will avoid using the same event queue and breaks the strict orderness of events between different LeaderElectionService instances.;;;","09/Mar/23 11:13;dannycranmer;[~mapohl] since this is already merged into a fix version I suggest we create a follow up Jira and link them. Having a single Jira with fixVersion 1.5.3 and 1.15.5 does not make sense.;;;","09/Mar/23 12:08;mapohl;Fair point. Thanks for bringing that up, [~dannycranmer]. I created a follow-up FLINK-31379 to cover the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tasks get stuck during cancellation in ChannelStateWriteRequestExecutorImpl,FLINK-28077,13450184,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,mapohl,mapohl,15/Jun/22 09:14,23/Jun/22 12:40,04/Jun/24 20:42,21/Jun/22 12:24,1.16.0,,,,,1.16.0,,,Runtime / Checkpointing,Tests,,,,,,0,pull-request-available,test-stability,,,"[Build #36209|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36209&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9370] got stuck in {{KeyedStateCheckpointingITCase.testWithMemoryBackendSync}}:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f849c00b800 nid=0x19c3 waiting on condition [0x00007f84a45b7000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080074870> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1989)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1951)
	at org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.testProgramWithBackend(KeyedStateCheckpointingITCase.java:175)
	at org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.testWithMemoryBackendSync(KeyedStateCheckpointingITCase.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27880,,,,,FLINK-28076,FLINK-28024,FLINK-27832,,,,,FLINK-27251,,,,,,,,,FLINK-27792,,,,,FLINK-27792,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 12:24:10 UTC 2022,,,,,,,,,,"0|z1397c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 12:18;chesnay;The TM is crashing because a task gets stuck during cancellation:
{code}
 java.lang.Object.wait(Native Method)
java.lang.Thread.join(Thread.java:1252)
java.lang.Thread.join(Thread.java:1326)
org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:166)
org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1220/1213892815.close(Unknown Source)
org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$1(Task.java:923)
org.apache.flink.runtime.taskmanager.Task$$Lambda$1886/761627220.run(Unknown Source)
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
java.lang.Thread.run(Thread.java:748)
{code};;;","15/Jun/22 12:21;chesnay;Interestingly enough after the TM is shutdown we can see the ChannelStateWriter for the blocked task making progress again and continues the shutdown.;;;","15/Jun/22 12:22;chesnay;This is also not specific to this test; also saw it on another one (but I lost the build).;;;","15/Jun/22 14:19;chesnay;I could reproduce this by adding a 200ms sleep before the dispatch in {{org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl#loop}}. The thread gets stuck during the cancellation in the discard action ChannelStateWriteRequest, waiting on the dataFuture:

{code}
CloseableIterator.fromList(dataFuture.get(), Buffer::recycleBuffer).close();
{code};;;","15/Jun/22 14:20;chesnay;May be caused by FLINK-27251.;;;","15/Jun/22 15:06;chesnay;/cc [~fanrui] [~pnowojski];;;","16/Jun/22 02:10;fanrui;Hi [~mapohl] [~chesnay] , thanks for this information, I will take a look this week.:);;;","21/Jun/22 12:24;chesnay;master: 0912765acc25f179169f8e371683acaf3e9133a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamFaultToleranceTestBase runs into timeout,FLINK-28076,13450183,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,15/Jun/22 09:12,20/Aug/23 10:35,04/Jun/24 20:42,,1.16.0,1.17.0,,,,,,,Runtime / Checkpointing,,,,,,,0,auto-deprioritized-critical,test-stability,,,"[Build #36259|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36259&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=12054] got stuck in a {{StreamFaultToleranceTestBase}} subclass:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007eff8c00b800 nid=0xf1a waiting on condition [0x00007eff92eca000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000082213498> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:92)
	at org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.runCheckpointedProgram(StreamFaultToleranceTestBase.java:136)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
[...]
{code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27880,,,,,,,,FLINK-28077,,,,,,,,,,FLINK-31138,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Aug 20 10:35:05 UTC 2023,,,,,,,,,,"0|z13974:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 09:29;chesnay;Same cause as FLINK-28077.;;;","27/Feb/23 10:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46560&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=14522;;;","27/Feb/23 10:32;mapohl;[~fanrui] can you have a look at this one? It feels like we're experiencing an issue which we don't fully understand considering that there's also FLINK-31138 which might be a duplicate.;;;","07/Mar/23 07:22;fanrui;Hi [~mapohl] , sorry for late response due to I'm busy recently. I found this JIRA is duplicate to FLINK-28077, and it has been fixed. 

 

And you added a new CI fails link[1] recently, I have analyzed it, and just found task is blocking in `SubtaskCheckpointCoordinatorImpl.lambda$waitForPendingCheckpoints`[2] when task is closing, however I didn't find why these is checkpoint doesn't fail.

I'm going on vacation next week, so I've been busy lately, I can continue analyze after vacation.

 
[1] [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46560&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=14522]
[2] [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46560&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10656];;;","09/Mar/23 07:59;mapohl;Not sure why I didn't reopen it last time. I'm doing it now.

[~fanrui] sure thing. It's not a blocker because it was already present in 1.16. And if it would be a blocker, we could find someone else to look into it in the mean time. So, go ahead and enjoy your vacation :-);;;","12/Jun/23 10:33;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49865&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=17896;;;","13/Jun/23 02:32;fanrui;{quote}[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49865&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=17896]
{quote}
Thanks [~Sergey Nuyanzin] for reporting this. This log failed to upload, not sure why it timed out.

 ;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get statistics for partitioned table even without partition pruning,FLINK-28075,13450180,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,15/Jun/22 09:00,14/Jul/22 13:21,04/Jun/24 20:42,14/Jul/22 13:21,,,,,,1.16.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,"Currently, the statistics for partitioned table will not be collected if there is no partition pruning",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 14 13:21:49 UTC 2022,,,,,,,,,,"0|z1396g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 13:21;godfreyhe;Fixed in master: ffb6b4349c8bdff66fe057a2c66859f871ead088;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
show statistics details for DESCRIBE EXTENDED,FLINK-28074,13450179,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,337361684@qq.com,godfreyhe,godfreyhe,15/Jun/22 08:57,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,,1.20.0,,,Table SQL / Planner,,,,,,,0,pull-request-available,stale-assigned,,,"Currently, DESCRIBE command only show the schema of a given table, EXTENDED does not work. so for EXTENDED mode, the statistics details can also be shown.",,,,,,,,,,FLINK-34375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 12 22:38:12 UTC 2022,,,,,,,,,,"0|z13968:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store supports Flink 1.14,FLINK-28073,13450148,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,15/Jun/22 06:43,04/Jul/22 04:02,04/Jun/24 20:42,04/Jul/22 04:02,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-15 06:43:28.0,,,,,,,,,,"0|z138zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set Hadoop FileSystem for Orc reader,FLINK-28072,13450143,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,15/Jun/22 06:13,19/Jul/22 09:50,04/Jun/24 20:42,19/Jul/22 09:50,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Now orc reader uses Hadoop FileSystem, but orc writer uses Flink FileSystem.

This can lead to some inconsistencies and possibly classloader loading issues.
We can unify this by setting the Hadoop FileSystem that we get from Flink for the orc reader.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28041,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 19 09:50:47 UTC 2022,,,,,,,,,,"0|z138y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 09:50;lzljs3620320;master: 6d04cd57e77aa1d735dac556c2f5abb2f58e1d8f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support missing built-in functions in Table API,FLINK-28071,13450140,13348116,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ana4,dianfu,dianfu,15/Jun/22 05:53,14/Mar/24 07:36,04/Jun/24 20:42,,,,,,,1.20.0,,,Table SQL / API,,,,,,,0,,,,,"There are many built-in functions are not supported. See https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/ for more details. There are two columns for each built-in function: *SQL Function* and *Table Function*, if a function is not supported in *Table API*, the *Table Function* column is documented as *N/A*. We need to evaluate each of these functions to ensure that they could be used in both SQL and Table API.",,,,,,,,,,,,,,,,FLINK-28369,FLINK-28288,FLINK-28298,FLINK-28439,FLINK-28015,FLINK-27159,FLINK-28508,FLINK-28509,FLINK-28354,FLINK-28092,FLINK-29352,FLINK-29353,FLINK-29354,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jun 17 07:28:43 UTC 2022,,,,,,,,,,"0|z138xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 06:29;ana4;I would like to take this. [~dianfu] ;;;","15/Jun/22 09:22;twalthr;[~ana4] please also take a look at FLINK-22857, FLINK-20522, and FLINK-6810 for reference implementations. The stack has been prepared for unified functions. It is now necessary to use the new stack for more functions.;;;","15/Jun/22 09:23;twalthr;Until this issue is fixed: A current work around to use SQL functions in Table API is to use the {{callSql(""FROM_UNIXTIME(...)"")}} function.;;;","17/Jun/22 02:26;ana4;[~twalthr]  Thanks a lot, I will take a look at these issues.;;;","17/Jun/22 07:28;ana4;[~twalthr] [~Sergey Nuyanzin] I only support existing built-in functions no more new functions in this issue. Should I not use the new stack, or in the future we create a new issue to refactor all built-in functions to the new stack?

Like this PR [https://github.com/apache/flink/pull/19988] . If I use the new stack, I will delete the `StringCallGen#generateAscii` and add an `AsciiFunction` like IfNullFunction in https://issues.apache.org/jira/browse/FLINK-20522

IMO, if we change `StringCallGen#generateAscii`, we can create a new issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Migrate ScalaAPICompletenessTestBase to Junit5,FLINK-28070,13450138,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,15/Jun/22 05:51,05/Sep/22 12:03,04/Jun/24 20:42,05/Sep/22 12:03,,,,,,,,,API / Scala,Tests,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Sep 05 12:03:46 UTC 2022,,,,,,,,,,"0|z138x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 12:03;mapohl;master: f3a00d85006885302a1048f9bcea259ef3fa2448;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot attach SSL JKS file for Kafka connector,FLINK-28069,13450131,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,suns,suns,15/Jun/22 04:43,15/Jun/22 07:21,04/Jun/24 20:42,15/Jun/22 06:53,1.14.4,,,,,,,,,,,,,,,0,,,,,"Hi, I intend to connect to a SSL enabled Kafka, which require to attach JKS file for truststore and keystore, then I am trying to use the keyword {*}-yt, yarnship{*}, to pass the JKS files, and it's expected to find the find under the classpath of job manager or task manager.
{code:java}
val truststore_path = ""client.truststore.jks""
val keystore_path = ""client.keystore.jks""
val source_ddl =
s""""""
CREATE TABLE source_table(
`id` BIGINT,
`time` TIMESTAMP(3)
) WITH (
'connector' = 'kafka',
...
'properties.security.protocol' = 'SSL',
'properties.ssl.truststore.location' = '$truststore_path',
'properties.ssl.truststore.password' = '$truststore_pwd',
'properties.ssl.keystore.location' = '$keystore_path',
'properties.ssl.keystore.password' = '$keystore_pwd',
'scan.startup.mode' = 'latest-offset',
'format' = 'json'
)
"""""" {code}
I am using the session mode to launch the job as below:
{code:java}
./bin/yarn-session.sh --detached
./bin/flink run ./job/finance-libra-stats-1.0.jar -yt ./client.keystore.jks ./client.truststore.jks{code}
However, the FileNotFound exception is given when the job initiates, saying that the JKS files is not found from job manager.
{code:java}
Caused by: java.nio.file.NoSuchFileException: client/client.keystore.jks    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)    at java.nio.file.Files.newByteChannel(Files.java:361)    at java.nio.file.Files.newByteChannel(Files.java:407)    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)    at java.nio.file.Files.newInputStream(Files.java:152)    at org.apache.kafka.common.security.ssl.SslEngineBuilder$SecurityStore.load(SslEngineBuilder.java:285)    ... 21 more {code}
Anyone can help tell what's the best practice for such case, and any mistake I meet during the process.","Flink on Yarn, session mode",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 15 07:21:15 UTC 2022,,,,,,,,,,"0|z138vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 06:53;martijnvisser;[~suns] Thanks for opening the ticket. These type of user questions are better suited for the Flink mailing list, the Flink Slack workspace or Stackoverflow. See https://flink.apache.org/community.html;;;","15/Jun/22 07:21;suns;[~martijnvisser] Thanks for the kind info.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Control the total memory of a sink task,FLINK-28068,13450125,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,15/Jun/22 03:29,27/Jun/22 05:30,04/Jun/24 20:42,27/Jun/22 05:30,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Currently, Sink's MemTable memory is self-managed Heap, and compaction also consumes memory, which is not very easy to use, and the memory problem will be more serious if there are multiple partitions and multiple Buckets in a single task.
We need to control the total memory of a single sink task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 27 05:30:46 UTC 2022,,,,,,,,,,"0|z138u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 05:30;lzljs3620320;master: 981b8e36984803eb269e32a5f3116da8ce4185dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a tablestore HiveCatalog,FLINK-28067,13450124,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,lzljs3620320,lzljs3620320,15/Jun/22 03:24,30/Jun/22 09:05,04/Jun/24 20:42,30/Jun/22 09:05,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Now we have FileSystemCatalog, we can introduce a tablestore HiveCatalog, in this way.

There are some benefits:
 # Tables created in this catalog can be read directly by the Hive engine without having to create external tables.
 # Schema synchronization, users rely on hive to manage tables
 # lock to Object Store writing, support multiple concurrent writes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 09:05:37 UTC 2022,,,,,,,,,,"0|z138u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 09:05;lzljs3620320;master: cb688c5bed4e02ceff608d85a6578a73f4fe2c1f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use FileSystem.createRecoverableWriter in FileStoreCommit,FLINK-28066,13450123,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,15/Jun/22 03:22,22/Jun/22 06:09,04/Jun/24 20:42,22/Jun/22 06:09,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"In FileStoreCommitImpl, currently, it uses `rename` to support atomic commit.

But this is not work for object store like S3. We can use RecoverableWriter to support atomic commit for object store.

We can introduce `AtomicFileCommitter`:
 * Use RecoverableWriter if FileSystem supports createRecoverableWriter
 * Use rename if createRecoverableWriter is not supported",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 22 06:09:29 UTC 2022,,,,,,,,,,"0|z138ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 06:09;lzljs3620320;master: f5dda43823476f42271fb393a8a11e4842562ec1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A never reached code in ProcessMemoryUtils Class,FLINK-28065,13450122,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,Tommmmmm,Tommmmmm,Tommmmmm,15/Jun/22 03:20,11/Jul/22 04:02,04/Jun/24 20:42,11/Jul/22 04:02,1.16.0,,,,,,,,Runtime / Configuration,,,,,,,0,pull-request-available,,,,"The main logic of *sanityCheckTotalProcessMemory* method in ProcessMemoryUtils Class will {color:#FF0000}never be executed{color}.

 

this is reason:

In *deriveJvmMetaspaceAndOverheadFromTotalFlinkMemory* method of ProcessMemoryUtils Class, the [logic|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/util/config/memory/ProcessMemoryUtils.java#L170] determines whether TotalProcessMemory is explicitly configured.
In ""false"" branch(means TotalProcessMemory is not explicitly configured), the [logic|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/util/config/memory/ProcessMemoryUtils.java#L184] calls the *sanityCheckTotalProcessMemory* method.
However, the main logic of *sanityCheckTotalProcessMemory* method only executed [when TotalProcessMemory is explicitly configured|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/util/config/memory/ProcessMemoryUtils.java#L247], so the main logic probably never be executed.

 

Compare with [call location|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/util/config/memory/taskmanager/TaskExecutorFlinkMemoryUtils.java#L101] of *sanityCheckTotalFlinkMemory* method(this method is similar to the logic of the *sanityCheckTotalProcessMemory* method in that it compares whether the derived memory size is consistent with the explicitly configured memory size) in TaskExecutorFlinkMemoryUtils Class, i guess *sanityCheckTotalProcessMemory* method should be called behind ""if"" branch, not in the ""if"" branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 15 06:00:50 UTC 2022,,,,,,,,,,"0|z138tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 03:40;Tommmmmm;Hi [~yunta] , could you assign this ticket to me;;;","15/Jun/22 06:00;yunta;[~Tommmmmm] already assigned to you, please go ahead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PredicateBuilder.in should accept null parameters,FLINK-28064,13450118,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,15/Jun/22 02:32,01/Jul/22 08:17,04/Jun/24 20:42,01/Jul/22 08:17,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"PredicateBuilder.in(int idx, List<Literal> literals).

Literals must be not null, but this is not enough to meet the needs of SQL `in`.

It is allowed to have null parameters in `in`.",,,,,,,,,,,FLINK-28179,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 01 08:17:31 UTC 2022,,,,,,,,,,"0|z138so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 08:17;lzljs3620320;master: 0688ce66bbf5f2736c82a4d4c6e4279e33e24f66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize PredicateBuilder.in for lots of parameters,FLINK-28063,13450116,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,lzljs3620320,lzljs3620320,15/Jun/22 02:13,04/Jul/22 04:27,04/Jun/24 20:42,04/Jul/22 04:27,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"The current treatment of in is to expand into multiple equals and join them using or.
But this does not perform well if there are particularly many parameters, we need to introduce In's Predicate to handle this case separately.",,,,,,,,,,,FLINK-28179,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 04:27:00 UTC 2022,,,,,,,,,,"0|z138s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 04:27;lzljs3620320;master: d11a287c756713c6b16a6c93291566a780f97b4a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Upsert-Kafka can not support Flink1.14.x With Sink Buffer,FLINK-28062,13450115,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Jiangfei Liu,Jiangfei Liu,15/Jun/22 01:54,01/Jul/22 04:27,04/Jun/24 20:42,01/Jul/22 04:27,1.14.0,1.14.2,1.14.3,1.14.4,,,,,Connectors / Kafka,Table SQL / API,,,,,,0,,,,,"In Flink1.14.x，Table API，Upsert-Kafka Sink can not support with sink buffer
In Flink1.13.x，can support

I look Flink1.13.x、Flink1.14.x source code，

In Flink1.13.x，Upsert-Kafka use the class org.apache.flink.streaming.connectors.kafka.table.BufferedUpsertSinkFunction

In Flink1.14.x，Upert-Kafka use the class org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriter

I find some diffrent with two class，please look pictures",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28037,,,,,,,,,,,,,,,,,,,,"15/Jun/22 01:53;Jiangfei Liu;1.13-upsert-kafka.png;https://issues.apache.org/jira/secure/attachment/13045086/1.13-upsert-kafka.png","15/Jun/22 01:53;Jiangfei Liu;1.14-upsert-kafka.png;https://issues.apache.org/jira/secure/attachment/13045085/1.14-upsert-kafka.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,,,Fri Jul 01 04:27:20 UTC 2022,,,,,,,,,,"0|z138s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 04:27;renqs;Mark as duplicating FLINK-28037;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create new tech blog for connector development based on Source API,FLINK-28061,13450091,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,14/Jun/22 20:00,11/Mar/24 12:44,04/Jun/24 20:42,,1.15.3,,,,,1.20.0,,,Documentation,,,,,,,0,,,,,"The most up-to-date blog introduced how to implement SourceFunction which will be deprecated soon: [https://flink.apache.org/2021/09/07/connector-table-sql-api-part1.html]

 ",,,,,,,,,,,,,,,FLINK-28045,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 14 20:11:17 UTC 2022,,,,,,,,,,"0|z138mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 20:11;jingge;For more information please see the discussion thread: https://lists.apache.org/thread/d6cwqw9b3105wcpdkwq7rr4s7x4ywqr9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Commit on checkpointing fails repeatedly after a broker restart,FLINK-28060,13450052,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,Christian.Lorenz77,Christian.Lorenz77,14/Jun/22 15:08,25/Apr/23 07:46,04/Jun/24 20:42,10/Aug/22 10:14,1.15.0,,,,,1.16.0,,,API / DataStream,Connectors / Kafka,,,,,,3,pull-request-available,,,,"When Kafka Offset committing is enabled and done on Flinks checkpointing, an error might occur if one Kafka broker is shutdown which might be the leader of that partition in Kafkas internal __consumer_offsets topic.

This is an expected behaviour. But once the broker is started up again, the next checkpoint issued by flink should commit the meanwhile processed offsets back to kafka. Somehow this does not seem to happen always in Flink 1.15.0 anymore and the offset committing is broken. An warning like the following will be logged on each checkpoint:

{code}
[info] 14:33:13.684 WARN  [Source Data Fetcher for Source: input-kafka-source -> Sink: output-stdout-sink (1/1)#1] o.a.f.c.k.s.reader.KafkaSourceReader - Failed to commit consumer offsets for checkpoint 35
[info] org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
[info] Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
{code}

To reproduce this I've attached a small flink job program.  To execute this java8, scala sbt and docker / docker-compose is required.  Also see readme.md for more details.
The job can be run with `sbt run`, kafka cluster is started by `docker-compose up`. If then the kafka brokers are restarted gracefully by e.g. `docker-compose stop kafka1` and `docker-compose start kafka1` with kafka2 and kafka3 afterwards, this warning will occur and no offsets will be committed into kafka.

This is not reproducible in flink 1.14.4.

","Reproduced on MacOS and Linux.

Using java 8, Flink 1.15.0, Kafka 2.8.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27962,,,,,,,,,,FLINK-27962,,,,,,,,,,"14/Jun/22 15:01;Christian.Lorenz77;flink-kafka-testjob.zip;https://issues.apache.org/jira/secure/attachment/13045068/flink-kafka-testjob.zip",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 10 10:14:05 UTC 2022,,,,,,,,,,"0|z138e0:",9223372036854775807,The Kafka Client version has been updated to 3.2.1. ,,,,,,,,,,,,,,,,,,,"14/Jun/22 17:42;martijnvisser;[~renqs] This seems related to FLINK-27962 - Should we raise priority to at least Critical, potential Blocker? Seems like this is a regression. I linked the two tickets together, because they both have value. Probably we should close one of them in the end;;;","15/Jun/22 08:21;renqs;I did some investigation and strongly suspect that this is caused by KAFKA-13563. We upgraded the Kafka client from 2.4.1 to 2.8.1 in Flink 1.15, and the bug was introduced in 2.6.2 by KAFKA-10793 :(

The patch of KAFKA-13563 is applied only on Kafka 3.1 and 3.2, so we have to bump the version as well, or urge Kafka guys to cherry-pick the patch back on 2.x. 

WDYT? [~martijnvisser] ;;;","15/Jun/22 10:13;danderson;I'd like to include a fix for this in 1.15.1, but I'm not sure about bumping up the Kafka client to 3.x. What are you thinking? [~martijnvisser] [~renqs] ;;;","15/Jun/22 11:34;martijnvisser;I think it depends on the impact of bumping the Kafka Clients dependency. If it's ""just"" a dependency update without modifying the actual source code, I would probably be OK with it. If it requires refactoring of the interface, then I would say we can only fix this in Flink 1.16.0. I'll setup a test CI run to see what happens. 

Looking at the release notes, I do hope that the main reason for going to a new main version was the deprecation of Java 8 and the deprecation of Scala 2.12. If that's it, I do hope impact will be low. ;;;","15/Jun/22 13:24;chesnay;Could someone clarify what this issue means in practice to users? Could we just _not_ commit the offsets?

I'm quite wary of doing a major version upgrade, independent of whether we have to adjust our code or not.
Alternatives include reverting FLINK-24765, or going back to 2.6.1 (i.e., some version that doesn't have the Kafka bug).;;;","15/Jun/22 13:44;peter.schrott;From a users perspective I see 2 issues here:

1) Monitoring / Alerting: We are using consumer offsets / consumer lag to monitor potential issues with the Flink job, also in terms of performance, i.e. the lag gets too large.

2) StartingOffsets: Flink Kafka connector offers the feature of resuming from committed offsets [1]. This feature would be obsolet if offsets are not committed to Kafka (sure, one can always use savepoints when restarting a job)

 

[1] https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/kafka/#starting-offset;;;","16/Jun/22 07:07;danderson;Along the lines of what Chesnay mentioned, what would be the impact of either reverting FLINK-24765, or rolling back the kafka client dependency to something like 2.6.1 (i.e., something newer than 2.4.1 but before the Kafka bug that was introduced by KAFKA-10793 in 2.6.2 and 2.7.1)?

 ;;;","16/Jun/22 07:35;martijnvisser;I have sincere doubts about reverting FLINK-24765, because more changes has happened since then (like FLINK-25573, FLINK-27487, FLINK-27480 and I'm sure I'm missing some more).

So that probably leaves us with a downgrade of the Kafka Clients dependency or an upgrade of the Kafka Clients dependency? ;;;","16/Jun/22 07:40;Christian.Lorenz77;Does someone know if the Kafka Team is aware that this issue exists in 2.8.1? I think Flink might be one of the most prominent usescases of using topic#assign where this is occurring, but for sure this will harm other users as well. The most natural fix for this would be to upgrade to kafka-client 2.8.2 once available. ;;;","16/Jun/22 07:54;chesnay;Indeed going back to 2.4.1 doesn't seem feasible; going back to 2.6.1 at least doesn't produce compile errors; waiting on CI.;;;","16/Jun/22 08:03;renqs;Personally I prefer to upgrade the Kafka client. Downgrading the Kafka client could introduce new bugs (like the one fixed in KAFKA-10793). I think the backward compatibility of Kafka is trustable according to our previous experience. On consumer side we don't use any hacks in Kafka source, purely depend on APIs so I think it should be fine, but for the producer we use [reflection|https://github.com/apache/flink/blob/c9a706b8388b324a37da43298e37074d0a452a34/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java#L331-L343] which is concerning. I can have a try to bump the version and see if it works. 

Any ideas from [~becket_qin] ?;;;","16/Jun/22 08:10;martijnvisser;I'm waiting for CI to complete a run upgrading to 3.1.1, that's looking good as well without needing to change any interface/implementation details. ;;;","16/Jun/22 12:29;mason6345;+1 on [~peter.schrott] 's assessment–we need also need this for metrics and in case Flink state is lost or we need to do a migration where it is operationally easier to throwaway Flink state.

In addition, Flink hasn't removed support for FlinkKafkaConsumer, right? The group offsets are essential for the migration process to the FLIP 27 Kafka Source since users will have operational issues moving without committed offsets in Kafka.

[~Christian.Lorenz77] I can look at the reproduction code next week. Does the commit eventually succeed? e.g. after the 5th checkpoint, nth checkpoint, etc?;;;","16/Jun/22 12:33;martijnvisser;{{FlinkKafkaConsumer}} has not been removed, but it has been deprecated since 1.14.0 via FLINK-24055. ;;;","16/Jun/22 12:37;Christian.Lorenz77;[~mason6345] I did not observe that the commit error vanished. To fix the error we currently restart our taskmanagers gracefully. I guess this will implicitly reset the broken kafka consumer state.;;;","16/Jun/22 19:44;martijnvisser;In my private CI run the upgrade to 3.1.1 was successful (the failing tests are other test stabilities), see https://dev.azure.com/martijn0323/Flink/_build/results?buildId=2650&view=results

No interface changes were required. The only thing that would be good to verify is if this the newer version has any change with regards to a minimum version of Kafka protocol or broker.  I looked for it but I think this is not the case based on what I could find;;;","16/Jun/22 21:24;martijnvisser;I've opened a PR against {{master}} to bump Kafka Clients there. 

I've also opened a draft PR against {{release-1.15}} - I'm not sure we're yet OK with bumping Kafka Clients in a patch release, but I wanted to verify if CI works for this one too. Given the limited change and the arguments brought forward by [~renqs] on possible other bugs that we might re-introduce when we downgrade, I would probably +1 to bump Kafka Clients for Flink 1.15.1;;;","17/Jun/22 10:15;danderson;It doesn't seem wise to upgrade the Kafka client from 2.8.1 to 3.1.1 for Flink 1.15.1. That's a big change to make this close to our release, and we shouldn't delay this release any further. So here's a proposal:
 * We bump the Kafka Clients to 3.1.1 in master now.
 * We don't try to fix FLINK-28060 for 1.15.1.

 * We create the Flink 1.15.1 release straight away, noting that there's a known issue with Kafka (FLINK-28060).
 * We reach out to the Kafka community to see if they're willing to create a 2.8.2 release with this patch.
 * In parallel, we merge the Kafka Clients bump to 3.1.1 after release 1.15.1 is done, to see how it behaves on the CI for the next few weeks and plan a quick Flink 1.15.2 release (most likely something like a month later).;;;","17/Jun/22 11:38;martijnvisser;+1 for this approach;;;","18/Jun/22 11:03;martijnvisser;Fixed in master: 189f88485d75821fe285e61bbf6623e88aec24d3;;;","20/Jun/22 10:52;Christian.Lorenz77;I think the according issue in Kafka is https://issues.apache.org/jira/browse/KAFKA-13840.;;;","21/Jun/22 16:33;mason6345;[~renqs] [~martijnvisser] were we able to reproduce this issue in Flink CI/unit test?

+1, I think we need to closely monitor KAFKA-13840, not sure if bumping to 3.1.1 really fixed the issue. A user states that they still see the issue in the 3.1.1 upgrade.;;;","22/Jun/22 07:13;martijnvisser;I'm not sure that we have a test for this behaviour. It would actually be nice if we could get one in. ;;;","26/Jul/22 21:30;danderson;[~martijnvisser] [~renqs] [~mason6345] Are we any closer to understanding this issue? Do we know how to fix it, either for a 1.15.2 release, or for 1.16.0?;;;","28/Jul/22 02:31;renqs;[~danderson] The ticket KAFKA-13840 is still pending for validation. Few things we could do on our side but I'll try to find some time to validate the patch in Kafka and push it forward.;;;","02/Aug/22 16:45;kyle.stehbens;Hi, we we're experiencing this issues and downgraded our kafka client to 2.5.1. There we're changes to how the group coordinator reconnect logic works in 2.6.0 and later that seems to have broken the recovery of the group co-coordinator and all kafka client version post 2.6.0 (inclusive)

Downgrading to v2.5.1 of the kafka library is confirmed working for us;;;","04/Aug/22 10:11;showuon;Could we try to test with Kafka v3.2.1? As mentioned in this comment: https://issues.apache.org/jira/browse/KAFKA-13840?focusedCommentId=17575186&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17575186 , I believe this issue fix be fixed in Kafka v3.2.1. Thanks.;;;","09/Aug/22 11:45;chesnay;I ran CI with Kafka 3.2.1 and it passed without requiring any other changes.
https://dev.azure.com/chesnay/flink/_build/results?buildId=2944&view=results

We could think about including it in 1.16.0, although I'm not too fond of bumping dependencies so close before the feature freeze. (To be fair though, we would have a few weeks to observe it).;;;","10/Aug/22 00:05;mason6345;[~showuon] Earlier, I had attempted to write an integration test with testcontainers in an attempt to reproduce the issue by following the steps in the ticket details. I compared the versions kafka-clients 2.8.1 and 3.1.1.

My test is as follows:
 # Start Kafka container and create topic
 # Create reader and assign the topics to underlying consumer
 # Fetch and invoke poll()
 # Call commitAsync()
 # Restart Kafka container 
 # Call commitAsync()
 # Call commitAsync()

For 2.8.1, there are 2 failures in the commit async call in step 6 and 7. For 3.1.1, there is only 1 failure at step 6.

3.1.1 seems to be the desired behavior. Some concerns:
 # However, unusually, step 4 will fail if the test doesn't invoke poll regardless of the kafka clients version. I think it is possible for Flink to have a race condition where commitAsync is executed before poll (short checkpoint interval causing commitAsync before poll if topic partitions take long to assign). Is this behavior intended?
 # Otherwise, do you have any recommendations for reproducing the issue in a CI environment where we do not assume poll() is invoked?;;;","10/Aug/22 00:16;showuon;[~mason6345] , yes, Kafka v3.1.1 assumes consumer poll ran before commitAsync call. But in Kafka v3.2.1, this assumption is removed. So, in v3.2.1, even if commitAsync calls earlier than poll, it'll work well.

So, for your question:

1. However, unusually, step 4 will fail if the test doesn't invoke poll regardless of the kafka clients version. I think it is possible for Flink to have a race condition where commitAsync is executed before poll (short checkpoint interval causing commitAsync before poll if topic partitions take long to assign). Is this behavior intended?

--> Yes, Kafka should not assume poll calls first or commitAsync calls first. That's a bug fixed in Kafka v3.2.1

2. Otherwise, do you have any recommendations for reproducing the issue in a CI environment where we do not assume poll() is invoked?

--> I think it's good. I also use the reproducer in this ticket with Kafka v3.2.1, and also investigate Kafka client logs, I confirmed it works well even if commitAsync calls earlier than poll.

 

Thanks.;;;","10/Aug/22 10:14;chesnay;master: bc9b401ed1f2e7257c7b44c9838e34ede9c52ed5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parallelize e2e tests,FLINK-28059,13450045,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,14/Jun/22 14:38,24/Nov/22 01:02,04/Jun/24 20:42,20/Jun/22 08:34,kubernetes-operator-1.1.0,,,,,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Motivation:
 * Tests are running in a loop within a single step
 * It takes 15mins for the e2e tests to finish
 * We could run up to 256 parallel tasks instead of the current 6
 * Without looking at the logs it is hard to spot/verify which exact tests are running during e2e CI workflows

Suggestions:
 * Let's add the tests into an extra dimension of the test matrix instead of looping
 * Try to find a way to share the common steps before/after the tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 20 08:34:35 UTC 2022,,,,,,,,,,"0|z138cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 08:34;gyfora;merged to main dc1e89c52be847d4e57ad3862ab94566a1d905e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add option SKIP_FIRST_DATA_ROW,FLINK-28058,13450041,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,lucas_wu,lucas_wu,lucas_wu,14/Jun/22 13:40,19/Jun/22 10:34,04/Jun/24 20:42,19/Jun/22 10:34,,,,,,,,,Connectors / FileSystem,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,,,,,"now CsvFileSystemFormatFactory.CsvInputFormat do not support add option SKIP_FIRST_DATA_ROW , when my csv file has header, source table will read file's header ,then collect to downstream. but i do not need the header, i hope i can set the option when i create table.like 

""CREATE TABLE file_source ("" +
""a STRING,"" +
""b STRING,"" +
""c STRING"" +
"") "" +
""WITH (\n"" +
"" 'connector' = 'filesystem',\n"" +
"" 'path' = 'file:///xxx/xxx/test_sink.csv',\n"" +
"" 'format' = 'csv' ,\n"" +
"" 'csv.ignore-first-line' = 'true' "" +
"""" +
"")""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Jun 19 10:33:27 UTC 2022,,,,,,,,,,"0|z138bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 07:10;sonice_lj;[~jark] Could you assign this ticket to me?;;;","16/Jun/22 07:16;lucas_wu;i have solution to fix it, how can i commit the code;;;","16/Jun/22 07:41;sonice_lj;[~lucas_wu] OK, [Flink Contribution Guide |https://flink.apache.org/contributing/contribute-code.html]may help you.;;;","16/Jun/22 07:44;lucas_wu;thanks , but need [~jark] assign this ticket to me;;;","16/Jun/22 08:11;martijnvisser;[~lucas_wu] I've assigned it to you;;;","19/Jun/22 10:33;lucas_wu;i found CsvTableFactoryBase has fix it,so pls close this issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LD_PRELOAD is hardcoded to x64 on flink-docker,FLINK-28057,13450040,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nferrario,nferrario,nferrario,14/Jun/22 13:39,30/Aug/22 09:16,04/Jun/24 20:42,26/Jul/22 13:24,1.15.0,,,,,1.14.6,1.15.2,1.16.0,flink-docker,,,,,,,0,pull-request-available,,,,"ARM images are not using jemalloc because LD_PRELOAD is hardcoded to use an x64 path, causing this error:
{noformat}
ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libjemalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
{noformat}


Right now docker-entrypoint is using this:

{code:sh}
maybe_enable_jemalloc() {
    if [ ""${DISABLE_JEMALLOC:-false}"" == ""false"" ]; then
        export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libjemalloc.so
    fi
}
{code}

I propose we use this instead:
{code:sh}
maybe_enable_jemalloc() {
    if [ ""${DISABLE_JEMALLOC:-false}"" == ""false"" ]; then
        # Maybe use export LD_PRELOAD=$LD_PRELOAD:/usr/lib/$(uname -i)-linux-gnu/libjemalloc.so
        if [[ `uname -i` == 'aarch64' ]]; then
            export LD_PRELOAD=$LD_PRELOAD:/usr/lib/aarch64-linux-gnu/libjemalloc.so
        else
            export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libjemalloc.so
        fi
    fi
}
{code}

https://github.com/apache/flink-docker/pull/117",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Aug 30 09:16:06 UTC 2022,,,,,,,,,,"0|z138bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 13:46;yunta;Thanks for creating this issue. Flink seems do not have official arm-based docker image, do we need some changes to create arm-based docker image?;;;","14/Jun/22 13:51;nferrario;Hi [~yunta]. Flink is building official ARM images since 1.15 as far as I know, right? ([https://hub.docker.com/_/flink])

Everything works perfectly except for jemalloc, which doesn't prevent Flink from starting anyway.

 

A current workaround is to use these configs:
{code:yaml}
containerized.jobmanager.env.DISABLE_JEMALLOC: true
containerized.jobmanager.env.LD_PRELOAD: /usr/lib/aarch64-linux-gnu/libjemalloc.so
containerized.taskmanager.env.DISABLE_JEMALLOC: true
containerized.taskmanager.env.LD_PRELOAD: /usr/lib/aarch64-linux-gnu/libjemalloc.so 
{code};;;","14/Jun/22 13:52;chesnay;The images we release via official-images are actually also built for arm since FLINK-25679 (1.15).;;;","15/Jun/22 05:43;yunta;Thanks for the information [~chesnay].

[~nferrario] I have already assigned this ticket to you and we can discuss in the PR.;;;","26/Jul/22 13:24;yunta;merged in master: ed004f8cdac8e5c96b81ec069863a4f836d170d5;;;","26/Jul/22 13:25;yunta;Since Flink supports arm image from flink-1.14, [~nferrario]  could you also create PR targets for {{dev-1.14}} and {{dev-1.15}} branch? Remember to modify your commit message with the ticket ID {{{}[FLINK-xxx]{}}}.;;;","28/Jul/22 19:46;nferrario;Hi [~yunta]

[https://github.com/apache/flink-docker/pull/126] (1.14)

[https://github.com/apache/flink-docker/pull/125] (1.15);;;","29/Jul/22 02:25;yunta;merged

dev-1.15: ff2d07865f2f0d4a0adfda3525a42a5d758c64bb

dev-1.14: 04931762b8e9b6f0d29b902b8b195e562c8c4c0b

 ;;;","30/Aug/22 08:55;chesnay;[~yunta] Why do we see this warning on CI of every modified image:

??WARNING: attempted to load jemalloc from /usr/lib/x86_64-linux-gnu/libjemalloc.so but the library couldn't be found. glibc will be used instead.??;;;","30/Aug/22 09:06;chesnay;Ehhh...before that change we also got another error on CI...

??ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libjemalloc.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.??;;;","30/Aug/22 09:13;nferrario;[~chesnay] this happens because the CI host doesn’t have jemalloc installed, and it’s actually running as “bare metal”. We’d need to change the test suite to execute the Docker image instead to make that warning go away.;;;","30/Aug/22 09:16;chesnay;[~nferrario] Got it, thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce high level API for cases that do not require specific SourceEnumerator logic.,FLINK-28056,13450037,13544997,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,14/Jun/22 13:26,26/Jul/23 20:15,04/Jun/24 20:42,,,,,,,,,,Connectors / Common,,,,,,,0,,,,,"There are simple cases that do not require any specific SplitEnumerator logic. It should be as simple to implement them with the Source API as it is with SourceFunction.

NoOpSplitEnumerator?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 11 08:25:17 UTC 2022,,,,,,,,,,"0|z138ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 08:15;Leo Zhou;I'm working on FLINK-28227 now and found that it's would be nice to have this NoOpSplitEnumerator, maybe we should implement this class first. [~martijnvisser] , can you assign this ticket to me ?;;;","11/Jul/22 08:17;martijnvisser;[~Leo Zhou] I think that this ticket can't be implemented without having a FLIP first on this topic. It's most likely one of the most important public interfaces we would want to expose. Do you want to work on that?;;;","11/Jul/22 08:25;Leo Zhou;Hi [~martijnvisser], IMO, the change is trivial，we just need a simple implementation of SplitEnumerator, like [NoOpEnumerator|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-connector-test-utils/src/main/java/org/apache/flink/connector/testframe/source/enumerator/NoOpEnumerator.java];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Source API utilities for throttling number of records emitted per second.,FLINK-28055,13450032,13449984,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,afedulov,afedulov,afedulov,14/Jun/22 13:22,29/Jun/23 11:34,04/Jun/24 20:42,29/Jun/23 11:34,,,,,,1.7.3,,,Connectors / Common,,,,,,,0,,,,,SourceFunctions used in demos typically employ some throughput throttling during data generation. Utilities to achieve the same need to added to the new Source API.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 29 11:34:15 UTC 2023,,,,,,,,,,"0|z1389k:",9223372036854775807,Is covered by the DataGeneratorSource implementation: https://issues.apache.org/jira/browse/FLINK-27919,,,,,,,,,,,,,,,,,,,"29/Jun/23 11:34;afedulov;Was covered by the DataGeneratorSource implementation: https://issues.apache.org/jira/browse/FLINK-27919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Source API utilities to control records emitted per checkpoint,FLINK-28054,13450031,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,14/Jun/22 13:19,26/Jul/23 20:19,04/Jun/24 20:42,,,,,,,,,,,,,,,,,0,,,,,"For some connectors, it is required to verify the content of a checkpoint after processing specific records. See [1] for more details. This can be done using the SourceFunction like demonstrated here [2].  An abstraction to support this use case with the new Source API is required.

Idea: SourceReader interface has notifyCheckpointComplete() callback, so a higher level implementation could be provided to emit records in lockstep with checkpoints.

 

[1]https://lists.apache.org/thread/9kfppzbq7r2gk5mhykc7m786sv0fc8j2 [2]https://github.com/apache/iceberg/blob/master/flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTestSource.java

 ",,,,,,,,,,,,,,,FLINK-28048,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 28 13:06:53 UTC 2023,,,,,,,,,,"0|z1389c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 13:06;afedulov;For reference, in the DataGeneratorSource this was achieved with the RateLimiterStrategy: [DataGeneratorPerCheckpoint|https://github.com/apache/flink/blob/873a56361bfd77c828ee743febc9dda2bb044791/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/datagen/DataGeneratorPerCheckpoint.java#L30];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce queue to execute request in sequence,FLINK-28053,13450011,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,14/Jun/22 12:11,15/Jul/22 03:25,04/Jun/24 20:42,15/Jul/22 03:25,1.16.0,,,,,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,"There are two kinds of the parallel we need to consider:

1. The parallel among all the operations for the same session. For example, the user may submit a SQL to create a table and modify another table's schema in parallel.
2. The parallel is also mainly about the Operation itself. It is possible that one thread is reading the data from the Operation and another one closes the Operation in parallel. 

We may introduce the queue to make these requests execute in sequence. It brings the benefit that simplifying the logic in the Operation and OperationManager and moving all locks to the handover. But it may cause a performance regression. Therefore, it's better if we can start this issue until all components finish.



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Jul 15 03:25:18 UTC 2022,,,,,,,,,,"0|z1384w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 08:57;jark;This is very confusing just open a ticket with a title. Please add descriptions about the purpose of the ticket and how to do it (if have). ;;;","22/Jun/22 09:09;jark;Similar issue in HiveServer: HIVE-11402;;;","15/Jul/22 03:25;fsk119;Merged into the master: af9356c82ecb049e44b10fb4030fd12aebacddc0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove AdaptiveSchedulerTest#RunFailedJobListener,FLINK-28052,13450009,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Jun/22 12:01,15/Jun/22 12:07,04/Jun/24 20:42,15/Jun/22 12:07,,,,,,1.16.0,,,Runtime / Coordination,Tests,,,,,,0,pull-request-available,,,,"The RunFailedJobListener has rather obscure semantics.
It considers a job to be terminal after it was restarted. This behavior is quite specific to a particular test case.
A cleaner approach is just to just cancel the job and wait for it to terminate.

Additionally it considered a job as running purely based on the job status, whereas, in particular when checkpointing is involved, waiting for the tasks to be submitted is a better measure.
In fact, testExceptionHistoryWithTaskFailureFromStopWithSavepoint is broken since a savepoint is never triggered, as not all tasks are running.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 15 12:07:53 UTC 2022,,,,,,,,,,"0|z1384g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 12:07;chesnay;master: bcf56de51eeae89bcaadb37fa09c053aa3444f1f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote ExternallyInducedSourceReader to non-experimental @Public,FLINK-28051,13450000,13449984,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,14/Jun/22 11:31,30/Nov/23 01:39,04/Jun/24 20:42,,,,,,,,,,Connectors / Common,Tests,,,,,,0,,,,,It needs to be evaluated if ExternallyInducedSourceReader can be promoted.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Nov 30 01:39:04 UTC 2023,,,,,,,,,,"0|z1382g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 11:40;knaufk;Is it confirmed an alternative is needed? I am personally only aware of Pravega as a user and not sure they still use it. ;;;","17/Jun/22 12:03;afedulov;Not sure. Do you have a contact person at Pravega to ask?;;;","23/Nov/23 03:57;Brian Zhou;Hi [~afedulov] and [~knaufk] , thanks for bringing the topic to the Pravega community.

For the current connector usage, most users still use the legacy source function, and the FLIP-27 support using the ExternallyInducedSourceReader has been merged and released, but as the limitation of the ExternallyInducedSourceReader(forcing global recovery) , we still do not use that in production and also have a plan to improve the FLIP-27 API for the next quarter.

I think we will switch to use the normal FLIP-27 API with that plan, but I need to double confirm with the engineers if ExternallyInducedSourceReader will still be used in the design. Will get back here if I have the updates.;;;","27/Nov/23 11:14;afedulov;Hi [~Brian Zhou], thanks a lot for the clarification. Looking forward to contributions from the Pravega community.;;;","27/Nov/23 11:20;afedulov;[~Brian Zhou] can I assign this ticket to you or one of your colleagues?;;;","30/Nov/23 01:39;Brian Zhou;Hi [~afedulov] , we can confirm the usage of ExternallyInducedSourceReader will be removed in the coming FLIP-27 improvement.

As Pravega connector seems to be the only user of this API, and it also raises some special code diverge in Flink like [https://github.com/apache/flink/blob/72654384686d127172b48b0071ea7656b16e9134/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceOperatorStreamTask.java#L101] , instead of promotion, is it fine to deprecate or remove this API for Flink 2.0?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Source API alternative to SourceExecutionContext#fromElements(*) methods,FLINK-28050,13449997,13449984,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,14/Jun/22 11:16,27/Nov/23 10:18,04/Jun/24 20:42,27/Nov/23 10:18,,,,,,1.19.0,,,API / DataStream,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Nov 27 10:18:18 UTC 2023,,,,,,,,,,"0|z1381s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 10:18;chesnay;master:
68437b937a60c647abe1a4104289849c006b8fe7..d351c5bd9c1f28a3e5ffe98fb549c1b94618485b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FLIP-208 functionality to stop Source based on consumed records,FLINK-28049,13449995,13544997,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,14/Jun/22 11:13,26/Jul/23 20:14,04/Jun/24 20:42,,,,,,,,,,Connectors / Common,,,,,,,0,pull-request-available,,,,https://cwiki.apache.org/confluence/x/fZbkCw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Oct 12 05:13:46 UTC 2022,,,,,,,,,,"0|z1381c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 03:58;stroshkov;Hi [~afedulov]! I am interested in this issue. I would like to implement this.;;;","11/Oct/22 08:14;stroshkov;Hi! I created PR for this issue.

[https://github.com/apache/flink/pull/21014]

I implemented RecordEvaluator interface and methods for Kafka and Pulsar sources, as recommended in original FLIP. Please take a look at it.;;;","11/Oct/22 08:29;martijnvisser;[~stroshkov] Like mentioned in the PR, no vote has happened on FLIP-208 so it can't be merged and hence a review also doesn't really make sense yet;;;","11/Oct/22 08:31;martijnvisser;You can find the FLIP at https://cwiki.apache.org/confluence/display/FLINK/FLIP-208%3A+Add+RecordEvaluator+to+dynamically+stop+source+based+on+de-serialized+records and the link to the discussion thread too. It seems like it has stalled since, but you could restart the discussion by replying on that thread to ask what's the status of this topic and if you can help moving it forward. ;;;","12/Oct/22 05:13;stroshkov;Thank you! Will do that;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Source API alternative to FiniteTestSource,FLINK-28048,13449994,13449984,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,afedulov,afedulov,afedulov,14/Jun/22 11:09,20/Apr/24 18:10,04/Jun/24 20:42,20/Apr/24 18:10,,,,,,1.20.0,,,Connectors / Common,Tests,,,,,,0,pull-request-available,,,,This also has to verify that Iceberg connector tests mentioned in FLINK-28054 also get covered by the solution.,,,,,,,,,,,,,,,,FLINK-28054,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-14 11:09:59.0,,,,,,,,,,"0|z13814:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Annotate StreamExecutionEnvironment#readFile()/readTextFile(*) methods deprecated in favor of FileSource#forRecordStreamFormat/forBulkFileFormat,FLINK-28047,13449992,13449984,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,14/Jun/22 11:02,30/Jun/22 07:38,04/Jun/24 20:42,30/Jun/22 07:38,1.15.2,,,,,1.16.0,,,API / DataStream,Connectors / FileSystem,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 07:38:59 UTC 2022,,,,,,,,,,"0|z1380o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 15:04;afedulov;[~martijnvisser] this is also done, PTAL.;;;","30/Jun/22 07:38;martijnvisser;Fixed in master: 3e73fb543eb7540a28838bcefe07fb72157a9731;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Annotate SourceFunction as deprecated,FLINK-28046,13449985,13449984,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,afedulov,afedulov,afedulov,14/Jun/22 10:21,02/Aug/23 02:11,04/Jun/24 20:42,02/Aug/23 02:11,1.15.3,,,,,1.18.0,,,API / DataStream,,,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Aug 02 02:11:36 UTC 2023,,,,,,,,,,"0|z137z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","09/Mar/23 08:05;Wencong Liu;cc [~Weijie Guo] ;;;","30/Jun/23 14:19;Weijie Guo;master(1.18) via 07bf511a32b525f94d258daec347cf777b31bebb.;;;","17/Jul/23 06:34;xtsong;Per the opinions from this thread [1], it seems we need to revert this ticket?

WDYT? [~afedulov][~Weijie Guo][~leonard]

[1] https://lists.apache.org/thread/734zhkvs59w2o4d1rsnozr1bfqlr6rgm;;;","17/Jul/23 07:39;leonard;The java document[1] of SourceFuntion said: ""@deprecated ...., NOTE: All sub-tasks from FLINK-28045 must be closed before this API can be completely removed."" , as FLINK-28045 has too many subtasks did not been finished, *+1* to revert the change.

The correct way to deprecate SourceFuntion is resolve all subtasks for FLINK-28045 firstly and then deprecated it. 

[1]https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java#L97;;;","19/Jul/23 07:13;leonard;As above discussion, the original commits for this ticket reverts by master: 1635a284d75570cc8fc834e7b1bfb2d9d8df0c4a.
Let's start the ""deprecate the SourceFunction interface"" task until all other subtasks of FLINK-28045 have been finished.

 

 ;;;","20/Jul/23 19:45;afedulov;[~leonard] [~xtsong] 

Just to clarify, is marking ""Deprecated"" equal to ""can be completely removed right away"" for you? If so, is it defined as such somewhere in the project bylaws? I'd like to understand what speaks against using it for communicating that the interface is due to be removed so that users become aware that they need to start using the future-proof alternative. Ultimately, the longer the API remains Public without the deprecation hint, the more pain it will induce when it is finally dropped. What is the benefit in waiting for all the items in  FLINK-28045 to be complete both for us and for the users?;;;","20/Jul/23 19:49;afedulov;> The java document[1] of SourceFuntion said: ""@deprecated ...., NOTE: All sub-tasks from FLINK-28045 must be closed before this API can be completely removed."" , as FLINK-28045 has too many subtasks did not been finished, *+1* to revert the change.

Yes, *removed,* not marked as deprecated. For me these are two different things. I do not believe that waiting and hiding our clear intention from the users by keeping it Public without the Deprecation hint is the right approach here.;;;","21/Jul/23 01:50;xtsong;I think one of the reasons we want to send the message that an API is deprecated is that, we want the users to migrate from it to the new one. However, if the new API is not ready / stable and users cannot yet migrate, I don't see how users can benefit from getting an early message.

This is also required by the newly approved [FLIP-321|https://cwiki.apache.org/confluence/display/FLINK/FLIP-321%3A+Introduce+an+API+deprecation+process].
{quote}
In addition, a @Public interface can only be marked as deprecated if
- There are one or more @Public interfaces that serve as replacement of its functionality, or
- The functionality of the @Public interface is going to be removed completely without any full or partial replacement.
{quote}

I think this specific case is a bit different, because (correct me if I'm wrong) the new Source is ready for most of the use cases except for some special cases. That means the early deprecation does help more connectors to migrate early. However, for those connectors that cannot be migrated atm, it becomes hard to track when the prerequisites will be met. And it's also unfair if we start counting the migration period from a time point where part of the connectors cannot start their migration.

I think both ways have their pros and cons. I'm slightly leaning toward marking the APIs as deprecated after completing the other FLINK-28045 sub-tasks, but is also okay with the other option.;;;","21/Jul/23 08:31;afedulov;This is how the subject of the original vote was [formulated|https://lists.apache.org/thread/kv9rj3w2rmkb8jtss5bqffhw57or7v8v]:
{quote}This proposition implies marking the SourceFunction interface itself as @Deprecated + redirecting to the FLIP-27 Source API right away, without waiting for all the subtasks to be completed.
{quote}
As you can see in the voting [results|https://lists.apache.org/thread/hrpsddgz65hjvhjozhg72s0wsmxz145p], the community decision was in favor of this approach.;;;","21/Jul/23 08:57;mxm;Are there really any major features missing from the new source API which would prevent users from migrating? I would prefer marking as {{@Deprecated}} now. We will make sure to address any gaps in the feature set of the new source API. The tasks are clearly tracked here in JIRA and also referenced in the code. This is also a chance to collect feedback from users who migrate early.;;;","25/Jul/23 04:23;leonard;[~mxm] There're extra features to be done, but we still lack a lot works listed in this umbrella ticket to let user migrate to new Source API, current Source API is to complex for developers even they're experienced flink developers[1],  [~lzljs3620320] and  I also have introduced bugs for Apache Paimon Connector and Flink CDC Connectors due to understanding the new Source API incorrectly :(.

Although I understand the motivation to deprecate the API without waiting any subtasks, but it still doesn't look like a correct workflow, there're same concerns[2][3][4] that we should implement these subtasks before we deprecate this interfaces from umbrella issue or discussion email. 

If we must deprecate the API firstly ignore these potential subtasks(improvements) for the big deprecation API goal of 2.0,  I'd like to propose giving green light to this ticket's workflow and mark the SourceFunction as deprecated and need someone to say/promise that all subtasks will be finished in 1.19.

 
[1]https://lists.apache.org/thread/5olmnypjw2nvmsc1m2gmw1btzm9dl3ch
[2]https://issues.apache.org/jira/browse/FLINK-28045?focusedCommentId=17555561&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17555561
[3]https://lists.apache.org/thread/5olmnypjw2nvmsc1m2gmw1btzm9dl3ch
[4]https://lists.apache.org/thread/d6cwqw9b3105wcpdkwq7rr4s7x4ywqr9;;;","25/Jul/23 10:49;chesnay;I wouldn't put too much emphasize on [1].
The new API is certainly more complex, but after writing/reviewing a few I can definitely say that it's possible to do so.
The documentation can absolutely be improved, but blocking the deprecation because of that is imo ridiculous.

You ran into issues on the Paimon/CDC side, where are the tickets to fix the problems? Flavio ran into issues, where's the ticket for that?
I'd say we should deprecate things precisely to force people to handle these problems and not keep clinging onto the SourceFunction interface for dear life.

With the current discussion about having some 1.x it's possibly even less of a problem to remove the old source function. Yes, they may mean you can't upgrade to 2.0 immediately. Which is fine.;;;","25/Jul/23 14:46;leonard;

[~chesnay] We've discussed with [~afedulov], Konstantin offline and have posted the consensus to dev mail list.

> You ran into issues on the Paimon/CDC side, where are the tickets to fix the problems? Flavio ran into issues, where's the ticket for that?
We have walked around but have some initial ideas, I think we can propose a FLIP to enhance new Source in the future.;;;","02/Aug/23 02:11;leonard;Fixed in master(1.18): fc3035b40fbdf32694eb90c78c4dd68608c1c9d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[umbrella] Blockers for SourceFunction API removal (in Flink 2.0),FLINK-28045,13449984,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,14/Jun/22 10:11,20/Oct/23 12:58,04/Jun/24 20:42,,,,,,,,,,Connectors / Common,,,,,,,0,,,,,"This ticket should only contain items where SourceFunction is already used in the existing code base. For new functionality and Source V2 API improvements, use FLINK-32692.
—
Current state of migration to Source V2 API for officially-supported connectors
 (/) FileSystem (Public)
 (/) Kafka  (PublicEvolving)
 (/) Cassandra (PublicEvolving)
 (/) Pulsar (PublicEvolving)
 (/) HiveSource (PublicEvolving)
 (/) HybridSource (PublicEvolving)
 (/) MongoDB (PublicEvolving)
 (/) Hive (PublicEvolving)
 (?) Kinesis (Experimental)
  (flagoff) JDBC - not impacted (based on {{{}TableFunction{}}})
  (flagoff) HBase - not impacted (based on {{{}TableFunction{}}})
 (x) RabbitMQ
 (x) Google Pub Sub

 

Links to externalized connector repos: https://cwiki.apache.org/confluence/display/FLINK/Externalized+Connector+development",,,,,,,,,,,,,,,,FLINK-28061,,,,,,,,,,,,,,,,,,,FLINK-25852,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 27 10:04:09 UTC 2023,,,,,,,,,,"0|z137yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 11:40;knaufk;Are all of these subtasks considered blockers for the deprecation or is there a distincation between ""Must Have"" and ""Nice-to-Have""?;;;","17/Jun/22 12:00;afedulov;[~knaufk]  Specified in the [VOTE] thread: https://lists.apache.org/thread/kv9rj3w2rmkb8jtss5bqffhw57or7v8v;;;","20/Jun/22 10:52;airblader;What I'd like to see, personally, is a note on the deprecated API that guarantees that the (now deprecated) APIs will not be removed until these issues are resolved. Otherwise IMO it could easily happen in the future that 2.0 is released, deprecated APIs are removed in a cleanup process thereof, but no one actually remembers that these issues here must first be implemented. Basically, all of these issues are blockers for 2.0 if SourceFunction is to be dropped with 2.0.;;;","20/Jun/22 13:50;afedulov;Good idea, [~airblader] . I will add it to the deprecation PR that will follow as soon as the vote gets closed.;;;","26/Jul/23 07:37;knaufk;[~airblader] What we still need to do, though, is actually aligning on the list of these issues/blockers.;;;","26/Jul/23 21:31;afedulov;[~danny.cranmer] I see that Kinesis Source V2 connector is currently marked as Experimental. What is its current state? Do you see any obstacles in being able to promote it to PublicEvolving in the near(ish) future?;;;","27/Jul/23 08:55;dannycranmer;Hey [~afedulov] , thanks for driving this. The Kinesis Source V2 is still under active development. The parent task is FLINK-24438. We are keeping it {{@Experimental}} until we have feature parity with the legacy connector. So, it is not ready to become {{@PublicEvolving}} yet, but happy to aim for it to be ready by Flink 2.0. FYI I have marked FLINK-32696 as blocked by FLINK-24438.

 

Generally speaking, this is not a blocker for removing the legacy Source interface in v2.0 in my opinion.

 

FYI [~Hong Teoh] ;;;","27/Jul/23 10:04;afedulov;Hi [~dannycranmer], thanks for the confirmation!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add hadoop filesystems configuration possibility to all deployment targets,FLINK-28044,13449982,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,14/Jun/22 10:08,21/Jun/22 14:03,04/Jun/24 20:42,21/Jun/22 14:03,1.16.0,,,,,,,,Connectors / Common,,,,,,,0,pull-request-available,security,,,"At the moment only YARN supports delegation tokens for hadoop filesystesm with the following config:
{code:java}
yarn.security.kerberos.additionalFileSystems
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 14:03:04 UTC 2022,,,,,,,,,,"0|z137yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 14:03;mbalassi;f8f54bc in master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Invalid lambda deserialization"" in AvroParquetReaders",FLINK-28043,13449974,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,14/Jun/22 09:42,01/Jul/22 03:22,04/Jun/24 20:42,29/Jun/22 12:31,1.15.0,,,,,1.16.0,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,"I packed a bundle jar including flink-parquet and flink-avro with ""org.apache.avro"" relocated, to support PyFlink reading avro records from parquet file, and ""Invalid lambda deserialization"" error occurs at runtime. I guess this is similar to FLINK-18006 and points to MSHADE-260

 ",,,,,,,,,,,,,FLINK-28336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MSHADE-260,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 29 12:31:54 UTC 2022,,,,,,,,,,"0|z137wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 12:00;Juntao Hu;[~jingge] could you take a look at this PR?;;;","15/Jun/22 16:56;jingge;Just want to let you know that I will find time to do it this week. ;;;","29/Jun/22 12:31;dianfu;Merged to master via 5c7716b4c0b8e3a9c34a236333bf6f7745bcb0f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create an extension for resetting HiveConf,FLINK-28042,13449955,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,14/Jun/22 08:37,11/Mar/24 12:43,04/Jun/24 20:42,,,,,,,1.20.0,,,Connectors / Hive,Tests,,,,,,0,,,,,"The {{HiveConf}} is a singleton and modified by both production and test code.

We should think about writing an extension that prevents these changes from leaking into other tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27999,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-14 08:37:56.0,,,,,,,,,,"0|z137sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table store cannot  distinguish  filesystem Scheme when system have 'hadoop classpath',FLINK-28041,13449941,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,chaojipaopao,chaojipaopao,14/Jun/22 07:43,19/Jul/22 09:51,04/Jun/24 20:42,19/Jul/22 09:51,table-store-0.1.0,table-store-0.1.1,table-store-0.2.0,,,table-store-0.2.0,,,Table Store,,,,,,,0,easyfix,,,,"when using flink-table-store Quick Start 
{code:java}
//step5: 
SET 'table-store.path' = '/tmp/table_store'; {code}
then write data submit the  insert sql  to the cluster
{code:java}
//代码占位符
INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; {code}
wecan see the job failed ;

the log like this:


java.io.IOException: Could not perform checkpoint 1 for operator Writer -> Local Committer (1/1)#0.
at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1210)
at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:493)
at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint(AbstractAlignedBarrierHandlerState.java:74)
at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived(AbstractAlignedBarrierHandlerState.java:66)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: java.lang.RuntimeException: java.io.FileNotFoundException: File does not exist: /tmp/table_store/default_catalog.catalog/default_database.db/ word_count/bucket-0/sst-795c7ecf-40d9-433a-8a49-81336940be7a-0
at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)
at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2157)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2127)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2040)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:583)
at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:94)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2272)
at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:172)
at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:51)
at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.emitCommittables(SinkWriterOperator.java:196)
at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.prepareSnapshotPreBarrier(SinkWriterOperator.java:166)
at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.prepareSnapshotPreBarrier(RegularOperatorChain.java:89)
at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:300)
at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12(StreamTask.java:1253)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1241)
at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1198)
... 22 more

 

*the program can distinguish filesystem*

 ","flink 1.15.0

flink-table-store 0.1.0

hadoop 2.6.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28072,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Java,,,Tue Jul 19 09:51:13 UTC 2022,,,,,,,,,,"0|z137pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 09:20;lzljs3620320;Thanks [~chaojipaopao] for your reporting.

I think we should enrich the path to a URI for making sure a consistent default scheme.;;;","19/Jul/22 09:51;lzljs3620320;Fixed in  FLINK-28072;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Trino reader for table store,FLINK-28040,13449931,13449930,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,lzljs3620320,lzljs3620320,14/Jun/22 06:22,07/Jul/22 04:14,04/Jun/24 20:42,07/Jul/22 04:14,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,,,,,"Can refer to FLINK-27947 to write a Trino reader.

See https://trino.io/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jul 04 04:33:58 UTC 2022,,,,,,,,,,"0|z137n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 04:33;lzljs3620320;Trino binary is using JDK 11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Table Store Ecosystem: Compute Engine Readers,FLINK-28039,13449930,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,lzljs3620320,lzljs3620320,14/Jun/22 06:19,15/Jul/22 02:48,04/Jun/24 20:42,15/Jul/22 02:48,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,,,,,"After some refactor, we have a stable connector interfaces, we can develop compute engine connectors in a controlled cost.

The most classic scenario is that write by Flink and read by other engines.

We can have Readers for Apache Hive, Apache Spark and Trino.",,,,,,,,,,,,,,,,FLINK-27346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-14 06:19:17.0,,,,,,,,,,"0|z137mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB rescaling improvement & rescaling benchmark - umbrella ticket,FLINK-28038,13449929,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,14/Jun/22 06:13,14/Jun/22 06:34,04/Jun/24 20:42,14/Jun/22 06:34,1.16.0,,,,,1.16.0,,,Runtime / State Backends,,,,,,,0,,,,,Placeholder umbrella ticket for RocksDB rescaling improvement.,,,,,,,,,,,,,,,,FLINK-21321,FLINK-17971,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-14 06:13:50.0,,,,,,,,,,"0|z137mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Upsert-Kafka can not support Flink1.14.x,FLINK-28037,13449921,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,renqs,Jiangfei Liu,Jiangfei Liu,14/Jun/22 04:44,13/Aug/22 22:37,04/Jun/24 20:42,,1.14.4,,,,,,,,Connectors / Kafka,,,,,,,0,flink-connector-kafka,stale-assigned,upser-kafka,,"in Flink 1.14.x，flink sql upsert-kafka sink can not write data into kafka topic with sink buffer flush config，eg 
h5. sink.buffer-flush.max-rows
h5. sink.buffer-flush.interval

in Flink1.13.x，flink sql upsert-kafka sink can write data into kafka topic with sink buffer lush config",Flink Version: 1.14.0 1.14.2 1.14.3 1.14.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28062,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/22 10:41;Jiangfei Liu;flink1.14.x_org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriter.WrappedContext.png;https://issues.apache.org/jira/secure/attachment/13046766/flink1.14.x_org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriter.WrappedContext.png","14/Jun/22 04:42;Jiangfei Liu;kafka-sql.png;https://issues.apache.org/jira/secure/attachment/13045033/kafka-sql.png","14/Jun/22 04:42;Jiangfei Liu;kafka-sql2.png;https://issues.apache.org/jira/secure/attachment/13045032/kafka-sql2.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,,,Sat Aug 13 22:37:51 UTC 2022,,,,,,,,,,"0|z137kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 06:48;martijnvisser;[~renqs] Can you have a look at this one?;;;","14/Jun/22 07:01;Jiangfei Liu;in Flink1.14.x，flink sql upsert-kafka can write data into kafka topic without sink buffer flush config，when i add sink buffer flush configs，will throw exception,look pictures;;;","15/Jun/22 03:20;JasonLee;hi, It seems that StreamRecord lost timestamp which caused the NPE, but I didn't find out why;;;","01/Jul/22 04:13;renqs;Thanks for reporting this issue [~Jiangfei Liu] . I think it's a bug in `ReducingUpsertWriter` that the context wrapper doesn't handle the case without timestamp correctly. I'll take over the issue. ;;;","14/Jul/22 10:41;Jiangfei Liu;[~renqs] !flink1.14.x_org.apache.flink.streaming.connectors.kafka.table.ReducingUpsertWriter.WrappedContext.png!;;;","13/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveInspectors should use correct writable type to creaet ConstantObjectInspector,FLINK-28036,13449914,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,14/Jun/22 03:33,28/Jun/22 07:49,04/Jun/24 20:42,28/Jun/22 07:49,1.13.0,1.14.0,1.15.0,,,1.16.0,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,"In HiveInspectors, we will create Hive's ConstantObjectInspector by passing writable type such as ByteWritable, DoubleWritable, all of the writable class are classes of  package org.apache.hadoop.io. But the Hive's ConstantObjectInspector may require different class such as 

WritableConstantDoubleObjectInspector require class org.apache.hadoop.hive.serde2.io.DoubleWritable, which is class of package 

org.apache.hadoop.hive.serde2.io.DoubleWritable. Then when touch such code, it will throw ""no such method exception:  org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDoubleObjectinspector.<init>(org.apache.hadoop.io.DoubleWritable).

 

I found  ByteWritable, ShortWritable, DoubleWritable should be the class in package of `class org.apache.hadoop.hive.serde2.io` instead of `org.apache.hadoop.io`.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 28 07:48:57 UTC 2022,,,,,,,,,,"0|z137jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/22 07:48;jingzhang;fixed in master: 66d788484046c85d1d57a70c218120f8eb7c4a54;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support rescale overwrite,FLINK-28035,13449908,13443540,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,14/Jun/22 02:51,21/Jun/22 03:05,04/Jun/24 20:42,21/Jun/22 03:05,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"For an ordinary read-write job, the scan will check the numBuckets read from manifests against the current numBuckets, to avoid data corruption. See FLINK-27316.

 

However, this can be improved as follows.
 * If no new writes happen after changing the bucket number, the reads should not be blocked.
 * For rescale overwrite, we should support scan as the old bucket num, rescale and commit as the new bucket num.
 * The streaming job can be suspended and recovered from the rescaled data layout.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27316,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 03:05:04 UTC 2022,,,,,,,,,,"0|z137i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 06:29;lzljs3620320;Can you add some description?;;;","21/Jun/22 03:05;lzljs3620320;master: adc70cef2da6b1750fd19b5ef70e7e06e490a4fa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException occurred in creating a checkpoint with merge windows ,FLINK-28034,13449903,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,t-eimizu,t-eimizu,14/Jun/22 02:27,25/Aug/23 22:35,04/Jun/24 20:42,,1.15.0,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,auto-deprioritized-major,pull-request-available,,,"h1. Summary

In Flink 1.15.0, the combination of following functions always occur ClassCastException.
 - Session Window
 - Checkpoint
 - Keyed State

The following repository provides minimal source code that can combine these features to reproduce the exception.

[https://github.com/t-eimizu/flink-checkpoint-with-merging-window]

 
h1. Description
h2. How the Exception Occurred
 
In the process window function of the session window, we must use `context.globalState()`
instead of `context.windowState()`. If you use `context.windowState()` in this situation, Flink throws `UnsupportedOperationException`.
 
So we have to do following:
 
{code:java}
   stPreviousValue = context.globalState().getState(desc4PreviousValue);     {code}
 
Then stPreviousValue will have the following fields:
||Field Name||Value||
|currentNamespace|VoidNamespace|
|namespaceSerializer|TimeWindow$serializer|
As a result, when flink create checkpoint on this job, ClassCastException occurs.
{code:java}
2022-06-14 11:04:57,212 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - ProcessingData -> Sink: PrintData (1/1)#0 - asynchronous part of checkpoint 1 could not be completed. java.util.concurrent.ExecutionException: java.lang.ClassCastException: class org.apache.flink.runtime.state.VoidNamespace cannot be cast to class org.apache.flink.streaming.api.windowing.windows.TimeWindow (org.apache.flink.runtime.state.VoidNamespace and org.apache.flink.streaming.api.windowing.windows.TimeWindow are in unnamed module of loader 'app')     at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:?]     at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:?]     at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:645) ~[flink-core-1.15.0.jar:1.15.0]     at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:54) ~[flink-streaming-java-1.15.0.jar:1.15.0]     at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-streaming-java-1.15.0.jar:1.15.0]     at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) [flink-streaming-java-1.15.0.jar:1.15.0]     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]     at java.lang.Thread.run(Thread.java:834) [?:?] Caused by: java.lang.ClassCastException: class org.apache.flink.runtime.state.VoidNamespace cannot be cast to class org.apache.flink.streaming.api.windowing.windows.TimeWindow (org.apache.flink.runtime.state.VoidNamespace and org.apache.flink.streaming.api.windowing.windows.TimeWindow are in unnamed module of loader 'app')     at org.apache.flink.streaming.api.windowing.windows.TimeWindow$Serializer.serialize(TimeWindow.java:130) ~[flink-streaming-java-1.15.0.jar:1.15.0]     at org.apache.flink.runtime.state.heap.CopyOnWriteStateMapSnapshot.writeState(CopyOnWriteStateMapSnapshot.java:145) ~[flink-runtime-1.15.0.jar:1.15.0]     at org.apache.flink.runtime.state.heap.AbstractStateTableSnapshot.writeStateInKeyGroup(AbstractStateTableSnapshot.java:116) ~[flink-runtime-1.15.0.jar:1.15.0]     at org.apache.flink.runtime.state.heap.CopyOnWriteStateTableSnapshot.writeStateInKeyGroup(CopyOnWriteStateTableSnapshot.java:38) ~[flink-runtime-1.15.0.jar:1.15.0]     at org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.lambda$asyncSnapshot$3(HeapSnapshotStrategy.java:172) ~[flink-runtime-1.15.0.jar:1.15.0]     at org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:91) ~[flink-runtime-1.15.0.jar:1.15.0]     at org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:88) ~[flink-runtime-1.15.0.jar:1.15.0]     at org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:78) ~[flink-runtime-1.15.0.jar:1.15.0]     at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]     at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:642) ~[flink-core-1.15.0.jar:1.15.0]     ... 6 more  {code}
h2.  workaround
Turn off the checkpoint function.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18464,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri Aug 25 22:35:10 UTC 2023,,,,,,,,,,"0|z137gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 03:49;t-eimizu;I have already implemented a fix for this issue and will create a pull request shortly.;;;","14/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","15/Aug/22 00:16;t-eimizu;Can someone please review this issue and the solution?
Not being able to combine SessionWindow and Checkpoint should be a major problem.;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","25/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
find and output new min watermark mybe wrong when in multichannel,FLINK-28033,13449889,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ye-able,ye-able,14/Jun/22 01:02,30/Jun/22 02:44,04/Jun/24 20:42,,,,,,,,,,Runtime / Task,,,,,,,0,,,,,"File: StatusWatermarkValve.java

Method:  findAndOutputNewMinWatermarkAcrossAlignedChannels
{code:java}
//代码占位符
long newMinWatermark = Long.MAX_VALUE;
boolean hasAlignedChannels = false;

// determine new overall watermark by considering only watermark-aligned channels across all
// channels
for (InputChannelStatus channelStatus : channelStatuses) {
    if (channelStatus.isWatermarkAligned) {
        hasAlignedChannels = true;
        newMinWatermark = Math.min(channelStatus.watermark, newMinWatermark);
    }
}

// we acknowledge and output the new overall watermark if it really is aggregated
// from some remaining aligned channel, and is also larger than the last output watermark
if (hasAlignedChannels && newMinWatermark > lastOutputWatermark) {
    lastOutputWatermark = newMinWatermark;
    output.emitWatermark(new Watermark(lastOutputWatermark));
} {code}
 channelStatus's initalized watermark is Long.MIN_VALUE. when one channelStatus's watermark is changed,but other channelStatus's is not changed, the newMinWatermark is always Long.MIN_VALUE and output not emitwatermark。 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 30 02:44:17 UTC 2022,,,,,,,,,,"0|z137ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 01:16;ye-able;In my mind,it can do like this:
{code:java}
long newMinWatermark = Long.MAX_VALUE;
boolean hasAlignedChannels = false;

// determine new overall watermark by considering only watermark-aligned channels across all
// channels
for (InputChannelStatus channelStatus : channelStatuses) {
    if (channelStatus.isWatermarkAligned  
        && channelStatus.watermark != Long.MIN_VALUE) {
        hasAlignedChannels = true;
        newMinWatermark = Math.min(channelStatus.watermark, newMinWatermark);
    }
}

// we acknowledge and output the new overall watermark if it really is aggregated
// from some remaining aligned channel, and is also larger than the last output watermark
if (hasAlignedChannels && newMinWatermark > lastOutputWatermark) {
    lastOutputWatermark = newMinWatermark;
    output.emitWatermark(new Watermark(lastOutputWatermark));
}
{code}
I'm not sure is right, so i report  this question in Jira and find any help.;;;","19/Jun/22 18:42;Weijie Guo;hi [~ye-able] , there is no problem here, `newMinWatermark` is a local variable, it will be reset to the maximum value every time the method is called. When all input watermark advance, it will emit new watermark.;;;","30/Jun/22 02:44;ye-able;hi [~Weijie Guo] , i find this problem when in `over window`  always has some date don't output . so i thinked in this case : only one channel's watermark is advance and another channel's watermark don't change, this function's result  `newMinWatermark`  is not new min watermark ,that is the last min watermark , and changed channel's watermark is not emit always.   ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpointing hangs and times out with some jobs,FLINK-28032,13449877,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,pgandhijr,pgandhijr,13/Jun/22 22:41,15/Jun/22 05:59,04/Jun/24 20:42,14/Jun/22 06:49,1.14.3,,,,,,,,Runtime / Checkpointing,,,,,,,0,,,,,"We have noticed that Flink jobs hangs and eventually times out after 2 hours every time at the first checkpoint after it completes 15/23(65%) acknowledgments.  There is no cpu/record processing activity but yet there are a number of tasks reporting 100% back pressure.  It is peculiar to this job and slight modifications to this job.  We have created many Flink jobs in the past and never encountered the issue.  

Here are the things we tried to narrow down the problem
 * The job runs fine if checkpointing is disabled.
 * Increasing the number of task managers and parallelism to 2 seems to help the job complete.  However, it stalled again when we sent a larger data set.
 * Increased taskmanager memory from 4 GB to 16 GB and cpu from 1 to 4 but didn't help.
 * Sometimes restarting the job manager helps but at other times not.
 * Breaking up the job into smaller parts helps the job to finish.
 * Analyzed the the thread dump and it appears all threads are either in sleeping or wait state.

I have attached the task manager logs (including debug logs for checkpointing), thread dump, and screen shots of the job graph and stalled checkpoint.

Your help in resolving this issue is greatly appreciated.","Here are the environment details
 * Flink version 1.14.3
 * Running Kubernetes
 * Using RocksDB state backend.
 * Checkpoint storage is S3 storage using the Presto library
 * Exactly Once Semantics with unaligned checkpoints enabled.
 * Checkpoint timeout 2 hours
 * Maximum concurrent checkpoints is 1
 * Taskmanager CPU: 4, Slots: 1, Process Size: 12 GB
 * Using Kafka for input and output",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28031,FLINK-28030,FLINK-28029,,,,,,,,,,,,,,,,,,"13/Jun/22 22:40;pgandhijr;checkpoint snapshot.png;https://issues.apache.org/jira/secure/attachment/13045017/checkpoint+snapshot.png","13/Jun/22 22:40;pgandhijr;jobgraph.png;https://issues.apache.org/jira/secure/attachment/13045016/jobgraph.png","13/Jun/22 22:40;pgandhijr;taskmanager_10.112.55.143_6122-969889_log;https://issues.apache.org/jira/secure/attachment/13045019/taskmanager_10.112.55.143_6122-969889_log","13/Jun/22 22:40;pgandhijr;taskmanager_10.112.55.143_6122-969889_thread_dump;https://issues.apache.org/jira/secure/attachment/13045018/taskmanager_10.112.55.143_6122-969889_thread_dump",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 15 05:59:54 UTC 2022,,,,,,,,,,"0|z137b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 06:49;yunta;The thread dump on taskmanager does not have a good format for people to understand.
From my knowledge, the checkpoint lock was not guarantee by the task thread which leads to the job hangs.
Flink's jira issues is not a place to ask user questions, and slack or user mailing list is a better place to ask, you can find the information in the community info: https://flink.apache.org/community.html;;;","14/Jun/22 15:22;pgandhijr;Just for my understanding.  I created the ticket because the hang seemed obvious and appeared to be a Flink bug.  So is the procedure that only certain people can directly create tickets and regular users should use slack or mailing lists?;;;","15/Jun/22 05:59;yunta;[~pgandhijr] First of all, thanks for using Apache Flink and participated in the Flink community.

In general, we report a bug after we figure out the root cause. If we cannot find the root cause temporarily, this depends on the developing experiences on Flink project. 
For a broken or unstable unit test occurred in the CI, we can create the ticket to report the error directly so that related developer could search and then resolve.
For an unexpected behavior, if you cannot judge whether this is a bug caused by the flink framework itself. It's better to follow the order: ask in the mailing list and create related ticket if confirmed.
And for the problem you mentioned, from my experience, it should be related to the user code during sync phase of executing checkpoint.

In a word, we should avoid to use JIRA to ask questions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint always hangs when running some jobs,FLINK-28031,13449876,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,pgandhijr,pgandhijr,13/Jun/22 22:32,14/Jun/22 02:24,04/Jun/24 20:42,14/Jun/22 02:24,1.14.3,,,,,,,,Runtime / Checkpointing,,,,,,,0,,,,,"We have noticed that Flink jobs hangs and eventually times out after 2 hours every time at the first checkpoint after it completes 15/23 acknowledgments (65%).  There is no cpu activity but yet there are number of tasks reporting 100% back pressure.  It is peculiar to this job and slight modifications to this job.  We have created many Flink jobs in the past and never encountered the issue.  

Here are the things we tried to narrow down the problem
 * The job runs fine if checkpointing is disabled.
 * Increasing the number of task managers and parallelism to 2 seems to help the job complete.  However, it stalled again when we sent a larger data set.
 * Increased taskmanager memory from 4 GB to 16 GB and cpu from 1 to 4 but didn't help.
 * Sometimes restarting the job manager helps but at other times not.
 * Breaking up the job into smaller parts helps the job to finish.
 * Analyzed the the thread dump and it appears all threads are either in sleeping or wait state.

Here are the environment details
 * Flink version 1.14.3
 * Running Kubernetes
 * Using RocksDB state backend.
 * Checkpoint storage is S3 storage using the Presto library
 * Exactly Once Semantics with unaligned checkpoints enabled.
 * Checkpoint timeout 2 hours
 * Maximum concurrent checkpoints is 1
 * Taskmanager CPU: 4, Slots: 1, Process Size: 12 GB
 * Using Kafka for input and output

I have attached the task manager logs, thread dump, and screen shots of the job graph and stalled checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28032,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-13 22:32:42.0,,,,,,,,,,"0|z137aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint always hangs when running some jobs,FLINK-28030,13449875,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,pgandhijr,pgandhijr,13/Jun/22 22:32,14/Jun/22 02:24,04/Jun/24 20:42,14/Jun/22 02:24,1.14.3,,,,,,,,Runtime / Checkpointing,,,,,,,0,,,,,"We have noticed that Flink jobs hangs and eventually times out after 2 hours every time at the first checkpoint after it completes 15/23 acknowledgments (65%).  There is no cpu activity but yet there are number of tasks reporting 100% back pressure.  It is peculiar to this job and slight modifications to this job.  We have created many Flink jobs in the past and never encountered the issue.  

Here are the things we tried to narrow down the problem
 * The job runs fine if checkpointing is disabled.
 * Increasing the number of task managers and parallelism to 2 seems to help the job complete.  However, it stalled again when we sent a larger data set.
 * Increased taskmanager memory from 4 GB to 16 GB and cpu from 1 to 4 but didn't help.
 * Sometimes restarting the job manager helps but at other times not.
 * Breaking up the job into smaller parts helps the job to finish.
 * Analyzed the the thread dump and it appears all threads are either in sleeping or wait state.

Here are the environment details
 * Flink version 1.14.3
 * Running Kubernetes
 * Using RocksDB state backend.
 * Checkpoint storage is S3 storage using the Presto library
 * Exactly Once Semantics with unaligned checkpoints enabled.
 * Checkpoint timeout 2 hours
 * Maximum concurrent checkpoints is 1
 * Taskmanager CPU: 4, Slots: 1, Process Size: 12 GB
 * Using Kafka for input and output

I have attached the task manager logs, thread dump, and screen shots of the job graph and stalled checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28032,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-13 22:32:39.0,,,,,,,,,,"0|z137ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint always hangs when running some jobs,FLINK-28029,13449874,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,pgandhijr,pgandhijr,13/Jun/22 22:31,14/Jun/22 02:25,04/Jun/24 20:42,14/Jun/22 02:25,1.14.3,,,,,,,,Runtime / Checkpointing,,,,,,,0,,,,,"We have noticed that Flink jobs hangs and eventually times out after 2 hours every time at the first checkpoint after it completes 15/23 acknowledgments (65%).  There is no cpu activity but yet there are number of tasks reporting 100% back pressure.  It is peculiar to this job and slight modifications to this job.  We have created many Flink jobs in the past and never encountered the issue.  

Here are the things we tried to narrow down the problem
 * The job runs fine if checkpointing is disabled.
 * Increasing the number of task managers and parallelism to 2 seems to help the job complete.  However, it stalled again when we sent a larger data set.
 * Increased taskmanager memory from 4 GB to 16 GB and cpu from 1 to 4 but didn't help.
 * Sometimes restarting the job manager helps but at other times not.
 * Breaking up the job into smaller parts helps the job to finish.
 * Analyzed the the thread dump and it appears all threads are either in sleeping or wait state.

Here are the environment details
 * Flink version 1.14.3
 * Running Kubernetes
 * Using RocksDB state backend.
 * Checkpoint storage is S3 storage using the Presto library
 * Exactly Once Semantics with unaligned checkpoints enabled.
 * Checkpoint timeout 2 hours
 * Maximum concurrent checkpoints is 1
 * Taskmanager CPU: 4, Slots: 1, Process Size: 12 GB
 * Using Kafka for input and output

I have attached the task manager logs, thread dump, and screen shots of the job graph and stalled checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28032,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,2022-06-13 22:31:56.0,,,,,,,,,,"0|z137ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Common secure credential protection mechanism in Flink SQL,FLINK-28028,13449851,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,13/Jun/22 18:59,29/Nov/22 08:00,04/Jun/24 20:42,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,,,,0,,,,,"Currently, the most common way to use credential is to use:

 
CREATE TABLE mytable (
  ...
)
WITH (
  'connector' = 'kafka',
  'properties.bootstrap.servers' = '...:9092',
  'topic' = '...',
  'properties.ssl.keystore.password' = <password>,
  'properties.ssl.keystore.location' = ...,
  'properties.ssl.truststore.password' = <password>,
  'properties.ssl.truststore.location' = ...,
);

The credential could then be read by calling SHOW CREATE TABLE <TableName>. 

We should provide a more strong way to protect the credential. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 14 06:55:59 UTC 2022,,,,,,,,,,"0|z1375c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 06:55;martijnvisser;A potential solution was brought forward on the mailing list a while ago, see https://lists.apache.org/thread/ljn994s6xlzhz09ssxmynzotbw1mvt7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initialise Async Sink maximum number of in flight messages to low number for rate limiting strategy,FLINK-28027,13449836,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liangtl,CrynetLogistics,CrynetLogistics,13/Jun/22 16:28,11/Aug/22 10:11,04/Jun/24 20:42,11/Aug/22 10:11,1.15.0,1.15.1,,,,1.15.2,,,Connectors / Common,Connectors / Kinesis,,,,,,0,pull-request-available,,,,"*Background*

In the AsyncSinkWriter, we implement a rate limiting strategy.

The initial value for the maximum number of in flight messages is set extremely high ({{{}maxBatchSize * maxInFlightRequests{}}}).

However, in accordance with the AIMD strategy, the TCP implementation for congestion control has found a small value to start with [is better]([https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start]).

*Suggestion*

A better default might be:
 * maxBatchSize
 * maxBatchSize / parallelism",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28487,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 11 10:11:23 UTC 2022,,,,,,,,,,"0|z13720:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:20;Zsigner;Hi [~CrynetLogistics] ，When I use jdbc sink, I found that there are several parameters. I don't know if it can help you. I will provide you with reference.
{code:java}
JdbcExecutionOptions
.builder() 
.withBatchSize(4000) 
.withBatchIntervalMs(200) 
.withMaxRetries(3) 
.build()
{code}
 ;;;","10/Aug/22 14:32;dannycranmer;This is fixed by FLINK-28487 in 1.16. ;;;","11/Aug/22 10:11;dannycranmer;Merged commit [{{d22c52f}}|https://github.com/apache/flink/commit/d22c52f6dda6d5aec37f50b2657293157ca40d96] into apache:release-1.15 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add benchmark module for flink table store,FLINK-28026,13449822,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,Aiden Gong,Aiden Gong,Aiden Gong,13/Jun/22 14:32,09/Jan/23 03:13,04/Jun/24 20:42,09/Jan/23 03:13,,,,,,table-store-0.3.0,,,Table Store,,,,,,,0,pull-request-available,,,,Add benchmark module for flink table store.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 14 06:49:42 UTC 2022,,,,,,,,,,"0|z136yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 14:35;Aiden Gong;Hi, [~lzljs3620320] .I will use JMH to create benchmark module.Thank you!;;;","14/Jun/22 06:49;lzljs3620320;[~Aiden Gong] Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document the change of serviceAccount in upgrading doc of k8s opeator,FLINK-28025,13449814,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bgeng777,bgeng777,bgeng777,13/Jun/22 14:04,14/Jun/22 12:01,04/Jun/24 20:42,14/Jun/22 12:00,,,,,,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,"Since 1.0.0, we require users to specify serviceAccount and add corresponding validation.

According to the experience of [~czchen] , we had better document such change in the upgrading doc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 14 12:00:59 UTC 2022,,,,,,,,,,"0|z136x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 14:19;bgeng777;cc [~wangyang0918] ;;;","14/Jun/22 12:00;gyfora;merged:

main 8051ebd596147397480ab8445f871db572c957fe
release-1.0 ac50e32ac55b5b74b595ea0b9f796d6da74a37b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyedStateCheckpointingITCase.KeyedStateCheckpointingITCase ends up in infinite failover loop,FLINK-28024,13449803,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,mapohl,mapohl,mapohl,13/Jun/22 13:16,21/Jun/22 10:25,04/Jun/24 20:42,21/Jun/22 10:25,1.16.0,,,,,,,,Build System / Azure Pipelines,,,,,,,0,test-stability,,,,"-We observed several situations already where log files reached a file size of over 120G. This caused the worker's disk usage to reach 100% resulting in the worker machine to go ""offline"", i.e. not being available to pick up new tasks.-

The initially observed excessive log spilling is due to a TaskManager failing fatally which results in the requested number of slots never becoming available and the test job ending up in an infinite failover/restart loop. See further details in the comment section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28077,,,,,,,,,,FLINK-25374,FLINK-24433,,,,,,,,,"13/Jun/22 15:13;mapohl;testWithRocksDbBackendIncremental.log.gz;https://issues.apache.org/jira/secure/attachment/13045005/testWithRocksDbBackendIncremental.log.gz",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jun 21 10:25:31 UTC 2022,,,,,,,,,,"0|z136uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 13:22;mapohl;Initial suspicion was that it's excessive debug/trace output. The observed log file only contained 1371 debug messages. Not trace messages could be identified;;;","13/Jun/22 13:46;mapohl;The build in question seem to have stalled in {{{}org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.testWithRocksDbBackendIncremental{}}}. The following fragment gets repeated over and over again:
{code:java}
16:30:01,430 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No master state to restore
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_0_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_1_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_2_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_3_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_0_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_1_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_2_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_3_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_0_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_1_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_2_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_3_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_0_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_1_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_2_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_3_2) switched from CREATED to SCHEDULED.
16:30:01,431 [flink-akka.actor.default-dispatcher-45] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 25a853c8210c1d08c0560a1e512bb0c7: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
16:30:01,432 [flink-akka.actor.default-dispatcher-45] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 25a853c8210c1d08c0560a1e512bb0c7: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
16:30:01,432 [flink-akka.actor.default-dispatcher-45] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 25a853c8210c1d08c0560a1e512bb0c7: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
16:30:01,432 [flink-akka.actor.default-dispatcher-45] WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 25a853c8210c1d08c0560a1e512bb0c7. Free slots: 0
16:30:01,432 [flink-akka.actor.default-dispatcher-45] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 25a853c8210c1d08c0560a1e512bb0c7: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
16:30:01,433 [flink-akka.actor.default-dispatcher-45] WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 25a853c8210c1d08c0560a1e512bb0c7. Free slots: 0
16:30:01,433 [flink-akka.actor.default-dispatcher-38] WARN  org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge [] - Could not acquire the minimum required resources, failing slot requests. Acquired: [ResourceRequirement{resourceProfile=ResourceProfile{taskHeapMemory=512.000gb (549755813888 bytes), taskOffHeapMemory=512.000gb (549755813888 byte
s), managedMemory=6.000mb (6291456 bytes), networkMemory=32.000mb (33554432 bytes)}, numberOfRequiredSlots=2}]. Current slot pool status: Registered TMs: 1, registered slots: 2 free slots: 0
16:30:01,434 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_2_2) switched from SCHEDULED to FAILED on [unassigned resource].
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
16:30:01,435 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_2_2.
16:30:01,435 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task bc764cd8ddf7a0cff126f51c16239658_2.
16:30:01,436 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 16 tasks should be restarted to recover the failed task bc764cd8ddf7a0cff126f51c16239658_2.
16:30:01,436 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Streaming Job (25a853c8210c1d08c0560a1e512bb0c7) switched from state RUNNING to RESTARTING.
16:30:01,437 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_0_2) switched from SCHEDULED to CANCELING.
16:30:01,437 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_0_2) switched from CANCELING to CANCELED.
16:30:01,437 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_0_2.
16:30:01,437 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_1_2) switched from SCHEDULED to CANCELING.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_1_2) switched from CANCELING to CANCELED.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_1_2.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_1_2.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_0_2) switched from SCHEDULED to CANCELING.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_0_2) switched from CANCELING to CANCELED.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_0_2.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_2_2) switched from SCHEDULED to CANCELING.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_2_2) switched from CANCELING to CANCELED.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_2_2.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_2_2) switched from SCHEDULED to CANCELING.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_2_2) switched from CANCELING to CANCELED.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_2_2.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_3_2) switched from SCHEDULED to CANCELING.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_3_2) switched from CANCELING to CANCELED.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_3_2.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_3_2.
16:30:01,438 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_1_2) switched from SCHEDULED to CANCELING.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_1_2) switched from CANCELING to CANCELED.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_1_2.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_0_2) switched from SCHEDULED to CANCELING.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_0_2) switched from CANCELING to CANCELED.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_0_2.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_0_2) switched from SCHEDULED to CANCELING.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_0_2) switched from CANCELING to CANCELED.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_0_2.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_1_2) switched from SCHEDULED to CANCELING.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_1_2) switched from CANCELING to CANCELED.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_1_2.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_b27f31f3e3a199a9981d185a455185be_1_2.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_3_2) switched from SCHEDULED to CANCELING.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_3_2) switched from CANCELING to CANCELED.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_353a6b34b8b7f1c1d0fb4616d911049c_3_2.
16:30:01,439 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_3_2) switched from SCHEDULED to CANCELING.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_3_2) switched from CANCELING to CANCELED.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_3_2.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_3_2.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_2_2) switched from SCHEDULED to CANCELING.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_2_2) switched from CANCELING to CANCELED.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_2_2.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_1_2) switched from SCHEDULED to CANCELING.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (2/4) (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_1_2) switched from CANCELING to CANCELED.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_1_2.
16:30:01,440 [flink-akka.actor.default-dispatcher-45] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 25a853c8210c1d08c0560a1e512bb0c7: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_1_2.
16:30:01,440 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_3_2) switched from SCHEDULED to CANCELING.
16:30:01,441 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (4/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_3_2) switched from CANCELING to CANCELED.
16:30:01,441 [flink-akka.actor.default-dispatcher-45] WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 25a853c8210c1d08c0560a1e512bb0c7. Free slots: 0
16:30:01,441 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_3_2.
16:30:01,441 [flink-akka.actor.default-dispatcher-43] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 25a853c8210c1d08c0560a1e512bb0c7: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
16:30:01,441 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_3_2.
16:30:01,441 [flink-akka.actor.default-dispatcher-43] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 25a853c8210c1d08c0560a1e512bb0c7
16:30:01,442 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Streaming Job (25a853c8210c1d08c0560a1e512bb0c7) switched from state RESTARTING to RUNNING.
16:30:01,442 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 25a853c8210c1d08c0560a1e512bb0c7 from Checkpoint 1 @ 1654705506274 for 25a853c8210c1d08c0560a1e512bb0c7 located at <checkpoint-not-externally-addressable>.
16:30:01,444 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No master state to restore {code};;;","13/Jun/22 14:40;chesnay;It's just gonna be some test getting stuck for hours and repeatedly logging things.;;;","13/Jun/22 15:43;mapohl;Yes, that's what I figured. I'll keep this issue open for documentation purposes to see if it's happening repeatedly on the same test.;;;","15/Jun/22 08:58;mapohl;I linked FLINK-24433 and FLINK-25374. They are related in terms of CI workers running out of resource. But their cause seems to be a different one.;;;","21/Jun/22 10:07;mapohl;It appears that the TaskManager failed fatally in this test run indicated by the following log lines:
{code:java}
16:28:08,303 [Cancellation Watchdog for Source: Custom Source (2/4)#0 (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_1_0).] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Task 'Source: Custom Source (2/4)#0' did not react to cancelling signal - n
otifying TM; it is stuck for 180 seconds in method:
 java.lang.Object.wait(Native Method)
java.lang.Thread.join(Thread.java:1252)
java.lang.Thread.join(Thread.java:1326)
org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:166)
org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$1236/820205024.close(Unknown Source)
org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$1(Task.java:923)
org.apache.flink.runtime.taskmanager.Task$$Lambda$1951/1705350353.run(Unknown Source)
org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
java.lang.Thread.run(Thread.java:748)16:28:08,303 [Cancellation Watchdog for Source: Custom Source (2/4)#0 (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_1_0).] ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Task did not exit gracefully within 180 + seconds.
org.apache.flink.util.FlinkRuntimeException: Task did not exit gracefully within 180 + seconds.
        at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1778) [flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
16:28:08,303 [Cancellation Watchdog for Source: Custom Source (2/4)#0 (b8f79df70c20ee5cc51bd99bd6852bb8_feca28aff5a3958840bee985ee7de4d3_1_0).] ERROR org.apache.flink.runtime.minicluster.MiniCluster             [] - TaskManager #0 failed.
org.apache.flink.util.FlinkRuntimeException: Task did not exit gracefully within 180 + seconds.
        at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1778) [flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292] {code}

The job is set to have a parallelism of 4 with two slots per TaskManager and two TaskManager being available initially. After one TM shutdown fatally, we end up with only two slots that never satisfy the 4 required slots:
{code}
16:30:01,416 [flink-akka.actor.default-dispatcher-38] WARN  org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge [] - Could not acquire the minimum required resources, failing slot requests. Acquired: [ResourceRequirement{resourceProfile=ResourceProfile{taskHeapMemory=512.0
00gb (549755813888 bytes), taskOffHeapMemory=512.000gb (549755813888 bytes), managedMemory=6.000mb (6291456 bytes), networkMemory=32.000mb (33554432 bytes)}, numberOfRequiredSlots=2}]. Current slot pool status: Registered TMs: 1, registered slots: 2 free slots: 0
16:30:01,422 [flink-akka.actor.default-dispatcher-38] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source (3/4) (b8f79df70c20ee5cc51bd99bd6852bb8_bc764cd8ddf7a0cff126f51c16239658_2_1) switched from SCHEDULED to FAILED on [unassigned resource].
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
{code}

That's where we end up in the infinite loop of failover which spills the disk with logs.;;;","21/Jun/22 10:15;mapohl;[~roman] may you have a look at this? It looks like the thread in [ChannelstateWriteRequestExecutorImpl|https://github.com/apache/flink/blob/d1997b827a0e21308c57450dd7a6df1e8efa5bce/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequestExecutorImpl.java#L166] is not able to be stopped.;;;","21/Jun/22 10:17;chesnay;[~mapohl] that's already covered by https://issues.apache.org/jira/browse/FLINK-28077.;;;","21/Jun/22 10:22;mapohl;Good pointer. Especially considering that the other issue was created by me. :D Thanks, [~chesnay];;;","21/Jun/22 10:25;mapohl;I'm closing this issue since was initially about investigating the excessive disk usage. The actually issue will be covered by FLINK-28077;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Native Kubernetes"" page into Chinese",FLINK-28023,13449800,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,linqichen178,linqichen178,13/Jun/22 12:51,14/Mar/23 09:34,04/Jun/24 20:42,,1.14.4,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,"Hi,  I think it is very urgent to translate 'native kubernetes' page into chinese. 

Can you assign this to me ??",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Mar 14 09:34:57 UTC 2023,,,,,,,,,,"0|z136u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 08:39;Weijie Guo;[~linqichen178] Sorry for the delay, would you like to do this now?;;;","14/Mar/23 09:34;Wencong Liu;cc [~linqichen178] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Google Cloud PubSub connector in Python DataStream API,FLINK-28022,13449791,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pemide,pemide,pemide,13/Jun/22 12:17,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,,1.20.0,,,API / Python,Connectors / Google Cloud PubSub,,,,,,0,pull-request-available,stale-assigned,,,Support Google Cloud PubSub connector in Python DataStream API,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Aug 17 10:35:13 UTC 2023,,,,,,,,,,"0|z136s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 12:18;pemide;[~dianfu] Cound I take this ticket?;;;","13/Jun/22 12:24;dianfu;[~pemide] Done~;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add FLIP-33 metrics to FileSystem connector,FLINK-28021,13449766,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,shubham.bansal,martijnvisser,martijnvisser,13/Jun/22 09:34,19/Jun/22 02:10,04/Jun/24 20:42,,,,,,,,,,Connectors / FileSystem,,,,,,,1,,,,,Both the current FileSource and FileSink have no metrics implemented. They should have the FLIP-33 metrics implemented. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28117,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Sun Jun 19 02:10:37 UTC 2022,,,,,,,,,,"0|z136mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 09:18;shubham.bansal;As [~jackwangcs] has retracted his comment, Can I take this up if Jack is not picking it up?
 
 
 ;;;","17/Jun/22 09:31;martijnvisser;[~shubham.bansal] I've assigned it to you;;;","17/Jun/22 09:32;martijnvisser;[~shubham.bansal] I think that both https://github.com/apache/flink/pull/16838 and https://github.com/apache/flink/pull/16875 could potentially help out, since those were the PRs that implemented these for Kafka (both Source and Sink);;;","17/Jun/22 22:24;jingge;[~shubham.bansal] you could find more information w.r.t. how to build metric in file connector at: https://github.com/apache/flink/pull/19120;;;","18/Jun/22 08:25;shubham.bansal;Thanks. Taking a look.;;;","19/Jun/22 02:10;shubham.bansal;[~jingge] I looked at the code and understood what is required and it's different from Kafka metrics as the Kafka client provides most of the metrics that Flink is looking for, but FileWriterBucket used by file connector doesn't. It accepts the element to be inserted and asks InProgressFileWriter's like BulkPartWriter, RowWisePartWriter, and HadoopPathBasedPartFileWriter to write that element using the corresponding encoders or Cvs/Avro/Orc writers which don't really provide the length of the encoded element by default. So if we need to calculate the bytes sent as mentioned in FLIP-33, then we need to make changes in those Writers and propagate those encoded sizes to the FileWriterBucket.
This is what I could figure out. Let me know what you think.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor Flink connector for table store with FileStoreTable,FLINK-28020,13449759,13447187,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,13/Jun/22 09:03,13/Jun/22 09:41,04/Jun/24 20:42,13/Jun/22 09:41,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"We've prepared {{FileStoreTable}} for {{RowData}} reading and writing, so it's time to refactor Flink connector for table store with {{FileStoreTable}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 13 09:41:48 UTC 2022,,,,,,,,,,"0|z136kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 09:41;lzljs3620320;master: 5fd771aaca5c5b196cb246478c5b305c0be1a6ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in RetractableTopNFunction when retracting a stale record with state ttl enabled,FLINK-28019,13449748,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,13/Jun/22 08:32,22/Jun/22 09:57,04/Jun/24 20:42,16/Jun/22 06:53,1.14.4,1.15.0,,,,1.14.6,1.15.1,1.16.0,,,,,,,,0,pull-request-available,,,,"We found an error occurred when retract a staled record when enable state ttl in RetractableTopNFunction, a reproduce case:
{code}
    @Test
    public void testRetractAnStaledRecordWithRowNumber() throws Exception {
        StateTtlConfig ttlConfig = StateConfigUtil.createTtlConfig(1_000);
        AbstractTopNFunction func =
                new RetractableTopNFunction(
                        ttlConfig,
                        InternalTypeInfo.ofFields(
                                VarCharType.STRING_TYPE, new BigIntType(), new IntType()),
                        comparableRecordComparator,
                        sortKeySelector,
                        RankType.ROW_NUMBER,
                        new ConstantRankRange(1, 2),
                        generatedEqualiser,
                        true,
                        true);

        OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(func);
        testHarness.open();
        testHarness.setStateTtlProcessingTime(0);
        testHarness.processElement(insertRecord(""a"", 1L, 10));
        testHarness.setStateTtlProcessingTime(1001);
        testHarness.processElement(insertRecord(""a"", 2L, 11));
        testHarness.processElement(deleteRecord(""a"", 1L, 10));
        testHarness.close();

        List<Object> expectedOutput = new ArrayList<>();
        expectedOutput.add(insertRecord(""a"", 1L, 10, 1L));
        expectedOutput.add(insertRecord(""a"", 2L, 11, 1L));
        // the following delete record should not be sent because the left row is null which is
        // illegal.
        // -D{row1=null, row2=+I(1)};

        assertorWithRowNumber.assertOutputEquals(
                ""output wrong."", expectedOutput, testHarness.getOutput());
    }
{code}

the reason is the uncomplete path when deal with staled records.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jun 16 06:53:34 UTC 2022,,,,,,,,,,"0|z136ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 06:53;lzljs3620320;master: c4d4bb5c28d5319fe567b31464683e3f5f22ba67

release-1.14: 5dbb51c09e0d810eabbdc2f4c0f4045dee5be519

release-1.15: 921b608158288bc807493e1c425f6d7ec6f47b18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the start index to create empty splits in BinaryInputFormat#createInputSplits is inappropriate,FLINK-28018,13449747,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Leo Zhou,Leo Zhou,Leo Zhou,13/Jun/22 08:32,15/Jun/22 03:05,04/Jun/24 20:42,15/Jun/22 03:05,1.14.4,1.15.0,1.16.0,,,1.15.1,1.16.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,"when the number of created split is smaller than the minimum desired number of file splits, [BinaryInputFormat.java#L150|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/io/BinaryInputFormat.java#L150] use `{_}*files.size()*{_}` as the start index to create empty splits. That is inappropriate, the start index should be `{_}*inputSplits.size()*{_}`.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Wed Jun 15 03:05:43 UTC 2022,,,,,,,,,,"0|z136i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 08:34;Leo Zhou;Hi [~zhuzh] , can you take a look ?;;;","13/Jun/22 09:22;zhuzh;Yes it is a bug. Thanks for reporting this! [~Leo Zhou] 

Do you want to fix it?;;;","13/Jun/22 09:27;Leo Zhou;Thanks for confirming this [~zhuzh] , I'd like to fix it.;;;","13/Jun/22 09:31;zhuzh;I have assigned you the ticket. Feel free to open a fix for it.;;;","15/Jun/22 03:05;zhuzh;Fixed via:

master: 0cf8208ede55097987abb243874d670ed5f504ae

1.15: c9a706b8388b324a37da43298e37074d0a452a34;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce bucket-key to table store,FLINK-28017,13449743,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Jun/22 08:18,05/Jul/22 07:48,04/Jun/24 20:42,05/Jul/22 07:48,,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,pull-request-available,,,,"Specifies the table store distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.
 * It is primary key when table has primary key. The user can specify a bucket key, it should be part of primary keys.
 * It is all fields when table has no primary key.

If there are filter conditions for specific fields, reasonable settings can give a big performance boost to the table, but care needs to be taken to avoid data skewing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Tue Jul 05 07:48:02 UTC 2022,,,,,,,,,,"0|z136hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 07:48;lzljs3620320;master: 907747d3e37b7edb5c1bec5209e47bd4ff9f737b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Maven 3.3+,FLINK-28016,13449741,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,13/Jun/22 08:13,23/Jan/24 10:34,04/Jun/24 20:42,12/May/23 13:43,,,,,,1.18.0,,,Build System,,,,,,,1,,,,,"We are currently de-facto limited to Maven 3.2.5 because our packaging relies on the shade-plugin modifying the dependency tree at runtime when bundling dependencies, which is no longer possible on Maven 3.3+.

Being locked in to such an old Maven version isn't a good state to be in, and the contributor experience suffers as well.

I've been looking into removing this limitation by explicitly marking every dependency that we bundle as {{optional}} in the poms, which really means {{non-transitive}}. This ensures that the everything being bundled by one module is not visible to other modules. Some tooling to capture developer mistakes were also written.

Overall this is actually quite a nice change, as it makes things more explicit and reduces inconsistencies (e.g., the dependency plugin results are questionable if the shade-plugin didn't run!); and it already highlighted several problems in Flink.

This change will have no effect on users or the released poms, because the dependency-reduced poms will be generated as before and remove all modified dependencies.",,,,,,,,,,,FLINK-28194,FLINK-30089,FLINK-28260,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34148,FLINK-32734,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Fri May 12 13:43:59 UTC 2023,,,,,,,,,,"0|z136gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 06:42;wangyang0918;Thanks [~chesnay] for improving the build system. It seems that the maven 3.8.5 could work correctly by picking the reduced dependency pom. But it is true that at least maven 3.3.9 still could not work correctly. I am not sure when or whether they have fixed this.;;;","16/Jun/22 08:13;chesnay;> It seems that the maven 3.8.5 could work correctly by picking the reduced dependency pom.

AFAIK that is not possible.;;;","16/Jun/22 08:39;jingge;Hi [~chesnay] just want to make sure we are on the same page. If we will use optional to solve the issue, does it mean that we could jump to support the most up-to-date maven 3.8.6?;;;","16/Jun/22 09:06;chesnay;yes; you would be able to use _any_ maven version of your choosing.;;;","16/Jun/22 11:16;wangyang0918;[~chesnay] Could you share me some clues which classes will be wrongly bundled into the flink-dist uber jar when using maven 3.3+ versions?

 

I just do a simple manual verification in a testing project with multiple modules by using 3.8.5 and could find the reduced dependency pom is picked up when building the uber jar.;;;","16/Jun/22 12:05;chesnay;[~wangyang0918] One example are the dependencies bundled&relocated in flink-kubernetes. They will appear again in flink-dist without a relocation.

More generally, any time a module bundles dependencies, and other module will still see the original dependencies. This _may_ result in more stuff being bundled (e.g., flink-dist again bundles all dependencies from flink-kubernetes), more dependencies being on the classpath (e.g., during testing), or the version of dependencies being changed.

Note that you naturally only see this if both modules were build in the same run. If you just build flink-dist there's no problem (hence why this is the documented workaround).;;;","17/Jun/22 03:20;wangyang0918;[~chesnay] I appreciate for your detailed explanation. Just like what you have said, the non-relocation transitive dependencies of flink-kubernetes will bundled into the flink-dist again when using maven 3.3.9. The following output could prove this.
{code:java}
$ ~/Downloads/apache-maven-3.3.9/bin/mvn clean install -DskipTests
$ unzip -l build-target/lib/flink-dist-1.16-SNAPSHOT.jar | grep 'dk.brics.automaton'


        0  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/
    17519  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/Automaton.class
     2626  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/AutomatonMatcher.class
      282  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/AutomatonProvider.class
     8324  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/BasicAutomata.class
    11065  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/BasicOperations.class
    21514  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/Datatypes.class
      839  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/DatatypesAutomatonProvider.class
      390  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/MinimizationOperations$IntPair.class
      956  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/MinimizationOperations$StateList.class
     1181  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/MinimizationOperations$StateListNode.class
     7440  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/MinimizationOperations.class
     1506  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/RegExp$1.class
     1964  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/RegExp$Kind.class
    13840  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/RegExp.class
     4523  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/RunAutomaton.class
     2587  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/ShuffleOperations$ShuffleConfiguration.class
     7519  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/ShuffleOperations.class
    11175  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/SpecialOperations.class
     3566  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/State.class
     1079  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/StatePair.class
      830  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/StringUnionOperations$1.class
     3051  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/StringUnionOperations$State.class
     4562  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/StringUnionOperations.class
     2232  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/Transition.class
     1247  06-16-2022 19:40   org/apache/flink/kubernetes/shaded/dk/brics/automaton/TransitionComparator.class
        0  06-16-2022 19:40   dk/brics/automaton/
    14740  06-16-2022 19:40   dk/brics/automaton/Automaton.class
     2486  06-16-2022 19:40   dk/brics/automaton/AutomatonMatcher.class
      212  06-16-2022 19:40   dk/brics/automaton/AutomatonProvider.class
     7202  06-16-2022 19:40   dk/brics/automaton/BasicAutomata.class
     9422  06-16-2022 19:40   dk/brics/automaton/BasicOperations.class
    20524  06-16-2022 19:40   dk/brics/automaton/Datatypes.class
      699  06-16-2022 19:40   dk/brics/automaton/DatatypesAutomatonProvider.class
      320  06-16-2022 19:40   dk/brics/automaton/MinimizationOperations$IntPair.class
      676  06-16-2022 19:40   dk/brics/automaton/MinimizationOperations$StateList.class
      901  06-16-2022 19:40   dk/brics/automaton/MinimizationOperations$StateListNode.class
     6430  06-16-2022 19:40   dk/brics/automaton/MinimizationOperations.class
     1334  06-16-2022 19:40   dk/brics/automaton/RegExp$1.class
     1728  06-16-2022 19:40   dk/brics/automaton/RegExp$Kind.class
    11770  06-16-2022 19:40   dk/brics/automaton/RegExp.class
     4001  06-16-2022 19:40   dk/brics/automaton/RunAutomaton.class
     2038  06-16-2022 19:40   dk/brics/automaton/ShuffleOperations$ShuffleConfiguration.class
     6089  06-16-2022 19:40   dk/brics/automaton/ShuffleOperations.class
     9255  06-16-2022 19:40   dk/brics/automaton/SpecialOperations.class
     3041  06-16-2022 19:40   dk/brics/automaton/State.class
      764  06-16-2022 19:40   dk/brics/automaton/StatePair.class
      763  06-16-2022 19:40   dk/brics/automaton/StringUnionOperations$1.class
     2737  06-16-2022 19:40   dk/brics/automaton/StringUnionOperations$State.class
     3796  06-16-2022 19:40   dk/brics/automaton/StringUnionOperations.class
     1987  06-16-2022 19:40   dk/brics/automaton/Transition.class {code}
 

 

However, when I use maven 3.8.5 to do the same thing. I find that the non-relocation dependencies are *NOT* bundled into the flink-dist uber jar. 

 
{code:java}
$ ~/Downloads/apache-maven-3.8.5/bin/mvn clean install -DskipTests
$ unzip -l build-target/lib/flink-dist-1.16-SNAPSHOT.jar | grep 'dk.brics.automaton'

        0  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/
    17519  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/Automaton.class
     2626  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/AutomatonMatcher.class
      282  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/AutomatonProvider.class
     8324  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/BasicAutomata.class
    11065  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/BasicOperations.class
    21514  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/Datatypes.class
      839  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/DatatypesAutomatonProvider.class
      390  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/MinimizationOperations$IntPair.class
      956  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/MinimizationOperations$StateList.class
     1181  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/MinimizationOperations$StateListNode.class
     7440  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/MinimizationOperations.class
     1506  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/RegExp$1.class
     1964  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/RegExp$Kind.class
    13840  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/RegExp.class
     4523  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/RunAutomaton.class
     2587  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/ShuffleOperations$ShuffleConfiguration.class
     7519  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/ShuffleOperations.class
    11175  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/SpecialOperations.class
     3566  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/State.class
     1079  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/StatePair.class
      830  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/StringUnionOperations$1.class
     3051  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/StringUnionOperations$State.class
     4562  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/StringUnionOperations.class
     2232  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/Transition.class
     1247  06-17-2022 11:13   org/apache/flink/kubernetes/shaded/dk/brics/automaton/TransitionComparator.class {code}
 

Please note that I completely build all the modules at the same time for both above testing. I am not sure what I have missed in the testing.

 ;;;","17/Jun/22 07:06;chesnay;Not sure either, but I built it with Maven 3.8.6 and it works as I said it would.

This is also a well known behavior, see MNG-5899.;;;","17/Jun/22 10:05;wangyang0918;The maven 3.8.6 works as you said.;;;","17/Jun/22 10:15;chesnay;Then it may just be a bug in 3.8.5.;;;","12/May/23 13:43;chesnay;master: f0d01903aaa517af9c1be26b1244a778189dce65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FROM_UNIXTIME could not be used in Table API,FLINK-28015,13449724,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,dianfu,dianfu,13/Jun/22 05:19,07/Jul/22 11:13,04/Jun/24 20:42,07/Jul/22 11:13,,,,,,1.16.0,,,Table SQL / API,,,,,,,0,pull-request-available,,,,"This issue is reported in [slack|https://apache-flink.slack.com/archives/C03G7LJTS2G/p1655083223954149]. 

For the following code:
{code}
input_table = table_env.from_path(input_table_name)
    sliding_window_table = (
        input_table.window(
            Slide.over(sliding_window_over)
            .every(sliding_window_every)
            .on(sliding_window_on)
            .alias(sliding_window_alias)
        )
        .group_by('ticker, {}'.format(sliding_window_alias))
        .select('FROM_UNIXTIME(28*60 * (UNIX_TIMESTAMP({0}.end) / (28*60))), ticker, MIN(price) as min_price, MAX(price) as max_price, {0}.start as utc_start, {0}.end as utc_end'.format(
            sliding_window_alias
        ))
    )
{code}

The following exception will be thrown:
{code}
py4j.protocol.Py4JJavaError: An error occurred while calling o75.select.
: org.apache.flink.table.api.ValidationException: Undefined function: FROM_UNIXTIME
	at org.apache.flink.table.expressions.resolver.LookupCallResolver.lambda$visit$0(LookupCallResolver.java:53)
	at java.base/java.util.Optional.orElseThrow(Optional.java:408)
	at org.apache.flink.table.expressions.resolver.LookupCallResolver.visit(LookupCallResolver.java:49)
	at org.apache.flink.table.expressions.resolver.LookupCallResolver.visit(LookupCallResolver.java:36)
	at org.apache.flink.table.expressions.ApiExpressionVisitor.visit(ApiExpressionVisitor.java:35)
	at org.apache.flink.table.expressions.LookupCallExpression.accept(LookupCallExpression.java:66)
	at org.apache.flink.table.api.internal.TableImpl.lambda$preprocessExpressions$0(TableImpl.java:605)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)
	at org.apache.flink.table.api.internal.TableImpl.preprocessExpressions(TableImpl.java:606)
	at org.apache.flink.table.api.internal.TableImpl.access$300(TableImpl.java:66)
	at org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:775)
	at org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:770)
{code}",,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Thu Jul 07 11:13:19 UTC 2022,,,,,,,,,,"0|z136d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 11:13;dianfu;Merged to master via 12615d210af90613cd380c6ff779423faeb42129;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extract snapshot related methods to SnapshotManager,FLINK-28014,13449707,13447187,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,13/Jun/22 01:50,13/Jun/22 02:27,04/Jun/24 20:42,13/Jun/22 02:27,table-store-0.2.0,,,,,table-store-0.2.0,,,Table Store,,,,,,,0,,,,,"Currently snapshot related methods are scattered in multiple classes (including {{FileStorePathFactory}}, {{FileStoreScan}} and {{SnapshotFinder}}).

We should extract them into a dedicated {{SnapshotManager}} class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,,Mon Jun 13 02:27:57 UTC 2022,,,,,,,,,,"0|z1369c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 02:27;lzljs3620320;master: b28ccbfe4fda3ddaccbea4489dad84b2e003350a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
