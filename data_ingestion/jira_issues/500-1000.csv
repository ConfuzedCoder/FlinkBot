Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Outward issue link (Blocked),Outward issue link (Blocked),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Outward issue link (Required),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Support temporary model,FLINK-35013,13574675,13574379,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lihaosky,lihaosky,04/Apr/24 17:52,04/Apr/24 17:52,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-04 17:52:12.0,,,,,,,,,,"0|z1og4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogNormalizeRestoreTest.testRestore failure,FLINK-35012,13574665,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,04/Apr/24 16:07,30/May/24 01:58,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58716&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11921

{code}
Apr 03 22:57:43 22:57:43.159 [ERROR] Failures: 
Apr 03 22:57:43 22:57:43.160 [ERROR]   ChangelogNormalizeRestoreTest>RestoreTestBase.testRestore:337 
Apr 03 22:57:43 Expecting actual:
Apr 03 22:57:43   [""+I[two, 2, b]"",
Apr 03 22:57:43     ""+I[one, 1, a]"",
Apr 03 22:57:43     ""+I[three, 3, c]"",
Apr 03 22:57:43     ""-U[one, 1, a]"",
Apr 03 22:57:43     ""+U[one, 1, aa]"",
Apr 03 22:57:43     ""-U[three, 3, c]"",
Apr 03 22:57:43     ""+U[three, 3, cc]"",
Apr 03 22:57:43     ""-D[two, 2, b]"",
Apr 03 22:57:43     ""+I[four, 4, d]"",
Apr 03 22:57:43     ""+I[five, 5, e]"",
Apr 03 22:57:43     ""-U[four, 4, d]"",
Apr 03 22:57:43     ""+U[four, 4, dd]""]
Apr 03 22:57:43 to contain exactly in any order:
Apr 03 22:57:43   [""+I[one, 1, a]"",
Apr 03 22:57:43     ""+I[two, 2, b]"",
Apr 03 22:57:43     ""-U[one, 1, a]"",
Apr 03 22:57:43     ""+U[one, 1, aa]"",
Apr 03 22:57:43     ""+I[three, 3, c]"",
Apr 03 22:57:43     ""-D[two, 2, b]"",
Apr 03 22:57:43     ""-U[three, 3, c]"",
Apr 03 22:57:43     ""+U[three, 3, cc]"",
Apr 03 22:57:43     ""+I[four, 4, d]"",
Apr 03 22:57:43     ""+I[five, 5, e]"",
Apr 03 22:57:43     ""-U[four, 4, d]"",
Apr 03 22:57:43     ""+U[four, 4, dd]"",
Apr 03 22:57:43     ""+I[six, 6, f]"",
Apr 03 22:57:43     ""-D[six, 6, f]""]
Apr 03 22:57:43 but could not find the following elements:
Apr 03 22:57:43   [""+I[six, 6, f]"", ""-D[six, 6, f]""]
Apr 03 22:57:43 
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33421,,FLINK-34513,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 01:58:38 UTC 2024,,,,,,,,,,"0|z1og28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Apr/24 08:46;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59173&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=11604;;;","29/Apr/24 11:46;rmetzger;[~bvarghese] it looks like you've ""recently"" added this test. Could you take a look at the instability?
;;;","27/May/24 15:16;rskraba;* 1.20 AdaptiveScheduler / Test (module: table) https://github.com/apache/flink/actions/runs/9239908683/job/25419731096#step:10:10621;;;","30/May/24 01:58;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59920&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11678;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The change in visibility of MockDeserializationSchema cause compilation failure in kafka connector,FLINK-35011,13574662,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabaosun,jiabaosun,jiabaosun,04/Apr/24 15:53,06/Apr/24 12:43,04/Jun/24 20:40,06/Apr/24 12:43,1.20.0,,,,,,1.20.0,,,,Tests,,,,,0,pull-request-available,,,"Flink Kafka connector can't compile with 1.20-SNAPSHOT, see https://github.com/apache/flink-connector-kafka/actions/runs/8553981349/job/23438292087?pr=90#step:15:165

Error message is:

{code}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-kafka: Compilation failure
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBaseTest.java:[60,39] org.apache.flink.streaming.util.MockDeserializationSchema is not public in org.apache.flink.streaming.util; cannot be accessed from outside package
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25544,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 06 12:43:53 UTC 2024,,,,,,,,,,"0|z1og1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/24 12:43;jiabaosun;Fixed via master: 3590c2d86f4186771ffcd64712f756d31306eb88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.apache.commons:commons-compress from 1.24.0 to 1.26.1 for Flink Mongodb connector,FLINK-35010,13574636,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gongzhongqiang,gongzhongqiang,gongzhongqiang,04/Apr/24 13:50,09/Apr/24 06:42,04/Jun/24 20:40,09/Apr/24 06:39,,,,,,,mongodb-1.2.0,,,,Connectors / MongoDB,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 06:39:28 UTC 2024,,,,,,,,,,"0|z1ofvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/24 13:50;gongzhongqiang;[~Leonard] I'm willing to take this issue.;;;","07/Apr/24 05:54;jiabaosun;I think we should bump commons-compress version to 1.26.1 due to https://issues.apache.org/jira/browse/COMPRESS-659.;;;","07/Apr/24 06:32;gongzhongqiang;+1 , It's better to bump commons-compress version to 1.26.1.;;;","08/Apr/24 04:47;jiabaosun;I have rechecked the dependency of `commons-codec` in `commons-compress` and it is no longer optional. 
Even if upgraded to 1.26.1, `commons-codec` will still be a transitive dependency. 
Please ignore the previous noise, sorry for the disturbance.;;;","09/Apr/24 06:39;jiabaosun;Fixed via mongodb-connector main: ee1146dadf73e91ecb7a2b28cfa879e7fe3b3f22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change on getTransitivePredecessors breaks connectors,FLINK-35009,13574633,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,martijnvisser,martijnvisser,martijnvisser,04/Apr/24 13:33,23/Apr/24 07:01,04/Jun/24 20:40,23/Apr/24 07:01,1.18.2,1.19.1,1.20.0,,,,,,,,API / Core,Connectors / Kafka,,,,0,,,,"{code:java}
Error:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-kafka: Compilation failure: Compilation failure: 
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/DataGenerators.java:[214,24] org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.InfiniteStringsGenerator.MockTransformation is not abstract and does not override abstract method getTransitivePredecessorsInternal() in org.apache.flink.api.dag.Transformation
Error:  /home/runner/work/flink-connector-kafka/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/testutils/DataGenerators.java:[220,44] getTransitivePredecessors() in org.apache.flink.streaming.connectors.kafka.testutils.DataGenerators.InfiniteStringsGenerator.MockTransformation cannot override getTransitivePredecessors() in org.apache.flink.api.dag.Transformation
Error:    overridden method is final
{code}

Example: https://github.com/apache/flink-connector-kafka/actions/runs/8494349338/job/23269406762#step:15:167

",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32513,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 07:01:44 UTC 2024,,,,,,,,,,"0|z1ofv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 02:13;Weijie Guo;This change can be done in a compatible way. But one could argue that {{Transformation}} is not actually a public API(marked as {{@Internal}} indeed). The connector depend on it somehow shouldn't be guaranteed. 

BTW: The {{MockTransformation}} in kafka-connector seems unused and can be removed safely.;;;","08/Apr/24 06:32;zhuzh;Thanks for looking into the issue. [~Weijie Guo]
So it is just a test class which uses Flink `@Internal` classes is broken. And that test class is even not used. 
I think it's better to just remove `MockTransformation` from kafka connector.
WDYT? [~martijnvisser];;;","08/Apr/24 07:25;martijnvisser;[~zhuzh] From a quick glance, it indeed looks like {{code:java}}MockTransformation{{code}} isn't used in the Flink Kafka connector anymore, so then it should be safe to delete it. Let me put up a PR and see if it fixes the situation, and if so, I'll close this ticket. Apologies for the inconvenience. ;;;","10/Apr/24 03:07;zhuzh;[~martijnvisser] Thanks for monitoring the kafka connector builds and reporting this problem!
Feel free to loop me in to review the pr.;;;","23/Apr/24 07:01;martijnvisser;Closing this as Won't Fix, since we'll change the Flink Kafka connector for this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.apache.commons:commons-compress from 1.25.0 to 1.26.1 for Flink Kafka connector,FLINK-35008,13574622,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,04/Apr/24 12:01,15/Apr/24 11:59,04/Jun/24 20:40,11/Apr/24 11:46,,,,,,,kafka-3.2.0,kafka-4.0.0,,,Connectors / Kafka,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35113,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 11:46:36 UTC 2024,,,,,,,,,,"0|z1ofso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/24 05:47;jiabaosun;Hi [~martijnvisser], maybe we should bump the commons-compress version to 1.26.1.
In version 1.26.0, there should not be a dependency on commons-codec.

see: https://issues.apache.org/jira/browse/COMPRESS-659;;;","07/Apr/24 05:52;jiabaosun;Due to an incorrect dependency on the Charsets class in the commons-codec package in TarArchiveOutputStream, it is necessary to include the commons-codec dependency to avoid compilation errors. 
This issue has been fixed in version 1.26.1 of commons-compress.

https://github.com/GOODBOY008/flink-connector-mongodb/actions/runs/8557577952/job/23450146047#step:15:11104
{code}
Caused by: java.lang.RuntimeException: Failed to build JobManager image
	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:67)
	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configure(FlinkTestcontainersConfigurator.java:147)
	at org.apache.flink.connector.testframe.container.FlinkContainers$Builder.build(FlinkContainers.java:197)
	at org.apache.flink.tests.util.mongodb.MongoE2ECase.<clinit>(MongoE2ECase.java:90)
	... 56 more
Caused by: org.apache.flink.connector.testframe.container.ImageBuildException: Failed to build image ""flink-configured-jobmanager""
	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:234)
	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:65)
	... 59 more
Caused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/commons/codec/Charsets
	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)
	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:255)
	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:206)
	... 60 more
Caused by: java.lang.NoClassDefFoundError: org/apache/commons/codec/Charsets
	at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.<init>(TarArchiveOutputStream.java:212)
	at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.<init>(TarArchiveOutputStream.java:157)
	at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.<init>(TarArchiveOutputStream.java:147)
	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:129)
	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:40)
	at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17)
	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.codec.Charsets
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 11 more
{code}
;;;","08/Apr/24 04:46;jiabaosun;I have rechecked the dependency of `commons-codec` in `commons-compress` and it is no longer optional. 
Even if upgraded to 1.26.1, `commons-codec` will still be a transitive dependency. 
Please ignore the previous noise, sorry for the disturbance.;;;","09/Apr/24 05:41;Sergey Nuyanzin;I think it would make sense to go with 1.26.1

I faced https://issues.apache.org/jira/browse/COMPRESS-659 while doing same for jdbc connector FLINK-35057


https://github.com/apache/flink-connector-jdbc/actions/runs/8602243084/job/23571361570#step:15:266


 ;;;","09/Apr/24 05:54;jiabaosun;I agree with Sergey's opinion. 

In version 1.26.0, the dependency of commons-codec is optional and the dependency error of COMPRESS-659 cause CI failure. 
To avoid this error, we have to explicitly add the dependency of commons-codec. 

Although the issue of COMPRESS-659 import error has been fixed in version 1.26.1, the dependency of commons-codec has changed to a non-optional transitive dependency which is not necessary.
Using version 1.26.1, we don't need to explicitly declare the dependency of commons-codec, which may be better than using version 1.26.0.;;;","11/Apr/24 11:27;martijnvisser;I've updated to PR to use 1.26.1;;;","11/Apr/24 11:46;martijnvisser;Fixed in apache/flink-connector-kafka

v3.1: 4168d0f22f2fb6b696b5e09d7b8d1f99a6714b78
main: 1c39e3b7495640c9b3784ec672097741c072cebb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Update Flink Kafka connector to support 1.19,FLINK-35007,13574620,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,04/Apr/24 11:34,23/Apr/24 09:56,04/Jun/24 20:40,11/Apr/24 11:42,,,,,,,kafka-3.2.0,kafka-4.0.0,,,Connectors / Kafka,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 11:42:47 UTC 2024,,,,,,,,,,"0|z1ofs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 11:42;martijnvisser;Fixed in apache/flink-connector-kafka

v3.1: 809cb0786565b3515d1a17319b0f98f59b1ef6c2
main: 897001d5682a0708042d59be81a10485ffd0dde7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use try with-resource for StandaloneAutoscalerExecutor,FLINK-35006,13574605,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,plugatarev,plugatarev,plugatarev,04/Apr/24 10:50,08/Apr/24 01:24,04/Jun/24 20:40,08/Apr/24 01:24,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-04 10:50:14.0,,,,,,,,,,"0|z1ofow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlClientITCase Failed to build JobManager image,FLINK-35005,13574492,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,03/Apr/24 15:19,04/Apr/24 07:34,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Test Infrastructure,,,,,0,test-stability,,,"jdk21 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=dc1bf4ed-4646-531a-f094-e103042be549&t=fb3d654d-52f8-5b98-fe9d-b18dd2e2b790&l=15140

{code}
Apr 03 02:59:16 02:59:16.247 [INFO] -------------------------------------------------------
Apr 03 02:59:16 02:59:16.248 [INFO]  T E S T S
Apr 03 02:59:16 02:59:16.248 [INFO] -------------------------------------------------------
Apr 03 02:59:17 02:59:17.841 [INFO] Running SqlClientITCase
Apr 03 03:03:15 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
Apr 03 03:03:15 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
Apr 03 03:03:15 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
Apr 03 03:03:15 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Apr 03 03:03:15 Caused by: org.apache.flink.connector.testframe.container.ImageBuildException: Failed to build image ""flink-configured-jobmanager""
Apr 03 03:03:15 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:234)
Apr 03 03:03:15 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:65)
Apr 03 03:03:15 	... 12 more
Apr 03 03:03:15 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.DockerClientException: Could not build image: Head ""https://registry-1.docker.io/v2/library/eclipse-temurin/manifests/21-jre-jammy"": received unexpected HTTP status: 500 Internal Server Error
Apr 03 03:03:15 	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
Apr 03 03:03:15 	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
Apr 03 03:03:15 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:47)
Apr 03 03:03:15 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:255)
Apr 03 03:03:15 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:206)
Apr 03 03:03:15 	... 13 more
Apr 03 03:03:15 Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: Head ""https://registry-1.docker.io/v2/library/eclipse-temurin/manifests/21-jre-jammy"": received unexpected HTTP status: 500 Internal Server Error
Apr 03 03:03:15 	at com.github.dockerjava.api.command.BuildImageResultCallback.getImageId(BuildImageResultCallback.java:78)
Apr 03 03:03:15 	at com.github.dockerjava.api.command.BuildImageResultCallback.awaitImageId(BuildImageResultCallback.java:50)
Apr 03 03:03:15 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:159)
Apr 03 03:03:15 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:40)
Apr 03 03:03:15 	at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:19)
Apr 03 03:03:15 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:41)
Apr 03 03:03:15 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
Apr 03 03:03:15 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
Apr 03 03:03:15 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
Apr 03 03:03:15 	at java.base/java.lang.Thread.run(Thread.java:1583)
Apr 03 03:03:15 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-03 15:19:43.0,,,,,,,,,,"0|z1of00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayE2ECase could not start container,FLINK-35004,13574488,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,03/Apr/24 14:39,04/Apr/24 07:33,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Test Infrastructure,,,,,0,github-actions,test-stability,,"1.20, jdk17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=e8e46ef5-75cc-564f-c2bd-1797c35cbebe&t=60c49903-2505-5c25-7e46-de91b1737bea&l=15078

There is an error: ""Process failed due to timeout"" in {{SqlGatewayE2ECase.testSqlClientExecuteStatement}}.  In the maven logs, we can see:

{code:java}
02:57:26,979 [                main] INFO  tc.prestodb/hdp2.6-hive:10                                   [] - Image prestodb/hdp2.6-hive:10 pull took PT43.592666618S
02:57:26,991 [                main] INFO  tc.prestodb/hdp2.6-hive:10                                   [] - Creating container for image: prestodb/hdp2.6-hive:10
02:57:27,032 [                main] INFO  tc.prestodb/hdp2.6-hive:10                                   [] - Container prestodb/hdp2.6-hive:10 is starting: 162069678c7d03252a42ed81ca43e1911ca7357c476a4a5de294ffe55bd83145
02:57:42,846 [                main] INFO  tc.prestodb/hdp2.6-hive:10                                   [] - Container prestodb/hdp2.6-hive:10 started in PT15.855339866S
02:57:53,447 [                main] ERROR tc.prestodb/hdp2.6-hive:10                                   [] - Could not start container
java.lang.RuntimeException: java.net.SocketTimeoutException: timeout
	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:94) ~[test-classes/:?]
	at org.testcontainers.containers.GenericContainer.containerIsStarted(GenericContainer.java:723) ~[testcontainers-1.19.1.jar:1.19.1]
	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:543) ~[testcontainers-1.19.1.jar:1.19.1]
	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:354) ~[testcontainers-1.19.1.jar:1.19.1]
	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81) ~[duct-tape-1.0.8.jar:?]
	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:344) ~[testcontainers-1.19.1.jar:1.19.1]
	at org.apache.flink.table.gateway.containers.HiveContainer.doStart(HiveContainer.java:69) ~[test-classes/:?]
	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:334) ~[testcontainers-1.19.1.jar:1.19.1]
	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1144) ~[testcontainers-1.19.1.jar:1.19.1]
	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:28) ~[testcontainers-1.19.1.jar:1.19.1]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42) ~[junit-vintage-engine-5.10.1.jar:5.10.1]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80) ~[junit-vintage-engine-5.10.1.jar:5.10.1]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72) ~[junit-vintage-engine-5.10.1.jar:5.10.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198) ~[junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169) ~[junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93) ~[junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58) ~[junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141) [junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57) [junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103) [junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85) [junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47) [junit-platform-launcher-1.10.1.jar:1.10.1]
	at org.apache.maven.surefire.junitplatform.LazyLauncher.execute(LazyLauncher.java:56) [surefire-junit-platform-3.2.2.jar:3.2.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:184) [surefire-junit-platform-3.2.2.jar:3.2.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:148) [surefire-junit-platform-3.2.2.jar:3.2.2]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:122) [surefire-junit-platform-3.2.2.jar:3.2.2]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:385) [surefire-booter-3.2.2.jar:3.2.2]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162) [surefire-booter-3.2.2.jar:3.2.2]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507) [surefire-booter-3.2.2.jar:3.2.2]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495) [surefire-booter-3.2.2.jar:3.2.2]
Caused by: java.net.SocketTimeoutException: timeout
	at okio.Okio$4.newTimeoutException(Okio.java:232) ~[okio-1.17.2.jar:?]
	at okio.AsyncTimeout.exit(AsyncTimeout.java:286) ~[okio-1.17.2.jar:?]
	at okio.AsyncTimeout$2.read(AsyncTimeout.java:241) ~[okio-1.17.2.jar:?]
	at okio.RealBufferedSource.indexOf(RealBufferedSource.java:358) ~[okio-1.17.2.jar:?]
	at okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:230) ~[okio-1.17.2.jar:?]
	at okhttp3.internal.http1.Http1ExchangeCodec.readHeaderLine(Http1ExchangeCodec.java:242) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http1.Http1ExchangeCodec.readResponseHeaders(Http1ExchangeCodec.java:213) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.connection.Exchange.readResponseHeaders(Exchange.java:115) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.CallServerInterceptor.intercept(CallServerInterceptor.java:94) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:43) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:94) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:88) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[okhttp-3.14.9.jar:?]
	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:229) ~[okhttp-3.14.9.jar:?]
	at okhttp3.RealCall.execute(RealCall.java:81) ~[okhttp-3.14.9.jar:?]
	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:87) ~[test-classes/:?]
	... 34 more
Caused by: java.net.SocketException: Socket closed
	at sun.nio.ch.NioSocketImpl.endRead(NioSocketImpl.java:253) ~[?:?]
	at sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:332) ~[?:?]
	at sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355) ~[?:?]
	at sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808) ~[?:?]
	at java.net.Socket$SocketInputStream.read(Socket.java:966) ~[?:?]
	at okio.Okio$2.read(Okio.java:140) ~[okio-1.17.2.jar:?]
	at okio.AsyncTimeout$2.read(AsyncTimeout.java:237) ~[okio-1.17.2.jar:?]
	at okio.RealBufferedSource.indexOf(RealBufferedSource.java:358) ~[okio-1.17.2.jar:?]
	at okio.RealBufferedSource.readUtf8LineStrict(RealBufferedSource.java:230) ~[okio-1.17.2.jar:?]
	at okhttp3.internal.http1.Http1ExchangeCodec.readHeaderLine(Http1ExchangeCodec.java:242) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http1.Http1ExchangeCodec.readResponseHeaders(Http1ExchangeCodec.java:213) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.connection.Exchange.readResponseHeaders(Exchange.java:115) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.CallServerInterceptor.intercept(CallServerInterceptor.java:94) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:43) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:94) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:88) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142) ~[okhttp-3.14.9.jar:?]
	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117) ~[okhttp-3.14.9.jar:?]
	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:229) ~[okhttp-3.14.9.jar:?]
	at okhttp3.RealCall.execute(RealCall.java:81) ~[okhttp-3.14.9.jar:?]
	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:87) ~[test-classes/:?]
	... 34 more
02:57:53,524 [                main] ERROR tc.prestodb/hdp2.6-hive:10                                   [] - Log output from the failed container:
2024-04-03 08:42:27,788 CRIT Supervisor is running as root.  Privileges were not dropped because no user is specified in the config file.  If you intend to run as root, you can set user=root in the config file to avoid this message.
2024-04-03 08:42:27,788 INFO Included extra file ""/etc/supervisord.d/hdfs-datanode.conf"" during parsing
2024-04-03 08:42:27,788 INFO Included extra file ""/etc/supervisord.d/hdfs-namenode.conf"" during parsing
{code}

This is a different error than FLINK-33418 (""Connection refused"")",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-03 14:39:44.0,,,,,,,,,,"0|z1oez4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update zookeeper to 3.8.4 to address CVE-2024-23944,FLINK-35003,13574481,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,slfan1989,slfan1989,03/Apr/24 14:06,03/Apr/24 14:10,04/Jun/24 20:40,,,,,,,,,,,,BuildSystem / Shaded,,,,,0,pull-request-available,,,"Update zookeeper to 3.8.4 to address CVE-2024-23944

https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper/3.8.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-03 14:06:33.0,,,,,,,,,,"0|z1oexk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GitHub action request timeout  to ArtifactService,FLINK-35002,13574476,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rskraba,rskraba,03/Apr/24 13:37,27/May/24 15:16,04/Jun/24 20:40,,,,,,,,,,,,Build System,,,,,0,github-actions,test-stability,,"A timeout can occur when uploading a successfully built artifact:
 * [https://github.com/apache/flink/actions/runs/8516411871/job/23325392650]

{code:java}
2024-04-02T02:20:15.6355368Z With the provided path, there will be 1 file uploaded
2024-04-02T02:20:15.6360133Z Artifact name is valid!
2024-04-02T02:20:15.6362872Z Root directory input is valid!
2024-04-02T02:20:20.6975036Z Attempt 1 of 5 failed with error: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/CreateArtifact. Retrying request in 3000 ms...
2024-04-02T02:20:28.7084937Z Attempt 2 of 5 failed with error: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/CreateArtifact. Retrying request in 4785 ms...
2024-04-02T02:20:38.5015936Z Attempt 3 of 5 failed with error: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/CreateArtifact. Retrying request in 7375 ms...
2024-04-02T02:20:50.8901508Z Attempt 4 of 5 failed with error: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/CreateArtifact. Retrying request in 14988 ms...
2024-04-02T02:21:10.9028438Z ##[error]Failed to CreateArtifact: Failed to make request after 5 attempts: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/CreateArtifact
2024-04-02T02:22:59.9893296Z Post job cleanup.
2024-04-02T02:22:59.9958844Z Post job cleanup. {code}
(This is unlikely to be something we can fix, but we can track it.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 15:16:03 UTC 2024,,,,,,,,,,"0|z1oewg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 14:52;rskraba;1.20 jdk8 https://issues.apache.org/jira/browse/FLINK-35002
jdk8 https://github.com/apache/flink/actions/runs/8593289825/job/23544798614;;;","10/Apr/24 10:12;rskraba;Java 21: Test (module: python): [https://github.com/apache/flink/actions/runs/8624933787/job/23640950949]
Hadoop 3.1.3: Test (module: table): [https://github.com/apache/flink/actions/runs/8624933787/job/23641030028#step:14:25]
Java 11: Test (module: tests) [https://github.com/apache/flink/actions/runs/8624933967/job/23640984981]
Java 17: Test (module: python) [https://github.com/apache/flink/actions/runs/8624933967/job/23640970406]

 ;;;","16/Apr/24 10:05;rskraba;1.20 AdaptiveScheduler: Compile https://github.com/apache/flink/commit/9ae9c1463d87da2fa0e4dd58ee6fd10d38cda6bd/checks/23856554227/logs
1.18 Hadoop 3.1.3: Compile https://github.com/apache/flink/commit/f5c62abf7475ea8bc976de2a2079b1a9e29b79df/checks/23856554112/logs

Note that both the above successfully built and run tests, only the artifacts were never successfully uploaded for the run to complete.;;;","18/Apr/24 08:55;rskraba;I changed the title; requests to the artifact service can timeout during other stages of the build than just uploading, and it seems like the same network issue.

1.19 Java 17 / E2E (group 2) https://github.com/apache/flink/commit/a2c3d27f5dced2ba73307e8230cd07a11b26c401/checks/23956874905/logs 

During **Download build artifacts from compile job**:

{code:java}
2024-04-18T02:20:57.1951531Z ##[group]Run actions/download-artifact@v4
2024-04-18T02:20:57.1952046Z with:
2024-04-18T02:20:57.1952529Z   name: build-artifacts-nightly-beta-java17-229
2024-04-18T02:20:57.1953033Z   path: /home/runner/work/flink/flink
2024-04-18T02:20:57.1953552Z   merge-multiple: false
2024-04-18T02:20:57.1953902Z   repository: apache/flink
2024-04-18T02:20:57.1954306Z   run-id: 8731358696
2024-04-18T02:20:57.1954704Z env:
2024-04-18T02:20:57.1955038Z   MOUNTED_WORKING_DIR: /__w/flink/flink
2024-04-18T02:20:57.1955517Z   CONTAINER_LOCAL_WORKING_DIR: /root/flink
2024-04-18T02:20:57.1956053Z   FLINK_ARTIFACT_DIR: /home/runner/work/flink/flink
2024-04-18T02:20:57.1956590Z   FLINK_ARTIFACT_FILENAME: flink_artifacts.tar.gz
2024-04-18T02:20:57.1957184Z   MAVEN_REPO_FOLDER: /home/runner/work/flink/flink/.m2/repository
2024-04-18T02:20:57.1957943Z   MAVEN_ARGS: -Dmaven.repo.local=/home/runner/work/flink/flink/.m2/repository
2024-04-18T02:20:57.1958748Z   DOCKER_IMAGES_CACHE_FOLDER: /home/runner/work/flink/flink/.docker-cache
2024-04-18T02:20:57.1959387Z   GHA_JOB_TIMEOUT: 310
2024-04-18T02:20:57.1959867Z   E2E_CACHE_FOLDER: /home/runner/work/flink/flink/.e2e-cache
2024-04-18T02:20:57.1960480Z   E2E_TARBALL_CACHE: /home/runner/work/flink/flink/.e2e-tar-cache
2024-04-18T02:20:57.1961116Z   GHA_PIPELINE_START_TIME: 2024-04-18 02:19:06+00:00
2024-04-18T02:20:57.1961649Z   JAVA_HOME: /usr/lib/jvm/temurin-17-jdk-amd64
2024-04-18T02:20:57.1963231Z   PATH: /usr/lib/jvm/temurin-17-jdk-amd64/bin:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
2024-04-18T02:20:57.1964849Z ##[endgroup]
2024-04-18T02:20:57.3842499Z Downloading single artifact
2024-04-18T02:21:02.4187408Z Attempt 1 of 5 failed with error: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/ListArtifacts. Retrying request in 3000 ms...
2024-04-18T02:21:10.4281352Z Attempt 2 of 5 failed with error: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/ListArtifacts. Retrying request in 4605 ms...
2024-04-18T02:21:20.0388024Z Attempt 3 of 5 failed with error: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/ListArtifacts. Retrying request in 8717 ms...
2024-04-18T02:21:33.7715121Z Attempt 4 of 5 failed with error: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/ListArtifacts. Retrying request in 12219 ms...
2024-04-18T02:21:51.0125881Z ##[error]Unable to download artifact(s): Failed to ListArtifacts: Failed to make request after 5 attempts: Request timeout: /twirp/github.actions.results.api.v1.ArtifactService/ListArtifacts
{code}
 ;;;","22/Apr/24 11:55;rskraba;1.20 Java 21 / Compile https://github.com/apache/flink/commit/a4c71c8d021f5c07c81e69369139d4455da475ca/checks/24082452512/logs
1.20 AdaptiveScheduler / Test (module: python) https://github.com/apache/flink/commit/a4c71c8d021f5c07c81e69369139d4455da475ca/checks/24082689366/logs
;;;","03/May/24 15:54;rskraba;* 1.19 AdaptiveScheduler / Compile https://github.com/apache/flink/commit/ac4aa35c6e2e2da87760ffbf45d85888b1976c2f/checks/24453516397/logs
* 1.20 Java 8 / Compile https://github.com/apache/flink/commit/e412402ca4dfc438e28fb990dc53ea7809430aee/checks/24356511040/logs
* 1.19 Java 8 / Test (module: table) https://github.com/apache/flink/commit/e7816f714ef5298e1ca978aeddf62732794bb93f/checks/24231189927/logs
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/actions/runs/8810747051/job/24183773837#step:14:31;;;","07/May/24 08:03;rskraba;* 1.19 Java 8 / E2E (group 2) https://github.com/apache/flink/commit/fa426f104baa1343a07695dcf4c4984814f0fde4/checks/24659542455/logs;;;","10/May/24 09:41;rskraba;* 1.19 Java 11 / Compile https://github.com/apache/flink/commit/fa426f104baa1343a07695dcf4c4984814f0fde4/checks/24803211419/logs
* 1.18 Java 11 / Test (module: connect) https://github.com/apache/flink/commit/9d0858ee745bc835efa78a34d849d5f3ecb89f6d/checks/24709868165/logs
;;;","13/May/24 14:01;rskraba;* 1.18 AdaptiveScheduler / Compile https://github.com/apache/flink/commit/09f7b070989a906d777a000e6ec3d9b45e192a29/checks/24844337742/logs
;;;","14/May/24 13:39;rskraba;* 1.18 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/commit/1f604da2dfc831d04826a20b3cb272d2ad9dfb56/checks/24935906143/logs;;;","21/May/24 14:02;rskraba;* 1.18 Java 8 / Compile https://github.com/apache/flink/commit/08ba6497e3ec7106021043612118df77330f9797/checks/25205410372/logs
* 1.20 Default (Java 8) / Test (module: core) https://github.com/apache/flink/commit/e5398e1025ec4312bac74a8b32b98d03cb254667/checks/25204700233/logs;;;","27/May/24 15:16;rskraba;* 1.20 Java 11 / Compile https://github.com/apache/flink/commit/f860631c523c1d446c0d01046f0fbe6055174dc6/checks/25438061803/logs
* 1.19 Java 17 / Compile https://github.com/apache/flink/commit/a450980de65eaead734349ed44452f572e5e329d/checks/25402960967/logs;;;",,,,,,,,,,,,,,,,,,,,,
Avoid scientific notation for DOUBLE to STRING,FLINK-35001,13574448,13404080,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,twalthr,twalthr,03/Apr/24 12:40,03/Apr/24 12:42,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,"Flink currently uses Java semantics for some casts.

When executing:

{code}
SELECT CAST(CAST('19586232024.0' AS DOUBLE) AS STRING);
{code}

Leads to
{code}
1.9586232024E10
{code}

However, other vendors such as Postgres or MySQL return {{19586232024}}.

We should reconsider this behavior for consistency.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 03 12:42:02 UTC 2024,,,,,,,,,,"0|z1oeq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/24 12:42;twalthr;Since functions in CompiledPlan are versioned. We could introduce this in a backwards compatible way if necessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PullRequest template doesn't use the correct format to refer to the testing code convention,FLINK-35000,13574442,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mapohl,mapohl,mapohl,03/Apr/24 12:05,03/Apr/24 14:49,04/Jun/24 20:40,03/Apr/24 14:49,1.18.1,1.19.0,1.20.0,,,,1.18.2,1.19.1,1.20.0,,Build System / CI,Project Website,,,,0,pull-request-available,,,The PR template refers to https://flink.apache.org/contributing/code-style-and-quality-common.html#testing rather than https://flink.apache.org/how-to-contribute/code-style-and-quality-common/#7-testing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 03 14:49:09 UTC 2024,,,,,,,,,,"0|z1oeow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/24 14:49;mapohl;master: [d301839dfe2ed9b1313d23f8307bda76868a0c0a|https://github.com/apache/flink/commit/d301839dfe2ed9b1313d23f8307bda76868a0c0a]
1.19: [eb58599b434b6c5fe86f6e487ce88315c98b4ec3|https://github.com/apache/flink/commit/eb58599b434b6c5fe86f6e487ce88315c98b4ec3]
1.18: [9150f93b18b8694646092a6ed24a14e3653f613f|https://github.com/apache/flink/commit/9150f93b18b8694646092a6ed24a14e3653f613f];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PR CI stopped operating,FLINK-34999,13574436,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lorenzo.affetti,mapohl,mapohl,03/Apr/24 11:32,04/Apr/24 14:41,04/Jun/24 20:40,04/Apr/24 10:12,1.18.1,1.19.0,1.20.0,,,,,,,,Build System / CI,,,,,0,,,,"There are no [new PR CI runs|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=2] being picked up anymore. [Recently updated PRs|https://github.com/apache/flink/pulls?q=sort%3Aupdated-desc] are not picked up by the @flinkbot.

In the meantime there was a notification sent from GitHub that the password of the [@flinkbot|https://github.com/flinkbot] was reset for security reasons. It's quite likely that these two events are related.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 04 10:12:09 UTC 2024,,,,,,,,,,"0|z1oenk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/24 11:44;mapohl;CC [~uce] [~Weijie Guo] [~fanrui] [~rmetzger]
CC [~jingge] since it might be Ververica infrastructure-related;;;","03/Apr/24 13:14;lorenzo.affetti;Hello [~mapohl] , I think Jing is not available at the moment.

Let me have a pass over this from the Ververica side to double-check if I see any problem on CI machines.;;;","03/Apr/24 14:09;rmetzger;I'm trying to restore access to the flinkbot gh account.;;;","03/Apr/24 15:02;rmetzger;I have access to the Flinkbot gh account (Matthias Pohl and Chesnay have access too).

[~lorenzo.affetti] I pinged you in the Flink slack regarding the VV flinkbot stuff.;;;","04/Apr/24 09:53;lorenzo.affetti;Flink CI Bot had invalid credentials.

With the help of [~mapohl] and [~rmetzger] we restarted the bot, and now CI is behaving normally [https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=2.]

 

As a bonus we also restarted the pr-labeler that is now operative as well.

 

[~mapohl] I think we can now close this, thank you!;;;","04/Apr/24 10:12;mapohl;Thanks for working on it. I verified that [PR CI|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=2] is picked up again. (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Wordcount on Docker test failed on azure,FLINK-34998,13574417,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,03/Apr/24 09:57,04/Apr/24 16:13,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Build System / CI,,,,,0,,,,"/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_docker_embedded_job.sh: line 65: docker-compose: command not found
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_docker_embedded_job.sh: line 66: docker-compose: command not found
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/test_docker_embedded_job.sh: line 67: docker-compose: command not found
sort: cannot read: '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-24250435151/out/docker_wc_out*': No such file or directory
Apr 03 02:08:14 FAIL WordCount: Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected 0e5bd0a3dd7d5a7110aa85ff70adb54b.
Apr 03 02:08:14 head hexdump of actual:
head: cannot open '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-24250435151/out/docker_wc_out*' for reading: No such file or directory
Apr 03 02:08:14 Stopping job timeout watchdog (with pid=244913)
Apr 03 02:08:14 [FAIL] Test script contains errors.


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58709&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=5d91035e-8022-55f2-2d4f-ab121508bf7e&l=6043",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 04 16:13:10 UTC 2024,,,,,,,,,,"0|z1oejc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/24 11:27;mapohl;I guess, this one is a duplicate of FLINK-34997. In the end, the error happens due to the missing {{docker-compose}} binaries in the Azure VMs. WDYT?;;;","03/Apr/24 15:09;rskraba;* 1.20, jdk11 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=5d91035e-8022-55f2-2d4f-ab121508bf7e&l=6091]
 * 1.20, jdk17 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=64debf87-ecdb-5aef-788d-8720d341b5cb&t=2302fb98-0839-5df2-3354-bbae636f81a7&l=5267]
 * 1.20 jdk21 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=40819b3f-6406-53da-fb7a-b3c0f1535d7c&t=ec45d684-7283-5150-360d-c37269cd552a&l=5241]
 * 1.20 jdk21 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=22616

 ;;;","04/Apr/24 16:13;rskraba;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58716&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=5224
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58713&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=8939;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink YARN per-job on Docker test failed on azure,FLINK-34997,13574416,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,rmetzger,Weijie Guo,Weijie Guo,03/Apr/24 09:51,29/Apr/24 11:59,04/Jun/24 20:40,29/Apr/24 11:59,1.20.0,,,,,,,,,,Build System / CI,,,,,0,test-stability,,,"{code}
Apr 03 03:12:37 ==============================================================================
Apr 03 03:12:37 Running 'PyFlink YARN per-job on Docker test'
Apr 03 03:12:37 ==============================================================================
Apr 03 03:12:37 TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-37046085202
Apr 03 03:12:37 Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.19-SNAPSHOT-bin/flink-1.19-SNAPSHOT
Apr 03 03:12:38 Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.19-SNAPSHOT-bin/flink-1.19-SNAPSHOT
Apr 03 03:12:38 Docker version 24.0.9, build 2936816
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/common_docker.sh: line 24: docker-compose: command not found
Apr 03 03:12:38 [FAIL] Test script contains errors.
Apr 03 03:12:38 Checking of logs skipped.
Apr 03 03:12:38 
Apr 03 03:12:38 [FAIL] 'PyFlink YARN per-job on Docker test' failed after 0 minutes and 1 seconds! Test exited with exit code 1
{code}

{code}
/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/common_docker.sh: line 24: docker-compose: command not found
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58709&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=94ccd692-49fc-5c64-8775-d427c6e65440&l=10236",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 11:59:36 UTC 2024,,,,,,,,,,"0|z1oej4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/24 11:28;mapohl;The issue seems to be that {{docker-compose}} binaries are missing in the Azure VMs.;;;","03/Apr/24 15:10;rskraba;1.20 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=10071]
1.20 adaptive scheduler [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=011e961e-597c-5c96-04fe-7941c8b83f23&l=10519];;;","03/Apr/24 15:23;rskraba;1.20 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58710&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=10000;;;","29/Apr/24 11:59;rmetzger;> The issue seems to be that docker-compose binaries are missing in the Azure VMs.

Yes, the issue seems really strange, since those runners are hosted by Microsoft, and they use a standard image. I would close this issue and declare it a glitch for now.
If it happens again, I propose to add some ""check_environment.sh"" script, which checks for all the tools we need .. and if tools are missing, we print some debug information (packages installed, PATH contents etc.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Deserializer can't be instantiated when connector-kafka installed into Flink Libs,FLINK-34996,13574413,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,hugogu,hugogu,hugogu,03/Apr/24 09:39,19/Apr/24 05:47,04/Jun/24 20:40,,,,,,,,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,"The current implementation of the 
KafkaValueOnlyDeserializerWrapper Class instantiates Deserializer from the ClassLoader of the KafkaValueOnlyDeserializerWrapper itself as following figure shows.
 
!image-2024-04-03-17-34-00-120.png|width=799,height=293!
 
In case of both following conditions are met:
1. The connector-kafka get installed into Libs of Flink (rather than in the User Jar)
2. The user jar defines a customized Deserializer for Kafka Record. 
 
The instantiation of the custom deserializer will fail due to NoClassFound exception because it is indeed not available in the system class loader. 
 
As following figure illustrates
 
!image-2024-04-03-17-37-55-105.png|width=413,height=452!
 
It can be fixed by using either UserCodeClassLoader or the ClassLoader of current Thread.",,";04/Apr/24 14:49;hugogu;10800",,0,10800,,,0,10800,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/24 09:34;hugogu;image-2024-04-03-17-34-00-120.png;https://issues.apache.org/jira/secure/attachment/13067822/image-2024-04-03-17-34-00-120.png","03/Apr/24 09:37;hugogu;image-2024-04-03-17-37-55-105.png;https://issues.apache.org/jira/secure/attachment/13067821/image-2024-04-03-17-37-55-105.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 03 09:57:54 UTC 2024,,,,,,,,,,"0|z1oeig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/24 09:57;renqs;[~hugogu] Thanks for the detailed explanation! Would you like to create a PR for this one?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink kafka connector source stuck when partition leader invalid,FLINK-34995,13574409,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yanspirit,yanspirit,03/Apr/24 09:15,12/Apr/24 08:59,04/Jun/24 20:40,,1.17.0,1.18.1,1.19.0,,,,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,"when partition leader invalid(leader=-1),  the flink streaming job using KafkaSource can't restart or start a new instance with a new groupid,  it will stuck and got following exception:

""{*}org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition aaa-1 could be determined{*}""

when leader=-1,  kafka api like KafkaConsumer.position() will block until either the position could be determined or an unrecoverable error is encountered 

infact,  leader=-1 not easy to avoid,  even replica=3, three disk offline together will trigger the problem, especially when the cluster size is relatively large.    it rely on kafka administrator to fix in time,  but it take risk when in kafka cluster peak period.

I have solve this problem, and want to create a PR. 
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 07:51:59 UTC 2024,,,,,,,,,,"0|z1oehk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/24 07:41;martijnvisser;Why do you think this is a Flink bug, and not an issue on the Kafka side because you have no Kafka leader elected? ;;;","07/Apr/24 02:11;yanspirit;[~martijnvisser]  Apologies for the confusion; the issue described is not a bug within Flink itself. Instead, it pertains to Flink's implementation when utilizing Kafka's blocking API, which may lead to the problem mentioned. This can be addressed by using the invalid leader filter and discovery partition interval.;;;","07/Apr/24 07:51;yanspirit;https://github.com/apache/flink-connector-kafka/pull/91;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JobIDLoggingITCase fails because of ""checkpoint confirmation for unknown task""",FLINK-34994,13574402,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,03/Apr/24 08:13,04/Apr/24 06:10,04/Jun/24 20:40,04/Apr/24 00:08,1.20.0,,,,,,1.20.0,,,,Tests,,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58640&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8735]
{code:java}
Mar 30 03:46:07 03:46:07.807 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.147 s <<< FAILURE! -- in org.apache.flink.test.misc.JobIDLoggingITCase
Mar 30 03:46:07 03:46:07.807 [ERROR] org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(ClusterClient) -- Time elapsed: 2.301 s <<< FAILURE!
Mar 30 03:46:07 java.lang.AssertionError: 
Mar 30 03:46:07 [too many events without Job ID logged by org.apache.flink.runtime.taskexecutor.TaskExecutor] 
Mar 30 03:46:07 Expecting empty but was: [Logger=org.apache.flink.runtime.taskexecutor.TaskExecutor Level=DEBUG Message=TaskManager received a checkpoint confirmation for unknown task b45d406844d494592784a88e47d201e2_cbc357ccb763df2852fee8c4fc7d55f2_0_0.]
Mar 30 03:46:07 	at org.apache.flink.test.misc.JobIDLoggingITCase.assertJobIDPresent(JobIDLoggingITCase.java:264)
Mar 30 03:46:07 	at org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(JobIDLoggingITCase.java:149)
Mar 30 03:46:07 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 30 03:46:07 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
[https://github.com/apache/flink/actions/runs/8502821551/job/23287730632#step:10:8131]

[https://github.com/apache/flink/actions/runs/8507870399/job/23300810619#step:10:8086]

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34643,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 04 00:08:59 UTC 2024,,,,,,,,,,"0|z1oeg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/24 13:37;rskraba;1.20, jdk11 [https://github.com/apache/flink/actions/runs/8532178112/job/23373188793#step:10:9091];;;","04/Apr/24 00:08;roman;Fix merged into master as 875683082a58636b377bbb0a82bac4d273455e6e..f86c08041211bbeddf36c9ff0fbe6ae4abaa3b9d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support MODEL syntax in parser,FLINK-34993,13574381,13574379,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lihaosky,lihaosky,lihaosky,03/Apr/24 05:45,27/May/24 07:15,04/Jun/24 20:40,27/May/24 07:15,,,,,,,1.20.0,,,,,,,,,0,pull-request-available,,,Adds all parser changes required for FLIP-437.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 07:15:34 UTC 2024,,,,,,,,,,"0|z1oebc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 07:15;twalthr;Fixed in master: 4d39738587e0f07a4776eebb83e2bcaf28eef604;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-437: Support ML Models in Flink SQL,FLINK-34992,13574379,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lihaosky,lihaosky,03/Apr/24 05:44,04/Apr/24 07:34,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,Table SQL / Runtime,,,0,,,,This is an umbrella task for FLIP-437. FLIP-437: https://cwiki.apache.org/confluence/display/FLINK/FLIP-437%3A+Support+ML+Models+in+Flink+SQL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-03 05:44:45.0,,,,,,,,,,"0|z1oeaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Operator ClassPath Race Condition Bug,FLINK-34991,13574326,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ryanvanhuuksloot,ryanvanhuuksloot,02/Apr/24 16:28,02/Apr/24 16:32,04/Jun/24 20:40,,1.7.2,,,,,,,,,,Kubernetes Operator,,,,,0,,,,"Hello,

I believe we've found a bug with the Job Managers of the Kubernetes Operator. I think there is a race condition or an incorrect conditional where the operator is checking for High Availability instead of seeing if there is an issue with Class Loading in the Job Manager.

*Example:*
When deploying a SQL Flink Job, it starts the job managers in a failed state.
Describing the flink deployment returns the Error message 
{code:java}
RestoreFailed ... HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted.{code}
But upon further investigation, the actual error was that the class loading of the Job Manager wasn't correct. This was a log in the Job Manager
{code:java}
""Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n\nAvailable factory identifiers are:\n\nblackhole\ndatagen\nfilesystem\nprint"",""name"":""org.apache.flink.table.api.ValidationException"",""extendedStackTrace"":""org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n\nAvailable factory identifiers are:\n\nblackhole\ndatagen\nfilesystem\nprint\n\""{code}
There is also logging in the operator
{code:java}
... Cannot discover a connector using option: 'connector'='kafka'\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:798)\n\tat org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:215)\n\t... 52 more\nCaused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath ....{code}
I think that the operator should return this error in the CRD since the HA error is not the root cause. 

 

 

To recreate:

All I did was remove the `""org.apache.flink:flink-connector-kafka:$flinkConnectorKafkaVersion""` from my bundled jar so the class path was missing. This was executing a Flink SQL job. Which means the job manager starts before the class path error is thrown which seems to be the issue.","Standard Flink Operator with Flink Deployment.

To recreate, just remove a critical SQL connector library from the bundled jar",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-02 16:28:33.0,,,,,,,,,,"0|z1odz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[feature][cdc-connector][oracle] Oracle cdc support newly add table,FLINK-34990,13574307,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,pacinogong,pacinogong,pacinogong,02/Apr/24 14:47,21/May/24 06:20,04/Jun/24 20:40,,,,,,,,cdc-3.2.0,,,,Flink CDC,,,,,0,pull-request-available,,,[feature][cdc-connector][oracle] Oracle cdc support newly add table,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 14:50:13 UTC 2024,,,,,,,,,,"0|z1odv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 14:50;pacinogong;I will submit the PR. cc [~Leonard] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Infra requests to reduce the runner usage for a project,FLINK-34989,13574278,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,02/Apr/24 11:35,04/Apr/24 12:01,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,Build System / CI,,,,,0,pull-request-available,,,"The GitHub Actions CI utilizes runners that are hosted by Apache Infra right now. These runners are limited. The runner usage can be monitored via the following links:
* [Flink-specific report|https://infra-reports.apache.org/#ghactions&project=flink&hours=168] (needs ASF committer rights) This project-specific report can only be modified through the HTTP GET parameters of the URL.
* [Global report|https://infra-reports.apache.org/#ghactions] (needs ASF membership)

There was a policy change announced recently:
{quote}
Policy change on use of GitHub Actions

Due to misconfigurations in their builds, some projects have been using 
unsupportable numbers of GitHub Actions. As part of fixing this situation, 
Infra has added a 'resource use' section to the policy on GitHub Actions. 
This section of the policy will come into effect on April 20, 2024:

All workflows MUST have a job concurrency level less than or equal to 20. 
This means a workflow cannot have more than 20 jobs running at the same time 
across all matrices.
All workflows SHOULD have a job concurrency level less than or equal to 15. 
Just because 20 is the max, doesn't mean you should strive for 20.
The average number of minutes a project uses per calendar week MUST NOT 
exceed the equivalent of 25 full-time runners (250,000 minutes, or 4,200 
hours).
The average number of minutes a project uses in any consecutive five-day 
period MUST NOT exceed the equivalent of 30 full-time runners (216,000 
minutes, or 3,600 hours).
Projects whose builds consistently cross the maximum use limits will lose 
their access to GitHub Actions until they fix their build configurations.
The full policy is at  
https://infra.apache.org/github-actions-policy.html.
{quote}

Currently (last week of March 2024) Flink was ranked at #19 of projects that used the Apache Infra runner resources the most which doesn't seem too bad. This contained not only Apache Flink but also the Kubernetes operator, connectors and other resources. According to [this source|https://infra.apache.org/github-actions-secrets.html] Apache Infra manages 180 runners right now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34331,,FLINK-34937,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 04 12:01:36 UTC 2024,,,,,,,,,,"0|z1odoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 11:46;mapohl;Here's a summary of the requirements and whether we meet them based on the most-recent report:

|| Requirement || Flink CI ||
| Job concurrency level 20 (or better 15) or below | (n) |
| Do not exceed 25 full-time runners (FT runner), i.e. 4200hours per 7 days | (y) |
| Avg number of minutes should not exceed 3600 hours per 5 days | (y) |
;;;","02/Apr/24 11:51;mapohl;This Jira issue is about adding job concurrency support. Ideally, we should make it configurable in an easy way and set it to a concurrency level >20 as requested by Apache Infra. This affects the nightly builds which run per branch with 5 different test profiles and each test profile having 11 runners (10 stages + a short-running license check) being occupied in parallel.

Generally, we should make CI be more selective anyway. Apache Infra constantly criticizes projects for running heavy-load CI on changes like simple doc changes (see [here|https://infra.apache.org/github-actions-secrets.html]).;;;","02/Apr/24 11:54;mapohl;For this issue, we should keep in mind that it is only affecting the non-ephemeral runners. FLINK-34331 works on enabling ephemeral runners for Apache Flink. Ephemeral runners would allow us to donate project specific runners, i.e. someone could donate hardware to allow Flink to have its own runners and not to worry to much about blocking other projects with CI.;;;","04/Apr/24 12:01;mapohl;[~martijnvisser] pointed out that we might need to fix this in the connector repos as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class loading issues in JDK17 and JDK21,FLINK-34988,13574263,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,mapohl,mapohl,02/Apr/24 10:04,05/Apr/24 15:48,04/Jun/24 20:40,05/Apr/24 15:48,1.20.0,,,,,,1.20.0,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,"* JDK 17 (core; NoClassDefFoundError caused by ExceptionInInitializeError): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58676&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=12942
* JDK 17 (misc; ExceptionInInitializeError): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58676&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=22548
* JDK 21 (core; same as above): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58676&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=12963
* JDK 21 (misc; same as above): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58676&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=22506",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34548,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 05 15:48:43 UTC 2024,,,,,,,,,,"0|z1odlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 10:07;mapohl;It's most likely caused by FLINK-34548 based on the git history between the most recent successful nightly run on master [20240331.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58645&view=results] (based on {{3841f062}}) and [20240402.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58676&view=results] (based on {{d271495c}}):
{code}
$ git log 3841f062..d271495c --oneline
d271495c5be [hotfix] Fix compile error in DataStreamV2SinkTransformation
28762497bdf [FLINK-34548][API] Supports sink-v2 Sink
056660e0b69 [FLINK-34548][API] Supports FLIP-27 Source
ceafa5a5705 [FLINK-34548][API] Implement datastream
4f71c5b4660 [FLINK-34548][API] Implement process function's underlying operators
e1147ca7e39 [FLINK-34548][API] Introduce ExecutionEnvironment
9fa74a8a706 [FLINK-34548][API] Introduce stream interface and move KeySelector to flink-core-api
cedbcce6eff [FLINK-34548][API] Introduce variants of ProcessFunction
13cfaa76b5e [FLINK-34548][API] Introduce ProcessFunction and RuntimeContext related interfaces
13790e03207 [FLINK-34548][API] Move Function interface to flink-core-api
59525e460af [FLINK-34548][API] Create flink-core-api module and let flink-core depend on it
5b2e923be0a [FLINK-34548][API] Initialize the datastream v2 related modules
{code};;;","02/Apr/24 13:01;Weijie Guo;Yes, this not founded class is introduced by myself. I'm just wondering why the tests are passing for JDK8&JDK11 but not for higher release. I will take a look later, and any other input is helpful!

Update: I found out that this was due to the JDK17 restriction on reflection, which we needed to add --add-opens to get around.;;;","03/Apr/24 13:37;rskraba;Identical problems found in the GitHub Actions runs in JDK17 and JDK21:
 * [https://github.com/apache/flink/actions/runs/8516411771/job/23325640752#step:10:12870]
 * [https://github.com/apache/flink/actions/runs/8516411771/job/23325641276#step:10:22601]
 * [https://github.com/apache/flink/actions/runs/8516411771/job/23325644069#step:10:12904]
 * [https://github.com/apache/flink/actions/runs/8516411771/job/23325644866#step:10:22512]

And
 * [https://github.com/apache/flink/actions/runs/8532178112/job/23373191936#step:10:12874]
 * [https://github.com/apache/flink/actions/runs/8532178112/job/23373192587#step:10:22678]
 * [https://github.com/apache/flink/actions/runs/8532178112/job/23373174773#step:10:12918]
 * [https://github.com/apache/flink/actions/runs/8532178112/job/23373175427#step:10:22550]

 ;;;","03/Apr/24 15:08;rskraba;* 1.20 jdk17 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=13039
* 1.20 jdk17 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=22655
* 1.20 jdk21 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=13019
* 1.20 jdk21 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58708&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=22616;;;","04/Apr/24 07:42;martijnvisser;Since we have experimental support for Java 17 and 21, I would consider this a blocker for a next release. Bumped the priority;;;","04/Apr/24 16:12;rskraba;* 1.20 jdk17 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58717&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=13053
* 1.20 jdk17 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58717&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=23022
* 1.20 jdk21 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58717&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=13048
* 1.20 jdk21 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58717&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=22978;;;","05/Apr/24 14:54;rskraba;* 1.20 jdk17 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58740&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=13047
* 1.20 jdk17 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58740&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=23017
* 1.20 jdk21 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58740&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=12953
* 1.20 jdk21 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58740&view=logs&j=59a2b95a-736b-5c46-b3e0-cee6e587fd86&t=c301da75-e699-5c06-735f-778207c16f50&l=22978

* 1.20 jdk17  https://github.com/apache/flink/actions/runs/8548243133/job/23421925700#step:10:22662
* 1.20 jdk17 https://github.com/apache/flink/actions/runs/8548243133/job/23421925187#step:10:12881
* 1.20 jdk21 https://github.com/apache/flink/actions/runs/8548243133/job/23421910517#step:10:12917
* 1.20 jdk21 https://github.com/apache/flink/actions/runs/8548243133/job/23421911367#step:10:22589

* 1.20 jdk17  https://github.com/apache/flink/actions/runs/8563973008/job/23470051463#step:10:12882
* 1.20 jdk17 https://github.com/apache/flink/actions/runs/8563973008/job/23470052160#step:10:22638
* 1.20 jdk21 https://github.com/apache/flink/actions/runs/8563973008/job/23470043298#step:10:12917
* 1.20 jdk21 https://github.com/apache/flink/actions/runs/8563973008/job/23470043911#step:10:22589

;;;","05/Apr/24 15:48;Weijie Guo;I have just merged the fix:

master(1.20) via 54550e3b9b3c50992efe85d1b7716a2331b66a55.

Not sure if it can be completely fixed, feel free to reopen if this can still be reproduced.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Internal State Interface for Async State API,FLINK-34987,13574260,13574084,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,02/Apr/24 09:59,17/Apr/24 14:34,04/Jun/24 20:40,17/Apr/24 14:34,,,,,,,1.20.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 14:34:07 UTC 2024,,,,,,,,,,"0|z1odko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 14:34;masteryhx;merged 7699a0d2...2c5078bc into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic framework of async execution for state,FLINK-34986,13574221,13574083,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,zakelly,zakelly,zakelly,02/Apr/24 04:13,11/Apr/24 03:23,04/Jun/24 20:40,11/Apr/24 03:23,,,,,,,,,,,Runtime / State Backends,Runtime / Task,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 02:56:44 UTC 2024,,,,,,,,,,"0|z1odc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 02:56;Yanfei Lei;Merged 60c771b...ec2a140 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
It doesn't support to access fields by name for map function in thread mode,FLINK-34985,13574211,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianfu,dianfu,02/Apr/24 02:12,02/Apr/24 02:12,04/Jun/24 20:40,,,,,,,,,,,,API / Python,,,,,1,,,,"Reported in slack channel: [https://apache-flink.slack.com/archives/C065944F9M2/p1711640068929589]

```
hi all, I seem to be running into an issue when switching to thread mode in PyFlink. In an UDF the {{Row}} seems to get converted into a tuple and you cannot access fields by their name anymore. In process mode it works fine. This bug can easily be reproduced using this minimal example, which is close to the PyFlink docs:
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common import Row
from pyflink.table import StreamTableEnvironment, DataTypes
from pyflink.table.udf import udf

env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)
t_env.get_config().set(""parallelism.default"", ""1"")


# This does work:
t_env.get_config().set(""python.execution-mode"", ""process"")

# This doesn't work:
#t_env.get_config().set(""python.execution-mode"", ""thread"")


def map_function(a: Row) -> Row:
    return Row(a.a + 1, a.b * a.b)


# map operation with a python general scalar function
func = udf(
    map_function,
    result_type=DataTypes.ROW(
        [
            DataTypes.FIELD(""a"", DataTypes.BIGINT()),
            DataTypes.FIELD(""b"", DataTypes.BIGINT()),
        ]
    ),
)
table = (
    t_env.from_elements(
        [(2, 4), (0, 0)],
        schema=DataTypes.ROW(
            [
                DataTypes.FIELD(""a"", DataTypes.BIGINT()),
                DataTypes.FIELD(""b"", DataTypes.BIGINT()),
            ]
        ),
    )
    .map(func)
    .alias(""a"", ""b"")
    .execute()
    .print()
)```
 
The exception I get in this execution mode is:
2024-03-28 16:32:10 Caused by: pemja.core.PythonException: <class 'AttributeError'>: 'tuple' object has no attribute 'a'
2024-03-28 16:32:10     at /usr/local/lib/python3.10/dist-packages/pyflink/fn_execution/embedded/operation_utils.process_element(operation_utils.py:72)
2024-03-28 16:32:10     at /usr/local/lib/python3.10/dist-packages/pyflink/fn_execution/table/operations.process_element(operations.py:102)
2024-03-28 16:32:10     at <string>.<lambda>(<string>:1)
2024-03-28 16:32:10     at /opt/flink/wouter/minimal_example.map_function(minimal_example.py:19)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-02 02:12:52.0,,,,,,,,,,"0|z1od9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-423: Disaggregated State Storage and Management (Umbrella FLIP),FLINK-34984,13574150,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ym,ym,01/Apr/24 11:46,01/Apr/24 12:01,04/Jun/24 20:40,,,,,,,,,,,,API / Core,API / DataStream,Runtime / Checkpointing,Runtime / State Backends,,1,,,,"The past decade has witnessed a dramatic shift in Flink's deployment mode, workload patterns, and hardware improvements. We've moved from the map-reduce era where workers are computation-storage tightly coupled nodes to a cloud-native world where containerized deployments on Kubernetes become standard. To enable Flink's Cloud-Native future, we introduce Disaggregated State Storage and Management that uses DFS as primary storage in Flink 2.0

This new architecture is aimed to solve the following challenges brought in the cloud-native era for Flink.
1. Local Disk Constraints in containerization
2. Spiky Resource Usage caused by compaction in the current state model
3. Fast Rescaling for jobs with large states (hundreds of Terabytes)
4. Light and Fast Checkpoint in a native way

 

Design Details can be found in [FLIP-423|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=293046855]

Proposed changes can be found here:
 * [Asynchronous State APIs (FLIP-424) |https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=293046855#FLIP423:DisaggregatedStateStorageandManagement(UmbrellaFLIP)-AsynchronousStateAPIs(FLIP-424)]
 * [Non-blocking Asynchronous Execution Model: Parallel I/O (FLIP-425)|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=293046855#FLIP423:DisaggregatedStateStorageandManagement(UmbrellaFLIP)-Non-blockingAsynchronousExecutionModel:ParallelI/O(FLIP-425)]
 * [Batching for Network I/O: Beyond Parallel I/O (FLIP-426)|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=293046855#FLIP423:DisaggregatedStateStorageandManagement(UmbrellaFLIP)-BatchingforNetworkI/O:BeyondParallelI/O(FLIP-426)]
 * [Disaggregated State Store: ForSt (FLIP-427)|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=293046855#FLIP423:DisaggregatedStateStorageandManagement(UmbrellaFLIP)-DisaggregatedStateStore:ForSt(FLIP-427)]
 * [Faster Checkpoint/Restore/Rescale: Leverage Shared DFS (FLIP-428)|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=293046855#FLIP423:DisaggregatedStateStorageandManagement(UmbrellaFLIP)-FasterCheckpoint/Restore/Rescale:LeverageSharedDFS(FLIP-428)]",,,,,,,,,,,,,,,,FLINK-34974,FLINK-34975,FLINK-34982,FLINK-34981,FLINK-34973,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-01 11:46:20.0,,,,,,,,,,"0|z1ocw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive param sink.partition-commit.policy.kind default value is not work in stream mode,FLINK-34983,13574116,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,01/Apr/24 08:18,01/Apr/24 08:18,04/Jun/24 20:40,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,"This PR https://github.com/apache/flink/pull/20469/ added a default value for the hive parameter {{{}sink.partition-commit.policy.kind{}}}, but this does not take effect in the streaming mode because an earlier PR [https://github.com/apache/flink/pull/16370] will detect whether this parameter is set.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-01 08:18:39.0,,,,,,,,,,"0|z1ocoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-428: Fault Tolerance/Rescale Integration for Disaggregated State,FLINK-34982,13574114,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lijinzhong,lijinzhong,01/Apr/24 08:03,03/Jun/24 13:06,04/Jun/24 20:40,,,,,,,,2.0.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,,,,"This is a sub-FLIP for the disaggregated state management and its related work, please read the [FLIP-423|https://cwiki.apache.org/confluence/x/R4p3EQ] first to know the whole story.

As outlined in [FLIP-423|https://cwiki.apache.org/confluence/x/R4p3EQ] [1] and [FLIP-427|https://cwiki.apache.org/confluence/x/T4p3EQ] [2], we proposed to disaggregate StateManagement and introduced a disaggregated state storage named ForSt, which evolves from RocksDB. Within the new framework, where the primary storage is placed on the remote file system, several challenges emerge when attempting to reuse the existing fault-tolerance mechanisms of local RocksDB:
 * Because most remote file system don't support hard-link, ForSt can't utilize hard-link to capture a consistent snapshot during checkpoint synchronous phase as rocksdb currently does.
 * The existing file transfer mechanism within RocksDB is inefficient during checkpoints; it involves first downloading the remote working state data to local memory and then uploading it to the checkpoint directory. Likewise, both restore and rescale face the similar problems due to superfluous data transmission.

In order to solve the above problems and improve checkpoint/restore/rescaling performance of disaggregated storage, this FLIP proposes:
 # A new checkpoint strategy for disaggregated state storage: leverage RocksDB's low-level api to retain a consistent snapshot during the checkpoint synchronous phase; and then transfer the snapshot files to checkpoint directory during asynchronous phase;
 # Accelerating checkpoint/restore/rescaling by leverage fast-duplication of remote file system, which can bypass the local TaskManager when transferring data between remote working directory and checkpoint directory.",,,,,,,,,,,,,,,FLINK-34984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-01 08:03:11.0,,,,,,,,,,"0|z1oco8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-426: Grouping Remote State Access,FLINK-34981,13574094,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lijinzhong,lijinzhong,01/Apr/24 04:57,01/Apr/24 11:52,04/Jun/24 20:40,,,,,,,,2.0.0,,,,Runtime / State Backends,,,,,0,,,,"This is a sub-FLIP for the disaggregated state management and its related work, please read the [FLIP-423|https://cwiki.apache.org/confluence/x/R4p3EQ] first to know the whole story.

I/O speed and latency are critical for overall data throughput, particularly in jobs that manage large states. Implementing multiple asynchronous I/O operations is a proven strategy to enhance throughput by increasing parallelism of I/O execution. However, simply expanding I/O parallelism can quickly hit a ceiling due to finite I/O bandwidth. Additionally, when it comes to remote storage access, the time taken for RPC round trips significantly outweighs the impact of I/O size on individual I/O performance. So a promising optimization is to merge adjacent I/O requests into a single operation and fetch multiple keys with one I/O call. This approach requires a pre-prepared batch of keys for the query and the identification of I/O operations that can be combined. In this FLIP, we focus on the implementation details for batching state requests and processing them in batches.",,,,,,,,,,,,,,,FLINK-34984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-01 04:57:34.0,,,,,,,,,,"0|z1ocjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate overview document into Chinese,FLINK-34980,13574093,13573144,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,caicancai,caicancai,caicancai,01/Apr/24 04:55,06/May/24 09:56,04/Jun/24 20:40,06/May/24 09:56,,,,,,,kubernetes-operator-1.9.0,,,,,,,,,0,pull-request-available,,,Translate overview document into Chinese,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 06 09:56:26 UTC 2024,,,,,,,,,,"0|z1ocjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/24 09:56;fanrui;Merged to main(1.9.0) via :9db83bcb7afd9754ea538e293297a7e86cfae781;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement State Future and related utilities,FLINK-34979,13574091,13574084,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,01/Apr/24 04:27,07/Apr/24 02:30,04/Jun/24 20:40,07/Apr/24 02:19,,,,,,,1.20.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"Implement the StateFuture.

In the very first version, we leverage the CompletableFuture to ship the functionalities. Although this is not optimal in performance, we can swiftly build up the whole framework by this approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 02:19:02 UTC 2024,,,,,,,,,,"0|z1ocj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/24 02:19;masteryhx;merged 739bd337 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Asynchronous State APIs,FLINK-34978,13574090,13574084,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,01/Apr/24 04:27,02/Apr/24 02:24,04/Jun/24 20:40,02/Apr/24 02:24,,,,,,,1.20.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 02:24:48 UTC 2024,,,,,,,,,,"0|z1ociw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 02:24;masteryhx;merged f38d8ca4 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-433: State Access on DataStream API V2,FLINK-34977,13574087,13570313,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jeyhunkarimov,Weijie Guo,Weijie Guo,01/Apr/24 03:58,13/May/24 07:47,04/Jun/24 20:40,,1.20.0,,,,,,,,,,API / DataStream,API / State Processor,,,,0,pull-request-available,,,This is the umbrella ticket for FLIP-433: State Access on DataStream API V2.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-01 03:58:31.0,,,,,,,,,,"0|z1oci8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LD_PRELOAD environment may not be effective after su to flink user,FLINK-34976,13574086,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,zhoujira86,zhoujira86,01/Apr/24 03:54,01/Apr/24 07:51,04/Jun/24 20:40,01/Apr/24 07:51,1.19.0,,,,,,,,,,flink-docker,,,,,0,,,,"I am not sure if LD_PRELOAD  still takes effect after drop_privs_cmd. Should we  create a .bashrc file in home directory of flink, and export LD_PRELOAD for flink user？

 

[https://github.com/apache/flink-docker/blob/627987997ca7ec86bcc3d80b26df58aa595b91af/1.17/scala_2.12-java11-ubuntu/docker-entrypoint.sh#L92] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 07:51:36 UTC 2024,,,,,,,,,,"0|z1oci0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/24 06:18;yunta;I don't think the running Flink process would drop the {{LD_PRELOAD}} environment, please use `pmap` to check whether {{jemalloc.so}} is used by your process.;;;","01/Apr/24 07:51;zhoujira86;[~yunta] Ok, Thanks for clarification;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-427: ForSt - Disaggregated State Store,FLINK-34975,13574085,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,masteryhx,masteryhx,01/Apr/24 03:51,08/Apr/24 06:42,04/Jun/24 20:40,,,,,,,,2.0.0,,,,Runtime / State Backends,,,,,0,,,,"This is a sub-FLIP for the disaggregated state management and its related work, please read the [FLIP-423|https://cwiki.apache.org/confluence/x/R4p3EQ] first to know the whole story.

As described in FLIP-423, there are some tough issues about embedded state backend on local file system, respecially when dealing with extremely large state:
 # {*}Constraints of local disk space complicate the prediction of storage requirements, potentially leading to job failures{*}: Especially in cloud native deployment mode, pre-allocated local disks typically face strict capacity constraints, making it challenging to forecast the size requirements of job states. Over-provisioning disk space results in unnecessary resource overhead, while under-provisioning risks job failure due to insufficient space.
 # *The tight coupling of compute and storage resources leads to underutilization and increased waste:* Jobs can generally be categorized as either CPU-intensive or IO-intensive. In a coupled architecture, CPU-intensive jobs leave a significant portion of storage resources underutilized, whereas IO-intensive jobs result in idle computing resources.

By considering remote storage as the primary storage, all working states are maintained on the remote file system, which brings several advantages:
 # *Remote storages e.g. S3/HDFS typically offer elastic scalability, theoretically providing unlimited space.*
 # *The allocation of remote storage resources can be optimized by reducing them for CPU-intensive jobs and augmenting them for IO-intensive jobs, thus enhancing overall resource utilization.*
 # *This architecture facilitates a highly efficient and lightweight process for checkpointing, recovery, and rescaling through fast copy or simple move.*

This FLIP aims to realize disaggregated state for our new key-value store named *ForSt* which evloves from RocksDB and supports remote file system. This makes Flink get rid of the disadvantages by coupled state architecture and embrace the scalable as well as flexible cloud-native storage.

Please see [FLIP-427 |https://cwiki.apache.org/confluence/x/T4p3EQ]for more details.",,,,,,,,,,,,,,,FLINK-34984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 02:09:53 UTC 2024,,,,,,,,,,"0|z1ochs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/24 17:51;kkrugler;I'd previously explored using [AnyBlob|https://github.com/durner/AnyBlob] for maintaining state in S3, though I stopped when I ran into C++/JNI build issues on my Mac (M1 processor-related). AnyBlob is C++, so not sure it would be worth the effort, but I think they use some standard remote storage optimization techniques that could be interesting.;;;","02/Apr/24 02:09;masteryhx;Hi, [~kkrugler].

Thanks a lot for sharing the interesting topic. 

I just took a quick look and also found some interesting techniques (e.g. leverage unified C++ interfaces, io_uring for networking, MTU resolver) which should be helpful when we optimize ForSt in the future.

I think we could consider it in the next milestone.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-424: Asynchronous State APIs,FLINK-34974,13574084,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,01/Apr/24 03:40,01/Apr/24 11:52,04/Jun/24 20:40,,,,,,,,2.0.0,,,,Runtime / State Backends,,,,,0,,,,"This is a sub-FLIP for the disaggregated state management and its related work, please read the [FLIP-423|https://cwiki.apache.org/confluence/x/R4p3EQ] first to know the whole story.

To maximize I/O capacity utilization and enhance the use of pre-allocated computational resources, this FLIP proposes the introduction of asynchronous state APIs. These APIs permit state access to be executed in threads separate from the task thread, returning the result when available. Consequently, the task thread can process another element while awaiting multiple pending state results. This enables concurrent processing of multiple records, ensuring that the latency of individual I/O operations no longer has a direct impact on job performance. This approach is particularly advantageous in scenarios where I/O bandwidth is underutilized and I/O latency is the limiting factor. The Disaggregated Storage Architecture, as discussed in FLIP-423, is a prime example of a scenario characterized by abundant and easily scalable I/O bandwidth coupled with higher I/O latency. The asynchronous state APIs hold great promise for significantly enhancing Flink's performance when dealing with disaggregated state.
",,,,,,,,,,,,,,,FLINK-34984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-01 03:40:56.0,,,,,,,,,,"0|z1ochk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-425: Asynchronous Execution Model,FLINK-34973,13574083,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yanfei Lei,zakelly,zakelly,01/Apr/24 03:36,07/Apr/24 01:58,04/Jun/24 20:40,,,,,,,,2.0.0,,,,Runtime / Checkpointing,Runtime / State Backends,Runtime / Task,,,0,,,,"This is a sub-FLIP for the disaggregated state management and its related work, please read the FLIP-423 first to know the whole story.

FLIP-424 introduces asynchronous state APIs with callbacks allowing state access to be executed in threads separate from the task thread, making better usage of I/O bandwidth and enhancing throughput. This FLIP proposes an execution framework for asynchronous state APIs. The execution code path for the new API is completely independent from the original one, where many runtime components are redesigned. We intend to delve into the challenges associated with asynchronous execution and provide an in-depth design analysis for each module. Furthermore, we will conduct a performance analysis of the new framework relative to the current implementation and examine how it measures up against other potential alternatives.",,,,,,,,,,,,,,,FLINK-34984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-04-01 03:36:06.0,,,,,,,,,,"0|z1ochc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MigrationTestsSnapshotGenerator does not work properly for all subclasses of SnapshotMigrationTestBase,FLINK-34972,13574030,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lincoln.86xy,lincoln.86xy,31/Mar/24 15:56,31/Mar/24 16:01,04/Jun/24 20:40,,1.19.0,,,,,,,,,,Test Infrastructure,,,,,0,,,,"This issue was found on the followup work of 1.19.0 releasing https://issues.apache.org/jira/browse/FLINK-34712.

{code}MigrationTestsSnapshotGenerator{code} actually didn't generate the corresponding testing files for the new version(1.19) without any abnormal logs. And only appears in all subclasses of {code}SnapshotMigrationTestBase{code}.

more context: https://github.com/apache/flink/pull/24517",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-31 15:56:33.0,,,,,,,,,,"0|z1oc5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the function of History Server cleaning job archives regularly,FLINK-34971,13574018,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dengxiang,dengxiang,31/Mar/24 11:25,15/Apr/24 14:54,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"Currently, flink does not provide the ability to regularly clean the archives of completed jobs.  I think this ability is necessary.
When flink provides this ability, users can decide whether to use this ability. They also can define the time for regular cleaning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 02:54:16 UTC 2024,,,,,,,,,,"0|z1oc2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 02:54;Weijie Guo;The feature request should never be a blocker in my opinion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate architecture documents into Chinese,FLINK-34970,13573996,13573144,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,caicancai,caicancai,caicancai,31/Mar/24 03:35,23/Apr/24 04:38,04/Jun/24 20:40,23/Apr/24 04:38,1.9.0,,,,,,kubernetes-operator-1.9.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,Translate architecture documents into Chinese,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 04:38:50 UTC 2024,,,,,,,,,,"0|z1oby0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 04:38;fanrui;Merged to main(1.9.0) via: 9ea69eb1c15b9424fb08d8152e79b2b560d53a5c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error due to Flink CDC using deprecated flink-conf.yaml in light of Flink's full YAML 1.2 support since version 1.19,FLINK-34969,13573967,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,skymilong,skymilong,skymilong,30/Mar/24 13:22,23/Apr/24 09:58,04/Jun/24 20:40,23/Apr/24 09:58,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"Steps to Reproduce
1. Upgrade to Flink version 1.19 or later where full YAML 1.2 support is introduced.
2. Attempt to run a Flink CDC job using the latest version of Flink CDC, which still references the `flink-conf.yaml` file.
3.  Using flink-cdc.sh script : 

bin/flink-cdc.sh mysql2doris.yaml

exception: 
Exception in thread ""main"" java.io.FileNotFoundException: Cannot find configuration file at ""/opt/flink/conf/flink-conf.yaml""



 ","- Flink Version: 1.19.0
- Flink CDC Version: flink-cdc-3.0.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,#3193,https://github.com/apache/flink-cdc/issues/3193,,,,,,,,,,9223372036854775807,,,Java,Tue Apr 23 09:58:50 UTC 2024,,,,,,,,,,"0|z1obrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/24 09:58;renqs;flink-cdc master: 21032263c54b9b14999dc234fea551b7253c6e4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update flink-web copyright to 2024,FLINK-34968,13573948,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,Yu Chen,Yu Chen,30/Mar/24 07:21,02/Apr/24 03:30,04/Jun/24 20:40,02/Apr/24 03:29,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34967,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 03:29:03 UTC 2024,,,,,,,,,,"0|z1obnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 03:29;yunta;merged in asf-site: 3cda9946df98f0d2abda396ac87f7bc192ec70e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Website copyright footer to 2024,FLINK-34967,13573946,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,yunta,yunta,30/Mar/24 06:48,02/Apr/24 03:30,04/Jun/24 20:40,02/Apr/24 03:30,,,,,,,,,,,Project Website,,,,,0,,,,"It's already the year 2024, we should update the copyright footer.",,,,,,,,,,,,,,,,,,,,,,,FLINK-34968,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-30 06:48:10.0,,,,,,,,,,"0|z1obmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to read snapshot by table partitions in MySQL CDC Source,FLINK-34966,13573929,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wanghe,wanghe,30/Mar/24 04:00,08/Apr/24 02:30,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 02:30:26 UTC 2024,,,,,,,,,,"0|z1obj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 02:30;renqs;[~wanghe] Could you provide some context in the description?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
optimize code of ServiceType,FLINK-34965,13573835,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,baisui,baisui,29/Mar/24 09:02,29/Mar/24 10:30,04/Jun/24 20:40,,kubernetes,,,,,,1.9.4,,,,Deployment / Kubernetes,,,,,0,,,,"In the class of [ServiceType|[https://github.com/apache/flink/blob/bf60c8813598d3119375cec057930240642699d4/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/services/ServiceType.java]
now the code as below,in ServiceType.java
{code:java}
.endMetadata()
.withNewSpec()
.withType(
kubernetesJobManagerParameters
.getRestServiceExposedType()
.serviceType()
.getType())
.withSelector(kubernetesJobManagerParameters.getSelectors())
.addNewPort()
.withName(Constants.REST_PORT_NAME) {code}

since the process context is in class ServiceType，then the type of service type should get from the self class，Instead of from `kubernetesJobManagerParameters`
{code:java}
.endMetadata()
.withNewSpec()
.withType(
// modify to
this .getType())
.withSelector(kubernetesJobManagerParameters.getSelectors())
.addNewPort()
.withName(Constants.REST_PORT_NAME) {code}

i think that would be better",,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2024-03-29 09:02:38.0,,,,,,,,,,"0|z1oay8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScheduledTask leak in registering the processing timer,FLINK-34964,13573832,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zoucao,zoucao,29/Mar/24 08:47,29/Mar/24 08:59,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,,,,"I have come across a problem regarding a leak in the 'ScheduledTask' while registering the processing timer. Upon further investigation, I have identified two factors that are responsible for the leak.


*1. The ScheduledTask associated with the timer has not been synchronized for deletion*


see `org.apache.flink.streaming.api.operators.InternalTimerServiceImpl#deleteProcessingTimeTimer`, when a registered timer want be deleted, flink only removes it from the 'processingTimeTimersQueue'. However, it's possible that this timer is the earliest one that will be triggered in the future and has been scheduled as a task submitted to the ScheduledThreadPoolExecutor.

When deleting a registered timer, flink should check whether this timer is the next triggered time, if true, the current 'ScheduledTask' should be canceled.

 

*2. Re-submit a timer earlier than the System.currentTimeMillis*

Considering a case, the current time-millis is 100, and there exist 100、101、102 in the processingQueue, timer-100 has been submitted to ScheduledThreadPool. At this moment, the user registers a timer-99. 99 is less than 100(the peek timer in queue), then Flink will cancel timer 100‘s task, and re-register using timer 99. However, before canceling timer-100, the thread pool has submitted it to mailbox.
Then, the mail in mailbox is as follows:
{code:java}
->  * register timer-99
->     trigger timer-100
->     trigger timer-99
{code}
 - when executing 'trigger timer 100', Flink will flush records whose timer belongs to 99 and 100, then submit timer-101 to the scheduled thread pool.
 - when executing 'trigger timer-99', no records need to flush, then it also submits timer-101 to the scheduled thread pool, because timer-101 is the next timer needs to trigger.
Obviously, Two tasks are registered to Flink's scheduled thread pool with the same timer.

In our online job, the number of these leaked Scheduled Tasks could be in the thousands, plz check the attachment.

 

Here an example is posted, convenient for reproducing the case-2.
{code:java}
    @Test
    public void testTimerTaskLeak() {
        TaskMailboxImpl mailbox = new TaskMailboxImpl();
        MailboxExecutor mailboxExecutor =
                new MailboxExecutorImpl(
                        mailbox, 0, StreamTaskActionExecutor.IMMEDIATE);
        SystemProcessingTimeService processingTimeService =
                new SystemProcessingTimeService(ex -> handleException(ex));

        ProcessingTimeServiceImpl timeService = new ProcessingTimeServiceImpl(
                processingTimeService,
                callback -> deferCallbackToMailbox(mailboxExecutor, callback));

        TestKeyContext keyContext = new TestKeyContext();

        Queue<String> mailQueue = new LinkedBlockingDeque<>();
        long curr = System.currentTimeMillis();
        InternalTimerServiceImpl<Integer, String> timerService =
                createAndStartInternalTimerService(
                        mock(Triggerable.class),
                        keyContext,
                        timeService,
                        testKeyGroupRange,
                        createQueueFactory());

        ExecutorService executorService = Executors.newFixedThreadPool(1);
        executorService.execute(
                () -> {
                    try {
                            keyContext.setCurrentKey(1);
                            mailboxExecutor.execute(
                                    () -> timerService.registerProcessingTimeTimer(""void"", curr + 6 * 1000L), ""6"");

                            Thread.sleep(2L);

                            mailboxExecutor.execute(
                                    () -> timerService.registerProcessingTimeTimer(""void"", curr + 7 * 1000L), ""7"");
                        Thread.sleep(2L);

                            mailboxExecutor.execute(
                                    () -> timerService.registerProcessingTimeTimer(""void"", curr + 8 * 1000L), ""8"");
                        Thread.sleep(2L);
                            mailboxExecutor.execute(
                                    () -> {
                                        timerService.registerProcessingTimeTimer(""void"", curr + 1);
                                    }, ""1"");

                            mailboxExecutor.execute(
                                    () -> {
                                        Thread.sleep(3); // wait timer +1 submitted to mailbox
                                        timerService.registerProcessingTimeTimer(""void"", curr - 5);
                                    }, ""-5"");
                            Thread.sleep(5L);
                            mailboxExecutor.execute(
                                    () -> timerService.registerProcessingTimeTimer(""void"", curr + 4), ""4"");
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }
                }

        );

        while (mailQueue.size() < 14) {
            if (mailbox.mailQueue().size() > 0) {
                String mail = mailbox.mailQueue().peek().toString();
                if (mail.length() > 5) {
                    mailQueue.add(""trigger "" + (Long.parseLong(mail.split(""@ "")[1]) - curr));
                } else {
                    mailQueue.add(""register "" + mail);
                }
            }
            mailboxExecutor.tryYield();
        }

        System.out.println(mailQueue);
        executorService.shutdownNow();
    }
{code}

Print Result:

{code:java}
[register 6, register 7, register 8, register 1, register -5, trigger 1, trigger -5, register 4, trigger 4, trigger 6000, trigger 6000, trigger 7000, trigger 7000, trigger 8000]
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/24 08:40;zoucao;image-2024-03-29-16-40-11-928.png;https://issues.apache.org/jira/secure/attachment/13067739/image-2024-03-29-16-40-11-928.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-29 08:47:22.0,,,,,,,,,,"0|z1oaxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compilation error in ProcessFunctionTestHarnesses,FLINK-34963,13573822,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,29/Mar/24 08:17,02/Apr/24 10:42,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,test-stability,,," 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58627&view=logs&j=64debf87-ecdb-5aef-788d-8720d341b5cb&t=f041a596-2626-58e5-69fa-facfbaf86c0f&l=6435]

At the compile step in *e2e_1_cron_jdk17 / Build Flink* step:
{code:java}
-------------------------------------------------------------
00:24:17.239 [ERROR] COMPILATION ERROR : 
00:24:17.239 [INFO] -------------------------------------------------------------
00:24:17.239 [ERROR] /home/vsts/work/1/s/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/ProcessFunctionTestHarnesses.java:[55,54] incompatible types: cannot infer type arguments for org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness<>
    reason: inference variable IN has incompatible equality constraints OUT,IN,IN
00:24:17.239 [INFO] 1 error

{code}
This is particularly curious because *e2e_2_cron_jdk17* with an identical build step succeeds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23982,FLINK-32113,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 29 08:19:38 UTC 2024,,,,,,,,,,"0|z1oavc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/24 08:19;rskraba;There's a similar compilation error in FLINK-23982 – maybe a bug in javac?!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-pulsa starts failed due to incorrect use of Pulsar API: LookupService. getPartitionedTopicMetadata,FLINK-34962,13573734,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yubiao,yubiao,28/Mar/24 17:07,15/Apr/24 07:19,04/Jun/24 20:40,15/Apr/24 07:19,,,,,,,pulsar-4.2.0,,,,Connectors / Pulsar,,,,,0,easyfix,pull-request-available,,"- The unnecessary codes calls `pulsarClient.getLookup().getPartitionedTopicMetadata()` to create the partitioned topic metadata(in fact, this behavior of is not correct)
  - Why it is unnecessary: the [following code]([https://github.com/apache/flink-connector-pulsar/blob/main/flink-connector-pulsar/src/main/java/org/apache/flink/connector/pulsar/sink/writer/topic/ProducerRegister.java#L245]) that is creating a producer will also trigger partitioned topic metadata to create.
 - The method `pulsarClient.getLookup().getPartitionedTopicMetadata()` will not retry if the connection is closed so that users will get an error. The following code creates a producer that will retry if the connection is closed, reducing the probability of an error occurring.","* flink 1.17
 * pulsar client 3.0.0
 * org.apache.flink:flink-connector-pulsar:4.1.0-1.17 (connector)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 07:19:49 UTC 2024,,,,,,,,,,"0|z1oac0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 05:56;syhily;[~Tison] This issue should be marked resolved.;;;","15/Apr/24 07:19;tison;master via https://github.com/apache/flink-connector-pulsar/commit/7340f713422b1734e84ec0602f154441b8da7fab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GitHub Actions runner statistcs can be monitored per workflow name,FLINK-34961,13573713,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,28/Mar/24 14:46,14/May/24 07:04,04/Jun/24 20:40,,,,,,,,jdbc-3.1.3,jdbc-3.2.0,kafka-3.2.0,kafka-4.0.0,Build System / CI,,,,,0,pull-request-available,starter,,"Apache Infra allows the monitoring of runner usage per workflow (see [report for Flink|https://infra-reports.apache.org/#ghactions&project=flink&hours=168&limit=10];  only accessible with Apache committer rights). They accumulate the data by workflow name. The Flink space has multiple repositories that use the generic workflow name {{CI}}). That makes the differentiation in the report harder.

This Jira issue is about identifying all Flink-related projects with a CI workflow (Kubernetes operator and the JDBC connector were identified, for instance) and adding a more distinct name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 07:04:45 UTC 2024,,,,,,,,,,"0|z1oa7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 11:33;Sergey Nuyanzin;Merged to flink-connector-kafka main as [c47abb3933b7c1e567a9142c6495038d16d42dd0|https://github.com/apache/flink-connector-kafka/commit/c47abb3933b7c1e567a9142c6495038d16d42dd0]
flink-connector-kafka v3.1 [ef203c7d3bd4e3507cef973bf4fc1e73b24900f8|https://github.com/apache/flink-connector-kafka/commit/ef203c7d3bd4e3507cef973bf4fc1e73b24900f8]

flink-connector-jdbc main [64c7b754812fa163946808c92a08c8a3eb6ddc94|https://github.com/apache/flink-connector-jdbc/commit/64c7b754812fa163946808c92a08c8a3eb6ddc94]
flink-connector-jdbc v3.1 [cb26a9ca7484672bb2d557ff0b70fe4a273c6ffc|https://github.com/apache/flink-connector-jdbc/commit/cb26a9ca7484672bb2d557ff0b70fe4a273c6ffc];;;","14/May/24 07:04;martijnvisser;Merged to flink-connector-kudu@main as 409856698df1bf630c58f6e761d45e4dc8f06ad5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException while applying parallelism overrides for session jobs,FLINK-34960,13573681,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kunal4now,kunal4now,28/Mar/24 12:09,03/Apr/24 10:53,04/Jun/24 20:40,,1.8.0,,,,,,,,,,Kubernetes Operator,,,,,0,,,,"While using the autoscaler for session jobs, the operator throws a NullPointerException while trying to apply parallelism overrides, though it's able to generate parallelism suggestion report for scaling. The versions used here are flink-1.18.1 and flink-kubernetes-operator-1.8.0. 
{code:java}
2024-03-26 08:41:21,617 o.a.f.a.JobAutoScalerImpl [ERROR][default/clientsession-job] Error applying overrides. java.lang.NullPointerException at org.apache.flink.kubernetes.operator.autoscaler.KubernetesScalingRealizer.realizeParallelismOverrides(KubernetesScalingRealizer.java:52) at org.apache.flink.kubernetes.operator.autoscaler.KubernetesScalingRealizer.realizeParallelismOverrides(KubernetesScalingRealizer.java:40) at org.apache.flink.autoscaler.JobAutoScalerImpl.applyParallelismOverrides(JobAutoScalerImpl.java:161) at org.apache.flink.autoscaler.JobAutoScalerImpl.scale(JobAutoScalerImpl.java:111) at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.applyAutoscaler(AbstractFlinkResourceReconciler.java:192) at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:139) at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:116) at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:53) at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:152) at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:110) at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80) at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:109) at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:140) at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:121) at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:91) at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:64) at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:417) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.base/java.lang.Thread.run(Unknown Source){code}
 
{code:java}
2024-03-26 08:41:21,617 o.a.f.a.JobAutoScalerImpl [ERROR][default/clientsession-job] Error while scaling job java.lang.NullPointerException at org.apache.flink.kubernetes.operator.autoscaler.KubernetesScalingRealizer.realizeParallelismOverrides(KubernetesScalingRealizer.java:52) at org.apache.flink.kubernetes.operator.autoscaler.KubernetesScalingRealizer.realizeParallelismOverrides(KubernetesScalingRealizer.java:40) at org.apache.flink.autoscaler.JobAutoScalerImpl.applyParallelismOverrides(JobAutoScalerImpl.java:161) at org.apache.flink.autoscaler.JobAutoScalerImpl.scale(JobAutoScalerImpl.java:111) at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.applyAutoscaler(AbstractFlinkResourceReconciler.java:192) at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:139) at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:116) at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:53) at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:152) at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:110) at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80) at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:109) at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:140) at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:121) at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:91) at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:64) at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:417) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.base/java.lang.Thread.run(Unknown Source){code}
The document suggests to use latest custom build of 1.18 or 1.19 that contains https://issues.apache.org/jira/browse/FLINK-33534 but a NullPointerException should never happen ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 09:43:09 UTC 2024,,,,,,,,,,"0|z1oa0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 08:14;rmetzger;Is it possible that your `FlinkSessionJobSpec` does not contain any `flinkConfiguration`? Can you try adding some dummy Flink config (e.g. foo: bar), just to validate if this resolves the bug?;;;","02/Apr/24 09:36;kunal4now;Thanks, not getting NPE anymore but to actually apply overrides I would need a custom build of 1.18 as suggested in the documentation ? where can I get this custom image ? or how do I build one ?;;;","02/Apr/24 09:40;rmetzger;Sorry, that's something you need to figure out yourself.

Thanks for validating that the missing configuration caused the issue. I think this is a valid bug we need to fix in Flink.;;;","02/Apr/24 09:43;kunal4now;Okay, thanks for the help.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update old flink-cdc-connectors artifactId,FLINK-34959,13573645,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xleoken,xleoken,xleoken,28/Mar/24 09:10,07/Apr/24 09:16,04/Jun/24 20:40,07/Apr/24 09:16,,,,,,,,,,,Flink CDC,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 09:15:53 UTC 2024,,,,,,,,,,"0|z1o9sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/24 09:15;renqs;cdc-master: 6510e670faa154d619937d97dc3f146eb162fac2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support Flink 1.20-SNAPSHOT and bump flink-connector-parent to 1.1.0 for mongodb connector,FLINK-34958,13573617,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,gongzhongqiang,gongzhongqiang,gongzhongqiang,28/Mar/24 05:44,29/Mar/24 03:02,04/Jun/24 20:40,29/Mar/24 02:59,mongodb-1.1.0,,,,,,mongodb-1.2.0,,,,Connectors / MongoDB,,,,,0,pull-request-available,,,"Changes:
 * Add support Flink 1.20-SNAPSHOT
 * Bump flink-connector-parent to 1.1.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 29 02:59:03 UTC 2024,,,,,,,,,,"0|z1o9m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/24 05:45;gongzhongqiang;[~Leonard] I'm willing to take this.;;;","29/Mar/24 02:59;jiabaosun;Implemented via (mongodb:main) 0dc2640922b3dd2d0ea8565d1bf6606b5d715b0b
a0fe686a6647cb6eba3908bbba336079569959e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC Autoscaler event handler throws Column 'message' cannot be null ,FLINK-34957,13573611,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,28/Mar/24 04:02,28/Mar/24 10:25,04/Jun/24 20:40,28/Mar/24 10:25,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,,,,,0,pull-request-available,,,"JDBC Autoscaler event handler doesn't allow the event message is null, but the message may be null when we handle the exception.

We consider the exception message as the event message, but the exception message may be null, such as: TimeoutException. (It has been shown in following picture.)

Also, recording a event without any message is meaningless. It doesn't have any benefit for troubleshooting.

Solution: 
* Consider the exception message as the event message when exception message isn't null
* The whole Exception as the event message if exception message is null.

 !image-2024-03-28-11-57-35-234.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/24 03:57;fanrui;image-2024-03-28-11-57-35-234.png;https://issues.apache.org/jira/secure/attachment/13067720/image-2024-03-28-11-57-35-234.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 10:25:38 UTC 2024,,,,,,,,,,"0|z1o9kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/24 10:25;fanrui;Merged master(1.9.0) via: 9119d73a904bc5a5eb675c2edffe8f8de8ed8ef2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The config type is wrong for Duration,FLINK-34956,13573609,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,28/Mar/24 03:22,29/Mar/24 06:13,04/Jun/24 20:40,29/Mar/24 06:13,,,,,,,1.19.1,1.20.0,,,Documentation,,,,,0,pull-request-available,,,"The Config type is Boolean, but it should be Duration.

https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/

 !image-2024-03-28-11-21-31-802.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/24 03:21;fanrui;image-2024-03-28-11-21-31-802.png;https://issues.apache.org/jira/secure/attachment/13067719/image-2024-03-28-11-21-31-802.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 29 06:13:35 UTC 2024,,,,,,,,,,"0|z1o9kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/24 06:13;fanrui;Merged to 
master(1.20) via : f31c128bfc457b64dd7734f71123b74faa2958ba
1.19 via: e4ac2ee6257f35a1e4602bf684618f9165ab5a96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade commons-compress to 1.26.0,FLINK-34955,13573601,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,slfan1989,slfan1989,slfan1989,28/Mar/24 01:40,10/Apr/24 09:16,04/Jun/24 20:40,03/Apr/24 13:09,,,,,,,1.18.2,1.19.1,1.20.0,,,,,,,0,pull-request-available,,,"commons-compress 1.24.0 has CVE issues, try to upgrade to 1.26.0, we can refer to the maven link

https://mvnrepository.com/artifact/org.apache.commons/commons-compress",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35040,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 04:49:11 UTC 2024,,,,,,,,,,"0|z1o9io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/24 06:21;mbalassi;Thanks! Let us get this in quick and backport it to 1.19 and 1.18.;;;","02/Apr/24 13:27;mbalassi;[163b9cc|https://github.com/apache/flink/commit/163b9cca6d2ccac0ff89dd985e3232667ddfb14f] in master, creating necessary backports.;;;","03/Apr/24 13:09;mbalassi;[{{f172171}}|https://github.com/apache/flink/commit/f17217100cf7d28bf6a1b687427c01e30b77e900] in release-1.19 and [{{1711ba8}}|https://github.com/apache/flink/commit/1711ba85744d917ca63d989bf4c120c6aebda9ba] in release-1.18.;;;","07/Apr/24 13:50;gongzhongqiang;[~slfan1989] [~mbalassi] According to https://issues.apache.org/jira/browse/COMPRESS-659 , [~jiabaosun]  and I think it's better bump version to 1.26.1 and remove `commons-codec` dependence.;;;","07/Apr/24 14:07;slfan1989;[~gongzhongqiang] From my personal perspective, I believe upgrading to version 1.26.0 should be sufficient as this version has already fixed the CVE issue. As for upgrading to 1.26.1, I think we can consider it after some time. Removing commons-codec might prove to be challenging because Flink has dependencies on Hadoop and HBase (both of which directly depend on commons-codec). If we remove commons-codec, it may result in the Hadoop and HBase modules being unable to compile successfully.;;;","07/Apr/24 14:10;slfan1989;[~gongzhongqiang] Of course, if upgrading is possible, it would be a positive step forward. I think we should give it a try. I see that you have created the relevant JIRA ticket, so you can go ahead and attempt it. Hopefully, it will be successful. (However, my preference would be to stick with version 1.26 for now, and consider upgrading to 1.26.1 in the future. If other components of Flink need upgrading, I think it would be best to upgrade them to version 1.26 as well. Removing dependencies, in my opinion, is not a good option.);;;","08/Apr/24 02:12;gongzhongqiang;Hi [~slfan1989]  ,I apologize for the ambiguity. `{color:#c1c7d0}remove commons-codec dependence{color}`  means  `{color:#c1c7d0}we don't have to manually add a dependency to commons-codec.{color}` .;;;","08/Apr/24 04:49;jiabaosun;I have rechecked the dependency of `commons-codec` in `commons-compress` and it is no longer optional. Even if upgraded to 1.26.1, `commons-codec` will still be a transitive dependency. 
Sorry for the disturbance.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Kryo input implementation NoFetchingInput fails to handle zero length bytes,FLINK-34954,13573579,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,q.xu,q.xu,q.xu,27/Mar/24 19:17,21/May/24 08:42,04/Jun/24 20:40,19/Apr/24 09:56,1.19.0,,,,,,1.20.0,,,,API / Type Serialization System,,,,,0,pull-request-available,,,"If the serailized bytes are empty, `NoFetchingInput` will run into error when Kryo tries to deserialize it.

Example: a protobuf 3 object that contains only default values will be serialized as 0 length byte array, and the deserialization later will fail. Illustration:
{noformat}
import com.esotericsoftware.kryo.Kryo
import com.esotericsoftware.kryo.io.{ByteBufferInput, ByteBufferOutput, Input, Output}
import com.google.protobuf.{DescriptorProtos, Message}import com.twitter.chill.protobuf.ProtobufSerializer
import org.apache.flink.api.java.typeutils.runtime.NoFetchingInput
import java.io.ByteArrayInputStream
 
object ProtoSerializationTest {
  def main(args: Array[String]) = {     
    val chillProtoSerializer = new ProtobufSerializer
    val protomessage = DescriptorProtos.DescriptorProto.getDefaultInstance
    val output: Output = new ByteBufferOutput(1000)
    chillProtoSerializer.write(null, output, protomessage)
    val serialized: Array[Byte] = output.toBytes
    println(s""Serialized : $serialized"")
    val input: Input = new NoFetchingInput(new ByteArrayInputStream(serialized))
    val deserialized = chillProtoSerializer.read(null, input, classOf[BillableClick].asInstanceOf[Class[Message]])
    println(deserialized)
  }
}
{noformat}
 

Error
{noformat}
Exception in thread ""main"" java.lang.RuntimeException: Could not create class com.criteo.glup.BillableClickProto$BillableClick
    at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:76)
    at com.criteo.streaming.common.bootstrap.ProtoSerialization$.main(ProtoSerialization.scala:22)
    at ProtoSerialization.main(ProtoSerialization.scala)
Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: No more bytes left.
    at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128)
    at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:332)
    at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73)
    ... 2 more
Caused by: java.io.EOFException: No more bytes left.
    ... 5 more{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35215,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 08 03:30:48 UTC 2024,,,,,,,,,,"0|z1o9e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/24 09:56;dannycranmer;Thanks for the bug report and fix, [~q.xu] .;;;","19/Apr/24 09:56;dannycranmer;Merged commit [{{3977982}}|https://github.com/apache/flink/commit/39779829b8853c61c9da9eaf193f0fedf5857f7b] into apache:master;;;","08/May/24 03:30;fanrui;Hi [~q.xu] , I wanna check with you did you meet this issue in your production job? or it only happens for your test code.

After I read the flink related code in detail, I guess this issue never happen for flink job.
h1. Reason:

This exception happens inside of NoFetchingInput.readBytes, and when inputStream.read return -1.
h2. Why doesn't flink job throw exception?

In flink code, the inputStream is DataInputViewStream, check KryoSerializer#deserialize method.
{code:java}
               DataInputViewStream inputStream = new DataInputViewStream(source);
                input = new NoFetchingInput(inputStream);{code}
DataInputViewStream#read calls org.apache.flink.core.memory.DataInputView#read(byte[], int, int). From its comment, we can see : <p>If <code>len</code> is zero, then no bytes are read and <code>0</code> is returned;

So when length is 0, inputStream.read will return 0. And NoFetchingInput.readBytes won't throw exception.
h2. Why does your test demo throw exception?

Your demo is using java.io.ByteArrayInputStream as the inputStream. From its comment, we can see 
{code:java}
* @return  the total number of bytes read into the buffer, or
*          <code>-1</code> if there is no more data because the end of
*          the stream has been reached. {code}
It returns -1 even if length is 0.

When return value is -1,  NoFetchingInput.readBytes will throw exception.

Please correct me if anything is wrong, thanks~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add github ci for flink-web to auto commit build files,FLINK-34953,13573518,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,gongzhongqiang,gongzhongqiang,27/Mar/24 12:25,30/May/24 07:02,04/Jun/24 20:40,,,,,,,,,,,,Project Website,,,,,0,pull-request-available,website,,"Currently, https://github.com/apache/flink-web commit build files by local build. So I want use github ci to build docs and commit.

 

Changes:
 * Add website build check for pr
 * Auto build and commit build files after pr was merged to `asf-site`
 * Optinal: this ci can triggered by manual",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 06:58:33 UTC 2024,,,,,,,,,,"0|z1o90o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 12:27;gongzhongqiang;[~martijnvisser] Can you give me some suggestion? I'm looking forward to hear from you. Thank you!;;;","27/Mar/24 13:46;martijnvisser;[~gongzhongqiang]  ASF policies don't allow anyone else besides committers (not even CI) to commit files. So this can't happen;;;","27/Mar/24 14:01;gongzhongqiang;[~martijnvisser] Thanks for your patience in explaining.;;;","28/Mar/24 08:20;mapohl;I guess we could do it. The [GitHub Actions Policy|https://infra.apache.org/github-actions-policy.html] excludes non-released artifacts like websites from the restriction:
{quote}Automated services such as GitHub Actions (and Jenkins, BuildBot, etc.) MAY work on website content and other non-released data such as documentation and convenience binaries. Automated services MUST NOT push data to a repository or branch that is subject to official release as a software package by the project, unless the project secures specific prior authorization of the workflow from Infrastructure.
{quote}
Not sure whether they updated that one recently. Or do you have another source which is stricter, [~martijnvisser] ?;;;","28/Mar/24 09:13;gongzhongqiang;I have a stupid question：Can we build website by ci and use Rsync Deployments Action to sync website content instead of maintain in repo, just like flink document website? ;;;","28/Mar/24 09:35;martijnvisser;[~mapohl] I guess our project-website isn't part of the official release as a software package, so that could open up opportunities;;;","29/Mar/24 02:28;gongzhongqiang;[~mapohl] [~martijnvisser] Could we reach a consensus on this matter? Or let's start a discussion in community.;;;","02/Apr/24 06:58;mapohl;Hi [~gongzhongqiang], it sounds like we reached consensus in this matter already. But you can bring this up in the dev ML to check whether there are some objections against this approach before going ahead with this ticket to have a proper backing from the community.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC pipeline supports SinkFunction ,FLINK-34952,13573512,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,loserwang1024,loserwang1024,loserwang1024,27/Mar/24 11:06,09/Apr/24 07:38,04/Jun/24 20:40,09/Apr/24 07:38,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"Though current Flink CDC pipeline define com.ververica.cdc.common.sink.FlinkSinkFunctionProvider to to provide a Flink SinkFunction for writing events to external systems. However, com.ververica.cdc.runtime.operators.sink.DataSinkWriterOperator don't support SouceFunction, which means sink implement SinkFunction cannot use CDC pipeline.
Why not support  SourceFunction in Flink CDC pipeline ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 07:38:10 UTC 2024,,,,,,,,,,"0|z1o8zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 07:38;renqs;flink-cdc master: 8c5437a1f110b337defa42275a16037f9f169993;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-ci-mirror stopped running for commits after 22nd of March,FLINK-34951,13573498,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,27/Mar/24 09:30,02/Apr/24 03:16,04/Jun/24 20:40,02/Apr/24 03:16,1.20.0,,,,,,1.20.0,,,,Build System / CI,,,,,0,,,,"Blocker since it impacts all branches

if we look at {{flink-ci.flink-master-mirror}} https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1

we could see that ci was run against commit https://github.com/flink-ci/flink-mirror/commit/4edafcc8b0b96920036a1afaaa37ae87b77668ed

The problem is that this commit was done on 22nd of March and after that there were a number of other commits. At the same time there no any ci at {{flink-ci.flink-master-mirror}} for commits after that

Same check could be done for other branches

and it doesn't run for newer commits
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 03:15:44 UTC 2024,,,,,,,,,,"0|z1o8w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 09:32;Sergey Nuyanzin;//cc all release managers since it blocks the builds for releases
[~rmetzger], [~uce], [~fanrui], [~guoweijie]

also [~jingge] since he was able to resolve similar issue last time FLINK-33074
;;;","02/Apr/24 03:15;Weijie Guo;I have reached out [~jingge]. Thanks to him for helping us fix this issue, close this ticket now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable spotless on Java 21 for connector-shared-utils,FLINK-34950,13573496,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,27/Mar/24 09:00,28/Mar/24 14:00,04/Jun/24 20:40,28/Mar/24 14:00,connector-parent-1.1.0,,,,,,connector-parent-1.2.0,,,,Connectors / Parent,,,,,0,pull-request-available,,,"after https://github.com/apache/flink-connector-shared-utils/pull/19
spotless was stopped being skipped for java17+ in parent pom
however we still need to skip it for java21+",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 17:43:35 UTC 2024,,,,,,,,,,"0|z1o8vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 17:43;Sergey Nuyanzin;Merged as [d719c95235db17f5932d1bb5d917f7d6e195c371|https://github.com/apache/flink-connector-shared-utils/commit/d719c95235db17f5932d1bb5d917f7d6e195c371];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suggestion notify / skip DDL at data inegration famework v3.X,FLINK-34949,13573490,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,seung-min,seung-min,27/Mar/24 08:29,27/Mar/24 08:30,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,,,,"Discussion under feature at data inegration famework v3.X ([~renqs] )

 

 

PR: [[mysql] notify DDL with GTIDs and skip all queries after DDL by SML0127 · Pull Request #3055 · apache/flink-cdc (github.com)|https://github.com/apache/flink-cdc/pull/3055?notification_referrer_id=NT_kwDOAM9bI7M5MzAxOTA4OTcxOjEzNTg5Mjgz]

We *implemented a notification DDL with GTIDs and skip DDL and after all queries after DDL*

Our team use Flink CDC to perform MySql CDC.
But since our team sources other team's database and table, we had a need to be notified about DDL.
Because without appling DDL to our target table, kafak connect that we used occurs erros (conflict table schema between chagne event log and kafka connect).
----
Here's the our team use case with more detail.
 # We source tables from other teams.
 # And we send change event log from flink to kafka to use kafka connect (w. Debezium's JDBC Sink Connector).
 # When DDL performed and schema in change event log changed, error occur in kafka connect. Because Kafka connect loads the table schema on startup, but the schema is different from the schema in the change event log and kafka connect has.
 # We don't want to occur above error, so we get notified that DDL has been performed in source table and skip the DDL and any subsequent queries.
 # When notified, we apply the same DDL to our target table, restart Kafka connect (to load the changed target table's schema), and then rerun the flink cdc app using the returned GTIDs.

 

We can't share a real picture of the notification, because our company recommends using in-house tools rather than Slack(🥲🥲🥲) and has some security policy..

But it looks something like the format below!
 * Notification about performed DDL

[SKIPPED DDL]
Host: database.host.com
DB & Table: db_name.table_name
GTIDs: UUID:1-100
Query: alter table table_name ...(performed DD)

  * Notification about first performed DML after DDL.

[FIRST SKIPPED DML]
Host: database.host.com
DB & Table: db_name.table_name
GTIDs: UUID:1-101
Event: Event\{header=EventHeaderV4{timestamp=1706603179000, eventType=EXT_UPDATE_ROWS, serverId=101761, headerLength=19, dataLength=527, nextPosition=21284895, flags=0}, data=UpdateRowsEventData\{tableId=1797, includedColumnsBeforeUpdate={0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38}, includedColumns=\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38}, rows=[    \{before=[...], after=[...]}]}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-27 08:29:14.0,,,,,,,,,,"0|z1o8ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC RowType can not convert to flink row type,FLINK-34948,13573483,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhongqishang,zhongqishang,zhongqishang,27/Mar/24 07:45,01/Apr/24 01:56,04/Jun/24 20:40,01/Apr/24 01:56,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"Fix cdc {{RowType}} can not convert to flink type

I meet the follow exception:

 
{code:java}
java.lang.ArrayStoreException
    at java.lang.System.arraycopy(Native Method)
    at java.util.Arrays.copyOf(Arrays.java:3213)
    at java.util.ArrayList.toArray(ArrayList.java:413)
    at java.util.Collections$UnmodifiableCollection.toArray(Collections.java:1036) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 01:56:49 UTC 2024,,,,,,,,,,"0|z1o8sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 11:14;loserwang1024;Can you provide a case or other more detailed explanation?;;;","27/Mar/24 12:01;zhongqishang;[~loserwang1024] There is no corresponding scenario in FlinkCDC. I used this conversion in the custom program, connected to a sink based on Flink RowType.

You can refer to the Test code in the [PR](https://github.com/apache/flink-cdc/pull/3130).;;;","01/Apr/24 01:56;jiabaosun;Fixed via cdc master: d099603ef15d9a1ed7ec33718db7ab2438ef1ab5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only scale down JM in Foreground deletion propagation and reduce timeout,FLINK-34947,13573474,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,27/Mar/24 06:45,15/Apr/24 11:50,04/Jun/24 20:40,15/Apr/24 11:50,kubernetes-operator-1.8.0,,,,,,kubernetes-operator-1.9.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"We introduced a logic to scale down the JobManager before the task managers are killed to have a more graceful shutdown sequence.

Currently this is always done in native mode, but it makes more sense to restrict this functionality to foreground deletion. Foreground deletion is the default and recommended propagation but in some cases users may want to use background deletion to speed up the process. In these cases we should not force a synchronous, potentially slow step.

Furthermore instead of using the entire deletion timeout we should reduce this to only use a portion of it as it's an optional step and we should always have keep enough time for the TM shutdown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 11:50:57 UTC 2024,,,,,,,,,,"0|z1o8qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 11:50;gyfora;merged to main a3ef0485c4e871ca5572f32379e69e0f8a49f627;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Website Menu Adjustment,FLINK-34946,13573469,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gongzhongqiang,gongzhongqiang,gongzhongqiang,27/Mar/24 06:16,01/Apr/24 08:41,04/Jun/24 20:40,01/Apr/24 08:41,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,"{color:#000000}Discuss Thread：[https://lists.apache.org/thread/fvxrhc9nxzkjkt9t4ctmmopy9wlbp7wp]{color}
Expected Outcome :
 
 - Menu ""Getting Started""

Before: 
 * With Flink
 * With Flink Stateful Functions
 * With Flink ML
 * With Flink Kubernetes Operator
 * With Paimon(incubating) (formerly Flink Table Store)
 * With Flink CDC
 * Training Course

 
After: 
 * With Flink
 * With Flink Kubernetes Operator
 * With Flink CDC
 * With Flink ML
 * With Flink Stateful Functions
 * Training Course

 
 - Menu ""Documentation"" will same with ""Getting Started""
 - Add cdc repo to {{community.md}} , update repo order same with Menu ""Getting Started""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 08:41:03 UTC 2024,,,,,,,,,,"0|z1o8ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 09:19;gongzhongqiang; [~leonard] [~renqs] I'm willing to take this.;;;","01/Apr/24 08:41;renqs;flink-web asf-site: 9d0761c266d29ae81efc95bc22b9c262fa9e487f
rebuild by: 434203cd8ece4d308629881b5b293a45b15e1e13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support recover shuffle descriptor and partition metrics from tiered storage,FLINK-34945,13573455,13562406,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,JunRuiLi,JunRuiLi,JunRuiLi,27/Mar/24 03:54,23/May/24 08:28,04/Jun/24 20:40,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 23 08:28:57 UTC 2024,,,,,,,,,,"0|z1o8mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/24 08:28;JunRuiLi;If we use only the task manager (TM) shuffle, no special actions are required to support batch job recovery when enable tiered storage.

And this ticket intends to address the scenario where tiered storage is employed alongside remote shuffle.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Incremental Source Framework in Flink CDC OceanBase Source Connector,FLINK-34944,13573453,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,wanghe,wanghe,wanghe,27/Mar/24 03:27,21/May/24 05:06,04/Jun/24 20:40,,,,,,,,cdc-3.2.0,,,,Flink CDC,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-27 03:27:47.0,,,,,,,,,,"0|z1o8m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support Flink 1.19, 1.20-SNAPSHOT for JDBC connector",FLINK-34943,13573444,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gongzhongqiang,gongzhongqiang,27/Mar/24 01:55,28/Mar/24 01:31,04/Jun/24 20:40,28/Mar/24 01:31,,,,,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 01:31:27 UTC 2024,,,,,,,,,,"0|z1o8k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 08:18;Sergey Nuyanzin;fyi there is already an existing PR for that submitted  about 10 days ago and passing existing tests
https://github.com/apache/flink-connector-jdbc/pull/107;;;","28/Mar/24 01:31;gongzhongqiang;Fixed by https://github.com/apache/flink-connector-jdbc/pull/107;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support Flink 1.19, 1.20-SNAPSHOT for OpenSearch connector",FLINK-34942,13573435,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,26/Mar/24 23:19,24/May/24 09:17,04/Jun/24 20:40,15/May/24 06:38,,,,,,,opensearch-1.2.0,opensearch-2.0.0,,,Connectors / Opensearch,,,,,0,pull-request-available,,,Currently it fails with similar issue as FLINK-33493,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 15 06:38:45 UTC 2024,,,,,,,,,,"0|z1o8i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/24 06:38;Sergey Nuyanzin;Merged as [00f1a5b13bfbadcb8efce8e16fb06ddea0d8e48e|https://github.com/apache/flink-connector-opensearch/commit/00f1a5b13bfbadcb8efce8e16fb06ddea0d8e48e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot convert org.apache.flink.streaming.api.CheckpointingMode  to org.apache.flink.core.execution.CheckpointingMode,FLINK-34941,13573430,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,26/Mar/24 21:27,27/Mar/24 08:14,04/Jun/24 20:40,27/Mar/24 08:14,1.20.0,,,,,,1.20.0,,,,API / Core,Connectors / ElasticSearch,Runtime / Checkpointing,,,0,pull-request-available,,,"After this change FLINK-34516 elasticsearch connector for 1.20-SNAPSHOT starts failing with
{noformat}
 Error:  /home/runner/work/flink-connector-elasticsearch/flink-connector-elasticsearch/flink-connector-elasticsearch-e2e-tests/flink-connector-elasticsearch-e2e-tests-common/src/main/java/org/apache/flink/streaming/tests/ElasticsearchSinkE2ECaseBase.java:[75,5] method does not override or implement a method from a supertype
Error:  /home/runner/work/flink-connector-elasticsearch/flink-connector-elasticsearch/flink-connector-elasticsearch-e2e-tests/flink-connector-elasticsearch-e2e-tests-common/src/main/java/org/apache/flink/streaming/tests/ElasticsearchSinkE2ECaseBase.java:[85,84] incompatible types: org.apache.flink.streaming.api.CheckpointingMode cannot be converted to org.apache.flink.core.execution.CheckpointingMode
{noformat}
https://github.com/apache/flink-connector-elasticsearch/actions/runs/8436631571/job/23104522666#step:15:12668

set blocker since now every build of elasticsearch connector against  1.20-SNAPSHOT  is failing
probably same issue is for opensearch connector",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 08:14:19 UTC 2024,,,,,,,,,,"0|z1o8h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 08:14;Sergey Nuyanzin;Merged as [7d390a982f1f6c42b98ce85ed38f60221bb7b526|https://github.com/apache/flink/commit/7d390a982f1f6c42b98ce85ed38f60221bb7b526];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaderContender implementations handle invalid state,FLINK-34940,13573343,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,26/Mar/24 12:40,26/Mar/24 12:40,04/Jun/24 20:40,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,"Currently, LeaderContender implementations (e.g. see [ResourceManagerServiceImplTest#grantLeadership_withExistingLeader_waitTerminationOfExistingLeader|https://github.com/apache/flink/blob/master/flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerServiceImplTest.java#L219]) allow the handling of leader events of the same type happening after each other which shouldn't be the case.

Two subsequent leadership grants indicate that the leading instance which received the leadership grant again missed the leadership revocation event causing an invalid state of the overall deployment (i.e. split brain scenario). We should fail fatally in these scenarios rather than handling them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-26 12:40:23.0,,,,,,,,,,"0|z1o7xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Harden TestingLeaderElection,FLINK-34939,13573326,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,26/Mar/24 10:50,26/Mar/24 13:33,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"The {{TestingLeaderElection}} implementation does not follow the interface contract of {{LeaderElection}} in all of its facets (e.g. leadership acquire and revocation events should be alternating).

This issue is about hardening {{LeaderElection}} contract in the test implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-26 10:50:32.0,,,,,,,,,,"0|z1o7u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect behaviour for comparison functions,FLINK-34938,13573324,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,26/Mar/24 10:45,27/Mar/24 08:39,04/Jun/24 20:40,27/Mar/24 08:39,1.19.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"There are a few issues with comparison functions.

Some versions throw:
{code}
Incomparable types: TIMESTAMP_LTZ(3) NOT NULL and TIMESTAMP(3)
{code}

Results of some depend on the comparison order, because the least restrictive precision is not calculated correctly.

E.g.

{code}
        final Instant ltz3 = Instant.ofEpochMilli(1_123);
        final Instant ltz0 = Instant.ofEpochMilli(1_000);

        TestSetSpec.forFunction(BuiltInFunctionDefinitions.EQUALS)
                        .onFieldsWithData(ltz3, ltz0)
                        .andDataTypes(TIMESTAMP_LTZ(3), TIMESTAMP_LTZ(0))
                        // compare same type, but different precision, should always adjust to the higher precision
                        .testResult($(""f0"").isEqual($(""f1"")), ""f0 = f1"", false, DataTypes.BOOLEAN())
                        .testResult($(""f1"").isEqual($(""f0"")), ""f1 = f0"", true /* but should be false */, DataTypes.BOOLEAN())
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 08:39:24 UTC 2024,,,,,,,,,,"0|z1o7tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 08:39;dwysakowicz;Implemented in 7ea5bcce6a58b69543b571e9746d7374ded028c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Infra GHA policy update,FLINK-34937,13573312,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,26/Mar/24 09:20,03/May/24 05:53,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,Build System / CI,,,,,0,pull-request-available,,,"There is a policy update [announced in the infra ML|https://www.mail-archive.com/jdo-dev@db.apache.org/msg13638.html] which asked Apache projects to limit the number of runners per job. Additionally, the [GHA policy|https://infra.apache.org/github-actions-policy.html] is referenced which I wasn't aware of when working on the action workflow.

This issue is about applying the policy to the Flink GHA workflows.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34989,,FLINK-33901,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 11:36:51 UTC 2024,,,,,,,,,,"0|z1o7qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/24 10:50;m.orazow;Hey [~mapohl], the first link is unreachable or is it only visible to infra subscribers?

 

Would it be possible to create some actionable points from the GHA policy? They are a little vague, not sure GHA offers matching settings for each point. For example, using something like `concurrency.cancel-in-progress` etc.;;;","26/Mar/24 10:56;mapohl;I updated the link to refer to a publicly available resource (y)

I haven't gone through the policy in detail. We might have to get back to infra if things are unclear. For this, it might be worth it to respond in the [infra ML thread|https://lists.apache.org/thread/6qw21x44q88rc3mhkn42jgjjw94rsvb1] (for which you would have to subscribe);;;","27/Mar/24 15:41;mapohl;We should pin all actions (i.e. use the git SHA rather than a version tag) for external actions (anything other than {{actions/\*}}, {{github/\*}} and {{apache/\*}} prefixed actions). That's not the case right now.;;;","28/Mar/24 07:56;mapohl;let's check https://github.com/assignUser/stash (which is provided by [~assignuser] from the Apache Arrow project and promoted in Apache Infra's roundtable group) whether our CI can benefit from it;;;","28/Mar/24 14:36;mapohl;Looks like Flink is on rank 19 in terms of runner minutes used for the past 7 days:

[Flink-specific report|https://infra-reports.apache.org/#ghactions&project=flink&hours=168] (needs ASF committer rights)

[Global report|https://infra-reports.apache.org/#ghactions] (needs ASF membership);;;","02/Apr/24 11:36;mapohl;I moved the runner usage discussion into FLINK-34989;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Register reused state handles to FileMergingSnapshotManager,FLINK-34936,13573284,13536037,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,lijinzhong,lijinzhong,26/Mar/24 06:17,17/Apr/24 10:32,04/Jun/24 20:40,17/Apr/24 10:32,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"The shared state files should be registered into the FileMergingSnapshotManager, so that these files can be properly cleaned up  when checkpoint aborted/subsumed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 10:32:41 UTC 2024,,,,,,,,,,"0|z1o7ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/24 06:57;zakelly;Thanks for reporting this! Assigned to you [~lijinzhong];;;","09/Apr/24 12:24;zakelly;Since [~lijinzhong] is kind of busy recently, I'll take over this.;;;","17/Apr/24 10:32;zakelly;Merged into master via 31ea1a93;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TIMESTAMP_LTZ type Unsupported when using JdbcCatalog to read from Postgres,FLINK-34935,13573281,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,frankie.du,frankie.du,26/Mar/24 05:24,22/May/24 15:22,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Connectors / JDBC,,,,,0,flink,jdbc_connector,postgres,"When I use JdbcCatalog to select from postgres table, it throw Exception:
{code:java}
Exception in thread ""main"" java.lang.UnsupportedOperationException: Unsupported type:TIMESTAMP_LTZ(6)
    at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.createInternalConverter(AbstractJdbcRowConverter.java:186)
    at org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.createPrimitiveConverter(PostgresRowConverter.java:99)
    at org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.createInternalConverter(PostgresRowConverter.java:58)
    at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.createNullableInternalConverter(AbstractJdbcRowConverter.java:118)
    at org.apache.flink.connector.jdbc.converter.AbstractJdbcRowConverter.<init>(AbstractJdbcRowConverter.java:68)
    at org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter.<init>(PostgresRowConverter.java:47)
    at org.apache.flink.connector.jdbc.dialect.psql.PostgresDialect.getRowConverter(PostgresDialect.java:50)
    at org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource.getScanRuntimeProvider(JdbcDynamicTableSource.java:182)
    at org.apache.flink.table.planner.connectors.DynamicSourceUtils.validateScanSource(DynamicSourceUtils.java:466)
    at org.apache.flink.table.planner.connectors.DynamicSourceUtils.prepareDynamicSource(DynamicSourceUtils.java:161)
    at org.apache.flink.table.planner.connectors.DynamicSourceUtils.convertSourceToRel(DynamicSourceUtils.java:125)
    at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:118)
    at org.apache.calcite.rel.core.RelFactories$TableScanFactoryImpl.createScan(RelFactories.java:481)
    at org.apache.calcite.tools.RelBuilder.scan(RelBuilder.java:1757)
    at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:366)
    at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:158)
    at org.apache.flink.table.operations.SourceQueryOperation.accept(SourceQueryOperation.java:86)
    at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:155)
    at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:135)
    at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:92)
    at org.apache.flink.table.operations.SourceQueryOperation.accept(SourceQueryOperation.java:86)
    at org.apache.flink.table.planner.calcite.FlinkRelBuilder.queryOperation(FlinkRelBuilder.java:261)
    at org.apache.flink.table.planner.catalog.QueryOperationCatalogViewTable.convertToRel(QueryOperationCatalogViewTable.java:80)
    at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.expand(ExpandingPreparingTable.java:70)
    at org.apache.flink.table.planner.plan.schema.ExpandingPreparingTable.toRel(ExpandingPreparingTable.java:57)
    at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3743)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2666)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2233)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2147)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2092)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:700)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:686)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3589)
    at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:599)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:216)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:192)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:1580)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:1285)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertValidatedSqlNode(SqlToOperationConverter.java:397)
    at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:282)
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:738)
    at com.demo.flow.ExportCsvFlow.run(ExportCsvFlow.java:76)
    at com.demo.DataflowJob.main(DataflowJob.java:110) {code}
 

The source table is a postgres table, which schema is:
{code:java}
create table t_test (
 id integer primary key,
 name timestamptz
);{code}
 

I've read the relevant code and roughly located where the problem is. Can I make some code contribution to fix this problem?","flink: 1.17.0
flink-connector-jdbc: 3.1.0-1.17
postgres: 14.5
java: 11",,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,java,Wed May 22 15:22:26 UTC 2024,,,,,,,,,,"0|z1o7k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/24 15:22;pietro97;Related issue: FLINK-35053;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translation Flink-Kubernetes-Operator document framework construction,FLINK-34934,13573274,13573144,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,caicancai,caicancai,caicancai,26/Mar/24 02:32,01/Apr/24 04:54,04/Jun/24 20:40,01/Apr/24 04:54,1.9.0,,,,,,1.9.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"Build the Flink-Kubernetes-Operator document translation framework, and subsequently translate English documents one by one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-26 02:32:41.0,,,,,,,,,,"0|z1o7ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterServiceLeadershipRunnerTest#testResultFutureCompletionOfOutdatedLeaderIsIgnored isn't implemented properly,FLINK-34933,13573220,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,25/Mar/24 15:41,28/Mar/24 13:01,04/Jun/24 20:40,28/Mar/24 13:01,1.17.2,1.18.1,1.19.0,1.20.0,,,1.18.2,1.19.1,1.20.0,,Runtime / Coordination,,,,,0,pull-request-available,,,"{{testResultFutureCompletionOfOutdatedLeaderIsIgnored}} doesn't test the desired behavior: The {{TestingJobMasterService#closeAsync()}} callback throws an {{UnsupportedOperationException}} by default which prevents the test from properly finalizing the leadership revocation.

The test is still passing because the test checks implicitly for this error. Instead, we should verify that the runner's resultFuture doesn't complete until the runner is closed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34672,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 13:01:01 UTC 2024,,,,,,,,,,"0|z1o76g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/24 13:01;mapohl;master: [1668a07276929416469392a35a77ba7699aac30b|https://github.com/apache/flink/commit/1668a07276929416469392a35a77ba7699aac30b]
1.19: [c11656a2406f07e2ae7cd6f80c46afb14385ee0e|https://github.com/apache/flink/commit/c11656a2406f07e2ae7cd6f80c46afb14385ee0e]
1.18: [94d1363c27e26fc8313721e138c7b4de744ca69e|https://github.com/apache/flink/commit/94d1363c27e26fc8313721e138c7b4de744ca69e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate concepts of Flink-Kubernetes-Operator documentation,FLINK-34932,13573215,13573144,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,caicancai,caicancai,caicancai,25/Mar/24 14:31,07/Apr/24 02:17,04/Jun/24 20:40,,1.9.0,,,,,,1.9.0,,,,chinese-translation,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-25 14:31:30.0,,,,,,,,,,"0|z1o75c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Kudu DataStream connector to use Sink V2,FLINK-34931,13573186,13573180,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,ferenc-csaky,ferenc-csaky,25/Mar/24 12:30,14/May/24 14:42,04/Jun/24 20:40,,,,,,,,,,,,Connectors / Kudu,,,,,0,pull-request-available,,,Update the DataSource API classes to use the current interfaces.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-25 12:30:37.0,,,,,,,,,,"0|z1o6yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move existing Kudu connector code from Bahir repo to dedicated repo,FLINK-34930,13573182,13573180,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,25/Mar/24 12:23,14/May/24 07:06,04/Jun/24 20:40,14/May/24 07:06,,,,,,,kudu-2.0.0,,,,Connectors / Kudu,,,,,0,pull-request-available,,,"Move the existing Kudu connector code from the Bahir [1] repository to the dedicated connector repo.

Code should be moved only with necessary changes (bump version, change groupID, integrate to common connector CI) and we sould state explicitly that the state was forked from the Bahir repo.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 07:05:47 UTC 2024,,,,,,,,,,"0|z1o6y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 07:05;martijnvisser;Code has been migrated;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Create ""flink-connector-kudu"" repository",FLINK-34929,13573181,13573180,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,ferenc-csaky,ferenc-csaky,25/Mar/24 12:22,26/Mar/24 09:01,04/Jun/24 20:40,26/Mar/24 09:00,,,,,,,,,,,Connectors / Kudu,,,,,0,,,,"We should create a ""flink-connector-kudu"" repositry under the ""apache"" GitHub organization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 26 09:00:53 UTC 2024,,,,,,,,,,"0|z1o6xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/24 09:00;martijnvisser;Repo created and initialized ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-439: Externalize Kudu Connector from Bahir,FLINK-34928,13573180,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ferenc-csaky,ferenc-csaky,25/Mar/24 12:20,15/Apr/24 13:27,04/Jun/24 20:40,,,,,,,,,,,,Connectors / Kudu,,,,,0,,,,Umbrella issue for: https://cwiki.apache.org/confluence/display/FLINK/FLIP-439%3A+Externalize+Kudu+Connector+from+Bahir,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-25 12:20:32.0,,,,,,,,,,"0|z1o6xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate flink-kubernetes-operator documentation,FLINK-34927,13573144,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,caicancai,caicancai,caicancai,25/Mar/24 02:44,31/Mar/24 03:34,04/Jun/24 20:40,,,,,,,,kubernetes-operator-1.9.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"Currently, the flink-kubernetes-operator documentation is only in English. I hope it can be translated into Chinese so that more Chinese users can use it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 06:50:34 UTC 2024,,,,,,,,,,"0|z1o6pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/24 02:48;caicancai;[~fanrui] [~gyfora] Hello, you can check to see if you accept this feature. If you accept it, I will create the corresponding subtask. At present, I have translated part of the document. Thank you.;;;","25/Mar/24 03:03;fanrui;Thanks [~caicancai] driving it! 

Translating flink-kubernetes-operator documentation into Chinese is useful for flink community and Chinese flink users. I saw flink documentation has Chinese documentation before, and flink-cdc is translating in FLINK-34730. 

Translating flink-kubernetes-operator documentation into Chinese will make it easier for Chinese flink users to use flink-kubernets-operator and autoscaler. If community think it's acceptable, I'm happy to review it.:)

;;;","25/Mar/24 06:50;gyfora;I think this would be great, I won't be able to review the content though :D ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive auto parallelism doesn't work for a query,FLINK-34926,13573136,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xccui,xccui,24/Mar/24 22:57,24/Mar/24 22:59,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Table SQL / Planner,,,,,0,,,,"We have the following query running in batch mode.
{code:java}
WITH FEATURE_INCLUSION AS (
    SELECT
        insertion_id, -- Not unique
        features -- Array<Row<key, value>>
    FROM
        features_table
),
TOTAL AS (
    SELECT
        COUNT(DISTINCT insertion_id) total_id
    FROM
        FEATURE_INCLUSION
),
FEATURE_INCLUSION_COUNTS AS (
    SELECT
        `key`,
        COUNT(DISTINCT insertion_id) AS id_count
    FROM
        FEATURE_INCLUSION,
        UNNEST(features) as t (`key`, `value`)
    WHERE
        TRUE
    GROUP BY
        `key`
),
RESULTS AS (
    SELECT
        `key`
    FROM
        FEATURE_INCLUSION_COUNTS,
        TOTAL
    WHERE
       (1.0 * id_count)/total_id > 0.1
)
SELECT
    JSON_ARRAYAGG(`key`) AS feature_ids,
FROM
    RESULTS{code}
The parallelism adaptively set by Flink for the following operator was always 1.
{code:java}
[37]:HashAggregate(isMerge=[true], groupBy=[key, insertion_id], select=[key, insertion_id])
+- [38]:LocalHashAggregate(groupBy=[key], select=[key, Partial_COUNT(insertion_id) AS count$0]){code}
If we turn off `execution.batch.adaptive.auto-parallelism.enabled` and manually set `parallelism.default` to be greater than one, it worked.

The screenshot of the full job graph is attached.  !image.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/24 22:59;xccui;image.png;https://issues.apache.org/jira/secure/attachment/13067650/image.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-24 22:57:56.0,,,,,,,,,,"0|z1o6ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
" FlinkCDC 3.0 for extracting data from sqlserver,  errors occur",FLINK-34925,13573107,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,chenyu.mr,chenyu.mr,24/Mar/24 10:24,26/Mar/24 05:37,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,,,,"when I use FlinkCDC 3.0 for extracting data from sqlserver,  errors occur

!image-2024-03-24-18-12-52-747.png!

.........is referenced as PRIMARY KEY, but a matching column is not defined in table.........

I found some information on Debezuim's website，The official website says，

This bug was fixed in the debe 2.0 version, I checked the current flinkcdc debezuim is version 1.97, I want to know what is the cause of this problem, can I directly upgrade the debezuim version to fix it? Debezuim's  link   [Debezium 2.0.0.Beta1 Released|https://debezium.io/blog/2022/07/27/debezium-2.0-beta1-released/]

!image-2024-03-24-18-23-19-657.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/24 10:12;chenyu.mr;image-2024-03-24-18-12-52-747.png;https://issues.apache.org/jira/secure/attachment/13067644/image-2024-03-24-18-12-52-747.png","24/Mar/24 10:23;chenyu.mr;image-2024-03-24-18-23-19-657.png;https://issues.apache.org/jira/secure/attachment/13067643/image-2024-03-24-18-23-19-657.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 26 05:37:40 UTC 2024,,,,,,,,,,"0|z1o6hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/24 01:57;loserwang1024;Debezium 2.0 is a major version upgrade which does not guarantee compatibility. You can make some minor changes based on [DBZ-5398|https://issues.redhat.com/browse/DBZ-5398] similar to what is done in [https://github.com/apache/flink-cdc/pull/2842].;;;","25/Mar/24 06:33;chenyu.mr;[~loserwang1024]   thanks for ur replay,  may I have your details  about the problem,  process steps  and so on.,Thanks again.;;;","25/Mar/24 12:14;loserwang1024;This  [DBZ-5398|https://issues.redhat.com/browse/DBZ-5398] is for PG Table with functional index, maybe same problem with sqlserver? What's your table DDL?;;;","26/Mar/24 01:37;chenyu.mr;  thanks for ur replay again.  my table  DDL sql is  

 

SET ANSI_NULLS ON
GO
 
SET QUOTED_IDENTIFIER ON
GO
 
CREATE TABLE [dbo].[history](
    [id] [dbo].[dtMMM_TableID] IDENTITY(1,1) NOT NULL,
    [mmm_code_id] [int] NULL,
    [reference_no] [nvarchar](255) NOT NULL,
    [img_url] [text] NULL,
    [quantity] [float] NOT NULL,
    [handover_from] [nvarchar](255) NOT NULL,
    [handover_to] [nvarchar](255) NOT NULL,
    [handover_time] [datetime] NOT NULL,
    [return_type] [nvarchar](50) NULL,
    [PhysicalCheck] [bit] NULL,
    [reference_type] [varchar](32) NULL,
    [sign_off_no] [varchar](32) NULL,
    [part_no] [varchar](64) NULL,
    [serial_no] [varchar](64) NULL,
    [remark] [varchar](250) NULL,
    [version] [int] NULL,
    [exception] [varchar](500) NULL,
    [update_time] [datetime] NULL,
    [update_by] [varchar](16) NULL,
CONSTRAINT [PK_history] PRIMARY KEY CLUSTERED 
(
    [id] ASC
)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]
) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY]
GO

 

 ;;;","26/Mar/24 02:14;loserwang1024;[~gongzhongqiang] , CC, I am not familiar with Sqlserver. Whether is it an index format that Debezium cannot parse.;;;","26/Mar/24 05:37;chenyu.mr;[~loserwang1024] Maybe,I dont konw how to resolve it, and why.if u have any  views ,pls contact me. thanks a a lot.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support partition pushdown for join queries,FLINK-34924,13573088,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jeyhunkarimov,jeyhunkarimov,23/Mar/24 23:44,28/Mar/24 05:12,04/Jun/24 20:40,,1.19.0,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"Consider the following tables: 
{code:java}
create table partitionedTable1 (
   a int, 
   b int, 
   c int)  
partitioned by (a, b) 
with ( ... )    {code}
 
{code:java}
create table partitionedTable2 (
    c int, 
    d int, 
    e int)  
 partitioned by (d, e) 
 with ( ... )  {code}
 

And the following queries:
{code:java}
select t1.b 
from partitionedTable1 t1 inner join partitionedTable2 t2 
on t1.a = t2.d 
where t1.a > 1

or 

select t1.b  from partitionedTable1 t1 inner join  partitionedTable2 t2 
on t1.a = t2.d and t1.b = t2.e 
where t1.a > 1{code}
 

For the above-mentioned queries, currently, the partition pushdown rules in Flink only consider the filter clause (t1.a > 1) and pushe the related partitions to the source operator. 

However, we should be able to also pushdown partitions because of join clause. Note that in the above-mentioned queries partitioned columns are the same as join fields (or prefix-subset of them). So, we can fetch existing partitions from each table, intersect them, and push their intersection to their source operators. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-23 23:44:35.0,,,,,,,,,,"0|z1o6d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Behavioral discrepancy between `TableEnvironment.execute_sql()` and `TableEnvironment.sql_query()`,FLINK-34923,13573027,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chloehe,chloehe,23/Mar/24 00:12,23/Mar/24 00:12,04/Jun/24 20:40,,1.19.0,,,,,,,,,,API / Python,,,,,0,,,,"I found that there is some behavioral discrepancy between `TableEnvironment.execute_sql()` and `TableEnvironment.sql_query()`.

A minimal reproducible example:
{code:java}
SELECT `value` FROM (VALUES (CAST(ARRAY[ROW(1, 2), ROW(2, 2)] AS ARRAY<ROW<`a` INT, `b` INT>>))) AS `t`(`value`) {code}
This throws
{code:java}
File ~/anaconda3/envs/ibis-dev-flink/lib/python3.10/site-packages/pyflink/table/table.py:943, in Table.to_pandas(self)
    939 import pytz
    940 timezone = pytz.timezone(
    941     self._j_table.getTableEnvironment().getConfig().getLocalTimeZone().getId())
    942 serializer = ArrowSerializer(
--> 943     create_arrow_schema(self.get_schema().get_field_names(),
    944                         self.get_schema().get_field_data_types()),
    945     self.get_schema().to_row_data_type(),
    946     timezone)
    947 import pyarrow as pa
    948 table = pa.Table.from_batches(serializer.load_from_iterator(batches_iterator))

File ~/anaconda3/envs/ibis-dev-flink/lib/python3.10/site-packages/pyflink/table/types.py:2194, in create_arrow_schema(field_names, field_types)
   2190 """"""
   2191 Create an Arrow schema with the specified filed names and types.
   2192 """"""
   2193 import pyarrow as pa
-> 2194 fields = [pa.field(field_name, to_arrow_type(field_type), field_type._nullable)
   2195           for field_name, field_type in zip(field_names, field_types)]
   2196 return pa.schema(fields)

File ~/anaconda3/envs/ibis-dev-flink/lib/python3.10/site-packages/pyflink/table/types.py:2194, in <listcomp>(.0)
   2190 """"""
   2191 Create an Arrow schema with the specified filed names and types.
   2192 """"""
   2193 import pyarrow as pa
-> 2194 fields = [pa.field(field_name, to_arrow_type(field_type), field_type._nullable)
   2195           for field_name, field_type in zip(field_names, field_types)]
   2196 return pa.schema(fields)

File ~/anaconda3/envs/ibis-dev-flink/lib/python3.10/site-packages/pyflink/table/types.py:2316, in to_arrow_type(data_type)
   2314 elif isinstance(data_type, ArrayType):
   2315     if type(data_type.element_type) in [LocalZonedTimestampType, RowType]:
-> 2316         raise ValueError(""%s is not supported to be used as the element type of ArrayType."" %
   2317                          data_type.element_type)
   2318     return pa.list_(to_arrow_type(data_type.element_type))
   2319 elif isinstance(data_type, RowType):

ValueError: ROW is not supported to be used as the element type of ArrayType. {code}
when I tried to execute it with `TableEnvironment.sql_query()`, but works when I tried it with `TableEnvironment.execute_sql()`:
{code:java}
+----+--------------------------------+
| op |                          value |
+----+--------------------------------+
| +I |               [(1, 2), (2, 2)] |
+----+--------------------------------+ {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,python,2024-03-23 00:12:03.0,,,,,,,,,,"0|z1o5zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception History should support multiple Global failures,FLINK-34922,13573013,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,pgaref,pgaref,22/Mar/24 19:39,28/Mar/24 17:06,04/Jun/24 20:40,28/Mar/24 16:46,1.18.0,,,,,,1.18.2,1.19.1,1.20.0,,Runtime / REST,,,,,0,pull-request-available,,,"Before source coordinators were introduced, global failures were rare and only triggered by the JM ensuring they only happened once per failure. Since this has changed now we should adjust accordingly and support multiple global failures as part of the exception history.

Relevant discussion under: https://github.com/apache/flink/pull/23440#pullrequestreview-1701775436",,,,,,,,,,,,,,,,,,,,,,,,FLINK-33121,,,,,,,FLINK-33121,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 12:47:08 UTC 2024,,,,,,,,,,"0|z1o5wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/24 12:47;chesnay;master:
dc957bfdc3aa6a8e3bce603cfc68c5c553c72220
f4c945cb9ca882ae485c2e58c74825938f154119
1.19:
faa880c703cadba4521fc8ef885a242ded4b2ac7
b54edc886ce5a533bafe74fa3629657b6266cad5
1.18:
fcb581f0039f9704b6eaf15a2fabaa4e05d79048
20c506d76c99c2e6c3f30a039acd0366d3448c87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SystemProcessingTimeServiceTest fails due to missing output,FLINK-34921,13572972,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,22/Mar/24 14:38,08/Apr/24 09:36,04/Jun/24 20:40,,1.20.0,,,,,,,,,,API / DataStream,,,,,0,test-stability,,,"This PR CI build with {{AdaptiveScheduler}} enabled failed:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58476&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=11224

{code}
""ForkJoinPool-61-worker-25"" #863 daemon prio=5 os_prio=0 tid=0x00007f8c19eba000 nid=0x60a5 waiting on condition [0x00007f8bc2cf9000]
Mar 21 17:19:42    java.lang.Thread.State: WAITING (parking)
Mar 21 17:19:42 	at sun.misc.Unsafe.park(Native Method)
Mar 21 17:19:42 	- parking to wait for  <0x00000000d81959b8> (a java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask)
Mar 21 17:19:42 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Mar 21 17:19:42 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
Mar 21 17:19:42 	at java.util.concurrent.FutureTask.get(FutureTask.java:191)
Mar 21 17:19:42 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest$$Lambda$1443/1477662666.call(Unknown Source)
Mar 21 17:19:42 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
Mar 21 17:19:42 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
Mar 21 17:19:42 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
Mar 21 17:19:42 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
Mar 21 17:19:42 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeServiceTest.testQuiesceAndAwaitingCancelsScheduledAtFixRateFuture(SystemProcessingTimeServiceTest.java:92)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20438,,,,,FLINK-34409,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 09:36:22 UTC 2024,,,,,,,,,,"0|z1o5nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 09:36;jiabaosun;Maybe we shouldn't use ScheduledFuture.get() to check the scheduled task is completed.

https://stackoverflow.com/questions/28116301/scheduledfuture-get-is-still-blocked-after-executor-shutdown;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderRetrievalConnectionHandlingTest fails with Exit Code 2,FLINK-34920,13572956,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,rskraba,rskraba,22/Mar/24 13:24,22/Mar/24 16:07,04/Jun/24 20:40,22/Mar/24 16:07,1.19.1,,,,,,,,,,Runtime / Coordination,,,,,0,test-stability,,,"[https://github.com/apache/flink/actions/runs/8384423618/job/22961979482#step:10:8939]
{code:java}
[ERROR] Process Exit Code: 2
[ERROR] Crashed tests:
[ERROR] org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest
[ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34475,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-22 13:24:42.0,,,,,,,,,,"0|z1o5js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebMonitorEndpointTest.cleansUpExpiredExecutionGraphs fails starting REST server,FLINK-34919,13572946,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,22/Mar/24 11:07,16/Apr/24 10:06,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,Runtime / Coordination,,,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58482&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8641]
{code:java}
Mar 22 04:12:50 04:12:50.260 [INFO] Running org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest
Mar 22 04:12:50 04:12:50.609 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.318 s <<< FAILURE! -- in org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest
Mar 22 04:12:50 04:12:50.609 [ERROR] org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest.cleansUpExpiredExecutionGraphs -- Time elapsed: 0.303 s <<< ERROR!
Mar 22 04:12:50 java.net.BindException: Could not start rest endpoint on any port in port range 8081
Mar 22 04:12:50 	at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:286)
Mar 22 04:12:50 	at org.apache.flink.runtime.webmonitor.WebMonitorEndpointTest.cleansUpExpiredExecutionGraphs(WebMonitorEndpointTest.java:69)
Mar 22 04:12:50 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 22 04:12:50 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 22 04:12:50 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 22 04:12:50 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 22 04:12:50 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 22 04:12:50 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Mar 22 04:12:50  {code}
This was noted as a symptom of FLINK-22980, but doesn't have the same failure.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 10:06:47 UTC 2024,,,,,,,,,,"0|z1o5hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/24 14:23;rskraba;1.19 https://github.com/apache/flink/actions/runs/8406174809/job/23019927478#step:10:8165;;;","08/Apr/24 14:52;rskraba;1.18 jdk17 https://github.com/apache/flink/actions/runs/8577964007/job/23511408119#step:10:7145
jdk8 https://github.com/apache/flink/actions/runs/8585675607/job/23527404664#step:10:8153
;;;","16/Apr/24 10:06;rskraba;1.19 Java 21: Test (module: core) https://github.com/apache/flink/actions/runs/8698881986/job/23856839686#step:10:8836;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce comment for Catalog,FLINK-34918,13572943,13572934,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,liyubin117,liyubin117,liyubin117,22/Mar/24 10:44,20/Apr/24 03:11,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Table SQL / API,,,,,0,,,,"We propose to introduce `getComment()` method in `CatalogDescriptor`, and the reasons are as follows.

1. For the sake of design consistency, follow the design of FLIP-295 [1] which introduced `CatalogStore` component, `CatalogDescriptor` includes names and attributes, both of which are used to describe the catalog, and `comment` can be added smoothly.

2. Extending the existing class rather than add new method to the existing interface, Especially, the `Catalog` interface, as a core interface, is used by a series of important components such as `CatalogFactory`, `CatalogManager` and `FactoryUtil`, and is implemented by a large number of connectors such as JDBC, Paimon, and Hive. Adding methods to it will greatly increase the implementation complexity, and more importantly, increase the cost of iteration, maintenance, and verification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34917,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-22 10:44:49.0,,,,,,,,,,"0|z1o5gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support enhanced `CREATE CATALOG` syntax,FLINK-34917,13572937,13572934,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,liyubin117,liyubin117,liyubin117,22/Mar/24 10:32,25/Mar/24 02:36,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Table SQL / API,,,,,0,,,,"{{IF NOT EXISTS}}  clause: If the catalog already exists, nothing happens.

{{COMMENT}} clause: An optional string literal. The description for the catalog.

NOTICE: we just need to introduce the '[IF NOT EXISTS]' and '[COMMENT]' clause to the 'create catalog' statement.

!image-2024-03-22-18-31-59-632.png|width=795,height=87!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34918,,,,"22/Mar/24 10:31;liyubin117;image-2024-03-22-18-31-59-632.png;https://issues.apache.org/jira/secure/attachment/13067608/image-2024-03-22-18-31-59-632.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-22 10:32:33.0,,,,,,,,,,"0|z1o5fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support `ALTER CATALOG SET` syntax,FLINK-34916,13572936,13572934,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,22/Mar/24 10:30,09/May/24 02:26,04/Jun/24 20:40,09/May/24 02:26,1.20.0,,,,,,1.20.0,,,,Table SQL / API,,,,,0,pull-request-available,,,"Set one or more properties in the specified catalog. If a particular property is already set in the catalog, override the old value with the new one.

!image-2024-03-22-18-30-33-182.png|width=736,height=583!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/24 10:30;liyubin117;image-2024-03-22-18-30-33-182.png;https://issues.apache.org/jira/secure/attachment/13067607/image-2024-03-22-18-30-33-182.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 09 02:26:45 UTC 2024,,,,,,,,,,"0|z1o5fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 02:26;qingyue;Fixed in master 4611817591c38019c27ffad6d8cdc68292f079a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complete `DESCRIBE CATALOG` syntax,FLINK-34915,13572935,13572934,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,22/Mar/24 10:27,28/Apr/24 11:58,04/Jun/24 20:40,28/Apr/24 11:58,1.20.0,,,,,,1.20.0,,,,Table SQL / API,,,,,0,pull-request-available,,,"Describe the metadata of an existing catalog. The metadata information includes the catalog’s name, type, and comment. If the optional {{EXTENDED}} option is specified, catalog properties are also returned.

NOTICE: The parser part of this syntax has been implemented in FLIP-69 , and it is not actually available. we can complete the syntax in this FLIP. 

!image-2024-04-07-17-54-51-203.png|width=545,height=332!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/24 10:28;liyubin117;image-2024-03-22-18-29-00-454.png;https://issues.apache.org/jira/secure/attachment/13067606/image-2024-03-22-18-29-00-454.png","07/Apr/24 09:54;liyubin117;image-2024-04-07-17-54-51-203.png;https://issues.apache.org/jira/secure/attachment/13067923/image-2024-04-07-17-54-51-203.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 28 11:58:15 UTC 2024,,,,,,,,,,"0|z1o5f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/24 11:58;qingyue;Fixed in master e412402ca4dfc438e28fb990dc53ea7809430aee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-436: Introduce Catalog-related Syntax,FLINK-34914,13572934,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,liyubin117,liyubin117,liyubin117,22/Mar/24 10:21,23/May/24 04:13,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Table SQL / API,,,,,0,,,,Umbrella issue for: https://cwiki.apache.org/confluence/display/FLINK/FLIP-436%3A+Introduce+Catalog-related+Syntax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-22 10:21:40.0,,,,,,,,,,"0|z1o5ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException SubTaskInitializationMetricsBuilder.addDurationMetric,FLINK-34913,13572920,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,22/Mar/24 08:14,23/Apr/24 07:47,04/Jun/24 20:40,23/Apr/24 07:47,1.19.0,,,,,,1.20.0,,,,Runtime / Metrics,Runtime / State Backends,,,,0,pull-request-available,,,"The following failures can occur during job's recovery when using clip & ingest. This code path is currently not available, so the bug can not happen for users.

{noformat}
java.util.ConcurrentModificationException
	at java.base/java.util.HashMap.compute(HashMap.java:1230)
	at org.apache.flink.runtime.checkpoint.SubTaskInitializationMetricsBuilder.addDurationMetric(SubTaskInitializationMetricsBuilder.java:49)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.runAndReportDuration(RocksDBIncrementalRestoreOperation.java:988)
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.lambda$createAsyncCompactionTask$2(RocksDBIncrementalRestoreOperation.java:291)
{noformat}

{noformat}
java.util.ConcurrentModificationException
	at java.base/java.util.HashMap.compute(HashMap.java:1230)
	at org.apache.flink.runtime.checkpoint.SubTaskInitializationMetricsBuilder.addDurationMetric(SubTaskInitializationMetricsBuilder.java:49)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreStateAndGates(StreamTask.java:794)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$restoreInternal$3(StreamTask.java:744)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:744)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:704)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:922)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.base/java.lang.Thread.run(Thread.java:829)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34199,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 10:01:22 UTC 2024,,,,,,,,,,"0|z1o5bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/24 10:01;pnowojski;merged commit 1a5ca7f into apache:master

The change was not backported to 1.19 as the code is disabled in that branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace all occurrences of com.ververica in the project with org.apache.flink,FLINK-34912,13572916,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,ShawnHx,ShawnHx,22/Mar/24 08:00,17/May/24 03:29,04/Jun/24 20:40,17/May/24 03:29,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 09:52:00 UTC 2024,,,,,,,,,,"0|z1o5aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/24 09:52;ShawnHx;There is a pr has done this.

[[hotfix] Change old com.ververica dependency to flink by xleoken · Pull Request #3110 · apache/flink-cdc (github.com)|https://github.com/apache/flink-cdc/pull/3110];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogRecoveryRescaleITCase failed fatally with 127 exit code,FLINK-34911,13572845,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rskraba,rskraba,21/Mar/24 15:02,28/Mar/24 13:44,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Runtime / State Backends,,,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58455&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=9029]

 
{code:java}
Mar 21 01:50:42 01:50:42.553 [ERROR] Command was /bin/sh -c cd '/__w/1/s/flink-tests' && '/usr/lib/jvm/jdk-21.0.1+12/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/1/s/flink-tests/target/surefire/surefirebooter-20240321010847189_810.jar' '/__w/1/s/flink-tests/target/surefire' '2024-03-21T01-08-44_720-jvmRun3' 'surefire-20240321010847189_808tmp' 'surefire_207-20240321010847189_809tmp'
Mar 21 01:50:42 01:50:42.553 [ERROR] Error occurred in starting fork, check output in log
Mar 21 01:50:42 01:50:42.553 [ERROR] Process Exit Code: 127
Mar 21 01:50:42 01:50:42.553 [ERROR] Crashed tests:
Mar 21 01:50:42 01:50:42.553 [ERROR] org.apache.flink.test.checkpointing.ChangelogRecoveryRescaleITCase
Mar 21 01:50:42 01:50:42.553 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456)
Mar 21 01:50:42 01:50:42.553 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:418)
Mar 21 01:50:42 01:50:42.553 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
Mar 21 01:50:42 01:50:42.553 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:250)
Mar 21 01:50:42 01:50:42.554 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1240)
{code}
From the watchdog, only {{ChangelogRecoveryRescaleITCase}} didn't complete, specifically parameterized with an {{EmbeddedRocksDBStateBackend}} with incremental checkpointing enabled.

The base class ({{{}ChangelogRecoveryITCaseBase{}}}) starts a {{MiniClusterWithClientResource}}
{code:java}
~/Downloads/CI/logs-cron_jdk21-test_cron_jdk21_tests-1710982836$ cat watchdog| grep ""Tests run\|Running org.apache.flink"" | grep -o ""org.apache.flink[^ ]*$"" | sort | uniq -c | sort -n | head
      1 org.apache.flink.test.checkpointing.ChangelogRecoveryRescaleITCase
      2 org.apache.flink.api.connector.source.lib.NumberSequenceSourceITCase
{code}
 
{color:#000000} {color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 13:44:58 UTC 2024,,,,,,,,,,"0|z1o4v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/24 13:44;xiarui;Hello Skraba
I see the system error in the log file:


{code:java}
Inconsistency detected by ld.so: dl-tls.c: 493: _dl_allocate_tls_init: Assertion `listp->slotinfo[cnt].gen <= GL(dl_tls_generation)' failed!
{code}

I think this problem is not related to Flink. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not plan window join without projections,FLINK-34910,13572824,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,21/Mar/24 12:54,21/Mar/24 16:04,04/Jun/24 20:40,21/Mar/24 16:04,1.19.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"When running:
{code}
  @Test
  def testWindowJoinWithoutProjections(): Unit = {
    val sql =
      """"""
        |SELECT *
        |FROM
        |  TABLE(TUMBLE(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) AS L
        |JOIN
        |  TABLE(TUMBLE(TABLE MyTable2, DESCRIPTOR(rowtime), INTERVAL '15' MINUTE)) AS R
        |ON L.window_start = R.window_start AND L.window_end = R.window_end AND L.a = R.a
      """""".stripMargin
    util.verifyRelPlan(sql)
  }
{code}

It fails with:
{code}
FlinkLogicalCalc(select=[a, b, c, rowtime, PROCTIME_MATERIALIZE(proctime) AS proctime, window_start, window_end, window_time, a0, b0, c0, rowtime0, PROCTIME_MATERIALIZE(proctime0) AS proctime0, window_start0, window_end0, window_time0])
+- FlinkLogicalCorrelate(correlation=[$cor0], joinType=[inner], requiredColumns=[{}])
   :- FlinkLogicalTableFunctionScan(invocation=[TUMBLE(DESCRIPTOR($3), 900000:INTERVAL MINUTE)], rowType=[RecordType(INTEGER a, VARCHAR(2147483647) b, BIGINT c, TIMESTAMP(3) *ROWTIME* rowtime, TIMESTAMP_LTZ(3) *PROCTIME* proctime, TIMESTAMP(3) window_start, TIMESTAMP(3) window_end, TIMESTAMP(3) *ROWTIME* window_time)])
   :  +- FlinkLogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($3, 1000:INTERVAL SECOND)])
   :     +- FlinkLogicalCalc(select=[a, b, c, rowtime, PROCTIME() AS proctime])
   :        +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, rowtime])
   +- FlinkLogicalTableFunctionScan(invocation=[TUMBLE(DESCRIPTOR(CAST($3):TIMESTAMP(3)), 900000:INTERVAL MINUTE)], rowType=[RecordType(INTEGER a, VARCHAR(2147483647) b, BIGINT c, TIMESTAMP(3) *ROWTIME* rowtime, TIMESTAMP_LTZ(3) *PROCTIME* proctime, TIMESTAMP(3) window_start, TIMESTAMP(3) window_end, TIMESTAMP(3) *ROWTIME* window_time)])
      +- FlinkLogicalWatermarkAssigner(rowtime=[rowtime], watermark=[-($3, 1000:INTERVAL SECOND)])
         +- FlinkLogicalCalc(select=[a, b, c, rowtime, PROCTIME() AS proctime])
            +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable2]], fields=[a, b, c, rowtime])

Failed to get time attribute index from DESCRIPTOR(CAST($3):TIMESTAMP(3)). This is a bug, please file a JIRA issue.
Please check the documentation for the set of currently supported SQL features.
{code}

In prior versions this had another problem of ambiguous {{rowtime}} column, but this has been fixed by [FLINK-32648]. In versions < 1.19 WindowTableFunctions were incorrectly scoped, because they were not extending from Calcite's SqlWindowTableFunction and the scoping implemented in SqlValidatorImpl#convertFrom was incorrect. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 16:04:48 UTC 2024,,,,,,,,,,"0|z1o4qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 16:04;dwysakowicz;Fixed in 709bf93534fcdfd2b4452667af450f1748bf1ccc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OceanBase transctionID by Flink CDC,FLINK-34909,13572823,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,xiaotouming,xiaotouming,21/Mar/24 12:37,22/Mar/24 02:32,04/Jun/24 20:40,22/Mar/24 01:54,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,,,,want to get OceanBase transctionID by Flink CDC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 01:54:57 UTC 2024,,,,,,,,,,"0|z1o4q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/24 01:54;leonard;Please change to English to describe your ticket, feel free to reopen once changed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Pipeline] Mysql pipeline to doris and starrocks will lost precision for timestamp,FLINK-34908,13572811,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,pacinogong,pacinogong,pacinogong,21/Mar/24 10:50,21/May/24 06:20,04/Jun/24 20:40,,,,,,,,cdc-3.2.0,,,,Flink CDC,,,,,0,pull-request-available,,,"flink cdc pipeline will decide timestamp zone by config of pipeline. I found mysql2doris and mysql2starracks will specific datetime format

yyyy-MM-dd HH:mm:ss, it will cause lost datatime precision. I think we should't set fixed datetime format, just return LocalDateTime object.
 
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 03:34:14 UTC 2024,,,,,,,,,,"0|z1o4nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 11:05;pacinogong;I will submit a PR. cc [~Leonard] 
 ;;;","21/Mar/24 11:31;leonard;Thanks [~pacinogong], assigned this ticket to you.;;;","07/Apr/24 03:34;pacinogong;Starrocks only support second precision. [https://docs.starrocks.io/zh/docs/sql-reference/data-types/date-types/DATETIME/.|https://docs.starrocks.io/zh/docs/sql-reference/data-types/date-types/DATETIME/] So I will process doris.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jobRunningTs should be the timestamp that all tasks are running,FLINK-34907,13572804,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,21/Mar/24 10:12,17/Apr/24 03:09,04/Jun/24 20:40,,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,,,,,0,,,,"Currently, we consider the timestamp that JobStatus is changed to RUNNING as jobRunningTs. But the JobStatus will be RUNNING once job starts schedule, so it doesn't mean all tasks are running(It doesn't include request TM resources from kubernetes/yarn, deploy tasks and restore states, these steps will take a lot of time).

It will let the isStabilizing or estimating restart time are not accurate.

Solution: jobRunningTs should be the timestamp that all tasks are running.

It can be got from SubtasksTimesHeaders rest api.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 10:27:08 UTC 2024,,,,,,,,,,"0|z1o4m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 10:16;gyfora;similar to the other Jirayou opened this only seems to affect the standalone autoscaler logic;;;","21/Mar/24 10:27;fanrui;I see the jobRunningTs is generated from JobDetailsInfo instead of kubernetes operator. The JobDetailsInfo is got via rest api in autoscaler module, so it may be a common issue.

Feel free to correct me if my understanding is wrong, thanks a lot.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't start autoscaling when some tasks are not running,FLINK-34906,13572801,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,21/Mar/24 09:41,16/Apr/24 07:44,04/Jun/24 20:40,16/Apr/24 07:44,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,,,,,0,pull-request-available,,,"Currently, the autoscaler will scale a job when the JobStatus is RUNNING. But the JobStatus will be RUNNING once job starts schedule, so it doesn't mean all tasks are running. Especially, when the resource isn't enough or job recovers from large state.

The autoscaler will throw exception and generate the AutoscalerError event when tasks are not ready, such as: 

 !image-2024-03-21-17-40-23-523.png! 


Also, we don't need to scale it when some tasks are not ready.

Solution: we only scale job that all tasks are running(some of tasks may be finished). ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/24 09:40;fanrui;image-2024-03-21-17-40-23-523.png;https://issues.apache.org/jira/secure/attachment/13067579/image-2024-03-21-17-40-23-523.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 07:44:02 UTC 2024,,,,,,,,,,"0|z1o4lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 07:44;fanrui;Merged to main(1.9.0) via : 75eb206e863993727b62bd7374adadbc4760e417;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The default length of CHAR/BINARY data type of Add column DDL,FLINK-34905,13572798,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zhongqishang,zhongqishang,zhongqishang,21/Mar/24 08:56,11/Apr/24 05:43,04/Jun/24 20:40,11/Apr/24 05:43,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"I run the DDL in mysql
{code:java}
ALTER TABLE test.products ADD Column1 BINARY NULL;  
ALTER TABLE test.products ADD Column2 CHAR NULL; {code}
Encountered the follow error:
{code:java}

Caused by: java.lang.IllegalArgumentException: Binary string length must be between 1 and 2147483647 (both inclusive).
	at org.apache.flink.cdc.common.types.BinaryType.<init>(BinaryType.java:53)
	at org.apache.flink.cdc.common.types.BinaryType.<init>(BinaryType.java:61)
	at org.apache.flink.cdc.common.types.DataTypes.BINARY(DataTypes.java:42)
	at org.apache.flink.cdc.connectors.mysql.utils.MySqlTypeUtils.convertFromColumn(MySqlTypeUtils.java:221)
	at org.apache.flink.cdc.connectors.mysql.utils.MySqlTypeUtils.fromDbzColumn(MySqlTypeUtils.java:111)
	at org.apache.flink.cdc.connectors.mysql.source.parser.CustomAlterTableParserListener.toCdcColumn(CustomAlterTableParserListener.java:256)
	at org.apache.flink.cdc.connectors.mysql.source.parser.CustomAlterTableParserListener.lambda$exitAlterByAddColumn$0(CustomAlterTableParserListener.java:126)
	at io.debezium.connector.mysql.antlr.MySqlAntlrDdlParser.runIfNotNull(MySqlAntlrDdlParser.java:358)
	at org.apache.flink.cdc.connectors.mysql.source.parser.CustomAlterTableParserListener.exitAlterByAddColumn(CustomAlterTableParserListener.java:98)
	at io.debezium.ddl.parser.mysql.generated.MySqlParser$AlterByAddColumnContext.exitRule(MySqlParser.java:15459)
	at io.debezium.antlr.ProxyParseTreeListenerUtil.delegateExitRule(ProxyParseTreeListenerUtil.java:64)
	at org.apache.flink.cdc.connectors.mysql.source.parser.CustomMySqlAntlrDdlParserListener.exitEveryRule(CustomMySqlAntlrDdlParserListener.java:124)
	at org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:48)
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30)
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
	at io.debezium.antlr.AntlrDdlParser.parse(AntlrDdlParser.java:87)
	at org.apache.flink.cdc.connectors.mysql.source.MySqlEventDeserializer.deserializeSchemaChangeRecord(MySqlEventDeserializer.java:88)
	at org.apache.flink.cdc.debezium.event.SourceRecordEventDeserializer.deserialize(SourceRecordEventDeserializer.java:52)
	at org.apache.flink.cdc.debezium.event.DebeziumEventDeserializationSchema.deserialize(DebeziumEventDeserializationSchema.java:93)
	at org.apache.flink.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitElement(MySqlRecordEmitter.java:119)
	at org.apache.flink.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.processElement(MySqlRecordEmitter.java:96)
	at org.apache.flink.cdc.connectors.mysql.source.reader.MySqlPipelineRecordEmitter.processElement(MySqlPipelineRecordEmitter.java:120)
	at org.apache.flink.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitRecord(MySqlRecordEmitter.java:73)
	at org.apache.flink.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitRecord(MySqlRecordEmitter.java:46)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:160)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:419)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:750) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 05:43:12 UTC 2024,,,,,,,,,,"0|z1o4ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/24 05:43;renqs;flink-cdc master: 18dd51fe22c26347bc9988d2fb8b6a78c2e922b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature] submit Flink CDC pipeline job to yarn application cluster.,FLINK-34904,13572791,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,czy006,czy006,21/Mar/24 08:35,28/May/24 02:37,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,Flink CDC,,,,,0,,,,support flink cdc cli submit pipeline job to yarn application cluster.discuss in FLINK-34853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 02:34:15 UTC 2024,,,,,,,,,,"0|z1o4j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 08:36;czy006;cc [~czy006] ;;;","28/May/24 02:34;kwafor;[~czy006] Looking forward to the realization of this and issue FLINK-35360, if possible I'm willing to investigate yarn appliaction mode after [https://github.com/apache/flink-cdc/pull/3093].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add mysql-pipeline-connector with tables.exclude option to exclude unnecessary tables,FLINK-34903,13572784,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fffy,fffy,fffy,21/Mar/24 07:40,29/Apr/24 08:25,04/Jun/24 20:40,29/Apr/24 08:25,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,cdc,pull-request-available,,"    When using the MySQL Pipeline connector for whole-database synchronization, users currently cannot exclude unnecessary tables. Taking reference from Debezium's parameters, specifically the {*}table.exclude.list{*}, if the *table.include.list* is declared, then the *table.exclude.list* parameter will not take effect. However, the tables specified in the tables parameter of the MySQL Pipeline connector are effectively added to the *table.include.list* in Debezium's context.

!screenshot-1.png!

!screenshot-2.png|width=834,height=86!

debezium opthion  desc

!screenshot-3.png|width=831,height=217!

    In summary, it is necessary to introduce an externally-exposed *table.exclude.list* parameter within the MySQL Pipeline connector to facilitate the exclusion of tables. This is because the current setup does not allow for excluding unnecessary tables when including others through the tables parameter.",,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/24 07:59;fffy;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13067571/screenshot-1.png","21/Mar/24 08:01;fffy;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13067572/screenshot-2.png","21/Mar/24 08:02;fffy;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13067573/screenshot-3.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Java,Mon Apr 29 08:25:08 UTC 2024,,,,,,,,,,"0|z1o4hk:",9223372036854775807,[features] Add mysql-pipeline-connector with  tables.exclude option to exclude unnecessary tables ,,,,,,,,,,,,,,,,,,,"21/Mar/24 07:56;fffy;I will take a pr for this;;;","21/Mar/24 11:48;fffy;i take a pr in https://github.com/apache/flink-cdc/pull/3186;;;","22/Mar/24 01:53;loserwang1024;[~fffy] , Please change PR titile by [FLINK-34903][xxxx]Add mysql-pipeline-connector with table.exclude.list option to exclude unnecessary tables;;;","22/Mar/24 02:03;fffy;ok,tks,i have do it;;;","29/Apr/24 08:25;renqs;flink-cdc master: 0d797a6021ea2f3973f8dc74c0c741b695e85a12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
INSERT INTO column mismatch leads to IndexOutOfBoundsException,FLINK-34902,13572779,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeyhunkarimov,twalthr,twalthr,21/Mar/24 07:16,30/Apr/24 08:52,04/Jun/24 20:40,30/Apr/24 08:52,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"SQL:
{code}
INSERT INTO t (a, b) SELECT 1;
{code}

 

Stack trace:
{code}

org.apache.flink.table.api.ValidationException: SQL validation failed. Index 1 out of bounds for length 1
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:200)
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:117)
    at
Caused by: java.lang.IndexOutOfBoundsException: Index 1 out of bounds for length 1
    at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
    at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
    at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
    at java.base/java.util.Objects.checkIndex(Objects.java:374)
    at java.base/java.util.ArrayList.get(ArrayList.java:459)
    at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$reorder$1(PreValidateReWriter.scala:355)
    at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$reorder$1$adapted(PreValidateReWriter.scala:355)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 08:52:51 UTC 2024,,,,,,,,,,"0|z1o4gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Mar/24 10:13;twalthr;Fixed in master: 3a55a3ff750d67cfa48d15f818c1307b89d84fe9;;;","25/Apr/24 04:57;twalthr;[~jeyhunkarimov] could you take another look at your changes? It seems this broke statements including VALUES.
works:
{code}
INSERT INTO t_3_columns (id, name, num) SELECT 'id_0', 'name_0', 5;
{code}

doesn't work:
{code}
INSERT INTO t_3_columns (id, name, num) VALUES ('id_0', 'name_0', 5);
{code}

I will reopen this issue.;;;","25/Apr/24 10:43;jeyhunkarimov;Hi [~twalthr] sure, I will check the issue;;;","30/Apr/24 08:52;twalthr;Fixed in master: 8a75f8f22b8e08e45f1a6453b87718eab6db115a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update clause must EXCLUDED unique index,FLINK-34901,13572775,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,ouyangwuli,ouyangwuli,21/Mar/24 06:44,27/May/24 08:11,04/Jun/24 20:40,,jdbc-3.1.2,,,,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,"When writing Postgres data using flinke-connector-jdbc, using upsert mode, As docs '[https://www.postgresql.org/docs/current/sql-insert.html]', when we use ON CONFLICT, Update clause must EXCLUDED unique index.
Or it will throw exceptions.like "" modification of distribution columns in OnConflictUpdate is not supported Call getNextException to see other errors in the batch.""


Whether you can modify the code is as follows：
String updateClause =
        
{code:java}
Arrays.stream(fieldNames)
.filter(fieldName -> !Arrays.asList(uniqueKeyFields).contains(fieldName))
.map(f -> quoteIdentifier(f) + ""=EXCLUDED."" + quoteIdentifier(f))
.collect(Collectors.joining("", ""));{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 09:13:08 UTC 2024,,,,,,,,,,"0|z1o4fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 07:10;ouyangwuli;Please assign this ticket for me。[~wanglijie] ;;;","14/May/24 09:13;ouyangwuli;Some versions of pgsql also give this error：
{code:java}
//ERROR: modification of distribution columns in OnConflictUpdate is not supported  Call getNextException to see other errors in the batch.
 at org.postgresql.jdbc.BatchResultHandler.handleCompletion(BatchResultHandler.java:186) ~[postgresql-42.5.4.jar:42.5.4]
 at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:878) ~[postgresql-42.5.4.jar:42.5.4]
 at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916) ~[postgresql-42.5.4.jar:42.5.4]
 at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684) ~[postgresql-42.5.4.jar:42.5.4]
 at org.apache.flink.connector.jdbc.statement.FieldNamedPreparedStatementImpl.executeBatch(FieldNamedPreparedStatementImpl.java:65) ~[flink-connector-jdbc-3.1.2-1.18.jar:3.1.2-1.18]
 at org.apache.flink.connector.jdbc.internal.executor.TableSimpleStatementExecutor.executeBatch(TableSimpleStatementExecutor.java:64) ~[flink-connector-jdbc-3.1.2-1.18.jar:3.1.2-1.18]
 at org.apache.flink.connector.jdbc.internal.executor.TableBufferReducedStatementExecutor.executeBatch(TableBufferReducedStatementExecutor.java:101) ~[flink-connector-jdbc-3.1.2-1.18.jar:3.1.2-1.18]
 at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.attemptFlush(JdbcOutputFormat.java:246) ~[flink-connector-jdbc-3.1.2-1.18.jar:3.1.2-1.18]
 at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.flush(JdbcOutputFormat.java:216) ~[flink-connector-jdbc-3.1.2-1.18.jar:3.1.2-1.18]
 at org.apache.flink.connector.jdbc.internal.JdbcOutputFormat.writeRecord(JdbcOutputFormat.java:195) ~[flink-connector-jdbc-3.1.2-1.18.jar:3.1.2-1.18]
 ... 15 more
Caused by: org.postgresql.util.PSQLException: ERROR: modification of distribution columns in OnConflictUpdate is not supported
 at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676) ~[postgresql-42.5.4.jar:42.5.4]
 at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366) ~[postgresql-42.5.4.jar:42.5.4]
 at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:356) ~[postgresql-42.5.4.jar:42.5.4]
 at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:316) ~[postgresql-42.5.4.jar:42.5.4]
 at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:874) ~[postgresql-42.5.4.jar:42.5.4]
 at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916) ~[postgresql-42.5.4.jar:42.5.4]
 at org. {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check compatibility for classes in flink-core-api that skip japicmp,FLINK-34900,13572763,13570313,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,Weijie Guo,Weijie Guo,21/Mar/24 03:57,21/Mar/24 03:59,04/Jun/24 20:40,,,,,,,,,,,,API / Core,,,,,0,,,,"As we said in FLINK-34899, some classes are temporarily ignored by japicmp. Before the next release, we need to manually check if any of these classes have been changed in an incompatible way then.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-21 03:57:19.0,,,,,,,,,,"0|z1o4cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include all classes in flink-core-api that skip the japicmp check back,FLINK-34899,13572761,13570313,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,Weijie Guo,Weijie Guo,21/Mar/24 03:38,21/Mar/24 03:59,04/Jun/24 20:40,,,,,,,,,,,,API / Core,,,,,0,,,,"We have moved some classes from flink-core to flink-core-api without changing the package name, but the japicmp check will also consider this incompatible changes. As a result, we excluded them for now. But we'll have to include them back in the next release. 

The list of these classes:
org.apache.flink.api.common.functions.Function
org.apache.flink.api.java.functions.KeySelector",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-21 03:38:15.0,,,,,,,,,,"0|z1o4cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot create ARRAY of named STRUCTs,FLINK-34898,13572756,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chloehe,chloehe,21/Mar/24 01:54,07/Apr/24 14:22,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,,0,,,,"I want to construct data that consists of arrays of named STRUCT. For example, one field may look like `[\{""a"": 1}]`. I am able to construct this named STRUCT as
{code:java}
SELECT CAST(ROW(1) as ROW<a INT>) AS row1;  {code}
but when I try to wrap this in an ARRAY, it fails:
{code:java}
SELECT ARRAY[CAST(ROW(1) as ROW<a INT>)] AS row1;  

// error
Caused by: java.lang.UnsupportedOperationException: class org.apache.calcite.sql.SqlBasicCall: ROW(1)
{code}
These are the workarounds that I found:
{code:java}
SELECT ROW(ROW(CAST(ROW(1) as ROW<a INT>))) AS row1; 
// or
SELECT cast(ARRAY[ROW(1)] as ARRAY<ROW<a INT>>); {code}
but I think this is a bug that we need to follow up and fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/24 03:59;liyubin117;image-2024-03-21-12-00-00-183.png;https://issues.apache.org/jira/secure/attachment/13067565/image-2024-03-21-12-00-00-183.png","07/Apr/24 14:13;hackergin;截屏2024-04-07 22.05.40.png;https://issues.apache.org/jira/secure/attachment/13067924/%E6%88%AA%E5%B1%8F2024-04-07+22.05.40.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 14:22:09 UTC 2024,,,,,,,,,,"0|z1o4bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 04:00;liyubin117;I use the latest version to run the syntax and work fine, maybe you could try 1.19.

!image-2024-03-21-12-00-00-183.png|width=688,height=161!;;;","21/Mar/24 06:49;chloehe;[~liyubin117] Yes this does work for me, but this generates a named STRUCT with two fields, `a` and `b`. I want to construct a named STRUCT with a single field, i.e., something like 
{code:java}
SELECT CAST((1) as ROW<a INT>) AS row1; {code}
This is not supported.

I am using 1.19.;;;","21/Mar/24 07:02;hackergin;You can try using   


{code:java}
SELECT ROW(1) 
{code}


https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/#value-construction-functions

>  The implicit row constructor requires at least two fields. The explicit row constructor can deal with an arbitrary number of fields. Both of them support arbitrary expressions as fields. ;;;","21/Mar/24 07:08;chloehe;[~hackergin] Thanks for the pointer. 
{code:java}
SELECT CAST(ROW(1) as ROW<a INT>) AS row1; {code}
works for me.

I want to also wrap this in an ARRAY, so that the data in this cell looks like `[\{""a"": 1}]` (i.e., it's an ARRAY constructed from named STRUCTs). 
{code:java}
SELECT ARRAY[CAST(ROW(1) as ROW<a INT>)] AS row1; {code}
does not work and it seems that the only way that I can get this to work properly is to wrap this in two ROWs, i.e.,
{code:java}
SELECT ROW(ROW(CAST(ROW(1) as ROW<a INT>))) AS row1; {code}
Is this the only way to achieve this?;;;","21/Mar/24 09:27;martijnvisser;It does look like this is a user support ticket, not an actual bug problem. Questions like these should be posted on the User mailing list, Slack or Stackoverflow. ;;;","21/Mar/24 11:50;hackergin;I tried this case and indeed encountered an error, but I managed to pass the test by trying a different approach.

 

 
{code:java}
//代码占位符


-- The query is normal and results can be obtained.
Flink SQL> SELECT cast(ARRAY[ROW(1)] as ARRAY<ROW<a INT>>);
[INFO] Result retrieval cancelled.


-- Got the exception

Flink SQL> SELECT ARRAY[cast(ROW(1) as ROW<a INT>)];
[ERROR] Could not execute SQL statement. Reason:
java.lang.UnsupportedOperationException: class org.apache.calcite.sql.SqlBasicCall: ROW(1)
 {code}
 

 

I think this might indeed be a bug, we need to follow up and fix it.

 

[~chloehe]  Can you help to modify the title and content of Jira?  Please provide the specific query and corresponding error message.

 

 ;;;","21/Mar/24 17:00;chloehe;[~hackergin] I have updated the title and the content of this ticket. Thank you!;;;","07/Apr/24 14:22;hackergin;[~chloehe]   Thank you for the update, sorry for the late reply. Based on my testing, this seems to be an issue with the CAST function implementation. The specific reason may be related to this Jira ticket.     https://issues.apache.org/jira/browse/FLINK-18673 

 

FLINK-18673 added support for ROW() as a parameter for UDFs. During the validation of SqlCall, the Validate function's Operand Node will be skipped.

+org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator#validateColumnListParams+

 
{code:java}
//代码占位符

@Override
public void validateColumnListParams(
        SqlFunction function, List<RelDataType> argTypes, List<SqlNode> operands) {
    // we don't support column lists and translate them into the unknown type in the type
    // factory,
    // this makes it possible to ignore them in the validator and fall back to regular row types
    // see also SqlFunction#deriveType
}
{code}
 

But SqlFunction::deriveType will indirectly call org.apache.calcite.sql.validate.SqlValidator#deriveType to infer the type of Row().

 

 

!截屏2024-04-07 22.05.40.png!

 

In the SqlCastFunction, it will directly obtaining ValidatedNode type. 

 
{code:java}
//代码占位符

    @Override
    public boolean checkOperandTypes(SqlCallBinding callBinding, boolean throwOnFailure) {
        final SqlNode left = callBinding.operand(0);
        final SqlNode right = callBinding.operand(1);
        if (SqlUtil.isNullLiteral(left, false) || left instanceof SqlDynamicParam) {
            return true;
        }
        RelDataType validatedNodeType = callBinding.getValidator().getValidatedNodeType(left);
//        RelDataType validatedNodeType = SqlTypeUtil.deriveType(callBinding, left);
        RelDataType returnType = SqlTypeUtil.deriveType(callBinding, right);
        if (!canCastFrom(returnType, validatedNodeType)) {
            if (throwOnFailure) {
                throw callBinding.newError(
                        RESOURCE.cannotCastValue(
                                validatedNodeType.toString(), returnType.toString()));
            }
            return false;
        } {code}
 

 

And the corresponding error message is as follows:

 
{code:java}
//代码占位符

.SqlBasicCall: ROW(1, 2)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:200)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:117)
	at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:259)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)
	at org.apache.flink.table.examples.java.basics.WordCountSQLExample.main(WordCountSQLExample.java:49)
Caused by: java.lang.UnsupportedOperationException: class org.apache.calcite.sql.SqlBasicCall: ROW(1, 2)
	at org.apache.calcite.util.Util.needToImplement(Util.java:1101)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.getValidatedNodeType(SqlValidatorImpl.java:1777)
	at org.apache.calcite.sql.fun.SqlCastFunction.checkOperandTypes(SqlCastFunction.java:138) {code}
 

 

If we use SqlTypeUtil.deriveType, we can get the correct type. Therefore, I think we can modify this section to fix the issue.

 
{code:java}
//代码占位符

RelDataType validatedNodeType = SqlTypeUtil.deriveType(callBinding, left); {code}
 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterServiceLeadershipRunnerTest#testJobMasterServiceLeadershipRunnerCloseWhenElectionServiceGrantLeaderShip needs to be enabled again,FLINK-34897,13572708,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Mar/24 14:51,27/Mar/24 14:37,04/Jun/24 20:40,27/Mar/24 14:37,1.18.1,1.19.0,1.20.0,,,,1.18.2,1.19.1,1.20.0,,Runtime / Coordination,,,,,0,pull-request-available,,,"While working on FLINK-34672 I noticed that {{JobMasterServiceLeadershipRunnerTest#testJobMasterServiceLeadershipRunnerCloseWhenElectionServiceGrantLeaderShip}} is disabled without a reason.

It looks like I disabled it accidentally as part of FLINK-31783.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31783,,,,,,,FLINK-34672,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 14:37:49 UTC 2024,,,,,,,,,,"0|z1o40o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 14:37;mapohl;master: [0e70d89ad9f807a5816290e9808720e71bdad655|https://github.com/apache/flink/commit/0e70d89ad9f807a5816290e9808720e71bdad655]
1.19: [6b5c48ff53ddc6e75056a9050afded2ac44a413a|https://github.com/apache/flink/commit/6b5c48ff53ddc6e75056a9050afded2ac44a413a]
1.18: [a6aa569f5005041934a2e6398b6749584beeaabd|https://github.com/apache/flink/commit/a6aa569f5005041934a2e6398b6749584beeaabd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate CorrelateSortToRankRule,FLINK-34896,13572691,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,20/Mar/24 13:12,14/May/24 21:29,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 13:12:36.0,,,,,,,,,,"0|z1o3ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate FlinkRewriteSubQueryRule,FLINK-34895,13572690,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,20/Mar/24 12:36,20/Mar/24 12:46,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 12:36:21.0,,,,,,,,,,"0|z1o3wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate JoinDependentConditionDerivationRule,FLINK-34894,13572687,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,20/Mar/24 12:16,20/Mar/24 12:29,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 12:16:07.0,,,,,,,,,,"0|z1o3w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Checkstyle to 9+,FLINK-34893,13572681,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,20/Mar/24 11:16,21/Mar/24 13:48,04/Jun/24 20:40,21/Mar/24 13:48,,,,,,,1.20.0,,,,Build System,,,,,0,pull-request-available,,,"The issue with current checkstyle is that there is checkstyle IntellijIdea plugin

And recently it dropped checkstyle 8 support [1]

At the same time we can not move to Checkstyle 10 since 10.x requires java 11+

[1] https://github.com/jshiell/checkstyle-idea/blob/main/CHANGELOG.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 13:48:08 UTC 2024,,,,,,,,,,"0|z1o3uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 13:48;Sergey Nuyanzin;Merged as [https://github.com/apache/flink/commit/23c2fd0a32de93c31f3afd1422575d1d459eb90d|23c2fd0a32de93c31f3afd1422575d1d459eb90d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nightly AWS connectors build fails on running python tests,FLINK-34892,13572673,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,20/Mar/24 10:04,21/Mar/24 08:55,04/Jun/24 20:40,21/Mar/24 08:55,aws-connector-4.2.0,,,,,,,,,,Connectors / AWS,,,,,0,pull-request-available,,,"Build for externalized python connector code fails: [https://github.com/apache/flink-connector-aws/actions/runs/8351768294/job/22860710449]
{code:java}
2024-03-20T00:14:35.5215863Z __________________ FlinkKinesisTest.test_kinesis_streams_sink __________________
2024-03-20T00:14:35.5216781Z .tox/py310-cython/lib/python3.10/site-packages/pyflink/testing/test_case_utils.py:149: in setUp
2024-03-20T00:14:35.5217584Z     self.env = StreamExecutionEnvironment.get_execution_environment()
2024-03-20T00:14:35.5218901Z .tox/py310-cython/lib/python3.10/site-packages/pyflink/datastream/stream_execution_environment.py:876: in get_execution_environment
2024-03-20T00:14:35.5219751Z     gateway = get_gateway()
2024-03-20T00:14:35.5220635Z .tox/py310-cython/lib/python3.10/site-packages/pyflink/java_gateway.py:64: in get_gateway
2024-03-20T00:14:35.5221378Z     _gateway = launch_gateway()
2024-03-20T00:14:35.5222111Z .tox/py310-cython/lib/python3.10/site-packages/pyflink/java_gateway.py:110: in launch_gateway
2024-03-20T00:14:35.5222956Z     p = launch_gateway_server_process(env, args)
2024-03-20T00:14:35.5223854Z .tox/py310-cython/lib/python3.10/site-packages/pyflink/pyflink_gateway_server.py:262: in launch_gateway_server_process
2024-03-20T00:14:35.5224649Z     java_executable = find_java_executable()
2024-03-20T00:14:35.5225583Z .tox/py310-cython/lib/python3.10/site-packages/pyflink/pyflink_gateway_server.py:75: in find_java_executable
2024-03-20T00:14:35.5226449Z     java_home = read_from_config(KEY_ENV_JAVA_HOME, None, flink_conf_file)
2024-03-20T00:14:35.5227099Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2024-03-20T00:14:35.5227450Z 
2024-03-20T00:14:35.5227774Z key = 'env.java.home', default_value = None
2024-03-20T00:14:35.5228925Z flink_conf_file = '/home/runner/work/flink-connector-aws/flink-connector-aws/flink-python/.tox/py310-cython/lib/python3.10/site-packages/pyflink/conf/flink-conf.yaml'
2024-03-20T00:14:35.5229778Z 
2024-03-20T00:14:35.5230010Z     def read_from_config(key, default_value, flink_conf_file):
2024-03-20T00:14:35.5230581Z         value = default_value
2024-03-20T00:14:35.5231236Z         # get the realpath of tainted path value to avoid CWE22 problem that constructs a path or URI
2024-03-20T00:14:35.5232195Z         # using the tainted value and might allow an attacker to access, modify, or test the existence
2024-03-20T00:14:35.5232940Z         # of critical or sensitive files.
2024-03-20T00:14:35.5233417Z >       with open(os.path.realpath(flink_conf_file), ""r"") as f:
2024-03-20T00:14:35.5234874Z E       FileNotFoundError: [Errno 2] No such file or directory: '/home/runner/work/flink-connector-aws/flink-connector-aws/flink-python/.tox/py310-cython/lib/python3.10/site-packages/pyflink/conf/flink-conf.yaml'
2024-03-20T00:14:35.5235954Z 
2024-03-20T00:14:35.5236484Z .tox/py310-cython/lib/python3.10/site-packages/pyflink/pyflink_gateway_server.py:58: FileNotFoundError {code}
Failure started after the release of apache-flink python package for 1.19.0 due to change of default config file provided within artifact.

 

 

Issue comes from outdated copy of pyflink_gateway_server.py created as part of [https://github.com/apache/flink-connector-kafka/pull/69] (same change is duplicated in AWS connectors repository).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 08:55:28 UTC 2024,,,,,,,,,,"0|z1o3sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 11:23;a.pilipenko;This issue should also be present for Kafka connector. However, currently there is no python tests running as part of the weekly build there.;;;","21/Mar/24 08:55;dannycranmer;Merged commit [{{e8ba71e}}|https://github.com/apache/flink-connector-aws/commit/e8ba71ec3c27903c838701d536a8ae05bc5bb523] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoteStorageScannerTest causes exit 239,FLINK-34891,13572672,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rskraba,rskraba,20/Mar/24 09:53,21/Mar/24 10:24,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Runtime / Network,,,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58432&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9121]
{code}
Mar 20 01:22:54 01:22:54.671 [ERROR] Error occurred in starting fork, check output in log
Mar 20 01:22:54 01:22:54.671 [ERROR] Process Exit Code: 239
Mar 20 01:22:54 01:22:54.671 [ERROR] Crashed tests:
Mar 20 01:22:54 01:22:54.671 [ERROR] org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScannerTest
Mar 20 01:22:54 01:22:54.671 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:643)
Mar 20 01:22:54 01:22:54.671 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.lambda$runSuitesForkOnceMultiple$3(ForkStarter.java:350)
Mar 20 01:22:54 01:22:54.671 [ERROR] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
Mar 20 01:22:54 01:22:54.671 [ERROR] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
Mar 20 01:22:54 01:22:54.671 [ERROR] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
Mar 20 01:22:54 01:22:54.671 [ERROR] 	at java.base/java.lang.Thread.run(Thread.java:829)
Mar 20 01:22:54 01:22:54.671 [ERROR] -> [Help 1]{code}
 

The build artifact {{mvn-1.log}} the following {{FATAL}} error is found.  It appears that the {{LocalFileSystem}} being used by the test can return null for listStatus, causing a NullPointerException in {{RemoteStorageScanner#[scanMaxSegmentId|https://github.com/apache/flink/blob/6e138f1a4832bd4379fa6289f0445f1ede9b5a54/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/hybrid/tiered/tier/remote/RemoteStorageScanner.java#L238]}}
{code:java}
================================================================================
01:16:09,428 [ForkJoinPool-39-worker-51] INFO  org.apache.flink.util.TestLoggerExtension                    [] -
================================================================================
Test org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScannerTest.testWatchSegmentIgnored[testWatchSegmentIgnored()] is running.
--------------------------------------------------------------------------------
01:16:10,142 [ForkJoinPool-39-worker-51] INFO  org.apache.flink.util.TestLoggerExtension                    [] -
--------------------------------------------------------------------------------
Test org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScannerTest.testWatchSegmentIgnored[testWatchSegmentIgnored()] successfully run.
================================================================================
01:16:10,142 [remote storage scanner-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'remote storage scanner-thread-1' produced an uncaught exception. Stopping the process...
java.lang.NullPointerException: null
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScanner.scanMaxSegmentId(RemoteStorageScanner.java:247) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScanner.run(RemoteStorageScanner.java:200) [classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:829) [?:?]
01:16:10,144 [ForkJoinPool-39-worker-51] INFO  org.apache.flink.util.TestLoggerExtension                    [] -
================================================================================
Test org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScannerTest.testScanStrategy[testScanStrategy()] is running.
--------------------------------------------------------------------------------
01:16:10,145 [ForkJoinPool-39-worker-51] INFO  org.apache.flink.util.TestLoggerExtension                    [] -
--------------------------------------------------------------------------------
Test org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.remote.RemoteStorageScannerTest.testScanStrategy[testScanStrategy()] successfully run.
================================================================================
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:53:12.0,,,,,,,,,,"0|z1o3so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource :.java.sql.SQLException: ORA-01291: missing log file,FLINK-34890,13572671,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:42,20/Mar/24 09:44,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues](https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.14.2

### Flink CDC version

<flink-connector-oracle-cdc.version>2.4.1</flink-connector-oracle-cdc.version>

### Database and its version

oracle19c

### Minimal reproduce step

2023-11-03 13:37:44.675 WARN [debezium-oracleconnector-oracle_logminer-change-event-source-coordinator] io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource : Failed to start Oracle LogMiner session, retrying...
2023-11-03 13:37:45.286 ERROR[debezium-oracleconnector-oracle_logminer-change-event-source-coordinator] io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource : Failed to start Oracle LogMiner after '5' attempts.

java.sql.SQLException: ORA-01291: missing log file
ORA-06512: at ""SYS.DBMS_LOGMNR"", line 72
ORA-06512: at line 1

at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:509) ~[com.oracle.database.jdbc-ojdbc8-19.3.0.0.jar!/:19.3.0.0.0]
at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:461) ~[com.oracle.database.jdbc-ojdbc8-19.3.0.0.jar!/:19.3.0.0.0]  

 

In addition, I clean the archive logs by executing commands from the rman tool command:
------------------------------------------------------------
Crosscheck archivelog all;
Delete noprompt archivelog until time 'sysdate -1';
------------------------------------------------------------

Through the above command, the archived logs were retained for one day. During the insertion, deletion, and modification of the table within one day, I also encountered the issue of losing the logs mentioned above.

May I ask how to solve this problem。

### What did you expect to see?

flink cdc normal run

### What did you see instead?


java.sql.SQLException: ORA-01291: missing log file
ORA-06512: at ""SYS.DBMS_LOGMNR"", line 72
ORA-06512: at line 1

 

 io.debezium.connector.common.BaseSourceTask : Going to restart connector after 10 sec. after a retriable exception
2023-11-03 16:09:11.458 INFO [pool-18-thread-1] io.debezium.jdbc.JdbcConnection : Connection gracefully closed
2023-11-03 16:09:11.459 INFO [debezium-engine] io.debezium.embedded.EmbeddedEngine : Retrieable exception thrown, connector will be restarted
org.apache.kafka.connect.errors.RetriableException: An exception occurred in the change event producer. This connector will be restarted.
        at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:46) ~[debezium-core-1.9.7.Final.jar!/:1.9.7.Final]


### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2615
Created by: https://github.com/jasonhewg
Labels: bug,
Created at: Mon Dec 18 19:07:44 CST 2023
State: open",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:44:56 UTC 2024,,,,,,,,,,"0|z1o3sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:43;flink-cdc-import;Date: Nov 16, 2023, Author: https://github.com/Naros

Hi @jasonhewg, there have been several changes to the Debezium Oracle connector since 1.9.7.Final, which includes several critical bugs around log management that could cause this error.

I would suggest (if its possible) to move to Debezium 2.4.1.Final or 2.5.0.Alpha2 as a baseline for Oracle to get all the fixes that should address common issues around ORA-01291.;;;","20/Mar/24 09:44;flink-cdc-import;Date: Nov 20, 2023, Author: https://github.com/jasonhewg

so:
Do you want to upgrade Flink cdc connectors. com. verica. 2.4.1 debezium. version 1.9.7. Final to 2.4.1. Final? Which version is the corresponding flink-cdc-connectors.com.ververica;;;","20/Mar/24 09:44;flink-cdc-import;Date: Nov 29, 2023, Author: https://github.com/Naros

We are planning to cut a 2.4.2.Final next week as the last build in the Debezium 2.4 release stream -- once we do that I'll look at upgrading them here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC may occur binlog can't be find when dynamic added table repeatedly,FLINK-34889,13572668,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:36,20/Mar/24 09:36,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"We met a strange problem of the Flink CDC: When repeatedly adding table to a Flink CDC link, it may fails and report a very old gtid can't be find, we digging the source code and found the reason is bellow:

1. When CDC full phase change to incremental phase, binlog need pull ending offset of all chunk, and it will take the minimum of these offset as the stating offset of the incremental phase.Ending offset of each chunk are store in the JM.

2. If we added table repeatedly, and each time we need to suspend the job, alter the config, and then resume form latest checkpoint.

3. Normally, when finished adding table, we pull the ending offset of each chunk. The pull process will transfer a size between the jm and tm, which means when there is 100 tables in jm, and we have processed 80, we need process 81 to pull the next offset.

4. There has one problem because the order of the split in jm and tm is not the same.The jm will order by table name (such as a:0, a:1, b:0, b:1), when added table, we need pull the ending offset of the newly added table, while jm order the split by the table name, and the newly added table may occurs in middle, so we may get a ending offset of a very old split.
<img width=""1386"" alt=""1"" src=""https://github.com/apache/flink-cdc/assets/5321584/f8383f59-82d9-4d97-bad7-1aea54c6ac81"">


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3141
Created by: [zlzhang0122|https://github.com/zlzhang0122]
Labels: 
Created at: Wed Mar 13 21:17:17 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:36:12 UTC 2024,,,,,,,,,,"0|z1o3rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:36;flink-cdc-import;Date: Fri Mar 15 11:32:57 CST 2024, Author: [loserwang1024|https://github.com/loserwang1024]

org.apache.flink.cdc.connectors.mysql.source.split.MySqlBinlogSplit#forwardHighWatermarkToStartingOffset will forwards highwatermark.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to do MySQL to MySQL pipeline ,FLINK-34888,13572667,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:36,20/Mar/24 09:36,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"Hi, I just found out demo of MySQL to Doris, But I want to do a whole sync database of MySQL to another MySQL. flink-cdc-pipeline-connector-mysql is just for source. How about the sink still is MySQL?

Cloud anyone tell me some information to implement.

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3140
Created by: [ysmintor|https://github.com/ysmintor]
Labels: 
Created at: Wed Mar 13 19:04:58 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:36:07 UTC 2024,,,,,,,,,,"0|z1o3rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:36;flink-cdc-import;Date: Thu Mar 14 17:43:09 CST 2024, Author: [ihadoop|https://github.com/ihadoop]

you can create sink mysql table

```sql
CREATE TABLE mysql_sink (
  `id`  bigint ,
  `s_name` varchar(100)  ,
  `s_int` bigint,
 PRIMARY KEY(id) NOT ENFORCED 
) WITH (
 'connector' = 'jdbc',
'driver' = 'com.mysql.cj.jdbc.Driver',
'url' = 'jdbc:mysql://192.168.1.15:3306/cdc?serverTimezone=UTC&useSSL=false',
 'username' = 'root',
 'password' = '111111',
 'table-name' = 't_t'
);


insert into mysql_sink select * from cdc_table
```
;;;","20/Mar/24 09:36;flink-cdc-import;Date: Thu Mar 14 21:51:49 CST 2024, Author: [ysmintor|https://github.com/ysmintor]

@ihadoop Thanks for you reply. 
But your method means that I have to create every table which is to not a suit case like pipeline way to sync MySQL database into Doris with each table has the corresponding tables in Doris database. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data lost after rename table ,FLINK-34887,13572666,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:36,09/Apr/24 22:43,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"""Rename table"" is a 'QUERY EVENT' that will not be added to the Map 'tableMapEventByTableId'.

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3137
Created by: [dua01|https://github.com/dua01]
Labels: 
Created at: Wed Mar 13 16:21:39 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 21:53:55 UTC 2024,,,,,,,,,,"0|z1o3rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 21:53;walls.flink.m;I am looking into this. Can [~renqs] assign this to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Sybase database,FLINK-34886,13572665,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,20/Mar/24 09:35,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"Support Sybase database

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3133
Created by: [melin|https://github.com/melin]
Labels: 
Created at: Wed Mar 13 15:19:25 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:35:57.0,,,,,,,,,,"0|z1o3r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mysql to  sr   pipeline， float  value  bug,FLINK-34885,13572664,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,20/Mar/24 09:35,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"env:   flink 1.18.1   flinkcdc 3.0.1   starrocks: 3.2.3    mysql:  5.6.* 

xxx.yaml :
`pipeline: {name: 5411_total_info_to_sr, parallelism: 1}
route: null
sink: {jdbc-url: 'jdbc:mysql://172.16.100.40:9030', load-url: '172.16.100.40:8030',
  name: StarRocks Sink, password:*******, properties.properties.max_filter_ratio: 0.9,
  properties.properties.strict_mode: false, table.create.properties.replication_num: 1,
  type: starrocks, username: ******}
source: {hostname: mysql-cluster.internal.******.com, password:******, port: 3306,
  scan.startup.mode: timestamp, scan.startup.timestamp-millis: 1710233721754, schema-change.enabled: false,
  server-id: 5411, server-time-zone: Asia/Shanghai, tables: 'enlightent_daily.\.*_total_info,enlightent_daily.\.video_basic_info',
  type: mysql, username:******}`

comand: bin/flink-cdc.sh   xxx.yaml

bug:   the fileds of float type  are   error，the fileds of double type are right.

then  I  try  myql-cdc alone, the values  print  right on the  flink sql client screen.

then I try  create flink table  for source table(mysql) and  sink table(starrocks)  on flink sql client.

`insert into master_cost_float_sr select * from master_cost_float/*+ OPTIONS('server-id'='5801-5804') */;

[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 14620b59ae166ea75a6ab3eda071872f
`
by this way, the  float  fields   are  right for  cdc. 




---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3132
Created by: [dickson-bit|https://github.com/dickson-bit]
Labels: 
Created at: Wed Mar 13 11:20:48 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:35:53 UTC 2024,,,,,,,,,,"0|z1o3qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:35;flink-cdc-import;Date: Wed Mar 13 11:54:22 CST 2024, Author: [dickson-bit|https://github.com/dickson-bit]

![image|https://github.com/apache/flink-cdc/assets/2010095/c26b0cea-56db-47ba-bf76-d226f96eacbe]
double  type is ok, float type isn't;;;","20/Mar/24 09:35;flink-cdc-import;Date: Wed Mar 13 11:55:59 CST 2024, Author: [dickson-bit|https://github.com/dickson-bit]

![image|https://github.com/apache/flink-cdc/assets/2010095/f5b71d97-d0a6-4919-a3e2-67ba68aa699d]
this is  msyql  data ;;;","20/Mar/24 09:35;flink-cdc-import;Date: Wed Mar 13 20:48:59 CST 2024, Author: [lvyanquan|https://github.com/lvyanquan]

Did you mean the wrong field is `click_rate`?
I have tested again using pipeline job and got the correct result. But my StarRocks version is 2.5.
;;;","20/Mar/24 09:35;flink-cdc-import;Date: Fri Mar 15 17:33:31 CST 2024, Author: [dickson-bit|https://github.com/dickson-bit]

I did a  simle test  for float type with   yaml file,    It's  a   primary key table , just have 5 fields.  two of them are  float type. 

run the script  flinkcdc.sh,  the float type is right.;;;","20/Mar/24 09:35;flink-cdc-import;Date: Mon Mar 18 11:10:32 CST 2024, Author: [dickson-bit|https://github.com/dickson-bit]

2024-03-18 10:45:21,172 DEBUG io.debezium.connector.base.ChangeEventQueue                  [] - Enqueuing source record 'DataChangeEvent [record=SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={transaction_id=null, ts_sec=1710628041, file=mysql-bin.019859, pos=429652238, row=1, server_id=2, event=2}} ConnectRecord{topic='mysql_binlog_source.opt.iqiyi_tuiguang_ad_data_report_daily', kafkaPartition=null, key=Struct{dt=19798,advertiser_id=51200211539}, keySchema=Schema{mysql_binlog_source.opt.iqiyi_tuiguang_ad_data_report_daily.Key:STRUCT}, value=Struct{after=Struct{dt=19798,advertiser_id=51200211539,advertiser_name=霍尔果斯两比特娱乐有限公司-僵尸参将,impressions=339994,clicks=9925,trueviews=0,downloading=0,downloaded=0,installed=0,**click_rate=0.029999999329447746**,cost=2250.43,avg_cpm=6.619999885559082,avg_cpc=0.23000000417232513,avg_cpv=0.0,conversion_count=3282,conversion_cost=0.69,deep_conversion_count=0,deep_conversion_cost=0.00,updatetime=2024-03-16T22:27:21Z,channel=iqiyi,checked=0,create_time=1710656841000},source=Struct{version=1.9.7.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1710628041000,db=opt,table=iqiyi_tuiguang_ad_data_report_daily,server_id=2,file=mysql-bin.019859,pos=429652441,row=0,thread=15436579},op=c,ts_ms=1710729921172}, valueSchema=Schema{mysql_binlog_source.opt.iqiyi_tuiguang_ad_data_report_daily.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}]'
2024-03-18 10:45:21,172 DEBUG io.debezium.connector.mysql.MySqlStreamingChangeEventSource  [] - Emitted 1 CREATE record(s) for event: Event{header=EventHeaderV4{timestamp=1710628041000, eventType=EXT_WRITE_ROWS, serverId=2, headerLength=19, dataLength=194, nextPosition=429652654, flags=0}, data=WriteRowsEventData{tableId=60915522, includedColumns={0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, rows=[
    [2024-03-16, 51200211539, [B@3a0143b0, 339994, 9925, 0, 0, 0, 0, **0.03**, 2250.43, 6.62, 0.23, 0.0, 3282, 0.69, 0, 0.00, 2024-03-16T22:27:21Z, [B@4d4c8f4a, null, null, null, null, 0, 2024-03-17T06:27:21]
]}}
从 mysql cdc 出来的 click_rate 还是正确的，

但是在入 stream load 之前的值，就已经是错误的了
`2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.impl.execchain.MainClientExec [] - Opening connection {}->http://172.16.100.42:8040
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator [] - Connecting to /172.16.100.42:8040
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator [] - Connection established 172.16.100.40:31797<->172.16.100.42:8040
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.impl.execchain.MainClientExec [] - Executing request PUT /api/transaction/load HTTP/1.1
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.impl.execchain.MainClientExec [] - Proxy auth state: UNCHALLENGED
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> PUT /api/transaction/load HTTP/1.1
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> Authorization: Basic cm9vdDpDRFBWQkFlOEY5RXJHZ2w1
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> Expect: 100-continue
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> strict_mode: false
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> format: json
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> where: dt is not NULL
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> max_filter_ratio: 0.9
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> strip_outer_array: true
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> ignore_json_size: true
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> timeout: 600
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> db: opt
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> table: iqiyi_tuiguang_ad_data_report_daily
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> label: flink-cea3d107-9d12-4ee1-a51d-c40bcf2057be
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> Content-Length: 2610390
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> Content-Type: application/octet-stream
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> Host: 172.16.100.42:8040
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> Connection: Keep-Alive
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> User-Agent: Apache-HttpClient/4.5.13 (Java/1.8.0_372)
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 >> Accept-Encoding: gzip,deflate
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""PUT /api/transaction/load HTTP/1.1[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""Authorization: Basic cm9vdDpDRFBWQkFlOEY5RXJHZ2w1[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""Expect: 100-continue[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""strict_mode: false[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""format: json[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""where: dt is not NULL[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""max_filter_ratio: 0.9[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""strip_outer_array: true[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""ignore_json_size: true[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""timeout: 600[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""db: opt[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""table: iqiyi_tuiguang_ad_data_report_daily[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""label: flink-cea3d107-9d12-4ee1-a51d-c40bcf2057be[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""Content-Length: 2610390[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""Content-Type: application/octet-stream[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""Host: 172.16.100.42:8040[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""Connection: Keep-Alive[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""User-Agent: Apache-HttpClient/4.5.13 (Java/1.8.0_372)[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""Accept-Encoding: gzip,deflate[\r][\n]""
2024-03-18 10:45:31,997 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""[\r][\n]""
2024-03-18 10:45:31,999 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 << ""HTTP/1.1 100 Continue[\r][\n]""
2024-03-18 10:45:31,999 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 << ""[\r][\n]""
2024-03-18 10:45:31,999 DEBUG com.starrocks.streamload.shade.org.apache.http.headers       [] - http-outgoing-227 << HTTP/1.1 100 Continue
2024-03-18 10:45:32,000 DEBUG com.starrocks.streamload.shade.org.apache.http.wire          [] - http-outgoing-227 >> ""[{""installed"":0,""avg_cpm"":-2.0,""click_rate"":2.0,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200225465,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":-1.0842022E-19,""checked"":0,""deep_conversion_cost"":0.00,""cost"":4957.33,""create_time"":""2024-03-16 06:27:19"",""deep_conversion_count"":0,""conversion_count"":6816,""trueviews"":0,""impressions"":787680,""downloaded"":0,""__op"":0,""clicks"":12430,""advertiser_name"":""[0xe4][0xb8][0x8a][0xe6][0xb5][0xb7][0xe5][0x87][0xa1][0xe9][0x85][0xb7][0xe6][0x96][0x87][0xe5][0x8c][0x96][0xe4][0xbc][0xa0][0xe5][0xaa][0x92][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe6][0x80][0xaa][0xe7][0x89][0xa9][0xe6][0x9d][0xa5][0xe8][0xa2][0xad]"",""conversion_cost"":0.73,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":0.0,""**click_rate"":2.0**,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200224995,""dt"":""2024-03-15"",""downloading"":1,""avg_cpc"":2.0,""checked"":0,""deep_conversion_cost"":0.00,""cost"":23241.00,""create_time"":""2024-03-16 06:27:19"",""deep_conversion_count"":0,""conversion_count"":18748,""trueviews"":0,""impressions"":3276347,""downloaded"":1,""__op"":0,""clicks"":74007,""advertiser_name"":""[0xe4][0xb8][0xa4][0xe6][0xaf][0x94][0xe7][0x89][0xb9][0xef][0xbc][0x88][0xe5][0x8c][0x97][0xe4][0xba][0xac][0xef][0xbc][0x89][0xe7][0xa7][0x91][0xe6][0x8a][0x80][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe5][0x91][0xa8][0xe5][0x85][0xac][0xe4][0xbc][0x8f][0xe5][0xa6][0x96]"",""conversion_cost"":1.24,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":0.0,""click_rate"":0.0,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200224947,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":0.0,""checked"":0,""deep_conversion_cost"":0.00,""cost"":0.00,""create_time"":""2024-03-16 06:27:19"",""deep_conversion_count"":0,""conversion_count"":0,""trueviews"":0,""impressions"":2,""downloaded"":0,""__op"":0,""clicks"":0,""advertiser_name"":""[0xe4][0xba][0x91][0xe5][0x8d][0x97][0xe9][0x87][0x91][0xe5][0xbd][0xa9][0xe8][0xa7][0x86][0xe7][0x95][0x8c][0xe5][0xbd][0xb1][0xe4][0xb8][0x9a][0xe8][0x82][0xa1][0xe4][0xbb][0xbd][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe6][0x9e][0x81][0xe4][0xb9][0x90][0xe6][0xb1][0x82][0xe7][0x94][0x9f][0xe5][0x8f][0xb7]"",""conversion_cost"":0.00,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":0.0,""click_rate"":2.0,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200224681,""dt"":""2024-03-15"",""downloading"":0,**""avg_cpc"":1.0842022E-19,**""checked"":0,""deep_conversion_cost"":0.00,""cost"":7838.65,""create_time"":""2024-03-16 06:27:19"",""deep_conversion_count"":0,""conversion_count"":6566,""trueviews"":0,""impressions"":1176855,""downloaded"":0,""__op"":0,""clicks"":27834,""advertiser_name"":""[0xe4][0xb8][0x8a][0xe6][0xb5][0xb7][0xe8][0x89][0xba][0xe6][0xac][0xa3][0xe7][0x92][0x80][0xe7][0x92][0xa8][0xe5][0xbd][0xb1][0xe4][0xb8][0x9a][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe9][0x95][0xbf][0xe7][0x9b][0xb8][0xe5][0x8b][0xbf]"",""conversion_cost"":1.19,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":-2.0,**""click_rate"":-3.689349E19**,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200224275,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":0.0,""checked"":0,""deep_conversion_cost"":0.00,""cost"":175.83,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":251,""trueviews"":0,""impressions"":22123,""downloaded"":0,""__op"":0,""clicks"":680,""advertiser_name"":""[0xe6][0x80][0x80][0xe5][0xae][0x81][0xe9][0x95][0xbf][0xe4][0xbc][0xa0][0xe6][0x96][0x87][0xe5][0x8c][0x96][0xe4][0xbc][0xa0][0xe6][0x92][0xad][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe5][0xa4][0x9c][0xe9][0x9b][0xa8][0xe7][0xa7][0x8b][0xe7][0x81][0xaf][0xe9][0x97][0xbb][0xe8][0xaf][0xa1][0xe4][0xba][0x8b]"",""conversion_cost"":0.70,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":2.0,""click_rate"":-3.689349E19,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200224185,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":-3.689349E19,""checked"":0,""deep_conversion_cost"":0.00,""cost"":28672.01,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":38333,""trueviews"":0,""impressions"":3926913,""downloaded"":0,""__op"":0,""clicks"":118534,""advertiser_name"":""[0xe4][0xb8][0x8a][0xe6][0xb5][0xb7][0xe9][0xb8][0xa3][0xe6][0xb6][0xa7][0xe5][0xbd][0xb1][0xe4][0xb8][0x9a][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe9][0xa3][0x9e][0xe6][0x9d][0xa5][0xe6][0xa8][0xaa][0xe8][0xb4][0xa2][0xef][0xbc][0x88]2024[0xef][0xbc][0x89]-3"",""conversion_cost"":0.75,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":-1.0842022E-19,""click_rate"":-3.689349E19,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200224007,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":3.689349E19,""checked"":0,""deep_conversion_cost"":0.00,""cost"":1874.00,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":1598,""trueviews"":0,""impressions"":165777,""downloaded"":0,""__op"":0,""clicks"":5620,""advertiser_name"":""[0xe4][0xb8][0xa4][0xe6][0xaf][0x94][0xe7][0x89][0xb9][0xef][0xbc][0x88][0xe5][0x8c][0x97][0xe4][0xba][0xac][0xef][0xbc][0x89][0xe7][0xa7][0x91][0xe6][0x8a][0x80][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe4][0xba][0xb2][0xe7][0x88][0xb1][0xe7][0x9a][0x84][0xe5][0x8f][0xb8][0xe4][0xb8][0x9e][0xe5][0xa4][0xa7][0xe4][0xba][0xba]-3"",""conversion_cost"":1.17,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":-0.0,""click_rate"":-1.0842022E-19,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200223903,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":0.0,""checked"":0,""deep_conversion_cost"":0.00,""cost"":7175.64,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":9489,""trueviews"":0,""impressions"":584902,""downloaded"":0,""__op"":0,""clicks"":28826,""advertiser_name"":""[0xe9][0x9c][0x8d][0xe5][0xb0][0x94][0xe6][0x9e][0x9c][0xe6][0x96][0xaf][0xe4][0xb8][0xa4][0xe6][0xaf][0x94][0xe7][0x89][0xb9][0xe5][0xa8][0xb1][0xe4][0xb9][0x90][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe7][0xbd][0xaa][0xe6][0x81][0xb6][0xe5][0x90][0x8d][0xe5][0x8d][0x95]-2"",""conversion_cost"":0.76,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":-3.689349E19,""click_rate"":2.0,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200220125,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":2.0,""checked"":0,""deep_conversion_cost"":0.00,""cost"":1040.16,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":1262,""trueviews"":0,""impressions"":88759,""downloaded"":0,""__op"":0,""clicks"":3509,""advertiser_name"":""[0xe5][0x9c][0xa3][0xe4][0xb8][0x96][0xe4][0xba][0x92][0xe5][0xa8][0xb1][0xe5][0xbd][0xb1][0xe8][0xa7][0x86][0xe7][0xa7][0x91][0xe6][0x8a][0x80][0xe6][0xb1][0x9f][0xe8][0x8b][0x8f][0xe8][0x82][0xa1][0xe4][0xbb][0xbd][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]- [0xe6][0x96][0x97][0xe7][0xa0][0xb4][0xe8][0x8b][0x8d][0xe7][0xa9][0xb9][0xc2][0xb7][0xe6][0xad][0xa2][0xe6][0x88][0x88]"",""conversion_cost"":0.82,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":-2.0,""click_rate"":-3.689349E19,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200219795,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":-1.0842022E-19,""checked"":0,""deep_conversion_cost"":0.00,""cost"":675.92,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":10627,""trueviews"":0,""impressions"":374634,""downloaded"":0,""__op"":0,""clicks"":13040,""advertiser_name"":""[0xe4][0xb9][0x90][0xe7][0xbf][0xbb][0xe6][0x96][0x87][0xe5][0x8c][0x96][0xe4][0xbc][0xa0][0xe5][0xaa][0x92][0xef][0xbc][0x88][0xe5][0x8c][0x97][0xe4][0xba][0xac][0xef][0xbc][0x89][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe5][0xa4][0xa7][0xe8][0x84][0x9a][0xe4][0xb8][0xab][0xe6][0x81][0x90][0xe9][0xbe][0x99][0xe5][0xae][0xb6][0xe6][0x97][0x8f][0xe7][0xac][0xac][0xe4][0xb8][0x80][0xe5][0xad][0xa3]-2"",""conversion_cost"":0.06,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":3.689349E19,""click_rate"":-3.689349E19,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200219769,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":-3.689349E19,""checked"":0,""deep_conversion_cost"":0.00,""cost"":313.62,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":392,""trueviews"":0,""impressions"":40337,""downloaded"":0,""__op"":0,""clicks"":1305,""advertiser_name"":""[0xe9][0x9c][0x8d][0xe5][0xb0][0x94][0xe6][0x9e][0x9c][0xe6][0x96][0xaf][0xe4][0xb8][0xa4][0xe6][0xaf][0x94][0xe7][0x89][0xb9][0xe5][0xa8][0xb1][0xe4][0xb9][0x90][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe6][0x92][0x9e][0xe9][0x82][0xaa]"",""conversion_cost"":0.80,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":1.0842022E-19,""click_rate"":-3.689349E19,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200219455,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":-1.0842022E-19,""checked"":0,""deep_conversion_cost"":0.00,""cost"":949.89,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":1039,""trueviews"":0,""impressions"":85247,""downloaded"":0,""__op"":0,""clicks"":2323,""advertiser_name"":""[0xe5][0x9c][0xa3][0xe4][0xb8][0x96][0xe4][0xba][0x92][0xe5][0xa8][0xb1][0xe5][0xbd][0xb1][0xe8][0xa7][0x86][0xe7][0xa7][0x91][0xe6][0x8a][0x80][0xe6][0xb1][0x9f][0xe8][0x8b][0x8f][0xe8][0x82][0xa1][0xe4][0xbb][0xbd][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe6][0x96][0x97][0xe7][0xa0][0xb4][0xe8][0x8b][0x8d][0xe7][0xa9][0xb9][0xc2][0xb7][0xe8][0xa7][0x89][0xe9][0x86][0x92]"",""conversion_cost"":0.91,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":-0.0,""click_rate"":-3.689349E19,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200218905,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":1.0842022E-19,""checked"":0,""deep_conversion_cost"":0.00,""cost"":2762.22,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":2976,""trueviews"":0,""impressions"":333866,""downloaded"":0,""__op"":0,""clicks"":10416,""advertiser_name"":""[0xe5][0x8c][0x97][0xe4][0xba][0xac][0xe4][0xb8][0xa4][0xe6][0xaf][0x94][0xe7][0x89][0xb9][0xe4][0xba][0x92][0xe5][0xa8][0xb1][0xe7][0xa7][0x91][0xe6][0x8a][0x80][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe5][0xa5][0xb3][0xe5][0x84][0xbf][0xe5][0x9b][0xbd][0xe5][0x89][0x8d][0xe4][0xbc][0xa0]-2"",""conversion_cost"":0.93,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":-2.0,""click_rate"":-3.689349E19,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200218897,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":3.689349E19,""checked"":0,""deep_conversion_cost"":0.00,""cost"":8201.12,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":7786,""trueviews"":0,""impressions"":910167,""downloaded"":0,""__op"":0,""clicks"":24526,""advertiser_name"":""[0xe9][0x9c][0x8d][0xe5][0xb0][0x94][0xe6][0x9e][0x9c][0xe6][0x96][0xaf][0xe4][0xb8][0xa4][0xe6][0xaf][0x94][0xe7][0x89][0xb9][0xe5][0xa8][0xb1][0xe4][0xb9][0x90][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe5][0x86][0xb0][0xe9][0x9b][0xaa][0xe5][0xa4][0xa7][0xe5][0x9b][0xb4][0xe6][0x8d][0x95]-2"",""conversion_cost"":1.05,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":2.0,""click_rate"":2.0,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200214397,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":2.0,""checked"":0,""deep_conversion_cost"":0.00,""cost"":41096.14,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":22323,""trueviews"":0,""impressions"":7883672,""downloaded"":0,""__op"":0,""clicks"":133022,""advertiser_name"":""[0xe4][0xb8][0x9c][0xe9][0x98][0xb3][0xe5][0xa5][0x87][0xe6][0xa0][0x91][0xe6][0x9c][0x89][0xe9][0xb1][0xbc][0xe6][0x96][0x87][0xe5][0x8c][0x96][0xe4][0xbc][0xa0][0xe5][0xaa][0x92][0xe6][0x9c][0x89][0xe9][0x99][0x90][0xe5][0x85][0xac][0xe5][0x8f][0xb8]-[0xe5][0x87][0xb6][0xe6][0xa1][0x88][0xe6][0xb7][0xb1][0xe5][0xa4][0x84]"",""conversion_cost"":1.84,""updatetime"":""2024-03-16 06:27:19""},{""installed"":0,""avg_cpm"":-1.0842022E-19,""click_rate"":2.0,""channel"":""iqiyi"",""avg_cpv"":0.0,""advertiser_id"":51200214343,""dt"":""2024-03-15"",""downloading"":0,""avg_cpc"":-0.0,""checked"":0,""deep_conversion_cost"":0.00,""cost"":12867.96,""create_time"":""2024-03-16 06:27:20"",""deep_conversion_count"":0,""conversion_count"":6660,""trueviews"":0,""impressions"":1864632,""downloaded"":0,""__op"":0,""clicks"":33469,""advertiser_n""`


;;;","20/Mar/24 09:35;flink-cdc-import;Date: Mon Mar 18 19:59:50 CST 2024, Author: [dickson-bit|https://github.com/dickson-bit]

EventRecordSerializationSchema.java：129
    private String serializeRecord(TableInfo tableInfo, RecordData record, boolean isDelete) {
        List<Column> columns = tableInfo.schema.getColumns();
        Preconditions.checkArgument(columns.size() == record.getArity());
        Map<String, Object> rowMap = new HashMap<>(record.getArity() + 1);
        for (int i = 0; i < record.getArity(); i++) {
            rowMap.put(columns.get(i).getName(),     
### tableInfo.fieldGetters[i].getFieldOrNull(record));
        }
        rowMap.put(""__op"", isDelete ? 1 : 0);
        return jsonWrapper.toJSONString(rowMap);
    }

`2024-03-18 17:48:39,906 TRACE com.ververica.cdc.debezium.event.SourceRecordEventDeserializer [] - Process data change record: SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={transaction_id=null, ts_sec=1710755544, file=mysql-bin.007075, pos=884086420, row=2, server_id=2, event=2}} ConnectRecord{topic='mysql_binlog_source.opt.iqiyi_tuiguang_ad_data_report_daily_new', kafkaPartition=null, key=Struct{dt=18098,advertiser_id=51200107313}, keySchema=Schema{mysql_binlog_source.opt.iqiyi_tuiguang_ad_data_report_daily_new.Key:STRUCT}, value=Struct{after=Struct{dt=18098,advertiser_id=51200107313,impressions=705513,clicks=13371,trueviews=0,downloading=0,downloaded=0,installed=0,click_rate=1.899999976158142,cost=3286.56,avg_cpm=4.659999847412109,
avg_cpc=0.25

},source=Struct{version=1.9.7.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1710755544000,db=opt,table=iqiyi_tuiguang_ad_data_report_daily_new,server_id=2,file=mysql-bin.007075,pos=884086604,row=1,thread=116195990},op=c,ts_ms=1710755319488}, valueSchema=Schema{mysql_binlog_source.opt.iqiyi_tuiguang_ad_data_report_daily_new.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}`

2024-03-18 17:48:40,000 TRACE com.starrocks.data.load.stream.v2.StreamLoadManagerV2        [] - Write uniqueKey null, database test, table iqiyi_tuiguang_ad_data_report_daily_simple, row {""installed"":0,""cost"":6295.47,""avg_cpm"":-2.0,""click_rate"":-1.0842022E-19,""trueviews"":0,""impressions"":1315300,""downloaded"":0,""__op"":0,""advertiser_id"":51200110735,""dt"":""2019-07-20"",""downloading"":0,""clicks"":43084,""
avg_cpc"":2.0
}
;;;","20/Mar/24 09:35;flink-cdc-import;Date: Tue Mar 19 19:04:31 CST 2024, Author: [dickson-bit|https://github.com/dickson-bit]

2024-03-19 18:52:24,603 INFO  com.ververica.cdc.connectors.starrocks.sink.EventRecordSerializationSchema [] - the json string:{""avg_cpm"":-0.0,""click_rate"":-3.689349E19,""channel"":""qq"",""leads_purchase_rate"":0.0,""avg_cpv"":3.689349E19,""advertiser_id"":34131923,""dt"":""2024-03-11"",""leads_purchase_cost"":0.00,""avg_cpc"":-1.0842022E-19,""checked"":1,""cost"":1198.70,""create_time"":""2024-03-12 00:39:42"",""conversion_count"":22083,""impressions"":456154,""__op"":0,""key_page_view_count"":0,""leads_purchase_count"":0,""clicks"":11859,""advertiser_name"":""有限公司"",""conversion_cost"":0.05,""updatetime"":""2024-03-13 06:32:01""};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug]MySqlSource An error occurred when adding a table to the table.include.list attribute,FLINK-34884,13572663,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,20/Mar/24 09:35,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"Flink-cdc Version : 2.3.0 

When I was using MySqlSource, when starting the program for the first time, the 'table.include.list'=sqluser.pa_adm，and enable checkpoint，The data in the table ‘sqluser.pa_adm’  is normal.

When I modify 'table.include.list'=sqluser.pa_adm，sqluser.pa_person，program recovery from savepoint，The program started reporting an error and was unable to capture data for sqluser.pa_person，throwing exceptions 'Encountered change event 'Event{header=EventHeaderV4{timestamp=170917       7391000, eventType=TABLE_MAP, serverId=1, headerLength=19, dataLength=117, nextPosition=769436194, flags=0}, data=TableMapEventData{tableId=5303, database='sqluser', table='pa_person', columnTypes=8, 15, 18, 18, 18, 18, 18, 18, 18, 18, 18, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1, 15, columnMetadata=0, 192,        0, 0, 0, 0, 0, 0, 0, 0, 0, 192, 96, 96, 96, 96, 96, 384, 96, 96, 384, 30, 30, 30, 30, 0, 96, columnNullability={5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1       9, 20, 21, 22, 23, 24, 25, 26}, eventMetadata=TableMapEventMetadata{signedness={1}, defaultCharset=33, charsetCollations=null, columnCharsets=null, columnNames=null,        setStrValues=null, enumStrValues=null, geometryTypes=null, simplePrimaryKeys=null, primaryKeysWithPrefix=null, enumAndSetDefaultCharset=null, enumAndSetColumnCharse       ts=null,visibility=null}}}' at offset {transaction_id=null, ts_sec=1709177391, file=binlog.000476, pos=769435520, server_id=1, event=3} for table sqluser.pa_person whose schema isn't known to this connector. One possible cause is an incomplete database history topic. Take a new snapshot in this case.'

I will read the source code later，When the program is restarted and restored from state, the schema of the  added table sqluser.pa_person no exists



---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3112
Created by: [xiahuya|https://github.com/xiahuya]
Labels: 
Created at: Thu Mar 07 15:38:59 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:35:41.0,,,,,,,,,,"0|z1o3qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error on Postgres-CDC using incremental snapshot with UUID column as PK,FLINK-34883,13572662,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,03/Jun/24 16:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,pull-request-available,,"A majority of our Postgres databases use UUIDs as primary keys.
When we enable 'scan.incremental.snapshot.enabled = true', Flink-CDC will try to split into chunks.
The splitTableIntoChunks function relies on the queryMinMax function, which fails when trying to calculate the MIN(UUID) and MAX(UUID), as that is not supported in Postgres.

Is there a way around this?

When we convert our column to VARCHAR, rather than UUID, everything seems to work.
We did not find a way to cast our UUIDs to VARCHAR while splitting them into chunks without editing the source code or altering the source table.

Disabling incremental snapshots also fixes the issue, as we do not split into chunks anymore, but this would mean we get a global read lock on the data before snapshot reading, which we want to avoid.

Thanks in advance for the help!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3108
Created by: [olivier-derom|https://github.com/olivier-derom]
Labels: 
Created at: Wed Mar 06 16:55:03 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:35:37 UTC 2024,,,,,,,,,,"0|z1o3qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:35;flink-cdc-import;Date: Wed Mar 06 22:09:53 CST 2024, Author: [olivier-derom|https://github.com/olivier-derom]

I figured this is a limitation of Postgres and not Flink-CDC, so tried to create my own min(uuid, uuid) and max(uuid, uuid) as well as aggregate min(uuid) and max(uuid).
They work perfectly when I execute the SQL statement myself on the database, but Flink-CDC still says 'function min(uuid) does not exist'.
Does it somehow not have access to custom functions?
The database connection in Flink-CDC is made using the same credentials as I created the function with, so the user should have access.;;;","20/Mar/24 09:35;flink-cdc-import;Date: Mon Mar 18 18:14:03 CST 2024, Author: [drorventura|https://github.com/drorventura]

@olivier-derom were you able to make that work?

if not then this should solve your missing functions:
```
create or replace function min(uuid, uuid)
    returns uuid
    immutable parallel safe
    language plpgsql as
$$
begin
    return least($1, $2);
end
$$;

create aggregate min(uuid) (
    sfunc = min,
    stype = uuid,
    combinefunc = min,
    parallel = safe,
    sortop = operator (<)
    );

create or replace function max(uuid, uuid)
    returns uuid
    immutable parallel safe
    language plpgsql as
$$
begin
    return greatest($1, $2);
end
$$;

create aggregate max(uuid) (
    sfunc = max,
    stype = uuid,
    combinefunc = max,
    parallel = safe,
    sortop = operator (>)
    );
```

however it would still not work since uuid type is not supported or mapper in the 
`com.ververica.cdc.connectors.postgres.source.utils.PostgresTypeUtils.convertFromColumn`
and requires a mapping to DataType.STRING()

I wasn't able to proceed with the contribution yet, however I do wonder how did you manage to get through?
;;;","20/Mar/24 09:35;flink-cdc-import;Date: Mon Mar 18 19:44:42 CST 2024, Author: [olivier-derom|https://github.com/olivier-derom]

@drorventura 
As per my last reply, I indeed created my own functions that are able to calculate the min/max of UUID's, similar to how you defined them.
While this allows us to select min/max(UUID) when executing a query on the database in an SQL client like DBeaver, Flink was not able to use these functions, and I haven't figured out how to make it work yet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mysql cdc to doris:  TIME type converting fail,FLINK-34882,13572661,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,20/Mar/24 09:35,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"```txt
2024-03-05 18:23:42
java.lang.RuntimeException: Failed to schema change, CreateTableEvent{tableId=app.xxx, schema=columns={`id` BIGINT NOT NULL,`create_time` TIMESTAMP(6) NOT NULL,`end_time` TIME(0) NOT NULL,`start_time` TIME(0) NOT NULL,`update_time` TIMESTAMP(6)}, primaryKeys=id, options=()}, reason: Flink doesn't support converting type TIME(0) to Doris type yet.
	at com.ververica.cdc.connectors.doris.sink.DorisMetadataApplier.applySchemaChange(DorisMetadataApplier.java:86)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistryRequestHandler.applySchemaChange(SchemaRegistryRequestHandler.java:82)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistryRequestHandler.flushSuccess(SchemaRegistryRequestHandler.java:149)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistry.handleEventFromOperator(SchemaRegistry.java:123)
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.handleEventFromOperator(OperatorCoordinatorHolder.java:204)
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:121)
	at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1062)
	at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:604)
	at jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)


```

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3099
Created by: [laizuan|https://github.com/laizuan]
Labels: 
Created at: Tue Mar 05 18:30:52 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:35:30 UTC 2024,,,,,,,,,,"0|z1o3q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:35;flink-cdc-import;Date: Wed Mar 06 13:37:01 CST 2024, Author: [laizuan|https://github.com/laizuan]

flink cdc: 3.0.1
flink: 1.8.1

mysql:8.x
doris:2.x;;;","20/Mar/24 09:35;flink-cdc-import;Date: Wed Mar 06 15:40:30 CST 2024, Author: [w517424787|https://github.com/w517424787]

> flink cdc: 3.0.1 flink: 1.8.1
> 
> mysql:8.x doris:2.x

Flink 1.18.1 ? not flink 1.8.1;;;","20/Mar/24 09:35;flink-cdc-import;Date: Wed Mar 06 15:55:26 CST 2024, Author: [laizuan|https://github.com/laizuan]

yes, 1.18.1

;;;","20/Mar/24 09:35;flink-cdc-import;Date: Sat Mar 09 22:48:09 CST 2024, Author: [vinlee19|https://github.com/vinlee19]

Maybe, https://github.com/apache/flink-cdc/pull/2848/files will fix the problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature] Flink CDC Oracle support tables without primary keys and scan newly added tables,FLINK-34881,13572660,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,20/Mar/24 09:35,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation
   In the  Incremental Snapshot based DataStream,I hope this support, thank you!
-  Like MySQL CDC support tables that do not have a primary key. To use a table without primary keys, you must configure the scan.incremental.snapshot.chunk.key-column option and specify one non-null field.
- Scan Newly Added Tables feature enables you add new tables to monitor for existing running pipeline, the newly added tables will read theirs snapshot data firstly and then read their changelog automatically.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3095
Created by: [Fly365|https://github.com/Fly365]
Labels: enhancement, 
Created at: Tue Mar 05 10:34:53 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:35:22.0,,,,,,,,,,"0|z1o3q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Pipeline] SchemeRegistry saves the routing rules used to restore the TransformDataOperator,FLINK-34880,13572659,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,20/Mar/24 09:35,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

To be supplemented.

### Solution

To be supplemented.

### Alternatives

None.

### Anything else?

To be supplemented.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3081
Created by: [aiwenmo|https://github.com/aiwenmo]
Labels: enhancement, 
Created at: Mon Feb 26 23:50:04 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:35:17.0,,,,,,,,,,"0|z1o3ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Pipeline] Flink CDC pipeline transform supports the strategy of schema evolution,FLINK-34879,13572658,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,20/Mar/24 09:35,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

To be supplemented.

### Solution

To be supplemented.

### Alternatives

None.

### Anything else?

To be supplemented.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3080
Created by: [aiwenmo|https://github.com/aiwenmo]
Labels: enhancement, 
Created at: Mon Feb 26 23:48:38 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:35:13.0,,,,,,,,,,"0|z1o3pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Pipeline] Flink CDC pipeline transform supports CASE WHEN,FLINK-34878,13572657,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wink,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,29/Apr/24 08:23,04/Jun/24 20:40,29/Apr/24 08:23,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,github-import,pull-request-available,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

To be supplemented.

### Solution

To be supplemented.

### Alternatives

None.

### Anything else?

To be supplemented.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3079
Created by: [aiwenmo|https://github.com/aiwenmo]
Labels: enhancement, 
Created at: Mon Feb 26 23:47:53 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 29 03:35:43 UTC 2024,,,,,,,,,,"0|z1o3pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Apr/24 03:35;renqs;flink-cdc master: 75a553eb924a9108f57ce6c388aef69facc73af0

flink-cdc release-3.1: 7a1947927df29cb5c8a78129a5819409b069c602;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Pipeline] Flink CDC pipeline transform supports type conversion,FLINK-34877,13572656,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,27/May/24 06:04,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,pull-request-available,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

To be supplemented.

### Solution

To be supplemented.

### Alternatives

None.

### Anything else?

To be supplemented.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3078
Created by: [aiwenmo|https://github.com/aiwenmo]
Labels: enhancement, 
Created at: Mon Feb 26 23:47:03 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:35:06.0,,,,,,,,,,"0|z1o3p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Pipeline] Flink CDC pipeline transform supports UDF,FLINK-34876,13572655,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:35,20/Mar/24 09:35,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

To be supplemented.

### Solution

To be supplemented.

### Alternatives

None.

### Anything else?

To be supplemented.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3077
Created by: [aiwenmo|https://github.com/aiwenmo]
Labels: enhancement, 
Created at: Mon Feb 26 23:44:59 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:35:02.0,,,,,,,,,,"0|z1o3ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature] CDC Action supports update and delete record (logical deletion) ,FLINK-34875,13572654,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Motivation
There is a very common scenario in government departments (such as health insurance data reporting]. When data is reported from the city to the province and from the province to the country, deleted and modified data must be retained to prevent data from being deleted at the next level.



### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3075
Created by: [melin|https://github.com/melin]
Labels: enhancement, 
Created at: Mon Feb 26 13:01:57 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:34:59 UTC 2024,,,,,,,,,,"0|z1o3oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:34;flink-cdc-import;Date: Thu Feb 29 10:26:01 CST 2024, Author: [klion26|https://github.com/klion26]

+1 for the feature

@melin could you please change the description to English.;;;","20/Mar/24 09:34;flink-cdc-import;Date: Thu Feb 29 20:11:52 CST 2024, Author: [melin|https://github.com/melin]

@lvyanquan ;;;","20/Mar/24 09:34;flink-cdc-import;Date: Fri Mar 01 10:43:50 CST 2024, Author: [lvyanquan|https://github.com/lvyanquan]

This is indeed a universal requirement. 
However, currently our schemaChangeEvent is not bound to the Checkpoint timing, so we have not implemented exactly once write and rely on primary key deduplication. 

This can be added to the Todo work item.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[MongoDB] Support initial.snapshotting.pipeline related configs in table api,FLINK-34874,13572653,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Motivation

MongoDB's startup.mode.copy.existing.pipeline(akka initial.snapshotting.pipeline in mongo-cdc] is an array of JSON objects describing the pipeline operations to run when copying existing data, see [link|https://www.mongodb.com/docs/kafka-connector/current/source-connector/configuration-properties/startup/#std-label-source-configuration-startup]. This can improve the use of indexes by the copying manager and make copying more efficient, which is very important in some user scenarios. Besides, there are also some related configs, like startup.mode.copy.existing.queue.size, startup.mode.copy.existing.max.threads.
Currently we only support these configs in datastream api, for the convenience of users, we should also support them in table api.

### Solution

Support initial.snapshotting.pipeline related configs in table api

### Alternatives

_No response_

### Anything else?

Note that in 2.3.0, we remove these configs from table api when support incremental snapshot mode for MongoDB in this [commit|https://github.com/ververica/flink-cdc-connectors/commit/301e5a8ab08f7b6c4414c0a81561b9a1bf7fab19], since in incremental snapshot mode, the semantic is inconsistent when uses the pipeline operations. The reason is that in snapshot phase of incremental snapshot mode, the oplog will be played back after each snapshot to compensate for changes, but the pipeline operations in copy.existing.pipeline are not applied to the playback oplog, which means the semantic of this config is inconsistent.
But in legacy debezium mode, the behaviour is correct, so we add these configs back in debezium mode for better forward compatibility. And notify user not to use them in incremental snapshot mode due to above reason.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3069
Created by: [herunkang2018|https://github.com/herunkang2018]
Labels: enhancement, 
Assignee: [herunkang2018|https://github.com/herunkang2018]
Created at: Tue Feb 20 10:28:11 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:34:54 UTC 2024,,,,,,,,,,"0|z1o3og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:34;flink-cdc-import;Date: Tue Feb 20 10:46:00 CST 2024, Author: [Jiabao-Sun|https://github.com/Jiabao-Sun]

Thanks @herunkang2018 for reporting this issue.
Assigned to you, please go ahead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Bug] After starting Streaming ELT from MySQL to StarRocks using Flink CDC 3.0, the newly created tables are not being synchronized.",FLINK-34873,13572652,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

image flink:scala_2.12-java8， consistent with the doc

### Flink CDC version

Both 3.0.0 and 3.0.1 have the same issue

### Database and its version

debezium/example-mysql:1.1，registry.starrocks.io/starrocks/allin1-ubuntu 
consistent with the doc

### Minimal reproduce step

Upon following the steps in the documentation, after completing the 'Submit job using FlinkCDC cli' step, I created a new table orders_1  and inserted columns into it
Here is the schema for the orders_1 table I created:
CREATE TABLE `orders_1` (
`id` INT NOT NULL,
`price` DECIMAL(10,2) NOT NULL,
PRIMARY KEY (`id`)
];

### What did you expect to see?

Table orderrs_1 should be synchronized to starrocks

### What did you see instead?

A few days ago, while testing and inserting columns into 'orders_1', an error stating 'schema orders_1 does not exist' occurred. Now, I am unable to reproduce this error. However, the issue of CDC showing no response to the new table 'orders_1' persists.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3068
Created by: [tatianguiqu|https://github.com/tatianguiqu]
Labels: bug, 
Created at: Mon Feb 19 18:59:33 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:34:50 UTC 2024,,,,,,,,,,"0|z1o3o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:34;flink-cdc-import;Date: Mon Mar 11 17:11:57 CST 2024, Author: [avatarTaier|https://github.com/avatarTaier]

Seems like a bug, or is it not supported?;;;","20/Mar/24 09:34;flink-cdc-import;Date: Mon Mar 11 17:14:14 CST 2024, Author: [avatarTaier|https://github.com/avatarTaier]

@ruanhang1993   ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Bug] Mysql is synchronized to doris, and an error is reported when adding or deleting data after dropping column.",FLINK-34872,13572651,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.18.1

### Flink CDC version

3.0.0

### Database and its version

mysql ：5.6.24
doris  ：doris-2.0.0-alpha1

### Minimal reproduce step

Data can be monitored normally and can be inserted, deleted, etc. normally, but once the data is re-inserted after deleting the field, the program will exit with an error.
![image|https://github.com/ververica/flink-cdc-connectors/assets/22792154/0445ae02-7d9c-4f94-98eb-f677c7297ae7]

![image|https://github.com/ververica/flink-cdc-connectors/assets/22792154/4d494ec3-cab5-4be7-be9c-86dcb533f5c4)
The error message is as follows:
2024-02-06 16:05:39
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)
    at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)
    at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)
    at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
    at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
    at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
    at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
    at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
    at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
    at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
    at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
    at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
    at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
    at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
    at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
    at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
    at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
    at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
    at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
    at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
    at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
    at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
    at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
    at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: java.lang.IllegalStateException: Column size does not match the data size
    at com.ververica.cdc.common.utils.Preconditions.checkState(Preconditions.java:160)
    at com.ververica.cdc.connectors.doris.sink.DorisEventSerializer.serializerRecord(DorisEventSerializer.java:119)
    at com.ververica.cdc.connectors.doris.sink.DorisEventSerializer.applyDataChangeEvent(DorisEventSerializer.java:98)
    at com.ververica.cdc.connectors.doris.sink.DorisEventSerializer.serialize(DorisEventSerializer.java:69)
    at com.ververica.cdc.connectors.doris.sink.DorisEventSerializer.serialize(DorisEventSerializer.java:47)
    at org.apache.doris.flink.sink.batch.DorisBatchWriter.write(DorisBatchWriter.java:96)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:161)
    at com.ververica.cdc.runtime.operators.sink.DataSinkWriterOperator.processElement(DataSinkWriterOperator.java:154)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:237)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:146)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Thread.java:834]

### What did you expect to see?

After deleting the field and reinserting the data, the program will not report an error.

### What did you see instead?

flink program failed

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3064
Created by: [didiaode18|https://github.com/didiaode18]
Labels: bug, 
Created at: Wed Feb 07 16:59:03 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:34:45 UTC 2024,,,,,,,,,,"0|z1o3o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:34;flink-cdc-import;Date: Thu Feb 08 08:54:35 CST 2024, Author: [didiaode18|https://github.com/didiaode18]

Please assign this task to me;;;","20/Mar/24 09:34;flink-cdc-import;Date: Wed Feb 21 11:35:43 CST 2024, Author: [didiaode18|https://github.com/didiaode18]

> Please assign this task to me
@leonardBang
;;;","20/Mar/24 09:34;flink-cdc-import;Date: Sat Mar 09 22:18:30 CST 2024, Author: [vinlee19|https://github.com/vinlee19]

Do the log messages contain any errors related to DorisRuntimeException? If so, you might need to ask for help from Apache Doris community.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oracle add data has error,FLINK-34871,13572650,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.18.1

### Flink CDC version

3.0.0

### Database and its version

oracle 11g

### Minimal reproduce step

I run the document at https://ververica.github.io/flink-cdc-connectors/release-3.0/content/connectors/oracle-cdc.html,when i add a data to table,it has some error     
   Properties debeziumProperties = new Properties();
        debeziumProperties.setProperty(""log.mining.strategy"", ""online_catalog"");
        debeziumProperties.setProperty(""include.schema.changes"", ""true"");
        debeziumProperties.setProperty(""value.converter.schemas.enable"", ""true"");

        OracleSourceBuilder.OracleIncrementalSource oracleChangeEventSource =
                new OracleSourceBuilder()
                        .hostname(""hostname"")
                        .port(1521)
                        .databaseList(""ORCL"")
                        .schemaList(""FLINKUSER"")
                        .tableList(""FLINKUSER.CREATETABLE"")
                        .username(""flinkuser"")
                        .password(""flinkpw"")
                        .deserializer(new JsonDebeziumDeserializationSchema())
                        .includeSchemaChanges(true) // output the schema changes as well
                        .startupOptions(StartupOptions.initial())
                        .debeziumProperties(debeziumProperties)
                        .splitSize(2)
                        .build();

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.enableCheckpointing(3000L);

        DataStreamSource oracleChangeEventStream = env.fromSource(
                        oracleChangeEventSource,
                        WatermarkStrategy.noWatermarks(),
                        ""OracleParallelSource"")
                .setParallelism(4)
                .setParallelism(1);

        oracleChangeEventStream.print(];

![image|https://github.com/ververica/flink-cdc-connectors/assets/43168824/14d5e0e2-daed-4d8a-bce5-1ed83ccd5c88]


### What did you expect to see?

Is it my configuration problem

### What did you see instead?

null error

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3056
Created by: [ccczhouxin|https://github.com/ccczhouxin]
Labels: bug, 
Created at: Fri Feb 02 10:05:38 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:34:40 UTC 2024,,,,,,,,,,"0|z1o3ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:34;flink-cdc-import;Date: Sun Feb 04 13:55:11 CST 2024, Author: [ccczhouxin|https://github.com/ccczhouxin]

When i add column at table,id has other error. 
![image|https://github.com/ververica/flink-cdc-connectors/assets/43168824/c658f28e-fb2d-4265-963b-9a933741f590]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] MongoCDC checkpoint error,FLINK-34870,13572649,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.15.2

### Flink CDC version

2.4.1

### Database and its version

5

### Minimal reproduce step

When MongoCDC starts from a checkpoint, if there have been no data changes (such as inserts, updates, or deletes] before the checkpoint, starting from the checkpoint will still initiate the capture of all data. Only after data changes occur, will the initialization data not be captured.

### What did you expect to see?

After starting from a checkpoint, regardless of whether there have been data changes in the database, data will be captured starting from the offset.

### What did you see instead?

capture all data

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3053
Created by: [illsiveblue|https://github.com/illsiveblue]
Labels: bug, 
Created at: Thu Feb 01 10:22:33 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:34:34.0,,,,,,,,,,"0|z1o3nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug][mysql] Remove all previous table and add new added table will throw Exception.,FLINK-34869,13572648,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jmahonin,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,24/Apr/24 04:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.18

### Flink CDC version

3.0.1

### Database and its version

anyone 

### Minimal reproduce step

1. Stop job in savepoint.
2. Set 'scan.incremental.snapshot.enabled' = 'true' and then set tableList with tables which not includes in last time.
3. Then assign status will be chaos.
Take a test case for example:
```java
public class NewlyAddedTableITCase extends MySqlSourceTestBase {
    @Test
    public void testRemoveAndAddTablesOneByOne() throws Exception {
        testRemoveAndAddTablesOneByOne(
                1, ""address_hangzhou"", ""address_beijing"", ""address_shanghai"");
    }

    private void testRemoveAndAddTablesOneByOne(int parallelism, String... captureAddressTables)
            throws Exception {

        MySqlConnection connection = getConnection();
        // step 1: create mysql tables with all tables included
        initialAddressTables(connection, captureAddressTables);

        final TemporaryFolder temporaryFolder = new TemporaryFolder();
        temporaryFolder.create();
        final String savepointDirectory = temporaryFolder.newFolder().toURI().toString();

        // get all expected data
        List<String> fetchedDataList = new ArrayList<>();

        String finishedSavePointPath = null;
        // test removing and adding table one by one
        for (int round = 0; round < captureAddressTables.length; round++] {
            String captureTableThisRound = captureAddressTables[round];
            String cityName = captureTableThisRound.split(""_"")[1];
            StreamExecutionEnvironment env =
                    getStreamExecutionEnvironment(finishedSavePointPath, parallelism);
            StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

            String createTableStatement =
                    getCreateTableStatement(new HashMap<>(), captureTableThisRound);
            tEnv.executeSql(createTableStatement);
            tEnv.executeSql(
                    ""CREATE TABLE sink (""
                            + "" table_name STRING,""
                            + "" id BIGINT,""
                            + "" country STRING,""
                            + "" city STRING,""
                            + "" detail_address STRING,""
                            + "" primary key (table_name,id) not enforced""
                            + "") WITH (""
                            + "" 'connector' = 'values',""
                            + "" 'sink-insert-only' = 'false'""
                            + "")"");
            TableResult tableResult = tEnv.executeSql(""insert into sink select * from address"");
            JobClient jobClient = tableResult.getJobClient().get();

            // this round's snapshot data
            fetchedDataList.addAll(
                    Arrays.asList(
                            format(
                                    ""+I[%s, 416874195632735147, China, %s, %s West Town address 1]"",
                                    captureTableThisRound, cityName, cityName),
                            format(
                                    ""+I[%s, 416927583791428523, China, %s, %s West Town address 2]"",
                                    captureTableThisRound, cityName, cityName),
                            format(
                                    ""+I[%s, 417022095255614379, China, %s, %s West Town address 3]"",
                                    captureTableThisRound, cityName, cityName)));
            waitForSinkSize(""sink"", fetchedDataList.size());
            assertEqualsInAnyOrder(fetchedDataList, TestValuesTableFactory.getRawResults(""sink""));

            // only this round table's data is captured.
            // step 3: make binlog data for all tables before this round(also includes this round)
            for (int i = 0; i <= round; i++) {
                String tableName = captureAddressTables[i];
                makeBinlogForAddressTable(connection, tableName, round);
            }
            // this round's binlog data
            fetchedDataList.addAll(
                    Arrays.asList(
                            format(
                                    ""-U[%s, 416874195632735147, China, %s, %s West Town address 1]"",
                                    captureTableThisRound, cityName, cityName),
                            format(
                                    ""+U[%s, 416874195632735147, CHINA_%s, %s, %s West Town address 1]"",
                                    captureTableThisRound, round, cityName, cityName),
                            format(
                                    ""+I[%s, %d, China, %s, %s West Town address 4]"",
                                    captureTableThisRound,
                                    417022095255614380L + round,
                                    cityName,
                                    cityName)));

            // step 4: assert fetched binlog data in this round
            waitForSinkSize(""sink"", fetchedDataList.size());

            assertEqualsInAnyOrder(fetchedDataList, TestValuesTableFactory.getRawResults(""sink""));
            // step 5: trigger savepoint
            finishedSavePointPath = triggerSavepointWithRetry(jobClient, savepointDirectory);
            jobClient.cancel().get();
        }
    }

  // setting  primary key as id rather than <id, city> is more more realistic.
   private String getCreateTableStatement(
            Map<String, String> otherOptions, String... captureTableNames) {
        return format(
                ""CREATE TABLE address (""
                        + "" table_name STRING METADATA VIRTUAL,""
                        + "" id BIGINT NOT NULL,""
                        + "" country STRING,""
                        + "" city STRING,""
                        + "" detail_address STRING,""
                        + "" primary key (id) not enforced""
                        + "") WITH (""
                        + "" 'connector' = 'mysql-cdc',""
                        + "" 'scan.incremental.snapshot.enabled' = 'true',""
                        + "" 'hostname' = '%s',""
                        + "" 'port' = '%s',""
                        + "" 'username' = '%s',""
                        + "" 'password' = '%s',""
                        + "" 'database-name' = '%s',""
                        + "" 'table-name' = '%s',""
                        + "" 'scan.incremental.snapshot.chunk.size' = '2',""
                        + "" 'server-time-zone' = 'UTC',""
                        + "" 'server-id' = '%s',""
                        + "" 'scan.newly-added-table.enabled' = 'true'""
                        + "" %s""
                        + "")"",
                MYSQL_CONTAINER.getHost(),
                MYSQL_CONTAINER.getDatabasePort(),
                customDatabase.getUsername(),
                customDatabase.getPassword(),
                customDatabase.getDatabaseName(),
                getTableNameRegex(captureTableNames),
                getServerId(),
                otherOptions.isEmpty()
                        ? """"
                        : "",""
                        + otherOptions.entrySet().stream()
                        .map(
                                e ->
                                        String.format(
                                                ""'%s'='%s'"",
                                                e.getKey(), e.getValue()))
                        .collect(Collectors.joining("","")));
    }

}
```

### What did you expect to see?

return true

### What did you see instead?

An exception will occurs: 
```java
org.apache.flink.util.FlinkRuntimeException: The assigner is not ready to offer finished split information, this should not be called
	at com.ververica.cdc.connectors.mysql.source.assigners.MySqlSnapshotSplitAssigner.getFinishedSplitInfos(MySqlSnapshotSplitAssigner.java:379) ~[classes/:?]
	at com.ververica.cdc.connectors.mysql.source.assigners.MySqlHybridSplitAssigner.getFinishedSplitInfos(MySqlHybridSplitAssigner.java:139) ~[classes/:?]
	at com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator.handleLatestFinishedSplitNumberRequest(MySqlSourceEnumerator.java:324) ~[classes/:?]
	at com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator.handleSourceEvent(MySqlSourceEnumerator.java:170) ~[classes/:?]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.handleSourceEvent(SourceCoordinator.java:590) ~[flink-runtime-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$handleEventFromOperator$3(SourceCoordinator.java:297) ~[flink-runtime-1.18.0.jar:1.18.0]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$runInEventLoop$10(SourceCoordinator.java:469) ~[flink-runtime-1.18.0.jar:1.18.0]
	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40) [flink-core-1.18.0.jar:1.18.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_362]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_362]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_362]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_362]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362]

```

### Reason
When restarted with newly added table, MySqlBinlogSplit#filterOutdatedSplitInfos will filter previous table' FinishedSnapshotSplitInfo. In this case, list of FinishedSnapshotSplitInfo will be empty.

Then when add binlog split back to split reader, com.ververica.cdc.connectors.mysql.debezium.reader.BinlogSplitReader#configureFilter cBinlogSplitReader#shouldEmit will seen an empty list of FinishedSnapshotSplitInfo as binlog-only job
```java
    // specific offset mode
        if (finishedSplitInfos.isEmpty()) {
            for (TableId tableId : currentBinlogSplit.getTableSchemas().keySet()) {
                tableIdBinlogPositionMap.put(tableId, currentBinlogSplit.getStartingOffset());
            }
        }
        // initial mode
        else {
            for (FinishedSnapshotSplitInfo finishedSplitInfo : finishedSplitInfos) {
                TableId tableId = finishedSplitInfo.getTableId();
                List<FinishedSnapshotSplitInfo> list =
                        splitsInfoMap.getOrDefault(tableId, new ArrayList<>());
                list.add(finishedSplitInfo);
                splitsInfoMap.put(tableId, list);

                BinlogOffset highWatermark = finishedSplitInfo.getHighWatermark();
                BinlogOffset maxHighWatermark = tableIdBinlogPositionMap.get(tableId);
                if (maxHighWatermark == null || highWatermark.isAfter(maxHighWatermark)) {
                    tableIdBinlogPositionMap.put(tableId, highWatermark);
                }
            }
        }
```



### Are you willing to submit a PR?

- [x] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3051
Created by: [loserwang1024|https://github.com/loserwang1024]
Labels: bug, 
Created at: Wed Jan 31 17:12:21 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 04:19:44 UTC 2024,,,,,,,,,,"0|z1o3nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:34;flink-cdc-import;Date: Wed Jan 31 17:17:11 CST 2024, Author: [loserwang1024|https://github.com/loserwang1024]

@leonardBang , @ruanhang1993 , CC;;;","23/Apr/24 19:13;jmahonin;We seem to be hitting this issue as well. Is this issue possibly fixed by another commit, or is it still outstanding [~loserwang1024] ?

If the issue is still outstanding, with some guidance we could attempt a patch. Thanks.;;;","24/Apr/24 04:19;loserwang1024;It seems I haven't fixed it [~jmahonin] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] oracle-cdc cannot read oracle multitenant pdb binlog: ORA-00942: table or view does not exist (LOG_MINING_FLUSH),FLINK-34868,13572647,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.15.3

### Flink CDC version

2.4.0

### Database and its version

oracle 19c

### Minimal reproduce step

1. setup oracle env according to oracle cdc cdb-database doc: https://ververica.github.io/flink-cdc-connectors/release-2.4/content/connectors/oracle-cdc.html
2. using oracle cdc to read oracle log and print the data

### What did you expect to see?

when i insert one record to my oracle table, i expect to see the line printed in flink log.
When i use oracle-cdc 2.3, it works fine.

### What did you see instead?

error message as below:
java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: com.ververica.cdc.connectors.shaded.org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped.
	at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:50)
	at io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource.execute(LogMinerStreamingChangeEventSource.java:261)
	at com.ververica.cdc.connectors.oracle.source.reader.fetch.OracleStreamFetchTask$RedoLogSplitReadTask.execute(OracleStreamFetchTask.java:134)
	at com.ververica.cdc.connectors.oracle.source.reader.fetch.OracleStreamFetchTask.execute(OracleStreamFetchTask.java:72)
	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceStreamFetcher.lambda$submitTask$0(IncrementalSourceStreamFetcher.java:89)
	... 5 more
Caused by: io.debezium.DebeziumException: Failed to flush Oracle LogWriter (LGWR) buffers to disk
	at io.debezium.connector.oracle.logminer.logwriter.CommitLogWriterFlushStrategy.flush(CommitLogWriterFlushStrategy.java:89)
	at io.debezium.connector.oracle.logminer.LogMinerStreamingChangeEventSource.execute(LogMinerStreamingChangeEventSource.java:208)
	... 8 more
Caused by: java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist

	at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:509)
	at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:461)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1104)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:550)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:268)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:655)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:229)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:41)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:928)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1205)
	at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1823)
	at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1778)
	at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:303)
	at io.debezium.jdbc.JdbcConnection.lambda$execute$3(JdbcConnection.java:430)
	at io.debezium.jdbc.JdbcConnection.execute(JdbcConnection.java:447)
	at io.debezium.jdbc.JdbcConnection.execute(JdbcConnection.java:424)
	at io.debezium.connector.oracle.logminer.logwriter.CommitLogWriterFlushStrategy.flush(CommitLogWriterFlushStrategy.java:86)
	... 9 more
Caused by: Error : 942, Position : 7, Sql = UPDATE LOG_MINING_FLUSH SET LAST_SCN = 11664853015964, OriginalSql = UPDATE LOG_MINING_FLUSH SET LAST_SCN = 11664853015964, Error Msg = ORA-00942: table or view does not exist

	at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:513]
	... 25 more

### Anything else?

the message say ""LOG_MINING_FLUSH doesnot exist"". i find this table only exist in oracle pdb container. 

i guess the oracle-cdc code intends to create this table in cdb. but it incorrectly create this table in pdb.

so i manually create this table in cdb and the oracle-cdc works. the flink job just print the record in log

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3044
Created by: [abcdeian|https://github.com/abcdeian]
Labels: bug, 
Created at: Mon Jan 29 18:04:25 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:34:25.0,,,,,,,,,,"0|z1o3n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] [StarRocks] [cdc master branch] Unsupported CDC data type BYTES/VARBINARY,FLINK-34867,13572646,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

Flink 1.18

### Flink CDC version

3.0.1 
master 

### Database and its version

mysql 5.7.44
StarRocks 3.2.2

### Minimal reproduce step

To synchronize the entire database using regular expressions, tables: db.\.*,


```log
2024-01-29 16:32:18
java.lang.UnsupportedOperationException: Unsupported CDC data type BYTES
	at com.ververica.cdc.connectors.starrocks.sink.StarRocksUtils$CdcDataTypeTransformer.defaultMethod(StarRocksUtils.java:368)
	at com.ververica.cdc.connectors.starrocks.sink.StarRocksUtils$CdcDataTypeTransformer.defaultMethod(StarRocksUtils.java:236)
	at com.ververica.cdc.common.types.DataTypeDefaultVisitor.visit(DataTypeDefaultVisitor.java:49)
	at com.ververica.cdc.common.types.VarBinaryType.accept(VarBinaryType.java:86)
	at com.ververica.cdc.connectors.starrocks.sink.StarRocksUtils.toStarRocksDataType(StarRocksUtils.java:112)
	at com.ververica.cdc.connectors.starrocks.sink.StarRocksUtils.toStarRocksTable(StarRocksUtils.java:86)
	at com.ververica.cdc.connectors.starrocks.sink.StarRocksMetadataApplier.applyCreateTable(StarRocksMetadataApplier.java:87)
	at com.ververica.cdc.connectors.starrocks.sink.StarRocksMetadataApplier.applySchemaChange(StarRocksMetadataApplier.java:70)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistryRequestHandler.applySchemaChange(SchemaRegistryRequestHandler.java:82)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistryRequestHandler.flushSuccess(SchemaRegistryRequestHandler.java:149)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistry.handleEventFromOperator(SchemaRegistry.java:123)
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.handleEventFromOperator(OperatorCoordinatorHolder.java:204)
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:121)
	at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1062)
	at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:604)
	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172]

```


### What did you expect to see?

the data synchronization process to proceed smoothly.

### What did you see instead?

The data synchronization process has been interrupted and cannot continue.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3042
Created by: [everhopingandwaiting|https://github.com/everhopingandwaiting]
Labels: bug, 
Created at: Mon Jan 29 17:00:27 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:34:22 UTC 2024,,,,,,,,,,"0|z1o3mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:34;flink-cdc-import;Date: Mon Jan 29 18:04:03 CST 2024, Author: [everhopingandwaiting|https://github.com/everhopingandwaiting]

alse : java.lang.UnsupportedOperationException: Unsupported CDC data type  
 VARBINARY;;;","20/Mar/24 09:34;flink-cdc-import;Date: Wed Jan 31 13:58:16 CST 2024, Author: [joyCurry30|https://github.com/joyCurry30]

Can you offer your create-table sql? ;;;","20/Mar/24 09:34;flink-cdc-import;Date: Wed Jan 31 14:13:00 CST 2024, Author: [everhopingandwaiting|https://github.com/everhopingandwaiting]

 These are all the error messages. I compiled the latest branch of Flink CDC 3.0.x code and used a YAML script to synchronize the entire database. The syntax for synchronizing tables is tables: db.[^qrtz].\.* The problem may be due to the existence of fields with the data types longblob or blob in other tables.;;;","20/Mar/24 09:34;flink-cdc-import;Date: Wed Jan 31 15:21:42 CST 2024, Author: [joyCurry30|https://github.com/joyCurry30]

Could you please share the MySQL and StarRocks 'CREATE TABLE' DDL statements, as well as the associated YAML configuration files? I would like to replicate the issue in my local environment;;;","20/Mar/24 09:34;flink-cdc-import;Date: Wed Jan 31 15:31:36 CST 2024, Author: [everhopingandwaiting|https://github.com/everhopingandwaiting]

> Could you please share the MySQL and StarRocks 'CREATE TABLE' DDL statements, as well as the associated YAML configuration files? I would like to replicate the issue in my local environment

I will provide the table structures of  database and the contents of the YAML configuration file later
;;;","20/Mar/24 09:34;flink-cdc-import;Date: Fri Feb 02 10:05:24 CST 2024, Author: [joyCurry30|https://github.com/joyCurry30]

Hi, could you please share the DDL statements and YAML configuration files?
;;;","20/Mar/24 09:34;flink-cdc-import;Date: Fri Feb 02 10:49:40 CST 2024, Author: [everhopingandwaiting|https://github.com/everhopingandwaiting]

> Hi, could you please share the DDL statements and YAML configuration files?

cdc script config:
```yaml
source:
  type: mysql
  hostname: 1x
  port: 3306
  username: xxx
  password: xxx
  server-id: 45354-49440
  #tables: db.t\.*,db.sys\.*,db.code\.*,
  tables: db.\.*
  server-time-zone: Asia/Shanghai
  ignore.noprimarykey.table: true
  enable.column.comments: true
  #scan.startup.mode: specific-offset
  scan.startup.specific-offset.file: master-bin.000087
  scan.startup.specific-offset.pos: 1
sink:
  type: starrocks
  name: StarRocks Sink
  jdbc-url: jdbc:mysql://xxx:9030
  load-url: xxx:8030
  username: xx
  password: xxx
  table.create.properties.replication_num: 1
  table.create.properties.light_schema_change: true
  table.create.properties.fast_schema_evolution: true
  sink.connect.timeout-ms: 3000009
pipeline:
  name: Sync MySQL Database to StarRocks 20 jyao enable.column.comments  ignore.noprimarykey.table  ignore yjxx qrtz  logbin 87 fixed2908
  parallelism: 2
  enable-schema-evolution: true
route:
- source-table: db.[\S]*
  sink-table: db_.s<>
  replace-symbol: s<>
- source-table: db-test.[\S]*
  sink-table: db_.s<>
  replace-symbol: s<>

```

```sql
create table if not exists qrtz_calendars
(
    SCHED_NAME    varchar(120) not null,
    CALENDAR_NAME varchar(190) not null,
    CALENDAR      blob         not null,
    primary key (SCHED_NAME, CALENDAR_NAME)
)
    charset = latin1
    row_format = COMPACT;

create table if not exists qrtz_fired_triggers
(
    SCHED_NAME        varchar(120) not null,
    ENTRY_ID          varchar(95)  not null,
    TRIGGER_NAME      varchar(190) not null,
    TRIGGER_GROUP     varchar(190) not null,
    INSTANCE_NAME     varchar(190) not null,
    FIRED_TIME        bigint(13)   not null,
    SCHED_TIME        bigint(13)   not null,
    PRIORITY          int          not null,
    STATE             varchar(16)  not null,
    JOB_NAME          varchar(190) null,
    JOB_GROUP         varchar(190) null,
    IS_NONCONCURRENT  varchar(1)   null,
    REQUESTS_RECOVERY varchar(1)   null,
    primary key (SCHED_NAME, ENTRY_ID)
)
    charset = latin1
    row_format = COMPACT;

create index IDX_QRTZ_FT_INST_JOB_REQ_RCVRY
    on qrtz_fired_triggers (SCHED_NAME, INSTANCE_NAME, REQUESTS_RECOVERY);

create index IDX_QRTZ_FT_JG
    on qrtz_fired_triggers (SCHED_NAME, JOB_GROUP);

create index IDX_QRTZ_FT_J_G
    on qrtz_fired_triggers (SCHED_NAME, JOB_NAME, JOB_GROUP);

create index IDX_QRTZ_FT_TG
    on qrtz_fired_triggers (SCHED_NAME, TRIGGER_GROUP);

create index IDX_QRTZ_FT_TRIG_INST_NAME
    on qrtz_fired_triggers (SCHED_NAME, INSTANCE_NAME);

create index IDX_QRTZ_FT_T_G
    on qrtz_fired_triggers (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP);

create table if not exists qrtz_job_details
(
    SCHED_NAME        varchar(120) not null,
    JOB_NAME          varchar(190) not null,
    JOB_GROUP         varchar(190) not null,
    DESCRIPTION       varchar(250) null,
    JOB_CLASS_NAME    varchar(250) not null,
    IS_DURABLE        varchar(1)   not null,
    IS_NONCONCURRENT  varchar(1)   not null,
    IS_UPDATE_DATA    varchar(1)   not null,
    REQUESTS_RECOVERY varchar(1)   not null,
    JOB_DATA          blob         null,
    primary key (SCHED_NAME, JOB_NAME, JOB_GROUP)
)
    charset = latin1
    row_format = COMPACT;

create index IDX_QRTZ_J_GRP
    on qrtz_job_details (SCHED_NAME, JOB_GROUP);

create index IDX_QRTZ_J_REQ_RECOVERY
    on qrtz_job_details (SCHED_NAME, REQUESTS_RECOVERY);

create table if not exists qrtz_locks
(
    SCHED_NAME varchar(120) not null,
    LOCK_NAME  varchar(40)  not null,
    primary key (SCHED_NAME, LOCK_NAME)
)
    charset = latin1
    row_format = COMPACT;

create table if not exists qrtz_paused_trigger_grps
(
    SCHED_NAME    varchar(120) not null,
    TRIGGER_GROUP varchar(190) not null,
    primary key (SCHED_NAME, TRIGGER_GROUP)
)
    charset = latin1
    row_format = COMPACT;

create table if not exists qrtz_scheduler_state
(
    SCHED_NAME        varchar(120) not null,
    INSTANCE_NAME     varchar(190) not null,
    LAST_CHECKIN_TIME bigint(13)   not null,
    CHECKIN_INTERVAL  bigint(13)   not null,
    primary key (SCHED_NAME, INSTANCE_NAME)
)
    charset = latin1
    row_format = COMPACT;

create table if not exists qrtz_triggers
(
    SCHED_NAME     varchar(120) not null,
    TRIGGER_NAME   varchar(190) not null,
    TRIGGER_GROUP  varchar(190) not null,
    JOB_NAME       varchar(190) not null,
    JOB_GROUP      varchar(190) not null,
    DESCRIPTION    varchar(250) null,
    NEXT_FIRE_TIME bigint(13)   null,
    PREV_FIRE_TIME bigint(13)   null,
    PRIORITY       int          null,
    TRIGGER_STATE  varchar(16)  not null,
    TRIGGER_TYPE   varchar(8)   not null,
    START_TIME     bigint(13)   not null,
    END_TIME       bigint(13)   null,
    CALENDAR_NAME  varchar(190) null,
    MISFIRE_INSTR  smallint(2)  null,
    JOB_DATA       blob         null,
    primary key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP),
    constraint qrtz_triggers_ibfk_1
        foreign key (SCHED_NAME, JOB_NAME, JOB_GROUP) references qrtz_job_details (SCHED_NAME, JOB_NAME, JOB_GROUP)
)
    charset = latin1
    row_format = COMPACT;

create table if not exists qrtz_blob_triggers
(
    SCHED_NAME    varchar(120) not null,
    TRIGGER_NAME  varchar(190) not null,
    TRIGGER_GROUP varchar(190) not null,
    BLOB_DATA     blob         null,
    primary key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP),
    constraint qrtz_blob_triggers_ibfk_1
        foreign key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP) references qrtz_triggers (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP)
)
    charset = latin1
    row_format = COMPACT;

create index SCHED_NAME
    on qrtz_blob_triggers (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP);

create table if not exists qrtz_cron_triggers
(
    SCHED_NAME      varchar(120) not null,
    TRIGGER_NAME    varchar(190) not null,
    TRIGGER_GROUP   varchar(190) not null,
    CRON_EXPRESSION varchar(120) not null,
    TIME_ZONE_ID    varchar(80)  null,
    primary key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP),
    constraint qrtz_cron_triggers_ibfk_1
        foreign key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP) references qrtz_triggers (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP)
)
    charset = latin1
    row_format = COMPACT;

create table if not exists qrtz_simple_triggers
(
    SCHED_NAME      varchar(120) not null,
    TRIGGER_NAME    varchar(190) not null,
    TRIGGER_GROUP   varchar(190) not null,
    REPEAT_COUNT    bigint(7)    not null,
    REPEAT_INTERVAL bigint(12)   not null,
    TIMES_TRIGGERED bigint(10)   not null,
    primary key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP),
    constraint qrtz_simple_triggers_ibfk_1
        foreign key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP) references qrtz_triggers (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP)
)
    charset = latin1
    row_format = COMPACT;

create table if not exists qrtz_simprop_triggers
(
    SCHED_NAME    varchar(120)   not null,
    TRIGGER_NAME  varchar(190)   not null,
    TRIGGER_GROUP varchar(190)   not null,
    STR_PROP_1    varchar(512)   null,
    STR_PROP_2    varchar(512)   null,
    STR_PROP_3    varchar(512)   null,
    INT_PROP_1    int            null,
    INT_PROP_2    int            null,
    LONG_PROP_1   bigint         null,
    LONG_PROP_2   bigint         null,
    DEC_PROP_1    decimal(13, 4) null,
    DEC_PROP_2    decimal(13, 4) null,
    BOOL_PROP_1   varchar(1)     null,
    BOOL_PROP_2   varchar(1)     null,
    primary key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP),
    constraint qrtz_simprop_triggers_ibfk_1
        foreign key (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP) references qrtz_triggers (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP)
)
    charset = latin1
    row_format = COMPACT;

create index IDX_QRTZ_T_C
    on qrtz_triggers (SCHED_NAME, CALENDAR_NAME);

create index IDX_QRTZ_T_G
    on qrtz_triggers (SCHED_NAME, TRIGGER_GROUP);

create index IDX_QRTZ_T_J
    on qrtz_triggers (SCHED_NAME, JOB_NAME, JOB_GROUP);

create index IDX_QRTZ_T_JG
    on qrtz_triggers (SCHED_NAME, JOB_GROUP);

create index IDX_QRTZ_T_NEXT_FIRE_TIME
    on qrtz_triggers (SCHED_NAME, NEXT_FIRE_TIME);

create index IDX_QRTZ_T_NFT_MISFIRE
    on qrtz_triggers (SCHED_NAME, MISFIRE_INSTR, NEXT_FIRE_TIME);

create index IDX_QRTZ_T_NFT_ST
    on qrtz_triggers (SCHED_NAME, TRIGGER_STATE, NEXT_FIRE_TIME);

create index IDX_QRTZ_T_NFT_ST_MISFIRE
    on qrtz_triggers (SCHED_NAME, MISFIRE_INSTR, NEXT_FIRE_TIME, TRIGGER_STATE);

create index IDX_QRTZ_T_NFT_ST_MISFIRE_GRP
    on qrtz_triggers (SCHED_NAME, MISFIRE_INSTR, NEXT_FIRE_TIME, TRIGGER_GROUP, TRIGGER_STATE);

create index IDX_QRTZ_T_N_G_STATE
    on qrtz_triggers (SCHED_NAME, TRIGGER_GROUP, TRIGGER_STATE);

create index IDX_QRTZ_T_N_STATE
    on qrtz_triggers (SCHED_NAME, TRIGGER_NAME, TRIGGER_GROUP, TRIGGER_STATE);

create index IDX_QRTZ_T_STATE
    on qrtz_triggers (SCHED_NAME, TRIGGER_STATE);




```;;;","20/Mar/24 09:34;flink-cdc-import;Date: Fri Feb 02 10:52:52 CST 2024, Author: [everhopingandwaiting|https://github.com/everhopingandwaiting]

> Hi, could you please share the DDL statements and YAML configuration files?
code branch 
https://github.com/everhopingandwaiting/flink-cdc-connectors/tree/dev_3.x;;;",,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Meet NoClassDefFoundError when add new table in mysql cdc source,FLINK-34866,13572645,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.18

### Flink CDC version

3.0.1

### Database and its version

source: mysql: 5.7
sink: file system

### Minimal reproduce step

1. create mysql cdc flink job
2. add new table in mysql source

 meet exception:

java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:263)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:185)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:147)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:419)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.NoClassDefFoundError: com/ververica/cdc/common/utils/StringUtils
	at com.ververica.cdc.connectors.mysql.source.utils.RecordUtils.isTableChangeRecord(RecordUtils.java:395)
	at com.ververica.cdc.connectors.mysql.debezium.reader.BinlogSplitReader.shouldEmit(BinlogSplitReader.java:258)
	at com.ververica.cdc.connectors.mysql.debezium.reader.BinlogSplitReader.pollSplitRecords(BinlogSplitReader.java:165)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.pollSplitRecords(MySqlSplitReader.java:125)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader.fetch(MySqlSplitReader.java:87)
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:117)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624]
	... 1 more



### What did you expect to see?

should not capture exception when add new table on mysql cdc source

### What did you see instead?

meet java.lang.NoClassDefFoundError Exception and job failover

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3035
Created by: [pengmide|https://github.com/pengmide]
Labels: bug, 
Created at: Fri Jan 26 09:20:19 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:34:12 UTC 2024,,,,,,,,,,"0|z1o3mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:34;flink-cdc-import;Date: Sat Mar 02 03:00:55 CST 2024, Author: [avinashupadhya99|https://github.com/avinashupadhya99]

I had the same issue when adding a new column to an existing table. I worked around it by including the `flink-cdc-dist-3.0.1.jar` file from the Flink CDC binary release. The error arises when I only use the MySQL source connector 3.0.1 jar (flink-sql-connector-mysql-cdc-3.0.1.jar);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[enhancement] [StarRocks] When synchronizing tables using Flink CDC 3.x, is it possible to include the comments of the source table's fields when creating the target table?",FLINK-34865,13572644,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

When synchronizing tables using Flink CDC 3.x, is it possible to include the comments of the source table's fields when creating the target table?

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3007
Created by: [everhopingandwaiting|https://github.com/everhopingandwaiting]
Labels: enhancement, 
Created at: Thu Jan 18 11:58:45 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:34:05.0,,,,,,,,,,"0|z1o3mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[enhancement] [ cdc-connector] [ignore no primary key table] When using regular expressions to synchronize the entire database with Flink CDC, skip tables that do not have a primary key.",FLINK-34864,13572643,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:34,20/Mar/24 09:34,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [x] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

If regular expressions are used to match the tables to be synchronized, and some tables in the database do not have a primary key, when submitting a task with Flink CDC 3.0, an error occurs. Can we add an option to skip the synchronization of tables without a primary key, such as ignore-no-primary-key-table: true?


**Flink version**
1.18

**Flink CDC version**
3.0.0

**Database and its version**
Mysql 5.7 ,  StarRocks 3.2.2

### Solution

ignore-no-primary-key-table: true

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3006
Created by: [everhopingandwaiting|https://github.com/everhopingandwaiting]
Labels: enhancement, 
Created at: Thu Jan 18 11:41:12 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:34:01.0,,,,,,,,,,"0|z1o3m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need report metric or other ways to indicate whether the current is  Incremental Snapshot Reading or binlog reading,FLINK-34863,13572642,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

I think flink cdc needs to report to the outside whether it is an incremental snapshot read. I found that after I started the task, even in the incremental snapshot read phase, the indicators

The value of `currentFetchEventTimeLag` is always 0, which will obviously cause a certain degree of misunderstanding
![image|https://github.com/ververica/flink-cdc-connectors/assets/40817998/3ba502cf-3691-432e-ba85-5f14ab638eb8]


### Solution

This information should be reported



### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/3001
Created by: [huyuanfeng2018|https://github.com/huyuanfeng2018]
Labels: enhancement, 
Created at: Mon Jan 15 17:39:16 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:33:58.0,,,,,,,,,,"0|z1o3m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] oracle-cdc ORA-01371: Complete LogMiner dictionary not found,FLINK-34862,13572641,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

flink-1.15.2

### Flink CDC version

flink-connector-oracle-cdc-2.4.2

### Database and its version

oracle 12c cdb

### Minimal reproduce step

**Translation: There is no problem running the following code segment individually：**
        Properties prop = new Properties();
        prop.setProperty(""log.mining.strategy"", ""redo_log_catalog"");
        prop.setProperty(""log.mining.continuous.mine"", ""true"");
        prop.setProperty(""decimal.handling.mode"", ""string"");
        prop.setProperty(""interval.handling.mode"", ""string"");
        prop.setProperty(""database.tablename.case.insensitive"", ""false"");
        prop.setProperty(""snapshot.locking.mode"", ""none"");
        prop.setProperty(""lob.enabled"", ""true"");
        prop.setProperty(""database.history.store.only.captured.tables.ddl"", ""true"");
        prop.setProperty(""database.dbname"", ""ORCL"");
        prop.setProperty(""database.pdb.name"", ""ORCLPDB"");

        SourceFunction<String> sourceFunction = OracleSource.<String>builder()
                .url(""jdbc:oracle:thin:@//192.168.xxx.xxx:1521/ORCL"")
                .hostname(""192.168.xxx.xxx"")
                .port(1521)
                .database(""ORCL"")
                .schemaList(""C##DBZUSER"")
                .tableList(""C##DBZUSER.TEST_TABLE_02"")
                .username(""c##dbzuser"")
                .password(""dbz"")
                .startupOptions(StartupOptions.initial())
                .debeziumProperties(prop)
                .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String
                .build();


**Run the following code simultaneously, an error is reported：**
        Properties prop = new Properties();
        prop.setProperty(""log.mining.strategy"", ""redo_log_catalog"");
        prop.setProperty(""log.mining.continuous.mine"", ""true"");
        prop.setProperty(""decimal.handling.mode"", ""string"");
        prop.setProperty(""interval.handling.mode"", ""string"");
        prop.setProperty(""database.tablename.case.insensitive"", ""false"");
        prop.setProperty(""snapshot.locking.mode"", ""none"");
        prop.setProperty(""lob.enabled"", ""true"");
        prop.setProperty(""database.history.store.only.captured.tables.ddl"", ""true"");
        prop.setProperty(""database.dbname"", ""ORCL"");
        prop.setProperty(""database.pdb.name"", ""ORCLPDB"");

        SourceFunction<String> sourceFunction = OracleSource.<String>builder()
                .url(""jdbc:oracle:thin:@//192.168.xxx.xxx:1521/ORCL"")
                .hostname(""192.168.xxx.xxx"")
                .port(1521)
                .database(""ORCL"")
                .schemaList(""C##DBZUSER"")
                .tableList(""C##DBZUSER.TEST_TABLE_01"")
                .username(""c##dbzuser"")
                .password(""dbz"")
                .startupOptions(StartupOptions.initial())
                .debeziumProperties(prop)
                .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String
                .build(];

error info：
java.sql.SQLException: ORA-01371: Complete LogMiner dictionary not found
ORA-06512: at ""SYS.DBMS_LOGMNR"", line 58
ORA-06512: at line 1



### What did you expect to see?

able to run multiple Oracle CDC instances simultaneously.

### What did you see instead?


Error occurred when running multiple oracle-cdc instances.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2998
Created by: [Toroidals|https://github.com/Toroidals]
Labels: bug, 
Created at: Mon Jan 15 09:56:17 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:33:54.0,,,,,,,,,,"0|z1o3ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Inconsistent Kafka shading among cdc connectors,FLINK-34861,13572640,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.14.3

### Flink CDC version

3.0.0

### Database and its version

MySQL 5.7
Mongo 4.4

### Minimal reproduce step

Put flink-sql-connector-tidb-cdc-3.0.0.jar, flink-sql-connector-mysql-cdc-3.0.0.jar and flink-sql-connector-mongodb-cdc-3.0.0.jar into lib folder at Flink home (Flink 1.14 in my case], then create a MySQL CDC table and submit a simple query like select * from mysql_cdc_table, there would be an error:

`java.lang.NoClassDefFoundError: org/apache/kafka/connect/source/SourceRecord`

It's caused by inconsistent shading of  Kafka classes. Only MySQL CDC connector relocated Kafka classes, so if the classpath contains un-relocated classes from other CDC connectors the above error occurs.

### What did you expect to see?

The cdc connectors would not have class conflicts.

### What did you see instead?

The cdc connectors have class conflicts.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2984
Created by: [link3280|https://github.com/link3280]
Labels: bug, 
Created at: Wed Jan 10 11:27:16 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:33:46.0,,,,,,,,,,"0|z1o3lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Jackson version conflicts among MySQL/TiDB/MongoDB connectors,FLINK-34860,13572639,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.14

### Flink CDC version

3.0.0

### Database and its version

MySQL 5.7

### Minimal reproduce step

Put `flink-sql-connector-tidb-cdc-3.0.0.jar`, `flink-sql-connector-mysql-cdc-3.0.0.jar` and `flink-sql-connector-mongodb-cdc-3.0.0.jar` into lib folder at Flink home (Flink 1.14 in my case], then create a MySQL CDC table and submit a simple query like `select * from mysql_cdc_table`, there would be an error:

`java.io.InvalidClassException: com.ververica.cdc.connectors.shaded.com.fasterxml.jackson.databind.cfg.MapperConfig; incompatible types for field _mapperFeatures`

It's caused by inconsistent jackson versions of the CDC connectors:

- `flink-sql-connector-mongodb-cdc-3.0.0.jar`: 2.10.2
- `flink-sql-connector-mysql-cdc-3.0.0.jar`: 2.13.2
- `flink-sql-connector-tidb-cdc-3.0.0.jar`: 2.12.3

### What did you expect to see?

All CDC connectors use the same jackson version.

### What did you see instead?

Inconsistent jackson versions among the CDC connectors which conflicts with each other.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2983
Created by: [link3280|https://github.com/link3280]
Labels: bug, 
Assignee: [link3280|https://github.com/link3280]
Created at: Wed Jan 10 10:28:58 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:33:42 UTC 2024,,,,,,,,,,"0|z1o3lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:33;flink-cdc-import;Date: Wed Jan 10 10:34:46 CST 2024, Author: [link3280|https://github.com/link3280]

I'm planning to add a `dependencyManagement` in the root pom.xml to pin the common dependencies version (especially the ones in debezium modules) . WDYT? @leonardBang ;;;","20/Mar/24 09:33;flink-cdc-import;Date: Wed Jan 10 11:31:39 CST 2024, Author: [leonardBang|https://github.com/leonardBang]

+1 from my side, thanks @link3280 for raising up this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Oracle cdc in table api does no support server-time-zone option,FLINK-34859,13572638,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.17.1

### Flink CDC version

3.0.0

### Database and its version

Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit Production  With the Partitioning, OLAP, Data Mining and Real Application Testing options

### Minimal reproduce step

## Create a cdc source in table api with `server-time-zone` option specified.

```java
        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
        env.setParallelism(1);

        Schema schema = Schema.newBuilder()
                .column(""NAME"", DataTypes.STRING())
                .column(""ADDR"", DataTypes.STRING())
                .build();

        String factoryIdentifier = new OracleTableSourceFactory().factoryIdentifier();
        TableDescriptor tableDescriptor = TableDescriptor.forConnector(factoryIdentifier)
                .schema(schema)
                // .format(DebeziumJsonFormatFactory.IDENTIFIER)
                .option(OracleSourceOptions.HOSTNAME, ""my-oracle-host"")
                .option(OracleSourceOptions.PORT, 1521)
                .option(OracleSourceOptions.USERNAME, ""my-oracle-username"")
                .option(OracleSourceOptions.PASSWORD, ""my-oracle-password"")
                .option(OracleSourceOptions.DATABASE_NAME, ""my-oracle-database"")
                .option(OracleSourceOptions.SCHEMA_NAME, ""my-oracle-schema"")
                .option(OracleSourceOptions.TABLE_NAME, ""TEST"")
                .option(OracleSourceOptions.SCAN_STARTUP_MODE, ""initial"")
                .option(OracleSourceOptions.SCAN_INCREMENTAL_SNAPSHOT_ENABLED, false)
                .option(OracleSourceOptions.SCAN_SNAPSHOT_FETCH_SIZE, 10)
                .option(OracleSourceOptions.SERVER_TIME_ZONE, ""Asia/Shanghai"")
                .option(""debezium.include.schema.changes"", ""false"")
                .option(""debezium.database.history.store.only.captured.tables.ddl"", ""true"")
                .build();

        StreamTableEnvironmentImpl tEnv = (StreamTableEnvironmentImpl) StreamTableEnvironmentImpl.create(env, EnvironmentSettings.newInstance().inStreamingMode().build());
        Table table = tEnv.from(tableDescriptor);
        tEnv.toChangelogStream(table).print();
        env.execute();
```
## Exceptions are:
```text
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Unable to create a source for reading table '*anonymous_oracle-cdc$1*'.

Table options are:

'connector'='oracle-cdc'
'database-name'='my-oracle-database'
'debezium.database.history.store.only.captured.tables.ddl'='true'
'debezium.include.schema.changes'='false'
'hostname'='my-oracle-host'
'password'='******'
'port'='1521'
'scan.incremental.snapshot.enabled'='false'
'scan.snapshot.fetch.size'='10'
'scan.startup.mode'='initial'
'schema-name'='my-oracle-schema'
'server-time-zone'='Asia/Shanghai'
'table-name'='TEST'
'username'='my-oracle-username'
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:167)
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:192)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175)
	at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115)
	at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:357)
	at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:158)
	at org.apache.flink.table.operations.SourceQueryOperation.accept(SourceQueryOperation.java:86)
	at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:155)
	at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:135)
	at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:92)
	at org.apache.flink.table.operations.SourceQueryOperation.accept(SourceQueryOperation.java:86)
	at org.apache.flink.table.planner.calcite.FlinkRelBuilder.queryOperation(FlinkRelBuilder.java:261)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:289)
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:194)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:194)
	at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
	at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
	at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toChangelogStream(StreamTableEnvironmentImpl.java:263)
	at org.codebase.flink.cdc.FlinkOracleCdcTest.main(FlinkOracleCdcTest.java:70)
Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'oracle-cdc'.

Unsupported options:

server-time-zone

Supported options:

chunk-key.even-distribution.factor.lower-bound
chunk-key.even-distribution.factor.upper-bound
chunk-meta.group.size
connect.max-retries
connect.timeout
connection.pool.size
connector
database-name
debezium.database.history.store.only.captured.tables.ddl
debezium.include.schema.changes
hostname
password
port
property-version
scan.incremental.close-idle-reader.enabled
scan.incremental.snapshot.backfill.skip
scan.incremental.snapshot.chunk.key-column
scan.incremental.snapshot.chunk.size
scan.incremental.snapshot.enabled
scan.snapshot.fetch.size
scan.startup.mode
schema-name
split-key.even-distribution.factor.lower-bound
split-key.even-distribution.factor.upper-bound
table-name
url
username
	at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:632)
	at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:931)
	at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validateExcept(FactoryUtil.java:955)
	at com.ververica.cdc.connectors.oracle.table.OracleTableSourceFactory.createDynamicTableSource(OracleTableSourceFactory.java:70)
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:164]
	... 28 more
```

### What did you expect to see?

The program should run and print data

### What did you see instead?

Exception

### Anything else?

![image|https://github.com/ververica/flink-cdc-connectors/assets/23203149/b3bd94c8-6388-4c4d-b614-0084dd262a5c]
![image|https://github.com/ververica/flink-cdc-connectors/assets/23203149/df2dc0b9-c45e-450a-ab41-c6998e7d49ef]

As we can see in the screenshots, `OracleTableSourceFactory` does not support `server-time-zone` option as well as `MySqlTableSourceFactory` support this option. By the way, we can specify this option in the `OracleSourceBuilder` class.


### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2977
Created by: [LiuBodong|https://github.com/LiuBodong]
Labels: bug, 
Created at: Tue Jan 09 09:24:10 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:33:36.0,,,,,,,,,,"0|z1o3l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support modifying partition keys and primary keys of Schema.,FLINK-34858,13572637,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Many sinks of datalakes and OLAP databases use partitions to accelerate queries, but this information is not included in the current [Schema|https://github.com/ververica/flink-cdc-connectors/blob/15a734efa09343228dc7e69e2074a16bfec40f48/flink-cdc-common/src/main/java/com/ververica/cdc/common/schema/Schema.java#L45].
We can add this API first and then consider the timing of use.


### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2960
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Thu Jan 04 09:43:15 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:33:32.0,,,,,,,,,,"0|z1o3kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Can't restore from checkpoint because assignedSplits does not equal to splitFinishedOffsets,FLINK-34857,13572636,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.14.5

### Flink CDC version

2.4.0

### Database and its version

MySQL

### Minimal reproduce step

Can't restore from the checkpoint(please unzip the [ck.tar.gz|https://github.com/ververica/flink-cdc-connectors/files/13818472/ck.tar.gz) and restore from the _metadata)

can use the following ut code to debug
```
@Test
    public void testRestoreFaild() throws IOException {
        String checkpoint = ""/Path/to/_metadata"";
        Path chkPath = new Path(checkpoint);
        InputStream in = chkPath.getFileSystem().open(chkPath);
        DataInputStream inputStream = new DataInputStream(in);
        CheckpointMetadata metadata =
                Checkpoints.loadCheckpointMetadata(inputStream, Thread.currentThread().getContextClassLoader(), checkpoint);
        HybridSourceEnumeratorStateSerializer serializer = new HybridSourceEnumeratorStateSerializer();
        PendingSplitsStateSerializer splitsStateSerializer = new PendingSplitsStateSerializer(MySqlSplitSerializer.INSTANCE);
        for (OperatorState operatorState : metadata.getOperatorStates()) {
            ByteStreamStateHandle coordinatorState = operatorState.getCoordinatorState();
            if (coordinatorState != null && ""6dc0226b15c44c9c2e1f9ea1a65fd400"".equals(operatorState.getOperatorID().toHexString())) {

                HybridSourceEnumeratorState hybridSourceEnumeratorState =
                        serializer.deserialize(0, coordinatorState.getData());
                System.out.println(hybridSourceEnumeratorState);
                HybridPendingSplitsState pendingSplitsState =
                        (HybridPendingSplitsState) splitsStateSerializer.deserialize(
                                hybridSourceEnumeratorState.getWrappedStateSerializerVersion(),
                                hybridSourceEnumeratorState.getWrappedState());
                System.out.println(""================================================================================"");
                Set<String> splits = new HashSet<>();
                for (MySqlSchemalessSnapshotSplit split : pendingSplitsState.getSnapshotPendingSplits().getAssignedSplits().values()) {
                    if (!pendingSplitsState.getSnapshotPendingSplits().getSplitFinishedOffsets().containsKey(split.splitId())) {
                        System.out.println(split);
                    }
                    splits.add(split.getTableId().identifier());
                }
                System.out.println(""================================================================================"");
                Set<String> tables = new HashSet<>();
                for (TableId id : pendingSplitsState.getSnapshotPendingSplits().getAlreadyProcessedTables()) {
                    tables.add(id.identifier());
                }

                System.out.println(""================================"");
                System.out.println(splits);
                System.out.println(""================================"");
                System.out.println(tables);
                System.out.println(""================================"");
//                System.out.println(pendingSplitsState);

            }
        }
    }
```

### What did you expect to see?

Can restore from checkpoint

### What did you see instead?

The exception is below
```
java.lang.NullPointerException: null
        at com.ververica.cdc.connectors.mysql.source.assigners.MySqlHybridSplitAssigner.createBinlogSplit(MySqlHybridSplitAssigner.java:208] ~[flink-sql-connector-mysql-cdc-2.4.0.jar:2.4.0]
        at com.ververica.cdc.connectors.mysql.source.assigners.MySqlHybridSplitAssigner.getNext(MySqlHybridSplitAssigner.java:112) ~[flink-sql-connector-mysql-cdc-2.4.0.jar:2.4.0]
        at com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator.assignSplits(MySqlSourceEnumerator.java:199) ~[flink-sql-connector-mysql-cdc-2.4.0.jar:2.4.0]
        at com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator.handleSplitRequest(MySqlSourceEnumerator.java:104) ~[flink-sql-connector-mysql-cdc-2.4.0.jar:2.4.0]
        at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$handleEventFromOperator$1(SourceCoordinator.java:157) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
```

### Anything else?

_No response_

### Are you willing to submit a PR?

- [x] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2958
Created by: [klion26|https://github.com/klion26]
Labels: bug, 
Created at: Wed Jan 03 19:40:17 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:33:28 UTC 2024,,,,,,,,,,"0|z1o3ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:33;flink-cdc-import;Date: Wed Jan 03 19:42:48 CST 2024, Author: [klion26|https://github.com/klion26]

Please see more information in the [analysis doc|https://docs.qq.com/doc/DQU5SemJCUXpmT3FN]

PS  Can't restore the checkpoint with FlinkCDC-2.4.2 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] timeout when Invocating Remote RPC.,FLINK-34856,13572635,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.18.0

### Flink CDC version

3.0

### Database and its version

Mysql 8.0

### Minimal reproduce step

```
source:
  type: mysql
  hostname: localhost
  port: 3306
  username: root
  password: 123456
  tables: inventory.\.*
  server-id: 5400-5404
  server-time-zone: Asia/Shanghai

sink:
  type: values

pipeline:
  name: Sync MySQL Database to Values
  parallelism: 2
```

### What did you expect to see?

running correctly.

### What did you see instead?

```
2024-01-03 11:20:14
java.lang.IllegalStateException: Failed to send request to coordinator: com.ververica.cdc.runtime.operators.schema.event.SchemaChangeRequest@f75b9e4
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.sendRequestToCoordinator(SchemaOperator.java:126)
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.requestSchemaChange(SchemaOperator.java:110)
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.handleSchemaChangeEvent(SchemaOperator.java:95)
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.processElement(SchemaOperator.java:85)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:309)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlPipelineRecordEmitter.sendCreateTableEvent(MySqlPipelineRecordEmitter.java:125)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlPipelineRecordEmitter.processElement(MySqlPipelineRecordEmitter.java:109)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitRecord(MySqlRecordEmitter.java:82)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitRecord(MySqlRecordEmitter.java:55)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:160)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:419)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(JobMasterOperatorEventGateway.sendRequestToCoordinator(OperatorID, SerializedValue)]] at recipient [pekko.tcp://flink@localhost:6123/user/rpc/jobmanager_11] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2005)
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.sendRequestToCoordinator(SchemaOperator.java:123)
	... 26 more
```

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2956
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: bug, 
Created at: Wed Jan 03 11:24:39 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:33:24 UTC 2024,,,,,,,,,,"0|z1o3kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:33;flink-cdc-import;Date: Thu Mar 14 10:46:47 CST 2024, Author: [Memory5210|https://github.com/Memory5210]

How to solve it？;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Improve] When using miniCluster, don't check 'FLINK_HOME'. ",FLINK-34855,13572634,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

When Flink CDC uses a mini-cluster, pipelines don't need to load Flink JARs, but instead use the local environment. So, when using mini cluster, we don't need to check 'FLINK_HOME'.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2955
Created by: [joyCurry30|https://github.com/joyCurry30]
Labels: enhancement, 
Created at: Tue Jan 02 23:01:19 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:33:20 UTC 2024,,,,,,,,,,"0|z1o3k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:33;flink-cdc-import;Date: Tue Jan 02 23:02:33 CST 2024, Author: [joyCurry30|https://github.com/joyCurry30]

PR as ([#2813|https://github.com/apache/flink-cdc/issues/2813] | [FLINK-2813|https://issues.apache.org/jira/browse/FLINK-2813]);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature] Optimizing the last chunk with a large amount of data to avoid memory overflow.,FLINK-34854,13572633,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

In certain difficulties, the last shard requires a large amount of data to be processed, resulting in memory overflow.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2941
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Thu Dec 28 17:50:52 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:33:14.0,,,,,,,,,,"0|z1o3k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature] submit Flink CDC pipeline job to k8s cluster.,FLINK-34853,13572632,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,17/May/24 03:23,04/Jun/24 20:40,,,,,,,,cdc-3.2.0,,,,Flink CDC,,,,,0,github-import,pull-request-available,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Currently, there is no a clear description to tell how to run a pipeline on k8s.
So If there is no need to modify the code, please add some docs to guide users how to submit job to a Kubernetes cluster.
If it's necessary to modify the code, you can submit a pr and add doc to fix it. 

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2934
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Wed Dec 27 11:01:17 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:33:10 UTC 2024,,,,,,,,,,"0|z1o3js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:33;flink-cdc-import;Date: Sat Dec 30 15:00:25 CST 2023, Author: [czy006|https://github.com/czy006]

I think it can be regarded as an ordinary Flink application and can be submitted to Flink Operator or Flink Cluster. I am willing to take on this work. Can you assign it to me? @lvyanquan ;;;","20/Mar/24 09:33;flink-cdc-import;Date: Sun Dec 31 22:53:24 CST 2023, Author: [lvyanquan|https://github.com/lvyanquan]

This is a comment requirement from many users, looking forward to your code 😊 @czy006 ;;;","20/Mar/24 09:33;flink-cdc-import;Date: Fri Jan 19 14:34:28 CST 2024, Author: [czy006|https://github.com/czy006]

After discussion, we produced a version of relevant design documents : https://docs.google.com/document/d/1HlvRS5aVNqPtOx14mynEL3RFaZORuTFkWZAjvl4k998/edit?usp=sharing ;;;","20/Mar/24 09:33;flink-cdc-import;Date: Mon Jan 22 10:47:00 CST 2024, Author: [yeezychao|https://github.com/yeezychao]

I don’t see any discussion on yarn deployment in the document. Are you planning to support yarn mode? @czy006;;;","20/Mar/24 09:33;flink-cdc-import;Date: Fri Feb 02 11:25:07 CST 2024, Author: [liuxinjie5211990|https://github.com/liuxinjie5211990]

> After discussion, we produced a version of relevant design documents : https://docs.google.com/document/d/1HlvRS5aVNqPtOx14mynEL3RFaZORuTFkWZAjvl4k998/edit?usp=sharing

hello,i have the same problem,is there any plans to support yarn mode?All of our flink jobs are running on yarn flink session.
@czy006 ;;;","20/Mar/24 09:33;flink-cdc-import;Date: Tue Feb 20 23:15:34 CST 2024, Author: [edhe1988|https://github.com/edhe1988]

> After discussion, we produced a version of relevant design documents : https://docs.google.com/document/d/1HlvRS5aVNqPtOx14mynEL3RFaZORuTFkWZAjvl4k998/edit?usp=sharing

Flink on YARN is the main deployment mode, why is there no discussion about the YARN deployment mode? @czy006;;;","20/Mar/24 09:33;flink-cdc-import;Date: Fri Mar 01 15:57:18 CST 2024, Author: [herunkang2018|https://github.com/herunkang2018]

@czy006 @lvyanquan Compared with submitting using flink-cdc.sh, I think reuse the flink client to submit flink cdc job is more reasonable and easier to maintain, while flink client already supports all kinds of submitting way, including k8s session/application mode and yarn session/application mode, and widely used in production environments.;;;","20/Mar/24 09:33;flink-cdc-import;Date: Thu Mar 07 19:12:33 CST 2024, Author: [czy006|https://github.com/czy006]

> > After discussion, we produced a version of relevant design documents : https://docs.google.com/document/d/1HlvRS5aVNqPtOx14mynEL3RFaZORuTFkWZAjvl4k998/edit?usp=sharing
> 
> Flink on YARN is the main deployment mode, why is there no discussion about the YARN deployment mode? @czy006

YARN will be implemented at a later date and is not discussed in this thread.;;;","20/Mar/24 09:33;flink-cdc-import;Date: Thu Mar 07 19:16:46 CST 2024, Author: [czy006|https://github.com/czy006]

> @czy006 @lvyanquan Compared with submitting using flink-cdc.sh, I think reuse the flink client to submit flink cdc job is more reasonable and easier to maintain, while flink client already supports all kinds of submitting way, including k8s session/application mode and yarn session/application mode, and widely used in production environments.


From another perspective, when we use Flink CDC **as an application**, we need to support deployment target in the script, which essentially uses flink-client to submit jobs;;;",,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Pipeline] Flink-CDC-Pipeline support database mapper and table add unified prefix or suffix,FLINK-34852,13572631,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:33,20/Mar/24 09:33,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

  When synchronize the entire MySQL database or multiple databases to Doris, In the current configuration, if I want to implement library name mapping, I need to specify all tables in the route parameter, which is quite cumbersome. So i want to specify a unified database name mapping and add a unified prefix or suffix to the table name.

### Solution

Add the mapper parameter and prioritize using the Route rule for replacement. If a table does not specify a Route rule, use the Mapper rule for replacement
```yaml
source:
  type: mysql
  hostname: localhost
  port: 3306
  username: root
  password: 123456
  tables: app_db.\.*
  server-id: 5400-5404
  server-time-zone: UTC

sink:
  type: doris
  fenodes: 127.0.0.1:8030
  username: root
  password: """"
  table.create.properties.light_schema_change: true
  table.create.properties.replication_num: 1
  
route:
  - source-table: app_db.orders
    sink-table: ods_db.ods_orders
  - source-table: app_db.shipments
    sink-table: ods_db.ods_shipments
  - source-table: app_db.products
    sink-table: ods_db.ods_products
  - source-database: app_db
    sink-database: ods_db
    table-prefix: ods_mapper_
    table-suffix: _di

pipeline:
  name: Sync MySQL Database to Doris
  parallelism: 2
```

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2933
Created by: [icchux|https://github.com/icchux]
Labels: enhancement, 
Created at: Wed Dec 27 10:37:37 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:33:02 UTC 2024,,,,,,,,,,"0|z1o3jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:33;flink-cdc-import;Date: Mon Jan 08 11:12:03 CST 2024, Author: [tercel53|https://github.com/tercel53]

I also have the same requirement, and I really appreciate this feature.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Pipeline] Flink CDC pipeline supports transform,FLINK-34851,13572630,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

```
source:
  type: mysql
  name: source-database
  host: localhost
  port: 3306
  username: admin
  password: pass
  tables: adb.*, bdb.user_table_[0-9]+, [app|web]_order_.*
  chunk-column: app_order_.*:id,web_order:product_id
  capture-new-tables: true

sink:
  type: kafka
  name: sink-queue
  bootstrap-servers: localhost:9092
  auto-create-table: true

route:
  - source-table: mydb.default.app_order_.*
    sink-table: odsdb.default.app_order
    description: sync all sharding tables to one
  - source-table: mydb.default.web_order
    sink-table: odsdb.default.ods_web_order
    description: sync table to with given prefix ods_

transform:
  - source-table: mydb.app_order_.*
    projection: id, order_id, TO_UPPER(product_name)
    filter: id > 10 AND order_id > 100
    description: project fields from source table
  - source-table: mydb.web_order_.*
    projection: CONCAT(id, order_id) as uniq_id, *
    filter: uniq_id > 10
    description: add new uniq_id for each row

pipeline:
  name: source-database-sync-pipe
  parallelism: 4
  enable-schema-evolution: false
```


### Solution

I am currently working on the implementation of this feature, and the relevant content will be added gradually in the following text.

### Alternatives

None.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2932
Created by: [aiwenmo|https://github.com/aiwenmo]
Labels: enhancement, 
Created at: Wed Dec 27 00:22:36 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:32:58 UTC 2024,,,,,,,,,,"0|z1o3jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:32;flink-cdc-import;Date: Thu Dec 28 18:13:46 CST 2023, Author: [aiwenmo|https://github.com/aiwenmo]

```
transform:
  - source-table: mydb.app_order_.*
    projection: id, order_id, TO_UPPER(product_name)
    filter: id > 10 AND order_id > 100
    description: project fields from source table
  - source-table: mydb.web_order_.*
    projection: CONCAT(id, order_id) as uniq_id, *
    filter: uniq_id > 10
    description: add new uniq_id for each row
```;;;","20/Mar/24 09:32;flink-cdc-import;Date: Thu Dec 28 18:15:44 CST 2023, Author: [aiwenmo|https://github.com/aiwenmo]

- [X] ([#2937|https://github.com/apache/flink-cdc/issues/2937] | [FLINK-2937|https://issues.apache.org/jira/browse/FLINK-2937]);;;","20/Mar/24 09:32;flink-cdc-import;Date: Mon Feb 26 23:42:17 CST 2024, Author: [aiwenmo|https://github.com/aiwenmo]

> Completed features:

1. Support projection.
2. Support filter.
3. Support automatic evolution of projection's column.
4. Support SQL operators, functions, constants, and metadata columns.
5. Support restoring jobs from save points.

> Todo:

- [ ] ([#3081|https://github.com/apache/flink-cdc/issues/3081] | [FLINK-3081|https://issues.apache.org/jira/browse/FLINK-3081]) 
- [ ] ([#3077|https://github.com/apache/flink-cdc/issues/3077] | [FLINK-3077|https://issues.apache.org/jira/browse/FLINK-3077]) 
- [ ] ([#3078|https://github.com/apache/flink-cdc/issues/3078] | [FLINK-3078|https://issues.apache.org/jira/browse/FLINK-3078]) 
- [ ] ([#3079|https://github.com/apache/flink-cdc/issues/3079] | [FLINK-3079|https://issues.apache.org/jira/browse/FLINK-3079]) 
- [ ] ([#3080|https://github.com/apache/flink-cdc/issues/3080] | [FLINK-3080|https://issues.apache.org/jira/browse/FLINK-3080]) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[test] Support to execute the (mysql) test cases with custom database deployment,FLINK-34850,13572629,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Our IT workflow are  built on GitHub now, but in some cases I need to execute some of test cases locally with local deployment but not docker container. I think it will be better to add some configuration to support locally execution of test cases so that we can reuse existing test cases easily.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2922
Created by: [whhe|https://github.com/whhe]
Labels: enhancement, 
Created at: Mon Dec 25 14:55:38 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:32:51.0,,,,,,,,,,"0|z1o3j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC3.0 Demo Mysql binlog to Starrocks Exception amount of AddColumnEvent is already existed[Bug] ,FLINK-34849,13572628,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.18

### Flink CDC version

3.0

### Database and its version

Starrocks  3.1.6

### Minimal reproduce step

I Can run the Doris Demo (mysq to doris)，But run Starrocks Demo Exception; For the Step:
1.Mysql Sync  to Starrocks database and table pass.
2. INSERT INTO app_db.orders (id, price) VALUES (3, 100.00); pass.
3. ALTER TABLE app_db.orders ADD amount varchar(100) NULL;  Flink CDC Exception :2023-12-21 11:40:16
java.lang.IllegalStateException: Failed to send request to coordinator: com.ververica.cdc.runtime.operators.schema.event.SchemaChangeRequest@ecbe495a .......Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: amount of AddColumnEvent is already existed.
But Starrocks table: order.amount is added; 


### What did you expect to see?

2023-12-21 11:40:16
java.lang.IllegalStateException: Failed to send request to coordinator: com.ververica.cdc.runtime.operators.schema.event.SchemaChangeRequest@ecbe495a
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.sendRequestToCoordinator(SchemaOperator.java:126)
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.requestSchemaChange(SchemaOperator.java:110)
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.handleSchemaChangeEvent(SchemaOperator.java:95)
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.processElement(SchemaOperator.java:85)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:309)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter$OutputCollector.collect(MySqlRecordEmitter.java:150)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at com.ververica.cdc.debezium.event.DebeziumEventDeserializationSchema.deserialize(DebeziumEventDeserializationSchema.java:92)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitElement(MySqlRecordEmitter.java:128)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.processElement(MySqlRecordEmitter.java:105)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlPipelineRecordEmitter.processElement(MySqlPipelineRecordEmitter.java:119)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitRecord(MySqlRecordEmitter.java:82)
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitRecord(MySqlRecordEmitter.java:55)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:160)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:419)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: amount of AddColumnEvent is already existed
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.sendRequestToCoordinator(SchemaOperator.java:123)
	... 30 more
Caused by: java.lang.IllegalArgumentException: amount of AddColumnEvent is already existed
	at com.ververica.cdc.common.utils.SchemaUtils.applyAddColumnEvent(SchemaUtils.java:73)
	at com.ververica.cdc.common.utils.SchemaUtils.applySchemaChangeEvent(SchemaUtils.java:53)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaManager.applySchemaChange(SchemaManager.java:113)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistryRequestHandler.handleSchemaChangeRequest(SchemaRegistryRequestHandler.java:102)
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistry.handleCoordinationRequest(SchemaRegistry.java:157)
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverCoordinationRequestToCoordinator(DefaultOperatorCoordinatorHandler.java:143)
	at org.apache.flink.runtime.scheduler.SchedulerBase.deliverCoordinationRequestToCoordinator(SchedulerBase.java:1070)
	at org.apache.flink.runtime.jobmaster.JobMaster.sendRequestToCoordinator(JobMaster.java:616)
	at sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175]


### What did you see instead?

Fix it

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2910
Created by: [biyinggun|https://github.com/biyinggun]
Labels: bug, 
Created at: Thu Dec 21 16:49:06 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:32:46 UTC 2024,,,,,,,,,,"0|z1o3iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:32;flink-cdc-import;Date: Fri Dec 22 09:18:49 CST 2023, Author: [lvyanquan|https://github.com/lvyanquan]

Thank you for discovering this issue. I occasionally encounter it and will try to fix it.;;;","20/Mar/24 09:32;flink-cdc-import;Date: Wed Jan 10 00:17:53 CST 2024, Author: [fanqiejiang8|https://github.com/fanqiejiang8]

@lvyanquan hello  这个bug有修复好了吗  在那个分支呢;;;","20/Mar/24 09:32;flink-cdc-import;Date: Tue Jan 16 12:57:37 CST 2024, Author: [fanqiejiang8|https://github.com/fanqiejiang8]

@biyinggun 修改一下这个参数  pekko.ask.timeout: 120s;;;","20/Mar/24 09:32;flink-cdc-import;Date: Mon Jan 29 14:38:49 CST 2024, Author: [Jazzylol|https://github.com/Jazzylol]

> @biyinggun 修改一下这个参数 pekko.ask.timeout: 120s
这个参数的作用是？;;;","20/Mar/24 09:32;flink-cdc-import;Date: Wed Jan 31 14:21:36 CST 2024, Author: [everhopingandwaiting|https://github.com/everhopingandwaiting]

After compiling and using the latest code branch, I discovered that when I attempt to synchronize data starting from a specific binlog file, I still encounter an exception ""AddColumnEvent is already existed,"" which leads to the termination of the task.;;;","20/Mar/24 09:32;flink-cdc-import;Date: Sun Feb 18 17:05:53 CST 2024, Author: [liisaxin|https://github.com/liisaxin]

i discovered the same bug, too.;;;","20/Mar/24 09:32;flink-cdc-import;Date: Mon Feb 19 20:41:47 CST 2024, Author: [herunkang2018|https://github.com/herunkang2018]

Hi @lvyanquan, could you have time to fix this issue? This is quite improtant for users who use schema sync feature.;;;","20/Mar/24 09:32;flink-cdc-import;Date: Fri Feb 23 10:22:01 CST 2024, Author: [liisaxin|https://github.com/liisaxin]

> Hi @lvyanquan, could you have time to fix this issue? This is quite improtant for users who use schema sync feature.

this is very very important . hope fix this bug quickly . @lvyanquan ;;;","20/Mar/24 09:32;flink-cdc-import;Date: Mon Mar 11 14:22:18 CST 2024, Author: [sheltonsuen|https://github.com/sheltonsuen]

After upstream add a new column, my flink cdc task keep trying to recover from the exception, but failed, seems like the same issue we have here. is there any suggestion to solve this? thanks
```java
2024-03-05 19:25:58,688 INFO  com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistryRequestHandler [] - Received schema change event request from table newchiwan_trade.t_freight_source. Start to buffer requests for others.
2024-03-05 19:25:58,688 INFO  com.ververica.cdc.runtime.operators.schema.coordinator.SchemaManager [] - Handling schema change event: AddColumnEvent{tableId=newchiwan_trade.t_freight_source, addedColumns=[ColumnWithPosition{column=`channel` TINYINT, position=AFTER, existingColumn=`oil_fee_type` BIGINT}]}
2024-03-05 19:25:58,740 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Flink CDC Event Source: mysql -> SchemaOperator -> PrePartition (1/1) (4e910aad380ccfd4548820d42d93e88d_cbc357ccb763df2852fee8c4fc7d55f2_0_154823) switched from RUNNING to FAILED on flink-cdc-dp-sr-cluster-taskmanager-1-3 @ 172.16.68.31 (dataPort=40224).
java.lang.IllegalStateException: Failed to send request to coordinator: com.ververica.cdc.runtime.operators.schema.event.SchemaChangeRequest@cfa0af2
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.sendRequestToCoordinator(SchemaOperator.java:126) ~[?:?]
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.requestSchemaChange(SchemaOperator.java:110) ~[?:?]
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.handleSchemaChangeEvent(SchemaOperator.java:95) ~[?:?]
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.processElement(SchemaOperator.java:85) ~[?:?]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:309) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101) ~[flink-dist-1.18.1.jar:1.18.1]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter$OutputCollector.collect(MySqlRecordEmitter.java:150) ~[?:?]
	at java.util.ArrayList.forEach(Unknown Source) ~[?:?]
	at com.ververica.cdc.debezium.event.DebeziumEventDeserializationSchema.deserialize(DebeziumEventDeserializationSchema.java:92) ~[?:?]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitElement(MySqlRecordEmitter.java:128) ~[?:?]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.processElement(MySqlRecordEmitter.java:105) ~[?:?]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlPipelineRecordEmitter.processElement(MySqlPipelineRecordEmitter.java:119) ~[?:?]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitRecord(MySqlRecordEmitter.java:82) ~[?:?]
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlRecordEmitter.emitRecord(MySqlRecordEmitter.java:55) ~[?:?]
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:160) ~[flink-connector-files-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:419) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.18.1.jar:1.18.1]
	at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: channel of AddColumnEvent is already existed
	at java.util.concurrent.CompletableFuture.reportGet(Unknown Source) ~[?:?]
	at java.util.concurrent.CompletableFuture.get(Unknown Source) ~[?:?]
	at com.ververica.cdc.runtime.operators.schema.SchemaOperator.sendRequestToCoordinator(SchemaOperator.java:123) ~[?:?]
	... 30 more
Caused by: java.lang.IllegalArgumentException: channel of AddColumnEvent is already existed
	at com.ververica.cdc.common.utils.SchemaUtils.applyAddColumnEvent(SchemaUtils.java:73) ~[?:?]
	at com.ververica.cdc.common.utils.SchemaUtils.applySchemaChangeEvent(SchemaUtils.java:53) ~[?:?]
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaManager.applySchemaChange(SchemaManager.java:113) ~[?:?]
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistryRequestHandler.handleSchemaChangeRequest(SchemaRegistryRequestHandler.java:102) ~[?:?]
	at com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistry.handleCoordinationRequest(SchemaRegistry.java:157) ~[?:?]
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverCoordinationRequestToCoordinator(DefaultOperatorCoordinatorHandler.java:143) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.runtime.scheduler.SchedulerBase.deliverCoordinationRequestToCoordinator(SchedulerBase.java:1070) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.runtime.jobmaster.JobMaster.sendRequestToCoordinator(JobMaster.java:616) ~[flink-dist-1.18.1.jar:1.18.1]
	at jdk.internal.reflect.GeneratedMethodAccessor63.invoke(Unknown Source) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
	at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309) ~[flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-dist-1.18.1.jar:1.18.1]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307) ~[flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222) ~[flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85) ~[flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168) ~[flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) [flink-rpc-akkae9f6bb37-e207-4e36-999a-f42a36fa8818.jar:1.18.1]
	at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
	at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
	at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
	at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
2024-03-05 19:25:58,742 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Removing registered reader after failure for subtask 0 (([#154823|https://github.com/apache/flink-cdc/issues/154823] | [FLINK-154823|https://issues.apache.org/jira/browse/FLINK-154823])) of source Source: Flink CDC Event Source: mysql.
2024-03-05 19:25:58,742 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - 2 tasks will be restarted to recover the failed task 4e910aad380ccfd4548820d42d93e88d_cbc357ccb763df2852fee8c4fc7d55f2_0_154823.
2024-03-05 19:25:58,742 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Sync newchiwan Databases to StarRocks (41d15c80977ce8df2c3c5609c41e4554) switched from state RUNNING to RESTARTING.
2024-03-05 19:25:58,742 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - PostPartition -> Sink Writer: newchiwan_cdc -> Sink Committer: newchiwan_cdc (1/1) (4e910aad380ccfd4548820d42d93e88d_0deb1b26a3d9eb3c8f0c11f7110b2903_0_154823) switched from RUNNING to CANCELING.
2024-03-05 19:25:58,772 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - PostPartition -> Sink Writer: newchiwan_cdc -> Sink Committer: newchiwan_cdc (1/1) (4e910aad380ccfd4548820d42d93e88d_0deb1b26a3d9eb3c8f0c11f7110b2903_0_154823) switched from CANCELING to CANCELED.
2024-03-05 19:25:58,772 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager [] - Clearing resource requirements of job 41d15c80977ce8df2c3c5609c41e4554
2024-03-05 19:25:58,772 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedTaskManagerTracker [] - Clear all pending allocations for job 41d15c80977ce8df2c3c5609c41e4554.
2024-03-05 19:25:59,509 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 147500 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1709637959509 for job 0124a1157b4545b0e88933f591baa153.
2024-03-05 19:25:59,688 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 147500 for job 0124a1157b4545b0e88933f591baa153 (465658 bytes, checkpointDuration=179 ms, finalizationTime=0 ms).
2024-03-05 19:25:59,688 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 147500 as completed for source Source: Flink CDC Event Source: mysql.
2024-03-05 19:25:59,742 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Sync newchiwan Databases to StarRocks (41d15c80977ce8df2c3c5609c41e4554) switched from state RESTARTING to RUNNING.
2024-03-05 19:25:59,743 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Restoring job 41d15c80977ce8df2c3c5609c41e4554 from Checkpoint 49479 @ 1709637952671 for 41d15c80977ce8df2c3c5609c41e4554 located at file:/opt/storage/checkpoints/41d15c80977ce8df2c3c5609c41e4554/chk-49479.
2024-03-05 19:25:59,743 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No master state to restore
2024-03-05 19:25:59,743 ERROR com.ververica.cdc.runtime.operators.schema.coordinator.SchemaRegistry [] - Subtask 0 reset at checkpoint 49479.

```;;;",,,,,,,,,,,,,,,,,,,,,,,,
refactor: some class in flink-connector-mysql-cdc do not inherit from the base class in flink-cdc-base,FLINK-34848,13572627,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Currently, some classes in `flink-connector-mysql-cdc` do not inherit from the base classes in `flink-cdc-base`, making the code somewhat redundant and not very tidy. such as MySqlSourceEnumerator...

Seizing this opportunity for refactoring, many functionalities implemented in `flink-connector-mysql-cdc` might be exposed in the form of interfaces, allowing other connectors to implement them. 

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2907
Created by: [cobolbaby|https://github.com/cobolbaby]
Labels: enhancement, 
Created at: Thu Dec 21 14:59:30 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:32:36 UTC 2024,,,,,,,,,,"0|z1o3io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:32;flink-cdc-import;Date: Fri Dec 22 14:00:32 CST 2023, Author: [cobolbaby|https://github.com/cobolbaby]

In addition to inheritance issues, the subdirectory division styles of each connector are very different. If I want to add a new connector, which one should I refer to as the template?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MySQL database using the compress function, can provide analytic function",FLINK-34847,13572626,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

MySQL database using the compress function, now use the base64 and udf are not parsed, the community did not see a solution, I hope to solve such problems

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2894
Created by: [zhbdesign|https://github.com/zhbdesign]
Labels: enhancement, 
Created at: Tue Dec 19 23:22:33 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:32:31 UTC 2024,,,,,,,,,,"0|z1o3ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:32;flink-cdc-import;Date: Wed Dec 20 09:22:52 CST 2023, Author: [zhbdesign|https://github.com/zhbdesign]

https://issues.redhat.com/projects/DBZ/issues/DBZ-3471?filter=allopenissues，
debezium也查不到相关资料，也没有任何社区使用者发布相关issue
Debezium cannot find any relevant information, nor have any community users posted any related issues;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug][cdc-connectors][cdc-base]  Transaction log of high_watermark read twice,FLINK-34846,13572625,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"## Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


## Flink version

1.18

## Flink CDC version

3.0

## Database and its version

anyway

## Minimal reproduce step
## Reason
Current, read snapshot split then backfill between  [low_watermark, high_watermark], then read in stream phase between [high_watermark, +∞). The message of high_watermark will be read twice.


### What did you see instead?


### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2885
Created by: [loserwang1024|https://github.com/loserwang1024]
Labels: bug, 
Created at: Mon Dec 18 19:07:44 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:32:27 UTC 2024,,,,,,,,,,"0|z1o3i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:32;flink-cdc-import;Date: Tue Feb 27 18:03:14 CST 2024, Author: [link3280|https://github.com/link3280]

@loserwang1024 Hi, does this issue affect mysql-cdc as well (since it involves cdc-base)?  Thanks!;;;","20/Mar/24 09:32;flink-cdc-import;Date: Wed Feb 28 10:10:09 CST 2024, Author: [loserwang1024|https://github.com/loserwang1024]

@link3280，just a minor optimization. To be honest, this rarely happens because high_watermark is mostly non dml record, such as heartbeat record.;;;","20/Mar/24 09:32;flink-cdc-import;Date: Wed Feb 28 11:44:03 CST 2024, Author: [link3280|https://github.com/link3280]

@loserwang1024 Thanks a lot for your input! The reason why I ask is that I met a data duplication issue with MySQL CDC 3.0.0 when it reads binlogs with the position set to `earliest-offset` or `timestamp`. The data showed up twice exactly. I checked the logs and all the splits were `MySqlBinlogSplit`, so the logs may be read twice. I wonder if it's the same issue.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.1][cdc-composer] Support routing sharding tables with the same primary key into one.,FLINK-34845,13572624,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Now we can support routing sharding tables into one only if they don't have a same value of primary key. We can improve it by add two metadata columns like `database` and `table`, and add they to a composite primary key.

You can separate it into some subtasks if needed.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2882
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Mon Dec 18 11:25:52 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:32:21.0,,,,,,,,,,"0|z1o3i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.1][pipeline-connectors] Add pipeline Sink for Pulsar.,FLINK-34844,13572623,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Add pipeline sink  support for [pulsar|https://github.com/apache/pulsar]

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2879
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Mon Dec 18 10:49:25 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:32:18 UTC 2024,,,,,,,,,,"0|z1o3hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:32;flink-cdc-import;Date: Thu Dec 28 11:48:14 CST 2023, Author: [ToJ112|https://github.com/ToJ112]

Hi, [@lvyanquan|https://github.com/lvyanquan] I am willing to try this task, can you assign this task to me?;;;","20/Mar/24 09:32;flink-cdc-import;Date: Thu Dec 28 14:45:50 CST 2023, Author: [lvyanquan|https://github.com/lvyanquan]

@ToJ112 Hi, I don't have the authority to assign, but I raised a [pr for Kafka|https://github.com/ververica/flink-cdc-connectors/pull/2938) sink(Although there are some details to be determined], You may take it as a reference. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] BinlogSplitReader#pollSplitRecords return finishedSplit when exception occurs,FLINK-34843,13572622,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"## Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


## Flink version

1.18

## Flink CDC version

3.0

## Database and its version

any

## Minimal reproduce step

### Current Code
In current BinlogSplitReader#pollSplitRecords, when the currentTaskRunning = false, will return null, which is seen as fininished split.
See:
```java
//com.ververica.cdc.connectors.mysql.source.reader.MySqlSplitReader#pollSplitRecords
return dataIt == null ? finishedSplit() : forRecords(dataIt);
```

### Problem occurs:
However, the currentTaskRunning = false in four situations:
1. the bounded stream split is finished( later in issue: https://github.com/ververica/flink-cdc-connectors/issues/2867)
2. the stream split is paused for new scanly tables(See MySqlSplitReader#suspendBinlogReaderIfNeed)
3. some exception occurs(See executorService.submit)
4. The BinlogSplitReader#close
Only in the former two situations, the spilt is finished, otherwise problem will occor.

For example, there is an unbounded stream split:
<img width=""1193"" alt=""image"" src=""https://github.com/ververica/flink-cdc-connectors/assets/125648852/ca9dd77c-d111-47c2-afb6-fc13232339a5"">
* t1, add this unbouned stream split and start a new thread to fetch binlog.
* t2, BinlogSplitReader#pollSplitRecords check there is no Exception at first.
* t3,  some excetpion occurs in binlogSplitTask(network, data error, and more), set currentTaskRunning = false.
* t4, BinlogSplitReader#pollSplitRecords check currentTaskRunning is fasle, so return null, which is seen as fininished split. Then MysqlSourceReader move to next split.

Thus, when the task is not running, we also need to distinguish whether the split is finished more carefully. I have two idea:
1. add lock(not a good choice]
2. when the stream split is paused, we also add an END watermark to queue. Only when get an END watermark, BinlogSplitReader#pollSplitRecords return null, otherwise return empty collections.

## What did you expect to see?

 Only when get an END watermark, BinlogSplitReader#pollSplitRecords return null, otherwise return empty collections.

## What did you see instead?

[Bug] BinlogSplitReader#pollSplitRecords return finishedSplit(null) when exception occurs

## Anything else?

_No response_

## Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2878
Created by: [loserwang1024|https://github.com/loserwang1024]
Labels: bug, 
Created at: Mon Dec 18 09:48:18 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:32:13 UTC 2024,,,,,,,,,,"0|z1o3hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:32;flink-cdc-import;Date: Tue Dec 19 21:30:43 CST 2023, Author: [gong|https://github.com/gong]

 @loserwang1024 Can BinlogSplitReader#pollSplitRecords call checkReadException() address this problem before return null?;;;","20/Mar/24 09:32;flink-cdc-import;Date: Tue Dec 19 23:21:54 CST 2023, Author: [loserwang1024|https://github.com/loserwang1024]

> @loserwang1024 Can BinlogSplitReader#pollSplitRecords call checkReadException() address this problem before return null?

@gong , Yes, it can solve this case.But there are also other case, for example,  it still some other problems when add [SNAPSHOT_ONLY ablitity |https://github.com/ververica/flink-cdc-connectors/issues/2867], which will read the binlog until reaching the max high_watermark in stream phase.The problem like this may happens:
<img width=""1245"" alt=""image"" src=""https://github.com/ververica/flink-cdc-connectors/assets/125648852/d77e8454-9c5a-4bdf-83e9-86752e379513"">
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[cdc-connector][db2] Db2 support incremental source,FLINK-34842,13572621,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,20/Mar/24 09:32,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

db2 support incremental source

### Solution

Db2 debezium is inspired by sqlserver debezium, So db2 incremental source will refer to the implementation of SQL Server CDC incremental source. 

 

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2869
Created by: [gong|https://github.com/gong]
Labels: enhancement, 
Created at: Thu Dec 14 01:58:49 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:32:07.0,,,,,,,,,,"0|z1o3hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.1][pipeline-connectors] Add jdbc pipeline sink ,FLINK-34841,13572620,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gongzhongqiang,flink-cdc-import,flink-cdc-import,20/Mar/24 09:32,25/May/24 08:51,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

From my side and I saw in dingding group, Some user want to sync data to relation database by flink cdc.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2866
Created by: [GOODBOY008|https://github.com/GOODBOY008]
Labels: enhancement, 
Created at: Wed Dec 13 15:34:21 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 25 08:51:35 UTC 2024,,,,,,,,,,"0|z1o3h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/24 06:45;gongzhongqiang; [~leonard]   [~renqs] I'm willing to take this. This work will be started after https://issues.apache.org/jira/browse/FLINK-25421 ({*}Port JDBC Sink to new Unified Sink API (FLIP-143){*}) was merged and flink-jdbc-connector released a new version.;;;","25/May/24 08:51;ouyangwuli;[~gongzhongqiang] Looks like a very good job, cdc data entry warehouse is needed in many scenarios.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.1][pipeline-connectors] Add Implementation of DataSink in Iceberg.,FLINK-34840,13572619,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:31,27/May/24 05:41,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,2,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Add pipeline sink Implementation for https://github.com/apache/iceberg.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2863
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Wed Dec 13 14:37:54 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 05:41:55 UTC 2024,,,,,,,,,,"0|z1o3gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:32;flink-cdc-import;Date: Mon Dec 25 20:17:04 CST 2023, Author: [czy006|https://github.com/czy006]

@lvyanquan Hi I want to take this task. Can you assign me to try this challenge?;;;","27/May/24 05:41;pvary;If you need help with Iceberg reviews, feel free to ping me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC 3.1.0 Plan,FLINK-34839,13572618,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:28,20/Mar/24 09:28,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"## Motivation

This is an umbrella issue for Flink CDC 3.1 version

## Solution

**[module] flink-cdc-common**
 * [x] ([#2857|https://github.com/apache/flink-cdc/issues/2857] | [FLINK-2857|https://issues.apache.org/jira/browse/FLINK-2857])
 * [x] ([#2936|https://github.com/apache/flink-cdc/issues/2936] | [FLINK-2936|https://issues.apache.org/jira/browse/FLINK-2936])
 * [x] ([#2943|https://github.com/apache/flink-cdc/issues/2943] | [FLINK-2943|https://issues.apache.org/jira/browse/FLINK-2943])

**[module] flink-cdc-cli**
 * [ ] ([#2934|https://github.com/apache/flink-cdc/issues/2934] | [FLINK-2934|https://issues.apache.org/jira/browse/FLINK-2934])
 * [x] ([#2940|https://github.com/apache/flink-cdc/issues/2940] | [FLINK-2940|https://issues.apache.org/jira/browse/FLINK-2940])

**[module] flink-cdc-composer**
* [ ] ([#2932|https://github.com/apache/flink-cdc/issues/2932] | [FLINK-2932|https://issues.apache.org/jira/browse/FLINK-2932])
* [ ] ([#2882|https://github.com/apache/flink-cdc/issues/2882] | [FLINK-2882|https://issues.apache.org/jira/browse/FLINK-2882])
* [ ] ([#2854|https://github.com/apache/flink-cdc/issues/2854] | [FLINK-2854|https://issues.apache.org/jira/browse/FLINK-2854])

**[module] flink-cdc-connect/flink-cdc-source-connectors**
* [ ] ([#2869|https://github.com/apache/flink-cdc/issues/2869] | [FLINK-2869|https://issues.apache.org/jira/browse/FLINK-2869])
* [x] ([#1747|https://github.com/apache/flink-cdc/issues/1747] | [FLINK-1747|https://issues.apache.org/jira/browse/FLINK-1747])
* [x] ([#2867|https://github.com/apache/flink-cdc/issues/2867] | [FLINK-2867|https://issues.apache.org/jira/browse/FLINK-2867])
* [x] ([#1152|https://github.com/apache/flink-cdc/issues/1152] | [FLINK-1152|https://issues.apache.org/jira/browse/FLINK-1152])
* [ ] ([#2941|https://github.com/apache/flink-cdc/issues/2941] | [FLINK-2941|https://issues.apache.org/jira/browse/FLINK-2941])

**[module] flink-cdc-connect/flink-cdc-pipeline-connectors**
* [ ] ([#2691|https://github.com/apache/flink-cdc/issues/2691] | [FLINK-2691|https://issues.apache.org/jira/browse/FLINK-2691])
* [ ] ([#2856|https://github.com/apache/flink-cdc/issues/2856] | [FLINK-2856|https://issues.apache.org/jira/browse/FLINK-2856])

**[module] flink-cdc-e2e-tests**
* [ ] ([#2859|https://github.com/apache/flink-cdc/issues/2859] | [FLINK-2859|https://issues.apache.org/jira/browse/FLINK-2859])

**[module] docs** 
* [x] ([#2935|https://github.com/apache/flink-cdc/issues/2935] | [FLINK-2935|https://issues.apache.org/jira/browse/FLINK-2935])
* [x] ([#2940|https://github.com/apache/flink-cdc/issues/2940] | [FLINK-2940|https://issues.apache.org/jira/browse/FLINK-2940])

**Bug fix**
* [x] ([#2865|https://github.com/apache/flink-cdc/issues/2865] | [FLINK-2865|https://issues.apache.org/jira/browse/FLINK-2865])
* [ ] ([#2853|https://github.com/apache/flink-cdc/issues/2853] | [FLINK-2853|https://issues.apache.org/jira/browse/FLINK-2853])
* [x] ([#2966|https://github.com/apache/flink-cdc/issues/2966] | [FLINK-2966|https://issues.apache.org/jira/browse/FLINK-2966]) 
* [x] ([#2905|https://github.com/apache/flink-cdc/issues/2905] | [FLINK-2905|https://issues.apache.org/jira/browse/FLINK-2905])

## Design docs
https://docs.google.com/document/d/1sC045l08hqZ8C9bVxeZdH5fY5A4Ljz__f5SqZ2Z8qrM/edit?usp=sharing

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2861
Created by: [leonardBang|https://github.com/leonardBang]
Labels: enhancement, 3.1, 
Milestone: V3.1.0 
Assignee: [leonardBang|https://github.com/leonardBang]
Created at: Wed Dec 13 13:58:30 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:28:36 UTC 2024,,,,,,,,,,"0|z1o3go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:28;flink-cdc-import;Date: Wed Jan 03 14:43:39 CST 2024, Author: [lvyanquan|https://github.com/lvyanquan]

Please add this Bug fix to the plan:
Got NoSuchMethodError when running with Flink 1.17.1 and before. https://github.com/ververica/flink-cdc-connectors/issues/2943;;;","20/Mar/24 09:28;flink-cdc-import;Date: Thu Jan 04 10:00:28 CST 2024, Author: [ChenShuai1981|https://github.com/ChenShuai1981]

please support kubernetes deployment and auto create database;;;","20/Mar/24 09:28;flink-cdc-import;Date: Fri Jan 05 10:32:32 CST 2024, Author: [ChenShuai1981|https://github.com/ChenShuai1981]

please support pipeline with one source and many sinks;;;","20/Mar/24 09:28;flink-cdc-import;Date: Fri Jan 05 10:44:59 CST 2024, Author: [miaogr86|https://github.com/miaogr86]

What are the ways to reduce memory usage during the snapshot phase?
https://github.com/ververica/flink-cdc-connectors/pull/1629
https://github.com/ververica/flink-cdc-connectors/issues/2700;;;","20/Mar/24 09:28;flink-cdc-import;Date: Sat Jan 06 15:59:33 CST 2024, Author: [ChenShuai1981|https://github.com/ChenShuai1981]

Support add customized type mapping like paimon does, for example, tinyint1-not-bool and bigint-unsigned-to-bigint
https://paimon.apache.org/docs/master/cdc-ingestion/mysql-cdc/;;;","20/Mar/24 09:28;flink-cdc-import;Date: Wed Jan 10 20:11:11 CST 2024, Author: [leonardBang|https://github.com/leonardBang]

> Support add customized type mapping like paimon does, for example, tinyint1-not-bool and bigint-unsigned-to-bigint https://paimon.apache.org/docs/master/cdc-ingestion/mysql-cdc/

+1, could you open a ticket for this ?;;;","20/Mar/24 09:28;flink-cdc-import;Date: Thu Jan 11 16:40:33 CST 2024, Author: [xiaolianghu|https://github.com/xiaolianghu]

Look forward to the 3.1 release soon;;;","20/Mar/24 09:28;flink-cdc-import;Date: Thu Jan 25 16:06:48 CST 2024, Author: [klion26|https://github.com/klion26]

hi  @leonardBang could ([#2315|https://github.com/apache/flink-cdc/issues/2315] | [FLINK-2315|https://issues.apache.org/jira/browse/FLINK-2315])  be fixed in 3.1.0 ?

Seems the PR has been filed a long time ago, the author active on it, and it can fix many problems.;;;","20/Mar/24 09:28;flink-cdc-import;Date: Sun Feb 04 11:28:10 CST 2024, Author: [komiblog|https://github.com/komiblog]

Please merge this pr ([#2494|https://github.com/apache/flink-cdc/issues/2494] | [FLINK-2494|https://issues.apache.org/jira/browse/FLINK-2494]) to fix the MariaDB connector bug, thank you very much！;;;","20/Mar/24 09:28;flink-cdc-import;Date: Tue Feb 27 19:06:33 CST 2024, Author: [maver1ck|https://github.com/maver1ck]

Do you have plans to include general JDBC sink ? (not only starrocks ?);;;","20/Mar/24 09:28;flink-cdc-import;Date: Thu Feb 29 10:20:00 CST 2024, Author: [ChenShuai1981|https://github.com/ChenShuai1981]

Support add kafka source with debezium and canal json format;;;","20/Mar/24 09:28;flink-cdc-import;Date: Thu Mar 07 10:17:51 CST 2024, Author: [viethung2281996|https://github.com/viethung2281996]

hi @leonardBang , do you have plan or roadmap for module flink-cdc-pipeline-connectors to support pipeline with MongoDB source !;;;","20/Mar/24 09:28;flink-cdc-import;Date: Thu Mar 07 13:56:44 CST 2024, Author: [leonardBang|https://github.com/leonardBang]

> hi @leonardBang , do you have plan or roadmap for module flink-cdc-pipeline-connectors to support pipeline with MongoDB source !

Yes, it's on the plan but maybe we consider it in next version;;;","20/Mar/24 09:28;flink-cdc-import;Date: Wed Mar 13 14:35:17 CST 2024, Author: [melin|https://github.com/melin]

support kafka pipeline data source connector

Sometimes cdc data needs to be written to a different data source storage and needs to be written to kafka first. It is then forwarded to different databases or data lakes in real time。;;;","20/Mar/24 09:28;flink-cdc-import;Date: Tue Mar 19 16:20:17 CST 2024, Author: [ChenShuai1981|https://github.com/ChenShuai1981]

How to handle different primary keys need between mysql and doris? For example, in mysql table we always use auto-incremental `id` column as primary key and `device_id` as biz unique key, while in doris table, considering query performance by `device_id`, we choose `device_id` + `id` columns as primary key.;;;","20/Mar/24 09:28;flink-cdc-import;Date: Tue Mar 19 17:19:31 CST 2024, Author: [lvyanquan|https://github.com/lvyanquan]

> How to handle different primary keys need between mysql and doris? For example, in mysql table we always use auto-incremental `id` column as primary key and `device_id` as biz unique key, while in doris table, considering query performance by `device_id`, we choose `device_id` + `id` columns as primary key.

@ChenShuai1981 this https://github.com/apache/flink-cdc/pull/2937 pr provide the ability to modify primary key definition.;;;",,,,,,,,,,,,,,,,,
[3.1][cdc-e2e] Add end to end test for pipeline.,FLINK-34838,13572617,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:28,20/Mar/24 09:28,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Add end to end test for Flink CDC 3.0 pipeline.
You may separate it in to several subtasks like doris / starrocks.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2859
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Tue Dec 12 16:42:07 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:28:23 UTC 2024,,,,,,,,,,"0|z1o3gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:28;flink-cdc-import;Date: Wed Dec 13 11:43:00 CST 2023, Author: [joyCurry30|https://github.com/joyCurry30]

Hi, @lvyanquan I am will to do this task, can you assign this to me?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.1][cdc-common] Add metric for DataChangeEvent,FLINK-34837,13572616,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:28,20/Mar/24 09:28,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Motivation

Currently, in Flink CDC 3.0 pipeline, numRecordIn and numRecordOut will contain all events(SchemaChangeEvent/DataChangeEvent/FlushEvent], however, user may not really care the number of FlushEvent and SchemaChangeEvent. So we need to Add metric for DataChangeEvent.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2858
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Assignee: [TyrantLucifer|https://github.com/TyrantLucifer]
Created at: Tue Dec 12 16:29:38 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:28:19 UTC 2024,,,,,,,,,,"0|z1o3g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:28;flink-cdc-import;Date: Wed Dec 20 16:30:00 CST 2023, Author: [TyrantLucifer|https://github.com/TyrantLucifer]

Hi @lvyanquan , Could you please assign it to me?;;;","20/Mar/24 09:28;flink-cdc-import;Date: Fri Dec 22 09:23:21 CST 2023, Author: [lvyanquan|https://github.com/lvyanquan]

I don't have the permission to assign. You can raise a PR and I guess there will no be duplicate work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.1][pipeline-connectors] Add Implementation of DataSink in Paimon.,FLINK-34836,13572615,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:28,20/Mar/24 09:28,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Add DataSink for https://github.com/apache/incubator-paimon. 

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2856
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Assignee: [lvyanquan|https://github.com/lvyanquan]
Created at: Tue Dec 12 16:04:01 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:28:14 UTC 2024,,,,,,,,,,"0|z1o3g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:28;flink-cdc-import;Date: Wed Dec 20 15:05:50 CST 2023, Author: [lvyanquan|https://github.com/lvyanquan]

I am willing to do this.;;;","20/Mar/24 09:28;flink-cdc-import;Date: Wed Jan 17 12:16:51 CST 2024, Author: [Pandas886|https://github.com/Pandas886]

Look forward to this .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[cdc-common] Add default value of pipeline parallelism.,FLINK-34835,13572614,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:28,20/Mar/24 09:28,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Our current 'parallelism' parameter doesn't have a default value. In cases where users do not provide input, it leads to a npe. Additionally, I believe users may not necessarily need to concern pipelines with parallelism. In the absence of a specified value, the default behavior should be to execute tasks with a single concurrency.

### Solution

Add default value for PipelineOptions#PIPELINE_PARALLELISM

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2855
Created by: [joyCurry30|https://github.com/joyCurry30]
Labels: enhancement, 
Created at: Tue Dec 12 15:08:57 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:28:10 UTC 2024,,,,,,,,,,"0|z1o3fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:28;flink-cdc-import;Date: Fri Dec 15 12:00:36 CST 2023, Author: [lvyanquan|https://github.com/lvyanquan]

This may overlap with [parallelism.default|https://nightlies.apache.org/flink/flink-docs-master/zh/docs/deployment/config/] in flink.conf?
And do we need to set different parallelism for source and sink in the future, as [FlinkPipelineComposer|https://github.com/ververica/flink-cdc-connectors/blob/08beca52d0ec15b42dd2f0290161056189cfb733/flink-cdc-composer/src/main/java/com/ververica/cdc/composer/flink/FlinkPipelineComposer.java#L94] use the same parallelism now. 
CC @PatrickRen ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[cdc-pipeline] Add 'flink-confing' config for pipeline.,FLINK-34834,13572613,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:28,20/Mar/24 09:28,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Our current task is to submit to the Flink cluster. Currently, the Flink configuration can only be read from 'FLINK_HOME' in the flink-conf.yaml file. This limitation is too restrictive, as it does not allow for flexible configuration of individual pipeline settings.

### Solution

Therefore, I suggest adding a configuration item, ""flink-config,"" to the pipeline YAML configuration file. This will be used to configure Flink settings for individual pipeline.
The values in the ""flink-config"" configuration item will override those in flink-conf.yaml.

The 'flink-config' configuration items are as follows:
```yaml
flink-config:
  execution.checkpointing.interval: 30000
  execution.checkpointing.mode: EXACTLY_ONCE
```

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2854
Created by: [joyCurry30|https://github.com/joyCurry30]
Labels: enhancement, 
Created at: Tue Dec 12 14:08:55 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:28:04.0,,,,,,,,,,"0|z1o3fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug][sqlserver] SqlServer incremental source cannot support exactly-once ,FLINK-34833,13572612,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:28,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"## Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


## Flink version

1.18

## Flink CDC version

3.0


## Reason
### overview 
At first,  we can see what SqlServer incremental source do to guarantee exactly-once sematics in parallel read。
First, split the table into multiple chunks based on the key, each chunk as a snapshot split. These  splits can be read in parallel.

In snapshot phase, for each snapshot split between [low_key, high_key]: 
1. use SqlServerDialect#displayCurrentOffset  to get LSN as low_watermark
2. read snapshot data between [low_key, high_key] as a temporary state `state1` by JDBC connection query.
3. use SqlServerDialect#displayCurrentOffset  to get LSN as high_watermark
4. read log between (low_watermark, high_watermark), will update temporary state `state1` and generate  **final state of  high_watermark**, then emit to downstream.

Then in stream phase, we read log between [high_watermark, +∞）for this split between [low_key, high_key].

### problem
However,  `SqlServerDialect#displayCurrentOffset → SqlServerUtils#currentLsn → SqlServerConnection#getMaxTransactionLsn` return the max LSN of system table `cdc.lsn_time_mapping` , which is not the real latest LSN of whole database system. In this incremental source framework, only the real latest LSN of whole database system can guarantee exactly-once sematics.

As shown in https://stackoverflow.com/questions/29477391/cdc-data-only-shows-up-after-5-minutes,  this developer find that [CDC Data Only Shows up After 5 Minutes|https://stackoverflow.com/questions/29477391/cdc-data-only-shows-up-after-5-minutes) , and the reason is :
> Because the capture process extracts change data from the transaction log, there is a built in latency between the time that a change is committed to a source table and the time that the change appears within its associated change table. 

For example, low_mark and high_watermark maybe  5 minutes lower than the Lsn of step two(read snapshot data). Then in streaming phase, the log which should be ignored will be read again.



## How to verify
Modify test: com.ververica.cdc.connectors.oracle.source.OracleSourceITCase#testEnableBackfillWithDMLPostLowWaterMark, we can see three dml operations are read twicely.
```java
  @Test
    public void testEnableBackfillWithDMLPostLowWaterMark() throws Exception {

        List<String> records = testBackfillWhenWritingEvents(false, 25, USE_POST_LOWWATERMARK_HOOK];

        List<String> expectedRecords =
                Arrays.asList(
                        ""+I[101, user_1, Shanghai, 123567891234]"",
                        ""+I[102, user_2, Shanghai, 123567891234]"",
                        ""+I[103, user_3, Shanghai, 123567891234]"",
                        ""+I[109, user_4, Shanghai, 123567891234]"",
                        ""+I[110, user_5, Shanghai, 123567891234]"",
                        ""+I[111, user_6, Shanghai, 123567891234]"",
                        ""+I[118, user_7, Shanghai, 123567891234]"",
                        ""+I[121, user_8, Shanghai, 123567891234]"",
                        ""+I[123, user_9, Shanghai, 123567891234]"",
                        ""+I[1009, user_10, Shanghai, 123567891234]"",
                        ""+I[1010, user_11, Shanghai, 123567891234]"",
                        ""+I[1011, user_12, Shanghai, 123567891234]"",
                        ""+I[1012, user_13, Shanghai, 123567891234]"",
                        ""+I[1013, user_14, Shanghai, 123567891234]"",
                        ""+I[1014, user_15, Shanghai, 123567891234]"",
                        ""+I[1015, user_16, Shanghai, 123567891234]"",
                        ""+I[1016, user_17, Shanghai, 123567891234]"",
                        ""+I[1017, user_18, Shanghai, 123567891234]"",
                        ""+I[1018, user_19, Shanghai, 123567891234]"",
                        ""+I[2000, user_21, Pittsburgh, 123567891234]"",
                        ""+I[15213, user_15213, Shanghai, 123567891234]"",
                        // the operations  already be applied in snapshot phase, but are read again in streaming phase
                        ""+I[15213, user_15213, Shanghai, 123567891234]"",
                        ""-U[2000, user_21, Shanghai, 123567891234]"",
                        ""+U[2000, user_21, Pittsburgh, 123567891234]"",
                        ""-D[1019, user_20, Shanghai, 123567891234]"");
        assertEqualsInAnyOrder(expectedRecords, records);
    }
 private List<String> testBackfillWhenWritingEvents(
            boolean skipSnapshotBackfill, int fetchSize, int hookType) throws Exception {
        createAndInitialize(""customer.sql"");
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.enableCheckpointing(200L);
        env.setParallelism(1);

        ResolvedSchema customersSchema =
                new ResolvedSchema(
                        Arrays.asList(
                                physical(""ID"", BIGINT().notNull()),
                                physical(""NAME"", STRING()),
                                physical(""ADDRESS"", STRING()),
                                physical(""PHONE_NUMBER"", STRING())),
                        new ArrayList<>(),
                        UniqueConstraint.primaryKey(""pk"", Collections.singletonList(""ID"")));
        TestTable customerTable =
                new TestTable(ORACLE_DATABASE, ORACLE_SCHEMA, ""CUSTOMERS"", customersSchema);
        String tableId = customerTable.getTableId();

        OracleSourceBuilder.OracleIncrementalSource source =
                OracleSourceBuilder.OracleIncrementalSource.<RowData>builder()
                        .hostname(ORACLE_CONTAINER.getHost())
                        .port(ORACLE_CONTAINER.getOraclePort())
                        .username(CONNECTOR_USER)
                        .password(CONNECTOR_PWD)
                        .databaseList(ORACLE_DATABASE)
                        .schemaList(ORACLE_SCHEMA)
                        .tableList(""DEBEZIUM.CUSTOMERS"")
                        .skipSnapshotBackfill(skipSnapshotBackfill)
                        .startupOptions(StartupOptions.initial())
                        .deserializer(customerTable.getDeserializer())
                        .build();

        // Do some database operations during hook in snapshot period.
        SnapshotPhaseHooks hooks = new SnapshotPhaseHooks();
        String[] statements =
                new String[] {
                    String.format(
                            ""INSERT INTO %s VALUES (15213, 'user_15213', 'Shanghai', '123567891234')"",
                            tableId),
                    String.format(""UPDATE %s SET address='Pittsburgh' WHERE id=2000"", tableId),
                    String.format(""DELETE FROM %s WHERE id=1019"", tableId)
                };
        SnapshotPhaseHook snapshotPhaseHook =
                (sourceConfig, split) -> {
                    // database update operations use TEST_USER rather than CONNECTOR_USER
                    JdbcConfiguration configuration =
                            JdbcConfiguration.copy(
                                            ((JdbcSourceConfig) sourceConfig)
                                                    .getDbzConnectorConfig()
                                                    .getJdbcConfig())
                                    .withUser(""debezium"")
                                    .withPassword(""dbz"")
                                    .build();
                    try (OracleConnection oracleConnection =
                            OracleConnectionUtils.createOracleConnection(configuration)) {
                        oracleConnection.setAutoCommit(false);
                        oracleConnection.execute(statements);
                        oracleConnection.commit();
                    }
                };

        if (hookType == USE_POST_LOWWATERMARK_HOOK) {
            hooks.setPostLowWatermarkAction(snapshotPhaseHook);
        } else if (hookType == USE_PRE_HIGHWATERMARK_HOOK) {
            hooks.setPreHighWatermarkAction(snapshotPhaseHook);
        }
        source.setSnapshotHooks(hooks);

        List<String> records = new ArrayList<>();
        try (CloseableIterator<RowData> iterator =
                env.fromSource(source, WatermarkStrategy.noWatermarks(), ""Backfill Skipped Source"")
                        .executeAndCollect()) {
            records = fetchRowData(iterator, fetchSize, customerTable::stringify);
            env.close();
        }
        return records;
    }
```


### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2853
Created by: [loserwang1024|https://github.com/loserwang1024]
Labels: bug, 
Assignee: [loserwang1024|https://github.com/loserwang1024]
Created at: Tue Dec 12 10:44:24 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:28:02 UTC 2024,,,,,,,,,,"0|z1o3fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:28;flink-cdc-import;Date: Tue Dec 12 11:03:29 CST 2023, Author: [loserwang1024|https://github.com/loserwang1024]

@gong , @GOODBOY008 , @leonardBang , @ruanhang1993 , @Shawn-Hx , CC, WDYT?
By the way, I search a lot and even ask in [stackoverflow|https://stackoverflow.com/questions/77639528/whats-the-difference-between-sys-fn-cdc-get-max-lsn-and-max-value-of-cdc-lsn], but cannot find how to get the real latest LSN of whole database system, would you like to give me some advice?;;;","20/Mar/24 09:28;flink-cdc-import;Date: Tue Dec 12 17:50:30 CST 2023, Author: [gong|https://github.com/gong]

> @gong , @GOODBOY008 , @leonardBang , @ruanhang1993 , @Shawn-Hx , CC, WDYT? By the way, I search a lot and even ask in [stackoverflow|https://stackoverflow.com/questions/77639528/whats-the-difference-between-sys-fn-cdc-get-max-lsn-and-max-value-of-cdc-lsn], but cannot find how to get the real latest LSN of whole database system, would you like to give me some advice?

@loserwang1024 https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#_effect_of_sql_server_capture_job_agent_configuration_on_server_load_and_latency  maybe we can set wait time before get lsn?;;;","20/Mar/24 09:28;flink-cdc-import;Date: Wed Dec 13 10:10:52 CST 2023, Author: [loserwang1024|https://github.com/loserwang1024]

> maybe we can set wait time before get lsn?

@gong ,  low_watermark smaller is no effect. But for high_watermark, what is the optimal waiting time to ensure that updated data is successfully transferred to CDC table?

;;;","20/Mar/24 09:28;flink-cdc-import;Date: Thu Jan 18 15:27:16 CST 2024, Author: [loserwang1024|https://github.com/loserwang1024]

@gong, thanks a lot,  it seems `pollinginterval` is the param of sys.sp_cdc_change_job( https://learn.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sys-sp-cdc-change-job-transact-sql?view=sql-server-ver16) to control the interval to scan cdc data. So we can provide a params for user which depends on the real sqlserver situation, and the default value is 5s(same with sqlserver default value).  So user can control the time depending on their actual demand  .Then we suspend pollinginterval + 1s to get highwatermark.

CC, @GOODBOY008 , WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] BinlogOffset's comparative method may be cause incremental snapshot scan data lose,FLINK-34832,13572611,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.14.5

### Flink CDC version

2.4.0

### Database and its version

MariaDB 10.4.13

### Minimal reproduce step

1. create mysql test table
2. create a flink cdc sql task and set the startup mode to initial
3. while writing 10000 records to the test table, start the flink task and check the number of records sent by the data.

### What did you expect to see?

The number of records is consistent with the mysql test table

### What did you see instead?

data loss or duplication

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!
![932A614C-53BF-4592-9E6C-D7F87BD6CD6F|https://github.com/ververica/flink-cdc-connectors/assets/39044001/742cd5a2-1312-47e6-9f6b-14e250198c36]


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2794
Created by: [EchoLee5|https://github.com/EchoLee5]
Labels: bug, 
Created at: Fri Dec 01 19:28:48 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:27:53 UTC 2024,,,,,,,,,,"0|z1o3f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:27;flink-cdc-import;Date: Fri Dec 01 19:36:09 CST 2023, Author: [EchoLee5|https://github.com/EchoLee5]

I figured out that there is a problem with the logic of judging whether it is finished when reading binlog data.

MySqlBinlogSplitReadTask#handleEvent
```
protected void handleEvent(
            MySqlPartition partition, MySqlOffsetContext offsetContext, Event event) {
        if (!eventFilter.test(event)) {
            return;
        }
        super.handleEvent(partition, offsetContext, event);
        // check do we need to stop for read binlog for snapshot split.
        if (isBoundedRead()) {
            final BinlogOffset currentBinlogOffset = getBinlogPosition(offsetContext.getOffset());
            // reach the high watermark, the binlog reader should finished
            if (currentBinlogOffset.isAtOrAfter(binlogSplit.getEndingOffset())) {
                // send binlog end event
                try {
                    signalEventDispatcher.dispatchWatermarkEvent(
                            binlogSplit,
                            currentBinlogOffset,
                            SignalEventDispatcher.WatermarkKind.BINLOG_END);
                } catch (InterruptedException e) {
                    LOG.error(""Send signal event error."", e);
                    errorHandler.setProducerThrowable(
                            new DebeziumException(""Error processing binlog signal event"", e));
                }
                // tell reader the binlog task finished
                ((SnapshotBinlogSplitChangeEventSourceContextImpl) context).finished();
            }
        }
    }
```

BinlogOffset#compareTo(BinlogOffset that)
```
// Both offsets are missing GTIDs. Look at the servers ...
        long serverId = this.getServerId();
        long targetServerId = that.getServerId();
        if (serverId != targetServerId) {
            // These are from different servers, and their binlog coordinates are not related. So
            // the only thing we can do
            // is compare timestamps, and we have to assume that the server timestamps can be
            // compared ...
            long timestamp = this.getTimestampSec();
            long targetTimestamp = that.getTimestampSec();
            return Long.compare(timestamp, targetTimestamp);
        }
```

![Dingtalk_20231201193402|https://github.com/ververica/flink-cdc-connectors/assets/39044001/701ab5c1-86ca-4564-8d6d-89578b2afa33]

Since the serverId in the high watermark is null, binlog reading that ends early is judged by the timestamp.;;;","20/Mar/24 09:27;flink-cdc-import;Date: Fri Dec 01 20:04:41 CST 2023, Author: [EchoLee5|https://github.com/EchoLee5]

@leonardBang Can you help me look into this issue?;;;","20/Mar/24 09:27;flink-cdc-import;Date: Fri Dec 01 20:05:35 CST 2023, Author: [EchoLee5|https://github.com/EchoLee5]

Is it possible to remove the serverId comparison logic?;;;","20/Mar/24 09:27;flink-cdc-import;Date: Fri Dec 01 20:24:16 CST 2023, Author: [leonardBang|https://github.com/leonardBang]

Hi, @EchoLee5  thanks for opening this ticket.
I don't get your meaning, could you describe how to reproduce this data loss issue or add a test to reproduce it?;;;","20/Mar/24 09:27;flink-cdc-import;Date: Mon Dec 04 10:19:52 CST 2023, Author: [EchoLee5|https://github.com/EchoLee5]

@leonardBang sorry i'm late, the way I reproduce the issue is as follows:
First, I start the task of writing test data to mysql. 
After writing part of the data, I start the flink sql task to ensure that the task will use incremental data to correct the snapshot data.
I expected that the final data consumed would be 10000, but the actual number was 9961

My mysql version is 10.3.10-MariaDB-log, **gtid is not enabled**, the test table ddl is:
```
CREATE TABLE mysql_table(
  `c1` int,
  `c2` int ,
  `c3` int,
  PRIMARY KEY (`c1`)
) 
```
The logic for sending the test table is:
  ```
  public static void main(String[] args) throws Exception {
        Connection con = null;
        PreparedStatement stmt = null;
        int count = 0;
        int totalCount = 10000;
        try {
            Class.forName(DBDRIVER);
            con = DriverManager.getConnection(DBURL, DBUSER, DBPASS);
            stmt = con.prepareStatement(""insert into mysql_table(c1, c2, c3) values(?,?,?)"");
            for (int i = 0; i < totalCount; i++) {
                count++;

                stmt.setInt(1, i);
                stmt.setInt(2, 2);
                stmt.setInt(3, 3);

                stmt.addBatch();
                stmt.executeBatch();
                System.out.println(""Insert rows "" + count);
                Thread.sleep(2);
            }
        } catch (SQLException e) {
            System.out.printf(""Insert data has error "" + e.getMessage());
        } finally {
            if (con != null) {
                con.close();
            }
            if (stmt != null) {
                stmt.close();
            }
        }
    }
```

flink configured includes:
```
execution.checkpointing.interval: 10s
parallelism.default: 1
pipeline.operator-chaining: false
```

flink sql is:
```
CREATE TABLE flink_cdc(
  `c1` int,
  `c2` int ,
  `c3` int,
  PRIMARY KEY (`c1`)  NOT ENFORCED
) WITH (
    'connector' = 'mysql-cdc',
       'hostname' = 'xxx',
       'port' = '33061',
       'username' = 'root',
       'password' = 'xxx',
       'database-name' = 'test',
       'table-name' = 'mysql_table',
       'scan.startup.mode' = 'initial'
);

CREATE TABLE print (
  `c1` int,
  `c2` int ,
  `c3` int,
  PRIMARY KEY (`c1`)  NOT ENFORCED
) WITH (
   'connector' = 'print'
);

insert into print select * from flink_cdc;
```
![Dingtalk_20231204100350|https://github.com/ververica/flink-cdc-connectors/assets/39044001/ade7074d-467e-427b-858e-5cd0f45356c0]
![Dingtalk_20231204100337|https://github.com/ververica/flink-cdc-connectors/assets/39044001/d302c8fa-62e2-409e-b6ea-b7e914e61a24)

During breakpoint debugging, I found that when comparing the end point and the high watermark size, the server-id will be determined first, however, when we determine the high watermark through `show master status`, the serverId is not set.
DebeziumUtils#currentBinlogOffset
  ```
  public static BinlogOffset currentBinlogOffset(JdbcConnection jdbc) {
        final String showMasterStmt = ""SHOW MASTER STATUS"";
        try {
            return jdbc.queryAndMap(
                    showMasterStmt,
                    rs -> {
                        if (rs.next()) {
                            final String binlogFilename = rs.getString(1);
                            final long binlogPosition = rs.getLong(2);
                            final String gtidSet =
                                    rs.getMetaData().getColumnCount() > 4 ? rs.getString(5) : null;
                            return BinlogOffset.builder()
                                    .setBinlogFilePosition(binlogFilename, binlogPosition)
                                    .setGtidSet(gtidSet)
                                    .build();
                        } else {
                            throw new FlinkRuntimeException(
                                    ""Cannot read the binlog filename and position via '""
                                            + showMasterStmt
                                            + ""'. Make sure your server is correctly configured"");
                        }
                    });
        } catch (SQLException e) {
            throw new FlinkRuntimeException(
                    ""Cannot read the binlog filename and position via '""
                            + showMasterStmt
                            + ""'. Make sure your server is correctly configured"",
                    e];
        }
    }
```
![Dingtalk_20231204095858|https://github.com/ververica/flink-cdc-connectors/assets/39044001/497c58fa-e3c3-4768-a1c8-ef3b4fdd9a5c]
![Dingtalk_20231204095858|https://github.com/ververica/flink-cdc-connectors/assets/39044001/7fd1bfa9-fe26-47ea-a673-50d9a85d37e8]


;;;","20/Mar/24 09:27;flink-cdc-import;Date: Mon Dec 04 14:26:09 CST 2023, Author: [EchoLee5|https://github.com/EchoLee5]

@leonardBang I don't know if I explained it clearly. I hope you can help me look at this problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLINK-CDC-BASE] Reorder dependency  flink-table-runtime in pom.xml to avoid dependency conflict.,FLINK-34831,13572610,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

flink-cdc-base declares flink-table-runtime as its direct dependency. There are two transitive dependencies org.codehaus.janino:commons-compiler and org.codehaus.janino:janino in the dependency tree as listed below. They are introduced by flink-table-planner_2.12.
``` 
[INFO] com.ververica:flink-cdc-base:jar:3.0-SNAPSHOT
[INFO] +- org.apache.flink:flink-table-planner_2.12:jar:1.18.0:test
[INFO] |  +- org.codehaus.janino:commons-compiler:jar:3.1.10:test
[INFO] |  +- org.codehaus.janino:janino:jar:3.1.10:test
[INFO] +- org.apache.flink:flink-table-runtime:jar:1.18.0:test
```

Some duplicate classes in flink-table-planner are also in commons-compiler and janino as listed below. 
```
    org.codehaus.commons.compiler.lang.ClassLoaders$SubresourceGetter
    org.codehaus.commons.compiler.Location
    org.codehaus.commons.compiler.util.LineAndColumnTracker$1
    org.codehaus.commons.compiler.AbstractCompilerFactory
    ...

    org.codehaus.janino.util.ClassFile$CodeAttribute
    org.codehaus.janino.Java$ArrayAccessExpression
    org.codehaus.janino.util.ClassFile$AnnotationDefaultAttribute
    org.codehaus.janino.Java$TryStatement$LocalVariab
    ...
```

The problem is that the two transitive dependencies are ahead of flink-table-runtime in the dependency tree, thus ahead of flink-table-runtime in the classpath. When multiple JAR files contain the same class, the class loader will load the first occurrence it finds based on the classpath order. This means the classloader will **select classes in the transitive dependencies rather than direcly declared flink-table-runtime**.


### Solution

Change the order of flink-table-runtime and flink-table-planner_2.12 so that flink-table-runtime appears ahead of flink-table-planner_2.12 in classpath. In this way, flink-table-runtime is used as it's meant to be. 

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2753
Created by: [HappyHacker123|https://github.com/HappyHacker123]
Labels: enhancement, 
Created at: Sun Nov 26 17:00:07 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:27:44.0,,,,,,,,,,"0|z1o3ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.1][cdc-composer] verify the options of Context for Factory.,FLINK-34830,13572609,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

[Factory interface|https://github.com/ververica/flink-cdc-connectors/blob/a26607a02767fcaba5eb3524fd0257973caac876/flink-cdc-common/src/main/java/com/ververica/cdc/common/factories/Factory.java#L54C1-L54C1] defines a set of ConfigOption in requiredOptions and optionalOptions method, we need to check them When using Factory.

We can do this before calling createDataSource and createDataSink method, you can refer to [FactoryUtil|https://github.com/apache/flink/blob/6c429c5450a003d6521693116e0fbb2dab543d6e/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FactoryUtil.java#L1000C25-L1000C25].

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2752
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Sat Nov 25 17:42:41 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:27:41 UTC 2024,,,,,,,,,,"0|z1o3eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:27;flink-cdc-import;Date: Mon Nov 27 11:50:17 CST 2023, Author: [joyCurry30|https://github.com/joyCurry30]

I'm willing to do this task, please assign to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When will opengauss or gaussdb be supported?,FLINK-34829,13572608,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Opengauss and Huawei Cloud's gaussdb have been widely used in the financial industry in mainland China, and I hope that flink-cdc-connectors will have plans to support opengauss and gaussdb.

### Solution

I see a two year old case here, not sure if it can work. Reference: https://github.com/dafei1288/flink-connector-opengauss

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2697
Created by: [wangbinhang|https://github.com/wangbinhang]
Labels: enhancement, 
Created at: Wed Nov 15 11:13:08 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:27:36.0,,,,,,,,,,"0|z1o3eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sql server Limited date synchronization,FLINK-34828,13572607,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.16

### Flink CDC version

2.4.1

### Database and its version

sqlserver2014

### Minimal reproduce step

Support for functions  SettlementTime>=CURRENT_DATE - INTERVAL '90' DAY;
Cannot start from function definition range

### What did you expect to see?

I hope to start from the function definition range, rather than starting from all data starting positions

### What did you see instead?

I hope to synchronize the data from the last three months, but the program still starts from 2022. Although the data has not been synchronized, there has been continuous detection
![image|https://github.com/ververica/flink-cdc-connectors/assets/8586973/8488159c-a95b-4528-9ff9-1438afac46d7]
The program execution will not have any problems, but it will cause the first synchronization to be very slow

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2696
Created by: [ysq5202121|https://github.com/ysq5202121]
Labels: bug, 
Created at: Wed Nov 15 10:05:40 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:27:32.0,,,,,,,,,,"0|z1o3e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[flink-cdc-pipeline-connectors] Add Implementation of DataSink in Kafka,FLINK-34827,13572606,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

 Add Implementation of DataSink in Kafka

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2691
Created by: [leonardBang|https://github.com/leonardBang]
Labels: enhancement, task, 【3.0】, 
Milestone: V3.0.0
Created at: Tue Nov 14 12:16:27 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:27:29 UTC 2024,,,,,,,,,,"0|z1o3e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:27;flink-cdc-import;Date: Tue Dec 26 17:48:20 CST 2023, Author: [lvyanquan|https://github.com/lvyanquan]

I'm willing to do this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-connector-mysql-cdc-2.4.2 when a multi-column primary key index starts with a varchar column, a snapshot chunk can potentially return a large number of rows and cause jvm to OOM.",FLINK-34826,13572605,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.17.1

### Flink CDC version

flink-connector-mysql-cdc-2.4.2

### Database and its version

source: MySQL 8.0.19
destination: Doris 2.0.2

### Minimal reproduce step

1. create a MySQL table with multi-column primary key and its first column is varchar

```
CREATE DATABASE `mydb`;
CREATE TABLE `t` (
  `col1` varchar(64) NOT NULL,
  `col2` varchar(64) NOT NULL,
  `col3` datetime(6) NOT NULL,
  `col4` varchar(200) DEFAULT NULL,
  `col5` varchar(500) DEFAULT NULL,
  PRIMARY KEY (`col1`,`col2`,`col3`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

2. insert a lot of rows into the table
```
use mydb;
# execute the below SQL for 5000000 times
insert into t values('aaa'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
insert into t values('aaa'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
insert into t values('aaa'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
...

# execute the below SQL for 5000000 times
insert into t values('bbb'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
insert into t values('bbb'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
insert into t values('bbb'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
...

# execute the below SQL for 5000000 times
insert into t values('ccc'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
insert into t values('ccc'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
insert into t values('ccc'), repeat('a', 64), now(6), repeat('a', 200), repeat('a', 500));
...
```

3. create database in doris
```
CREATE DATABASE `mydb`;
```

4. create user in MySQL and doris

In MySQL:
```
create user flink identified by 'pass';
grant all on *.* to flink;
```

In Doris:
```
create user flink identified by 'pass';
grant all on *.* to flink;
```

5. download flink-doris and flink-cdc jar and put them into the lib directory of flink-1.17.1

```
flink-doris-connector-1.17-1.4.0.jar
flink-sql-connector-mysql-cdc-2.4.2.jar
```

6. use flink to sync the data from MySQL to doris
```
bin/flink run -d \
    -Dexecution.checkpointing.interval=10s \
    -Dparallelism.default=1 \
    -c org.apache.doris.flink.tools.cdc.CdcTools \
    lib/flink-doris-connector-1.17-1.4.0.jar \
    mysql-sync-database \
    --database idc_manager \
    --job-name flink_sync_mysql_to_doris \
    --mysql-conf hostname=<MYSQL_IP> \
    --mysql-conf port=3306 \
    --mysql-conf username=flink \
    --mysql-conf password=pass \
    --mysql-conf database-name=mydb\
    --including-tables ""t"" \
    --sink-conf fenodes=<DORIS_FE_IP>:8030 \
    --sink-conf username=flink \
    --sink-conf password=pass \
    --sink-conf jdbc-url=jdbc:mysql://<DORIS_FE_IP>:9030 \
    --sink-conf sink.label-prefix=label1 \
    --table-conf replication_num=3
```

### What did you expect to see?

The table should be split into 8096 rows per chunk.

### What did you see instead?

The table was split into 500M rows per chunk and finally flink job OOM.

The split chunk SQL in MySQL will return 500M rows.
```
SELECT * FROM `mydb`.`t` WHERE `col1` <= 'bbb' AND NOT (`col1` = 'bbb'];
```

flink job OOM:
```
Caused by: java.lang.OutOfMemoryError: Java heap space
```

### Anything else?

When the below conditions are met, the above bad case will occur:

1. table with multi-column primary key;
2. the first column of the primary key is varchar;
3. there are a lot of rows with the same value of the first column;

Root cause:

The flink-mysql-cdc will use the first column to split the table into chunks and a chunk should contain 8096 rows approximately. But this is a ideal case. In a table with multi-column primary key, multiple rows can have the same first column value and it will make flink-cdc-mysql to read a lot of rows as a chunk because the begin and end of the chunk is only marked with the first column so it
cannot just read 8096 rows. Instead it will read the next different value of the first column and this may result a lot of rows in a chunk.

Suggest fix:

Use the full column names of the primary key to split the table into chunks.

I am willing to submit a PR but I am new to Java and I don't know how much time it will take.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2676
Created by: [trikker|https://github.com/trikker]
Labels: enhancement, 
Created at: Thu Nov 09 21:01:34 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:27:25 UTC 2024,,,,,,,,,,"0|z1o3ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:27;flink-cdc-import;Date: Fri Nov 24 13:58:21 CST 2023, Author: [WanYukun|https://github.com/WanYukun]

会走非公平切分那一块逻辑,每个chunk size的大小应该不会特别夸张吧;;;","20/Mar/24 09:27;flink-cdc-import;Date: Thu Dec 28 11:59:37 CST 2023, Author: [link3280|https://github.com/link3280]

The same issue here. @trikker Do you have some workaround that could share? Thanks!;;;","20/Mar/24 09:27;flink-cdc-import;Date: Thu Dec 28 12:00:00 CST 2023, Author: [WanYukun|https://github.com/WanYukun]

这是来自QQ邮箱的自动回复邮件。你好，邮件正常收到，谢谢！                          -----万玉坤;;;","20/Mar/24 09:27;flink-cdc-import;Date: Tue Jan 09 10:36:03 CST 2024, Author: [AidenPerce|https://github.com/AidenPerce]

This bug is caused by different charsets and collation rules in the MySQL database. 
Because we use JAVA to compare the max value of table and the Chunk-End value to check the End-Bound, but the max value comes from the `SELECT MAX(columnName) FROM TABLEANAME`, this result is affected by the database's character set and collation rules.
I have a pr to fixed it, but I'm not sure if it is appropriate or not;
https://github.com/ververica/flink-cdc-connectors/pull/2968;;;","20/Mar/24 09:27;flink-cdc-import;Date: Tue Jan 09 10:40:26 CST 2024, Author: [AidenPerce|https://github.com/AidenPerce]

When use a primaryKey which is varchar, we can reproduce this problem;
Example:
+ Create a table with a varchar-primaryKey,  charset is ""utf8mb4"", collation is ""utf8mb4_general_ci"";
+ The values of primaryKey like ['0000','1111','2222','aaaa','bbbb','cccc','ZZZZ']，which includes uppercase letters.
+ Create a cdc-source with 'scan.incremental.snapshot.chunk.size'='2'
The expect chunks is :
['0000]['1111']['2222']['aaaa']['bbbb']['cccc','ZZZZ']
The actual chunks is :
['0000]['1111']['2222']['aaaa','bbbb','cccc','ZZZZ'];;;","20/Mar/24 09:27;flink-cdc-import;Date: Sun Feb 04 15:01:21 CST 2024, Author: [ruanhang1993|https://github.com/ruanhang1993]

Hi, @trikker . Mysql CDC supports to set `scan.incremental.snapshot.chunk.key-column` to select a column in the primary key to split chunks. 

;;;","20/Mar/24 09:27;flink-cdc-import;Date: Sun Feb 04 15:01:41 CST 2024, Author: [WanYukun|https://github.com/WanYukun]

这是来自QQ邮箱的自动回复邮件。你好，邮件正常收到，谢谢！                          -----万玉坤;;;","20/Mar/24 09:27;flink-cdc-import;Date: Mon Feb 05 14:41:19 CST 2024, Author: [AidenPerce|https://github.com/AidenPerce]

> Hi, @trikker . Mysql CDC supports to set `scan.incremental.snapshot.chunk.key-column` to select a column in the primary key to split chunks.
When the type of the partition key is set to varchar, the original logic of unevenly-chunked data can be affected by the database's character-set/sorting-rules, leading to the creation of very large chunks and causing OOM errors. This issue is unrelated to whether there are multiple primary keys.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
[flink-cdc-pipeline-connectors] Add Implementation of DataSource in MongoDB,FLINK-34825,13572604,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:27,20/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

After https://github.com/ververica/flink-cdc-connectors/pull/2638 merged, we can try to add implementation in MongoDB to simulate enterprise scenarios.

You may need to wait for https://github.com/ververica/flink-cdc-connectors/issues/2642 https://github.com/ververica/flink-cdc-connectors/issues/2644 completed. 

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2648
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, task, 【3.0】, 
Assignee: [Jiabao-Sun|https://github.com/Jiabao-Sun]
Created at: Tue Nov 07 11:18:18 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:27:14.0,,,,,,,,,,"0|z1o3dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Mysql-connector cdc sourcerecord not have data type information,bug dez record have",FLINK-34824,13572603,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:24,20/Mar/24 09:24,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

when we develop streaming data platform , we also need data types to satisfied dynamic change  table schema. But i fund Mysql-connector cdc record has too many useless information. I suggested to use dbz record information type to meet more needed.

### Solution

change sourcerecord code  to dbz record . such as below 

### Alternatives

EmbeddedEngineChangeEvent[key = {
		""schema"": {
			""type"": ""struct"",
			""fields"": [{
				""type"": ""int64"",
				""optional"": false,
				""field"": ""id""
			}],
			""optional"": false,
			""name"": ""mysql_connector.gmall.activity_info.Key""
		},
		""payload"": {
			""id"": 3
		}
	}, value = {
		""schema"": {
			""type"": ""struct"",
			""fields"": [{
				""type"": ""struct"",
				""fields"": [{
					""type"": ""int64"",
					""optional"": false,
					""field"": ""id""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""activity_name""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""activity_type""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""activity_desc""
				}, {
					""type"": ""int64"",
					""optional"": true,
					""name"": ""io.debezium.time.Timestamp"",
					""version"": 1,
					""field"": ""start_time""
				}, {
					""type"": ""int64"",
					""optional"": true,
					""name"": ""io.debezium.time.Timestamp"",
					""version"": 1,
					""field"": ""end_time""
				}, {
					""type"": ""int64"",
					""optional"": true,
					""name"": ""io.debezium.time.Timestamp"",
					""version"": 1,
					""field"": ""create_time""
				}],
				""optional"": true,
				""name"": ""mysql_connector.gmall.activity_info.Value"",
				""field"": ""before""
			}, {
				""type"": ""struct"",
				""fields"": [{
					""type"": ""int64"",
					""optional"": false,
					""field"": ""id""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""activity_name""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""activity_type""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""activity_desc""
				}, {
					""type"": ""int64"",
					""optional"": true,
					""name"": ""io.debezium.time.Timestamp"",
					""version"": 1,
					""field"": ""start_time""
				}, {
					""type"": ""int64"",
					""optional"": true,
					""name"": ""io.debezium.time.Timestamp"",
					""version"": 1,
					""field"": ""end_time""
				}, {
					""type"": ""int64"",
					""optional"": true,
					""name"": ""io.debezium.time.Timestamp"",
					""version"": 1,
					""field"": ""create_time""
				}],
				""optional"": true,
				""name"": ""mysql_connector.gmall.activity_info.Value"",
				""field"": ""after""
			}, {
				""type"": ""struct"",
				""fields"": [{
					""type"": ""string"",
					""optional"": false,
					""field"": ""version""
				}, {
					""type"": ""string"",
					""optional"": false,
					""field"": ""connector""
				}, {
					""type"": ""string"",
					""optional"": false,
					""field"": ""name""
				}, {
					""type"": ""int64"",
					""optional"": false,
					""field"": ""ts_ms""
				}, {
					""type"": ""string"",
					""optional"": true,
					""name"": ""io.debezium.data.Enum"",
					""version"": 1,
					""parameters"": {
						""allowed"": ""true,last,false,incremental""
					},
					""default"": ""false"",
					""field"": ""snapshot""
				}, {
					""type"": ""string"",
					""optional"": false,
					""field"": ""db""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""sequence""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""table""
				}, {
					""type"": ""int64"",
					""optional"": false,
					""field"": ""server_id""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""gtid""
				}, {
					""type"": ""string"",
					""optional"": false,
					""field"": ""file""
				}, {
					""type"": ""int64"",
					""optional"": false,
					""field"": ""pos""
				}, {
					""type"": ""int32"",
					""optional"": false,
					""field"": ""row""
				}, {
					""type"": ""int64"",
					""optional"": true,
					""field"": ""thread""
				}, {
					""type"": ""string"",
					""optional"": true,
					""field"": ""query""
				}],
				""optional"": false,
				""name"": ""io.debezium.connector.mysql.Source"",
				""field"": ""source""
			}, {
				""type"": ""string"",
				""optional"": false,
				""field"": ""op""
			}, {
				""type"": ""int64"",
				""optional"": true,
				""field"": ""ts_ms""
			}, {
				""type"": ""struct"",
				""fields"": [{
					""type"": ""string"",
					""optional"": false,
					""field"": ""id""
				}, {
					""type"": ""int64"",
					""optional"": false,
					""field"": ""total_order""
				}, {
					""type"": ""int64"",
					""optional"": false,
					""field"": ""data_collection_order""
				}],
				""optional"": true,
				""field"": ""transaction""
			}],
			""optional"": false,
			""name"": ""mysql_connector.gmall.activity_info.Envelope""
		},
		""payload"": {
			""before"": null,
			""after"": {
				""id"": 3,
				""activity_name"": ""ccccc"",
				""activity_type"": ""1003"",
				""activity_desc"": ""fffff"",
				""start_time"": null,
				""end_time"": null,
				""create_time"": null
			},
			""source"": {
				""version"": ""1.9.5.Final"",
				""connector"": ""mysql"",
				""name"": ""mysql-connector"",
				""ts_ms"": 1694568910248,
				""snapshot"": ""true"",
				""db"": ""gmall"",
				""sequence"": null,
				""table"": ""activity_info"",
				""server_id"": 0,
				""gtid"": null,
				""file"": ""mysql-bin.000015"",
				""pos"": 154,
				""row"": 0,
				""thread"": null,
				""query"": null
			},
			""op"": ""r"",
			""ts_ms"": 1694568910248,
			""transaction"": null
		}
	}, sourceRecord = SourceRecord {
		sourcePartition = {
			server = mysql - connector
		}, sourceOffset = {
			ts_sec = 1694568910,
			file = mysql - bin .000015,
			pos = 154,
			snapshot = true
		}
	}
	ConnectRecord {
		topic = 'mysql-connector.gmall.activity_info', kafkaPartition = null, key = Struct {
			id = 3
		}, keySchema = Schema {
			mysql_connector.gmall.activity_info.Key: STRUCT
		}, value = Struct {
			after = Struct {
				id = 3, activity_name = ccccc, activity_type = 1003, activity_desc = fffff
			}, source = Struct {
				version = 1.9 .5.Final, connector = mysql, name = mysql - connector, ts_ms = 1694568910248, snapshot = true, db = gmall, table = activity_info, server_id = 0, file = mysql - bin .000015, pos = 154, row = 0
			}, op = r, ts_ms = 1694568910248
		}, valueSchema = Schema {
			mysql_connector.gmall.activity_info.Envelope: STRUCT
		}, timestamp = null, headers = ConnectHeaders(headers = )
	}]

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2614
Created by: [niuhu3|https://github.com/niuhu3]
Labels: enhancement, 
Created at: Fri Nov 03 16:23:04 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:24:25 UTC 2024,,,,,,,,,,"0|z1o3dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:24;flink-cdc-import;Date: Sun Feb 04 20:02:16 CST 2024, Author: [ruanhang1993|https://github.com/ruanhang1993]

Actually you could provide a custom deserializer in MySqlSource by using the DataStream API.
But there is no custom deserializer setting option for the sql.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] flink-postgresql-cdc buzy 100%,FLINK-34823,13572602,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:24,20/Mar/24 09:24,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.17.0

### Flink CDC version

2.4.1

### Database and its version

PostgreSQL 11.9 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44], 64-bit

### Minimal reproduce step

At present, too many tables seem to cause this problem.

### What did you expect to see?

I thought that if I saw that the buzy was not 100% checkpoint, it would not time out, and the task would not fail.

### What did you see instead?

Here are a few questions.

one. I have 1000 tables, but only 10 tables in the test environment have data, about 600W, but according to the log, does it make sense that the snapshot phase ended in an hour?

After the checkpoint completes the snapshot phase, it will time out, causing the task to fail, and the task failure and cancellation will block until the taskmanager kill is dropped.
![image|https://github.com/ververica/flink-cdc-connectors/assets/42414099/a9f646ea-82cc-46cc-ae06-d8c491838f67]


From stack, flame diagram, database query process, all see the information with tableScheme has replacement, as shown below.
Is it possible that there are too many meters, resulting in a long-term blockage here?

![image|https://github.com/ververica/flink-cdc-connectors/assets/42414099/ae91b53e-7cb8-4d82-b7c9-b5759189ed82]
![image|https://github.com/ververica/flink-cdc-connectors/assets/42414099/77e3aa4f-41b9-4098-a710-2f5f9c87dc4e]


### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2586
Created by: [yanye666|https://github.com/yanye666]
Labels: bug, 
Created at: Wed Oct 25 18:17:15 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:24:19 UTC 2024,,,,,,,,,,"0|z1o3d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:24;flink-cdc-import;Date: Wed Oct 25 18:31:43 CST 2023, Author: [yanye666|https://github.com/yanye666]

([#1458|https://github.com/apache/flink-cdc/issues/1458] | [FLINK-1458|https://issues.apache.org/jira/browse/FLINK-1458])  I saw a similar mistake.
![image|https://github.com/ververica/flink-cdc-connectors/assets/42414099/5cb76b02-1f92-471b-9ba4-bff0b231f1c6]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] oracle clob,FLINK-34822,13572601,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:24,20/Mar/24 09:24,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.12

### Flink CDC version

2.1.0

### Database and its version

oracle11

### Minimal reproduce step

In Oracle with Flink CDC, one of the fields is a CLOB. When modifying a data record, as long as the CLOB field remains unchanged, Flink CDC, during the incremental phase, will not include the CLOB field data in the received Oracle data updates. All other fields will be included. Only when modifying the CLOB field data will Flink CDC transmit it. Is this a bug? Why isn’t the CLOB field transmitted?

### What did you expect to see?

flinkcdc will transfer clob field even oralce not update this field

### What did you see instead?

1

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2573
Created by: [liquidmavis|https://github.com/liquidmavis]
Labels: bug, 
Created at: Mon Oct 23 10:40:22 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:24:15 UTC 2024,,,,,,,,,,"0|z1o3cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:24;flink-cdc-import;Date: Mon Oct 30 15:51:52 CST 2023, Author: [e-mhui|https://github.com/e-mhui]

By default, Oracle will not synchronize data in clob format. You can try adding this advanced parameter `debezium.lob.enabled=true`.
Refer： https://debezium.io/documentation/reference/1.9/connectors/oracle.html#oracle-connector-properties
<img width=""1311"" alt=""image"" src=""https://github.com/ververica/flink-cdc-connectors/assets/111486498/2b745b47-576a-4013-81de-bcf5856c03be"">
;;;","20/Mar/24 09:24;flink-cdc-import;Date: Wed Nov 01 09:56:59 CST 2023, Author: [fangchuanming|https://github.com/fangchuanming]

i meet this problem too, when update some columns (not clob)，will get null value in clob columns . maybe this is oracle official act, information from :https://access.redhat.com/documentation/zh-cn/red_hat_integration/2022.q4/html/debezium_user_guide/debezium-connector-for-oracle


```
Note
Oracle only supplies column values for CLOB, NCLOB, and BLOB data types if they’re explicitly set or changed in a SQL statement. As a result, change events never contain the value of an unchanged CLOB, NCLOB, or BLOB column. Instead, they contain placeholders as defined by the connector property, unavailable.value.placeholder.

If the value of a CLOB, NCLOB, or BLOB column is updated, the new value is placed in the after element of the corresponding update change event. The before element contains the unavailable value placeholder.
```;;;","20/Mar/24 09:24;flink-cdc-import;Date: Thu Nov 16 00:21:53 CST 2023, Author: [Naros|https://github.com/Naros]

Hi, Debezium's Oracle maintainer here.

Oracle, PostgreSQL, and SQL Server have specific use cases with certain column types that will not be written to the transaction logs unless those fields are explicitly modified.  For the Oracle database, this includes data types such as CLOB, BLOB, and XMLTYPE.

What you should be observing (assuming that `lob.enabled` is set to `true`), is that you will see the Debezium unavailable placeholder injected as the value for CLOB/XMLTYPE columns and the unavailable value placeholder injected as the value for BLOB columns, except the latter is converted to a byte array to satisfy the schema type. 

For consumers, when you see the unavailable value placeholder sentinel value for these columns, your consumer should ignore that column in this case, so when using something like a JDBC sink; you simply don't add that column to your INSERT or UPDATE operation.

As @fangchuanming points out, this is not a Debezium limitation, but how the database works.  And as I mentioned above, PostgreSQL and SQL Server also have specific data types that work in an identical way.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[improve] Alter scan.incremental.close-idle-reader.enabled Default is true,FLINK-34821,13572600,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:24,20/Mar/24 09:24,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Version 2.5 will remove support for flink 1.13, which means scan.incremental.close-idle-reader.enabled the preconditions for the characteristics of the parameters are all met. Do we consider modifying this parameter to true by default, which is a significant enhancement to the user.

### Solution

Modify the parameter default value to true

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2572
Created by: [gtk96|https://github.com/gtk96]
Labels: enhancement, 
Created at: Mon Oct 23 09:17:53 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:24:08.0,,,,,,,,,,"0|z1o3co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[postgres] Remove unnecessary schema fresh to improve performance.,FLINK-34820,13572599,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:24,20/Mar/24 09:24,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

It's very time-consuming for postgres to refresh schema if there are many tables to read. According to our testing, refreshing 400 tables takes 15 minutes. Because it takes a long time to refresh the current table's schema when reading a chunk in the scan stage, the data rate shows the following sawtooth pattern. Therefore, we need to minimize unnecessary shema refreshes as much as possible。

<img width=""437"" alt=""image"" src=""https://github.com/ververica/flink-cdc-connectors/assets/5181963/ffadeae0-5e8f-46d7-8941-3c1e3a0e7240"">


### Solution

Firstly, the origin schema of postgres cdc is [the schema filed of PostgresSourceFetchTaskContext|https://github.com/ververica/flink-cdc-connectors/blob/e0fd6f965b702cc2876372dc068379dafe066277/flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/source/fetch/PostgresSourceFetchTaskContext.java#L91], which is created and refreshed when [PostgresSourceFetchTaskContext#configure|https://github.com/ververica/flink-cdc-connectors/blob/e0fd6f965b702cc2876372dc068379dafe066277/flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/source/fetch/PostgresSourceFetchTaskContext.java#L176] is called, and both [the schema refresh of scan stage|https://github.com/ververica/flink-cdc-connectors/blob/e0fd6f965b702cc2876372dc068379dafe066277/flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/source/fetch/PostgresScanFetchTask.java#L281] and [stream stage|https://github.com/ververica/flink-cdc-connectors/blob/e0fd6f965b702cc2876372dc068379dafe066277/flink-connector-postgres-cdc/src/main/java/io/debezium/connector/postgresql/PostgresStreamingChangeEventSource.java#L118] are refreshing the schema of PostgresSourceFetchTaskContext. A new PostgresSourceFetchTaskContext is created in IncrementalSourceSplitReader#checkSplitOrStartNext for each split (both [SnapshotSplit|https://github.com/ververica/flink-cdc-connectors/blob/e0fd6f965b702cc2876372dc068379dafe066277/flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/IncrementalSourceSplitReader.java#L124] and [StreamSplit|https://github.com/ververica/flink-cdc-connectors/blob/e0fd6f965b702cc2876372dc068379dafe066277/flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/IncrementalSourceSplitReader.java#L134)]. For snapshot splits, even with the condition of [whether the currentFetcher is equal to null|https://github.com/ververica/flink-cdc-connectors/blob/e0fd6f965b702cc2876372dc068379dafe066277/flink-cdc-base/src/main/java/com/ververica/cdc/connectors/base/source/reader/IncrementalSourceSplitReader.java#L129], in many cases it still leads to the re creation of the PostgresSourceFetchTaskContext because the IncrementalSourceSplitReader is often discarded by the Flink kernal when a snapshot chunk is finished and become idle. See
1. [SourceReaderBase#pollNext|https://github.com/apache/flink/blob/c7beda0da81ffc4bbb01befafd2eed08b7b35854/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java#L149] -> [SourceReaderBase#finishedOrAvailableLater|https://github.com/apache/flink/blob/c7beda0da81ffc4bbb01befafd2eed08b7b35854/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java#L332] -> [SplitFetcherManager#maybeShutdownFinishedFetchers|https://github.com/apache/flink/blob/c7beda0da81ffc4bbb01befafd2eed08b7b35854/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/fetcher/SplitFetcherManager.java#L233) -> SplitFetcher#shutdown and SplitFetcher is removed. 
2. IncrementalSourceSplitReader(implements SplitReader] is [SplitFetcher#splitReader|https://github.com/apache/flink/blob/c7beda0da81ffc4bbb01befafd2eed08b7b35854/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/fetcher/SplitFetcher.java#L57] and removed also.

Based on the analysis of the above, we get two optimizations.
1. It's enough to refresh the schema when [PostgresSourceFetchTaskContext#configure|https://github.com/ververica/flink-cdc-connectors/blob/e0fd6f965b702cc2876372dc068379dafe066277/flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/source/fetch/PostgresSourceFetchTaskContext.java#L176] is called, and there is no need to refresh the schema afterwards.
2. Reuse PostgresSourceFetchTaskContext between SnapshotSplits based on sourceConfig, as PostgresSourceFetchTaskContext is created for almost every SnapshotSplit.

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2570
Created by: [lzshlzsh|https://github.com/lzshlzsh]
Labels: enhancement, 
Created at: Sun Oct 22 00:28:09 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:24:04.0,,,,,,,,,,"0|z1o3cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
oracle 19c PDB mode SplitFetcher thread 0 received unexpected exception while polling the records,FLINK-34819,13572598,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:24,20/Mar/24 09:24,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.14.2

### Flink CDC version

current

### Database and its version

oracle 19c

### Minimal reproduce step

CREATE TABLE products (
    db_name STRING METADATA FROM 'database_name' VIRTUAL,
    schema_name STRING METADATA FROM 'schema_name' VIRTUAL,
    table_name STRING METADATA  FROM 'table_name' VIRTUAL,
    operation_ts TIMESTAMP_LTZ(3) METADATA FROM 'op_ts' VIRTUAL,
    ID INT NOT NULL,
    NAME STRING,
    DESCRIPTION STRING,
    PRIMARY KEY(ID) NOT ENFORCED
) WITH (
      'connector' = 'oracle-cdc',
      'hostname' = 'localhost',
      'port' = '1521',
      'username' = 'c##flinkuser',
      'password' = 'flinkpw',
      'database-name' = 'ORCLCDB',
      'schema-name' = 'flink_pdb',
      'table-name' = 'products',
      'debezium.database.pdb.name' = 'ORCLPDB1',
      'scan.incremental.snapshot.enabled' = 'true'
--       'debezium.log.mining.strategy' = 'online_catalog'
--       'debezium.log.mining.continuous.mine' = 'true'
);

### What did you expect to see?

can use 'scan.incremental.snapshot.enabled' = 'true' option to read oracle change log

### What did you see instead?

java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:342)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: io.debezium.DebeziumException: The db history topic or its content is fully or partially missing. Please check database history topic configuration and re-execute the snapshot.
	at io.debezium.relational.HistorizedRelationalDatabaseSchema.recover(HistorizedRelationalDatabaseSchema.java:59)
	at com.ververica.cdc.connectors.oracle.source.reader.fetch.OracleSourceFetchTaskContext.validateAndLoadDatabaseHistory(OracleSourceFetchTaskContext.java:282)
	at com.ververica.cdc.connectors.oracle.source.reader.fetch.OracleSourceFetchTaskContext.configure(OracleSourceFetchTaskContext.java:116)
	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceStreamFetcher.submitTask(IncrementalSourceStreamFetcher.java:84)
	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.checkSplitOrStartNext(IncrementalSourceSplitReader.java:138)
	at com.ververica.cdc.connectors.base.source.reader.IncrementalSourceSplitReader.fetch(IncrementalSourceSplitReader.java:70)
	at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142]
	... 6 more
[flink-akka.actor.default-dispatcher-10] INFO org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy - Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.
[SourceCoordinator-Source: TableSourceScan(table=[[default_catalog, default_database, products]], fields=[ID, NAME, DESCRIPTION, database_name, schema_name, op_ts, table_name]) -> Calc(select=[CAST(database_name) AS db_name, CAST(schema_name) AS schema_name, CAST(table_name) AS table_name, CAST(op_ts) AS operation_ts, ID, NAME, DESCRIPTION]) -> NotNullEnforcer(fields=[ID]) -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[db_name, schema_name, table_name, operation_ts, ID, NAME, DESCRIPTION])] INFO org.apache.flink.runtime.source.coordinator.SourceCoordinator - Removing registered reader after failure for subtask 0 of source Source: TableSourceScan(table=[[default_catalog, default_database, products]], fields=[ID, NAME, DESCRIPTION, database_name, schema_name, op_ts, table_name]) -> Calc(select=[CAST(database_name) AS db_name, CAST(schema_name) AS schema_name, CAST(table_name) AS table_name, CAST(op_ts) AS operation_ts, ID, NAME, DESCRIPTION]) -> NotNullEnforcer(fields=[ID]) -> Sink: Sink(table=[default_catalog.default_database.print_table], fields=[db_name, schema_name, table_name, operation_ts, ID, NAME, DESCRIPTION]).

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2531
Created by: [yuangjiang|https://github.com/yuangjiang]
Labels: bug, 
Created at: Wed Sep 27 15:46:24 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:24:01.0,,,,,,,,,,"0|z1o3c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add external listeners to obtain split information,FLINK-34818,13572597,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

As what https://github.com/ververica/flink-cdc-connectors/pull/2453 described, we may need to add a mechanism to notify users about current split status.
[Debezium|https://debezium.io/documentation/reference/stable/configuration/notification.html#debezium-notifications-about-the-progress-of-incremental-snapshots] support notifications in 2.3, it's a helpful reference.

### Solution

1. Add event and listener definition. 
2. Notify listeners about the events in SourceEnumerator.

### Alternatives

Use Flink metric to check whether incremental reading is started

### Anything else?

Still some question to be defined:
1. What events should be contained?
2. Should we add some extra information about the slit of tables?

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2520
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Mon Sep 25 16:04:09 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:56.0,,,,,,,,,,"0|z1o3c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Can't map utf8mb4 given for characterSetResults to a supported MySQL encoding,FLINK-34817,13572596,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

flink：1.13.6


### Flink CDC version

flinkcdc：2.2.1

### Database and its version

DataBase： mysql 8.x


### Minimal reproduce step

When using Flinkcdc to collect MySQL, the fields in the MySQL table contain emoji emojis. If Flinkcdc is directly used to collect MySQL, emoji will become ????.
Therefore, jdbcProperty. setProperty (""characterSetResults"", ""utf8mb4"") has been set;
However, the following error was reported:
Caused by: com.mysql.cj.exceptions.WrongArgumentException: Can't map UTF8mb4 given for characterSetResults to a supported MySQL encoding.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61)
	at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:85)
	at com.mysql.cj.NativeSession.configureClientCharacterSet(NativeSession.java:561)
	at com.mysql.cj.jdbc.ConnectionImpl.initializePropsFromServer(ConnectionImpl.java:1316)
	at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:967)
	at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826)
	... 50 more

### What did you expect to see?

I hope that by setting jdbcProperty. setProperty (""characterSetResults"", ""utf8mb4""), emoji can be collected normally

### What did you see instead?

Caused by: com.mysql.cj.exceptions.WrongArgumentException: Can't map UTF8mb4 given for characterSetResults to a supported MySQL encoding.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61)
	at com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:85)
	at com.mysql.cj.NativeSession.configureClientCharacterSet(NativeSession.java:561)
	at com.mysql.cj.jdbc.ConnectionImpl.initializePropsFromServer(ConnectionImpl.java:1316)
	at com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:967)
	at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826]
	... 50 more

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2518
Created by: [gujincheng|https://github.com/gujincheng]
Labels: bug, 
Created at: Fri Sep 22 16:25:38 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:53.0,,,,,,,,,,"0|z1o3bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow custom Configurations for ConnectionPool,FLINK-34816,13572595,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

In https://github.com/ververica/flink-cdc-connectors/pull/534, we introduced Hikari connection pool to reduce connection usage.
Then, we added some predefined configurations like maximumPoolSize = 20, minimumIdle = 1, connectionTimeout = 30s.
Howerver, there are still some frequently-used configurations that we haven't allow user to set, as [configurations|https://github.com/brettwooldridge/HikariCP#frequently-used] showed, which may be useful like `keepaliveTime`.

It would be better to allow user to add custom pool configurations based on their condition, like what debezium.* and [jdbc.properties.*|https://github.com/ververica/flink-cdc-connectors/pull/921] do.



### Solution

allow custom pool configurations like `hikari.*`, and add them to `HikariConfig`

### Alternatives

_No response_

### Anything else?

Requires compatibility with previous parameters like `connection.pool.size` and `connect.timeout`, and jdbcUrl、driverClassName could not be modified too. 

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2509
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: enhancement, 
Created at: Wed Sep 20 11:51:23 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:49.0,,,,,,,,,,"0|z1o3bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Sqlserver transaction hung when get change data.,FLINK-34815,13572594,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.17.1

### Flink CDC version

master

### Database and its version

Sqlserver 2016SP3

### Minimal reproduce step

1. Prepare a long running flink cdc sql job to synchronize sqlserver data to postgresql
2. The Job works fine at first.
3. After a few days, the job will no longer synchronize data any more. and there is no error in flink log.


### What did you expect to see?

A long running job should be always works fine

### What did you see instead?

Hi, 
There is no flink error log but we found one sql blocked for a long time in sqlserver database:
```
(@P0 varbinary(8000),@P1 varbinary(8000)]
SELECT *, TODATETIMEOFFSET([XXXX].sys.fn_cdc_map_lsn_to_time([__$start_lsn]), DATEPART(TZOFFSET, SYSDATETIMEOFFSET())) 
FROM [XXXXXX].cdc.[fn_cdc_get_all_changes_XXXXX|@P0, @P1, N'all update old'] 
order by [__$start_lsn] ASC, [__$seqval] ASC, [__$operation] ASC
```

it looks the db transaction blocked for some reason when get change data, 
and we have checked code in SqlServerStreamingChangeEventSource.java(L460), 
dispatcher.dispatchDataChangeEvent()  blocked because it have no timeout settings for transaction.

Any suggestions about this issue?

Thanks a lot for your help ~
 

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2507
Created by: [YongkuiYang|https://github.com/YongkuiYang]
Labels: bug, 
Created at: Tue Sep 19 17:11:14 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:45.0,,,,,,,,,,"0|z1o3bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] java.lang.NoClassDefFoundError: Could not initialize class org.apache.kafka.connect.json.JsonConverterConfig,FLINK-34814,13572593,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.14.0

### Flink CDC version

2.3.0

### Database and its version

MySQL5.7

### Minimal reproduce step

 //设置相关ck的参数
        env.enableCheckpointing(360000L);
        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
        env.getCheckpointConfig().setCheckpointTimeout(360000L);
        env.getCheckpointConfig().setFailOnCheckpointingErrors(false);
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

        //cdc的相关参数
        Properties prop = new Properties();
        prop.setProperty(""snapshot.locking.mode"",""none"");

        //使用cdc来读取binlog日志
        DebeziumSourceFunction<String> dataSource = MySqlSource.<String>builder()
                .hostname("""")
                .port(3306)
                .username(""rp"")
                .password(""u5"")
                .databaseList(""iap"")
                .tableList(""iap.cdc_test_1"")
                .deserializer(new JsonDebeziumDeserializationSchema())
                .startupOptions(StartupOptions.latest())
                .debeziumProperties(prop)
                .build();

        SingleOutputStreamOperator<String> streamSource = env.addSource(dataSource).name(""data-source"");
        streamSource.print(""======>""+streamSource];
but it causes an error:
 java.lang.NoClassDefFoundError: Could not initialize class org.apache.kafka.connect.json.JsonConverterConfig

### What did you expect to see?

run success

### What did you see instead?

success

### Anything else?

no

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2491
Created by: [ccc66666|https://github.com/ccc66666]
Labels: bug, 
Created at: Thu Sep 14 11:14:15 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:23:39 UTC 2024,,,,,,,,,,"0|z1o3b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:23;flink-cdc-import;Date: Thu Nov 23 14:51:29 CST 2023, Author: [mayaming|https://github.com/mayaming]

I encounter the same problem and the reason is the version of my flink kafka connector is too low.

It's NOT a problem that JsonConverterConfig is not found. In my case the class could be found but fails in static block initialization:

```java
CONFIG.define(
  DECIMAL_FORMAT_CONFIG, Type.STRING, DECIMAL_FORMAT_DEFAULT,
  ConfigDef.CaseInsensitiveValidString.in(
    DecimalFormat.BASE64.name(),
    DecimalFormat.NUMERIC.name()),
  Importance.LOW, DECIMAL_FORMAT_DOC, group, orderInGroup++,
  Width.MEDIUM, DECIMAL_FORMAT_DISPLAY);
```
Here ConfigDef is defined in org.apache.kafka:kafka-clients, and CaseInsensitiveValidString is not available if your version is too low for some reason. I upgrade my kafka connector the problem is gone.

You could check your JsonConverterConfig code if there's any problem in its static block initialization. Hope it helps.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Using tidb-cdc to connect to the data source cpu usage is always high,FLINK-34813,13572592,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

17.1

### Flink CDC version

        <dependency>
            <groupId>com.ververica</groupId>
            <artifactId>flink-connector-tidb-cdc</artifactId>
            <version>2.4.1</version>
        </dependency>

### Database and its version

tidb 5.4.3

### Minimal reproduce step

        String xxx = ""CREATE TABLE xxx ("" +
                ""  `id` STRING ,\n"" +
                ""  PRIMARY KEY (`id`) NOT ENFORCED"" +
                "") WITH ("" +
                ""  'connector' = 'tidb-cdc',\n"" +
                "" 'tikv.grpc.timeout_in_ms' = '00000', "" +
                ""  'pd-addresses' = '127.0.0.1:2379',\n"" +
                ""  'database-name' = 'xxx',"" +
                ""  'table-name' = 'xxx'"" +
                "")"";

        tableEnv.executeSql(xxx);

        String xxx = ""CREATE TABLE xxx ("" +
                ""  `id` STRING ,\n"" +
                
                ""  PRIMARY KEY (`id`) NOT ENFORCED"" +
                "") WITH ("" +
                ""    'connector' = 'jdbc',\n"" +
                ""    'url' = 'jdbc:mysql://xxx/xxx?serverTimezone=Asia/Shanghai&useUnicode=true&characterEncoding=utf8&useSSL=true',\n"" +
                ""    'username' = 'xxx',"" +
                ""    'password' = 'xxx',"" +
                ""    'lookup.cache.max-rows' = '10000',"" +
                ""    'lookup.cache.ttl' = '5s',"" +
                "" 'table-name' = 'xxx'"" +
                "")"";

        tableEnv.executeSql(xxx);
                statementSet.addInsertSql(
                ""insert into xxx select * from xxx ""];

### What did you expect to see?

After the synchronization is completed, the cpu usage is reduced

### What did you see instead?

After starting the job, the CPU usage is always very high.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2473
Created by: [tuyanzhang|https://github.com/tuyanzhang]
Labels: bug, 
Created at: Fri Sep 08 17:54:27 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:33.0,,,,,,,,,,"0|z1o3aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Bug] During the snapshot phase, projection is not being performed according to the user defined schema.",FLINK-34812,13572591,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.17.0

### Flink CDC version

<=master

### Database and its version

such as mysql,oracle,pg,sqlserver,take oracle 11g as an example.

### Minimal reproduce step

Assuming the ABC table contains three fields, A, B, and C.

**ddl:**
CREATE TABLE ABC (
     &emsp;A BIGINT NOT NULL,
     &emsp;B STRING,
     &emsp;PRIMARY KEY(A) NOT ENFORCED
     ) WITH (
     &emsp;'connector' = 'oracle-cdc',
     &emsp;'hostname' = '192.168.xxx.xxx',
     &emsp;'port' = '1521',
     &emsp;'username' = 'xxx',
     &emsp;'password' = 'xxx',
     &emsp;'database-name' = 'xxx',
     &emsp;'schema-name' = 'xxx',
     &emsp;'table-name' = 'ABC',
     &emsp;'debezium.log.mining.strategy' = 'online_catalog',
     &emsp;'debezium.log.mining.continuous.mine' = 'true',
     &emsp;'debezium.database.tablename.case.insensitive' = 'false'];

**dml:**
select * from ABC;

### What did you expect to see?

**should be:**
![查部分|https://github.com/ververica/flink-cdc-connectors/assets/57552918/b8a302e8-63dd-42ca-b2f2-67e4bd356bfa]
Projection is performed according to the schema definition, and it takes 2.534 seconds.

### What did you see instead?

**snapshot phase:**
![查全部|https://github.com/ververica/flink-cdc-connectors/assets/57552918/6504278f-4a00-43ba-8818-558a732c3f82]
The screenshot indicates that an `*` was used as the query condition, and projection was not performed as defined by the schema. And it took 3.532 seconds to execute this query.

### Anything else?

When dealing with a large amount of data and numerous partitions, this time can become quite significant.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2470
Created by: [hzjhjjyy|https://github.com/hzjhjjyy]
Labels: bug, 
Created at: Fri Sep 08 09:43:07 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:23:31 UTC 2024,,,,,,,,,,"0|z1o3ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:23;flink-cdc-import;Date: Mon Sep 11 20:38:00 CST 2023, Author: [hzjhjjyy|https://github.com/hzjhjjyy]

Is there any issue? or can I submit a pr?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Bug]  [sqlserver cdc] the field ""snapshot"" is always ""false"" ",FLINK-34811,13572590,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.13.6

### Flink CDC version

2.5-SNAPSHOT

### Database and its version

sqlserver 2016

### Minimal reproduce step

execute demo like
`        SqlServerSourceBuilder.SqlServerIncrementalSource<String> sqlServerSource = new SqlServerSourceBuilder()
                .hostname(""xxx"")
                .port(1433)
                .databaseList(""xxx"") // monitor sqlserver database
                .tableList(""dbo.xxx"") // monitor products table
                .username(""xxx"")
                .password(""xxx"")
                .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String
                .startupOptions(StartupOptions.initial())
                .build();
        DataStream<String> originDataStream = env.fromSource(sqlServerSource,WatermarkStrategy.noWatermarks(),""cdc"");
        originDataStream.print();
        env.execute(""Print SqlServer Snapshot + Change Stream""];`



### What did you expect to see?

I can distinguish whether the data printed is snapshot or incremental by the 'snapshot' field

### What did you see instead?

the field ""snapshot"" is always ""false"" 

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2465
Created by: [edmond-kk|https://github.com/edmond-kk]
Labels: bug, 
Created at: Wed Sep 06 17:31:20 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:25.0,,,,,,,,,,"0|z1o3ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[mysql] Add a module to periodically check ip on DNS (for DB Switching),FLINK-34810,13572589,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

First, we'd like to thank everyone who contributes to Flink CDC.

Our team uses Flink CDC to perform MySql CDC.

In the real-world DB use case, DBs are organized in a master-slave structure and DB switching occurs periodically.
When DB Switching occurs, **the connected slave server is promoted to the master server.**

--- 

The problem is that flink CDC connector keeps the connection, but **in the real-world, there is a policy that it should not be attached to the master server of DB except for some permitted processes.**

For resolve this issue, our team implemented DnsIpChecker modules. (We saw [similar issue and answer|https://github.com/ververica/flink-cdc-connectors/issues/2419))

Here's how it works
1. Allocate one thread at the time of `MySqlSourceReader(subtaskId=0)` creation. (to share lifecycle of `MySqlSourceReader`)
2. That thread periodically checks the IP in DNS based on the FQDN. 
3. If the IP changes, it raises a `FlinkRuntimeException` by sending a `SourceEvent` to `MySqlSourceEnumerator` to restart the Flink Job to reset the DB connection.
4. Add a close function to `MySqlSourceReader`, so that the thread is also terminated when the Flink CDC Job is terminated, 

This is portion of our code. We assumed that we only capture 1 table per flink cdc job

```scala
  def getMySQLSourceOperator(]: MySqlSource[String] = {
    MySqlSource.builder[String|)
      .hostname(mySqlConfig.host)
      .port(mySqlConfig.port)
      .serverTimeZone(mySqlConfig.timeZone)
      .databaseList(mySqlConfig.database)
      .tableList(mySqlConfig.table)
      .username(mySqlConfig.user)
      .serverId(mySqlConfig.serverIdRange)
      .password(mySqlConfig.password)
      .startupOptions(mySqlConfig.startupMode)
      .fetchSize(mySqlConfig.fetchSize)
      .splitSize(mySqlConfig.splitSize)
      .chunkKeyColumn(new ObjectPath(mySqlConfig.database, mySqlConfig.table), mySqlConfig.chunkKeyColumn)
      .connectionPoolSize(mySqlConfig.poolSize)
      .scanNewlyAddedTableEnabled(false)
      .includeSchemaChanges(false)
      .debeziumProperties(mySqlConfig.dbzProps)
      .closeIdleReaders(true)
      .restartOnDbSwitch(true) // here what we implemented
      .deserializer(new JsonDebeziumDeserializationSchema(true, mySqlConfig.jsonConverterProps))
      .build()
  }
```

We considered just resetting the DB connection only...
but it was difficult to get a recovery point, so we chose to restart the Flink Job for using Flink's checkpoints.

---

Like the log below, we also take periodic logs to check if the IP scan is performing normally. 
(Hostname and ip masked]
```d
2023-09-02 07:10:45.003 INFO  com.ververica.cdc.connectors.mysql.source.utils.DNSIpChecker [] - [264361] Current IP address for hostname test_host_name.db.server is XXX
...
2023-09-02 07:11:45.026 INFO  com.ververica.cdc.connectors.mysql.source.utils.DNSIpChecker [] - [264421] Current IP address for hostname test_host_name.db.server is XXX
```


### Solution

Our implementation and PR is here. https://github.com/ververica/flink-cdc-connectors/pull/2458

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2456
Created by: [SML0127|https://github.com/SML0127]
Labels: enhancement, 
Created at: Sat Sep 02 15:17:58 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:21.0,,,,,,,,,,"0|z1o3a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[mysql] Add notifications to Slack when the Snapshot phase ends or Binlog stream phase begins,FLINK-34809,13572588,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

On our team, we use Flink CDC to perform MySql CDC.

Since [there is no Snapshot Only mode for MySql yet|https://github.com/ververica/flink-cdc-connectors/issues/1687], we had a need to be notified when a snapshot is completed and when the binlog stream is started.

To accomplish this, we **implemented a notification when a snapshot ends and when a binlog stream starts with  GTIDs.**

--- 

Here's the team's use case in more detail 
1. We set parallelism to 2 or more for large tables.
2. And we send change event log to kafka to use Debezium's JDBC Sink Connector, which supports [Schema Evolution|https://debezium.io/documentation/reference/stable/connectors/jdbc.html#jdbc-schema-evolution).
3. Sinking to Kafka is slower than MySqlSource operator, so we give more paralleisms to sink operator more parallelism than MySqlSource.
4. In this case, the transfer is done in rebalance mode from source operator to sink operator, so the order for the same PK is not guaranteed when transferring binlogs.
5. So we restart the job based on GTIDs with parallelism equal to 1 at the end of the snapshot phase .

To do this, we needed (1) to be notified that the snapshot ended and the binlog stream started, and (2) to know from which GTIDs the binlog stream started.

---

This is portion of our code. We **assumed that we only capture 1 table per flink cdc job.**
```scala
  def getMySQLSourceOperator(]: MySqlSource[String] = {
    MySqlSource.builder[String|)
      .hostname(mySqlConfig.host)
      .port(mySqlConfig.port)
      .serverTimeZone(mySqlConfig.timeZone)
      .databaseList(mySqlConfig.database)
      .tableList(mySqlConfig.table)
      .username(mySqlConfig.user)
      .serverId(mySqlConfig.serverIdRange)
      .password(mySqlConfig.password)
      .startupOptions(mySqlConfig.startupMode)
      .fetchSize(mySqlConfig.fetchSize)
      .splitSize(mySqlConfig.splitSize)
      .chunkKeyColumn(new ObjectPath(mySqlConfig.database, mySqlConfig.table), mySqlConfig.chunkKeyColumn)
      .connectionPoolSize(mySqlConfig.poolSize)
      .scanNewlyAddedTableEnabled(false)
      .includeSchemaChanges(false)
      .debeziumProperties(mySqlConfig.dbzProps)
      .closeIdleReaders(true)
      .notifySnapshotToBinlogSwitch(""slack-hook-url"") // here what we implemented
      .deserializer(new JsonDebeziumDeserializationSchema(true, mySqlConfig.jsonConverterProps))
      .build()
  }
```

--- 

We can't share a real picture of the notification, because our company recommends using in-house tools rather than Slack(🥲🥲🥲] and has some security policy.

But it looks something like the format below!
- Snapshot finished notifiaction.
```
[SNAPSHOT FINISHED]
 Database: test_database
 Table: test_table
```
- Binlog stream start notification.
```
[BINLOG STREAM START]
 Database: test_database
 Table: test_table
 GTIDs: 3bda59bb-2fc8-11eb-855f-fa163e2550e3:1-128377129,3c3a6a1b-c931-11ed-b0db-b4055dec129e:1-273703345,901d637c-8add-11eb-8e3f-b4055d3355a6:1-3641352422,b46b8251-5254-11ed-a648-d0946637df48:1-1069556331,db449f07-c53e-11e8-b8c2-d094663d3d1d:1-3543229125,e8d62f95-c77a-11e8-9270-d0946637df48:1-5715021533
```


### Solution

Our implementation and PR is here. https://github.com/ververica/flink-cdc-connectors/pull/2453

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2454
Created by: [SML0127|https://github.com/SML0127]
Labels: enhancement, 
Created at: Sat Sep 02 14:34:50 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:17.0,,,,,,,,,,"0|z1o3a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug]  Mysql cdc connector  connection pool,FLINK-34808,13572587,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.16.0

### Flink CDC version

2.3.0

### Database and its version

mysql 8.0.32

### Minimal reproduce step

using flink-sql-connector-mysql-cdc:2.3.0，
Synchronizing a substantial number of MySQL tables, around 100 in quantity, within a single job.

### What did you expect to see?

No errors were reported.






### What did you see instead?


org.apache.flink.util.FlinkRuntimeException: org.apache.flink.util.FlinkRuntimeException: java.sql.SQLTransientConnectionException: connection-pool-mysqltest-primary.mysql-bitnami:3306 - Connection is not available, request timed out after 30002ms.
at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.openJdbcConnection(DebeziumUtils.java:64] ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
at com.ververica.cdc.connectors.mysql.MySqlValidator.validate(MySqlValidator.java:72) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
at com.ververica.cdc.connectors.mysql.source.MySqlSource.createEnumerator(MySqlSource.java:170) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
at org.apache.flink.runtime.source.coordinator.SourceCoordinator.start(SourceCoordinator.java:222) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.applyCall(RecreateOnResetOperatorCoordinator.java:315) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.start(RecreateOnResetOperatorCoordinator.java:70) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:198) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:165) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:82) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:605) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:1046) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:963) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:422) ~[flink-runtime-1.16.0.jar:1.16-SNAPSHOT]
at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:198) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:622) ~[flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:621) ~[flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:190) ~[flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_7e7f5d74-19da-43b1-93a0-b22723578747.jar:1.16.0]
at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
Caused by: org.apache.flink.util.FlinkRuntimeException: java.sql.SQLTransientConnectionException: connection-pool-mysqltest-primary.mysql-bitnami:3306 - Connection is not available, request timed out after 30002ms.
at com.ververica.cdc.connectors.mysql.source.connection.JdbcConnectionFactory.connect(JdbcConnectionFactory.java:72) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
at io.debezium.jdbc.JdbcConnection.connection(JdbcConnection.java:890) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
at io.debezium.jdbc.JdbcConnection.connection(JdbcConnection.java:885) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
at io.debezium.jdbc.JdbcConnection.connect(JdbcConnection.java:418) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
at com.ververica.cdc.connectors.mysql.debezium.DebeziumUtils.openJdbcConnection(DebeziumUtils.java:61) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
... 37 more

### Anything else?

When using flink-sql-connector-mysql-cdc:2.3.0, I encountered an error upon task startup. It appears to be related to the unavailability of connections in the MySQL connection pool.
Adding 'connection.pool.size' ='200' resolves the issue mentioned above, and the task can execute normally.

However, in version 2.2.0, I didn't encounter this error. Without adding the aforementioned parameter, there are no issues, and the task can run normally.

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2441
Created by: [nathan-szz|https://github.com/nathan-szz]
Labels: bug, 
Created at: Tue Aug 29 18:26:27 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:23:14 UTC 2024,,,,,,,,,,"0|z1o39s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:23;flink-cdc-import;Date: Mon Sep 04 13:57:03 CST 2023, Author: [Wintle|https://github.com/Wintle]

+1;;;","20/Mar/24 09:23;flink-cdc-import;Date: Wed Oct 11 18:41:23 CST 2023, Author: [zjf0128|https://github.com/zjf0128]

+1;;;","20/Mar/24 09:23;flink-cdc-import;Date: Wed Dec 13 09:58:28 CST 2023, Author: [nathan-szz|https://github.com/nathan-szz]

@leonardBang  @wuchong
Can you provide any information to help？thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][mysql] Mysql-cdc source adapts mariadb.,FLINK-34807,13572586,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Currently, MySQL-CDC Source will have the following problems when reading MariaDB:
1. Can not receive mariadb gtidSet event from mariadb server
2. Can not compare mariadb gtidSet when restart from specific binlog offset/

### Solution

1. overrride BinaryLogClient.requestBinaryLogStreamMaria method to get mariadb gtidSet event
2. Identify gtidSet type in BinlogOffset.compareTo method.

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2398
Created by: [ThisisWilli|https://github.com/ThisisWilli]
Labels: enhancement, 
Created at: Wed Aug 16 12:11:05 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:23:08.0,,,,,,,,,,"0|z1o39k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Feature][Postgres] Support automatically identify newly added tables,FLINK-34806,13572585,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:23,20/Mar/24 09:23,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

When we start a job with a regular expression, if a new table name matches the regular expression and is created, the event emitted by the table will not be captured and IllegalArgumentException will be thrown, causing the entire flash job to fail over.

### Solution

When instantiating EventDispatcher, InconsistenceSchemaHandler is passed in, and all internal schema information is refreshed at this time.

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2397
Created by: [TyrantLucifer|https://github.com/TyrantLucifer]
Labels: enhancement, 
Created at: Tue Aug 15 19:35:17 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:23:05 UTC 2024,,,,,,,,,,"0|z1o39c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:23;flink-cdc-import;Date: Mon Feb 05 17:36:24 CST 2024, Author: [loserwang1024|https://github.com/loserwang1024]

Great catch! Now debezium version has been upgraded to 1.9.8, maybe we can do some changes and then rebase this PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] About PostgresSql snapshot sql ,FLINK-34805,13572584,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,09/Apr/24 06:00,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,pull-request-available,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.14.6

### Flink CDC version

2.4.1

### Database and its version

PostgreSQL 13.2

### Minimal reproduce step


 
In the version of flink-connector-postgres-cdc  2.4.1, snapshot SQL is generated during a full snapshot and is split according to the command of key,When running POSTGRESQL  SQL, the column will be case-sensitive. But in fact, when the SQL is generated, double quotation marks are not added to this column. Therefore POSTGRESQL converts this column to lowercase, and lead to POSTGRESQL cannot identify this column, resulting in an error.
![638901E5-7EC3-41D2-88F6-8AA4D12D54FE|https://github.com/ververica/flink-cdc-connectors/assets/94167908/7448282b-9f72-4512-b433-6a9c19bb7f6e]


package com.ververica.cdc.connectors.postgres.source.utils;

![6027352F-2643-4D3B-9D75-4CE712CBAC27|https://github.com/ververica/flink-cdc-connectors/assets/94167908/4d4d0084-a5e9-49c7-8812-8da0df8f489b]
![9966F5AB-0E48-4600-8BC6-FDF6480E7297|https://github.com/ververica/flink-cdc-connectors/assets/94167908/f937df43-d277-4495-8ca9-24460e967ece]



### What did you expect to see?

“key”

### What did you see instead?

key

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2375
Created by: [linzhicong1996|https://github.com/linzhicong1996]
Labels: bug, 
Created at: Tue Aug 08 20:13:07 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:22:59 UTC 2024,,,,,,,,,,"0|z1o394:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:22;flink-cdc-import;Date: Wed Aug 09 13:59:51 CST 2023, Author: [linzhicong1996|https://github.com/linzhicong1996]

I modified the relevant code and the program is already working, but because I am not familiar with the whole program, I don't know if it will raise the related GUG

![0516B4A0-7C1B-4258-A95F-E1BE08DAF2A4|https://github.com/ververica/flink-cdc-connectors/assets/94167908/0692976c-a716-46eb-8cdb-08ccbb32d588]
;;;","20/Mar/24 09:22;flink-cdc-import;Date: Mon Aug 14 15:50:14 CST 2023, Author: [ruanhang1993|https://github.com/ruanhang1993]

@linzhicong1996 Assign this to you. Please ping me after you raise the PR. Thanks.;;;","20/Mar/24 09:22;flink-cdc-import;Date: Mon Aug 14 21:30:24 CST 2023, Author: [linzhicong1996|https://github.com/linzhicong1996]

@ruanhang1993 Code has been submitted.

[postgres] PostgresSql snapshot sql ([#2389|https://github.com/apache/flink-cdc/issues/2389] | [FLINK-2389|https://issues.apache.org/jira/browse/FLINK-2389]);;;","20/Mar/24 09:22;flink-cdc-import;Date: Mon Feb 05 18:01:29 CST 2024, Author: [loserwang1024|https://github.com/loserwang1024]

I don't understand why PostgresDialect#isDataCollectionIdCaseSensitive always returns true, meaning that is database always use case-sensitive strategy, but this PR will change it to case-insensitive?

What kind of strategy is actually needed? @ruanhang1993 ， CC, WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] flink cdc missing step to close mysql connection,FLINK-34804,13572583,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [x] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

flink-1.17.0

### Flink CDC version

flink-sql-connector-mysql-cdc-2.4.1

### Database and its version

5.7.23

### Minimal reproduce step

flink cdc missing step to close mysql connection.

mysql> select * from information_schema.PROCESSLIST where user='test__cdc' order by id;
+----------+-----------+-------------------+----------------+------------------+------+---------------------------------------------------------------+------+
| ID       | USER      | HOST              | DB             | COMMAND          | TIME | STATE                                                         | INFO |
+----------+-----------+-------------------+----------------+------------------+------+---------------------------------------------------------------+------+
| 73165285 | test__cdc | 88.88.88.88:47874 | test___cdc__db | Sleep            | **2181** |                                                               | NULL |
| 73165286 | test__cdc | 88.88.88.88:47876 | NULL           | Sleep            | **2181** |                                                               | NULL |
| 73165293 | test__cdc | 88.88.88.88:47898 | NULL           | Sleep            | **2180** |                                                               | NULL |
| 73165294 | test__cdc | 88.88.88.88:47900 | NULL           | Binlog Dump GTID | 2180 | Master has sent all binlog to slave; waiting for more updates | NULL |
+----------+-----------+-------------------+----------------+------------------+------+---------------------------------------------------------------+------+


[thread 73165285]
2023-08-07 16:31:11.564220	 88.88.88.88:47874	 73165285	 test__cdc	[SQL [LOGIN] SQL]
2023-08-07 16:31:11.565508	 88.88.88.88:47874	 73165285	 test__cdc	[SQL /* mysql-connector-java-8.0.28 (Revision: [7ff2161da3899f379fb3171b6538b191b1c5c7e2|https://github.com/apache/flink-cdc/commit/7ff2161da3899f379fb3171b6538b191b1c5c7e2]) */SELECT  @@session.auto_increment_increment AS auto_increment_increment, @@character_set_client AS character_set_client, @@character_set_connection AS character_set_connection, @@character_set_results AS character_set_results, @@character_set_server AS character_set_server, @@collation_server AS collation_server, @@collation_connection AS collation_connection, @@init_connect AS init_connect, @@interactive_timeout AS interactive_timeout, @@license AS license, @@lower_case_table_names AS lower_case_table_names, @@max_allowed_packet AS max_allowed_packet, @@net_write_timeout AS net_write_timeout, @@performance_schema AS performance_schema, @@query_cache_size AS query_cache_size, @@query_cache_type AS query_cache_type, @@sql_mode AS sql_mode, @@system_time_zone AS system_time_zone, @@time_zone AS time_zone, @@transaction_isolation AS transaction_isolation, @@wait_timeout AS wait_timeout SQL]
2023-08-07 16:31:11.566643	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SHOW WARNINGS SQL]
2023-08-07 16:31:11.567953	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SET autocommit=1 SQL]
2023-08-07 16:31:11.569078	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SELECT @@session.transaction_read_only SQL]
2023-08-07 16:31:11.571336	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SELECT VERSION() SQL]
2023-08-07 16:31:11.573046	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SHOW GLOBAL VARIABLES LIKE 'binlog_format' SQL]
2023-08-07 16:31:11.574562	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SHOW GLOBAL VARIABLES LIKE 'binlog_row_image' SQL]
2023-08-07 16:31:11.575901	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SELECT TIME_TO_SEC(TIMEDIFF(NOW(), UTC_TIMESTAMP())) SQL]
2023-08-07 16:31:11.642133	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SHOW CREATE TABLE `test___cdc__db`.`test_cdc_table` SQL]
2023-08-07 16:31:11.654385	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SELECT MIN(`machine_name`), MAX(`machine_name`) FROM `test___cdc__db`.`test_cdc_table` SQL]
2023-08-07 16:31:11.655673	 88.88.88.88:47874	 73165285	 test__cdc	[SQL USE `test___cdc__db`; SQL]
2023-08-07 16:31:11.660119	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SHOW TABLE STATUS LIKE 'test_cdc_table'; SQL]
2023-08-07 16:31:11.662946	 88.88.88.88:47874	 73165285	 test__cdc	[SQL SELECT MAX(`machine_name`) FROM (SELECT `machine_name` FROM `test___cdc__db`.`test_cdc_table` WHERE `machine_name` >= 'SVR8888' ORDER BY `machine_name` ASC LIMIT 8096) AS T SQL]
**[missing step to close connection]**

[thread 73165286]
2023-08-07 16:31:11.573639	 88.88.88.88:47876	 73165286	 test__cdc	[SQL [LOGIN] SQL]
2023-08-07 16:31:11.574619	 88.88.88.88:47876	 73165286	 test__cdc	[SQL /* mysql-connector-java-8.0.28 (Revision: [7ff2161da3899f379fb3171b6538b191b1c5c7e2|https://github.com/apache/flink-cdc/commit/7ff2161da3899f379fb3171b6538b191b1c5c7e2]) */SELECT  @@session.auto_increment_increment AS auto_increment_increment, @@character_set_client AS character_set_client, @@character_set_connection AS character_set_connection, @@character_set_results AS character_set_results, @@character_set_server AS character_set_server, @@collation_server AS collation_server, @@collation_connection AS collation_connection, @@init_connect AS init_connect, @@interactive_timeout AS interactive_timeout, @@license AS license, @@lower_case_table_names AS lower_case_table_names, @@max_allowed_packet AS max_allowed_packet, @@net_write_timeout AS net_write_timeout, @@performance_schema AS performance_schema, @@query_cache_size AS query_cache_size, @@query_cache_type AS query_cache_type, @@sql_mode AS sql_mode, @@system_time_zone AS system_time_zone, @@time_zone AS time_zone, @@transaction_isolation AS transaction_isolation, @@wait_timeout AS wait_timeout SQL]
2023-08-07 16:31:11.575502	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SHOW WARNINGS SQL]
2023-08-07 16:31:11.576469	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SET autocommit=1 SQL]
2023-08-07 16:31:11.577338	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SELECT @@session.transaction_read_only SQL]
2023-08-07 16:31:11.587311	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SHOW DATABASES SQL]
2023-08-07 16:31:11.588581	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SHOW FULL TABLES IN `information_schema` where Table_Type = 'BASE TABLE' SQL]
2023-08-07 16:31:11.598959	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SHOW FULL TABLES IN `magidb` where Table_Type = 'BASE TABLE' SQL]
2023-08-07 16:31:11.600502	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SHOW FULL TABLES IN `mysql` where Table_Type = 'BASE TABLE' SQL]
2023-08-07 16:31:11.606279	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SHOW FULL TABLES IN `performance_schema` where Table_Type = 'BASE TABLE' SQL]
2023-08-07 16:31:11.630433	 88.88.88.88:47876	 73165286	 test__cdc	[SQL SHOW FULL TABLES IN `sys` where Table_Type = 'BASE TABLE' SQL]
**[missing step to close connection]**

[thread 73165293]
2023-08-07 16:31:12.675640	 88.88.88.88:47898	 73165293	 test__cdc	[SQL [LOGIN] SQL]
2023-08-07 16:31:12.676587	 88.88.88.88:47898	 73165293	 test__cdc	[SQL /* mysql-connector-java-8.0.28 (Revision: [7ff2161da3899f379fb3171b6538b191b1c5c7e2|https://github.com/apache/flink-cdc/commit/7ff2161da3899f379fb3171b6538b191b1c5c7e2]) */SELECT  @@session.auto_increment_increment AS auto_increment_increment, @@character_set_client AS character_set_client, @@character_set_connection AS character_set_connection, @@character_set_results AS character_set_results, @@character_set_server AS character_set_server, @@collation_server AS collation_server, @@collation_connection AS collation_connection, @@init_connect AS init_connect, @@interactive_timeout AS interactive_timeout, @@license AS license, @@lower_case_table_names AS lower_case_table_names, @@max_allowed_packet AS max_allowed_packet, @@net_write_timeout AS net_write_timeout, @@performance_schema AS performance_schema, @@query_cache_size AS query_cache_size, @@query_cache_type AS query_cache_type, @@sql_mode AS sql_mode, @@system_time_zone AS system_time_zone, @@time_zone AS time_zone, @@transaction_isolation AS transaction_isolation, @@wait_timeout AS wait_timeout SQL]
2023-08-07 16:31:12.677424	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SHOW WARNINGS SQL]
2023-08-07 16:31:12.678321	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SET autocommit=1 SQL]
2023-08-07 16:31:12.683095	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SHOW MASTER STATUS SQL]
2023-08-07 16:31:12.684154	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SELECT GTID_SUBTRACT('5015a2c3-6a73-11ea-a91b-246e96882760:1-5303826168,b65182c3-b201-11ed-9217-0c42a19a1d62:1-7696392432', '5015a2c3-6a73-11ea-a91b-246e96882760:1-5303826168,b65182c3-b201-11ed-9217-0c42a19a1d62:1-7696391849') SQL]
2023-08-07 16:31:12.685018	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SELECT @@global.gtid_purged SQL]
2023-08-07 16:31:12.685887	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SELECT GTID_SUBTRACT('b65182c3-b201-11ed-9217-0c42a19a1d62:7696391850-7696392432', '5015a2c3-6a73-11ea-a91b-246e96882760:1-5303826168,b65182c3-b201-11ed-9217-0c42a19a1d62:1-7694307400') SQL]
2023-08-07 16:31:12.689066	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SHOW GLOBAL VARIABLES LIKE 'GTID_MODE' SQL]
2023-08-07 16:31:12.689930	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SHOW MASTER STATUS SQL]
2023-08-07 16:31:12.690788	 88.88.88.88:47898	 73165293	 test__cdc	[SQL SELECT @@global.gtid_purged SQL]
**[missing step to close connection]**

[thread 73165294]
2023-08-07 16:31:12.693765	 88.88.88.88:47900	 73165294	 test__cdc	[SQL [LOGIN] SQL]
2023-08-07 16:31:12.694955	 88.88.88.88:47900	 73165294	 test__cdc	[SQL show global variables like 'binlog_checksum' SQL]
2023-08-07 16:31:12.695767	 88.88.88.88:47900	 73165294	 test__cdc	[SQL set @master_binlog_checksum= @@global.binlog_checksum SQL]
2023-08-07 16:31:12.696551	 88.88.88.88:47900	 73165294	 test__cdc	[SQL select @@server_id SQL]
2023-08-07 16:31:12.697329	 88.88.88.88:47900	 73165294	 test__cdc	[SQL set @master_heartbeat_period=48000000000 SQL]


### What did you expect to see?

After successful initialization, only one dump thread should be retaine.

### What did you see instead?

After successful initialization, have 4 mysql threads.
One is dump thread, and the other three are zombie threads.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2371
Created by: [vzong|https://github.com/vzong]
Labels: bug, 
Created at: Mon Aug 07 17:06:06 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:22:52.0,,,,,,,,,,"0|z1o38w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support cdc catalog ,FLINK-34803,13572582,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

implify use and avoid create table ddl

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2370
Created by: [melin|https://github.com/melin]
Labels: enhancement, 
Created at: Mon Aug 07 10:58:20 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:22:50 UTC 2024,,,,,,,,,,"0|z1o38o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:22;flink-cdc-import;Date: Mon Aug 07 11:39:23 CST 2023, Author: [ThisisWilli|https://github.com/ThisisWilli]

I'm willing to submit a PR，please assign this task to me～ @ruanhang1993 @leonardBang ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Paimon CDC ingestion with mysql-cdc 2.4.1 cannot receive complete Debezium binlog,FLINK-34802,13572581,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.15.2

### Flink CDC version

2.4.1

### Database and its version

MySQL:
5.7.31-log MySQL Community Server (GPL)

### Minimal reproduce step

1. submit a Paimon database synchronization job with command:

```
./bin/flink run \
    ./lib/paimon-flink-action-0.5-SNAPSHOT.jar \
    mysql-sync-database \
    --warehouse oss://odps-prd/rtdp/paimon \
    --database flink \
    --ignore-incompatible true \
    --table-prefix ods_ \
    --mode combined \
    --mysql-conf hostname=127.0.0.1 \
    --mysql-conf username=flink \
    --mysql-conf password=flink \
    --mysql-conf database-name=flink \
    --catalog-conf metastore=hive \
    --catalog-conf uri=thrift://localhost:9083 \
    --table-conf bucket=2 \
    --table-conf changelog-producer=input \
    --table-conf sink.parallelism=8 \
    --table-conf page-size=4k \
    --table-conf write-buffer-size=512mb \
    --table-conf num-sorted-run.compaction-trigger=5 \
    --table-conf num-sorted-run.stop-trigger=2147483647 \
    --table-conf sort-spill-threshold=10 \
    --table-conf write-buffer-spillable=true
```

2. After the job steps into Incremental Phase, use MySQL Workbench to create a new table and insert some data:
```
create table Student(Sno char(9) primary key,Sname char(20) not null,Ssex char(2),Sage smallint,Sdept char(20))
```

3. An error occurs (this is in Paimon):
```
java.lang.IllegalArgumentException: Invalid historyRecord, because tableChanges should contain exactly 1 item.
```

The wrong hisroryRecord is:
```
{
  ""source"": {
    ""file"": ""mysql-bin.000126"",
    ""pos"": 103181,
    ""server_id"": 1
  },
  ""position"": {
    ""ts_sec"": 1690955907,
    ""file"": ""mysql-bin.000126"",
    ""pos"": 103368,
    ""server_id"": 1
  },
  ""databaseName"": ""flink"",
  ""ddl"": ""create table Student(Sno char(9) primary key,Sname char(20) not null,Ssex char(2),Sage smallint,Sdept char(20)]"",
  ""tableChanges"": []
}
```


### What did you expect to see?

the `tableChanges` should contains data.

### What did you see instead?

the `tableChanges` is empty.

### Anything else?

1. we test that use debezium to consume mysql record directly and the result is correct.  We have tested two Debezium versions that are same to 2.3.0 and 2.4.1 .
2. This job can work normally with cdc-2.3.0.

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2363
Created by: [yuzelin|https://github.com/yuzelin]
Labels: bug, 
Created at: Thu Aug 03 10:45:10 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:22:42.0,,,,,,,,,,"0|z1o38g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] An insufficient number of arguments were supplied for the procedure or function cdc.fn_cdc_get_all_changes_,FLINK-34801,13572580,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.17.0

### Flink CDC version

2.4.1

### Database and its version

sql server 2014

### Minimal reproduce step

1

### What did you expect to see?

Caused by: java.lang.RuntimeException: SplitFetcher thread 22 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.kafka.connect.errors.RetriableException: An exception occurred in the change event producer. This connector will be restarted.
	at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:46)
	at io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource.executeIteration(SqlServerStreamingChangeEventSource.java:458)
	at io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource.execute(SqlServerStreamingChangeEventSource.java:138)
	at com.ververica.cdc.connectors.sqlserver.source.reader.fetch.SqlServerStreamFetchTask$LsnSplitReadTask.execute(SqlServerStreamFetchTask.java:161)
	at com.ververica.cdc.connectors.sqlserver.source.reader.fetch.SqlServerScanFetchTask.execute(SqlServerScanFetchTask.java:123)
	at com.ververica.cdc.connectors.base.source.reader.external.IncrementalSourceScanFetcher.lambda$submitTask$0(IncrementalSourceScanFetcher.java:95)
	... 5 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: An insufficient number of arguments were supplied for the procedure or function cdc.fn_cdc_get_all_changes_ ... .
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:265)
	at com.microsoft.sqlserver.jdbc.SQLServerResultSet$FetchBuffer.nextRow(SQLServerResultSet.java:5471)
	at com.microsoft.sqlserver.jdbc.SQLServerResultSet.fetchBufferNext(SQLServerResultSet.java:1794)
	at com.microsoft.sqlserver.jdbc.SQLServerResultSet.next(SQLServerResultSet.java:1052)
	at io.debezium.pipeline.source.spi.ChangeTableResultSet.next(ChangeTableResultSet.java:63)
	at io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource.lambda$executeIteration$1(SqlServerStreamingChangeEventSource.java:269)
	at io.debezium.jdbc.JdbcConnection.prepareQuery(JdbcConnection.java:606)
	at io.debezium.connector.sqlserver.SqlServerConnection.getChangesForTables(SqlServerConnection.java:329)
	at io.debezium.connector.sqlserver.SqlServerStreamingChangeEventSource.executeIteration(SqlServerStreamingChangeEventSource.java:251]
	... 9 more

### What did you see instead?

希望修复这个报错

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2354
Created by: [tuyou-ly|https://github.com/tuyou-ly]
Labels: bug, 
Created at: Tue Aug 01 13:43:38 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:22:39 UTC 2024,,,,,,,,,,"0|z1o388:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:22;flink-cdc-import;Date: Sun Oct 08 19:19:00 CST 2023, Author: [hk-lrzy|https://github.com/hk-lrzy]

I meet same issue.
I think com.ververica.cdc.connectors.sqlserver.source.utils.SqlServerUtils.currentLsn should invoke `getMaxTransactionLsn`
rather than `getMaxLsn`
please double check for this, if it's correct i can submit a pr to fix it. @leonardBang 
;;;","20/Mar/24 09:22;flink-cdc-import;Date: Sun Oct 08 19:53:29 CST 2023, Author: [GOODBOY008|https://github.com/GOODBOY008]

@hk-lrzy I think should use `getMaxTransactionLsn` , add a unit test for reproduce and check will be fine.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Mysql Startup with TIMESTAMP，Restore From Checkpoint Failed：One or more fetchers have encountered exception,FLINK-34800,13572579,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

Flink version 1.16.1

### Flink CDC version

Flink CDC version 2.4.0

### Database and its version

Mysql: 8.0.31

### Minimal reproduce step

// 1. Create Source Table
CREATE TABLE user_10 (
id INT,
name STRING,
age INT,
PRIMARY KEY (id) NOT ENFORCED 
) WITH( 
'connector' = 'mysql-cdc', 
'hostname' = '192.168.2.29', 
'port' = '3306', 
'username' = 'root', 
'password' = '123456', 
'database-name' = 'flink_test', 
'table-name' = 'user_10', 
'scan.startup.mode' = 'timestamp', 
'scan.incremental.snapshot.enabled' = 'true' ,
'scan.startup.timestamp-millis' = '1689736158444'
)

// 2.Create Sink Table
CREATE TABLE user_10_sink_1 (
id INT,
name STRING,
age INT,
PRIMARY KEY (id) NOT ENFORCED 
) WITH( 
'connector' = 'jdbc', 
'url' = 'jdbc:mysql://192.168.2.29:3306/flink_test?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=Asia/Shanghai', 
'username' = 'root', 
'password' = '123456', 
'driver' = 'com.mysql.cj.jdbc.Driver', 
'table-name' = 'user_10_sink_1' 
)

//Execute Flink SQL
INSERT INTO user_10_sink_1 SELECT id AS id,name AS name,age AS age FROM user_10

After startup, terminate the task after generating a checkpoint. Modify some data in the source table. Then, restore the task from the checkpoint


### What did you expect to see?

Successfully restored from checkpoint

### What did you see instead?

2023-07-19 11:50:37
java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: com.ververica.cdc.connectors.shaded.org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped.
	at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:50)
	at com.ververica.cdc.connectors.mysql.debezium.task.context.MySqlErrorHandler.setProducerThrowable(MySqlErrorHandler.java:85)
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$ReaderThreadLifecycleListener.onCommunicationFailure(MySqlStreamingChangeEventSource.java:1545)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1079)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631)
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932)
	... 1 more
Caused by: io.debezium.DebeziumException: bogus data in log event; the first event '' at 45092, the last event read from './binlog.000006' at 23504414, the last byte read from './binlog.000006' at 23504433. Error code: 1236; SQLSTATE: HY000.
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.wrap(MySqlStreamingChangeEventSource.java:1489)
	... 5 more
Caused by: com.github.shyiko.mysql.binlog.network.ServerException: bogus data in log event; the first event '' at 45092, the last event read from './binlog.000006' at 23504414, the last byte read from './binlog.000006' at 23504433.
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1043)
	... 3 more


### Anything else?

# Operating environment
1.  Checkpoint Configuration：
env.enableCheckpointing(300000L, CheckpointingMode.EXACTLY_ONCE);
env.getCheckpointConfig().setCheckpointTimeout(60000);
env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
env.getCheckpointConfig().setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
env.getCheckpointConfig().setTolerableCheckpointFailureNumber(100);
env.setRestartStrategy(RestartStrategies.fixedDelayRestart(2147483647, 3000L)];

2.  MySQL does not have GTID enabled

3. Mysql Binlog list：
Log_name File_size Encrypted
binlog.000006	1073756181	No
binlog.000007	1073759694	No
binlog.000008	1160855925	No
binlog.000009	1073763792	No
binlog.000010	731076314	No
binlog.000011	47220	No
binlog.000012	180	No
binlog.000013	157	No

4. Mysql Configuration: 
max_allowed_packet	1073741824
mysqlx_max_allowed_packet	1073741824
replica_max_allowed_packet	1073741824
slave_max_allowed_packet	1073741824

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2319
Created by: [GaoYaokun|https://github.com/GaoYaokun]
Labels: bug, 
Created at: Wed Jul 19 14:27:05 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:22:31.0,,,,,,,,,,"0|z1o380:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL server cannot read the correct table[Bug] ,FLINK-34799,13572578,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.16.0

### Flink CDC version

2.4

### Database and its version

Microsoft SQL Server 2017 (RTM) - 14.0.1000.169 (X64)   Aug 22 2017 17:04:49   Copyright (C) 2017 Microsoft Corporation  Developer Edition (64-bit) on Windows 10 Enterprise 10.0 <X64> (Build 19044: ) (Hypervisor) 

### Minimal reproduce step

CREATE TABLE products (
    id INT,
    name STRING,
    description STRING,
    PRIMARY KEY (id) NOT ENFORCED
  ) WITH (
    'connector' = 'sqlserver-cdc',
    'hostname' = '***',
    'port' = '1433',
    'username' = 'sa',
    'password' = '***',
    'database-name' = 'inventory',
    'table-name' = 'dbo.products'
  ];
![image|https://github.com/ververica/flink-cdc-connectors/assets/8586973/610edf4d-de11-4892-9b79-acd57178194e]
![image|https://github.com/ververica/flink-cdc-connectors/assets/8586973/1276cddb-3dbe-4148-90c9-6d3b4c0d5a2d]
![image|https://github.com/ververica/flink-cdc-connectors/assets/8586973/4042bbfb-73ac-433b-b107-00e5ca7eca1e]



### What did you expect to see?

OK

### What did you see instead?

If a database instance has multiple databases and multiple databases have the same table, an error reading table will occur

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2289
Created by: [ysq5202121|https://github.com/ysq5202121]
Labels: bug, 
Created at: Tue Jul 11 12:47:07 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:22:27.0,,,,,,,,,,"0|z1o37s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Bug] flink cdc mysql, use datastream to synchronize multiple tables, read full amount of data in a loop",FLINK-34798,13572577,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.16

### Flink CDC version

2.4.0

### Database and its version

mysql 8

### Minimal reproduce step

Occasionally, when using the flinkcdc datastream api to synchronize MySQL multi-table data, after the full phase is read, the increment will not start, and the full read will continue in a loop.
No restarts or failures while the task is running

### What did you expect to see?

After the full amount is read, incremental reading is performed

### What did you see instead?

After reading in full, continue to read in full

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2286
Created by: [JNSimba|https://github.com/JNSimba]
Labels: bug, 
Created at: Tue Jul 11 11:16:26 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:22:24 UTC 2024,,,,,,,,,,"0|z1o37k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 11:38:47 CST 2023, Author: [JNSimba|https://github.com/JNSimba]

```
//first chunk
2023-07-05 19:23:21,700 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator assigns split MySqlSnapshotSplit{tableId=GAIA.GAIA_SD_SALE_H, splitId='GAIA.GAIA_SD_SALE_H:0', splitKeyType=[`CLIENT` VARCHAR(8) NOT NULL], splitStart=null, splitEnd=[10000003], highWatermark=null} to subtask 0
2023-07-05 19:23:21,700 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_CHANNEL:85={ts_sec=0, file=mysql-binlog.003960, pos=312924837, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-662966004,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-05 19:23:47,286 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:0={ts_sec=0, file=mysql-binlog.003960, pos=314074090, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-662967138,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.

//the last chunk
2023-07-05 22:40:22,181 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: MySQL CDC Source received split request from parallel task 0 (([#3|https://github.com/apache/flink-cdc/issues/3] | [FLINK-3|https://issues.apache.org/jira/browse/FLINK-3]))
2023-07-05 22:40:22,182 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator assigns split MySqlSnapshotSplit{tableId=GAIA.GAIA_SD_SALE_H, splitId='GAIA.GAIA_SD_SALE_H:138', splitKeyType=[`CLIENT` VARCHAR(8) NOT NULL], splitStart=[99000397], splitEnd=null, highWatermark=null} to subtask 0
2023-07-05 22:40:22,182 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003960, pos=416574504, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663064764,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-05 22:40:22,182 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003960, pos=416574504, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663064764,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-05 22:40:22,182 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003960, pos=416574504, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663064764,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-05 22:40:22,182 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003960, pos=416574504, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663064764,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-05 22:40:22,183 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003960, pos=416574504, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663064764,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-05 22:40:22,183 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003960, pos=416574504, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663064764,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-05 22:40:22,183 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003960, pos=416574504, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663064764,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-05 22:40:22,208 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1725 for job 59b000e9eb89d6a684c875e1e4559d07 (5427368 bytes, checkpointDuration=210945 ms, finalizationTime=0 ms).
2023-07-05 22:40:22,208 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 1725 as completed for source Source: MySQL CDC Source.
2023-07-05 22:40:42,208 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1726 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1688568042208 for job 59b000e9eb89d6a684c875e1e4559d07.
2023-07-05 22:40:46,549 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1726 for job 59b000e9eb89d6a684c875e1e4559d07 (5425844 bytes, checkpointDuration=4341 ms, finalizationTime=0 ms).
2023-07-05 22:40:46,549 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 1726 as completed for source Source: MySQL CDC Source.
2023-07-05 22:41:06,549 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1727 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1688568066549 for job 59b000e9eb89d6a684c875e1e4559d07.
2023-07-05 22:41:10,814 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:138={ts_sec=0, file=mysql-binlog.003960, pos=418357093, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663066458,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.


//second full sync
//first chunk
2023-07-06 06:40:29,885 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator assigns split MySqlSnapshotSplit{tableId=GAIA.GAIA_SD_SALE_H, splitId='GAIA.GAIA_SD_SALE_H:0', splitKeyType=[`CLIENT` VARCHAR(8) NOT NULL], splitStart=null, splitEnd=[10000003], highWatermark=null} to subtask 0
2023-07-06 06:40:56,154 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:0={ts_sec=0, file=mysql-binlog.003962, pos=202166098, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663522615,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.

2023-07-06 10:04:23,561 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator assigns split MySqlSnapshotSplit{tableId=GAIA.GAIA_SD_SALE_H, splitId='GAIA.GAIA_SD_SALE_H:137', splitKeyType=[`CLIENT` VARCHAR(8) NOT NULL], splitStart=[90000321], splitEnd=[99000397], highWatermark=null} to subtask 0
2023-07-06 10:08:37,472 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
//last chunk
2023-07-06 10:08:37,474 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator assigns split MySqlSnapshotSplit{tableId=GAIA.GAIA_SD_SALE_H, splitId='GAIA.GAIA_SD_SALE_H:138', splitKeyType=[`CLIENT` VARCHAR(8) NOT NULL], splitStart=[99000397], splitEnd=null, highWatermark=null} to subtask 0
2023-07-06 10:04:23,561 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator assigns split MySqlSnapshotSplit{tableId=GAIA.GAIA_SD_SALE_H, splitId='GAIA.GAIA_SD_SALE_H:137', splitKeyType=[`CLIENT` VARCHAR(8) NOT NULL], splitStart=[90000321], splitEnd=[99000397], highWatermark=null} to subtask 0
2023-07-06 10:04:23,572 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 2411 for job 59b000e9eb89d6a684c875e1e4559d07 (5517078 bytes, checkpointDuration=27952 ms, finalizationTime=0 ms).
2023-07-06 10:04:23,572 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 2411 as completed for source Source: MySQL CDC Source.
2023-07-06 10:04:43,572 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2412 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1688609083572 for job 59b000e9eb89d6a684c875e1e4559d07.
2023-07-06 10:04:43,605 INFO  org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder [] - Coordinator checkpoint 2412 for coordinator 7bc70a3bc0ab15f3b3f0f42c4e2f7497 is awaiting 1 pending events
2023-07-06 10:08:37,472 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-06 10:08:37,472 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source Source: MySQL CDC Source received split request from parallel task 0 (([#3|https://github.com/apache/flink-cdc/issues/3] | [FLINK-3|https://issues.apache.org/jira/browse/FLINK-3]))
2023-07-06 10:08:37,474 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator assigns split MySqlSnapshotSplit{tableId=GAIA.GAIA_SD_SALE_H, splitId='GAIA.GAIA_SD_SALE_H:138', splitKeyType=[`CLIENT` VARCHAR(8) NOT NULL], splitStart=[99000397], splitEnd=null, highWatermark=null} to subtask 0
2023-07-06 10:08:37,474 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-06 10:08:37,475 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-06 10:08:37,475 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-06 10:08:37,475 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-06 10:08:37,475 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-06 10:08:37,476 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-06 10:08:37,476 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:137={ts_sec=0, file=mysql-binlog.003962, pos=285230510, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663602786,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.
2023-07-06 10:08:37,502 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 2412 for job 59b000e9eb89d6a684c875e1e4559d07 (5518747 bytes, checkpointDuration=233930 ms, finalizationTime=0 ms).
2023-07-06 10:08:37,502 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 2412 as completed for source Source: MySQL CDC Source.
2023-07-06 10:08:57,502 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2413 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1688609337502 for job 59b000e9eb89d6a684c875e1e4559d07.
2023-07-06 10:09:02,084 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 2413 for job 59b000e9eb89d6a684c875e1e4559d07 (5516434 bytes, checkpointDuration=4582 ms, finalizationTime=0 ms).
2023-07-06 10:09:02,085 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 2413 as completed for source Source: MySQL CDC Source.
2023-07-06 10:09:22,085 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2414 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1688609362084 for job 59b000e9eb89d6a684c875e1e4559d07.
2023-07-06 10:09:27,804 INFO  com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator [] - The enumerator under NEWLY_ADDED_ASSIGNING receives finished split offsets FinishedSnapshotSplitsReportEvent{finishedOffsets={GAIA.GAIA_SD_SALE_H:138={ts_sec=0, file=mysql-binlog.003962, pos=287462925, kind=SPECIFIC, gtids=00bc2f94-1828-11ec-a1b7-0c42a131f94e:1-19115775,
0bbec36d-8c12-11ec-a5f8-30809b6be3aa:1-2207674,
3d0eeb73-2e1e-11ec-b01d-506b4ba8106a:1-38558279,
57edd47b-8f42-11ec-95fa-b8599f1ef90b:1-21448119,
6f322515-d3f5-11eb-9ffc-b8599f455b4b:1-8600761,
b03db451-d3f5-11eb-949a-0c42a14a0658:1-9585598,
b15cd247-8c11-11ec-bd39-08c0eb8ce5dd:1-850155,
be740c4b-5fb5-11ed-bf8f-84160cbbe340:1-663604686,
e258aacd-a438-11ec-8bc5-005056848778:1-238716060, row=0, event=0}}} from subtask 0.



```;;;","20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 11:42:34 CST 2023, Author: [JNSimba|https://github.com/JNSimba]

It can be seen that the positions of the binlog read twice have changed, but two full synchronizations have been performed.

finishedOffsets={GAIA.GAIA_SD_SALE_H:138={ts_sec=0, file=mysql-binlog.003960, pos=418357093, kind=SPECIFIC,
finishedOffsets={GAIA.GAIA_SD_SALE_H:138={ts_sec=0, file=mysql-binlog.003962, pos=287462925, kind=SPECIFIC）;;;","20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 11:49:45 CST 2023, Author: [gong|https://github.com/gong]

> 0003

@JNSimba hello. Do you open `scan.newly-added-table.enabled`?;;;","20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 11:55:16 CST 2023, Author: [JNSimba|https://github.com/JNSimba]

> scan.newly-added-table.enabled

@gong  Thank you for your reply, there is `scan.newly-added-table.enabled` configuration enabled, is it related to this configuration?;;;","20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 12:06:00 CST 2023, Author: [gong|https://github.com/gong]

> > scan.newly-added-table.enabled
> 
> @gong Thank you for your reply, there is `scan.newly-added-table.enabled` configuration enabled, is it related to this configuration?

@JNSimba Maybe you can review this PR https://github.com/ververica/flink-cdc-connectors/pull/2096;;;","20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 12:58:26 CST 2023, Author: [JNSimba|https://github.com/JNSimba]

> @JNSimba Maybe you can review this PR ([#2096|https://github.com/apache/flink-cdc/issues/2096] | [FLINK-2096|https://issues.apache.org/jira/browse/FLINK-2096])

@gong Thank you, this PR seems to happen when the task is restarted.
But my phenomenon is that the task did not restart during the first and second full synchronization, and no error was reported.;;;","20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 14:12:10 CST 2023, Author: [gong|https://github.com/gong]

> > @JNSimba Maybe you can review this PR ([#2096|https://github.com/apache/flink-cdc/issues/2096] | [FLINK-2096|https://issues.apache.org/jira/browse/FLINK-2096])
> 
> @gong Thank you, this PR seems to happen when the task is restarted. But my phenomenon is that the task did not restart during the first and second full synchronization, and no error was reported.

@JNSimba It will cause duplicate scan same chunk when you suspend task and add new table and task is snapshot reading phase.;;;","20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 15:05:17 CST 2023, Author: [JNSimba|https://github.com/JNSimba]

> @JNSimba It will cause duplicate scan same chunk when you suspend task and add new table and task is snapshot reading phase.

@gong  Thank you for your reply. I understand what you mean: When the snapshot phase is not completed, stop the task and add a new table. This problem occurs when starting from the savepoint.

But in my case, during the first synchronization of the snapshot of the table and the second synchronization, the task was not stopped

Here is the full log: [1-jobmanger.log|https://github.com/ververica/flink-cdc-connectors/files/12011658/1-jobmanger.log]
It can be seen that after **2023-07-05 19:23:21**, the table **GAIA.GAIA_SD_SALE_H** started the first full snapshot synchronization, with a total of 139 chunks, and then at **2023-07-06 06:40:29** the table Full sync started again;;;","20/Mar/24 09:22;flink-cdc-import;Date: Tue Jul 11 16:30:01 CST 2023, Author: [gong|https://github.com/gong]

> > @JNSimba It will cause duplicate scan same chunk when you suspend task and add new table and task is snapshot reading phase.
> 
> @gong Thank you for your reply. I understand what you mean: When the snapshot phase is not completed, stop the task and add a new table. This problem occurs when starting from the savepoint.
> 
> But in my case, during the first synchronization of the snapshot of the table and the second synchronization, the task was not stopped
> 
> Here is the full log: [1-jobmanger.log|https://github.com/ververica/flink-cdc-connectors/files/12011658/1-jobmanger.log] It can be seen that after **2023-07-05 19:23:21**, the table **GAIA.GAIA_SD_SALE_H** started the first full snapshot synchronization, with a total of 139 chunks, and then at **2023-07-06 06:40:29** the table Full sync started again

@JNSimba  This problem don't need stop task when first synchronization of the snapshot of the table and the second synchronization. 
step1. you enbale add new table
step2. you restart task when snapshot phase
It will cause this problem.
if you don't restart this task and It will be other problem;;;","20/Mar/24 09:22;flink-cdc-import;Date: Fri Jul 14 10:19:11 CST 2023, Author: [ruanhang1993|https://github.com/ruanhang1993]

Hi, @JNSimba .
Could we reproduce this problem by some fixed operations?;;;",,,,,,,,,,,,,,,,,,,,,,,
[Bug] Debezium's database.server.name property should not be hard-coded,FLINK-34797,13572576,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.17.1

### Flink CDC version

2.3.0

### Database and its version

Postgres

### Minimal reproduce step

Running multiple postgres CDC connectors in one JVM process; this will trigger warnings about conflicting MBean names for the Debezium metrics beans, caused by hard-coding the logical server name property to one constant value in [here|https://github.com/ververica/flink-cdc-connectors/blob/master/flink-connector-postgres-cdc/src/main/java/com/ververica/cdc/connectors/postgres/PostgreSQLSource.java#L157). As this name also is used to disambiguate the MBean names, it must be unique across connectors running on the same JVM (and should be unique globally anyways].

### What did you expect to see?

There should be a connector option for specifying the Debezium `database.server.name` property.

### What did you see instead?

Conflicting MBean names

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2235
Created by: [gunnarmorling|https://github.com/gunnarmorling]
Labels: bug, 
Created at: Mon Jun 26 17:07:25 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:22:11.0,,,,,,,,,,"0|z1o37c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Bug] flink cdc has lot of lamda class, it may leaked in flink standalone cluster after task start and stop many times",FLINK-34796,13572575,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.16.0

### Flink CDC version

2.2.1

### Database and its version

8.0

### Minimal reproduce step

-deploy a flink standalone cluster 
- start a flink cdc task and stop the task three or more times 


### What did you expect to see?

- the cluster could unload the related class in the task  and the metaspace could decrease after full gc

### What did you see instead?

- the metaspace of flink cluster always increase until oom 


### Anything else?

i dump the heap and analyse with jprofile, the ChildFirstClassLoader could not be unload because many lambda class
![Uploading image.png…]()

could someone help me give some advice, the lambda in java is common, and flink cdc used lot of lambda, i tried with a small demo, only lamda, but it could be unload when full gc.


### Are you willing to submit a PR?

- [x] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2196
Created by: [Fernandoize|https://github.com/Fernandoize]
Labels: bug, 
Created at: Sat Jun 10 22:41:42 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:22:08 UTC 2024,,,,,,,,,,"0|z1o374:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:22;flink-cdc-import;Date: Sat Jun 10 22:43:23 CST 2023, Author: [Fernandoize|https://github.com/Fernandoize]

![image|https://github.com/ververica/flink-cdc-connectors/assets/15117983/fc5ee0f2-5050-4958-a34b-16e1e28e50b1]
;;;","20/Mar/24 09:22;flink-cdc-import;Date: Sat Jun 10 22:44:17 CST 2023, Author: [Fernandoize|https://github.com/Fernandoize]

![image|https://github.com/ververica/flink-cdc-connectors/assets/15117983/bd35f0dc-ca31-498e-8c9e-0ffc41feeb23]
i stop the cdc task, but the classloader always exists;;;","20/Mar/24 09:22;flink-cdc-import;Date: Wed Jun 28 09:58:33 CST 2023, Author: [shiyiky|https://github.com/shiyiky]

how do you deploy the flink's application? run application with jar or  run application with remote submission of code for flink's cluster?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
expected to support flink-connector-db2-cd('scan.startup.mode'='specific_offsets'),FLINK-34795,13572574,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:22,20/Mar/24 09:22,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Expect DB2 to support 'scan.startup.mode'='specific_offsets' as MySQL does

### Solution

https://debezium.io/documentation/reference/stable/connectors/db2.html#db2-overview

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2191
Created by: [dv1314|https://github.com/dv1314]
Labels: enhancement, 
Created at: Wed Jun 07 17:56:25 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:22:02.0,,,,,,,,,,"0|z1o36w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Bug] if splitTable method execute fast than getNext, job will be blocked",FLINK-34794,13572573,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version
[e49e22065fdd6f1026db76f81bd8ed38e81ef173|https://github.com/apache/flink-cdc/commit/e49e22065fdd6f1026db76f81bd8ed38e81ef173]

### Flink CDC version
[e49e22065fdd6f1026db76f81bd8ed38e81ef173|https://github.com/apache/flink-cdc/commit/e49e22065fdd6f1026db76f81bd8ed38e81ef173]

### Database and its version

mysql:  mysql 8.0.27

### Minimal reproduce step

add `Thread.sleep(10000)` before MySqlSnapshotSplitAssigner.getNext().  lock.wait will not be notify any more, and it will wait forever.

### What did you expect to see?

job can execute normally

### What did you see instead?

nothing

### Anything else?

On MysqlCDC job start,  the method splitTable() &&  getNext(] from MySqlSnapshotSplitAssigner will be execute asynchronously, the two method use one lock, and depend on the order of execution。Under special circumstances,  the job does not perform as expected

### Are you willing to submit a PR?

- [x] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2187
Created by: [y0908105023|https://github.com/y0908105023]
Labels: bug, 
Assignee: [y0908105023|https://github.com/y0908105023]
Created at: Wed Jun 07 11:37:46 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:21:58 UTC 2024,,,,,,,,,,"0|z1o36o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:21;flink-cdc-import;Date: Wed Jun 07 11:43:04 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

the below jpg is job's stack, job will waiting forever, cannot handle data and throw exception
![image|https://github.com/ververica/flink-cdc-connectors/assets/8789291/0c04cc86-ff68-45f6-a84b-7aa927532fe2]
;;;","20/Mar/24 09:21;flink-cdc-import;Date: Wed Jun 07 11:49:45 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

git提交地址：https://github.com/ververica/flink-cdc-connectors/pull/2188
;;;","20/Mar/24 09:21;flink-cdc-import;Date: Thu Jun 08 11:18:33 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

@leonardBang 麻烦您review以下哈;;;","20/Mar/24 09:21;flink-cdc-import;Date: Fri Jun 30 16:35:23 CST 2023, Author: [ruanhang1993|https://github.com/ruanhang1993]

@y0908105023 Thanks for the contribution.
I do not think there is a problem when `splitTable` method execute faster than `getNext`.
If `splitTable` method execute faster than `getNext`, the `remainingSplits` will not be empty. And we will not reach this `lock.wait` code.;;;","20/Mar/24 09:21;flink-cdc-import;Date: Tue Jul 04 11:13:04 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

@ruanhang1993  if `chunkSplitter.splitChunks(nextTable);` throws exception, `remainingSplits` will be empty, and the lock wait for notify, job will be hanged, exception will not be catched. hope your response

![image|https://github.com/ververica/flink-cdc-connectors/assets/8789291/cfd2aa7b-d33b-44d0-afbf-2f91744d4119]
;;;","20/Mar/24 09:21;flink-cdc-import;Date: Wed Jul 05 16:46:13 CST 2023, Author: [ruanhang1993|https://github.com/ruanhang1993]

Hi, @y0908105023.

If there is an error in `splitTable`, the error will be caught and invoke `lock.notify()` at https://github.com/ververica/flink-cdc-connectors/blob/bb3b081f45d18191b9b68f52211375ebf06e8142/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/assigners/MySqlSnapshotSplitAssigner.java#L556.
 
Then the error will be thrown by `checkSplitterErrors` method.;;;","20/Mar/24 09:21;flink-cdc-import;Date: Thu Jul 06 17:52:50 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

@ruanhang1993 if error happened, `lock.notify();` will be executed soon, and the `remainingTables` is empty, when the async method `getNext` execute, the job will wait. the order of the method:
1. method throws exception
2. lock.notify()
3. lock.wait()  forever;;;","20/Mar/24 09:21;flink-cdc-import;Date: Mon Jul 10 10:44:09 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

@ruanhang1993 looking forward to your reply;;;","20/Mar/24 09:21;flink-cdc-import;Date: Mon Jul 17 15:06:30 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

no body， no body
@ruanhang1993 ;;;","20/Mar/24 09:21;flink-cdc-import;Date: Mon Jul 17 15:45:39 CST 2023, Author: [ruanhang1993|https://github.com/ruanhang1993]

@y0908105023 Sorry for my late reply.
I understand what you mean. This makes sense. Thanks.

;;;","20/Mar/24 09:21;flink-cdc-import;Date: Tue Jul 18 16:37:04 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

@ruanhang1993 
can this issue merge into the master branch, does i need do something else. ;;;","20/Mar/24 09:21;flink-cdc-import;Date: Wed Jul 19 17:53:08 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

@ruanhang1993 @leonardBang thanks for your attention, should i do something for this？
;;;","20/Mar/24 09:21;flink-cdc-import;Date: Thu Jul 20 14:19:35 CST 2023, Author: [ruanhang1993|https://github.com/ruanhang1993]

@y0908105023 I check the code again. And I think the problem has been fixed. 
When the exception code invoking the `notify`, we can mark sure to check it or reinvoke the `getNext` to check it because these parts are all under the lock, which I ignore before.

For your problem, I think you should add some logs to see whether the splits is produced successfully. Thanks.;;;","20/Mar/24 09:21;flink-cdc-import;Date: Tue Jul 25 14:45:38 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

@ruanhang1993 i add  log like the picture, is it ok?
<img width=""837"" alt=""image"" src=""https://github.com/ververica/flink-cdc-connectors/assets/8789291/c081cda0-9e5c-4213-8610-6ab332b6bba3"">
;;;","20/Mar/24 09:21;flink-cdc-import;Date: Wed Jul 26 16:01:31 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

@ruanhang1993 still there?;;;","20/Mar/24 09:21;flink-cdc-import;Date: Fri Aug 18 12:06:46 CST 2023, Author: [ruanhang1993|https://github.com/ruanhang1993]

Maybe we could add some logs when notify and wait besides these logs.;;;","20/Mar/24 09:21;flink-cdc-import;Date: Fri Sep 08 15:49:28 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

i think it does not need a log. job will failed soon if split error;;;","20/Mar/24 09:21;flink-cdc-import;Date: Fri Sep 08 15:50:55 CST 2023, Author: [y0908105023|https://github.com/y0908105023]

it takes long time discuss the issue, wait for your apply
;;;",,,,,,,,,,,,,,,
Supports synchronizing data from the postgreSQL standby database?,FLINK-34793,13572572,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

In many financial business scenarios, the postgreSQL database is used, generally in the active and standby mode. The main database undertakes the core transaction business, and dare not pull data from the main database in real time, otherwise it will have a great impact on the business. Small, whether it can support the function of synchronizing data from the standby database in real time

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2180
Created by: [vincent183|https://github.com/vincent183]
Labels: enhancement, 
Created at: Mon Jun 05 12:39:22 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:21:44.0,,,,,,,,,,"0|z1o36g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Incremental source never switches to streaming mode with disabled checkpointing and parallelism > 1,FLINK-34792,13572571,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.13.6, 1.14.4, 1.15.2, 1.16.0, 1.17.0

### Flink CDC version

2.3.0, 2.4-SNAPSHOT

### Database and its version

Any connector with incremental source implementation

### Minimal reproduce step

For instance, remove 
```sql 
SET 'execution.checkpointing.interval' = '3s'; 
``` 
from `MySqlE2eITCase#testMySqlCDC(]`

### What did you expect to see?

Failed job or at least error message describing that the user must enable checkpointing for the connector to work correctly 

### What did you see instead?

If parallelism of incremental source is greater than one, implementations of `SnapshotSplitAssigner` wait for completed checkpoint event to set flag field, that triggers `HybridSplitAssigner` to switch to streaming mode. If checkpointing is disabled, then flag field is never set to it's final state, which leads to the fact that the source is forever stuck in the snapshot mode. So, I think we need to forbid users to run jobs with incremental sources without enabled checkpointing. 

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2165
Created by: [tigrulya-exe|https://github.com/tigrulya-exe]
Labels: bug, 
Created at: Tue May 30 15:39:27 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:21:39.0,,,,,,,,,,"0|z1o368:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add docs for the experimental features.,FLINK-34791,13572570,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Add docs for the experimental features.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2164
Created by: [ruanhang1993|https://github.com/ruanhang1993]
Labels: enhancement, 
Created at: Tue May 30 14:14:59 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:21:36.0,,,,,,,,,,"0|z1o360:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The program encountered a blockage preventing it from reading snapshot data from mysql-cdc [Bug] ,FLINK-34790,13572569,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.14.5

### Flink CDC version

2.3

### Database and its version

mysql 5.7.41

### Minimal reproduce step

### flink config:
taskmanager.memory.process.size: 2 gb
taskmanager.numberOfTaskSlots: 1
parallelism.default: 1
### mysql table schema and data:
The data must be kept confidential.  If needed, please contact me at 997874603@qq.com.

### What did you expect to see?

Read the snapshot data of this table normally.

### What did you see instead?

The snapshot data query was successful, but there was no return value when calling the method com.ververica.cdc.connectors.mysql.debezium.task.MySqlSnapshotSplitReadTask.readField(ResultSet, int, Column, Table].

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2158
Created by: [gongzexin|https://github.com/gongzexin]
Labels: bug, 
Created at: Mon May 29 17:15:48 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:21:33 UTC 2024,,,,,,,,,,"0|z1o35s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:21;flink-cdc-import;Date: Sun Feb 04 20:13:11 CST 2024, Author: [ruanhang1993|https://github.com/ruanhang1993]

The code part you provide is about getting a field from the `ResultSet`. It is helpful to look at the thread dump and find where the problem is hanging on.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] DBZ-5589 Mysql connector can't handle the case sensitive of rename/change column statement,FLINK-34789,13572568,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

ALL

### Flink CDC version

ALL

### Database and its version

MySQL, ALL version

### Minimal reproduce step

1. create a table and write some data；
2. listen it‘s binlog Source:mysql , Sink:print；
3. alter a column case and update data.

### What did you expect to see?

when i modify a clumn case， the filed name in the binlog record should be changed.

### What did you see instead?

I changed a field name to lowercase, but its field name in binlog is still uppercase.

### Anything else?

its a bug of debezium. 
source code in 
debezium-connector-mysql/src/main/java/io/debezium/connector/mysql/antlr/listener/AlterTableParserListener.java:AlterTableParserListener.java
Debezium 2.0.0 fix it by modify line 187 and line 349.
Use equals insdead of equalsIgnoreCase method.
![alter_clumn_test|https://github.com/ververica/flink-cdc-connectors/assets/75104806/9e8e4e8f-cfac-4dfe-8400-e757eb49a951]


### Are you willing to submit a PR?

- [x] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2144
Created by: [dua01|https://github.com/dua01]
Labels: bug, 
Created at: Thu May 18 11:14:19 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:21:28 UTC 2024,,,,,,,,,,"0|z1o35k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:21;flink-cdc-import;Date: Sun Feb 04 20:08:06 CST 2024, Author: [ruanhang1993|https://github.com/ruanhang1993]

Would you like to cherry pick the bug fix to the cdc connector? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug]  Flink CDC Orcale  Connection Pool Number settings cannot take effect,FLINK-34788,13572567,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

Flink 1.6 version 

### Flink CDC version

Flink 2.3 version 

### Database and its version

Orcale 19c

### Minimal reproduce step

connection.pool.size=50, but it cannot take effect, orcale session show 500 connections

### What did you expect to see?

Fix the problem

### What did you see instead?

I dont know

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2120
Created by: [blackbryant|https://github.com/blackbryant]
Labels: bug, 
Created at: Mon May 08 17:15:50 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:21:23.0,,,,,,,,,,"0|z1o35c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Tidb, please support synchronization of multiple tables at the same time",FLINK-34787,13572566,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Tidb, please support synchronization of multiple tables at the same time

### Solution

Tidb, please support synchronization of multiple tables at the same time

### Alternatives

Tidb, please support synchronization of multiple tables at the same time

### Anything else?

Tidb, please support synchronization of multiple tables at the same time

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2110
Created by: [niezl|https://github.com/niezl]
Labels: enhancement, 
Created at: Fri May 05 14:28:40 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:21:19.0,,,,,,,,,,"0|z1o354:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] [tidb cdc]a error:java.lang.IllegalArgumentException: Invalid range: [+INF..{xxx}) at org.apache.flink.shaded.guava30.com.google.common.collect.Range.<init>(Range.java:358),FLINK-34786,13572565,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.14.5

### Flink CDC version

2.3.0

### Database and its version

tidb 6.5.1

### Minimal reproduce step

This is my tidbcdc test:
![image|https://user-images.githubusercontent.com/28680957/236363087-29b15923-b549-4f49-a6e4-c784fcad7150.png)




### What did you expect to see?

Tidb cdc is running normally.

### What did you see instead?

I meet this error:
`Exception in thread ""main"" java.lang.RuntimeException: Failed to fetch next result at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80) at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370) at org.apache.flink.table.utils.PrintUtils.printAsTableauForm(PrintUtils.java:152) at org.apache.flink.table.api.internal.TableResultImpl.print(TableResultImpl.java:160) at com.zjlab.shuzihu.MysqlToEs_online.main(MysqlToEs_online.java:204) Caused by: java.io.IOException: Failed to fetch job execution result at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:177) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:120) at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106) ... 5 more Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175) ... 7 more Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137) at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616) at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628) at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996) at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134) at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:174) ... 7 more Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) at akka.actor.Actor.aroundReceive(Actor.scala:537) at akka.actor.Actor.aroundReceive$(Actor.scala:535) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) at akka.actor.ActorCell.invoke(ActorCell.scala:548) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) at akka.dispatch.Mailbox.run(Mailbox.scala:231) at akka.dispatch.Mailbox.exec(Mailbox.scala:243) at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067) at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703) at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172) Caused by: java.lang.IllegalArgumentException: Invalid range: [+INF..{116,128,0,0,0,0,0,2,66,95,114,223,255,255,255,255,255,255,249}) at org.apache.flink.shaded.guava30.com.google.common.collect.Range.<init>(Range.java:358) at org.apache.flink.shaded.guava30.com.google.common.collect.Range.create(Range.java:156) at org.apache.flink.shaded.guava30.com.google.common.collect.Range.intersection(Range.java:558) at org.tikv.cdc.CDCClient.overlapWithRegion(CDCClient.java:235) at org.tikv.cdc.CDCClient.addRegions(CDCClient.java:185) at org.tikv.cdc.CDCClient.applyKeyRange(CDCClient.java:178) at org.tikv.cdc.CDCClient.start(CDCClient.java:98) at com.ververica.cdc.connectors.tidb.TiKVRichParallelSourceFunction.run(TiKVRichParallelSourceFunction.java:165) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323] 五月 04, 2023 5:33:41 下午 org.tikv.shade.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived 警告: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: too_many_pings 五月 04, 2023 5:33:41 下午 org.tikv.shade.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived 警告: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: too_many_pings 五月 04, 2023 5:33:41 下午 org.tikv.shade.io.grpc.netty.NettyClientHandler$3 onGoAwayReceived 警告: Received GOAWAY with ENHANCE_YOUR_CALM. Debug data: too_many_pings 五月 04, 2023 5:33:41 下午 org.tikv.shade.io.grpc.internal.AtomicBackoff$State backoff`

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2109
Created by: [zhangzhaohuazai|https://github.com/zhangzhaohuazai]
Labels: bug, 
Created at: Fri May 05 09:54:52 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:21:15.0,,,,,,,,,,"0|z1o34w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[bug] MySQL using utf8_ general_ Ci encoding, splitting will end prematurely",FLINK-34785,13572564,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:21,20/Mar/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Motivation

Support mysql primary keys using encoding types different from ASCII , in MySQL, the primary key uses utf8_ general_ Ci encoding type, ignoring case when sorting. The `ObjectUtils.compare(chunkEnd, max]` code comparison is based on ASCII, which will cause the split  to end prematurely.

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2075
Created by: [fuyun2024|https://github.com/fuyun2024]
Labels: enhancement, 
Created at: Fri Apr 07 19:17:05 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:21:05.0,,,,,,,,,,"0|z1o34o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Datetime data inconsistency (off by 8 hours) in snapshot phase,FLINK-34784,13572563,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.13.6/1.14

### Flink CDC version

2.2/2.3

### Database and its version

MySQL 5.7+ & 8.0+

### Minimal reproduce step

([#1739|https://github.com/apache/flink-cdc/issues/1739] | [FLINK-1739|https://issues.apache.org/jira/browse/FLINK-1739]) 
The issue still persists in snapshot phase and when MySQL is of CST timezone, and the reason is Debezium's Timestamp::toEpochMillis does not respect time zone:
![image|https://user-images.githubusercontent.com/20527912/230588397-e3d07c1b-1a5d-4659-b6b0-610ba36ac442.png)

PRECONDITION：Server/local time zone are Asia/Shanghai (UTC+8)，MySQL time zone is CST
To reproduce the issue:
1. create a time with pk and datetime field and insert some random data:

```sql
CREATE TABLE `batchdatetime` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `dt` datetime DEFAULT NULL,
  PRIMARY KEY (`id`)
]

# insert data here

mysql> select * from batchdatetime;
+----+---------------------+
| id | dt                  |
+----+---------------------+
|  1 | 2016-10-15 02:15:15 |
|  2 | 2016-10-15 02:15:15 |
+----+---------------------+

```

2. create cdc job using stream api
```java
  public static void main(String[] args) throws Exception {
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
    env.setParallelism(1);
    MySqlSource<String> mysqlSource = new MySqlSourceBuilder<String>()
	.databaseList(""aps_test"")
	.tableList(""aps_test.batchdatetime"")
	.hostname(""localhost"")
	.username(""root"")
	.password("""")
	.serverTimeZone(""Asia/Shanghai"")
	.includeSchemaChanges(true)
	.deserializer(new JsonDebeziumDeserializationSchema())
	.scanNewlyAddedTableEnabled(true)
	.build();

    DataStreamSource<String> source = env.fromSource(mysqlSource, WatermarkStrategy.noWatermarks(), ""mysql-source"");
    source.print();
    env.execute();
  }
```

3. Start the job and witness how debezium brings inconsistency to datetime columns:
![image|https://user-images.githubusercontent.com/20527912/230589178-54adf54d-a797-45f3-b156-ffd95cc76907.png)

 dateTime.toInstant(ZoneOffset.UTC).toEpochMilli() = 1476497715000

BUT 2016-10-15T02:15:15 corresponds to 1476468915000 in milliseconds in BOTH server and local time zone:
AND we can verify this using the following SQL:

```sql
mysql> select UNIX_TIMESTAMP(dt) * 1000 as aha from batchdatetime;
+---------------+
| aha           |
+---------------+
| 1476468915000 |
| 1476468915000 |
+---------------+
```

4. The correct way to convert datetime to timestamp is `dateTime.toInstant(ZoneOffset.of(""+8"")).toEpochMilli(]`

and the +8 should've come from user specific dbz properties OR a default time zone of the system.


### What did you expect to see?

{""before"":null,""after"":{""id"":1,""dt"":1476468915000},""source"":{""version"":""1.6.4.Final"",""connector"":""mysql"",""name"":""mysql_binlog_source"",""ts_ms"":0,""snapshot"":""false"",""db"":""aps_test"",""sequence"":null,""table"":""batchdatetime"",""server_id"":0,""gtid"":null,""file"":"""",""pos"":0,""row"":0,""thread"":null,""query"":null},""op"":""r"",""ts_ms"":1680861868595,""transaction"":null}


### What did you see instead?

{""before"":null,""after"":{""id"":1,""dt"":1476497715000},""source"":{""version"":""1.6.4.Final"",""connector"":""mysql"",""name"":""mysql_binlog_source"",""ts_ms"":0,""snapshot"":""false"",""db"":""aps_test"",""sequence"":null,""table"":""batchdatetime"",""server_id"":0,""gtid"":null,""file"":"""",""pos"":0,""row"":0,""thread"":null,""query"":null},""op"":""r"",""ts_ms"":1680861868595,""transaction"":null}


### Anything else?

Critical data inconsistency issue.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2074
Created by: [qidian99|https://github.com/qidian99]
Labels: bug, 
Created at: Fri Apr 07 18:05:12 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:20:59 UTC 2024,,,,,,,,,,"0|z1o34g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:20;flink-cdc-import;Date: Thu Mar 14 16:24:12 CST 2024, Author: [Xuxiaotuan|https://github.com/Xuxiaotuan]

Hello, until now this problem still exists, is it dependent on upstream updates?

 ([#1739|https://github.com/apache/flink-cdc/issues/1739] | [FLINK-1739|https://issues.apache.org/jira/browse/FLINK-1739]) This doesn't solve the problem
Flink version
1.18

Flink CDC version
3.0.1

Database and its version
MySQL 5.7+ & 8.0+;;;","20/Mar/24 09:20;flink-cdc-import;Date: Thu Mar 14 18:54:36 CST 2024, Author: [Xuxiaotuan|https://github.com/Xuxiaotuan]

So far I have implemented it this way and the snapshot and incremental timestamp values are normal

I can submit pr if necessary
![image|https://github.com/apache/flink-cdc/assets/29968120/455afe3c-59f1-4b16-83cb-994ad9a85207]
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] TiDB Flink CDC not support exactly once,FLINK-34783,13572562,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.13.6

### Flink CDC version

I had try three version:
2.3.0
2.4-SNAPSHOT
2.2.1

### Database and its version

I had try tow version:
tidb v5.4.3
tidb v6.1.3

### Minimal reproduce step

(1)Start a Flink CDC program locally, read a table in tidb, and enable checkpoints in the Flink program to remain in a directory (I tested the Flink program locally, so the checkpoints save location uses the local directory).
(2)Stop this Flink CDC program to simulate an exception scenario, and then insert, delete, and update this table in TiDB.
(3]Restoring the Flink cdc program from checkpoints will trace the operation of the Tidb table after the program experienced an exception. Insertion and update operations on the Tidb table can be restored to the Flink program for processing, but the deletion operation on the Tidb table has been lost.

### What did you expect to see?

Restoring the Flink cdc program from checkpoints will trace the operations of the Tidb table after the program experienced an exception. Insertion, update, and deletion operations on the Tidb table can be restored to the Flink program for processing, which means that the deletion operation will not be lost.

### What did you see instead?

Restoring the Flink cdc program from checkpoints will trace the operation of the Tidb table after the program experienced an exception. Insertion and update operations on the Tidb table can be restored to the Flink program for processing, but the deletion operation on the Tidb table has been lost.

### Anything else?

Does Flink cdc tidb really implement end-to-end exactly once?

### Are you willing to submit a PR?

- [x] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2034
Created by: [ArainChen|https://github.com/ArainChen]
Labels: bug, 
Created at: Wed Mar 22 15:59:22 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:20:51 UTC 2024,,,,,,,,,,"0|z1o348:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:20;flink-cdc-import;Date: Sat Dec 09 17:35:55 CST 2023, Author: [CtrlSorcerer|https://github.com/CtrlSorcerer]

Have you tested it? If you initially set 'scan. startup. mode' to 'latest offset', stop the task, generate a savepoint, and use this savepoint to start, the previous historical data will still be synchronized.
In my understanding, he should not start synchronizing from the latest time, why would he still synchronize historical data.;;;","20/Mar/24 09:20;flink-cdc-import;Date: Tue Dec 19 17:52:29 CST 2023, Author: [ArainChen|https://github.com/ArainChen]

> Have you tested it? If you initially set 'scan. startup. mode' to 'latest offset', stop the task, generate a savepoint, and use this savepoint to start, the previous historical data will still be synchronized. In my understanding, he should not start synchronizing from the latest time, why would he still synchronize historical data.

set 'scan. startup. mode' to 'latest offset' that can let the CDC not to scan all historical data in tidb,but you use savepoint to save the historical data and flink will read it when the job restart.In short,your historical data is not come from CDC but flink,I suggest you to disuse savepoint in this scene.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Multiple source read binlogs of the same job are all assigned to SubTask-0, which may have performance problems.",FLINK-34782,13572561,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Multiple source read binlogs of the same job are all assigned to SubTask-0, which may have performance problems.
<img width=""1242"" alt=""image"" src=""https://user-images.githubusercontent.com/5699014/226238858-bd3bbc63-d7b6-4e1f-b0ad-86c6d6df3f9b.png"">
<img width=""1200"" alt=""image"" src=""https://user-images.githubusercontent.com/5699014/226238965-f3ed29ed-4b21-4feb-8db1-9070a3af7a2f.png"">
As shown in the above two figures, the jobs running in our production environment have several sources, and all sources are assigned to the same subtask. When there are many sources, performance problems may occur.

### Solution

When a job has multiple subtasks, different sources can be randomly assigned to different subtasks for execution to prevent all sources from being executed on one subtask.

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/2027
Created by: [lintingbin|https://github.com/lintingbin]
Labels: enhancement, 
Created at: Mon Mar 20 11:26:04 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:20:33.0,,,,,,,,,,"0|z1o340:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] [oracle] oracle real-time synchronization is slow,FLINK-34781,13572560,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

flink1.15.3

### Flink CDC version

2.3-SNAPSHOT

### Database and its version

oracle 11

### Minimal reproduce step

In oracle real-time synchronization, when the data transmission changes, the data synchronization is very slow, usually about 10 minutes, especially when oracle has a large number of libraries and tables, resulting in akka timeout


### What did you expect to see?

 StringBuilder queryTablesSql =
                new StringBuilder(
                        ""SELECT OWNER ,TABLE_NAME,TABLESPACE_NAME FROM ALL_TABLES \n""
                                + ""WHERE TABLESPACE_NAME IS NOT NULL AND TABLESPACE_NAME NOT IN ('SYSTEM','SYSAUX')"");
        if (tableList != null && !tableList.isEmpty()) {
            StringJoiner stringJoiner = new StringJoiner("","");
            for (String tableId : tableList) {
                stringJoiner.add(""'"" + tableId + ""'"");
            }
            queryTablesSql
                    .append("" AND TABLE_NAME IN ("")
                    .append(stringJoiner.toString())
                    .append("")"");
        }
        try {
            jdbcConnection.query(
                    queryTablesSql.toString(),
                    rs -> {
                        while (rs.next()) {
                            String schemaName = rs.getString(1);
                            String tableName = rs.getString(2);
                            TableId tableId =
                                    new TableId(jdbcConnection.database(), schemaName, tableName);
                            tableIdSet.add(tableId);
                        }
                    });
        } catch (SQLException e) {
            LOG.warn("" SQL execute error, sql:{}"", queryTablesSql, e);
        }

### What did you see instead?

  String queryTablesSql =
                ""SELECT OWNER ,TABLE_NAME,TABLESPACE_NAME FROM ALL_TABLES \n""
                        + ""WHERE TABLESPACE_NAME IS NOT NULL AND TABLESPACE_NAME NOT IN ('SYSTEM','SYSAUX')"";
        try {
            jdbcConnection.query(
                    queryTablesSql,
                    rs -> {
                        while (rs.next()) {
                            String schemaName = rs.getString(1);
                            String tableName = rs.getString(2);
                            TableId tableId =
                                    new TableId(jdbcConnection.database(), schemaName, tableName);
                            tableIdSet.add(tableId);
                        }
                    });
        } catch (SQLException e) {
            LOG.warn("" SQL execute error, sql:{}"", queryTablesSql, e];
        }

### Anything else?

The limited table name can greatly reduce the query time. In the oraclexia of my current project, the original query took 1 minute, and the query data amount was very huge, exceeding 300,000 pieces, which occupied the memory. After adding the table name qualification condition, the millisecond level query was performed, and the data amount was small, reducing the memory consumption

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1999
Created by: [weAreFriendYo|https://github.com/weAreFriendYo]
Labels: bug, 
Created at: Wed Mar 15 15:05:26 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:20:28.0,,,,,,,,,,"0|z1o33s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Oracle cdc repeatedly consumes snapshot data and does not consume binlog data,FLINK-34780,13572559,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.13.5

### Flink CDC version

2.1.1

### Database and its version

	Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit Production
	and 
Non-CDB

### Minimal reproduce step

I follow the steps on the official website (https://ververica.github.io/flink-cdc-connectors/release-2.1/content/connectors/oracle-cdc.html],  everything is ok

### What did you expect to see?

I expect the oracle cdc consume snapshot data only once and continus consume binlog data

### What did you see instead?

when I consume the oracle data with flink sql client, it return all null value, when I consume the oracle data with datastream api ,it consumes snapshot data repeatedly and does not consume binlog data, what happened?

### Anything else?

no

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1982
Created by: [GeniusTian|https://github.com/GeniusTian]
Labels: bug, 
Created at: Fri Mar 10 17:58:35 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:20:26 UTC 2024,,,,,,,,,,"0|z1o33k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:20;flink-cdc-import;Date: Fri Mar 10 18:03:34 CST 2023, Author: [GeniusTian|https://github.com/GeniusTian]

        This is the code: 
        Properties properties = new Properties();
        properties.put(""database.tablename.case.insensitive"",""false"");
        properties.setProperty(""database.connection.adapter"", ""logminer"");
        properties.setProperty(""log.mining.strategy"", ""online_catalog"");
        properties.setProperty(""log.mining.continuous.mine"", ""true"");
        SourceFunction<String> sourceFunction = OracleSource.<String>builder()
                .hostname("""")
                .port(1521)
                .database(""hzdb"")
                .schemaList(""hzuser"")
                .tableList(""hzuser.flink_test_cdc"")
                .username(""hzuser"")
                .password(""hzuser_2019"")
                .deserializer(new JsonDebeziumDeserializationSchema())
                .startupOptions(StartupOptions.initial())
                .debeziumProperties(properties)
                .build();

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        env.enableCheckpointing(10000);
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

        env
                .addSource(sourceFunction)
                .print(""cdc"").setParallelism(1);

        env.execute();;;;","20/Mar/24 09:20;flink-cdc-import;Date: Fri Jun 30 16:00:57 CST 2023, Author: [ruanhang1993|https://github.com/ruanhang1993]

2.4.0 may fix this issue.;;;","20/Mar/24 09:20;flink-cdc-import;Date: Thu Jul 06 16:11:25 CST 2023, Author: [MaoHJ-Official|https://github.com/MaoHJ-Official]

> 2.4.0 may fix this issue.

什么原因导致的;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
look forward to adding dameng（dm8） connectors,FLINK-34779,13572558,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

I want to obtain incremental data from Dameng database and synchronize it to PostgreSQL database. Currently, the business data is frequently modified, and using JDBC for processing is somewhat stressful.

### Solution

Database official website：https://www.dameng.com/

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1960
Created by: [xqhxxx|https://github.com/xqhxxx]
Labels: enhancement, 
Created at: Fri Mar 03 16:15:14 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:20:19.0,,,,,,,,,,"0|z1o33c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
look forward to adding greenplum connectors,FLINK-34778,13572557,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,1,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

hope 

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1959
Created by: [fengchaochao|https://github.com/fengchaochao]
Labels: enhancement, 
Created at: Fri Mar 03 15:07:26 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:20:15.0,,,,,,,,,,"0|z1o334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] Select mysql cdc table in flink sql client caused java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapper,FLINK-34777,13572556,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.6.1

### Flink CDC version

mysql cdc 2.3.0
flink-sql-connector-mysql-cdc-2.3.0.jar

### Database and its version

MariaDB
10.10.2-MariaDB

### Minimal reproduce step

install flink from https://dlcdn.apache.org/flink/flink-1.16.1/flink-1.16.1-bin-scala_2.12.tgz
cd flink
cd lib
download flink-sql-connector-mysql-cdc-2.3.0.jar
cd ..
./bin/start-cluster.sh
./bin/sql-client.sh

CREATE TABLE mysql_test (
  Id bigint NULL,
  Guid varchar(255),
  PRIMARY KEY(Id) NOT ENFORCED
) WITH (
    'connector' = 'mysql-cdc',
    'hostname' = 'xxxxxx',
    'port' = 'xxxxx',
    'username' = 'xxxxx',
    'password' = 'xxxxx',
    'database-name' = 'dbo',
    'table-name' = 'testtable'
];

SELECT * FROM mysql_test;

### What did you expect to see?

able to see data in sql client

### What did you see instead?

Flink SQL> SELECT * FROM mysql_test;
[ERROR] Could not execute SQL statement. Reason:
java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapper


### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1958
Created by: [gojumprope|https://github.com/gojumprope]
Labels: bug, 
Created at: Fri Mar 03 10:03:51 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:20:13 UTC 2024,,,,,,,,,,"0|z1o32w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:20;flink-cdc-import;Date: Wed Oct 04 23:19:05 CST 2023, Author: [basketballboy|https://github.com/basketballboy]

解决了吗大佬;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mysql cdc restart from checkpoint failed when some binlog file has been purged,FLINK-34776,13572555,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.14.6

### Flink CDC version

2.3.X

### Database and its version

aws rds 

### Minimal reproduce step

1.start flink cdc connector job
2.clean some binlog file
3.restart from checkpoint

### What did you expect to see?

restart normally

### What did you see instead?

restart failed
io.debezium.DebeziumException: The connector is trying to read binlog starting at SourceInfo [currentGtid=null, currentBinlogFilename=mysql-bin.xxx, currentBinlogPosition=xxx, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], but this is no longer available on the server. Reconfigure the connector to use a snapshot when needed. at io.debezium.connector.mysql.MySqlConnectorTask.validateSnapshotFeasibility(MySqlConnectorTask.java:329) at io.debezium.connector.mysql.MySqlConnectorTask.start(MySqlConnectorTask.java:98) at io.debezium.connector.common.BaseSourceTask.start(BaseSourceTask.java:130) at io.debezium.embedded.EmbeddedEngine.run(EmbeddedEngine.java:759) at io.debezium.embedded.ConvertingEngineBuilder$2.run(ConvertingEngineBuilder.java:188) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1947
Created by: [dylenWu|https://github.com/dylenWu]
Labels: bug, 
Created at: Mon Feb 27 15:24:18 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:20:09 UTC 2024,,,,,,,,,,"0|z1o32o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:20;flink-cdc-import;Date: Mon Feb 27 15:33:29 CST 2023, Author: [dylenWu|https://github.com/dylenWu]

i found the bug in code 
```
 private boolean checkGtidSet(MySqlOffsetContext offset) {
       ........
        // **I think the gtid  set obtained by this method is the exectuted gtid set instead of available gtid set**
        String availableGtidStr = connection.knownGtidSet();
       ......
           ......
            if (!gtidSetToReplicate.equals(nonPurgedGtidSetToReplicate)) {
                LOG.warn(""Some of the GTIDs needed to replicate have been already purged"");
                return false;
            }
.......
    }
```
**the right avaliable gtid set should be** 
 ```
String executedGtidStr = connection.knownGtidSet();
GtidSet executedGtidSet = new GtidSet(executedGtidStr);
        final GtidSet purgedGtidSet = connection.purgedGtidSet();
        LOG.info(""Server has already purged {} GTIDs"", purgedGtidSet);
        GtidSet availableGtidSet = connection.subtractGtidSet(executedGtidSet, purgedGtidSet);
```
        

;;;","20/Mar/24 09:20;flink-cdc-import;Date: Mon Feb 27 15:38:39 CST 2023, Author: [dylenWu|https://github.com/dylenWu]

@leonardBang @wuchong If this solution works, can this issue be assigned to me, I hope I can contribute code to this project;;;","20/Mar/24 09:20;flink-cdc-import;Date: Fri Jun 16 11:10:38 CST 2023, Author: [dylenWu|https://github.com/dylenWu]

@leonardBang i have a solution for this  and I have a solution to this problem and have been proving it in production for months，please assign it to me,i can fix it;;;","20/Mar/24 09:20;flink-cdc-import;Date: Wed Mar 20 14:31:10 CST 2024, Author: [DivineDX|https://github.com/DivineDX]

Hi I think I encountered this issue, too, @dylenWu @leonardBang could you kindly handle it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] oracle cdc logminer can't catch up the latest records when scn huge increment occured.,FLINK-34775,13572554,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:20,20/Mar/24 09:20,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.16.0

### Flink CDC version

2.3.0

### Database and its version

- oracle 11g
- oracle 12c

### Minimal reproduce step

1. create a simple cdc source table（connector='oracle-cdc'）
2. this is no special requirements about sink
3. while oracle database instance scn huge increment, the LogminerStreamingChangeEvent can not catch up the latest record in time.

The problem is because `LogMinerQueryResultProcessor` can not feedback more reasonable `lastProcessedScn` after process mining view's data.  During data cycle processing，the first cycle could get the right last `processedScn`，the `endScn` will be reset to the `processedScn`, is will be used in the next cycle as `startScn`, unfortunately the next cycle can not query any mining view datas about the source table, the `startScn` can not be move an in short time any more. 
however oracle scn is already huge increment, the source table's new datas can't get in time.

the main code in `LogMinerStreamingChangeEventSource` :
```java
  final Scn lastProcessedScn = processor.getLastProcessedScn();
  if (!lastProcessedScn.isNull()
          && lastProcessedScn.compareTo(endScn) < 0) {
      // If the last processed SCN is before the endScn we need to
      // use the last processed SCN as the
      // next starting point as the LGWR buffer didn't flush all
      // entries from memory to disk yet.
      endScn = lastProcessedScn;
  }
  
  if (transactionalBuffer.isEmpty()) {
      LOGGER.debug(
              ""Buffer is empty, updating offset SCN to {}"",
              endScn);
      offsetContext.setScn(endScn];
  } 
```
BTW: The implementation has changed since version debezium 1.6.x, `LogminerStreamingChangeEvent` has been optimized in debezium-connector-oracle. 


### What did you expect to see?

reset the right `startScn` when there is no scn scan records.

### What did you see instead?

the first cycle `lastProcessedScn` worked as `startScn` in a long time.

### Anything else?

_No response_

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1940
Created by: [green1893|https://github.com/green1893]
Labels: bug, 
Created at: Fri Feb 24 14:32:22 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:20:02 UTC 2024,,,,,,,,,,"0|z1o32g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:20;flink-cdc-import;Date: Thu Mar 23 15:07:19 CST 2023, Author: [huanzui|https://github.com/huanzui]

同样的版本配置，遇到同样的问题，一直循环打日志，监听不到新记录;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports synchronizing data from the oracle standby database,FLINK-34774,13572553,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

In many financial business scenarios, the oracle database is used, generally in the active and standby mode. The main database undertakes the core transaction business, and dare not pull data from the main database in real time, otherwise it will have a great impact on the business. Small, whether it can support the function of synchronizing data from the standby database in real time

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1939
Created by: [hf200012|https://github.com/hf200012]
Labels: enhancement, 
Created at: Fri Feb 24 12:08:10 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:58 UTC 2024,,,,,,,,,,"0|z1o328:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Mon Jun 05 15:38:01 CST 2023, Author: [whmnoe4j|https://github.com/whmnoe4j]

这是一个很常见的需求，在电信，银行，保险，电力等行业基本都是oracle吧;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug][mysql-cdc] com.ververica.cdc.connectors.mysql.source.utils.StatementUtils#initStatement leads to uncommitted transactions,FLINK-34773,13572552,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Flink version

1.16

### Flink CDC version

2.3

### Database and its version

Mysql 8.0 official docker image

### Minimal reproduce step

1. Start flink-job with following config;
 - long checkpoint interval 
 - Mysql source with parallelism > 1 and initial mode.
2. Wait for snapshot read
3. Before checkpoint complete
Several uncommitted transactions will be observed from mysql console. May cause SQL execution hang.

### What did you expect to see?

No open transactions should be observed after snapshot read split finished. May cause SQL execution hang.

### What did you see instead?

Several uncommitted transactions can be observed after snapshot read split finished. May cause SQL execution hang.

### Anything else?

Not yet

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1926
Created by: [lucifer-yqh|https://github.com/lucifer-yqh]
Labels: bug, 
Created at: Mon Feb 20 17:24:22 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:53 UTC 2024,,,,,,,,,,"0|z1o320:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Mon Feb 20 17:34:07 CST 2023, Author: [lucifer-yqh|https://github.com/lucifer-yqh]

[code location|https://github.com/ververica/flink-cdc-connectors/blob/eb5610c29bf6deee9abc1f5a28208f0a0b02b99a/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/utils/StatementUtils.java#L237]

I am not sure if set auto-commit to false and never set back was intended . According to no isolation level required in jdbc connection, I will assume this is an issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Bug] [Critical] cdc-myql-connector snapshot/binlog phase MySQL -> MySQL temporal related field transfer WRONG with/or wrong epoch time across timezone ,FLINK-34772,13572551,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues) and found nothing similar.


### Flink version

1.14.3

### Flink CDC version

2.3.0

### Database and its version

mysql 8.0.23

### Minimal reproduce step

source db DDL:
```
CREATE TABLE `t_cdc_bench` (
  `order_id` bigint NOT NULL,
  `order_date` date DEFAULT NULL,
  `order_time` datetime DEFAULT NULL,
  `dt_3` datetime(3) DEFAULT CURRENT_TIMESTAMP(3),
  `ts_3` timestamp(3) NULL DEFAULT CURRENT_TIMESTAMP(3),
  `quantity` int DEFAULT NULL,
  `product_id` int DEFAULT NULL,
  `purchaser` varchar(32) COLLATE utf8mb4_bin DEFAULT NULL,
  `acct` varchar(3) COLLATE utf8mb4_bin DEFAULT '911',
  PRIMARY KEY (`order_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;
```
sink db DDL:
```
CREATE TABLE `t_cdc_bench_sink` (
  `order_id` bigint NOT NULL,
  `order_date` date DEFAULT NULL,
  `order_time` datetime DEFAULT NULL,
  `dt_3` datetime(3) DEFAULT NULL,
  `ts_3` timestamp(3) NULL DEFAULT NULL,
  `quantity` int DEFAULT NULL,
  `product_id` int DEFAULT NULL,
  `purchaser` varchar(32) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin DEFAULT NULL,
  PRIMARY KEY (`order_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;
```
source DML:
```
insert into t_cdc_bench values(1001, curdate(), now(), current_timestamp(3), current_timestamp(3), 1, 33, 'scala', 911];
```
run table cdc process:
```
public class MySqlToJdbcTableCDC {
    public static void main(String[] args) throws Exception{
        JSONObject json = JSONObject.parseObject(""{\""logConfDirection\"": \""\\/Users\\/nirvana.xu\\/IdeaProjects\\/Prod\\/realtime-ingestion\\/flink-cdc-jobs\\/src\\/main\\/resources\\/log4j2.xml\"" }"");
        SourceConfigUtil.doSetupLogConfigDir(json, MySqlToJdbcTableCDC.class);
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
                .useBlinkPlanner()
                .inStreamingMode()
                .build();
        env.setParallelism(1);
        // note: 增量同步需开启 cpkt
        env.enableCheckpointing(3000);
        StreamTableEnvironment tableEnvironment = StreamTableEnvironment.create(env, envSettings);

        tableEnvironment.executeSql("" CREATE TABLE demoOrders (\n"" +
                ""         `order_id` INTEGER ,\n"" +
                ""          `order_date` DATE ,\n"" +
                ""          `order_time` TIMESTAMP(3),\n"" +
                ""          `dt_3` TIMESTAMP(3),\n"" +
                ""          `ts_3` TIMESTAMP(3),\n"" +
                ""          `quantity` INT ,\n"" +
                ""          `product_id` INT ,\n"" +
                ""          `purchaser` STRING,\n"" +
                ""           primary key(order_id)  NOT ENFORCED"" +
                ""         ) WITH (\n"" +
                ""          'connector' = 'mysql-cdc',\n"" +
                ""          'hostname' = '10.162.34.110',\n"" +
                ""          'port' = '6606',\n"" +
                ""          'username' = 'root',\n"" +
                ""          'password' = 'root',\n"" +
                ""          'database-name' = 'testdb',\n"" +
                ""          'table-name' = 't_cdc_bench', "" +
                ""          'server-time-zone' = 'UTC+7', "" +
                //  全量 + 增量同步
                ""          'scan.startup.mode' = 'initial'      "" +
                "" )"");

        tableEnvironment.executeSql(""CREATE TABLE sink (\n"" +
                ""         `order_id` INTEGER ,\n"" +
                ""          `order_date` DATE ,\n"" +
                ""          `order_time` TIMESTAMP(3),\n"" +
                ""          `dt_3` TIMESTAMP(3),\n"" +
                ""          `ts_3` TIMESTAMP(3),\n"" +
                ""          `quantity` INT ,\n"" +
                ""          `product_id` INT ,\n"" +
                ""          `purchaser` STRING,\n"" +
                ""          primary key (order_id)  NOT ENFORCED "" +
                "") WITH (\n"" +
                "" 'connector' = 'jdbc',\n"" +
                "" 'url' = 'jdbc:mysql://localhost:3306/testdb?useSSL=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&GMT%2b8',\n"" +
                "" 'username' = 'root',\n"" +
                "" 'password' = 'root',\n"" +
                "" 'table-name' = 't_cdc_bench_sink',\n"" +
                "" 'driver' = 'com.mysql.cj.jdbc.Driver',\n"" +
                "" 'sink.buffer-flush.interval' = '3s',\n"" +
                "" 'sink.buffer-flush.max-rows' = '1',\n"" +
                "" 'sink.max-retries' = '5') "");

        tableEnvironment.executeSql(""insert into sink select * from demoOrders"");

    }
}
```


### What did you expect to see?

Presumably, after cdc change to binlog phase, we are expected to see :

**source**:
```
select * from t_cdc_bench where order_id = 1001 limit 1\G
```
*************************** 1. row ***************************
  order_id: 1001
order_date: 2023-02-10
order_time: 2023-02-10 17:15:33
      dt_3: 2023-02-10 17:15:33.312
      ts_3: 2023-02-10 17:15:33.312
  quantity: 1
product_id: 33
 purchaser: scala
      acct: 911

**sink**:
```
select * from t_cdc_bench_sink where order_id = 1001 limit 1\G
```
*************************** 1. row ***************************
  order_id: 1001
order_date: 2023-02-10
order_time: 2023-02-10 18:15:33
      dt_3: 2023-02-10 18:15:33.312
      ts_3: 2023-02-10 18:15:33.312
  quantity: 1
product_id: 33
 purchaser: scala


_**the datetime and timestamp field correctly converted by pre-settings of source/sink server timezone**_

### What did you see instead?

Actually, after cdc shift to binlog phase, query source/sink table result in snapshot data:

**source**:
```
select * from t_cdc_bench where order_id = 1001 limit 1\G
```
*************************** 1. row ***************************
  order_id: 1001
order_date: 2023-02-10
order_time: 2023-02-10 17:15:33
      dt_3: 2023-02-10 17:15:33.312
      ts_3: 2023-02-10 17:15:33.312
  quantity: 1
product_id: 33
 purchaser: scala
      acct: 911

**sink**:
```
select * from t_cdc_bench_sink where order_id = 1001 limit 1\G
```
*************************** 1. row ***************************
  order_id: 1001
order_date: 2023-02-10
order_time: 2023-02-10 17:15:33
      dt_3: 2023-02-10 17:15:33.312
      ts_3: 2023-02-11 00:15:33.312
  quantity: 1
product_id: 33
 purchaser: scala

**_Not only the LTZ is not correctly converted, the dt_3 and ts_3 field changed at same time are contradict to each other_**

### Anything else?

At start, We find this issue from internal version of cdc 2.3-SNAPSHOT with stream API with our Production realtime computing services we got from cdc source (the epoch time of source MySQL binlog datetime type transferred wrong) , and then fixed by temporal related deserializer remedy, then I was wondering is it a fixed issue with 2.3.0 mysql-cdc tables, but with no luck, it's still a bug

Notice when we were updating `ts_3` and `dt_3` field, the contradiction issue is updated, but still, the local timezone conversion is not seen

So the main issues are :

1. For the historical data that are scanned only in snapshot state, the  `ts_3` and `dt_3` field are definitely WRONG if the data not updated;

2. For the source and sink end across regions, the temporal fields cannot be correctly converted.

### Are you willing to submit a PR?

- [X] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1906
Created by: [Capri0110|https://github.com/Capri0110]
Labels: bug, 
Created at: Fri Feb 10 18:35:08 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:19:47.0,,,,,,,,,,"0|z1o31s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC connector with YugabyteDB,FLINK-34771,13572550,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"### Search before asking

- [X] I searched in the [issues|https://github.com/ververica/flink-cdc-connectors/issues] and found nothing similar.


### Motivation

Are there any plan to build a Flink CDC connector with YugabyteDB, an open source Distributed SQL database? Or what's required to take this forward?

### Solution

_No response_

### Alternatives

_No response_

### Anything else?

_No response_

### Are you willing to submit a PR?

- [ ] I'm willing to submit a PR!

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1899
Created by: [xinghua0123|https://github.com/xinghua0123]
Labels: enhancement, 
Created at: Tue Feb 07 12:56:04 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:43 UTC 2024,,,,,,,,,,"0|z1o31k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Wed Feb 08 11:09:40 CST 2023, Author: [MengXiangDing|https://github.com/MengXiangDing]

mark！;;;","20/Mar/24 09:19;flink-cdc-import;Date: Mon Sep 25 10:56:59 CST 2023, Author: [lvyanquan|https://github.com/lvyanquan]

Not support yet, maybe we can refer to https://github.com/yugabyte/debezium-connector-yugabytedb someday.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify download links of legacy connector to fat jar,FLINK-34770,13572549,13572360,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kunni,kunni,kunni,20/Mar/24 09:19,22/Mar/24 07:02,04/Jun/24 20:40,22/Mar/24 07:02,cdc-3.1.0,,,,,,cdc-3.1.0,,,,chinese-translation,Documentation,Flink CDC,,,0,pull-request-available,,,"Some links will lead to download fat jar, but some links will lead to download thin jar, and some are invalid.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 07:02:34 UTC 2024,,,,,,,,,,"0|z1o31c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/24 07:02;leonard;flink-cdc master: 0ab57ac1ab540bd7ebc62d89d5dc877ee62b8b57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[mysql-cdc] Add upsert changelog mode to avoid UPDATE_BEFORE records push down,FLINK-34769,13572548,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Is your feature request related to a problem? Please describe.**
I try to use flink sql to write mysql cdc-data into redis as a dimension table for other business use. When executing `UPDATE` DML, the cdc-data will be converted into `-D (UPDATE_BEFORE)` and `+I (UPDATE_AFTER)`  two records to sink redis. However, delete first will cause other data streams to be lost(NULL) when join data, which is unacceptable.


**Describe the solution you'd like**
I think we can add support for [upsert changelog mode|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables/#table-to-stream-conversion] by adding changelogMode option with mandatory primary key configuration.Basically, with `changelogMode=upsert` we will avoid `UPDATE_BEFORE` rows and we will require a primary key for the table. 

**Describe alternatives you've considered**
n/a

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1898
Created by: [yeezychao|https://github.com/yeezychao]
Labels: enhancement, 
Created at: Tue Feb 07 11:16:04 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:39 UTC 2024,,,,,,,,,,"0|z1o314:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Tue Feb 07 11:17:43 CST 2023, Author: [yeezychao|https://github.com/yeezychao]

Hi @leonardBang, I am happy to send a PR for this if you agree this makes sense. ;;;","20/Mar/24 09:19;flink-cdc-import;Date: Wed Mar 20 12:31:27 CST 2024, Author: [tony0021074|https://github.com/tony0021074]

Postgresql connector's changelog mode can be configurated through Flink SQL API while MySQL connector's cannot.

I hope @yeezychao proposed PR would be reviewed & merged soon. @leonardBang ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"simple tidb cdc demo,but caused 'java.lang.IllegalArgumentException: Invalid range: '",FLINK-34768,13572547,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Describe the bug(Please use English)**
A clear and concise description of what the bug is.

**Environment :**
 - Flink version :  1.14.5
 - Flink CDC version: 2.4-SNAPSHOT
 - Database and version: TIDB v5.3.1

**To Reproduce**
Steps to reproduce the behavior:
1. The test data :
2. The test code :
 CREATE TABLE tidb_source (
id INT,
name STRING,
description STRING,
PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'tidb-cdc',
    'tikv.grpc.timeout_in_ms' = '20000',
    'pd-addresses' = '127.0.1.1:2379',
    'database-name' = 'test',
    'table-name' = 'products'
);
CREATE TABLE sink (
id INT,
name STRING,
description STRING,
PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'print'
);
        stEnv.executeSql(sourcre);
        stEnv.executeSql(sink);
        TableResult result =
                stEnv.executeSql(
                        ""INSERT INTO sink select `id`, name, description FROM tidb_source t"");
        result.print();

4. The error :

Caused by: java.lang.IllegalArgumentException: Invalid range: [+INF..{116,128,0,0,0,0,0,0,57,95,114,85,85,85,85,85,85,85,84})
	at org.apache.flink.shaded.guava30.com.google.common.collect.Range.<init>(Range.java:358)
	at org.apache.flink.shaded.guava30.com.google.common.collect.Range.create(Range.java:156)
	at org.apache.flink.shaded.guava30.com.google.common.collect.Range.intersection(Range.java:558)
	at org.tikv.cdc.CDCClient.overlapWithRegion(CDCClient.java:235)
	at org.tikv.cdc.CDCClient.addRegions(CDCClient.java:185)
	at org.tikv.cdc.CDCClient.applyKeyRange(CDCClient.java:178)
	at org.tikv.cdc.CDCClient.start(CDCClient.java:98)
	at com.ververica.cdc.connectors.tidb.TiKVRichParallelSourceFunction.run(TiKVRichParallelSourceFunction.java:165)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:323)


**Additional Description**
If applicable, add screenshots to help explain your problem.


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1872
Created by: [Cqz666|https://github.com/Cqz666]
Labels: bug, 
Created at: Thu Jan 12 14:59:54 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:30 UTC 2024,,,,,,,,,,"0|z1o30w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Thu Jan 12 15:03:45 CST 2023, Author: [Cqz666|https://github.com/Cqz666]

@leonardBang help plz !;;;","20/Mar/24 09:19;flink-cdc-import;Date: Thu Apr 13 17:14:17 CST 2023, Author: [zhangzhaohuazai|https://github.com/zhangzhaohuazai]

I meet the error too:
`java.lang.IllegalArgumentException: Invalid range: [+INF..{116,128,0,0,0,0,0,3,162,95,114,63,255,255,255,255,255,255,254})`;;;","20/Mar/24 09:19;flink-cdc-import;Date: Mon Apr 17 10:21:50 CST 2023, Author: [hbk671104|https://github.com/hbk671104]

same error. please help.

<img width=""1858"" alt=""企业微信截图_168169783555"" src=""https://user-images.githubusercontent.com/5074788/232362839-1506132d-a25b-43a9-a83e-2b716c7d37f5.png"">
;;;","20/Mar/24 09:19;flink-cdc-import;Date: Mon May 08 09:13:01 CST 2023, Author: [zhangzhaohuazai|https://github.com/zhangzhaohuazai]

How is it going now？;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-sql-connector-mysql-cdc cannot read incremental data in initial mode,FLINK-34767,13572546,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Describe the bug(Please use English)**
A clear and concise description of what the bug is.

**Environment :**
 - Flink version :  flink-1.14.2
 - Flink CDC version: 2.3.0
 - Database and version: mysql 5.7.37

**To Reproduce**
Steps to reproduce the behavior:
1. The test data :Synchronize all tables in a database , about 7 tables;
2. The test code :
```java
public static MySqlSource<String> getMySqlSource() {
        return MySqlSource.<String>builder()
                .hostname(SOURCE_IP)
                .port(SOURCE_PORT)
                .databaseList(SOURCE_DB) // set captured database
                .tableList(SOURCE_TABLS) // set captured table
                .username(SOURCE_USER)
                .password(SOURCE_PWD)
                .serverTimeZone(""Asia/Shanghai"") //控制MYSQL中的时间戳类型如何转换为字符串
                .startupOptions(StartupOptions.initial())
                .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String
                .build();
    }
```
3. The error :
There is no error message, I opened the debug log of yarn, and found that the program could not reach this step
```java
 @Override
    public void addSplits(List<MySqlSplit> splits) {
        // restore for finishedUnackedSplits
        List<MySqlSplit> unfinishedSplits = new ArrayList<>();
        for (MySqlSplit split : splits) {
            LOG.info(""Add Split: "" + split);
            if (split.isSnapshotSplit()) {
                MySqlSnapshotSplit snapshotSplit = split.asSnapshotSplit();
                if (snapshotSplit.isSnapshotReadFinished()) {
                    finishedUnackedSplits.put(snapshotSplit.splitId(), snapshotSplit);
                } else {
                    unfinishedSplits.add(split);
                }
            } else {
                MySqlBinlogSplit binlogSplit = split.asBinlogSplit();
                // the binlog split is suspended
                if (binlogSplit.isSuspended()) {
                    suspendedBinlogSplit = binlogSplit;
                } else if (!binlogSplit.isCompletedSplit()) {
                    uncompletedBinlogSplits.put(split.splitId(), split.asBinlogSplit());
                    requestBinlogSplitMetaIfNeeded(split.asBinlogSplit());
                } else {
                    uncompletedBinlogSplits.remove(split.splitId());
                    MySqlBinlogSplit mySqlBinlogSplit =
                            discoverTableSchemasForBinlogSplit(split.asBinlogSplit());
                    unfinishedSplits.add(mySqlBinlogSplit);
                }
            }
        }
        // notify split enumerator again about the finished unacked snapshot splits
        reportFinishedSnapshotSplitsIfNeed();
        // add all un-finished splits (including binlog split) to SourceReaderBase
        if (!unfinishedSplits.isEmpty()) {
            super.addSplits(unfinishedSplits);
        }
    }
```
The log of the table DiscoveryUtils discovery table will not appear in the log like a normal program;
The default task manager memory of the cluster is 1.7GB. If I increase the memory allocated by yarn to task manager to 4GB, this problem can be solved



---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1869
Created by: [red-cy|https://github.com/red-cy]
Labels: bug, 
Created at: Wed Jan 11 19:15:30 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:19:24.0,,,,,,,,,,"0|z1o30o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure checkpointing is enabled for Postgres connector,FLINK-34766,13572545,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Is your feature request related to a problem? Please describe.**

Yes, it is: when not enabling checkpointing with the Debezium-based Postgres connector, then the replication slot in the database will never be advanced (as this is triggered by committing offsets in Debezium, which in turn is triggered by checkpointing in Flink CDC), causing WAL to be retained indefinitely.

**Describe the solution you'd like**
Raise a warning or even an exception when using this connector without checkpointing.

**Describe alternatives you've considered**
n/a


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1866
Created by: [gunnarmorling|https://github.com/gunnarmorling]
Labels: enhancement, 
Assignee: [gunnarmorling|https://github.com/gunnarmorling]
Created at: Tue Jan 10 20:43:21 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:21 UTC 2024,,,,,,,,,,"0|z1o30g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Tue Jan 10 20:46:39 CST 2023, Author: [gunnarmorling|https://github.com/gunnarmorling]

Hey @leonardBang, happy to send a PR for this if you agree this makes sense. After discussing with @rmetzger, we think it should be doable rather easily by checking `StreamingRuntimeContext::isCheckpointingEnabled()` in `DebeziumSourceFunction::open()`. WDYT?;;;","20/Mar/24 09:19;flink-cdc-import;Date: Wed Jan 11 10:32:38 CST 2023, Author: [leonardBang|https://github.com/leonardBang]

Thanks @gunnarmorling for reporting this ticket, it makes sense to me, this could improve all Debezium-based   connectors, maybe we can make this issue to an umbrella issue, the sub-tasks would contains :
- Ensure checkpointing is enabled for Debezium-based(DebeziumSourceFunction) connector
- Ensure checkpointing is enabled for MySQL/JdbcIncrementalSource connector
- Ensure checkpointing is enabled for OceanBase/TiDB(xxSourceFunction) connector


;;;","20/Mar/24 09:19;flink-cdc-import;Date: Mon Feb 05 19:22:16 CST 2024, Author: [loserwang1024|https://github.com/loserwang1024]

hi, @gunnarmorling, in JdbcIncrementalSource connector,  only when  checkpoint is enabled,  the enumerator will assign stream split. 
```java
    @Override
    public void notifyCheckpointComplete(long checkpointId) {
        // we have waited for at-least one complete checkpoint after all snapshot-splits are
        // finished, then we can mark snapshot assigner as finished.
        if (checkpointIdToFinish != null && !assignerFinished && allSplitsFinished()) {
            assignerFinished = checkpointId >= checkpointIdToFinish;
            LOG.info(""Snapshot split assigner is turn into finished status."");
        }
    }
```
The PG CDC connector no longer recommends the non-incremental version of CDC; therefore, I believe this issue has been resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Does flink cdc plan to support duckdb sink,FLINK-34765,13572544,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Is your feature request related to a problem? Please describe.**
Duckdb has fast query performance. If traditional RDMS data can be written to Duckdb for analysis through Flink CDC, it will be a more convenient solution

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1860
Created by: [dawsongzhao|https://github.com/dawsongzhao]
Labels: enhancement, 
Created at: Sat Jan 07 17:06:11 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:19:14.0,,,,,,,,,,"0|z1o308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkCDC is incompatible with MariaDB 10.0.24,FLINK-34764,13572543,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"FlinkCDC is incompatible with MariaDB 10.0.24.

There will be **exceptions** when **debezium** parses the binlog.

mysqlbinlog fails to read binlog event which inserts **TIME, DATETIME and TIMESTAMP** with wrong number of microseconds.

The low-level temporal format used by TIME, DATETIME and TIMESTAMP is different in MySQL 5.6 and [MariaDB 10.0|https://mariadb.com/kb/en/what-is-mariadb-100/]. (In [MariaDB 10.1|https://mariadb.com/kb/en/what-is-mariadb-101/], the MySQL implementation is used by default - see [mysql56_temporal_format|https://mariadb.com/kb/en/server-system-variables/#mysql56_temporal_format).)

**Environment :**
 - Flink version :  1.13.6
 - Flink CDC version: 2.2.1
 - Database and version: MariaDB 10.0.24

**To Reproduce**
Steps to reproduce the behavior:
1. The test code :
The field type in the database is **Datetime(6)**, and the value is **2016-12-21 17:57:16.000000**. 
After analysis, the obtained string is **103946-26-59 06:40:65**.
2. The error :
There will be exceptions when debezium parses the binlog:

`java.io.EOFException: Failed to read next byte from position -1536265298
	at com.github.shyiko.mysql.binlog.io.ByteArrayInputStream.read(ByteArrayInputStream.java:213)
	at com.github.shyiko.mysql.binlog.io.ByteArrayInputStream.readInteger(ByteArrayInputStream.java:52)
	at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeLong(AbstractRowsEventDataDeserializer.java:261)
	at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeCell(AbstractRowsEventDataDeserializer.java:166)
	at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeRow(AbstractRowsEventDataDeserializer.java:145)
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserializeRows(UpdateRowsEventDataDeserializer.java:72)
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserialize(UpdateRowsEventDataDeserializer.java:58)
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserialize(UpdateRowsEventDataDeserializer.java:33)
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:329)
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.nextEvent(EventDeserializer.java:238)
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$1.nextEvent(MySqlStreamingChangeEventSource.java:233)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:949)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:599)
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:854)
	at java.lang.Thread.run(Thread.java:748)
java.io.EOFException: Failed to read next byte from position -1536265298
	at com.github.shyiko.mysql.binlog.io.ByteArrayInputStream.read(ByteArrayInputStream.java:213)
	at com.github.shyiko.mysql.binlog.io.ByteArrayInputStream.readInteger(ByteArrayInputStream.java:52)
	at io.debezium.connector.mysql.RowDeserializers.deserializeVarString(RowDeserializers.java:298)
	at io.debezium.connector.mysql.RowDeserializers$UpdateRowsDeserializer.deserializeVarString(RowDeserializers.java:130)
	at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeCell(AbstractRowsEventDataDeserializer.java:194)
	at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeRow(AbstractRowsEventDataDeserializer.java:145)
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserializeRows(UpdateRowsEventDataDeserializer.java:72)
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserialize(UpdateRowsEventDataDeserializer.java:58)
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserialize(UpdateRowsEventDataDeserializer.java:33)
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:329)
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.nextEvent(EventDeserializer.java:238)
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$1.nextEvent(MySqlStreamingChangeEventSource.java:233)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:949)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:599)
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:854)
	at java.lang.Thread.run(Thread.java:748]
`

<img width=""946"" alt=""image"" src=""https://user-images.githubusercontent.com/35026750/210321758-45048701-d93c-488f-ac32-2321180e50d9.png"">



**Additional Description**

[MariaDB 10.1.2|https://mariadb.com/kb/en/mariadb-1012-release-notes/] introduced the [--mysql56-temporal-format|https://mariadb.com/kb/en/server-system-variables/#mysql56_temporal_format] option, on by default, which allows MariaDB to store DATETMEs using the same low-level format MySQL 5.6 uses. For more information, see [Internal Format|https://mariadb.com/kb/en/datetime/#internal-format], below.

[incompatibilities between MariaDB 10.0 and MySQL 5.6|https://mariadb.com/kb/en/mariadb-vs-mysql-compatibility]
[mysql56_temporal_format|https://mariadb.com/kb/en/server-system-variables/#mysql56_temporal_format]
[DATETIME|https://mariadb.com/kb/en/datetime]
[mysqlbinlog fails to read binlog event which inserts timestamp with wrong number of microseconds|https://jira.mariadb.org/browse/MDEV-9567]
[MariaDB 10.0.25 Datetime 类型无法解析毫秒级|https://github.com/alibaba/canal/issues/385]


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1851
Created by: [lollipopcs|https://github.com/lollipopcs]
Labels: bug, 
Created at: Tue Jan 03 16:12:57 CST 2023
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:11 UTC 2024,,,,,,,,,,"0|z1o300:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Mon Jan 16 10:35:17 CST 2023, Author: [ThisisWilli|https://github.com/ThisisWilli]

Maraidb's binlog format is different from mysql's binlog format, you should change debezium's source code to adapt this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OracleIncrementalSource at Oracle CDB mode,FLINK-34763,13572542,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Describe the bug(Please use English)**
it could not read the PDB tables when using OracleIncrementalSource at Oracle CDB mode,Therefore, check whether the pdb is empty when creating a connection. If it is not empty, switch to the pdb session

**Environment :**
 - Flink version :  1.16.0
 - Flink CDC version: 2.4-SNAPSHOT
 - Database and version: Oracle 19c cdb

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1837
Created by: [luckes-yang|https://github.com/luckes-yang]
Labels: bug, 
Created at: Wed Dec 21 16:45:12 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:06 UTC 2024,,,,,,,,,,"0|z1o2zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Wed Dec 21 16:46:01 CST 2022, Author: [luckes-yang|https://github.com/luckes-yang]

I have commit a pr:
https://github.com/ververica/flink-cdc-connectors/pull/1836, please merge it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"use earliest-offset or timestamp mode to consume data,flink job always restart",FLINK-34762,13572541,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:19,20/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Describe the bug(Please use English)**
A clear and concise description of what the bug is.

**Environment :**
 - Flink version :  1.13.6
 - Flink CDC version: 2.3
 - Database and version: mysql 5.7

**To Reproduce**
Steps to reproduce the behavior:
1. The test data :
2. The test code :
3. The error :

**Additional Description**
when we use mysql-cdc 's earliest-offset or timestamp mode to consume data,the flink job always restart,but after rebooting a few times,the job is normal again


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1829
Created by: [hnjincan|https://github.com/hnjincan]
Labels: bug, 
Created at: Wed Dec 14 17:37:16 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:19:01 UTC 2024,,,,,,,,,,"0|z1o2zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:19;flink-cdc-import;Date: Wed Dec 14 22:09:54 CST 2022, Author: [hnjincan|https://github.com/hnjincan]

java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:223)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:154)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:116)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:141)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:305)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:69)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:423)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:684)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:639)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:650)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:623)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:148)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:103)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped.
	at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:42)
	at com.ververica.cdc.connectors.mysql.debezium.task.context.MySqlErrorHandler.setProducerThrowable(MySqlErrorHandler.java:89)
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$ReaderThreadLifecycleListener.onCommunicationFailure(MySqlStreamingChangeEventSource.java:1439)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:980)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:599)
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:857)
	... 1 more
Caused by: io.debezium.DebeziumException: Failed to deserialize data of EventHeaderV4{timestamp=1671011406000, eventType=EXT_WRITE_ROWS, serverId=20181218, headerLength=19, dataLength=117, nextPosition=503157102, flags=0}
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.wrap(MySqlStreamingChangeEventSource.java:1383)
	... 5 more
Caused by: com.github.shyiko.mysql.binlog.event.deserialization.EventDataDeserializationException: Failed to deserialize data of EventHeaderV4{timestamp=1671011406000, eventType=EXT_WRITE_ROWS, serverId=20181218, headerLength=19, dataLength=117, nextPosition=503157102, flags=0}
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:309)
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.nextEvent(EventDeserializer.java:232)
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$1.nextEvent(MySqlStreamingChangeEventSource.java:256)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:952)
	... 3 more
Caused by: java.io.EOFException
	at com.github.shyiko.mysql.binlog.io.ByteArrayInputStream.fill(ByteArrayInputStream.java:113)
	at com.github.shyiko.mysql.binlog.io.ByteArrayInputStream.read(ByteArrayInputStream.java:104)
	at io.debezium.connector.mysql.RowDeserializers.deserializeVarString(RowDeserializers.java:264)
	at io.debezium.connector.mysql.RowDeserializers$WriteRowsDeserializer.deserializeVarString(RowDeserializers.java:192)
	at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeCell(AbstractRowsEventDataDeserializer.java:189)
	at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeRow(AbstractRowsEventDataDeserializer.java:143)
	at com.github.shyiko.mysql.binlog.event.deserialization.WriteRowsEventDataDeserializer.deserializeRows(WriteRowsEventDataDeserializer.java:64)
	at com.github.shyiko.mysql.binlog.event.deserialization.WriteRowsEventDataDeserializer.deserialize(WriteRowsEventDataDeserializer.java:56)
	at com.github.shyiko.mysql.binlog.event.deserialization.WriteRowsEventDataDeserializer.deserialize(WriteRowsEventDataDeserializer.java:32)
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:303)
	... 6 more
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support table.exclude.list as Debezium does,FLINK-34761,13572540,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:18,20/Mar/24 09:18,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Is your feature request related to a problem? Please describe.**
Parameter tableList is required in MySqlSourceConfig, so only table.include.list is effective even if table.exclude.list is specified in debezium properties.

**Describe the solution you'd like**
Remove the null check in MySqlSourceConfig:
```        this.tableList = checkNotNull(tableList);```
, or add a new parameter tableExcludeList in CDC


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1817
Created by: [ldwnt|https://github.com/ldwnt]
Labels: enhancement, 
Created at: Fri Dec 09 18:18:02 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:18:56.0,,,,,,,,,,"0|z1o2zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor state serializers in MySQL CDC source,FLINK-34760,13572539,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:18,20/Mar/24 09:18,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"Currently the logic of state serializer in MySQL CDC source is quite complex and hard to maintain, as there exists multiple kind of versions managed by different styles (versioned serializers, utils...). We could make a refactor to the current state serializers:

1. All subcomponents should be fully version managed and document the format of serialized binaries. 
2. Add compatibility tests to prevent potential unexpected incompatible changes. 

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1813
Created by: [PatrickRen|https://github.com/PatrickRen]
Labels: bug, 
Created at: Thu Dec 08 16:12:43 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:18:51.0,,,,,,,,,,"0|z1o2z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not start FlinkCDC 2.3 with startupOptions = specificOffset,FLINK-34759,13572538,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:18,20/Mar/24 09:18,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"I just upgraded the FlinkCDC in my job from 2.1.1 to 2.3.0 to leverage the new feature ""Scan Newly Added Tables"" (the table was not in the tableList in the first place). But I ended up with the ""Name is null"" exception:
```
java.lang.NullPointerException: Name is null
	at java.lang.Enum.valueOf(Enum.java:236) ~[?:1.8.0_312]
	at com.ververica.cdc.connectors.mysql.source.offset.BinlogOffsetKind.valueOf(BinlogOffsetKind.java:26) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.offset.BinlogOffset.getOffsetKind(BinlogOffset.java:136) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.utils.SerializerUtils.readBinlogPosition(SerializerUtils.java:73) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.utils.SerializerUtils.readBinlogPosition(SerializerUtils.java:59) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.split.MySqlSplitSerializer.deserializeSplit(MySqlSplitSerializer.java:153) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.split.MySqlSplitSerializer.deserialize(MySqlSplitSerializer.java:122) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.split.MySqlSplitSerializer.deserialize(MySqlSplitSerializer.java:46) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3.0]
	at org.apache.flink.core.io.SimpleVersionedSerialization.readVersionAndDeSerialize(SimpleVersionedSerialization.java:165) ~[flink-core-1.13.5.jar:1.13.5]
	at org.apache.flink.streaming.api.operators.util.SimpleVersionedListState$DeserializingIterator.next(SimpleVersionedListState.java:138) ~[flink-streaming-java_2.11-1.13.5.jar:1.13.5]
	at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[?:1.8.0_312]
	at org.apache.flink.util.CollectionUtil.iterableToList(CollectionUtil.java:95) ~[flink-core-1.13.5.jar:1.13.5]
	at org.apache.flink.streaming.api.operators.SourceOperator.open(SourceOperator.java:251) ~[flink-streaming-java_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:442) ~[flink-streaming-java_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:585) ~[flink-streaming-java_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-streaming-java_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:565) ~[flink-streaming-java_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:650) ~[flink-streaming-java_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:540) ~[flink-streaming-java_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_312]
```
I could not find solutions to this issue here, so turned to another new feature: scan.startup.mode = specific-offset. This time I got exception ""No TableMapEventData has been found for table id"". Issues (https://github.com/ververica/flink-cdc-connectors/issues/276, https://github.com/ververica/flink-cdc-connectors/issues/411) were closed but still reported to exist, as what I observed here. Hoping I could get some help here.

**Environment :**
 - Flink version :  1.13.5
 - Flink CDC version: 2.3.0
 - Database and version: mysql 8.0.25

**To Reproduce**
Steps to reproduce the behavior:
1. The test data :
2. The test code :
```
            MySqlSourceBuilder<String> builder = MySqlSource.<String>builder()
                .hostname(datasource.getConnect().getHost())
                .port(Integer.parseInt(datasource.getConnect().getPort()))
                .username(datasource.getConnect().getUsername())
                .password(datasource.getConnect().getPassword())
                .databaseList(datasource.getDatabase())
                .tableList(tableRegex)
                .scanNewlyAddedTableEnabled(true)
                .startupOptions(StartupOptions.specificOffset(""binlog.000017"", 499573306))
                .includeSchemaChanges(true)
                .deserializer(new ChangJsonDeserializationSchema(true))
                .serverId(serverId)
                .debeziumProperties(properties);
```
4. The error :
```
2022-12-05 16:02:06,396 INFO  [blc-mysql-sit.deepq.tech:3306] io.debezium.connector.mysql.MySqlStreamingChangeEventSource [1185] - Connected to MySQL binlog at mysql-sit.deepq.tech:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=binlog.000017, currentBinlogPosition=499573306, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], partition={server=mysql_binlog_source}, snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=binlog.000017, restartBinlogPosition=499573306, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]
2022-12-05 16:02:06,399 INFO  [debezium-reader-0] io.debezium.connector.mysql.MySqlStreamingChangeEventSource [917] - Waiting for keepalive thread to start
2022-12-05 16:02:06,399 INFO  [blc-mysql-sit.deepq.tech:3306] io.debezium.util.Threads [287] - Creating thread debezium-mysqlconnector-mysql_binlog_source-binlog-client
2022-12-05 16:02:06,400 INFO  [debezium-reader-0] io.debezium.connector.mysql.MySqlStreamingChangeEventSource [924] - Keepalive thread is running
2022-12-05 16:02:06,405 ERROR [blc-mysql-sit.deepq.tech:3306] io.debezium.connector.mysql.MySqlStreamingChangeEventSource [1054] - Error during binlog processing. Last offset stored = null, binlog reader near position = binlog.000017/499573306
2022-12-05 16:02:06,430 ERROR [blc-mysql-sit.deepq.tech:3306] io.debezium.pipeline.ErrorHandler [31] - Producer failure
io.debezium.DebeziumException: Failed to deserialize data of EventHeaderV4{timestamp=1670226722000, eventType=EXT_UPDATE_ROWS, serverId=1, headerLength=19, dataLength=31, nextPosition=499573356, flags=0}
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.wrap(MySqlStreamingChangeEventSource.java:1154) ~[debezium-connector-mysql-1.6.4.Final.jar:1.6.4.Final]
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$ReaderThreadLifecycleListener.onEventDeserializationFailure(MySqlStreamingChangeEventSource.java:1207) [debezium-connector-mysql-1.6.4.Final.jar:1.6.4.Final]
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:958) [mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:606) [mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:850) [mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_312]
Caused by: com.github.shyiko.mysql.binlog.event.deserialization.EventDataDeserializationException: Failed to deserialize data of EventHeaderV4{timestamp=1670226722000, eventType=EXT_UPDATE_ROWS, serverId=1, headerLength=19, dataLength=31, nextPosition=499573356, flags=0}
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:309) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.nextEvent(EventDeserializer.java:232) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$1.nextEvent(MySqlStreamingChangeEventSource.java:230) ~[debezium-connector-mysql-1.6.4.Final.jar:1.6.4.Final]
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:945) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	... 3 more
Caused by: com.github.shyiko.mysql.binlog.event.deserialization.MissingTableMapEventException: No TableMapEventData has been found for table id:905. Usually that means that you have started reading binary log 'within the logical event group' (e.g. from WRITE_ROWS and not proceeding TABLE_MAP
	at com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer.deserializeRow(AbstractRowsEventDataDeserializer.java:109) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserializeRows(UpdateRowsEventDataDeserializer.java:71) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserialize(UpdateRowsEventDataDeserializer.java:58) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at com.github.shyiko.mysql.binlog.event.deserialization.UpdateRowsEventDataDeserializer.deserialize(UpdateRowsEventDataDeserializer.java:33) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.deserializeEventData(EventDeserializer.java:303) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer.nextEvent(EventDeserializer.java:232) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$1.nextEvent(MySqlStreamingChangeEventSource.java:230) ~[debezium-connector-mysql-1.6.4.Final.jar:1.6.4.Final]
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:945) ~[mysql-binlog-connector-java-0.25.1.jar:0.25.1]
	... 3 more
```

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1795
Created by: [ldwnt|https://github.com/ldwnt]
Labels: bug, 
Created at: Mon Dec 05 18:40:41 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:18:47 UTC 2024,,,,,,,,,,"0|z1o2yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:18;flink-cdc-import;Date: Wed Dec 07 11:43:29 CST 2022, Author: [ruanhang1993|https://github.com/ruanhang1993]

Thanks for reporting. It is duplicate to ([#1757|https://github.com/apache/flink-cdc/issues/1757] | [FLINK-1757|https://issues.apache.org/jira/browse/FLINK-1757]). It has been fixed in ([#1758|https://github.com/apache/flink-cdc/issues/1758] | [FLINK-1758|https://issues.apache.org/jira/browse/FLINK-1758]) in the branch master.;;;","20/Mar/24 09:18;flink-cdc-import;Date: Wed Dec 07 21:46:51 CST 2022, Author: [ldwnt|https://github.com/ldwnt]

> Thanks for reporting. It is duplicate to ([#1757|https://github.com/apache/flink-cdc/issues/1757] | [FLINK-1757|https://issues.apache.org/jira/browse/FLINK-1757]). It has been fixed in ([#1758|https://github.com/apache/flink-cdc/issues/1758] | [FLINK-1758|https://issues.apache.org/jira/browse/FLINK-1758]) in the branch master.

I built the cdc jar with source code @ commit https://github.com/ververica/flink-cdc-connectors/commit/c1a049ed1bb38d0ab336681b8ac34e4ef34a6fc2, and repeated the scenario as below:
1) start a job A with cdc 2.1.1 (snapshot finished and binlog read)
2) stop job A, update some records in the source mysql
3) replace the jars and restart job A with cdc 2.3

The updated records were correctly read in job A, but the checkpoint can not be completed:
```
2022-12-07 18:44:56,860 INFO  [jobmanager-future-thread-1] org.apache.flink.runtime.source.coordinator.SourceCoordinator [303] - Restoring SplitEnumerator of source Source: @s -> @p -> (Sink: @ds, Sink: @ls) from checkpoint.
2022-12-07 18:44:56,901 ERROR [jobmanager-future-thread-1] org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator [384] - Failed to reset the coordinator to checkpoint and start.
java.lang.IllegalStateException: Invalid status code 16777216,the valid code range is [0, 4]
	at com.ververica.cdc.connectors.mysql.source.assigners.AssignerStatus.fromStatusCode(AssignerStatus.java:164) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.assigners.state.PendingSplitsStateSerializer.deserializeSnapshotPendingSplitsState(PendingSplitsStateSerializer.java:258) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.assigners.state.PendingSplitsStateSerializer.deserializeHybridPendingSplitsState(PendingSplitsStateSerializer.java:322) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.assigners.state.PendingSplitsStateSerializer.deserializePendingSplitsState(PendingSplitsStateSerializer.java:143) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.assigners.state.PendingSplitsStateSerializer.deserialize(PendingSplitsStateSerializer.java:112) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3-SNAPSHOT]
	at com.ververica.cdc.connectors.mysql.source.assigners.state.PendingSplitsStateSerializer.deserialize(PendingSplitsStateSerializer.java:50) ~[flink-connector-mysql-cdc-2.3.0.1.jar:2.3-SNAPSHOT]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.deserializeCheckpoint(SourceCoordinator.java:414) ~[flink-runtime_2.11-1.13.5.jar:?]
	at org.apache.flink.runtime.source.coordinator.SourceCoordinator.resetToCheckpoint(SourceCoordinator.java:309) ~[flink-runtime_2.11-1.13.5.jar:?]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.resetAndStart(RecreateOnResetOperatorCoordinator.java:377) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$resetToCheckpoint$6(RecreateOnResetOperatorCoordinator.java:136) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:719) ~[?:1.8.0_312]
	at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:731) ~[?:1.8.0_312]
	at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2023) ~[?:1.8.0_312]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.resetToCheckpoint(RecreateOnResetOperatorCoordinator.java:131) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.resetToCheckpoint(OperatorCoordinatorHolder.java:273) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreStateToCoordinators(CheckpointCoordinator.java:1815) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1577) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1642) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:342) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:190) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:122) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:107) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95) ~[flink-runtime_2.11-1.13.5.jar:1.13.5]
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112) ~[flink-core-1.13.5.jar:1.13.5]
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) [?:1.8.0_312]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_312]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_312]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_312]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_312]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_312]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_312]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_312]
...
2022-12-07 18:45:16,608 INFO  [Checkpoint Timer] org.apache.flink.runtime.checkpoint.CheckpointCoordinator [742] - Triggering checkpoint 4 (type=CHECKPOINT) @ 1670409916573 for job bce7153aa983463f434472bc58f25a0d.
2022-12-07 19:45:16,608 INFO  [Checkpoint Timer] org.apache.flink.runtime.checkpoint.CheckpointCoordinator [1985] - Checkpoint 4 of job bce7153aa983463f434472bc58f25a0d expired before completing.
```;;;","20/Mar/24 09:18;flink-cdc-import;Date: Thu Dec 08 16:05:47 CST 2022, Author: [PatrickRen|https://github.com/PatrickRen]

Thanks for the issue @ldwnt ! I took a look and found the incompatibility was already introduced in 2.2 by c94791fd. So unfortunately the state in 2.1 are not compatible with connector 2.2 and 2.3. 

To be honest the state serializer deserves a refactor to be fully version managed. ;;;","20/Mar/24 09:18;flink-cdc-import;Date: Thu Dec 08 18:31:58 CST 2022, Author: [ldwnt|https://github.com/ldwnt]

> Thanks for the issue @ldwnt ! I took a look and found the incompatibility was already introduced in 2.2 by [c94791f|https://github.com/ververica/flink-cdc-connectors/commit/c94791fd147522b0538c2bd43483ab6a4c1b767b]. So unfortunately the state in 2.1 are not compatible with connector 2.2 and 2.3.
> 
> To be honest the state serializer deserves a refactor to be fully version managed.

Sad to hear that. We're considering upgrading cdc to 2.3 because in this version adding new tables and consuming from specific offset is supported, at least for mysql. However, if an upgrade is impossible, we have to replay the snapshot of dozens of datasources, which troubles the downstream stakeholders. Do we have a planned fix for this issue in 2.3?;;;","20/Mar/24 09:18;flink-cdc-import;Date: Fri Mar 03 16:11:53 CST 2023, Author: [xuhaiL|https://github.com/xuhaiL]

@ruanhang1993 @PatrickRen I also encountered this bug, do we have a planned fix for this issue in 2.3?;;;","20/Mar/24 09:18;flink-cdc-import;Date: Thu Apr 27 10:41:50 CST 2023, Author: [caicancai|https://github.com/caicancai]

@ruanhang1993 I also encountered this bug，This seems to be a problem with the bottom layer relying on mysql-binglog-connector;;;","20/Mar/24 09:18;flink-cdc-import;Date: Tue Jun 06 16:31:48 CST 2023, Author: [Cmelon9|https://github.com/Cmelon9]

@ruanhang1993 @PatrickRen flink cdc version 2.3 still have this bug.;;;","20/Mar/24 09:18;flink-cdc-import;Date: Tue Dec 26 15:16:18 CST 2023, Author: [ldwnt|https://github.com/ldwnt]

@PatrickRen I give up the upgrade and start a new job with cdc 2.4. The problem is that the startup with specified binlog offset often leads to the MissingTableMapEventException error since the offset corresponds to a write_rows binlog record. So how are we supposed to use the startup mode ""specific-offset""?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Caused by: io.debezium.DebeziumException: could not find next log; ",FLINK-34758,13572537,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:18,20/Mar/24 09:18,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Describe the bug(Please use English)*An error was reported when the database was manually backed up*
A clear and concise description of what the bug is.

**Environment :**
 - Flink version :  1.14.3
 - Flink CDC version: 2.2.1
 - Database and version: 5.7

**To Reproduce**
Steps to reproduce the behavior:
1. The test data :
2. The test code :
3. The error :
An error was reported when the database was manually backed up.I have privileges like : select_priv, repl_slave_priv ,repl_client_priv,
when I use cdc sync the MySql tables  and the database was manually backed up together ,  this error occured.

**Additional Description**
If applicable, add screenshots to help explain your problem.

Caused by: io.debezium.DebeziumException: could not find next log; the first event 'Storage.000086' at 57144201, the last event read from '/bak/binlog/Storage.000086' at 101689506, the last byte read from '/bak/binlog/Storage.000086' at 101689506. Error code: 1236; SQLSTATE: HY000.
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.wrap(MySqlStreamingChangeEventSource.java:1146) ~[stationcmdsync.jar:?]
	at io.debezium.connector.mysql.MySqlStreamingChangeEventSource$ReaderThreadLifecycleListener.onCommunicationFailure(MySqlStreamingChangeEventSource.java:1185) ~[stationcmdsync.jar:?]
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:973) ~[stationcmdsync.jar:?]
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:606) ~[stationcmdsync.jar:?]
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:850) ~[stationcmdsync.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_161]
Caused by: com.github.shyiko.mysql.binlog.network.ServerException: could not find next log; the first event 'Storage.000086' at 57144201, the last event read from '/bak/binlog/Storage.000086' at 101689506, the last byte read from '/bak/binlog/Storage.000086' at 101689506.
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:937) ~[stationcmdsync.jar:?]
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:606) ~[stationcmdsync.jar:?]
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:850) ~[stationcmdsync.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_161]
2022-12-03 02:29:06,382 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Streaming Job (b2ea26f12c5fb818e236c64b660bb7e6) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444) ~[flink-dist_2.12-1.14.3.jar:1.14.3]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_161]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316) ~[flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314) ~[flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_6df5e012-1a24-4269-9837-b1b94fb7d977.jar:1.14.3]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_161]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_161]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_161]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) [?:1.8.0_161]

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1794
Created by: [zZxiaowang|https://github.com/zZxiaowang]
Labels: bug, 
Created at: Mon Dec 05 16:17:07 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:18:06 UTC 2024,,,,,,,,,,"0|z1o2yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 09:18;flink-cdc-import;Date: Tue Dec 06 09:35:29 CST 2022, Author: [zZxiaowang|https://github.com/zZxiaowang]

If I configure restart for the program ,the program will restart once ,then  it runs suitably.Is the backups affects CDC?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC Clinkhouse,FLINK-34757,13572536,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:17,20/Mar/24 09:17,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"Added Flink to Clinkhouse data synchronization。

---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1784
Created by: [BingchuanLi|https://github.com/BingchuanLi]
Labels: enhancement, 
Created at: Thu Dec 01 16:02:17 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:17:29.0,,,,,,,,,,"0|z1o2yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[MySQL-CDC] serverId parameter out of Integer range,FLINK-34756,13572535,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:16,14/May/24 03:40,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,1,github-import,,,"**Describe the bug(Please use English)**
Now the 'server_id' is store as int, but the value of parameter 'server_id' would be larger than INT_MAX, this would cause IntegerParseException

**Environment :**
 - Flink version :  1.16
 - Flink CDC version: 2.3.0
 - Database and version: mysql 5.7

**To Reproduce**
Steps to reproduce the behavior:
1. The test data :
MySQL instance in Aliyun Cloud, with a large server_id (out of INT range)
```bash
mysql> show variables like '%server_id%';
+----------------+------------+
| Variable_name  | Value      |
+----------------+------------+
| server_id      | 2437466879 |
| server_id_bits | 32         |
+----------------+------------+

```

2. The test code :
```java
public class MySqlSourceExample {
  public static void main(String[] args) throws Exception {
    MySqlSource<String> mySqlSource = MySqlSource.<String>builder()
      .hostname(""host"")
      .port(3306)
      .databaseList(""dbs"") // set captured database
      .tableList(""tables"") // set captured table
      .username(""user"")
      .password(""password"")
      .serverId(""1-2437466879"")
      .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String
      .build();

    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    env
      .fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), ""MySQL Source"")
      // set 4 parallel source tasks
      .setParallelism(4)
      .print().setParallelism(1); // use parallelism 1 for sink to keep message ordering

     env.execute(""Print MySQL Snapshot + Binlog"");
  }
}
```

3. The error :
```bash
Exception in thread ""main"" java.lang.IllegalStateException: The server id 2437466879 is not a valid numeric.
	at com.ververica.cdc.connectors.mysql.source.config.ServerIdRange.parseServerId(ServerIdRange.java:108)
	at com.ververica.cdc.connectors.mysql.source.config.ServerIdRange.from(ServerIdRange.java:96)
	at com.ververica.cdc.connectors.mysql.source.config.MySqlSourceConfigFactory.serverId(MySqlSourceConfigFactory.java:130)
	at com.ververica.cdc.connectors.mysql.source.MySqlSourceBuilder.serverId(MySqlSourceBuilder.java:108)
	at com.bytedance.openplatform.flink.cdc.examples.MySqlSourceExample.main(MySqlSourceExample.java:25)
Caused by: java.lang.NumberFormatException: For input string: ""2437466879""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:583)
	at java.lang.Integer.parseInt(Integer.java:615)
	at com.ververica.cdc.connectors.mysql.source.config.ServerIdRange.parseServerId(ServerIdRange.java:105)
	... 4 more
```

**Additional Description**
If applicable, add screenshots to help explain your problem.


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1770
Created by: [legendtkl|https://github.com/legendtkl]
Labels: bug, 
Created at: Wed Nov 23 17:36:24 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 03:40:28 UTC 2024,,,,,,,,,,"0|z1o2y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 03:40;chenyunde;The range of server-id should be between 1 to 2 ^ 32-1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encountered a problem reading tidb data,FLINK-34755,13572534,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 09:15,20/Mar/24 09:15,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"**Describe the bug(Please use English)**
The console has been updated as follows when reading tidb, using print.sink, but the console has no data printing.

**Environment :**
 - Flink version : 1.14.5
 - Flink CDC version: 2.3.0
 - Database and version: tidb-v6.1.0

**To Reproduce**
Steps to reproduce the behavior:
1. The test code to connect to tidb :
```
            ""     ) WITH (    "" +
            ""    'connector' = 'tidb-cdc',"" +
            ""    'scan.startup.mode' = 'latest-offset',"" +
            ""     'tikv.grpc.timeout_in_ms' = '20000', "" +
            ""     'pd-addresses' = '192.168.100.10:2379',"" +
            ""     'database-name' = 'device',"" +
            ""     'table-name' = 'real_time_record'"" +
            ""     )"";
```
1. The error :
```
[pool-24-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.234:20160] is not reachable
[pool-20-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.234:20160] is not reachable
[pool-26-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.71:20160] is not reachable
[pool-27-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.71:20160] is not reachable
[pool-23-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.71:20160] is not reachable
[pool-22-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.234:20160] is not reachable
[pool-25-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.64:20160] is not reachable
[pool-21-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.71:20160] is not reachable
[pool-26-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.64:20160] is not reachable
[pool-23-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.234:20160] is not reachable
[pool-27-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.234:20160] is not reachable
[pool-25-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.71:20160] is not reachable
[pool-21-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.234:20160] is not reachable
[pool-22-thread-1] WARN org.tikv.common.region.StoreHealthyChecker - store [172.16.0.64:20160] is not reachable
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (5/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862555353140, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (1/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862555353140, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (6/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862555353140, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (8/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862555353140, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (3/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862555353140, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (4/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862555353140, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (5/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862817497106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (2/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862555353140, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (3/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862817497106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (6/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862817497106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (2/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862817497106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (7/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862555353140, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (7/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862817497106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (4/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862817497106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (8/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862817497106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (1/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453862817497106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (8/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863079641102, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (3/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863079641102, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (6/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863079641102, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (4/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863079641102, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (1/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863079641102, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (5/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863079641102, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (5/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863341785118, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (3/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863341785118, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (6/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863341785118, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (2/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863079641102, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (2/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863341785118, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (7/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863079641102, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (7/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863341785118, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (4/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863341785118, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (4/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863603929106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (8/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863341785118, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (8/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863603929106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (2/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863603929106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (1/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863341785118, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (1/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863603929106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (1/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863879180289, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (5/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863603929106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (5/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863879180289, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (6/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863603929106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (8/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863879180289, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (6/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863879180289, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (4/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863879180289, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (3/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863603929106, regionId: 213496
[Legacy Source Thread - Source: TableSourceScan(table=[[default_catalog, default_database, time_record]], fields=[id, employee_id, date, fence_status]) (3/8)([#0|https://github.com/apache/flink-cdc/issues/0] | [FLINK-0|https://issues.apache.org/jira/browse/FLINK-0])] INFO org.tikv.cdc.CDCClient - handle resolvedTs: 437453863879180289, regionId: 213496
[PDClient-update-leader-pool-0] WARN org.tikv.common.PDClient - failed to get member from pd server.
org.tikv.shade.io.grpc.StatusRuntimeException: UNAVAILABLE: Keepalive failed. The connection is likely gone
	at org.tikv.shade.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:287)
	at org.tikv.shade.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:268)
	at org.tikv.shade.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:175)
	at org.tikv.kvproto.PDGrpc$PDBlockingStub.getMembers(PDGrpc.java:1868)
	at org.tikv.common.PDClient.getMembers(PDClient.java:443)
	at org.tikv.common.PDClient.tryUpdateLeader(PDClient.java:565)
	at org.tikv.common.PDClient.lambda$initCluster$15(PDClient.java:730)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[PDClient-update-leader-pool-0] WARN org.tikv.common.PDClient - failed to get member from pd server.
```

**Additional Description**
If applicable, add screenshots to help explain your problem.


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1761
Created by: [dadong-13|https://github.com/dadong-13]
Labels: bug, 
Created at: Fri Nov 18 16:57:15 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-20 09:15:40.0,,,,,,,,,,"0|z1o2y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[tidb-cdc]When the task runs for a period of time, there are only Resolved Events",FLINK-34754,13572531,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flink-cdc-import,flink-cdc-import,20/Mar/24 08:59,20/Mar/24 09:04,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,github-import,,,"
**Environment :**
 - Flink version :  1.14.4
 - Flink CDC version: 2.2 ,2.3 ,master
 - Database and version: tidb5.0 and tidb5.1 

**To Reproduce**
flink-tidb-cdc table： A

flink-jdbc table: B

query:
insert into A select * from B;

desc:
The task data is normal at the beginning of the program startup, but after running for a period of time, the data is not updated.
but there is no error message.

The log:
2022-10-31 17:56:03,786 INFO  org.tikv.cdc.CDCClient                                       [] - handle resolvedTs: 437049141169750048, regionId: 1721083
2022-10-31 17:56:04,787 INFO  org.tikv.cdc.CDCClient                                       [] - handle resolvedTs: 437049141431894020, regionId: 1721083
2022-10-31 17:56:05,787 INFO  org.tikv.cdc.CDCClient                                       [] - handle resolvedTs: 437049141694038029, regionId: 1721083
2022-10-31 17:56:06,789 INFO  org.tikv.cdc.CDCClient                                       [] - handle resolvedTs: 437049141956182110, regionId: 1721083
2022-10-31 17:56:07,790 INFO  org.tikv.cdc.CDCClient                                       [] - handle resolvedTs: 437049142218326018, regionId: 1721083
2022-10-31 17:56:08,793 INFO  org.tikv.cdc.CDCClient                                       [] - handle resolvedTs: 437049142480470176, regionId: 1721083

**Additional Description**
If applicable, add screenshots to help explain your problem.


---------------- Imported from GitHub ----------------
Url: https://github.com/apache/flink-cdc/issues/1755
Created by: [wangxiaojing|https://github.com/wangxiaojing]
Labels: bug, 
Created at: Thu Nov 17 11:44:16 CST 2022
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 08:59:30 UTC 2024,,,,,,,,,,"0|z1o2xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 08:59;flink-cdc-import;[Date: Thu Nov 17 13:51:04 CST 2022, Author: [GOODBOY008|https://github.com/GOODBOY008]]

Thanks for your report @wangxiaojing . I need to confirm some information. 
1、During the time , source table occurs region merge or split? 
2、How many data in source table?
3、When you restart job , it works well?;;;","20/Mar/24 08:59;flink-cdc-import;[Date: Thu Nov 17 15:40:21 CST 2022, Author: [wangxiaojing|https://github.com/wangxiaojing]]

1、During the time , source table occurs region merge or split?
It may have happened, but i didn’t see the tidb's log。
2、How many data in source table?
2213459
3、When you restart job , it works well?
Success after restart
4、The debug log
org.tikv.shade.io.grpc.netty.NettyClientHandler              [] - [id: 0xec8fa73f, L:/xxxxxxx:53214 - R:/xxxxxx:20160] INBOUND PING: ack=true bytes=1234;;;","20/Mar/24 08:59;flink-cdc-import;Date: Thu Nov 17 15:44:46 CST 2022, Author: [wangxiaojing|https://github.com/wangxiaojing]

2022-11-17 06:03:57,910 INFO  org.tikv.cdc.RegionCDCClient                                 [] - submit event: CDCEvent[RESOLVED_TS] {resolvedTs=437422977945436229}
2022-11-17 06:03:57,910 DEBUG org.tikv.shade.io.grpc.netty.NettyClientHandler              [] - [id: 0xcfce7184, L:/xxxxxxxxx:41108 - R:/xxxxx:20160] INBOUND DATA: streamId=6359 padding=0 endStream=false length=22 bytes=[0000000011120f08c3c0c80510c580a08898bc82|https://github.com/apache/flink-cdc/commit/0000000011120f08c3c0c80510c580a08898bc82]8906
2022-11-17 06:03:57,910 INFO  org.tikv.cdc.RegionCDCClient                                 [] - submit event: CDCEvent[RESOLVED_TS] {resolvedTs=437422977945436229}
2022-11-17 06:03:57,910 DEBUG org.tikv.shade.io.grpc.netty.NettyClientHandler              [] - [id: 0xcfce7184, L:/xxxxxxx:41108 - R:/xxxxxxx:20160] INBOUND DATA: streamId=6353 padding=0 endStream=false length=22 bytes=[0000000011120f0885eff00410c580a08898bc82|https://github.com/apache/flink-cdc/commit/0000000011120f0885eff00410c580a08898bc82]8906;;;","20/Mar/24 08:59;flink-cdc-import;[Date: Sun Nov 20 20:09:27 CST 2022, Author: [wangxiaojing|https://github.com/wangxiaojing]]

https://github.com/ververica/flink-cdc-connectors/pull/1763;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update outdated MongoDB CDC FAQ in doc,FLINK-34753,13572519,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ShawnHx,ShawnHx,ShawnHx,20/Mar/24 07:43,29/Mar/24 01:13,04/Jun/24 20:40,29/Mar/24 01:13,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 29 01:13:35 UTC 2024,,,,,,,,,,"0|z1o2uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/24 01:13;jiabaosun;Resolved by flink-cdc master: 927a0ec4743ac70c5d4edb811da7ffce09658e8b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""legacy-flink-cdc-sources"" Page of TIDB for Flink CDC Chinese Documentation.",FLINK-34752,13572517,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,siriusfan,kunni,kunni,20/Mar/24 07:35,17/May/24 03:27,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,Translate legacy-flink-cdc-sources pages of https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/legacy-flink-cdc-sources/tidb-cdc.md into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 05:54:21 UTC 2024,,,,,,,,,,"0|z1o2u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 08:06;siriusfan;Hi [~kunni]  ，I am a Chinese Speaker and glad to take this work , cloud you assign this ticket to me? :);;;","17/Apr/24 05:54;kunni;[~siriusfan] Thanks for taking this, I don't have the authority to assign, but you can go ahead and submit a PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestClusterClient APIs doesn't work with running Flink application on YARN,FLINK-34751,13572496,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vsowrirajan,vsowrirajan,20/Mar/24 04:57,26/Mar/24 11:00,04/Jun/24 20:40,,,,,,,,,,,,Deployment / YARN,,,,,0,,,,"Apache YARN uses web proxy in Resource Manager to expose the endpoints available through the AM process (in this case RestServerEndpoint that run as part of AM). Note: this is in the context of running Flink cluster in YARN application mode.

For eg: in the case of RestClusterClient#listJobs -

{{Standalone listJobs}} makes the request as - {{{}https://<host>:<port>/v1/{}}}{{{}jobs{}}}{{{}/overview{}}}

YARN the same request has to be proxified as -  {{{}https://<rm-host>:<rm-port>/proxy/<app-id>/v1/{}}}{{{}jobs{}}}{{{}/overview?proxyapproved=true{}}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 26 11:00:36 UTC 2024,,,,,,,,,,"0|z1o2pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/24 11:00;ferenc-csaky;Hi! Would you be able to share how to reproduce the issue?. My understanding given the description is {{flink list}} fails to list secured Yarn AM deployments. Is that correct?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""legacy-flink-cdc-sources"" Page of Postgresql for Flink CDC Chinese Documentation.",FLINK-34750,13572494,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,loserwang1024,kunni,kunni,20/Mar/24 03:58,17/May/24 03:27,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,Translate legacy-flink-cdc-sources pages of https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/legacy-flink-cdc-sources/postgres-cdc.md into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 08:21:19 UTC 2024,,,,,,,,,,"0|z1o2p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 08:21;loserwang1024;I am willing to do this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""legacy-flink-cdc-sources"" Page of SQLServer for Flink CDC Chinese Documentation.",FLINK-34749,13572493,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,siriusfan,kunni,kunni,20/Mar/24 03:57,17/May/24 03:26,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,"Translate legacy-flink-cdc-sources pages of [https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/legacy-flink-cdc-sources/sqlserver-cdc.md |https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/legacy-flink-cdc-sources/db2-cdc.md]into Chinese.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 08:11:01 UTC 2024,,,,,,,,,,"0|z1o2ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 08:11;siriusfan;Hi [~kunni]  ，I am a Chinese Speaker and glad to take this work , cloud you assign this ticket to me? :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""legacy-flink-cdc-sources"" Page of Oracle for Flink CDC Chinese Documentation.",FLINK-34748,13572492,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,siriusfan,kunni,kunni,20/Mar/24 03:56,17/May/24 03:26,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,Translate legacy-flink-cdc-sources pages of https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/legacy-flink-cdc-sources/oracle-cdc.md into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 08:05:00 UTC 2024,,,,,,,,,,"0|z1o2oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 08:05;siriusfan;Hi [~kunni]  ，I am a Chinese Speaker and glad to take this work , cloud you assign this ticket to me? :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""legacy-flink-cdc-sources"" Page of DB2 for Flink CDC Chinese Documentation.",FLINK-34747,13572489,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,ShawnHx,kunni,kunni,20/Mar/24 03:52,17/May/24 03:26,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,"Translate legacy-flink-cdc-sources pages of [https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/legacy-flink-cdc-sources/db2-cdc.md |https://github.com/apache/flink-cdc/blob/master/docs/content/docs/connectors/legacy-flink-cdc-sources/db2-cdc.md]into Chinese.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 07:42:13 UTC 2024,,,,,,,,,,"0|z1o2o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 07:42;ShawnHx;I'm willing to take this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switching to the Apache CDN for Dockerfile,FLINK-34746,13572484,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,lincoln.86xy,lincoln.86xy,20/Mar/24 02:29,30/May/24 14:24,04/Jun/24 20:40,24/May/24 10:53,,,,,,,1.18.2,1.19.1,1.20.0,,flink-docker,,,,,0,pull-request-available,,,"During publishing the official image, we received some comments

for Switching to the Apache CDN

 

See

https://github.com/docker-library/official-images/pull/16114

https://github.com/docker-library/official-images/pull/16430

 

Reason for switching: [https://apache.org/history/mirror-history.html] (also [https://www.apache.org/dyn/closer.cgi] and [https://www.apache.org/mirrors])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 14:24:05 UTC 2024,,,,,,,,,,"0|z1o2mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/24 10:53;martijnvisser;Fixed in apache/flink-docker@master 883600747505c128d97e9d25c9326f0c6f1d31e4;;;","30/May/24 14:24;hong; merged commit [{{7f63237}}|https://github.com/apache/flink-docker/commit/7f63237615138615826f2820ca54ff2054514fca] into   apache:dev-1.19

 merged commit [{{804c9f3}}|https://github.com/apache/flink-docker/commit/804c9f3bb6772751d09252b6d15e8a1aac4ca055] into   apache:dev-1.18

 merged commit [{{0ac313e}}|https://github.com/apache/flink-docker/commit/0ac313e39fda6c49778fbdf15f4b5d827476253a] into  apache:dev-1.17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parsing temporal table join throws cryptic exceptions,FLINK-34745,13572433,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,19/Mar/24 14:57,20/Mar/24 14:15,04/Jun/24 20:40,20/Mar/24 14:15,1.19.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"1. Wrong expression type in {{AS OF}}:
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF 'o.rowtime' AS r "" +
      ""ON o.currency = r.currency
{code}

throws: 

{code}
java.lang.AssertionError: cannot convert CHAR literal to class org.apache.calcite.util.TimestampString
{code}

2. Not a simple table reference in {{AS OF}}
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF o.rowtime + INTERVAL '1' SECOND AS r "" +
      ""ON o.currency = r.currency
{code}

throws:
{code}
java.lang.AssertionError: no unique expression found for {id: o.rowtime, prefix: 1}; count is 0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 14:15:26 UTC 2024,,,,,,,,,,"0|z1o2bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 14:15;dwysakowicz;Fixed in 4142c4386a92f1ec5016583f4832f8869782765e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
autoscaling-dynamic cannot run,FLINK-34744,13572425,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,19/Mar/24 13:46,20/Mar/24 02:15,04/Jun/24 20:40,20/Mar/24 02:15,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,,,,,0,pull-request-available,,,"autoscaling-dynamic cannot run on my Mac

 !image-2024-03-19-21-46-15-530.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/24 13:46;fanrui;image-2024-03-19-21-46-15-530.png;https://issues.apache.org/jira/secure/attachment/13067527/image-2024-03-19-21-46-15-530.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 02:15:22 UTC 2024,,,,,,,,,,"0|z1o29s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 02:15;fanrui;Merged to main(1.9.0) via: b584b08806c7e8366519acdb92bfb4725faaebba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory tuning takes effect even if the parallelism isn't changed,FLINK-34743,13572422,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,19/Mar/24 13:22,27/Mar/24 07:42,04/Jun/24 20:40,,,,,,,,,,,,Autoscaler,,,,,0,pull-request-available,,,"Currently, the memory tuning related logic is only called when the parallelism is changed.

See ScalingExecutor#scaleResource to get more details.

It's better to let the memory tuning takes effect even if the parallelism isn't changed. For example, one flink job runs with desired parallelisms, but it wastes memory.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-19 13:22:29.0,,,,,,,,,,"0|z1o294:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""FAQ"" Page for Flink CDC Chinese Documentation",FLINK-34742,13572382,13572360,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,loserwang1024,kunni,kunni,19/Mar/24 09:56,20/Mar/24 07:09,04/Jun/24 20:40,20/Mar/24 07:09,cdc-3.1.0,,,,,,cdc-3.1.0,,,,chinese-translation,Documentation,Flink CDC,,,0,pull-request-available,,,Translate [https://github.com/apache/flink-cdc/blob/master/docs/content/docs/faq/faq.md] page into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 02:06:13 UTC 2024,,,,,,,,,,"0|z1o208:",9223372036854775807,flink-cdc master: 18fa3c5dc495e022c38a0580bf33fcba2a808971,,,,,,,,,,,,,,,,,,,"20/Mar/24 02:06;loserwang1024;I am willing to do this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""get-started"" Page for Flink CDC Chinese Documentation",FLINK-34741,13572381,13572360,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,loserwang1024,kunni,kunni,19/Mar/24 09:54,20/Mar/24 09:48,04/Jun/24 20:40,20/Mar/24 09:48,cdc-3.1.0,,,,,,cdc-3.1.0,,,,chinese-translation,Documentation,Flink CDC,,,0,pull-request-available,,,Translate [https://github.com/apache/flink-cdc/tree/master/docs/content/docs/get-started] pages into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 09:48:07 UTC 2024,,,,,,,,,,"0|z1o200:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 02:07;loserwang1024;I am willing to do this.;;;","20/Mar/24 09:48;leonard;flink-cdc master: 43a5887cd7545de21723e357b0d1d11c280018cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""legacy-flink-cdc-sources"" Pages for Flink CDC Chinese Documentation existed in 2.x version.",FLINK-34740,13572380,13572360,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kunni,kunni,kunni,19/Mar/24 09:51,20/Mar/24 08:45,04/Jun/24 20:40,20/Mar/24 08:45,cdc-3.1.0,,,,,,cdc-3.1.0,,,,chinese-translation,Documentation,Flink CDC,,,0,pull-request-available,,,"Translate legacy-flink-cdc-sources pages of [https://github.com/apache/flink-cdc/tree/master/docs/content/docs/connectors/legacy-flink-cdc-sources] into Chinese.

This includes legacy MySQL\MongoDB\Oceanbase source.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 08:45:10 UTC 2024,,,,,,,,,,"0|z1o1zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 08:28;kunni;Sorry for [~loserwang1024], I didn't notice that you had comment on this and have already completed a code. I've raised a pr to solve it and  You can claim other unfinished subtasks.;;;","20/Mar/24 08:45;leonard;flink-cdc master: bacb6a3b60511856da5e396a9fcf7df217945b8e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Connectors"" Page for Flink CDC Chinese Documentation",FLINK-34739,13572379,13572360,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,loserwang1024,kunni,kunni,19/Mar/24 09:49,20/Mar/24 10:14,04/Jun/24 20:40,20/Mar/24 10:14,cdc-3.1.0,,,,,,cdc-3.1.0,,,,chinese-translation,Documentation,Flink CDC,,,0,pull-request-available,,,Translate pipeline connector pages [https://github.com/apache/flink-cdc/tree/master/docs/content/docs/connectors] into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 10:14:59 UTC 2024,,,,,,,,,,"0|z1o1zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 08:49;loserwang1024;I am willing to do it;;;","20/Mar/24 10:14;leonard;flink-cdc master: e3e024187befca04ec76fd1d3b869326dee049ee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Deployment - YARN"" Page for Flink CDC Chinese Documentation",FLINK-34738,13572377,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,vincentwoo,kunni,kunni,19/Mar/24 09:42,17/May/24 03:22,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,pull-request-available,,,Translate [https://github.com/apache/flink-cdc/blob/master/docs/content/docs/deployment/yarn.md] into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 31 07:07:45 UTC 2024,,,,,,,,,,"0|z1o1z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/24 07:07;vincentwoo;I'm willing to handle this issue, could someone assign it to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Deployment - Kubernetes"" Page for Flink CDC Chinese Documentation",FLINK-34737,13572376,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kunni,kunni,19/Mar/24 09:40,17/May/24 03:22,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,Translate [https://github.com/apache/flink-cdc/blob/master/docs/content/docs/deployment/kubernetes.md] into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 12:57:49 UTC 2024,,,,,,,,,,"0|z1o1yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 12:57;czy006;https://issues.apache.org/jira/browse/FLINK-34737 doc assign to me  cc  [~kunni] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Deployment - Standalone"" Page for Flink CDC Chinese Documentation",FLINK-34736,13572375,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,vincentwoo,kunni,kunni,19/Mar/24 09:38,17/May/24 03:25,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,Translate [https://github.com/apache/flink-cdc/blob/master/docs/content/docs/deployment/standalone.md] into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 31 07:04:28 UTC 2024,,,,,,,,,,"0|z1o1yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/24 07:04;vincentwoo;I'm willing to contribute, could someone assign this to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Developer Guide - Understanding Flink CDC API"" Page for Flink CDC Chinese Documentation",FLINK-34735,13572374,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kunni,kunni,19/Mar/24 09:36,17/May/24 03:25,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,Translate [https://github.com/apache/flink-cdc/blob/master/docs/content/docs/developer-guide/understand-flink-cdc-api.md] into Chinese.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 02:01:22 UTC 2024,,,,,,,,,,"0|z1o1yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 02:01;kunni;I am willing to do this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate directory and page titles of Flink CDC docs to Chinese,FLINK-34734,13572370,13572360,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,kunni,kunni,kunni,19/Mar/24 09:14,25/Mar/24 07:51,04/Jun/24 20:40,20/Mar/24 04:13,cdc-3.1.0,,,,,,cdc-3.1.0,,,,chinese-translation,Documentation,Flink CDC,,,0,pull-request-available,,,The titles is used to build directory and document names.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 04:13:49 UTC 2024,,,,,,,,,,"0|z1o1xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 09:18;leonard;Thanks [~kunni]for open this task, would you like to take this ?;;;","19/Mar/24 09:29;kunni;[~leonard]  Sure, I would like to do it.;;;","20/Mar/24 04:13;leonard;flink-cdc master: 8fdd151a0606f8bb707ce11b969bdb62f22c7182;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OSS Filesystem throws ClassNotFoundException,FLINK-34733,13572368,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kocomic,kocomic,19/Mar/24 08:53,19/Mar/24 09:58,04/Jun/24 20:40,,1.17.2,,,,,,,,,,FileSystems,,,,,0,pull-request-available,,,"The ClassNotFoundException was caused by this commit [[https://github.com/apache/flink/commit/52a2b98bb5af842633df0c051b5da95d437a6b2f]], which removed the relocation configuration from pom.xml.FLINK-31612.

In filesystem plugin flink-oss-fs-hadoop, the shaded prefix was hard-coded [https://github.com/apache/flink/blob/c0027e5777f9d77970fdb99bcc158d65ea48d514/flink-filesystems/flink-oss-fs-hadoop/src/main/java/org/apache/flink/fs/osshadoop/OSSFileSystemFactory.java#L50]

This resulted in the exception:

```

Caused by: java.lang.ClassNotFoundException: org.apache.flink.fs.osshadoop.shaded.com.aliyun.oss.common.auth.EnvironmentVariableCredentialsProvider
    at java.net.URLClassLoader.findClass(Unknown Source) ~[?:?]
    at java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:150) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromOwnerFirst(ComponentClassLoader.java:172) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:107) ~[flink-dist-1.17.2.jar:1.17.2]
    at java.lang.ClassLoader.loadClass(Unknown Source) ~[?:?]
    at java.lang.Class.forName0(Native Method) ~[?:?]
    at java.lang.Class.forName(Unknown Source) ~[?:?]
    at org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils.getCredentialsProvider(AliyunOSSUtils.java:118) ~[?:?]
    at org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore.initialize(AliyunOSSFileSystemStore.java:155) ~[?:?]
    at org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem.initialize(AliyunOSSFileSystem.java:349) ~[?:?]
    at org.apache.flink.fs.osshadoop.OSSFileSystemFactory.create(OSSFileSystemFactory.java:103) ~[?:?]
    at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:62) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:508) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:99) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:86) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.createHAServices(KubernetesHaServicesFactory.java:41) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:296) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:139) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:442) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:391) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.17.2.jar:1.17.2]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229) ~[flink-dist-1.17.2.jar:1.17.2]
    ... 2 more

```",Flink >= v1.17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-19 08:53:42.0,,,,,,,,,,"0|z1o1x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document dead link check for Flink CDC Documentation,FLINK-34732,13572365,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,gongzhongqiang,gongzhongqiang,gongzhongqiang,19/Mar/24 08:40,25/Mar/24 08:02,04/Jun/24 20:40,25/Mar/24 08:01,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,Add ci for check dead link in flink cdc document.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 08:01:45 UTC 2024,,,,,,,,,,"0|z1o1wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/24 06:41;gongzhongqiang; [~leonard]  [~renqs] I'm willing to take this. And pr is ready, please take a look.;;;","25/Mar/24 08:01;renqs;flink-cdc master: 1c69a650e3187734962e824e40ed35de0d94ccfc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SpeculativeScheduler and incorporate its features into AdaptiveBatchScheduler,FLINK-34731,13572361,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,19/Mar/24 08:23,22/Mar/24 02:41,04/Jun/24 20:40,22/Mar/24 02:41,,,,,,,1.20.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,"Presently, speculative execution is exposed to users as a feature of the AdaptiveBatchScheduler.

To streamline our codebase and reduce maintenance overhead, this ticket will consolidate the SpeculativeScheduler into the AdaptiveBatchScheduler, eliminating the need for a separate SpeculativeScheduler class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 02:41:22 UTC 2024,,,,,,,,,,"0|z1o1vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/24 02:41;zhuzh;master: cf0d75c4bb324825a057dc72243bb6a2046f8479;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate pages of Flink CDC docs to Chinese,FLINK-34730,13572360,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kunni,kunni,19/Mar/24 08:21,17/May/24 03:24,04/Jun/24 20:40,,,,,,,,cdc-3.2.0,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,"As https://issues.apache.org/jira/browse/FLINK-34677 introduced new structure of Document for Flink CDC, it's also necessary to update the Chinese Document for better user experience.

We need to translate the pages under content into Chinese, and update the titles.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-19 08:21:31.0,,,,,,,,,,"0|z1o1vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Core Concept"" Pages for Flink CDC Chinese Documentation",FLINK-34729,13572358,13572360,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,kunni,kunni,kunni,19/Mar/24 08:19,22/Mar/24 07:03,04/Jun/24 20:40,,,,,,,,,,,,chinese-translation,Documentation,Flink CDC,,,0,,,,"Translate [Core Concept|https://github.com/apache/flink-cdc/tree/master/docs/content/docs/core-concept] Pages into Chinese.
Include data-pipeline/data-source/data-sink/route/transform/table-id.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 02:01:07 UTC 2024,,,,,,,,,,"0|z1o1uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 02:01;kunni;I am willing to do this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
operator does not need to upload and download the jar when deploying session job,FLINK-34728,13572354,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Fei Feng,Fei Feng,19/Mar/24 07:58,19/Mar/24 10:34,04/Jun/24 20:40,,kubernetes-operator-1.5.0,kubernetes-operator-1.6.0,kubernetes-operator-1.7.0,,,,,,,,Deployment / Kubernetes,Kubernetes Operator,,,,0,,,,"Problem:

By reading the source code of the sessionjob's first reconcilition in the session mode of the flink kubernetes operator, a clear single point of bottleneck can be identified. When submitting a session job, the operator needs to first download the job jar from the jarURL to the local storage of kubernetes pod , then upload the jar to the job manager through the `/jars/upload` rest api, and finally call the `/jars/:jarid/run` rest api to launch the job.

In this process, the operator needs to first download the jar and then upload the jar. When multiple jobs are submitted to the session cluster simultaneously, the operator can become a single point of bottleneck, which may be limited by the network traffic or other resource constraints of the operator pod.

 

[https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L824]

!image-2024-03-19-15-59-20-933.png|width=548,height=432!

 

Solution:

We can modify the job submission process in the session mode. The jobmanager can provide a `/jars/run` rest api that supports self-downloading the job jar, and the operator only needs to send a rest request to submit the job, without download and upload the job jar. In this way, the submission pressure of the operator can be distributed to each job manager. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/24 07:59;Fei Feng;image-2024-03-19-15-59-20-933.png;https://issues.apache.org/jira/secure/attachment/13067522/image-2024-03-19-15-59-20-933.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 10:34:52 UTC 2024,,,,,,,,,,"0|z1o1u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 08:13;gyfora;That makes sense, however this is more a ticket for Flink core / web submission at this point. A similar improvement could be made for supporting job submissions without jars (from the cluster classpath). A FLIP would be necessary for making these changes for the rest job submission ;;;","19/Mar/24 10:34;Fei Feng;Yes, this is not an issue on the Flink kubernetes operator side, the operator is just a client, and the server side needs to provide new api in order to complete this lightweight submission remodeling.
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestClusterClient.requestJobResult throw ConnectionClosedException when the accumulator data is large,FLINK-34727,13572352,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,wanchengxiao,wanchengxiao,19/Mar/24 07:40,23/Apr/24 03:05,04/Jun/24 20:40,,1.16.2,1.19.0,,,,,,,,,Runtime / REST,,,,,0,pull-request-available,,,"The task was succeed, but ""RestClusterClient.requestJobResult()"" encountered an error reporting ConnectionClosedException. (Channel became inactive)
After debugging, it is speculated that the problem occurred in the flink task server-side ""AbstractRestHandler.respondToRequest()"" with the ""response.thenAccept(resp -> HandlerUtils.sendResponse())"", this ""thenAccept()"" did not pass the future returned by sendResponse, causing the server shutdown process before the request was sent. I suspect that ""thenAccept()"" needs to be replaced with ""thenCompose()""

The details are as follows:

 

*Pseudocode：*

!image-2024-03-19-15-51-20-150.png|width=802,height=222!

 

*Server handling steps:*

netty-thread: got request
flink-dispatcher-thread: exec requestJobResult[6] and complete shutDownFuture[8], then call HandlerUtils.sendResponse[13](netty async write)
netty-thread: write some data to channel.(not done)
flink-dispatcher-thread: call inFlightRequestTracker.deregisterRequest[15]
netty-thread: write some data to channel failed, channel not active

i added some log to trace this bug:

!AbstractHandler.png|width=406,height=313!

!AbstractRestHandler.png|width=418,height=322!

!MiniDispatcher.png|width=419,height=277!

!RestServerEndpoint.png|width=419,height=279!

then i got:

/{*}then call requestJobResult and shutDownFuture.complete; (close channel when request deregisted){*}/
2024-03-17 18:01:34.788 [flink-akka.actor.default-dispatcher-20] INFO  o.a.flink.runtime.rest.handler.job.JobExecutionResultHandler  - JobExecutionResultHandler gateway.requestJobStatus complete. [jobStatus=FINISHED]
/{*}submit sendResponse{*}/
2024-03-17 18:01:34.821 [flink-akka.actor.default-dispatcher-20] INFO  o.a.flink.runtime.rest.handler.job.JobExecutionResultHandler  - submit HandlerUtils.sendResponse().
/{*}thenAccept(sendResponse()) is complete, will call inFlightRequestTracker, but sendResponse's return future not completed{*}  /
2024-03-17 18:01:34.821 [flink-akka.actor.default-dispatcher-20] INFO  o.a.flink.runtime.rest.handler.job.JobExecutionResultHandler  - requestProcessingFuture complete. [requestProcessingFuture=java.util.concurrent.CompletableFuture@1329aca5[Completed normally]]
/{*}sendResponse's write task is still running{*}/
2024-03-17 18:01:34.822 [flink-rest-server-netty-worker-thread-10] INFO  o.a.f.s.netty4.io.netty.handler.stream.ChunkedWriteHandler  - write
/{*}deregister request and then shut down, then channel close{*}/
2024-03-17 18:01:34.826 [flink-akka.actor.default-dispatcher-20] INFO  o.a.flink.runtime.rest.handler.job.JobExecutionResultHandler  - call inFlightRequestTracker.deregisterRequest() done
2024-03-17 18:01:34.827 [flink-rest-server-netty-worker-thread-10] INFO  o.a.f.shaded.netty4.io.netty.channel.DefaultChannelPipeline  - pipeline close.
2024-03-17 18:01:34.827 [flink-rest-server-netty-worker-thread-10] INFO  org.apache.flink.runtime.rest.handler.util.HandlerUtils  - lastContentFuture complete. [future=DefaultChannelPromise@621f03ea(failure: java.nio.channels.ClosedChannelException)]

*more details in flink_bug_complex.log*

 

 

 

Additionally:

During the process of investigating this bug, FutureUtils.retryOperationWithDelay swallowed the first occurrence of the ""Channel became inactive"" exception and, after several retries, the server was shut down,then the client throw ""Connection refused"" Exception. which had some impact on the troubleshooting process. Could we consider adding some logging here to aid in future diagnostics?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/24 07:39;wanchengxiao;AbstractHandler.png;https://issues.apache.org/jira/secure/attachment/13067520/AbstractHandler.png","19/Mar/24 07:39;wanchengxiao;AbstractRestHandler.png;https://issues.apache.org/jira/secure/attachment/13067519/AbstractRestHandler.png","19/Mar/24 07:39;wanchengxiao;MiniDispatcher.png;https://issues.apache.org/jira/secure/attachment/13067518/MiniDispatcher.png","19/Mar/24 07:39;wanchengxiao;RestServerEndpoint.png;https://issues.apache.org/jira/secure/attachment/13067517/RestServerEndpoint.png","19/Mar/24 07:40;wanchengxiao;flink_bug_complex.log;https://issues.apache.org/jira/secure/attachment/13067515/flink_bug_complex.log","19/Mar/24 07:40;wanchengxiao;flink_bug_simple.log;https://issues.apache.org/jira/secure/attachment/13067516/flink_bug_simple.log","19/Mar/24 07:51;wanchengxiao;image-2024-03-19-15-51-20-150.png;https://issues.apache.org/jira/secure/attachment/13067521/image-2024-03-19-15-51-20-150.png",,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 23 03:05:15 UTC 2024,,,,,,,,,,"0|z1o1tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 03:26;Weijie Guo;This part of code hasn't changed in a while. I lowered the priority to critical.;;;","23/Apr/24 03:05;wanchengxiao;[~Weijie Guo] Hello, can you help find someone to take a look? We’ve been troubled by this issue for some time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kubernetes Operator has some room for optimizing performance.,FLINK-34726,13572326,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Fei Feng,Fei Feng,19/Mar/24 05:59,19/Mar/24 10:40,04/Jun/24 20:40,,kubernetes-operator-1.5.0,kubernetes-operator-1.6.0,kubernetes-operator-1.7.0,,,,,,,,Kubernetes Operator,,,,,0,,,,"When there is a huge number of FlinkDeployment and FlinkSessionJob in a kubernetes cluster, there will be a significant delay between event submit into reconcile thread pool and  event is processed. 

this is our test：we give operator enough resource（cpu: 10core, memory: 20g, reconcile thread pool  size was 200 ) and we deployed 10000 jobs firstly (one FlinkDeployment and one SessionJob per job) , then we do submit/delete job tests. we found that 
1. it cost about 2min between create new FlinkDeployment and FlinkSessionJob CR to k8s and the flink job submited to jobmanager.
2. it cost about 1min between delete a FlinkDeployment and FlinkSessionJob CR  and the flink job and session cluster cleared.

 

I use async-profiler to get flamegraph when  there is a huge number FlinkDeployment and FlinkSessionJob. I found two obvious areas for optimization

1. For Flinkdeployment: in the observe step, we call AbstractFlinkService.getClusterInfo/listJobs/getTaskManagerInfo , every time we call these method we need create RestClusterClient/ send requests/ close, I think we should reuse RestClusterClient as much as possible to avoid frequently creating objects to reduce GC pressure

2. For FlinkSessionJob （This issue is more obvious）: in the whole reconcile loop, we call getSecondaryResource 5 times to get FlinkDeployement resource info. Based on my current understanding of the Flink Operator, I think we do not need to call it 5 times in a single reconcile loop, calling it once is enough. If yes, we cloud save 30% cpu usage (every getSecondaryResource cost 6% cpu usage)

[^operator_no_submit_no_kill.flamegraph.html]

I hope we can discuss solutions to address this problem together. I'm very willing to optimize and resolve this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/24 05:38;Fei Feng;operator_no_submit_no_kill.flamegraph.html;https://issues.apache.org/jira/secure/attachment/13067513/operator_no_submit_no_kill.flamegraph.html",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 10:31:16 UTC 2024,,,,,,,,,,"0|z1o1ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 07:41;gyfora;Thanks for the detailed analysis [~Fei Feng] . You are completely right that we don't optimise the rest client usage and that may add a significant overhead. We have done similar optimisation in the past for config access/generation by using the FlinkResourceContext class. 

We could probably move the rest client generation logic there instead of hiding it under the FlinkService completely. This will be however a bigger change as it will affect the methods of the FlinkService interface as well.

Sounds a bit strange that getSecondaryResource is so expensive as that should happen from a cache. We should look into it while it's expensive in the first place because passing the FlinkDeployment objects around will make the code a bit more complicated, but I guess that could also be hidden under the FlinkSessionJobContext;;;","19/Mar/24 10:15;Fei Feng;""Sounds a bit strange that getSecondaryResource is so expensive as that should happen from a cache. ""
-------------------------------------------------------------
it's not strange actually, because there is object json de/serialization logic in getSecondaryResource 's implementation (you can see in flamgraph ), so FlinkDeployment CR's size may effect this process's cpu cost. (our FlinkDeployment CR size was 21K, it's not reasonable and we will reduce CR size) ;;;","19/Mar/24 10:31;Fei Feng;I think this two things (rest cluster client  and  sessionjob's secondary resource) should be contained in xxxContext , then we do not need to create them again and again,  and we can avoid unnecessary runtime overhead and GC pressure finally.

The difficulty lies in how to promptly update when changes occur. for example, rest cluster client should be recreate or update  if jobmanager rest address changed, and if FlinkDeployment object changed, sessionjob's SecondaryResource should be update too
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dockerfiles for release publishing has incorrect config.yaml path,FLINK-34725,13572323,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,lincoln.86xy,lincoln.86xy,19/Mar/24 04:53,21/Mar/24 06:35,04/Jun/24 20:40,21/Mar/24 06:28,1.19.0,,,,,,1.19.1,1.20.0,,,flink-docker,,,,,0,pull-request-available,,,"An issue found when do docker image publishing, unexpected error msg:
{code:java}
sed: can't read /config.yaml: No such file or directory{code}
 

also found in flink-docker/master daily Publish SNAPSHOTs  action:
[https://github.com/apache/flink-docker/actions/runs/8210534289/job/22458150514#step:8:588]

[https://github.com/apache/flink-docker/actions/runs/8210534289/job/22458150322#step:8:549]

 

This related to changes by https://issues.apache.org/jira/browse/FLINK-34205",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 06:35:30 UTC 2024,,,,,,,,,,"0|z1o1n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 05:50;JunRuiLi;Thanks [~lincoln.86xy] for reporting this issue. 

 
This issue occurs because the `config-parser-utils.sh` script is attempting to retrieve the Java Home setting specified by the user in `config.yaml` but fails to find the file due to an empty parameter being passed (see the code [[here]|https://github.com/apache/flink/blob/dd382894b4da5b2d153913c92b6679fbd877b18b/flink-dist/src/main/flink-bin/bin/config-parser-utils.sh#L28]).

This script is intended for use with flink-docker only, and primarily affects the reading of the Java Home configuration specified by the user. As a result,  the system-wide JAVA_HOME setting is always used.

I will prepare a PR to address this issue;;;","19/Mar/24 05:56;lincoln.86xy;[~JunRuiLi] Thanks for the updates!

So this seems not a flink-docker release issue but a bug of flink-1.19.0, I'm going to change the fixVersion to 1.19.1 & 1.20.0.

 ;;;","21/Mar/24 06:28;zhuzh;master: 3f4a80989fe7243983926f09fac2283f6fa63693
release-1.19: f53c5628e43777b4b924ec81224acc3df938800a;;;","21/Mar/24 06:35;lincoln.86xy;Thanks for fixing this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken web link in document of  legacy Flink CDC Sources,FLINK-34724,13572320,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,loserwang1024,loserwang1024,loserwang1024,19/Mar/24 03:51,19/Mar/24 08:49,04/Jun/24 20:40,19/Mar/24 08:49,,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,"In current document, many link is broken due to directory structure changes, such as:
 * http://localhost:1313/flink/flink-cdc-docs-master/docs/connectors/legacy-flink-cdc-sources/overview/mongodb-cdc.md
 * https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-mongodb-cdc/3.0-SNAPSHOT/flink-sql-connector-mongodb-cdc-3.0-SNAPSHOT.jar",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 08:49:26 UTC 2024,,,,,,,,,,"0|z1o1mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 08:49;renqs;flink-cdc master: 8098a9ca3e847a8f8ee2f46b37e19d124a22617a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet writer should restrict map keys to be not null,FLINK-34723,13572311,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xccui,xccui,19/Mar/24 00:50,19/Mar/24 14:06,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,"We got the following exception when reading a parquet file (with map types) generated by Flink.
{code:java}
Map keys must be annotated as required.{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-19 00:50:49.0,,,,,,,,,,"0|z1o1kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support conditional upserts with Postgres JDBC sink,FLINK-34722,13572277,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pietro97,pietro97,18/Mar/24 16:58,22/Mar/24 07:59,04/Jun/24 20:40,,jdbc-3.1.2,,,,,,,,,,Connectors / JDBC,,,,,0,,,,"The default Postgres dialect used by the JDBC sink for PostgreSQL DBs does not support custom _WHERE_ conditions inside upsert statements at the moment.

Indeed, upsert statements returned by the {{[getUpsertStatement()|https://github.com/apache/flink-connector-jdbc/blob/95294ffbc57c93c2af34cda94c27fc5781e06177/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractPostgresCompatibleDialect.java#L61]}} method are limited to:
{code:sql}
ON CONFLICT (col1, ..., colN) 
DO UPDATE SET (col1=EXCLUDED.col1, ..., colN=EXCLUDED.colN)
{code}
PostgreSQL allows a finer-grained control of upsert statements by specifying a _WHERE_ statement (see [ON CONFLICT Clause|https://www.postgresql.org/docs/current/sql-insert.html#SQL-ON-CONFLICT]), for instance:
{code:sql}
ON CONFLICT (col1, ..., colN) 
DO UPDATE SET (col1=EXCLUDED.col1, ..., colN=EXCLUDED.colN) 
WHERE colN < EXCLUDED.colN
{code}
 
This could be useful in many use cases, for instance, in a CDC scenario where a batch reconciliation process has written records in the destination, which now face the risk of being overwritten by late arriving, stale, records in the streaming pipeline (adding a condition on the operation timestamp could protect from these events). 

My proposal is to extend the {{[AbstractPostgresCompatibleDialect|https://github.com/apache/flink-connector-jdbc/blob/main/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractPostgresCompatibleDialect.java]}} functionalities by making the upsert query support _WHERE_ statements provided by users.

I'm thinking of two possible approaches, but I'd love to hear your opinion on this:
 # provide the statement through options of the JDBC sink connector.
 # allow users to plug custom dialects without them having to rewrite the whole JDBC sink (about this I'll open a separate issue soon) 

Thanks for your consideration",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 07:59:47 UTC 2024,,,,,,,,,,"0|z1o1cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/24 07:59;ouyangwuli;[~pietro97] I think it is a very useful provement for jdbc connector in our production env. we can impl provide the statement through options of the JDBC sink connector replace 'rewrite the whole JDBC'.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An grammar error occurs in the Barrier test file,FLINK-34721,13572258,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hctommy,hctommy,18/Mar/24 15:18,27/Apr/24 11:41,04/Jun/24 20:40,,,,,,,,,,,,Tests,,,,,0,pull-request-available,,,"In the flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/io/checkpointing/BarrierAlignmentUtilTest.java file, there is a grammar error in the description of a thrown Exception when testing  BarrierAlignmentUtil.DelayableTimer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/24 15:18;hctommy;incorrect grammar.png;https://issues.apache.org/jira/secure/attachment/13067499/incorrect+grammar.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-18 15:18:46.0,,,,,,,,,,"0|z1o18o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy Maven Snapshot failed on AZP,FLINK-34720,13572248,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,rskraba,rskraba,18/Mar/24 13:39,19/Mar/24 13:33,04/Jun/24 20:40,19/Mar/24 07:39,1.20.0,,,,,,,,,,Build System / CI,,,,,0,pull-request-available,test-stability,," 

There isn't any obvious reason that {{mvn: command not found}} could have occurred, but we saw it three times this weekend.
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f]  
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f&l=36]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f&l=36]

 
{code:java}
+ [[ tools != \t\o\o\l\s ]]
+ cd ..
+ echo 'Deploying to repository.apache.org'
+ COMMON_OPTIONS='-Prelease,docs-and-source -DskipTests -DretryFailedDeploymentCount=10 -Dmaven.repo.local=/__w/1/.m2/repository -Dmaven.wagon.http.pool=false -Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat=HH:mm:ss.SSS -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn --no-snapshot-updates -B   -Dgpg.skip -Drat.skip -Dcheckstyle.skip --settings /__w/1/s/tools/deploy-settings.xml'
+ mvn clean deploy -Prelease,docs-and-source -DskipTests -DretryFailedDeploymentCount=10 -Dmaven.repo.local=/__w/1/.m2/repository -Dmaven.wagon.http.pool=false -Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat=HH:mm:ss.SSS -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn --no-snapshot-updates -B -Dgpg.skip -Drat.skip -Dcheckstyle.skip --settings /__w/1/s/tools/deploy-settings.xml
Deploying to repository.apache.org
./releasing/deploy_staging_jars.sh: line 46: mvn: command not found

##[error]Bash exited with code '127'.
Finishing: Deploy maven snapshot
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34194,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 13:33:51 UTC 2024,,,,,,,,,,"0|z1o16g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 17:25;rskraba;As an example, using the last pipeline where Deploy Maven Snapshot worked:
 *  [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58320&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f]

The only difference that I see in the logs is that the Java 8 patch version was bumped:


{code:java}
Java version: 1.8.0_292, vendor: Private Build, runtime: /usr/lib/jvm/java-8-openjdk-amd64/jre {code}
to
{code:java}
Java version: 1.8.0_392, vendor: Private Build, runtime: /usr/lib/jvm/java-8-openjdk-amd64/jre {code}
which led me to the bump from the image in FLINK-34194, which removes a secondary installation of maven.

It looks like the right thing to do is to fall back to {{$MAVEN_WRAPPER}} and *then* to {{mvn}};;;","19/Mar/24 07:37;mapohl;Your conclusion is correct. I missed certain places when merging FLINK-34194. But it's not only the artifact deployment (also the docs build is failing now on master due to that issue). I reverted the FLINK-34194 change and will go over it once more.;;;","19/Mar/24 07:39;mapohl;master: 676a1995c2b20e2ffc20c0b9b99d14656878d3bf;;;","19/Mar/24 13:33;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58398&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f&l=34] (before the revert);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamRecordTest#testWithTimestamp fails on Azure,FLINK-34719,13572244,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabaosun,rskraba,rskraba,18/Mar/24 13:27,21/Mar/24 11:03,04/Jun/24 20:40,18/Mar/24 23:43,1.20.0,,,,,,1.20.0,,,,Tests,,,,,0,pull-request-available,test-stability,,"The ClassCastException *message* expected in StreamRecordTest#testWithTimestamp as well as StreamRecordTest#testWithNoTimestamp fails on JDK 11, 17, and 21
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=10341]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=9828]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=9833]

{code:java}
Expecting throwable message:
Mar 16 01:35:07   ""class org.apache.flink.streaming.runtime.streamrecord.StreamRecord cannot be cast to class org.apache.flink.streaming.api.watermark.Watermark (org.apache.flink.streaming.runtime.streamrecord.StreamRecord and org.apache.flink.streaming.api.watermark.Watermark are in unnamed module of loader 'app')""
Mar 16 01:35:07 to contain:
Mar 16 01:35:07   ""cannot be cast to org.apache.flink.streaming.api.watermark.Watermark""
Mar 16 01:35:07 but did not.
Mar 16 01:35:07 
Mar 16 01:35:07 Throwable that failed the check:
Mar 16 01:35:07 
Mar 16 01:35:07 java.lang.ClassCastException: class org.apache.flink.streaming.runtime.streamrecord.StreamRecord cannot be cast to class org.apache.flink.streaming.api.watermark.Watermark (org.apache.flink.streaming.runtime.streamrecord.StreamRecord and org.apache.flink.streaming.api.watermark.Watermark are in unnamed module of loader 'app')
Mar 16 01:35:07 	at org.apache.flink.streaming.runtime.streamrecord.StreamElement.asWatermark(StreamElement.java:92)
Mar 16 01:35:07 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
Mar 16 01:35:07 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25544,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 10:55:27 UTC 2024,,,,,,,,,,"0|z1o15k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 13:54;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9934]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=9739]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=9760]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9974]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=9719]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=9794];;;","18/Mar/24 14:57;rskraba;It looks like the problem is the changed exception message for ClassCastExceptions in Java 11+ [~jiabaosun] – we might just want to drop checking the message since the fact that it's a ClassCastException is probably sufficiently explicit!;;;","18/Mar/24 15:03;jiabaosun;Thanks [~rskraba] for reporting this.
I'm looking into this problem.;;;","18/Mar/24 23:43;jiabaosun;Fixed via master: 8ec5e7e830b5bda30ead3638a1faa3567d80bb7b;;;","21/Mar/24 10:55;mapohl;These build failures didn't include the fix, yet:
* https://github.com/apache/flink/actions/runs/8304571223/job/22730546057#step:10:10174
* https://github.com/apache/flink/actions/runs/8304571223/job/22730544007#step:10:10045
* https://github.com/apache/flink/actions/runs/8304571223/job/22730531437#step:10:10077
* https://github.com/apache/flink/actions/runs/8312246651/job/22747313654#step:10:10176
* https://github.com/apache/flink/actions/runs/8312246651/job/22747387684#step:10:10044
* https://github.com/apache/flink/actions/runs/8312246651/job/22747314118#step:10:10073
* https://github.com/apache/flink/actions/runs/8320242443/job/22764923155#step:10:10167
* https://github.com/apache/flink/actions/runs/8320242443/job/22764925256#step:10:10045
* https://github.com/apache/flink/actions/runs/8320242443/job/22764919890#step:10:10076;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyedPartitionWindowedStream and NonPartitionWindowedStream IllegalStateException in AZP,FLINK-34718,13572239,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rskraba,rskraba,rskraba,18/Mar/24 12:51,21/Mar/24 11:08,04/Jun/24 20:40,20/Mar/24 08:13,1.20.0,,,,,,1.20.0,,,,API / DataStream,,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58320&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9646]

18 of the KeyedPartitionWindowedStreamITCase and NonKeyedPartitionWindowedStreamITCase unit tests introduced in FLINK-34543 are failing in the adaptive scheduler profile, with errors similar to:
{code:java}
Mar 15 01:54:12 Caused by: java.lang.IllegalStateException: The adaptive scheduler supports pipelined data exchanges (violated by MapPartition (org.apache.flink.streaming.runtime.tasks.OneInputStreamTask) -> ddb598ad156ed281023ba4eebbe487e3).
Mar 15 01:54:12 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
Mar 15 01:54:12 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.assertPreconditions(AdaptiveScheduler.java:438)
Mar 15 01:54:12 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.<init>(AdaptiveScheduler.java:356)
Mar 15 01:54:12 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerFactory.createInstance(AdaptiveSchedulerFactory.java:124)
Mar 15 01:54:12 	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:121)
Mar 15 01:54:12 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:384)
Mar 15 01:54:12 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:361)
Mar 15 01:54:12 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:128)
Mar 15 01:54:12 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:100)
Mar 15 01:54:12 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
Mar 15 01:54:12 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
Mar 15 01:54:12 	... 4 more
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 10:48:24 UTC 2024,,,,,,,,,,"0|z1o14g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 13:56;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9325]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9670]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8354];;;","18/Mar/24 14:51;mapohl;[~Wencong Liu] can you have a look at this?;;;","18/Mar/24 15:00;Wencong Liu;Sure, I'll take a look now. [~mapohl] ;;;","18/Mar/24 15:16;Wencong Liu;The newly introduced DataStream operators are designed based on the mechanism of FLIP-331, which means that the ResultPartitionType for specific operators in a streaming job can be BLOCKING. However, the AdaptiveScheduler mandates that the ResultPartitionType for all operators must be PIPELINED, therefore, these operators are not suitable to be executed under the configuration of the AdaptiveScheduler. The default scheduler for IT tests is the {_}DefaultScheduler{_}, and I'm curious as to why it would change to the AdaptiveScheduler. 🤔 [~rskraba] ;;;","19/Mar/24 13:59;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58398&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9670];;;","19/Mar/24 14:52;rskraba;I haven't been following FLIP-331 – if these operators are never appropriate for the AdaptiveScheduler, then we should exclude them from that stage in the pipeline.  If I understand correctly, this is a relatively easy thing to do for both Azure and GitHub actions.  I'll take a look.;;;","20/Mar/24 08:13;mapohl;master: [9ae0d14d72ec40d1addaf272ba9a0fab32319ec0|https://github.com/apache/flink/commit/9ae0d14d72ec40d1addaf272ba9a0fab32319ec0];;;","20/Mar/24 11:55;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58429&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=9664] (before the fix was committed to master);;;","21/Mar/24 10:48;mapohl;before the fix was committed to master:
* https://github.com/apache/flink/actions/runs/8290287716/job/22688325865#step:10:9329
* https://github.com/apache/flink/actions/runs/8304571223/job/22730531076#step:10:8057
* https://github.com/apache/flink/actions/runs/8312246651/job/22747312383#step:10:9345
* https://github.com/apache/flink/actions/runs/8336454518/job/22813876201#step:10:9330
* https://github.com/apache/flink/actions/runs/8352823788/job/22863772571#step:10:9347;;;",,,,,,,,,,,,,,,,,,,,,,,,
BroadcastStateITCase failed fatally with 127 exit code,FLINK-34717,13572226,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rskraba,rskraba,18/Mar/24 10:48,18/Mar/24 14:47,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58306&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9069]
{code:java}
 Mar 14 13:58:43 13:58:43.330 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.2.2:test (integration-tests) on project flink-tests: 
Mar 14 13:58:43 13:58:43.330 [ERROR] 
Mar 14 13:58:43 13:58:43.330 [ERROR] Please refer to /__w/1/s/flink-tests/target/surefire-reports for the individual test results.
Mar 14 13:58:43 13:58:43.330 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Mar 14 13:58:43 13:58:43.330 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Mar 14 13:58:43 13:58:43.330 [ERROR] Command was /bin/sh -c cd '/__w/1/s/flink-tests' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/1/s/flink-tests/target/surefire/surefirebooter-20240314132147062_959.jar' '/__w/1/s/flink-tests/target/surefire' '2024-03-14T13-21-44_122-jvmRun1' 'surefire-20240314132147062_957tmp' 'surefire_254-20240314132147062_958tmp'
Mar 14 13:58:43 13:58:43.330 [ERROR] Error occurred in starting fork, check output in log
Mar 14 13:58:43 13:58:43.330 [ERROR] Process Exit Code: 127
Mar 14 13:58:43 13:58:43.330 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Mar 14 13:58:43 13:58:43.330 [ERROR] Command was /bin/sh -c cd '/__w/1/s/flink-tests' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '-Xmx1536m' '-jar' '/__w/1/s/flink-tests/target/surefire/surefirebooter-20240314132147062_959.jar' '/__w/1/s/flink-tests/target/surefire' '2024-03-14T13-21-44_122-jvmRun1' 'surefire-20240314132147062_957tmp' 'surefire_254-20240314132147062_958tmp'
Mar 14 13:58:43 13:58:43.330 [ERROR] Error occurred in starting fork, check output in log
Mar 14 13:58:43 13:58:43.330 [ERROR] Process Exit Code: 127
Mar 14 13:58:43 13:58:43.330 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456)
Mar 14 13:58:43 13:58:43.330 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:418)
Mar 14 13:58:43 13:58:43.330 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
Mar 14 13:58:43 13:58:43.331 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:250)
{code}
Looking at the watchdog, only the org.apache.flink.test.streaming.runtime.BroadcastStateITCase is started without finished.  It has two test methods which are both successfully run, so the problem might with the {{MiniClusterWithClientResource}} not shutting down.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-18 10:48:20.0,,,,,,,,,,"0|z1o11k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build 1.19 docs in GitHub Action and mark 1.19 as stable in docs,FLINK-34716,13572224,13572190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,18/Mar/24 10:38,21/Mar/24 14:52,04/Jun/24 20:40,18/Mar/24 13:54,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 13:54:28 UTC 2024,,,,,,,,,,"0|z1o114:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 13:54;lincoln.86xy;release-1.19: a6a4667202a0f89fe63ff4f2e476c0200ec66e63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix mysql ut about closing BinlogSplitReader,FLINK-34715,13572211,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pacinogong,pacinogong,pacinogong,18/Mar/24 08:03,18/Mar/24 10:12,04/Jun/24 20:40,18/Mar/24 10:12,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"BinlogSplitReaderTest#readBinlogSplitsFromSnapshotSplits should test binlog reader is closed after binlog reader close. But code always test snapshot split reader is closed.
{code:java}
binlogReader.close();
assertNotNull(snapshotSplitReader.getExecutorService());
assertTrue(snapshotSplitReader.getExecutorService().isTerminated());{code}
We shoud change code to 
{code:java}
binlogReader.close();
assertNotNull(binlogReader.getExecutorService());
assertTrue(binlogReader.getExecutorService().isTerminated()); {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-cdc/pull/3161,,,,,,,,,,9223372036854775807,,,JAVA,Mon Mar 18 10:12:24 UTC 2024,,,,,,,,,,"0|z1o0y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 09:03;pacinogong;[~Leonard] PTAL;;;","18/Mar/24 10:12;leonard;Fixed in flink-cdc(master): 14b81dc6ec804bd516005a177aea00be75df3cc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start End of Life discussion thread for now outdated Flink minor version,FLINK-34714,13572198,13572190,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lincoln,lincoln.86xy,18/Mar/24 06:25,18/Mar/24 06:49,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"The idea is to discuss whether we should do a final release for the now not supported minor version in the community. Such a minor release shouldn't be covered by the current minor version release managers. Their only responsibility is to trigger the discussion.

The intention of a final patch release for the now unsupported Flink minor version is to flush out all the fixes that didn't end up in the previous release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-18 06:25:17.0,,,,,,,,,,"0|z1o0vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updates the docs stable version,FLINK-34713,13572197,13572190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,18/Mar/24 06:25,21/Mar/24 14:59,04/Jun/24 20:40,20/Mar/24 16:14,,,,,,,,,,,,,,,,0,,,,"Update docs to ""stable"" in {{docs/config.toml}} in the branch of the _just-released_ version:
 * Change V{{{}ersion{}}} from {{{}x.y-SNAPSHOT }}to \{{{}x.y.z{}}}, i.e. {{1.6-SNAPSHOT}} to {{1.6.0}}
 * Change V{{{}ersionTitle{}}} from {{x.y-SNAPSHOT}} to {{{}x.y{}}}, i.e. {{1.6-SNAPSHOT}} to {{1.6}}
 * Change Branch from {{master}} to {{{}release-x.y{}}}, i.e. {{master}} to {{release-1.6}}
 * Change {{baseURL}} from {{//[ci.apache.org/projects/flink/flink-docs-master|http://ci.apache.org/projects/flink/flink-docs-master]}} to {{//[ci.apache.org/projects/flink/flink-docs-release-x.y|http://ci.apache.org/projects/flink/flink-docs-release-x.y]}}
 * Change {{javadocs_baseurl}} from {{//[ci.apache.org/projects/flink/flink-docs-master|http://ci.apache.org/projects/flink/flink-docs-master]}} to {{//[ci.apache.org/projects/flink/flink-docs-release-x.y|http://ci.apache.org/projects/flink/flink-docs-release-x.y]}}
 * Change {{IsStable}} to {{true}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 14:58:09 UTC 2024,,,,,,,,,,"0|z1o0v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 16:14;lincoln.86xy;Already fixed by 3d67a2b9b208ed662f81c1ef43d76d10a82a7d88 on release-1.19 branch;;;","21/Mar/24 14:58;lincoln.86xy;master also needed: [8ee552a326e9fbcad1df5cfc1abb23ac2cdd56af|https://github.com/apache/flink/commit/8ee552a326e9fbcad1df5cfc1abb23ac2cdd56af];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update reference data for Migration Tests,FLINK-34712,13572196,13572190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,18/Mar/24 06:25,07/Apr/24 08:53,04/Jun/24 20:40,07/Apr/24 08:53,,,,,,,1.19.1,1.20.0,,,,,,,,0,pull-request-available,,,"Update migration tests in master to cover migration from new version. Since 1.18, this step could be done automatically with the following steps. For more information please refer to [this page.|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-migration-test-utils/README.md]
 # {*}On the published release tag (e.g., release-1.16.0){*}, run 
{panel}
{panel}
|{{$ mvn clean }}{{package}} {{{}-Pgenerate-migration-test-data -Dgenerate.version={}}}{{{}1.16{}}} {{-nsu -Dfast -DskipTests}}|

The version (1.16 in the command above) should be replaced with the target one.

 # Modify the content of the file [apache/flink:flink-test-utils-parent/flink-migration-test-utils/src/main/resources/most_recently_published_version|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-migration-test-utils/src/main/resources/most_recently_published_version] to the latest version (it would be ""v1_16"" if sticking to the example where 1.16.0 was released). 
 # Commit the modification in step a and b with ""{_}[release] Generate reference data for state migration tests based on release-1.xx.0{_}"" to the corresponding release branch (e.g. {{release-1.16}} in our example), replace ""xx"" with the actual version (in this example ""16""). You should use the Jira issue ID in case of [release]  as the commit message's prefix if you have a dedicated Jira issue for this task.

 # Cherry-pick the commit to the master branch. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 08:24:21 UTC 2024,,,,,,,,,,"0|z1o0uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/24 08:24;lincoln.86xy;fixed in 1.19: 4397c9f1d0f300bd8c0f9de4e59e1a5d7883ec2c
fixed in master: 5bbcf8de79ce1979412879b919299ffa5a9b62fe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Other announcements,FLINK-34711,13572195,13572190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,lincoln,lincoln,lincoln.86xy,18/Mar/24 06:25,19/Mar/24 06:08,04/Jun/24 20:40,19/Mar/24 04:27,,,,,,,,,,,,,,,,0,,,,"h3. Recordkeeping

Use [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink] to seed the information about the release into future project reports.

(Note: Only PMC members have access report releases. If you do not have access, ask on the mailing list for assistance.)
h3. Flink blog

Major or otherwise important releases should have a blog post. Write one if needed for this particular release. Minor releases that don’t introduce new major functionality don’t necessarily need to be blogged (see [flink-web PR #581 for Flink 1.15.3|https://github.com/apache/flink-web/pull/581] as an example for a minor release blog post).

Please make sure that the release notes of the documentation (see section ""Review and update documentation"") are linked from the blog post of a major release.
We usually include the names of all contributors in the announcement blog post. Use the following command to get the list of contributors:
{code}
# first line is required to make sort first with uppercase and then lower
export LC_ALL=C
export FLINK_PREVIOUS_RELEASE_BRANCH=<previousReleaseBranch>
export FLINK_CURRENT_RELEASE_BRANCH=<currentReleaseBranch>
# e.g.
# export FLINK_PREVIOUS_RELEASE_BRANCH=release-1.17
# export FLINK_CURRENT_RELEASE_BRANCH=release-1.18
git log $(git merge-base master $FLINK_PREVIOUS_RELEASE_BRANCH)..$(git show-ref --hash ${FLINK_CURRENT_RELEASE_BRANCH}) --pretty=format:""%an%n%cn"" | sort  -u | paste -sd, | sed ""s/\,/\, /g""
{code}
h3. Social media

Tweet, post on Facebook, LinkedIn, and other platforms. Ask other contributors to do the same.
h3. Flink Release Wiki page

Add a summary of things that went well or that went not so well during the release process. This can include feedback from contributors but also more generic things like the release have taken longer than initially anticipated (and why) to give a bit of context to the release process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 04:27:32 UTC 2024,,,,,,,,,,"0|z1o0uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 04:27;lincoln.86xy;* Filled [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink] by [~leonard]

* Apache Flink twitter account has post the announcement

* release wiki page been updated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache mailing lists announcements,FLINK-34710,13572194,13572190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,18/Mar/24 06:25,18/Mar/24 09:09,04/Jun/24 20:40,18/Mar/24 09:09,,,,,,,,,,,,,,,,0,,,,"Announce on the {{dev@}} mailing list that the release has been finished.

Announce on the release on the {{user@}} mailing list, listing major improvements and contributions.

Announce the release on the [announce@apache.org|mailto:announce@apache.org] mailing list.
{panel}
{panel}
|{{From: Release Manager}}
{{To: dev@flink.apache.org, user@flink.apache.org, user-zh@flink.apache.org, announce@apache.org}}
{{Subject: [ANNOUNCE] Apache Flink 1.2.3 released}}
 
{{The Apache Flink community is very happy to announce the release of Apache Flink 1.2.3, which is the third bugfix release for the Apache Flink 1.2 series.}}
 
{{Apache Flink® is an open-source stream processing framework for distributed, high-performing, always-available, and accurate data streaming applications.}}
 
{{The release is available for download at:}}
{{[https://flink.apache.org/downloads.html]}}
 
{{Please check out the release blog post for an overview of the improvements for this bugfix release:}}
{{<blob post link>}}
 
{{The full release notes are available in Jira:}}
{{<jira release notes link>}}
 
{{We would like to thank all contributors of the Apache Flink community who made this release possible!}}
 
{{Feel free to reach out to the release managers (or respond to this thread) with feedback on the release process. Our goal is to constantly improve the release process. Feedback on what could be improved or things that didn't go so well are appreciated.}}
 
{{Regards,}}
{{Release Manager}}|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 09:09:37 UTC 2024,,,,,,,,,,"0|z1o0ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 09:09;lincoln.86xy;https://lists.apache.org/thread/sofmxytbh6y20nwot1gywqqc2lqxn4hm;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove outdated versions,FLINK-34709,13572193,13572190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lincoln,lincoln.86xy,18/Mar/24 06:25,18/Mar/24 09:09,04/Jun/24 20:40,18/Mar/24 09:09,,,,,,,,,,,,,,,,0,,,,"h4. dist.apache.org

For a new major release remove all release files older than 2 versions, e.g., when releasing 1.7, remove all releases <= 1.5.

For a new bugfix version remove all release files for previous bugfix releases in the same series, e.g., when releasing 1.7.1, remove the 1.7.0 release.
# If you have not already, check out the Flink section of the {{release}} repository on {{[dist.apache.org|http://dist.apache.org/]}} via Subversion. In a fresh directory:
{code}
svn checkout https://dist.apache.org/repos/dist/release/flink --depth=immediates
cd flink
{code}
# Remove files for outdated releases and commit the changes.
{code}
svn remove flink-<version_to_remove>
svn commit
{code}
# Verify that files  are [removed|https://dist.apache.org/repos/dist/release/flink]
(!) Remember to remove the corresponding download links from the website.

h4. CI

Disable the cron job for the now-unsupported version from (tools/azure-pipelines/[build-apache-repo.yml|https://github.com/apache/flink/blob/master/tools/azure-pipelines/build-apache-repo.yml]) in the respective branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-18 06:25:16.0,,,,,,,,,,"0|z1o0u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge website pull request,FLINK-34708,13572192,13572190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,18/Mar/24 06:25,18/Mar/24 08:58,04/Jun/24 20:40,18/Mar/24 08:58,,,,,,,,,,,,,,,,0,,,,"Merge the website pull request to [list the release|http://flink.apache.org/downloads.html]. Make sure to regenerate the website as well, as it isn't build automatically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 08:58:05 UTC 2024,,,,,,,,,,"0|z1o0u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 08:54;lincoln.86xy;Note: It's strongly recommend to use docker build when do the rebuild (hugo build may encounter version issues);;;","18/Mar/24 08:58;lincoln.86xy;web pr merged: 46b47589e5a77906febed94a17e2a2ba879e1015

effective web rebuild(helped by Martijn): 0bcb5514e9d93e9674ae1b832746294958170eae

(web rebuild doesn't take effect: f11ba16e6fd3c16d022f743ff5e99707bece8827 via a lower hugo version);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update japicmp configuration,FLINK-34707,13572191,13572190,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,18/Mar/24 06:25,22/Mar/24 13:07,04/Jun/24 20:40,22/Mar/24 13:07,,,,,,,1.19.1,1.20.0,,,,,,,,0,pull-request-available,,,"Update the japicmp reference version and wipe exclusions / enable API compatibility checks for {{@PublicEvolving}} APIs on the corresponding SNAPSHOT branch with the {{update_japicmp_configuration.sh}} script (see below).

For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.

Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.
{code:bash}
tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.sh
tools $ cd ..$ git add *$ git commit -m ""Update japicmp configuration for $RELEASE_VERSION"" {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 13:07:13 UTC 2024,,,,,,,,,,"0|z1o0ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/24 13:07;lincoln.86xy;1.19: 6eeae5fe6c9c48ee1e7546f26decbac429f248e1
master: a8e0936f1c40893907e79144233f65d7cd682184;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote release 1.19,FLINK-34706,13572190,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lincoln.86xy,lincoln,lincoln.86xy,18/Mar/24 06:25,22/Mar/24 13:09,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,,0,pull-request-available,,,"Once the release has been finalized (FLINK-32920), the last step of the process is to promote the release within the project and beyond. Please wait for 24h after finalizing the release in accordance with the [ASF release policy|http://www.apache.org/legal/release-policy.html#release-announcements].

*Final checklist to declare this issue resolved:*
 # Website pull request to [list the release|http://flink.apache.org/downloads.html] merged
 # Release announced on the user@ mailing list.
 # Blog post published, if applicable.
 # Release recorded in [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink].
 # Release announced on social media.
 # Completion declared on the dev@ mailing list.
 # Update Homebrew: [https://docs.brew.sh/How-To-Open-a-Homebrew-Pull-Request] (seems to be done automatically - at least for minor releases  for both minor and major releases)
 # Updated the japicmp configuration
 ** corresponding SNAPSHOT branch japicmp reference version set to the just released version, and API compatibiltity checks for {{@PublicEvolving}}  was enabled
 ** (minor version release only) master branch japicmp reference version set to the just released version
 ** (minor version release only) master branch japicmp exclusions have been cleared
 # Update the list of previous version in {{docs/config.toml}} on the master branch.
 # Set {{show_outdated_warning: true}} in {{docs/config.toml}} in the branch of the _now deprecated_ Flink version (i.e. 1.16 if 1.18.0 is released)
 # Update stable and master alias in [https://github.com/apache/flink/blob/master/.github/workflows/docs.yml]
 # Open discussion thread for End of Life for Unsupported version (i.e. 1.16)",,,,,,,,,,,,,,,,,,,,,,FLINK-32912,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 16:48:29 UTC 2024,,,,,,,,,,"0|z1o0tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 16:48;lincoln.86xy;# (/) Website pull request to [list the release|http://flink.apache.org/downloads.html] merged
 # (/) Release announced on the user@ mailing list: [[announcement link|https://lists.apache.org/thread/sofmxytbh6y20nwot1gywqqc2lqxn4hm]]
 # (/) Blog post published, if applicable:[ blog post|https://flink.apache.org/2024/03/18/announcing-the-release-of-apache-flink-1.19/]
 # (/) Release recorded in [reporter.apache.org: https://reporter.apache.org/addrelease.html?flink|https://reporter.apache.org/addrelease.html?flink]
 # (/) Release announced on social media: [Twitter|https://twitter.com/ApacheFlink/status/1638839542403981312?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1638839542403981312%7Ctwgr%5E7f3046f67668cf3ebbd929ef126a32473db2a1b5%7Ctwcon%5Es1_c10&ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2FApacheFlink2Fstatus2F1638839542403981312widget%3DTweet]
 # (/) Completion declared on the dev@ [mailing list |https://lists.apache.org/thread/z8sfwlppsodcyng62c584n76b69b16fc]
 # (/) Update Homebrew: [https://docs.brew.sh/How-To-Open-a-Homebrew-Pull-Request] (seems to be done automatically - at least for minor releases  for both minor and major releases): [https://formulae.brew.sh/formula/apache-flink#default]
 # (/) No need to update quickstart scripts in {{{}flink-web{}}}, under the {{q/}} directory (alread use global version variables) 
 # (/) Updated the japicmp configuration: Done in https://issues.apache.org/jira/browse/FLINK-34707
 # (/) Update the list of previous version in {{docs/config.toml}} on the master branch: Done in [https://github.com/apache/flink/pull/24548]
 # (/) Set {{show_outdated_warning: true}} in {{docs/config.toml}} in the branch of the _previous_ Flink version:  (for 1.17) [https://github.com/apache/flink/pull/24547]
 # (/) Update stable and master alias in [https://github.com/apache/flink/blob/master/.github/workflows/docs.yml] Done in [a6a4667|https://github.com/apache/flink/commit/a6a4667202a0f89fe63ff4f2e476c0200ec66e63]   [8ee552a|https://github.com/apache/flink/commit/8ee552a326e9fbcad1df5cfc1abb23ac2cdd56af];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase FAILURE,FLINK-34705,13572182,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mr_baixs,mr_baixs,18/Mar/24 04:18,18/Mar/24 04:20,04/Jun/24 20:40,,1.16.2,,,,,,,,,,Connectors / ElasticSearch,,,,,0,,,," 
{code:java}
// code placeholder
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 111.197 s <<< FAILURE! - in org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase
[ERROR] org.apache.flink.connector.elasticsearch.sink.ElasticsearchWriterITCase  Time elapsed: 111.197 s  <<< ERROR!
org.testcontainers.containers.ContainerLaunchException: Container startup failed
    at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:345)
    at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:326)
    at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.start(TestcontainersExtension.java:242)
    at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.access$200(TestcontainersExtension.java:229)
    at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$null$1(TestcontainersExtension.java:59)
    at org.junit.jupiter.engine.execution.ExtensionValuesStore.lambda$getOrComputeIfAbsent$4(ExtensionValuesStore.java:86)
    at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.computeValue(ExtensionValuesStore.java:223)
    at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.get(ExtensionValuesStore.java:211)
    at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.evaluate(ExtensionValuesStore.java:191)
    at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.access$100(ExtensionValuesStore.java:171)
    at org.junit.jupiter.engine.execution.ExtensionValuesStore.getOrComputeIfAbsent(ExtensionValuesStore.java:89)
    at org.junit.jupiter.engine.execution.NamespaceAwareStore.getOrComputeIfAbsent(NamespaceAwareStore.java:53)
    at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$beforeAll$2(TestcontainersExtension.java:59)
    at java.util.ArrayList.forEach(ArrayList.java:1259)
    at org.testcontainers.junit.jupiter.TestcontainersExtension.beforeAll(TestcontainersExtension.java:59)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$10(ClassBasedTestDescriptor.java:381)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:381)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:205)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
    at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
    at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
    at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)
    at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:338)
    ... 43 more
Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
    at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:537)
    at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:340)
    at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
    ... 44 more
Caused by: org.testcontainers.containers.ContainerLaunchException: Timed out waiting for log output matching '.*(""message"":\s?""started"".*|] started
$)'
    at org.testcontainers.containers.wait.strategy.LogMessageWaitStrategy.waitUntilReady(LogMessageWaitStrategy.java:49)
    at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)
    at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:926)
    at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:480)
    ... 46 more {code}
{code:java}
// code placeholder
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 64.189 s <<< FAILURE! - in org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase[ERROR] org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSinkITCase  Time elapsed: 64.189 s  <<< ERROR!org.testcontainers.containers.ContainerLaunchException: Container startup failed	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:345)	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:326)	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1063)	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)	at org.junit.rules.RunRules.evaluate(RunRules.java:20)	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:338)	... 29 moreCaused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:537)	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:340)	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)	... 30 moreCaused by: org.testcontainers.containers.ContainerLaunchException: Timed out waiting for log output matching '.*(""message"":\s?""started"".*|] started$)'	at org.testcontainers.containers.wait.strategy.LogMessageWaitStrategy.waitUntilReady(LogMessageWaitStrategy.java:49)	at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)	at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:926)	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:480)	... 32 more
[INFO] Running org.apache.flink.connector.elasticsearch.table.Elasticsearch7DynamicSinkITCase[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 61.856 s <<< FAILURE! - in org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkITCase[ERROR] org.apache.flink.connector.elasticsearch.sink.Elasticsearch7SinkITCase  Time elapsed: 61.856 s  <<< ERROR!org.testcontainers.containers.ContainerLaunchException: Container startup failed	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:345)	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:326)	at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.start(TestcontainersExtension.java:242)	at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.access$200(TestcontainersExtension.java:229)	at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$null$1(TestcontainersExtension.java:59)	at org.junit.jupiter.engine.execution.ExtensionValuesStore.lambda$getOrComputeIfAbsent$4(ExtensionValuesStore.java:86)	at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.computeValue(ExtensionValuesStore.java:223)	at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.get(ExtensionValuesStore.java:211)	at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.evaluate(ExtensionValuesStore.java:191)	at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.access$100(ExtensionValuesStore.java:171)	at org.junit.jupiter.engine.execution.ExtensionValuesStore.getOrComputeIfAbsent(ExtensionValuesStore.java:89)	at org.junit.jupiter.engine.execution.NamespaceAwareStore.getOrComputeIfAbsent(NamespaceAwareStore.java:53)	at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$beforeAll$2(TestcontainersExtension.java:59)	at java.util.ArrayList.forEach(ArrayList.java:1259)	at org.testcontainers.junit.jupiter.TestcontainersExtension.beforeAll(TestcontainersExtension.java:59)	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$10(ClassBasedTestDescriptor.java:381)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:381)	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:205)	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:338)	... 43 moreCaused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:537)	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:340)	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)	... 44 moreCaused by: org.testcontainers.containers.ContainerLaunchException: Timed out waiting for log output matching '.*(""message"":\s?""started"".*|] started$)'	at org.testcontainers.containers.wait.strategy.LogMessageWaitStrategy.waitUntilReady(LogMessageWaitStrategy.java:49)	at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)	at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:926)	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:480)	... 46 more
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 61.837 s <<< FAILURE! - in org.apache.flink.connector.elasticsearch.table.Elasticsearch7DynamicSinkITCase[ERROR] org.apache.flink.connector.elasticsearch.table.Elasticsearch7DynamicSinkITCase  Time elapsed: 61.837 s  <<< ERROR!org.testcontainers.containers.ContainerLaunchException: Container startup failed	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:345)	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:326)	at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.start(TestcontainersExtension.java:242)	at org.testcontainers.junit.jupiter.TestcontainersExtension$StoreAdapter.access$200(TestcontainersExtension.java:229)	at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$null$1(TestcontainersExtension.java:59)	at org.junit.jupiter.engine.execution.ExtensionValuesStore.lambda$getOrComputeIfAbsent$4(ExtensionValuesStore.java:86)	at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.computeValue(ExtensionValuesStore.java:223)	at org.junit.jupiter.engine.execution.ExtensionValuesStore$MemoizingSupplier.get(ExtensionValuesStore.java:211)	at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.evaluate(ExtensionValuesStore.java:191)	at org.junit.jupiter.engine.execution.ExtensionValuesStore$StoredValue.access$100(ExtensionValuesStore.java:171)	at org.junit.jupiter.engine.execution.ExtensionValuesStore.getOrComputeIfAbsent(ExtensionValuesStore.java:89)	at org.junit.jupiter.engine.execution.NamespaceAwareStore.getOrComputeIfAbsent(NamespaceAwareStore.java:53)	at org.testcontainers.junit.jupiter.TestcontainersExtension.lambda$beforeAll$2(TestcontainersExtension.java:59)	at java.util.ArrayList.forEach(ArrayList.java:1259)	at org.testcontainers.junit.jupiter.TestcontainersExtension.beforeAll(TestcontainersExtension.java:59)	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$10(ClassBasedTestDescriptor.java:381)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:381)	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:205)	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:338)	... 43 moreCaused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:537)	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:340)	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)	... 44 moreCaused by: org.testcontainers.containers.ContainerLaunchException: Timed out waiting for log output matching '.*(""message"":\s?""started"".*|] started$)'	at org.testcontainers.containers.wait.strategy.LogMessageWaitStrategy.waitUntilReady(LogMessageWaitStrategy.java:49)	at org.testcontainers.containers.wait.strategy.AbstractWaitStrategy.waitUntilReady(AbstractWaitStrategy.java:51)	at org.testcontainers.containers.GenericContainer.waitUntilContainerStarted(GenericContainer.java:926)	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:480)	... 46 more {code}
The scenario in which these errors occur is that I create a docker image using the dockerfile, mount flink in the specified directory and start compiling and executing the ci tests using maven, and these tests start reporting errors because I can't find docker in the environment, so I modified the dockerfile and added yum install docker.After installing docker in the image and compiling maven again ci test will be the above error

*My docker and maven compilation commands*
{code:java}
docker run --privileged --rm --cpus=""6"" -m 12g --user flinkci -v $WORKSPACE:/home/flinkci/flink -v /data1/flink/maven:/home/flinkci/.m2/repository -v /var/run/docker.sock:/var/run/docker-sock image:v1.0.0 /bin/bash -c \""cd /home/flinkci/flink && mvn clean install -Dscala-2.12 {code}
 

 ",Environment is x86 machine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-18 04:18:15.0,,,,,,,,,,"0|z1o0rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Process checkpoint barrier in AsyncWaitOperator when the element queue is full,FLINK-34704,13572181,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zakelly,zakelly,18/Mar/24 03:58,11/Apr/24 11:56,04/Jun/24 20:40,,,,,,,,,,,,Runtime / Task,,,,,0,,,,As discussed in https://lists.apache.org/thread/4f7ywn29kdv4302j2rq3fkxc6pc8myr2 . Maybe it is better to provide such a new `yield` that can process mail with low priority in the mailbox executor. More discussion needed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35051,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 11:56:59 UTC 2024,,,,,,,,,,"0|z1o0rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Apr/24 15:45;pnowojski;Adding new {{yield}} method wouldn't help, as we can not perform a checkpoint while operator is yielding, since that can lead to inconsistent state for upstream chained operators. For example when upstream operator does this:

{code}
OperatorXYZ::processElement(e, output) {
  stateA = e.foo;
  output.collect(e);
  stateB = e.bar;
}
{code}
Downstream chained operator inside {{output.collect(e)}} call, can not allow for checkpoint to be executed, since this would checkpoint the {{OperatorXYZ}} in an inconsistent state.

I've discovered this issue independently (but for other operators) and described the problem in FLINK-35051. I think a better solution would be to make {{StreamTask}} prioritise:
* processing priority messages from network stack over processing mails
* execute out of order ""urgent"" mails, like time outing aligned checkpoint barriers to unaligned ones or triggerCheckpoint RPCs for source tasks

I would propose to close this ticket for just {{AsyncWaitOperator}} in favour of FLINK-35051.;;;","11/Apr/24 03:34;zakelly;[~pnowojski] IIUC, the {{AsyncWaitOperator}} is a special one in following ways:
# It is a non-keyed operator, mainly for async IO fetching data from external storage/database. (https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/dev/datastream/operators/asyncio)
# Any on-going (invoked but not complete) requests will stay in a buffer and can be serialized into checkpoint, once a request finishes it will emit result to downstream and will not be included in a checkpoint.

So it is designed to be safe to checkpoint during waiting async results (during yield). This operator is the only one that allow for checkpoint in middle of waiting. I suggest that we could consider FLINK-35051 first and see if this ticket is still valid later.

;;;","11/Apr/24 07:39;pnowojski;{code:java}
So it is designed to be safe to checkpoint during waiting async results (during yield). This operator is the only one that allow for checkpoint in middle of waiting.
{code}
Yes and no. It can be checkpointed with in-flight requests, but it can not be checkpointed while it is calling {{.yield()}} anywhere in it's own code, due to the state corruption/inconsistency issue I described above. So the solution is not to let subtask checkpoint while {{AsyncWaitOperator}} is calling {{.yield()}}, but {{StreamTask}} should prioritize taking a checkpoint over processing {{AWOPs}} mails that are already enqueued. ;;;","11/Apr/24 08:45;zakelly;Actually, even if we don't checkpoint while it is calling {{{}yield(){}}}, the checkpoint still happen when some user code is running. You may see my investigation in ML ( [https://lists.apache.org/thread/4f7ywn29kdv4302j2rq3fkxc6pc8myr2] ), where all the records in queue actually invoked the `asyncInvoke` and are waiting for their result, and at this moment a checkpoint barrier mail comes which is able to trigger and finish the cp normally. The key point is the user code is stateless, only reading some external databases, thus the checkpoint can proceed in the meantime.

I agree that the {{StreamTask}} should prioritize taking a checkpoint over processing mails that are already enqueued. But this ticket is actually introducing a possible optimization for very specific scenario and only for this. For the original problem described in ML, I don't think the FLINK-35051 could resolve it. Yet I do think FLINK-35051 is more important and we could pay more attention on it.;;;","11/Apr/24 09:56;pnowojski;Yes, I've read the referenced dev mailing list thread and I still don't agree with the conclusion there, that AWOP should allow for a checkpoint to happen while it's {{.yield()}}'ing :)
{quote}
the checkpoint still happen when some user code is running.
{quote}
If you are talking about async user code from AWOP, that's not a problem and never has been. When I'm talking about problems with state inconsistency of upstream chained operators, that's exactly what I mean. Problem is for UPSTREAM operator that is CHAINED with AWOP:

network input -> OperatorA -> AWOP -> network output

OperatorA's state can become corrupted/inconsistent if you allow for checkpoint to happen while {{AWOP}} invokes {{.yield()}}. That's the whole point of the yield to downstream (https://issues.apache.org/jira/browse/FLINK-13063). And side note, it's not only a problem of {{AWOP}} but any operator that wants to {{.yield()}}. Operators can only yield to downstream, which also means checkpoints can not be executed, and it affects also firing timers. 

I have a WIP FLIP-443 that will introduce a neat trick to allow operators to return the execution back to {{StreamTask}} so that checkpoint can happen, but AFAIU it has no application for the {{AWOP}}. When {{AWOP}} wants to {{.yield()}}, it means it has no more space in the buffer to store any more records. So we can not checkpoint {{AWOP}} until some buffer space will become available. So {{AWOP}} needs to yield to downstream ({{.yield()}}) until buffer space becomes available, return from the {{#processElement()}} call, and then it needs to relay on a fix for FLINK-35051 for stream task to prioritise performing a checkpoint over executing more records/mails. ;;;","11/Apr/24 10:40;gyfora;I agree with [~pnowojski] here, the currently blocked element would be lost in the checkpoint. But [~Zakelly] also has a valid point.

I have played around with this and there is a simple optimisation to be made for the async operator though under certain circumstances.

If the AWOP is the head of the operator chain (no upstream), we could actually checkpoint during yielding but we would also need to checkpoint the current processed element as part of the buffer (temporarily increase the size of the buffer by 1).

This is still related to the other ticket in the sense that we need to get the checkpoint trigger during yield but it needs a custom logic for the AWOP to allow checkpointing while being blocked on the full buffer;;;","11/Apr/24 10:43;gyfora;So restricting the optimisation to the head of the operator chain is somewhat restricting but still the improvement in this particular scenario is actually huge and this may make or break some specialised use-cases so probably still worth considering after FLINK-35051;;;","11/Apr/24 10:55;pnowojski;Sounds good to me (y) Indeed we could later provide some kind of overdraft buffer capacity to be used just for checkpointing. I think that this might relate to the things I want to propose in FLIP-443 as it will give the AWOP some way of knowing that it should use the overdraft buffer. Let's discuss this later and keep the ticket open :);;;","11/Apr/24 11:56;zakelly;Thanks for your detailed explanation! Now I see your point. The problematic of my approach is that current record of AWOP will be lost.

+1 for temporarily increase the size of the buffer by 1. IIUC even if the AWOP has an upstream, if current record could normally finish its process due to increment of buffer, the cp could proceed.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Correct Scala Example for ORC Bulker Writer's Vectorizer,FLINK-34703,13572150,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,coltenp,coltenp,17/Mar/24 14:24,18/Mar/24 21:47,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Connectors / FileSystem,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,"This correction to the docs actually affects every version of the FileSystems documentation that utilizes the ORC Format.

I have created a Pull Request to improve the docs for Scala users. As it stands now if someone were to pull the code out of the Scala docs and tried to use it, only the first row will ever be written. This is because the Scala example has a hard coded ""batch.row + 1"", but batch.row is never incremented.

 

https://github.com/apache/flink/pull/24509",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,English,2024-03-17 14:24:17.0,,,,,,,,,,"0|z1o0ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rank should not convert to StreamExecDuplicate when the input is not insert only,FLINK-34702,13572137,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,17/Mar/24 06:32,21/Mar/24 04:32,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"{code:java}
@Test
def testSimpleFirstRowOnBuiltinProctime1(): Unit = {
  val sqlQuery =
    """"""
      |SELECT *
      |FROM (
      |  SELECT *,
      |    ROW_NUMBER() OVER (PARTITION BY a ORDER BY PROCTIME() ASC) as rowNum
      |  FROM (select a, count(b) as b from MyTable group by a)
      |)
      |WHERE rowNum = 1
    """""".stripMargin

  util.verifyExecPlan(sqlQuery)
} {code}
Exception:

org.apache.flink.table.api.TableException: StreamPhysicalDeduplicate doesn't support consuming update changes which is produced by node GroupAggregate(groupBy=[a], select=[a, COUNT(b) AS b])

because the StreamPhysicalDeduplicate can not consuming update changes now while StreamExecRank can.

so we should not convert the FlinkLogicalRank to StreamPhysicalDeduplicate in this case. and we can defer whether input contains update change in the ""optimize the physical plan"" phase. 

so we can add an option to solve it. and when the StreamPhysicalDeduplicate can support consuming update changes , we can deprecate it",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 04:30:36 UTC 2024,,,,,,,,,,"0|z1o0hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 05:33;lincoln.86xy;[~jackylau] Thank you for reporting this! 
This should be an improvement but not a bug. Currently, some operators do not support processing changelog input, and `StreamPhysicalDeduplicate` is one of them.
For the solution, can we consider taking a step forward, allowing `StreamPhysicalDeduplicate` to support changelog input (let the physical operator support this input, just like we added support changelog input for Window TVF Aggregation in 1.19), instead of patching with a switch during the optimization phase to bypass it, WDYT?;;;","20/Mar/24 03:47;jackylau; we have three proposed solutions :

{*}Solution 1{*}: Add a switch parameter, enabled by default. When a compilation error occurs, the switch is turned off based on the error message. However, this is noticeable by the user.

{*}Solution 2{*}: Make the deduplication operator support changelog messages. Currently, the deduplication operator is divided into eight combinations: processing time / event time, first row / last row, mini-batch / non-mini-batch. Supporting changelog would effectively double these combinations to sixteen, significantly complicating the code logic. Furthermore, taking processing time's first row as an example, if changelog is supported, it would require recording all historical data, which could severely degrade performance. Additionally, the deduplication operator is essentially a special case of the Rank operator, but simply adapting the logic to use Rank isn't feasible either, as the Rank operator currently does not support mini-batch. This change would be imperceptible to users.

{*}Solution 3{*}: An improvement over Solution 1, since the logical to physical stage cannot detect whether there are changelog, the transformation could be done during the physical rewrite phase. This phase comes after the FlinkChangelogModeInferenceProgram.

 

After discussion with [~lincoln.86xy] offline, we agree with *Solution 3*;;;","21/Mar/24 04:30;jackylau;When adopting Solution 3 it was found to be unviable for two main reasons:
 # The rule cannot be placed within the physical rewrite phase. This approach is somewhat tricky, as noted in my poc. The reason is that the MiniBatchAssigner operator is whether the proctime / eventime  is related to the downstream operators. The current Rank operator does not require watermarks, whereas the Deduplicate operator for rowtime does. This can be observed in the return value of requireWatermark. See the plan change in the test {{{}testMiniBatchInferFirstRowOnRowtime{}}}.

 # The RelTraitSet produced by the Deduplicate operator differs from that of the Rank operator. Although we can modify the RelTrait of the deduplication operator using the code below, there is no way to change the RelTraits of other downstream operators belonging to the deduplication operator, unless we run the logic of the FlinkChangelogModeInferenceProgram again. The local-global two-phase rule can be dealt with within the physical rewrite phase because the RelTraitSet of the local aggregate is same with the upstream node. See the plan change in the test
testUpdatableRankWithDeduplicate
{code:java}
ModifyKindSetTrait modifyKindSetTrait;
UpdateKindTrait updateKindTrait;
if (!isLastRow && !isRowtime) {
    // only proctime first row deduplicate does not produce UPDATE changes
    modifyKindSetTrait = ModifyKindSetTrait.INSERT_ONLY();
    updateKindTrait = UpdateKindTrait.NONE();
} else {
    // other deduplicate produce update changes
    modifyKindSetTrait = ModifyKindSetTrait.ALL_CHANGES();
    updateKindTrait = rank.getTraitSet()
            .getTrait(UpdateKindTraitDef.INSTANCE());
}

RelTraitSet duplicateTraitSet = rank.getTraitSet()
        .replace(modifyKindSetTrait)
        .replace(updateKindTrait); {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish the Dockerfiles for the new release,FLINK-34701,13572111,13572107,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/Mar/24 00:53,21/Mar/24 01:00,04/Jun/24 20:40,21/Mar/24 01:00,,,,,,,,,,,,,,,,0,pull-request-available,,,"Note: the official Dockerfiles fetch the binary distribution of the target Flink version from an Apache mirror. After publishing the binary release artifacts, mirrors can take some hours to start serving the new artifacts, so you may want to wait to do this step until you are ready to continue with the ""Promote the release"" steps in the follow-up Jira.

Follow the [release instructions in the flink-docker repo|https://github.com/apache/flink-docker#release-workflow] to build the new Dockerfiles and send an updated manifest to Docker Hub so the new images are built and published.

 
----
h3. Expectations

 * Dockerfiles in [flink-docker|https://github.com/apache/flink-docker] updated for the new Flink release and pull request opened on the Docker official-images with an updated manifest",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 01:00:00 UTC 2024,,,,,,,,,,"0|z1o0c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/24 16:00;lincoln.86xy;Pull request opened on the Docker Library [{{official-images}}|https://github.com/docker-library/official-images] repo: https://github.com/docker-library/official-images/pull/16430

*DockerHub: apache/flink*  has been published  [https://hub.docker.com/r/apache/flink];;;","21/Mar/24 01:00;lincoln.86xy;merged in [{{official-images}}|https://github.com/docker-library/official-images] master: fe836e1e82ee92222a807d9e9825d98a9a47b8c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Git tag and mark version as released in Jira,FLINK-34700,13572110,13572107,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/Mar/24 00:53,16/Mar/24 00:59,04/Jun/24 20:40,16/Mar/24 00:59,,,,,,,,,,,,,,,,0,,,,"Create and push a new Git tag for the released version by copying the tag for the final release candidate, as follows:
{code:java}
$ git tag -s ""release-${RELEASE_VERSION}"" refs/tags/${TAG}^{} -m ""Release Flink ${RELEASE_VERSION}""
$ git push <remote> refs/tags/release-${RELEASE_VERSION}
{code}
In JIRA, inside [version management|https://issues.apache.org/jira/plugins/servlet/project-config/FLINK/versions], hover over the current release and a settings menu will appear. Click Release, and select today’s date.

(Note: Only PMC members have access to the project administration. If you do not have access, ask on the mailing list for assistance.)

If PRs have been merged to the release branch after the the last release candidate was tagged, make sure that the corresponding Jira tickets have the correct Fix Version set.

 
----
h3. Expectations
 * Release tagged in the source code repository
 * Release version finalized in JIRA. (Note: Not all committers have administrator access to JIRA. If you end up getting permissions errors ask on the mailing list for assistance)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 16 00:59:36 UTC 2024,,,,,,,,,,"0|z1o0bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/24 00:59;lincoln.86xy;https://github.com/apache/flink/releases/tag/release-1.19.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy artifacts to Maven Central Repository,FLINK-34699,13572109,13572107,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,16/Mar/24 00:53,16/Mar/24 00:58,04/Jun/24 20:40,16/Mar/24 00:58,,,,,,,,,,,,,,,,0,,,,"Use the [Apache Nexus repository|https://repository.apache.org/] to release the staged binary artifacts to the Maven Central repository. In the Staging Repositories section, find the relevant release candidate orgapacheflink-XXX entry and click Release. Drop all other release candidates that are not being released.
h3. Deploy source and binary releases to dist.apache.org

Copy the source and binary releases from the dev repository to the release repository at [dist.apache.org|http://dist.apache.org/] using Subversion.
{code:java}
$ svn move -m ""Release Flink ${RELEASE_VERSION}"" https://dist.apache.org/repos/dist/dev/flink/flink-${RELEASE_VERSION}-rc${RC_NUM} https://dist.apache.org/repos/dist/release/flink/flink-${RELEASE_VERSION}
{code}
(Note: Only PMC members have access to the release repository. If you do not have access, ask on the mailing list for assistance.)
h3. Remove old release candidates from [dist.apache.org|http://dist.apache.org/]

Remove the old release candidates from [https://dist.apache.org/repos/dist/dev/flink] using Subversion.
{code:java}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
$ cd flink
$ svn remove flink-${RELEASE_VERSION}-rc*
$ svn commit -m ""Remove old release candidates for Apache Flink ${RELEASE_VERSION}
{code}
 
----
h3. Expectations
 * Maven artifacts released and indexed in the [Maven Central Repository|https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.flink%22] (usually takes about a day to show up)
 * Source & binary distributions available in the release repository of [https://dist.apache.org/repos/dist/release/flink/]
 * Dev repository [https://dist.apache.org/repos/dist/dev/flink/] is empty
 * Website contains links to new release binaries and sources in download page
 * (for minor version updates) the front page references the correct new major release version and directs to the correct link",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 16 00:58:25 UTC 2024,,,,,,,,,,"0|z1o0bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/24 00:58;lincoln.86xy;release: [https://dist.apache.org/repos/dist/release/flink/flink-1.19.0/]

dev cleanup: https://dist.apache.org/repos/dist/dev/flink/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy Python artifacts to PyPI,FLINK-34698,13572108,13572107,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,16/Mar/24 00:53,16/Mar/24 00:56,04/Jun/24 20:40,16/Mar/24 00:56,,,,,,,,,,,,,,,,0,,,,"Release manager should create a PyPI account and ask the PMC add this account to pyflink collaborator list with Maintainer role (The PyPI admin account info can be found here. NOTE, only visible to PMC members) to deploy the Python artifacts to PyPI. The artifacts could be uploaded using twine([https://pypi.org/project/twine/]). To install twine, just run:
{code:java}
pip install --upgrade twine==1.12.0
{code}
Download the python artifacts from dist.apache.org and upload it to pypi.org:
{code:java}
svn checkout https://dist.apache.org/repos/dist/dev/flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
cd flink-${RELEASE_VERSION}-rc${RC_NUM}
 
cd python
 
#uploads wheels
for f in *.whl; do twine upload --repository-url https://upload.pypi.org/legacy/ $f $f.asc; done
 
#upload source packages
twine upload --repository-url https://upload.pypi.org/legacy/ apache-flink-libraries-${RELEASE_VERSION}.tar.gz apache-flink-libraries-${RELEASE_VERSION}.tar.gz.asc
 
twine upload --repository-url https://upload.pypi.org/legacy/ apache-flink-${RELEASE_VERSION}.tar.gz apache-flink-${RELEASE_VERSION}.tar.gz.asc
{code}
If upload failed or incorrect for some reason (e.g. network transmission problem), you need to delete the uploaded release package of the same version (if exists) and rename the artifact to \{{{}apache-flink-${RELEASE_VERSION}.post0.tar.gz{}}}, then re-upload.

(!) Note: re-uploading to pypi.org must be avoided as much as possible because it will cause some irreparable problems. If that happens, users cannot install the apache-flink package by explicitly specifying the package version, i.e. the following command ""pip install apache-flink==${RELEASE_VERSION}"" will fail. Instead they have to run ""pip install apache-flink"" or ""pip install apache-flink==${RELEASE_VERSION}.post0"" to install the apache-flink package.

 
----
h3. Expectations
 * Python artifacts released and indexed in the [PyPI|https://pypi.org/project/apache-flink/] Repository",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 16 00:56:05 UTC 2024,,,,,,,,,,"0|z1o0bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/24 00:56;lincoln.86xy;pypi:

[https://pypi.org/project/apache-flink/#files]

[https://pypi.org/project/apache-flink-libraries/#files]

 

Release Wiki Page Updates
 * 
 ** swith to use token for uploading python

!https://cwiki.apache.org/confluence/download/attachments/276105702/image-2024-3-15_20-54-28.png?version=1&modificationDate=1710507268804&api=v2|height=150!
 * 
 ** no need to upload signatures to PyPI !https://cwiki.apache.org/confluence/download/attachments/276105702/image-2024-3-15_20-55-54.png?version=1&modificationDate=1710507355000&api=v2|height=150!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Finalize release 1.19.0,FLINK-34697,13572107,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lincoln.86xy,lincoln,lincoln.86xy,16/Mar/24 00:53,16/Mar/24 00:53,04/Jun/24 20:40,,,,,,,,1.19.0,,,,,,,,,0,release,,,"Once the release candidate has been reviewed and approved by the community, the release should be finalized. This involves the final deployment of the release candidate to the release repositories, merging of the website changes, etc.",,,,,,,,,,,,,,,,,,,,,,FLINK-32920,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-16 00:53:15.0,,,,,,,,,,"0|z1o0b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GSRecoverableWriterCommitter is generating excessive data blobs,FLINK-34696,13572060,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sisp,sisp,15/Mar/24 14:03,26/Mar/24 14:26,04/Jun/24 20:40,,,,,,,,,,,,Connectors / FileSystem,,,,,1,,,,"The `composeBlobs` method in `org.apache.flink.fs.gs.writer.GSRecoverableWriterCommitter` is designed to merge multiple small blobs into a single large blob using Google Cloud Storage's compose method. This process is iterative, combining the result from the previous iteration with 31 new blobs until all blobs are merged. Upon completion of the composition, the method proceeds to remove the temporary blobs.

*Issue:*

This methodology results in significant, unnecessary data storage consumption during the blob composition process, incurring considerable costs due to Google Cloud Storage pricing models.

*Example to Illustrate the Problem:*
 - Initial state: 64 blobs, each 1 GB in size (totaling 64 GB).
 - After 1st step: 32 blobs are merged into a single blob, increasing total storage to 96 GB (64 original + 32 GB new).
 - After 2nd step: The newly created 32 GB blob is merged with 31 more blobs, raising the total to 159 GB.
 - After 3rd step: The final blob is merged, culminating in a total of 223 GB to combine the original 64 GB of data. This results in an overhead of 159 GB.

*Impact:*

This inefficiency has a profound impact, especially at scale, where terabytes of data can incur overheads in the petabyte range, leading to unexpectedly high costs. Additionally, we have observed an increase in storage exceptions thrown by the Google Storage library, potentially linked to this issue.

*Suggested Solution:*

To mitigate this problem, we propose modifying the `composeBlobs` method to immediately delete source blobs once they have been successfully combined. This change could significantly reduce data duplication and associated costs. However, the implications for data recovery and integrity need careful consideration to ensure that this optimization does not compromise the ability to recover data in case of a failure during the composition process.

*Steps to Reproduce:*

1. Initiate the blob composition process in an environment with a significant number of blobs (e.g., 64 blobs of 1 GB each).
2. Observe the temporary increase in data storage as blobs are iteratively combined.
3. Note the final amount of data storage used compared to the initial total size of the blobs.

*Expected Behavior:*

The blob composition process should minimize unnecessary data storage use, efficiently managing resources to combine blobs without generating excessive temporary data overhead.

*Actual Behavior:*

The current implementation results in significant temporary increases in data storage, leading to high costs and potential system instability due to frequent storage exceptions.
 
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 26 14:26:47 UTC 2024,,,,,,,,,,"0|z1o00o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 14:59;galenwarren;Yes, the issue is recoverability. However, there is one thing you can do. Create a separate bucket in GCP for temporary files and initialize the filesystem configuration (via FileSystem.initialize) with *_[gs.writer.temporary.bucket.name|http://gs.writer.temporary.bucket.name/]_* set to the name of that bucket. This will cause the GSRecoverableWriter to write intermediate/temporary files to that bucket instead of the ""real"" bucket. Then, you can apply a TTL[ lifecycle policy |https://cloud.google.com/storage/docs/lifecycle]to the temporary bucket to have files be deleted after whatever TTL you want. 
 
If you try to recover to a check/savepoint farther back in time than that TTL interval, the recovery will probably fail, but this will let you dial in whatever recoverability period you want, i.e. longer (at higher storage cost) or shorter (at lower storage cost).;;;","15/Mar/24 16:59;tshofer;Will the temporary bucket prevent the use of object compose? Letting the GSRecoverableWriterCommitter not recursively compose objects? To ask more precisely: will the use of a temporary bucket just double the amount of data that is being written (one time in temporary bucket, the other time in the final target)?

 ;;;","15/Mar/24 17:47;galenwarren;No, this just moves the storage of the temporary files to a different bucket so that a lifecycle policy can be applied to them. The intermediate blobs still have to be composed into the final blob either way. And yes there is one extra copy if you use the temp bucket, but the benefit is you won't orphan temporary files (more below).

Upon rereading and looking back at the code (it's been a while!), I may have misunderstood the initial issue. There could be a couple things going on here.

1) There could be orphaned temporary blobs sticking around, because they were written at one point and then due to restores from check/savepoints, they were ""forgotten"". Writing all intermediate files to a temporary bucket and then applying a lifecycle policy to that bucket would address that issue. It does come at the cost of one extra copy operation; in GCP blobs cannot be composed into a different bucket, so using the temporary bucket incurs one extra copy that doesn't occur if the temporary bucket is the same as the final bucket.

2) There could be large numbers of temporary blobs being composed together, i.e. the writing pattern is such that large numbers of writes occur between checkpoints. Since there's a limit to how many blobs can be composed together at one time (32), there's no way to avoid a temporary blob's data being involved in more than compose operation, in principle, but I do think the composition could be optimized differently.

I think #2 is the issue here, not #1? Though one should also be careful about orphaned blobs if state is ever restored from check/savepoints.

Regarding optimization: Currently, the composing of blobs occurs [here|https://github.com/apache/flink/blob/f6e1b493bd6292a87efd130a0e76af8bd750c1c9/flink-filesystems/flink-gs-fs-hadoop/src/main/java/org/apache/flink/fs/gs/writer/GSRecoverableWriterCommitter.java#L131].

It does it in a way that minimizes the number of compose operations but not necessarily the total volume of data involved in those compose operations, which I think is the issue here.

Consider the situation where there are 94 temporary blobs to be composed into a committed blob. As it stands now, the first 32 would be committed into 1 blob, leaving 1 + (94 - 32) = 63 blobs remaining. This process would be repeated, composing the first 32 against into 1, leaving 1 + (63 - 32) = 32 blobs. Then these remaining 32 blobs would be composed into 1 final blob. So, 3 total compose operations.

This could be done another way – the first 32 blobs could be composed into 1 blob, the second 32 blobs could be composed into 1 blob, and the third 30 blobs could be composed into 1 blob, resulting in three intermediate blobs that would then be composed into the final blob. So, 4 total compose operations in this case. Still potentially recursive with large numbers of blobs due to the 32-blob limit, but this would avoid the piling up of data in the first blob when there are large numbers of temporary blobs.

Even though the second method would generally involve more total compose operations than the first, it would minimize the total bytes being composed. I doubt the difference would be significant for small numbers of temporary blobs and could help with large numbers of temporary blobs. That would seem like a reasonable enhancement to me.;;;","15/Mar/24 19:11;galenwarren;One more thing, regarding:
{quote}To mitigate this problem, we propose modifying the `composeBlobs` method to immediately delete source blobs once they have been successfully combined. This change could significantly reduce data duplication and associated costs. 
{quote}
I'm not sure it would be safe to delete the raw temporary blobs (i.e. the uncomposed ones) until the commit succeeds, because they are referenced in the Recoverable object and would need to be there if a commit were retried. I suppose it would be fine to delete truly intermediate blobs along the way during the composition process, but these are deleted anyway at the end of the commit, so does that buy much? Perhaps it does with very large files.

 ;;;","18/Mar/24 09:00;sisp;Dear Galen,

Thank you for your detailed feedback. I have several suggestions and questions regarding the current implementation:

{*}Efficiency in Blob Composition{*}: Instead of creating temporary composite blobs, could we directly append all blobs to the final blob? This approach would reduce data duplication significantly, i.e. to only double the amount of data. Currently, the process involves creating multiple temporary blobs that in the final steps nearly match the size of the final blob, leading to an unnecessary increase in data size, especially when dealing with TiB-sized blobs.  In our case, e.g., we saw a more than 4000 times increase in the data amount. We see about  4.5 million small blobs being created, in the end multiple TiB sized temporary composite blobs. This means that although our data is on the size of TiB we need PiB of storage.

 
{code:java}
I.e. instead of
Step 1:  32 blobs = A
Step 2: A + 31 blobs = B
Step 3: B + 31 blobs = C
 
The code would do
Step 1: Select 1st seen blob as the “temporary final” = A
A + 31 blobs = A
Step 2: A + 31 blobs = A
Etc.
{code}
 

{*}Optimization of the Recovery Mechanism{*}: Is it possible to optimize the recovery mechanism to avoid data duplication? Ideally, once the minor blobs are merged into the final blob, we could unregister (and delete) them from the recovery state to save space.

{*}Handling the 5 TB Blob Limit{*}: The current code doesn't seem to account for the 5 TB blob limit. A simple solution could be to initiate an additional final blob when nearing this limit.

{*}Rationale Behind Blob Composition{*}: What is the primary reason for composing multiple small blobs into a single large blob? Is it to optimize the reading process later, or is there another benefit? If the goal is to end up with a large final blob, might it be more efficient to append data directly to it instead of creating multiple smaller blobs first?

*TTL and temporary bucket* I like the idea of using the time-to-live setting on a temporary bucket to ensure that temporary blobs are removed, e.g., if the job fails. But I would not up front have an easy way to determine a good TTL. That seems difficult to estimate unless you already ran the job and have generated statistics for how quickly it runs etc. Also it should be documented that e.g. the new soft delete feature of GCS means the temporary objects by default will be stored (and paid for) for 7 days (unless you switch off this feature), and also that if you have versioning on appending to a blob will generate multiple versions (and you will pay for all of these versions)

 

Looking forward to your thoughts on these points.;;;","18/Mar/24 19:20;galenwarren;Re: {*}Composition{*}: My understanding is that Flink recoverable writers are supposed to follow a few rules, one of which is that data should not be visible in the final destination until a successful commit occurs. From the description of the [RecoverableWriter|[https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/core/fs/RecoverableWriter.html]:|https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/core/fs/RecoverableWriter.html]
{quote}The RecoverableWriter creates and recovers [{{RecoverableFsDataOutputStream}}|https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/core/fs/RecoverableFsDataOutputStream.html]. It can be used to write data to a file system in a way that the writing can be resumed consistently after a failure and recovery without loss of data or possible duplication of bytes.

The streams do not make the files they write to immediately visible, but instead write to temp files or other temporary storage. To publish the data atomically in the end, the stream offers the [{{RecoverableFsDataOutputStream.closeForCommit()}}|https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/core/fs/RecoverableFsDataOutputStream.html#closeForCommit--] method to create a committer that publishes the result.
{quote}
This atomicity is required, afaik, so writing to the final blob along the way wouldn't really work.

(Though, as I mentioned above, I do think it would be possible to improve the the efficiency of the blob composition somewhat in terms of total data storage required in cases where there are lots of files to be composed, even when using intermediate files.)

Re: *Optimization of the Recovery Mechanism:* Yes, it would be possible to delete some of the intermediate composed blobs along the way, i.e. any that are not ""root"" temporary blobs. They are currently deleted at the end of the commit.  I suppose that doing it along the way would cause those blobs to live a _slightly_ shorter period of time and not overlap as much. So I understand, which is the issue? Blob storage costs are prorated pretty granularly so I wouldn't think that short window of time would affect costs that much, but I could be wrong about that. Are you running into cost issues, total storage limits, something else (?) during the current bursts?

Re: *Handling the 5 TB Blob Limit:* Yes, it would be possible to write two (or more) blobs to stay under this limit when composing. I'm not completely sure what that means in terms of the atomicity requirement, i.e. I don't think there's any way to do multiple atomic writes in GCP, so there would be a possibility that data could appear and then disappear, i.e. in the case that write of final blob #1 succeeded and then the write of final blob #2 failed and both were deleted because of the failed commit. But, not sure what other options exist. It would be interesting to know how the S3 recoverable writer handles similar limits.

Re: *Rationale Behind Blob Composition:* see above.

Re: *TTL and temporary bucket:* Could you start without a defined TTL on the bucket while gathering the necessary data about the app, and then turn it on later? Or, you can just leave it off entirely, in which case the temp files go into the main bucket, just in a different location.;;;","19/Mar/24 09:58;sisp;Dear Galen,

 

*Composition:* Regarding that final blob should not written before the end, can be solved using a staging blob.

e.g. like this:
{code:java}
GSBlobIdentifier stagingBlob = blobContainer.getOneBlob();
while (blobContainer.hasNext()) {
  List<GSBlobIdentifier> listOfBlobs = blobContainer.getNextBlock();
  listOfBlobs.add(0, stagingBlob);
  if (blobContainer.lastBlock()) {
    Storage.compose(listOfBlobs, finalBlob);
  } else {
    Storage.compose(listOfBlobs, stagingBlob);
  }
}{code}
This code avoids using intermediate composition blobs, and, therefore, only doubles the storage size.

 

*Recovery and deletion of blobs:* 

For the recovery and deletion of temporary blobs, it's critical to consider scenarios without checkpointing or in BATCH mode, where {{closeForCommit}} operates with the entire blob list. Our current implementation, handling roughly 4.5 million blobs, generates numerous temporary composition blobs. So no, we do not see ""bursts"" of storage usage, but relative long duration usage of large storage usage. To streamline this, we could modify {{GSCommitRecoverable}} to update the list of {{{}componentObjectIds{}}}, allowing the removal of blobs already appended to the {{{}stagingBlob{}}}. This adjustment would maintain data integrity in recovery situations without duplicating blobs.

*Exception handling:*

I notice an other issue with the code - because the storage.compose might throw a StorageException. With the current code this would mean the intermediate composition blobs are not cleaned up. 

*5 TB limit*

Regarding the 5TB limit, employing temporary staging blobs could offer a solution, with all staging blobs ultimately being committed to final blobs upon successful completion of the process. 

*TTL*

If the code above is implemented, I think there is no longer a need the TTL feature since all necessary blobs should be written to a final blob. Any leftover blobs post-job completion would indicate a failed state.

 ;;;","19/Mar/24 10:08;sisp;[~pnowojski] I noticed you changed the issue type to ""improvement"". In my opinion this is a ""Bug"" as code the unexpectedly writes huge amount of duplicated data is not behaving as intended. That would be my definition of a bug, not just an ""improvement"".. (of course fixing a bug is also an ""improvement"")..;;;","19/Mar/24 15:29;galenwarren;I think your proposed algorithm is just another way of implementing the ""delete intermediate blobs that are no longer necessary as soon as possible"" idea we've considered. You accomplish it by overwriting the staging blob on each iteration; a similar effect could be achieved by writing to a new intermediate staging blob on each iteration (as is done now) and deleting the old one right away (which is not done now. instead this deletion occurs shortly thereafter, after the commit succeeds).

Aside: I'm not sure whether it's possible to overwrite the existing staging blob like you suggest. The [docs |[Objects: compose  |  Cloud Storage  |  Google Cloud|https://cloud.google.com/storage/docs/json_api/v1/objects/compose]] for the compose operation say:
{quote}Concatenates a list of existing objects into a new object in the same bucket. The existing source objects are unaffected by this operation
{quote}
In your proposal, the same staging blob is both the target of the compose operation and one of the input blobs to be composed. That _might_ work, but would have to be tested to see if it's allowed. If it isn't, writing to a new blob and deleting the old one would have essentially the same effect. That's almost certainly what happens behind the scenes anyway, since blobs are immutable.

Bigger picture, if you're trying to combine millions of immutable blobs together in one step, 32 at a time, I don't see how you avoid having lots of intermediate composed blobs, one way or another, at least temporarily. The main question would be how quickly ones that are no longer needed are discarded.
{quote}To streamline this, we could modify {{GSCommitRecoverable}} to update the list of {{{}componentObjectIds{}}}, allowing the removal of blobs already appended to the {{stagingBlob}}
{quote}
Not quite following you here; the GSCommitRecoverable is provided as an input to the GSRecoverableWriterCommitter, providing the list of ""raw"" (i.e. uncomposed) temporary blobs that need to be composed and committed. If you removed those raw blobs from that list as they're added to the staging blob, what would that accomplish? The GSCommitRecoverable provided to the GSRecoverableWriterCommitter isn't persisted anywhere after the commit succeeds or fails, and if the commit were to be retried it would need the complete list anyway.
{quote}I notice an other issue with the code - because the storage.compose might throw a StorageException. With the current code this would mean the intermediate composition blobs are not cleaned up. 

If the code above is implemented, I think there is no longer a need the TTL feature since all necessary blobs should be written to a final blob. Any leftover blobs post-job completion would indicate a failed state.
{quote}
There is no real way to prevent the orphaning of intermediate blobs in all failure scenarios. Even in your proposed algorithm, the staging blob could be orphaned if the composition process failed partway through. This is what the temp bucket and TTL mechanism are for. It's optional, so you don't have to use it, but yes you would have to keep an eye out for orphaned intermediate blobs via some other mechanism f you choose not to use it.

Honestly, I think some of your difficulties are coming from trying to combine so much data together at once vs. doing it along the way, with commits at incremental checkpoints. I was going to suggest you reduce your checkpoint interval, to aggregate more frequently, but obviously that doesn't work if you're not checkpointing at all.

I do think the RecoverableWriter mechanism is designed to make writing predictable and repeatable in checkpoint/recovery situations (hence the name); if you're not doing any of that, you may run into some challenges. If you were to use checkpoints, you would aggregate more frequently, meaning fewer intermediate blobs with shorter lifetimes, and you'd be less likely to run up against the 5TB limit, as you would end up with a series of smaller files, written at each checkpoint, vs. one huge file.

If we do want to consider changes, here's what I suggest:
 * Test whether GCP allows composition to overwrite an existing blob that is also an input to the compose operation. If that works, then we could use that technique in the composeBlobs function. As mentioned above, I doubt that this actually reduces the number of blobs that exist temporarily – since blobs are immutable objects – but it would minimize their lifetime, effectively deleting them as soon as possible. If overwriting doesn't work, then we could achieve a similar effect by deleting intermediate blobs as soon as they are not needed anymore, rather than waiting until the commit succeeds.

 
 * Investigate how other RecoverableWriter implementations handle blob-size limitations. For example, S3 has the same 5TB limit. As we've discussed, there would be ways to write multiple files in such scenarios, but we'd have to give up a bit of atomicity that is assumed in the design. I'm not really sure whether that would be allowed or if the answer is ""commit frequently enough to keep the size of final blobs under 5TB."" Someone else from the Flink team would have to weigh in on that.

 
 * Consider a change to the composing algorithm that I'll describe below, which optimizes for total bytes written/composed vs. number of compose operations.

 

Possible alternate algorithm:

Consider the case where there are 125 raw temporary blobs to be aggregated, each of size 1GB. The current method (and your proposed method) goes like this:
 * Combine the first 32 blobs into 1 blob – 1 compose operation, 32GB written into a new blob
 * Combine that blob with the next 31 blobs – 1 compose operation, 31GB + 32GB = 63GB written into a new blob
 * Combine that blob with the next 31 blobs - 1 compose operation, 31GB + 63GB = 94GB written into a new blob
 * Combine that blob with the next 31 blobs – 1 compose operation, 31GB + 94GB = 125GB written into a new blob

So, a total of 4 compose operations and 32GB + 63GB + 94GB  + 125GB = 314GB written/composed.

Alternately, we could do this:
 * Combine the first 32 blobs into 1 intermediate blob – 1 compose operation, 32GB written into a new blob
 * Combine the second 32 blobs into 1 intermediate blob – 1 compose operation 32GB written into a new blob
 * Combine the third 32 blob into 1 intermediate blob – 1 compose operation, 32GB written into a new blob
 * Combine the last 29 blobs into 1 intermediate blob – 1 compose operation, 29GB written into a new blob
 * Combine the 4 intermediate blobs into one final blob – 1 compose operation, 32GB + 32GB + 32GB + 29GB = 125GB written into a new blob

This method has 5 compose operations – one more than the existing way – but only writes a total of 32GB + 32GB + 32GB + 29GB + 125GB = 250GB. So, more compose operations but less intermediate data and total data written. 

EDIT: Assuming I did my math right: If N is the total number of raw temporary blobs to aggregate, and they're all the same size, the total volume of data written in the first algorithm (existing) is approximately proportional to N^2 as compared to N * log N for the second algorithm. My guess is that change would have the biggest impact in high-volume scenarios. Overwriting the same staging blob would be moot in this case. We could still delete intermediate blobs along the way, just not the raw temporary blobs, which have to be retained until the commit succeeds.

 ;;;","20/Mar/24 16:15;sisp;I checked using the CLI

gcloud storage objects compose blobA blobB blobA

that worked. But yes indeed the documentation is a bit unclear. How the actual compose works on the GCS side is a good question. 

 

If I understand correctly - this committer is called regardless of the execution mode: ie. BATCH / STREAMING? In BATCH mode checkpointing is not supported, so the code should not depend on frequent checkpointing to work efficiently. 

 

The code above also avoids inserting the intermediate compose blob identifier at index 0. That looks expensive if the list is very big (and as far as I see not a LinkedList);;;","20/Mar/24 16:49;galenwarren;I'm not aware of any special considerations in RecoverableWriter for batch mode, but to be honest I'm not very knowledgeable about batch mode in general. Maybe someone else knows more about that ...

 
{quote}The code above also avoids inserting the intermediate compose blob identifier at index 0. That looks expensive if the list is very big (and as far as I see not a LinkedList)
{quote}
 

I don't think it's inserting anything, really, as the blobs are immutable. I think that just controls the order of the bytes in the newly created blob.;;;","26/Mar/24 09:09;sisp;Hi Galen,

How do we proceed with this issue?

Should I prepare a pull-request following the code example above, or do we need to explore more?

 

 ;;;","26/Mar/24 14:26;galenwarren;I'm not a committer so I can't approve anything on my own, but if we nail down the proposal I can loop in the person who helped me with the original implementation to see how to proceed.

To make sure we're on the same page, are we talking about implementing the more efficient composing algorithm (NlogN vs N^2)? I would suggest making it an opt-in to the new algorithm.;;;",,,,,,,,,,,,,,,,,,,,
Move Flink's CI docker container into a public repo,FLINK-34695,13572053,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,15/Mar/24 13:02,15/Mar/24 13:30,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,Build System / CI,,,,,0,,,,"Currently, Flink's CI (GitHub Actions and Azure Pipelines) use a container to run the logic. The intention behind it is to have a way to mimick the CI setup locally as well.

The current Docker image is maintained from the [zentol/flink-ci-docker|https://github.com/zentol/flink-ci-docker] fork (owned by [~chesnay]) of [flink-ci/flink-ci-docker|https://github.com/flink-ci/flink-ci-docker] (owned by Ververica) which is not ideal. We should move this repo into a Apache-owned repository.

Additionally, the there's no workflow pushing the image automatically to a registry from where it can be used. Instead, the images were pushed to personal Docker Hub repos in the past (rmetzger, chesnay, mapohl). This is also not ideal. We should use a public repo using a GHA workflow to push the image to that repo.

Questions to answer here:
# Where shall the Docker image code be located?
# Which Docker registry should be used?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34194,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 15 13:14:14 UTC 2024,,,,,,,,,,"0|z1nzz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 13:13;mapohl;{quote}
Where shall the Docker image code be located?
{quote}

* A docker subfolder in [apache/flink:.github/|https://github.com/apache/flink/tree/master/.github] since that's the closes it can be to the actual CI. Right now, there is some redundancy with [apache/flink-connector-shared-utils|https://github.com/apache/flink-connector-shared-utils/blob/ci_utils/docker/base/Dockerfile] that is used for connector CI. Having a dedicated CI Docker image in {{apache/flink}} would not resolve this redundancy. The question is just: Do we want to resolve it?
* Use [apache/flink-connector-shared-utils|https://github.com/apache/flink-connector-shared-utils/blob/ci_utils/docker/base/Dockerfile] as the Docker image not only for the connector repos but also the Flink main repo. This would remove the redundancy.

{quote}
Which Docker registry should be used?
{quote}
* We already use ghcr.io for the Flink nightly builds (see snapshot workflow config in [apache/flink-docker:.github/workflows/snapshot.yml:24|https://github.com/apache/flink-docker/blob/master/.github/workflows/snapshot.yml#L24]. That could be used for {{flink-docker-ci}} as well.;;;","15/Mar/24 13:14;mapohl;This should be discussed in the dev ML before going ahead with the change, I guess. ...if someone wants to pick this up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete num of associations for streaming outer join,FLINK-34694,13572050,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rovboyko,rovboyko,15/Mar/24 12:54,22/May/24 09:53,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,"Currently in StreamingJoinOperator (non-window) in case of OUTER JOIN the OuterJoinRecordStateView is used to store additional field - the number of associations for every record. This leads to store additional Tuple2 and Integer data for every record in outer state.

This functionality is used only for sending:
 * -D[nullPaddingRecord] in case of first Accumulate record
 * +I[nullPaddingRecord] in case of last Revoke record

The overhead of storing additional data and updating the counter for associations can be avoided by checking the input state for these events.

 

The proposed solution can be found here - [https://github.com/rovboyko/flink/commit/1ca2f5bdfc2d44b99d180abb6a4dda123e49d423]

 

According to the nexmark q20 test (changed to OUTER JOIN) it could increase the performance up to 20%:
 * Before:

!image-2024-03-15-19-52-24-391.png!
 * After:

!image-2024-03-15-19-51-29-282.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/24 12:51;rovboyko;image-2024-03-15-19-51-29-282.png;https://issues.apache.org/jira/secure/attachment/13067468/image-2024-03-15-19-51-29-282.png","15/Mar/24 12:52;rovboyko;image-2024-03-15-19-52-24-391.png;https://issues.apache.org/jira/secure/attachment/13067467/image-2024-03-15-19-52-24-391.png","15/Apr/24 08:45;rovboyko;image-2024-04-15-15-45-51-027.png;https://issues.apache.org/jira/secure/attachment/13068194/image-2024-04-15-15-45-51-027.png","15/Apr/24 08:46;rovboyko;image-2024-04-15-15-46-17-671.png;https://issues.apache.org/jira/secure/attachment/13068195/image-2024-04-15-15-46-17-671.png","15/Apr/24 12:14;rovboyko;image-2024-04-15-19-14-14-735.png;https://issues.apache.org/jira/secure/attachment/13068198/image-2024-04-15-19-14-14-735.png","15/Apr/24 12:14;rovboyko;image-2024-04-15-19-14-41-909.png;https://issues.apache.org/jira/secure/attachment/13068200/image-2024-04-15-19-14-41-909.png","15/Apr/24 12:15;rovboyko;image-2024-04-15-19-15-23-010.png;https://issues.apache.org/jira/secure/attachment/13068199/image-2024-04-15-19-15-23-010.png","26/Apr/24 09:55;rovboyko;image-2024-04-26-16-55-19-800.png;https://issues.apache.org/jira/secure/attachment/13068469/image-2024-04-26-16-55-19-800.png","26/Apr/24 09:55;rovboyko;image-2024-04-26-16-55-56-994.png;https://issues.apache.org/jira/secure/attachment/13068468/image-2024-04-26-16-55-56-994.png",,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 09:53:10 UTC 2024,,,,,,,,,,"0|z1nzyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/24 07:21;rovboyko;After failed CI I've realized the state is changed for outer side states and can't be restored from previously made checkpoints. Thanks to RestoreTest functionality for checking it=)

So now I have two options here: either use the old state schema for outer side (it would still have the same performance) or increase the version of ExecNodeMetadata for StreamExecJoin node.;;;","22/Mar/24 07:40;rovboyko;[~libenchao] What do you think about feature and performance results? Is it worth to do?;;;","10/Apr/24 05:45;rovboyko;[~martijnvisser] , could you please help with assigning this task to me or getting any feedback? From my point of view 20% performance boost - is a good result especially when it has no drawbacks. Or I missed something?;;;","10/Apr/24 08:29;xu_shuai_;Hi [~rovboyko] , your idea looks interesting. Actually I found that this optimization does not  reduce the overhead of state access after reading your code rather reduces the state to some extent . IMO, the marginal reduction in size may not significantly impact the overhead of storage, given that it constitutes a small fraction relative to the records held in the state.

BTW, if you plan to pursue this optimization further, could you provide more comprehensive benchmark details? The benchmark results of multiple tests and overall performance of all queries are convincing.;;;","10/Apr/24 08:51;rovboyko;Hi [~xu_shuai_] ! Thank you for your reply!

Actually this optimization reduces not only state size but the state access too: the method `updateNumOfAssociations()` is not invoked anymore. And the main performance improvement is gained mostly because of reducing state access operations.

According to benchmarks - ok, I will measure all the queries of nexmark which have JOIN keyword inside and prepare the comparison table. Would it be ok?;;;","10/Apr/24 09:42;xu_shuai_;Hi [~rovboyko],

The method `otherRecordHasNoAssociationsInInputSide` in your code would be invoked for every associatedRecord. This indeed increases the overhead of state access. It is difficult to say which one has a greater proportion between the increased costs and the reduced expenses of the method 'updateNumOfAssociations()' you mentioned. Intuitively, this may depend on the data distribution itself.

So a detailed test report could better illustrate the problem. And a comparison table that covers JOIN keyword in queries of nexmark is good. Besides, rewrite sql for hitting this optimization can also indicate the scenarios in which this optimization takes effect.;;;","15/Apr/24 11:58;rovboyko;Hi [~xu_shuai_] !

I prepared and executed all nexmark which uses streaming join (q4, q9 and q20). Because all of them use INNER JOIN (but this optimization works only for outer join) I created the copy with FULL OUTER JOIN for every one.

BEFORE optimization:

!image-2024-04-26-16-55-19-800.png!

AFTER optimization:

!image-2024-04-26-16-55-56-994.png!

As you can see here - for all INNER JOIN queries the result remains almost the same (small difference most probably cause the measurement error). But for all FULL OUTER JOIN benchmarks the performance is increased. Especially for q20_outer where it was more than 3 times better. The reason of such huge difference can be found on flame graph:

BEFORE optimization:

!image-2024-04-15-19-15-23-010.png!

 

AFTER optimization:

!image-2024-04-15-19-14-41-909.png!

 

Because of prevalence of state.update operation in before-optimization case the rocksdb CompactionJob is invoked more often spending the most CPU time.

Totally the performance boost is 6.75 / 5.15 = 1.31 (31%).;;;","27/Apr/24 08:52;rovboyko;Hi [~xu_shuai_] !

Could you please take a look on benchmark results above?;;;","14/May/24 02:42;rovboyko;[~xu_shuai_] , please take a look.;;;","22/May/24 09:53;rovboyko;Hi [~xu_shuai_] !

Could you please take a look on benchmark results above?;;;",,,,,,,,,,,,,,,,,,,,,,,
Memory leak in KafkaWriter,FLINK-34693,13572018,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,srichter,srichter,15/Mar/24 09:31,21/Mar/24 02:28,04/Jun/24 20:40,,kafka-3.0.2,kafka-3.1.0,,,,,,,,,Connectors / Kafka,,,,,0,,,,"KafkaWriter is keeping objects in Dequeue of closeables ({{{}producerCloseables{}}}) that are never removed so that the can be GC’ed.

From heap dump:!04599375-f923-4d1a-8d68-9e17e54b363c#media-blob-url=true&id=9d1e022e-8762-45b3-877b-d298ec956078&collection=&contextId=870337&height=306&width=2284&alt=!  !image-2024-03-15-10-30-08-280.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/24 09:30;srichter;image-2024-03-15-10-30-08-280.png;https://issues.apache.org/jira/secure/attachment/13067464/image-2024-03-15-10-30-08-280.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 02:28:22 UTC 2024,,,,,,,,,,"0|z1nzrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 13:03;leonard;[~renqs] Would you like look into this issue?;;;","15/Mar/24 13:05;leonard;[~srichter] Thanks for your report, but the Affects version should Kafka connector version instead of Flink’s version as Kafka connector has been move from Flink main repo and release separately. ;;;","21/Mar/24 02:28;renqs;I suspect it's the same one as FLINK-29492. [~srichter] Was KafkaCommitter chained together with KafkaWriter in your test?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Flink website to point to the new Flink CDC “Get Started” page ,FLINK-34692,13572005,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,ruanhang1993,renqs,renqs,15/Mar/24 07:35,22/Mar/24 06:34,04/Jun/24 20:40,22/Mar/24 06:34,,,,,,,,,,,Flink CDC,Project Website,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 06:34:07 UTC 2024,,,,,,,,,,"0|z1nzo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/24 06:34;leonard;flink-cdc master: b92bed50448ee2b106d6a04076f009ed7a0bbe0e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
failed to convert time type when using mysql to doris pipeline.,FLINK-34691,13571992,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,kunni,kunni,15/Mar/24 06:08,15/Mar/24 07:13,04/Jun/24 20:40,15/Mar/24 06:14,,,,,,,cdc-3.1.0,pre-apache,,,Flink CDC,,,,,0,github-import,,,"currently, we will get `reason: Flink doesn't support converting type TIME(0) to Doris type yet.` error.

---------------- Imported from GitHub ----------------
Url: https://github.com/lvyanquan/flink-cdc/issues/1
Created by: [lvyanquan|https://github.com/lvyanquan]
Labels: 
Created at: Fri Mar 15 12:54:09 CST 2024
State: open
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-15 06:08:09.0,,,,,,,,,,"0|z1nzl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If data from upstream is decimal and primary key , starrocks sink will not support.",FLINK-34690,13571989,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,loserwang1024,loserwang1024,loserwang1024,15/Mar/24 04:08,11/Apr/24 02:58,04/Jun/24 20:40,11/Apr/24 02:58,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"h3. What's the problem

Flink only have bigint type, if mysql data is unsigned bigint , maybe out of range, so cast to DECIMAL(20, 0).

If starrocks supports unsigned bigint, maybe can transform DECIMAL(20, 0) to unsigned bigint in sink. To be honest, unsigned bigint is not in SQL standard, thus many databases maybe not support it. [https://docs.starrocks.io/zh/docs/sql-reference/sql-statements/data-types/data-type-list/]

And starrock not support DECIMAL as primary key, thus should be transformed to String. I think we can fix it.

 
h3. How to solve

StarRocks is not support Decimal as primary key, so decimal should be cast to VARHCAR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 11 02:58:28 UTC 2024,,,,,,,,,,"0|z1nzkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 07:21;loserwang1024;[~renqs] ,[~Leonard] , [~Lv Yanquan] , [~banmoy] , CC;;;","11/Apr/24 02:58;renqs;flink-cdc master: abb98ee257527079a013462de0893bb9e2276693;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
check binlog_row_value_optoins,FLINK-34689,13571987,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,seung-min,seung-min,seung-min,15/Mar/24 03:57,26/Apr/24 05:24,04/Jun/24 20:40,12/Apr/24 02:14,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"When {{binlog_row_value_optoins}} is set to {{{}PARTIAL_JSON{}}},
the update operator remains as {{{}Update_rows_partial{}}}.

Flink CDC does not parse this event because {{Update_row_partial}} binlog event is mapped to {{PARTIAL_UPDATE_ROWS_EVENT}} and Flink CDC do not handle that event type

 

Example of Update_row_partial (when {{binlog_row_value_optoins}} = {{PARTIAL_JSON)}}

!image-2024-03-15-12-56-49-344.png|width=1015,height=30!

So, we have to check {{binlog_row_value_optoins}} before starting.

 

 

Cretae PR: [[MySQL][Feature] check binlog_row_value_optoins by SML0127 · Pull Request #3148 · apache/flink-cdc (github.com)|https://github.com/apache/flink-cdc/pull/3148]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/24 03:56;seung-min;image-2024-03-15-12-56-49-344.png;https://issues.apache.org/jira/secure/attachment/13067461/image-2024-03-15-12-56-49-344.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 12 02:13:56 UTC 2024,,,,,,,,,,"0|z1nzk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 04:34;seung-min;Assign this pr to me please;;;","12/Apr/24 02:13;renqs;flink-cdc master: af7665d33834b4141f875862df59ec1f56dddcbb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC framework split snapshot chunks asynchronously,FLINK-34688,13571986,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,loserwang1024,loserwang1024,15/Mar/24 03:20,25/May/24 08:45,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,Flink CDC,,,,,0,,,,"In Mysql CDC,  MysqlSnapshotSplitAssigner splits snapshot chunks asynchronously([https://github.com/apache/flink-cdc/pull/931).] But CDC framework lacks it.

If table is too big to split, the enumerator will be stuck, and checkpoint will be influenced( sometime will checkpoint timeout occurs).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 25 08:45:04 UTC 2024,,,,,,,,,,"0|z1nzjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 03:21;loserwang1024;Please assign it to me.;;;","25/May/24 08:45;ouyangwuli;When the size of the data is relatively large, this feature is really needed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Home Page of Flink CDC Documentation,FLINK-34687,13571985,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,leonard,renqs,renqs,15/Mar/24 03:18,18/Mar/24 06:47,04/Jun/24 20:40,18/Mar/24 06:47,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 06:47:54 UTC 2024,,,,,,,,,,"0|z1nzjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 06:47;leonard;Resolved in flink-cdc(master) via : aa9e91ce4599d8c5a51df82cea4e31aed7d2f634;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Deployment - YARN"" Page for Flink CDC Documentation",FLINK-34686,13571984,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,ShawnHx,renqs,renqs,15/Mar/24 03:10,19/Mar/24 08:18,04/Jun/24 20:40,19/Mar/24 08:18,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 08:17:55 UTC 2024,,,,,,,,,,"0|z1nzj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 08:31;ShawnHx;I'm willing to take this.;;;","19/Mar/24 08:17;renqs;flink-cdc master: 32c8e0bc2a2a8b3f6ea73d2f86becbec8f7522a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Deployment - Kubernetes"" Page for Flink CDC Documentation",FLINK-34685,13571983,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,kunni,renqs,renqs,15/Mar/24 03:10,19/Mar/24 04:23,04/Jun/24 20:40,19/Mar/24 04:23,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 04:23:10 UTC 2024,,,,,,,,,,"0|z1nziw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 08:27;loserwang1024;I am willing to take this.;;;","17/Mar/24 09:47;loserwang1024;It seem this should wait for [https://github.com/apache/flink-cdc/pull/3093] be merged.;;;","19/Mar/24 02:17;kunni;I am willing to take this.;;;","19/Mar/24 03:19;leonard;Hi, [~loserwang1024] As [~kunni] has opened the PR and I assign this ticket to him, thanks for the taking and it will be grateful if you'd like to help review this PR ;;;","19/Mar/24 04:23;renqs;flink-cdc master: 1fd0cfec8970cf3216c06155152dfc9d308eb669;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Developer Guide - Licenses"" Page for Flink CDC Documentation",FLINK-34684,13571982,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,ruanhang1993,renqs,renqs,15/Mar/24 03:04,18/Mar/24 09:50,04/Jun/24 20:40,18/Mar/24 09:50,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 09:50:04 UTC 2024,,,,,,,,,,"0|z1nzio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 03:16;ruanhang1993;I would like to help on this issue. Please assign this to me. Thanks~ [~renqs] ;;;","18/Mar/24 09:50;leonard;Resolved in flink-cdc(master): 776dcbab3f570123e4040f08bd70a1240cf84c17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Developer Guide - Contribute to Flink CDC"" Page for Flink CDC Documentation",FLINK-34683,13571981,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,ruanhang1993,renqs,renqs,15/Mar/24 03:03,18/Mar/24 09:50,04/Jun/24 20:40,18/Mar/24 09:50,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 09:50:53 UTC 2024,,,,,,,,,,"0|z1nzig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 06:27;ruanhang1993;I would like to help on this issue. Please assign this to me. Thanks~ [~renqs] ;;;","18/Mar/24 09:50;leonard;Resolved in flink-cdc(master): 96909eae5df032b5726a8c83f1c919b34945f54d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Developer Guide - Understanding Flink CDC API"" Page for Flink CDC Documentation",FLINK-34682,13571980,13571975,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,renqs,renqs,renqs,15/Mar/24 03:02,18/Mar/24 11:04,04/Jun/24 20:40,18/Mar/24 11:04,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 11:04:15 UTC 2024,,,,,,,,,,"0|z1nzi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 11:04;leonard;Resolved in flink-cdc(master): 4d3da2649b1d8ec7aa81d998fab3d22b95cbec0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Deployment - Standalone"" Page for Flink CDC Documentation",FLINK-34681,13571979,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,kunni,renqs,renqs,15/Mar/24 03:00,19/Mar/24 04:26,04/Jun/24 20:40,19/Mar/24 04:26,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 04:25:52 UTC 2024,,,,,,,,,,"0|z1nzjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 07:49;kunni;I am willing to take this.;;;","19/Mar/24 04:25;renqs;flink-cdc master: e93cc833f94b1a7645511a907855d5e129539cb3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Connectors - Overview"" Page for  Flink CDC Documentation",FLINK-34680,13571978,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,renqs,renqs,renqs,15/Mar/24 03:00,18/Mar/24 11:07,04/Jun/24 20:40,18/Mar/24 11:07,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 11:07:12 UTC 2024,,,,,,,,,,"0|z1nzhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 11:07;leonard;Resolved in flink-cdc(master): 8fd28f1788f35ef6a8a2560811520170ff70a10f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Core Concept"" Pages for Flink CDC Documentation",FLINK-34679,13571977,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,kunni,renqs,renqs,15/Mar/24 02:59,18/Mar/24 11:47,04/Jun/24 20:40,18/Mar/24 11:46,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 11:46:18 UTC 2024,,,,,,,,,,"0|hzzvcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 03:20;kunni;I'm willing to take this.;;;","18/Mar/24 11:46;leonard;Resolved in flink-cdc(master): e73f3adaa365c65882fa0863385e90612d44b278;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Introduction"" Page for Flink CDC Documentation",FLINK-34678,13571976,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,renqs,renqs,renqs,15/Mar/24 02:58,18/Mar/24 09:51,04/Jun/24 20:40,18/Mar/24 09:51,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 09:51:52 UTC 2024,,,,,,,,,,"0|z1nzhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 09:51;leonard;Resolved in flink-cdc(master): bd760b10d729f17755edc1297b6412c1d2d59aa6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor the structure of documentation for Flink CDC,FLINK-34677,13571975,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,15/Mar/24 02:57,25/Mar/24 08:02,04/Jun/24 20:40,25/Mar/24 08:02,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,"The documentation structure of Flink CDC is not quite in good shape currently. We plan to refactor it as below (✅ for existed pages and 🖊️  for new pages to write):
 * Get Started
 ** 🖊️ Introduction
 ** ✅ Quickstart
 * Core Concept
 ** 🖊️ (Pages for data pipeline / sources / sinks / table ID / transform / route)
 * Connectors
 ** 🖊️ Overview
 ** ✅ (Pages for connectors)
 ** ✅ Legacy Flink CDC Sources (For CDC sources before 3.0)
 * Deployment
 ** 🖊️ Standalone
 ** 🖊️ Kubernetes
 ** 🖊️ YARN
 * Developer Guide
 ** 🖊️ Understand Flink CDC API
 ** 🖊️ Contribute to Flink CDC
 ** 🖊️ Licenses
 * FAQ
 ** ✅ Frequently Asked Questions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 07:53:27 UTC 2024,,,,,,,,,,"0|z1nzh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 03:26;gongzhongqiang;[~renqs] Should add sub task to change `With Flink CDC`'s link  in `Get Started` on [https://flink.apache.org/]  should be changed. ;;;","15/Mar/24 07:37;renqs;[~gongzhongqiang] Thanks for the reminder! Added just now.;;;","19/Mar/24 07:10;gongzhongqiang;[~renqs] [~leonard]  About ci of document , I want to add dead-link check ci for pr ,and also will be added to doc build ci for pre-check.;;;","19/Mar/24 07:53;leonard;[~gongzhongqiang] Good idea, +1 for this proposal;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate ConvertToNotInOrInRule,FLINK-34676,13571966,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,14/Mar/24 23:03,14/Mar/24 23:15,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-14 23:03:58.0,,,,,,,,,,"0|z1nzf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate AggregateReduceGroupingRule,FLINK-34675,13571964,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,14/Mar/24 22:45,14/Mar/24 22:54,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-14 22:45:12.0,,,,,,,,,,"0|z1nzeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate CalcSnapshotTransposeRule,FLINK-34674,13571955,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,14/Mar/24 21:58,14/Mar/24 22:08,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-14 21:58:47.0,,,,,,,,,,"0|z1nzco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionRelatedITCase#testTouchSession failure on GitHub Actions,FLINK-34673,13571899,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rskraba,rskraba,14/Mar/24 14:22,22/May/24 11:38,04/Jun/24 20:40,,1.19.0,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,starter,test-stability,"[https://github.com/apache/flink/actions/runs/8258416388/job/22590907051#step:10:12155]
{code:java}
 Error: 03:08:21 03:08:21.304 [ERROR] org.apache.flink.table.gateway.rest.SessionRelatedITCase.testTouchSession -- Time elapsed: 0.015 s <<< FAILURE!
Mar 13 03:08:21 java.lang.AssertionError: 
Mar 13 03:08:21 
Mar 13 03:08:21 Expecting actual:
Mar 13 03:08:21   1710299301198L
Mar 13 03:08:21 to be greater than:
Mar 13 03:08:21   1710299301198L
Mar 13 03:08:21 
Mar 13 03:08:21     at org.apache.flink.table.gateway.rest.SessionRelatedITCase.testTouchSession(SessionRelatedITCase.java:175)
Mar 13 03:08:21     at java.base/java.lang.reflect.Method.invoke(Method.java:580)
Mar 13 03:08:21     at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Mar 13 03:08:21     at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
Mar 13 03:08:21     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
Mar 13 03:08:21     at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
Mar 13 03:08:21     at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
Mar 13 03:08:21     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 11:38:20 UTC 2024,,,,,,,,,,"0|z1nz08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 16:50;rskraba;While I normally dislike putting ""Thread.sleep(...)"" in a test, it would probably be the most appropriate fix for this and ensure that the two requests don't happen in the same millisecond.;;;","21/May/24 06:28;juanvi;[~rskraba] I tried to reproduce this without success:

 
{code:java}
./mvnw -f ./flink-table/flink-sql-gateway/pom.xml -Dtest=SessionRelatedITCase#testTouchSession test [INFO] Scanning for projects...
[INFO] 
[INFO] -----------------< org.apache.flink:flink-sql-gateway >-----------------
[INFO] Building Flink : Table : SQL Gateway 1.19-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ flink-sql-gateway ---
[WARNING] Old version of checkstyle detected. Consider updating to >= v8.30
[WARNING] For more information see: https://maven.apache.org/plugins/maven-checkstyle-plugin/examples/upgrading-checkstyle.html
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- spotless-maven-plugin:2.27.1:check (spotless-check) @ flink-sql-gateway ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.1.0:enforce (enforce-maven-version) @ flink-sql-gateway ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.1.0:enforce (enforce-maven) @ flink-sql-gateway ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.1.0:enforce (ban-unsafe-snakeyaml) @ flink-sql-gateway ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.1.0:enforce (ban-unsafe-jackson) @ flink-sql-gateway ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.1.0:enforce (forbid-log4j-1) @ flink-sql-gateway ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.1.0:enforce (forbid-direct-akka-rpc-dependencies) @ flink-sql-gateway ---
[INFO] 
[INFO] --- maven-enforcer-plugin:3.1.0:enforce (forbid-direct-table-planner-dependencies) @ flink-sql-gateway ---
[INFO] 
[INFO] --- directory-maven-plugin:0.1:directory-of (directories) @ flink-sql-gateway ---
[INFO] Directory of org.apache.flink:flink-parent set to: /Users/juanvicentes/projects/forks/flink
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (process-resource-bundles) @ flink-sql-gateway ---
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ flink-sql-gateway ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ flink-sql-gateway ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ flink-sql-gateway ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 14 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ flink-sql-gateway ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.2.2:test (default-test) @ flink-sql-gateway ---
[INFO] Using auto detected provider org.apache.maven.surefire.junitplatform.JUnitPlatformProvider
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.flink.table.gateway.rest.SessionRelatedITCase
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.359 s -- in org.apache.flink.table.gateway.rest.SessionRelatedITCase
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  5.891 s
[INFO] Finished at: 2024-05-21T08:23:16+02:00
[INFO] ------------------------------------------------------------------------ {code}
Any ideas?

 ;;;","21/May/24 09:08;rskraba;Hello!  This is an easy one to reproduce, by launching it in IntelliJ (for example) and setting the run configuration to Repeat until failure.  It normally only takes about 100 or so runs to demonstrate the flaky test.

In this case, the motivation is to reduce the (vast) number of flaky tests so that we can trust that CI results are reliably real failures.  The test is faulty, not any code that would find itself in production!;;;","22/May/24 10:07;juanvi;Thanks [~rskraba] ! I wasn't able to reproduce it but created [https://github.com/apache/flink/pull/24826]

Let me know what you think;;;","22/May/24 11:38;juanvi;Ended up just adding before the second request:

 
{code:java}
Thread.sleep(1); // ensure requests are not sent at the same time {code}
 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
HA deadlock between JobMasterServiceLeadershipRunner and DefaultLeaderElectionService,FLINK-34672,13571898,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,chesnay,chesnay,14/Mar/24 14:19,03/Jun/24 07:40,04/Jun/24 20:40,,1.17.2,1.18.1,1.19.0,1.20.0,,,1.18.2,1.19.2,1.20.0,,Runtime / Coordination,,,,,0,pull-request-available,,,"We recently observed a deadlock in the JM within the HA system.
(see below for the thread dump)

[~mapohl] and I looked a bit into it and there appears to be a race condition when leadership is revoked while a JobMaster is being started.
It appears to be caused by {{JobMasterServiceLeadershipRunner#createNewJobMasterServiceProcess}} forwarding futures while holding a lock; depending on whether the forwarded future is already complete the next stage may or may not run while holding that same lock.
We haven't determined yet whether we should be holding that lock or not.

{code}
""DefaultLeaderElectionService-leadershipOperationExecutor-thread-1"" #131 daemon prio=5 os_prio=0 cpu=157.44ms elapsed=78749.65s tid=0x00007f531f43d000 nid=0x19d waiting for monitor entry  [0x00007f53084fd000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.runIfStateRunning(JobMasterServiceLeadershipRunner.java:462)
        - waiting to lock <0x00000000f1c0e088> (a java.lang.Object)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.revokeLeadership(JobMasterServiceLeadershipRunner.java:397)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.notifyLeaderContenderOfLeadershipLoss(DefaultLeaderElectionService.java:484)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1252/0x0000000840ddec40.accept(Unknown Source)
        at java.util.HashMap.forEach(java.base@11.0.22/HashMap.java:1337)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onRevokeLeadershipInternal(DefaultLeaderElectionService.java:452)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1251/0x0000000840dcf840.run(Unknown Source)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.lambda$runInLeaderEventThread$3(DefaultLeaderElectionService.java:549)
        - locked <0x00000000f0e3f4d8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService$$Lambda$1075/0x0000000840c23040.run(Unknown Source)
        at java.util.concurrent.CompletableFuture$AsyncRun.run(java.base@11.0.22/CompletableFuture.java:1736)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.22/ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.22/ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(java.base@11.0.22/Thread.java:829)
{code}

{code}
""jobmanager-io-thread-1"" #636 daemon prio=5 os_prio=0 cpu=125.56ms elapsed=78699.01s tid=0x00007f5321c6e800 nid=0x396 waiting for monitor entry  [0x00007f530567d000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.hasLeadership(DefaultLeaderElectionService.java:366)
        - waiting to lock <0x00000000f0e3f4d8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElection.hasLeadership(DefaultLeaderElection.java:52)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.isValidLeader(JobMasterServiceLeadershipRunner.java:509)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$forwardIfValidLeader$15(JobMasterServiceLeadershipRunner.java:520)
        - locked <0x00000000f1c0e088> (a java.lang.Object)
        at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner$$Lambda$1320/0x0000000840e1a840.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(java.base@11.0.22/CompletableFuture.java:859)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(java.base@11.0.22/CompletableFuture.java:837)
        at java.util.concurrent.CompletableFuture.postComplete(java.base@11.0.22/CompletableFuture.java:506)
        at java.util.concurrent.CompletableFuture.complete(java.base@11.0.22/CompletableFuture.java:2079)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.registerJobMasterServiceFutures(DefaultJobMasterServiceProcess.java:124)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:114)
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess$$Lambda$1319/0x0000000840e1a440.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(java.base@11.0.22/CompletableFuture.java:859)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(java.base@11.0.22/CompletableFuture.java:837)
        at java.util.concurrent.CompletableFuture.postComplete(java.base@11.0.22/CompletableFuture.java:506)
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(java.base@11.0.22/CompletableFuture.java:1705)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@11.0.22/ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@11.0.22/ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(java.base@11.0.22/Thread.java:829)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34897,FLINK-34933,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 07:23:52 UTC 2024,,,,,,,,,,"0|z1nz00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 08:12;mapohl;The two locks need to be nested because we need to check both: whether leadership is granted and whether the instance is still running. But we should always treat the leadership lock as the higher priority lock (i.e. acquiring it first) to ensure ordering within the locks.

The {{LeaderElection}}/{{LeaderElectionService}} interface is flawed in this sense. We shouldn't expose the {{hasLeadership}} but have a {{runLeaderOperation}} method that abstracts away the leadership check. In {{DefaultLeaderElectionService}} all these operations would be handled by the leader-operation executor. That's how we can ensure that the leadership lock is acquired first.

This is only an issue in the {{JobMasterServiceLeadershipRunner}} because we're executing the creation asynchronously in an io thread. The other place where we check within the contender whether leadership is acquired is the {{DefaultDispatcherRunner}}. But we're not doing any async calls there during leadership handling (the {{DefaultDispatcherRunner}} is created directly in the leader-operation executor while handling the leadership acquired event).

I also verified that this is not something that was introduced in Flink 1.18 with the [FLIP-285|https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+Refactoring+LeaderElection+to+make+Flink+support+multi-component+leader+election+out-of-the-box] changes. AFAIS, it can also happen in 1.17- (I didn't check the pre-FLINK-24038 code but only looked into {{release-1.17}}).
;;;","22/May/24 14:29;hong;[~mapohl] do we have any updates here? Wondering if this should be a blocker to the Flink 1.19.1 patch release!;;;","22/May/24 14:37;mapohl;I'm still trying to find a reviewer. It's on my plate. But it's not a blocker because the issue already existed in older versions of Flink:
{quote}
I also verified that this is not something that was introduced in Flink 1.18 with the FLIP-285 changes. AFAIS, it can also happen in 1.17- (I didn't check the pre-FLINK-24038 code but only looked into release-1.17).
{quote};;;","28/May/24 07:23;hong;Ok - sounds good. Will proceed with 1.19.1 RC without this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the content of README.md in FlinkCDC project,FLINK-34671,13571889,13571975,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,kunni,kunni,kunni,14/Mar/24 12:47,18/Mar/24 13:25,04/Jun/24 20:40,18/Mar/24 13:23,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Documentation,Flink CDC,,,,0,pull-request-available,,,"As we have updated the doc site of FlinkCDC, we should modify the content of README.md to update those links and add some more accurate description.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 13:23:50 UTC 2024,,,,,,,,,,"0|z1nyy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 13:23;leonard;Resolved in flink-cdc(master): a332fad3c6e6c76c3ac008ac38e5872fbed6fdda;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The asyncOperationsThreadPool in SubtaskCheckpointCoordinatorImpl can only create one worker thread,FLINK-34670,13571885,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,lijinzhong,lijinzhong,14/Mar/24 12:33,27/Mar/24 12:57,04/Jun/24 20:40,,1.18.0,1.19.0,,,,,1.20.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,"Now, the asyncOperations ThreadPoolExecutor of SubtaskCheckpointCoordinatorImpl is create with a LinkedBlockingQueue and zero corePoolSize.

!image-2024-03-14-20-24-14-198.png|width=614,height=198!

And in the ThreadPoolExecutor, except for the first time the task is submitted, *no* new thread is created until the queue is full. But the capacity of LinkedBlockingQueue is Integer.Max. This means that there is almost *only one thread* working in this thread pool, *even if* {*}there are many concurrent checkpoint requests or checkpoint abort requests waiting to be processed{*}.

!image-2024-03-14-20-27-37-540.png|width=614,height=175!

This problem can be verified by changing ExecutorService implementation in UT 
SubtaskCheckpointCoordinatorTest#testNotifyCheckpointAbortedDuringAsyncPhase. When the LinkedBlockingQueue and zero corePoolSize are configured, this UT will deadlock because only one worker thread can be created.
!image-2024-03-14-20-33-28-851.png|width=606,height=235!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/24 12:24;lijinzhong;image-2024-03-14-20-24-14-198.png;https://issues.apache.org/jira/secure/attachment/13067444/image-2024-03-14-20-24-14-198.png","14/Mar/24 12:27;lijinzhong;image-2024-03-14-20-27-37-540.png;https://issues.apache.org/jira/secure/attachment/13067443/image-2024-03-14-20-27-37-540.png","14/Mar/24 12:33;lijinzhong;image-2024-03-14-20-33-28-851.png;https://issues.apache.org/jira/secure/attachment/13067442/image-2024-03-14-20-33-28-851.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 12:52:42 UTC 2024,,,,,,,,,,"0|z1nyx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 12:53;lijinzhong;This issue seems to be a bug from [FLINK-30251|https://issues.apache.org/jira/browse/FLINK-30251].  [~pnowojski]  Could you please take a look at this?;;;","27/Mar/24 12:52;lijinzhong;[~roman]    [~pnowojski]  I think this is a critical bug that will result in concurrent checkpoints being forced to execute sequentially, as well as causing a drastic performance regression of checkpoint aborts. Could you please help confirm this problem?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimization of Arch Rules for Connector Constraints and Violation File Updates,FLINK-34669,13571880,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,qingyue,qingyue,14/Mar/24 11:59,14/Mar/24 14:40,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Test Infrastructure,,,,,0,,,,"Description:
Potential optimization within our Arch rules that could improve the development workflow. This originated from the discussion for PR [https://github.com/apache/flink/pull/24492]

1. Connector Constraints:
The current Arch rule, `CONNECTOR_CLASSES_ONLY_DEPEND_ON_PUBLIC_API`, was implemented to prevent internal code changes in Flink from affecting the compilation of connectors in external repositories. This rule is crucial for connectors that are external, but it may be unnecessarily restrictive for the filesystem connector, which remains within the same code repository as Flink. Maybe we should consider excluding the filesystem connector from this rule to better reflect its status as an internal component.

2. Preconditions Class Promotion:
The majority of Arch rule violations for connectors are related to the use of `Preconditions#checkX`. This consistent pattern of violations prompts the question of whether we should reclassify `Preconditions` from its current internal status to a `Public` or `PublicEvolving` interface, allowing broader and more official usage within our codebase.

3. Violation File Updates:
Updating the violation file following the `freeze.refreeze=true` process outlined in the readme proves to be difficult. The diffs generated include the line numbers, which complicates the review process, especially when substantial changes are submitted. Reviewers face a considerable challenge in distinguishing between meaningful changes and mere line number alterations. To alleviate this issue, I suggest that we modify the process so that line numbers are not included in the violation file diffs, streamlining reviews and commits.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-14 11:59:46.0,,,,,,,,,,"0|z1nyw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Report State handle of file merging directory to JM,FLINK-34668,13571860,13536037,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,zakelly,zakelly,14/Mar/24 09:56,27/Mar/24 02:01,04/Jun/24 20:40,27/Mar/24 02:01,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 02:00:53 UTC 2024,,,,,,,,,,"0|z1nyrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 02:00;Yanfei Lei;Merged via 05b27be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changelog state backend support local rescaling,FLINK-34667,13571847,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,14/Mar/24 08:46,20/Mar/24 02:46,04/Jun/24 20:40,20/Mar/24 02:46,1.20.0,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"FLINK-33341 uses the available local keyed state for rescaling, this will cause changelog state to incorrectly treat part of the local state as the complete local state.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 02:46:44 UTC 2024,,,,,,,,,,"0|z1nyoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/24 02:46;Yanfei Lei;Merged via 501de48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keep assigned splits in order to fix wrong meta group calculation,FLINK-34666,13571838,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,loserwang1024,loserwang1024,14/Mar/24 08:24,08/May/24 03:10,04/Jun/24 20:40,08/May/24 03:10,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,,,,"h3. Reason

When added newly tables, and then restart job, IncrementalSourceEnumerator#sendStreamMetaRequestEvent -> SplitAssigner#getFinishedSplitInfos maybe return unordered  finishedSplitInfos (newly snapshot infos is ahead of older one). When Reader request newly table's infos, will get older one, then never read all the infos and restart changelog read.

 
h3. How to reproduced it?

Add  chunk-meta.group.size = 2 in 
getCreateTableStatement, then run test of org.apache.flink.cdc.connectors.postgres.source.NewlyAddedTableITCase#testNewlyAddedTableForExistsPipelineTwiceWithAheadWalLog
{code:java}
//代码占位符
private String getCreateTableStatement(
        Map<String, String> otherOptions, String... captureTableNames) {
    return String.format(
            ""CREATE TABLE address (""
                    + "" table_name STRING METADATA VIRTUAL,""
                    + "" id BIGINT NOT NULL,""
                    + "" country STRING,""
                    + "" city STRING,""
                    + "" detail_address STRING,""
                    + "" primary key (id) not enforced""
                    + "") WITH (""
                    + "" 'connector' = 'postgres-cdc',""
                    + "" 'scan.incremental.snapshot.enabled' = 'true',""
                    + "" 'hostname' = '%s',""
                    + "" 'port' = '%s',""
                    + "" 'username' = '%s',""
                    + "" 'password' = '%s',""
                    + "" 'database-name' = '%s',""
                    + "" 'schema-name' = '%s',""
                    + "" 'table-name' = '%s',""
                    + "" 'slot.name' = '%s', ""
                    + "" 'scan.incremental.snapshot.chunk.size' = '2',""
                    + "" 'chunk-meta.group.size' = '2',""
                    + "" 'scan.newly-added-table.enabled' = 'true'""
                    + "" %s""
                    + "")"",
            customDatabase.getHost(),
            customDatabase.getDatabasePort(),
            customDatabase.getUsername(),
            customDatabase.getPassword(),
            customDatabase.getDatabaseName(),
            SCHEMA_NAME,
            PostgresTestUtils.getTableNameRegex(captureTableNames),
            slotName,
            otherOptions.isEmpty()
                    ? """"
                    : "",""
                            + otherOptions.entrySet().stream()
                                    .map(
                                            e ->
                                                    String.format(
                                                            ""'%s'='%s'"",
                                                            e.getKey(), e.getValue()))
                                    .collect(Collectors.joining("","")));
} {code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-34634,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 08 02:42:14 UTC 2024,,,,,,,,,,"0|z1nymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/24 03:21;loserwang1024;Please assign it to me.;;;","08/May/24 02:42;loserwang1024;Have done it in FLINK-34634, please close it. [~renqs] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add streaming rule for union to Rand and it convert to StreamExecDeduplicate finally,FLINK-34665,13571835,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,14/Mar/24 07:44,14/Mar/24 07:46,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,,,,"The semantics of a union in SQL involves deduplication, and in Calcite, when converting a SQL node to a RelNode, a Distinct Aggregate is inserted above the Union to achieve this deduplication. In Flink, the Distinct Aggregate eventually gets converted into a StreamExecGroupAggregate operator. This operator accesses the state multiple times, and from our observations of numerous jobs, we can see that the stack often gets stuck at state access. This is because the key for the distinct aggregate is all the fields of the union, meaning that for the state, the key will be relatively large, and repeated access and comparisons to the state can be time-consuming.

In fact, a potential optimization is to add a rule to convert the Union into a Rank with processing time, which then ultimately gets converted into a StreamExecDeduplicate. Currently, we have users rewrite their SQL to use Row_number for deduplication, and this approach works very well. Therefore, it is possible to add a rule at the engine level to support this optimization.

 

and it will break the change of plan, it will cause user upgrade flink version failed. so i suggest add a flag.default value is not change the behavior",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 07:45:47 UTC 2024,,,,,,,,,,"0|z1nym0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 07:45;jackylau;hi [~jark] [~xuyangzhong] what do you think? and if you think it is ok, i can do it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add .asf.yaml for Flink CDC,FLINK-34664,13571818,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,renqs,renqs,renqs,14/Mar/24 02:21,14/Mar/24 05:47,04/Jun/24 20:40,14/Mar/24 05:47,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,We need to add .asf.yaml file to Flink CDC repo to get auto-links to Apache Jira and update project description,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 05:47:17 UTC 2024,,,,,,,,,,"0|z1nyi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 05:47;leonard;Resolved in flink-cdc(master) via: 88c23b59a016d7b023992d2a8fb1865f46fed00d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-opensearch connector Unable to parse response body for Response,FLINK-34663,13571793,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wgendy,wgendy,13/Mar/24 19:06,22/Mar/24 11:39,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Connectors / Opensearch,,,,,0,,,,"I`m trying to use flink-sql-opensearch connector to sink stream data to OpenSearch via Flink …
After submitting the Job to Flink cluster successfully , the job runs normally for 30sec and create the index with data … then it fails with the following message:
_*org.apache.flink.util.FlinkRuntimeException: Complete bulk has failed… Caused by: java.io.IOException: Unable to parse response body for Response*_

_*{requestLine=POST /_bulk?timeout=1m HTTP/1.1, host=[http://172.20.0.6:9200|http://172.20.0.6:9200/], response=HTTP/1.1 200 OK}*_

at org.opensearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1942)
at org.opensearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:662)
at org.opensearch.client.RestClient$1.completed(RestClient.java:396)
at org.opensearch.client.RestClient$1.completed(RestClient.java:390)
at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122)
at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:182)
at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448)
at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338)
at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265)
at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:87)
at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:40)
at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
… 1 more
*Caused by: java.lang.NullPointerException*
*at java.base/java.util.Objects.requireNonNull(Unknown Source)*
*at org.opensearch.action.DocWriteResponse.(DocWriteResponse.java:140)*
*at org.opensearch.action.index.IndexResponse.(IndexResponse.java:67) …*

It seems that this error is common but without any solution …
the flink connector despite it was built for OpenSearch 1.3 , but it still working in sending and creating index to OpenSearch 2.12.0 … but this error persists with all OpenSearch versions greater than 1.13 …

*Opensearch support reply was:*
*""this is unexpected, could you please create an issue here [1], the issue is caused by _type property that has been removed in 2.x""*","Docker-Compose:
Flink 1.18.1 - Java11
OpenSearch 2.12.0
Flink-Sql-Opensearch-connector (flink 1.18.1 → Os 1.3)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/24 22:10;wgendy;image-2024-03-14-00-10-40-982.png;https://issues.apache.org/jira/secure/attachment/13067432/image-2024-03-14-00-10-40-982.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Fri Mar 22 11:39:12 UTC 2024,,,,,,,,,,"0|z1nyco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/24 19:29;reta;[https://github.com/apache/flink-connector-opensearch/pull/38] would solve this, I will try to find out if we could make it work with 1.x client as well;;;","13/Mar/24 21:55;wgendy;Dear Andriy ,I sincerely appreciate your valuable comment ...

But I've encountered difficulty locating the jar file for the ""opensearch-2"" Flink connector. Could you kindly confirm if it has already been released or if it is still under development?

As of now, my search has reached only the standard flink-opensearch-connector jar file, which exclusively supports Opensearch V1.x. Not Opensearch V2.x..

If you happen to have any information regarding the availability of the opensearch-2 Flink connector jar file, I would be immensely grateful for your assistance in this matter.

Warm regards;;;","13/Mar/24 22:15;wgendy;knowing that after building the codebase ([GitHub - apache/flink-connector-opensearch: Apache flink|https://github.com/apache/flink-connector-opensearch]) project using maven ( ./mvn clean package -DskipTests )... 8-jars are created with the following names:

!image-2024-03-14-00-10-40-982.png|width=380,height=250!

and when using these jars (supposedly are the ones for ""opensearch-v2"" ) i got the problem as mentioned above .. and if i changed flink version (to 1.8.1) and opensearch version  (to 2.12.0) .. the project wont built and give an error of missing many classes that are no longer exist in the new versions ...

How then to get or build ""opensearch-v2"" flink connector ?

warm regards

 ;;;","13/Mar/24 22:21;reta;Thanks for trying it out, [~wgendy] , the pull request is not merged yet (I've just mentioned that there is an attempt to have separate OS v1 and v2 support), I will try to look into the possible options issue shortly.;;;","14/Mar/24 11:53;wgendy;Thanks  [~reta]  for the feedback .. ;;;","21/Mar/24 13:15;reta;[~wgendy] apologies for the delay, it seems like the only path to move forward is to have dedicated support for OSv1 and OSv2 (as for Elasticsearch), that should be fixed by FLINK-33859 (expecting to get it merged soon), thank you;;;","22/Mar/24 11:39;wgendy;Thanks [~reta]  for your reply ... all the best regards.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add new issue template for Flink CDC repo,FLINK-34662,13571728,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,13/Mar/24 10:24,14/Mar/24 01:51,04/Jun/24 20:40,14/Mar/24 01:51,,,,,,,,,,,Flink CDC,,,,,0,,,,"As we migrated to Apache Jira for managing issues, we need to provide a new template to remind users and contributors about the new issue reporting way.

The reason we don't close issue functionality is that some historical commits and PRs are linked to issues before the donation, and closing the functionality will make those not traceable anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 01:51:07 UTC 2024,,,,,,,,,,"0|z1nxy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 01:51;renqs;flink-cdc master: dba00625b11e77539794346890f3cb76c3b9c0cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskExecutor supports retain partitions after JM crashed.,FLINK-34661,13571717,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,13/Mar/24 09:10,13/May/24 09:26,04/Jun/24 20:40,13/May/24 09:26,,,,,,,1.20.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 09:26:49 UTC 2024,,,,,,,,,,"0|z1nxvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/24 09:26;zhuzh;4e6b42046adbe2f337460d2e50f1fee12cff21a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutoRescalingITCase#testCheckpointRescalingInKeyedState AssertionError,FLINK-34660,13571705,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,13/Mar/24 06:11,21/Mar/24 10:51,04/Jun/24 20:40,21/Mar/24 10:38,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58249&view=ms.vss-test-web.build-test-results-tab&runId=4036370&resultId=100718&paneView=debug]

 
{code:java}
expected:<[(0,8000), (0,32000), (0,48000), (0,72000), (1,78000), (1,30000), (1,54000), (0,2000), (0,10000), (0,50000), (0,66000), (0,74000), (0,82000), (1,80000), (1,0), (1,16000), (1,24000), (1,40000), (1,56000), (1,64000), (0,12000), (0,28000), (0,52000), (0,60000), (0,68000), (0,76000), (1,18000), (1,26000), (1,34000), (1,42000), (1,58000), (0,6000), (0,14000), (0,22000), (0,38000), (0,46000), (0,62000), (0,70000), (1,4000), (1,20000), (1,36000), (1,44000)]> but was:<[(0,8000), (0,32000), (0,48000), (0,72000), (1,78000), (1,30000), (1,54000), (0,2000), (0,10000), (0,50000), (0,66000), (0,74000), (0,82000), (0,23000), (0,31000), (1,80000), (1,0), (1,16000), (1,24000), (1,40000), (1,56000), (1,64000), (0,12000), (0,28000), (0,52000), (0,60000), (0,68000), (0,76000), (1,18000), (1,26000), (1,34000), (1,42000), (1,58000), (0,6000), (0,14000), (0,22000), (0,19000), (0,35000), (1,4000), (1,20000), (1,36000), (1,44000)]> {code}
 

This maybe related to FLINK-34624 as we could see from the log:
{code:java}
03:31:02,073 [ main] INFO org.apache.flink.runtime.testutils.PseudoRandomValueSelector [] - Randomly selected true for state.changelog.enabled
03:31:02,163 [jobmanager-io-thread-2] INFO org.apache.flink.state.changelog.AbstractChangelogStateBackend [] - ChangelogStateBackend is used, delegating EmbeddedRocksDBStateBackend. {code}
FLINK-34624 disables changelog since it doesn't support local rescaling currently.

Even if disabling changelog for AutoRescalingITCase manually, 
randomization may still be applied to it.
We should apply randomization only when it's not pre-defined.
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34624,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 21 10:46:55 UTC 2024,,,,,,,,,,"0|z1nxt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 01:46;masteryhx;merged aa71589 into master;;;","14/Mar/24 14:38;rskraba;On Github Actions, this might still fail: If I read correctly, this was after the fix was committed to master:
 * [https://github.com/apache/flink/actions/runs/8274402076/job/22639938749#step:10:8127] (Java 21 1.20-SNAPSHOT)

 
{code:java}
 Error: 02:41:58 02:41:58.807 [ERROR] Tests run: 32, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 123.0 s <<< FAILURE! -- in org.apache.flink.test.checkpointing.AutoRescalingITCase
Error: 02:41:58 02:41:58.807 [ERROR] org.apache.flink.test.checkpointing.AutoRescalingITCase.testCheckpointRescalingInKeyedState[backend = filesystem, buffersPerChannel = 2, useIngestDB = false] -- Time elapsed: 2.529 s <<< FAILURE!
Mar 14 02:41:58 java.lang.AssertionError: expected:<[(0,8000), (0,32000), (0,48000), (0,72000), (1,78000), (1,30000), (1,54000), (0,2000), (0,10000), (0,50000), (0,66000), (0,74000), (0,82000), (1,80000), (1,0), (1,16000), (1,24000), (1,40000), (1,56000), (1,64000), (0,12000), (0,28000), (0,52000), (0,60000), (0,68000), (0,76000), (1,18000), (1,26000), (1,34000), (1,42000), (1,58000), (0,6000), (0,14000), (0,22000), (0,38000), (0,46000), (0,62000), (0,70000), (1,4000), (1,20000), (1,36000), (1,44000)]> but was:<[(0,8000), (0,32000), (0,48000), (0,72000), (1,78000), (1,30000), (1,54000), (0,2000), (0,10000), (0,50000), (0,66000), (0,74000), (0,82000), (1,80000), (1,0), (1,16000), (1,24000), (1,40000), (1,56000), (1,64000), (0,28000), (0,52000), (0,68000), (1,18000), (1,26000), (1,34000), (1,42000), (1,58000), (0,6000), (0,30000), (0,38000), (0,14000), (0,22000), (0,46000), (0,62000), (0,70000), (1,4000), (1,20000), (1,36000), (1,44000)]>
Mar 14 02:41:58     at org.junit.Assert.fail(Assert.java:89)
Mar 14 02:41:58     at org.junit.Assert.failNotEquals(Assert.java:835)
Mar 14 02:41:58     at org.junit.Assert.assertEquals(Assert.java:120)
Mar 14 02:41:58     at org.junit.Assert.assertEquals(Assert.java:146)
Mar 14 02:41:58     at org.apache.flink.test.checkpointing.AutoRescalingITCase.testCheckpointRescalingKeyedState(AutoRescalingITCase.java:312)
Mar 14 02:41:58     at org.apache.flink.test.checkpointing.AutoRescalingITCase.testCheckpointRescalingInKeyedState(AutoRescalingITCase.java:208)
Mar 14 02:41:58     at java.base/java.lang.reflect.Method.invoke(Method.java:580)
Mar 14 02:41:58     at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45){code}
 ;;;","14/Mar/24 16:01;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58293&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8367];;;","15/Mar/24 08:29;masteryhx;I found it's because AutoRescalingITCase#testCheckpointRescalingInKeyedState doesn't apply setup configuration so that some random configuration may still work.

Will prepare a PR to fix.;;;","18/Mar/24 13:53;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58320&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8051]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58320&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8442]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58320&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8363]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58330&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8353]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58341&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8355]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58345&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8207]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8154]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8450]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8344]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=8376]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8395]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8158]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8277]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8179]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8385]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8515]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8341]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=8344];;;","21/Mar/24 10:38;mapohl;* master
** [e79df3544a59ec7bcd97c9946eb54b7bc3ecf009|https://github.com/apache/flink/commit/e79df3544a59ec7bcd97c9946eb54b7bc3ecf009]
** [aa7158934bbe17fa13945dddc32d14e2aa613ae5|https://github.com/apache/flink/commit/aa7158934bbe17fa13945dddc32d14e2aa613ae5];;;","21/Mar/24 10:46;mapohl;These test failures didn't include the fix from above, yet:
* https://github.com/apache/flink/actions/runs/8290287716/job/22688337407#step:10:8913
* https://github.com/apache/flink/actions/runs/8297049625/job/22707711131#step:10:8391;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
How to implement global sort in latest flink datastream API,FLINK-34659,13571701,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,pegasas,pegasas,13/Mar/24 03:21,13/Mar/24 16:19,04/Jun/24 20:40,13/Mar/24 07:51,1.18.1,,,,,,,,,,API / DataStream,,,,,0,,,,"[https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/dataset_migration/#%E7%AC%AC%E4%B8%89%E7%B1%BB]
 

!image-2024-03-13-11-21-57-846.png!

 

{{will this cause OOM in streaming execution mode?}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/24 03:21;pegasas;image-2024-03-13-11-21-57-846.png;https://issues.apache.org/jira/secure/attachment/13067412/image-2024-03-13-11-21-57-846.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Wed Mar 13 16:19:48 UTC 2024,,,,,,,,,,"0|z1nxs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/24 03:24;pegasas;[https://github.com/apache/flink/blob/94b55d1ae61257f21c7bb511660e7497f269abc7/docs/content/docs/dev/table/data_stream_api.md?plain=1#L842-L845]

It there only available on batch mode? flink will use sort shuffle for datastream;;;","13/Mar/24 07:51;martijnvisser;User questions shouldn't be posted as a Jira, but sent to the User mailing list, Slack or Stackoverflow. ;;;","13/Mar/24 14:50;pegasas;If you ever search on hive/spark SQL/Dataframe/RDD,

you will found that global sort is basic example on first site.

 

But I can not even see a standard solution on flink docs,

only finds that in table api comments.

[https://github.com/apache/flink/blob/7d0111dfab640f2f590dd710d76de927c86cf83e/docs/content/docs/dev/table/data_stream_api.md?plain=1#L843]

 

Would this will not be a problem/bug?

 

Especially Stream/Batch Unify is currently major part on Flink Roadmap.

 

Honestly I am shock/upset on this reply.;;;","13/Mar/24 16:19;martijnvisser;{quote}If you ever search on hive/spark SQL/Dataframe/RDD, you will found that global sort is basic example on first site.{quote}

Isn't that to be expected, given that all of these systems are batch systems first and foremost?

The user documentation that you linked even says so:
{quote}In theory, a streaming pipeline can execute all operators. However, in practice, some operations might
not make much sense as they would lead to ever-growing state and are therefore not supported. A global
sort would be an example that is only available in batch mode. Simply put: it should be possible to
run a working streaming pipeline in batch mode but not necessarily vice versa.{quote}

A global sort in a streaming application will mean that you will have to indefinitely store all state. That's not a bug, it does not make sense (as explained in the docs) to do a global sort in a streaming application. 

{quote}Honestly I am shock/upset on this reply.{quote}

I'm sorry for that, but you are asking a user question in the Jira of this ticket. As documented at https://flink.apache.org/how-to-contribute/getting-help/, you see that questions are meant for User mailing list, Slack or Stackoverflow. Bugs are to be reported in Jira. If it's unsure if something is a bug, the ask is to first post it on the User mailin glist. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala API unusable on Flink 1.18.1/Java 17 Docker image,FLINK-34658,13571684,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mbernst,mbernst,12/Mar/24 23:21,13/Mar/24 07:54,04/Jun/24 20:40,,1.18.0,1.18.1,,,,,,,,,API / Scala,,,,,0,docker,Java17,scala,"The Scala API should still work in Flink 1.18. The official Docker image for Flink 1.18.1 on Java 17 (""flink:1.18.1-scala_2.12-java17"") causes jobs using the Scala API to immediately throw a ReflectiveOperationException. Jobs using the Scala API still work correctly on the Java 11 image (""flink:1.18.1-scala_2.12-java11"").

The problem happens because the flink-scala JAR file included in the image (""flink-scala_2.12-1.18.1.jar"") has been built with an old Scala compiler that has a [compatibility bug with Java 17|https://github.com/scala/bug/issues/12419]. Rebuilding the flink-scala JAR file with the Scala compiler set to 2.12.15 or later fixes the bug. At my day job I cannot use Java 11 for a particular Flink job due to dependency on a Java library that uses [Java records|https://openjdk.org/jeps/395] (introduced in Java 16).

I have created a github repository with an example application and a longer description of the bug and how to fix it with a newer Scala compiler version: https://github.com/mattbernst/scala-java17-flink",This bug has been reproduced under macOS (Intel x64) and Linux (AMD 64) on a Flink cluster running in session mode.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 13 07:54:14 UTC 2024,,,,,,,,,,"0|z1nxog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/24 07:54;martijnvisser;[~mbernst] I think that's a known issue that we can't do too much about. Java 17 requieres a newer Scala compiler, but we can't use a newer version of Scala 2.12 beyond 2.12.8 because Scala introduces a binary incompatibility in 2.12.8. That breaks all savepoint compatibility for Scala users. That was reported and documented in FLINK-12461;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Lineage Graph for streaming API use cases,FLINK-34657,13571659,13526635,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,12/Mar/24 18:20,05/Apr/24 15:49,04/Jun/24 20:40,05/Apr/24 15:49,,,,,,,1.20.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 05 15:49:21 UTC 2024,,,,,,,,,,"0|z1nxiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Apr/24 15:49;mbalassi;[{{d3d8ae4}}|https://github.com/apache/flink/commit/d3d8ae428397794683e5ba05c5ca70622da99e5e] in master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generated code for `ITEM` operator should return null when getting element of a null map/array/row,FLINK-34656,13571629,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nilerzhou,nilerzhou,12/Mar/24 13:58,28/Mar/24 05:08,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Table SQL / Planner,,,,,0,,,,"In FieldAccessFromTableITCase we can find that the expected result of f0[1] is null when f0 is a null array. 

However, behavior in generated code for ITEM is not consistent with case above. The main code is:

 
{code:java}
val arrayAccessCode =
  s""""""
     |${array.code}
     |${index.code}
     |boolean $nullTerm = ${array.nullTerm} || ${index.nullTerm} ||
     |   $idxStr < 0 || $idxStr >= ${array.resultTerm}.size() || $arrayIsNull;
     |$resultTypeTerm $resultTerm = $nullTerm ? $defaultTerm : $arrayGet;
     |"""""".stripMargin {code}
If `array.nullTerm` is true, a default value of element type will be returned, for example -1 for null bigint array.

The reason why FieldAccessFromTableITCase can get expected result is that the 
ReduceExpressionsRule generated an expression code for that case like:
{code:java}
boolean isNull$0 = true || false ||
   ((int) 1) - 1 < 0 || ((int) 1) - 1 >= ((org.apache.flink.table.data.ArrayData) null).size() || ((org.apache.flink.table.data.ArrayData) null).isNullAt(((int) 1) - 1);
long result$0 = isNull$0 ? -1L : ((org.apache.flink.table.data.ArrayData) null).getLong(((int) 1) - 1);
if (isNull$0) {
  out.setField(0, null);
} else {
  out.setField(0, result$0);
} {code}
The reduced expr will be a null literal.
 

I think the behaviors for getting element of a null value should be unified.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 05:08:44 UTC 2024,,,,,,,,,,"0|z1nxc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 02:20;libenchao;Although the {{resultTerm}} is a default value instead of {{NULL}}, the {{nullTerm}} is correct, the the {{GeneratedExpression}} for {{generateArrayElementAt}} should be correct. Could you provide a test case that reproduce the problem you described?;;;","26/Mar/24 07:30;nilerzhou;[~libenchao]  Hi ,sorry for the late reply.  To reproduce the problem, you can add code below in
`org.apache.flink.table.planner.runtime.stream.sql.CalcITCase`:
{code:java}
@Test
def testElementGet(): Unit = {
  val sqlQuery =
    s""""""
       | SELECT a[1] FROM MyTableRow
       |"""""".stripMargin

  val rowData1: GenericRowData = new GenericRowData(2)
  rowData1.setField(0, null)

  val data = List(rowData1)

  implicit val dataType: TypeInformation[GenericRowData] =
    InternalTypeInfo
      .ofFields(ARRAY(BIGINT.notNull()).nullable().getLogicalType)
      .asInstanceOf[TypeInformation[GenericRowData]]

  val ds = env.fromCollection(data)

  val t = ds.toTable(tEnv, 'a)
  tEnv.createTemporaryView(""MyTableRow"", t)

  val outputType = InternalTypeInfo.ofFields(new BigIntType())

  val result = tEnv.sqlQuery(sqlQuery)
  val sink = new TestingAppendRowDataSink(outputType)
  tEnv.toDataStream(result, outputType.getDataType).addSink(sink)
  env.execute()

  val expected = List(
    ""+I(-1)""
  )
  assertThat(sink.getAppendResults.sorted).isEqualTo(expected.sorted)
} {code}
The root reason is that element is of not-null type, therefore the nullTerm won't be checked.;;;","28/Mar/24 05:08;libenchao;[~nilerzhou] Thank you for the test case, it helps a lot, we indeed should fix it. The problem seems related to the type of {{ARRAY(BIGINT.notNull()).nullable()}}, although the inner type is {{NOT NULL}}, the outer type is {{NULLABLE}}, there is a similar discussion in FLINK-31830 (and a corresponding discussion thread : https://lists.apache.org/thread/fzrfc9c3rtgw761ofdydl0q96km558q7).

Since this is a codegen issue which does not affect the user interface, we can solve this directly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler standalone doesn't work for flink 1.15,FLINK-34655,13571597,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,12/Mar/24 11:01,14/Mar/24 02:58,04/Jun/24 20:40,14/Mar/24 02:57,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,,0,pull-request-available,,,"flink-ubernetes-operator is committed to supporting the latest 4 flink minor versions, and autoscaler is a part of flink-ubernetes-operator. Currently,  the latest 4 flink minor versions are 1.15, 1.16, 1.17 and 1.18.

But autoscaler doesn't work for  flink 1.15.

h2. Root cause: 

* FLINK-28310 added some properties in IOMetricsInfo in flink-1.16
* IOMetricsInfo is a part of JobDetailsInfo
* JobDetailsInfo is necessary for autoscaler [1]
* flink's RestClient doesn't allow miss any property during deserializing the json

That means that the RestClient after 1.15 cannot fetch JobDetailsInfo for 1.15 jobs.

h2. How to fix it properly?

- [[FLINK-34655](https://issues.apache.org/jira/browse/FLINK-34655)] Copy IOMetricsInfo to flink-autoscaler-standalone module
- Removing them after 1.15 are not supported

[1] https://github.com/apache/flink-kubernetes-operator/blob/ede1a610b3375d31a2e82287eec67ace70c4c8df/flink-autoscaler/src/main/java/org/apache/flink/autoscaler/ScalingMetricCollector.java#L109
[2] https://cwiki.apache.org/confluence/display/FLINK/FLIP-401%3A+REST+API+JSON+response+deserialization+unknown+field+tolerance",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 02:57:50 UTC 2024,,,,,,,,,,"0|z1nx54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/24 12:08;gyfora;The bigger issue is that aggregated busy time metrics are not part of Flink 1.15;;;","12/Mar/24 12:09;gyfora;But the vertex parallelism overrides feature was introduced in 1.17 so the autoscaler never really officially supported anything before that. What do you think [~mxm] ?;;;","12/Mar/24 12:10;gyfora;Also this issue is fixed in the Kubernetes-operator package where we have an override version of IoMetricsInfo;;;","12/Mar/24 13:04;fanrui;{quote}But the vertex parallelism overrides feature was introduced in 1.17 so the autoscaler never really officially supported anything before that{quote}

We(our internal platform) want to use the autoscaler to give some parallelism setting suggestions to our users. We suggest they upgrade job to 1.17 or later version if users want to scaling automatically.

And that's why we want to parse scaling report. In the short term, we only use the autoscaler to give suggestion instead of scaling directly. After our users think the parallelism calculation is reliable, they will have stronger motivation to upgrade the flink version.

I guess most of new services or features are similar for users, users wanna check whether the new feature is stable first. And use it after they think it works as expected.;;;","13/Mar/24 11:28;mxm;Thanks for raising awareness for the Flink version compatibility, [~fanrui]! Although we've been using Flink Autoscaling with 1.16, it is true that only Flink 1.17 supports it out of the box.
{quote}In the short term, we only use the autoscaler to give suggestion instead of scaling directly. After our users think the parallelism calculation is reliable, they will have stronger motivation to upgrade the flink version.
{quote}
I understand the idea behind providing suggestions. However, it is difficult to assess the quality of Autoscaling decisions without applying them automatically. The reason is that suggestions become stale very quickly if the load pattern is not completely static. Even for static load patterns, if the user doesn't redeploy in a matter of minutes, the suggestions might already be stale again when the number of pending records increased too much. In any case, production load patterns are rarely static which means that autoscaling will inevitable trigger multiple times a day, but that is where its real power is unleashed. It would be great to hear about any concerns your users have for turning on automatic scaling. We've been operating it in production for about a year now.

Back to the issue here, should we think about a patch release for 1.15 / 1.16 to add support for overriding vertex parallelism?;;;","13/Mar/24 13:28;gyfora;[~mxm] I would be hesitant to try to backport these changes to 1.15/1.16, the community doesn't generally backport new features to older releases and also these are already out of the supported version scope of Flink core anyways. 

For 1.15 we would have to backport the aggregated metrics changes which is not backward compatible with the current 1.15 rest api, so not possible to do.;;;","14/Mar/24 02:57;fanrui;{quote}I understand the idea behind providing suggestions. However, it is difficult to assess the quality of Autoscaling decisions without applying them automatically. The reason is that suggestions become stale very quickly if the load pattern is not completely static. Even for static load patterns, if the user doesn't redeploy in a matter of minutes, the suggestions might already be stale again when the number of pending records increased too much. In any case, production load patterns are rarely static which means that autoscaling will inevitable trigger multiple times a day, but that is where its real power is unleashed. It would be great to hear about any concerns your users have for turning on automatic scaling. {quote}

Thanks for pointing it out! 

It is indeed difficult to observe the dynamic changes of the load. But users don't want to use a huge feature without observe. This does not only refer to autoscaler, but to all major features, users need to do enough research before they can be applied to the production environment. 

Although the parallelism may change dynamically, based on historical experience, users are more concerned about whether the parallelism is reasonable during peak periods. Currently, jdbc event handler recorded all ScalingReports. The ScalingReport includes the create time, users can check them conveniently. 

{quote}We've been operating it in production for about a year now.{quote}

It's great to see that your users have been using autoscaler for a long time. I believe it will give the entire community more confidence in using the autoscaler.

{quote}Back to the issue here, should we think about a patch release for 1.15 / 1.16 to add support for overriding vertex parallelism?{quote}

I agree with [~gyfora], the 1.15 and 1.16 won't be released anymore. So community doesn't need to backport them. If some users want to use these features, it's better to use the new version or cherry pick them to their internal flink version.;;;","14/Mar/24 02:57;fanrui;Merged to main(1.8.0) via: ab41083f38cbe27c7d0ee3d8ba29b527e13a4fcc;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""Special Thanks"" Page on the Flink Website",FLINK-34654,13571580,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jark,jark,jark,12/Mar/24 08:58,14/Mar/24 06:52,04/Jun/24 20:40,,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,"This issue aims to add a ""Special Thanks"" page on the Flink website (https://flink.apache.org/) to honor and appreciate the companies and organizations that have sponsored machines or services for our project.

Discussion thread: https://lists.apache.org/thread/y5g0nd5t8h2ql4gq7d0kb9tkwv1wkm1j",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-12 08:58:58.0,,,,,,,,,,"0|z1nx1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support table merging with route in Flink CDC,FLINK-34653,13571577,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,12/Mar/24 07:23,16/Apr/24 15:02,04/Jun/24 20:40,16/Apr/24 15:01,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,Currently route in Flink CDC only supports very simple table id replacing. It should support more complex table merging strategies. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 15:02:17 UTC 2024,,,,,,,,,,"0|z1nx0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 15:02;leonard;master via 6017b165289d8e6f40db396cd07c62285be7fca9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use CheckpointStreamFactory for StateChangeFsUploader,FLINK-34652,13571569,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,masteryhx,masteryhx,12/Mar/24 06:27,12/Mar/24 06:28,04/Jun/24 20:40,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,,,,"As discussed before, we may consider supporting CheckpointStreamFactory for StateChangeFsUploader:
 * need for checkpointId in the current API to resolve the location
 * different settings for materialized/changelog (e.g. timeouts)
 * re-use closeAndGetHandle
 * re-use in-memory handles (.metadata)
 * handle in-memory handles duplication

It could be considered together with FLINK-32085",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32085,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-12 06:27:24.0,,,,,,,,,,"0|z1nwyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The HiveTableSink of Flink does not support writing to S3,FLINK-34651,13571560,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,tinny,tinny,12/Mar/24 04:02,10/May/24 03:14,04/Jun/24 20:40,,1.13.6,1.14.6,1.15.4,1.16.3,1.17.2,1.18.1,,,,,Connectors / Hive,,,,,0,,,,"My Hive table is located on S3. When I try to write to Hive using Flink Streaming SQL, I find that it does not support writing to S3. Furthermore, this issue has not been fixed in the latest version. The error I got is as follows:
{code:java}
//代码占位符
java.io.IOException: No FileSystem for scheme: s3
    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2586)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2593)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2632)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2614)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
    at org.apache.flink.connectors.hive.HadoopFileSystemFactory.create(HadoopFileSystemFactory.java:44)
    at org.apache.flink.table.filesystem.stream.StreamingSink.lambda$compactionWriter$8dbc1825$1(StreamingSink.java:95)
    at org.apache.flink.table.filesystem.stream.compact.CompactCoordinator.initializeState(CompactCoordinator.java:102)
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:118)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:290)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:441)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:585)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:565)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:650)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:540)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
    at java.lang.Thread.run(Thread.java:750)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 03:14:36 UTC 2024,,,,,,,,,,"0|z1nwww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Mar/24 12:39;easonqin;I think add [hadooppresto-s3-file-systems-plugins|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/s3/#hadooppresto-s3-file-systems-plugins] to your classpath can solve this issue.;;;","19/Mar/24 08:19;tinny;[~easonqin]  It won't take effect because the HiveTableSink directly uses the HadoopFileSystemFactory.;;;","10/May/24 03:14;ZhenqiuHuang;Have you tried to put lib into your Flink App uber jar? Hadoop path should be able to find its own s3 file system implementation.

https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/2.7.3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate PushProjectIntoLegacyTableSourceScanRule,FLINK-34650,13571556,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,12/Mar/24 02:03,12/Mar/24 02:03,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-12 02:03:37.0,,,,,,,,,,"0|z1nww0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate PushFilterIntoLegacyTableSourceScanRule,FLINK-34649,13571555,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,12/Mar/24 02:02,08/Apr/24 10:14,04/Jun/24 20:40,08/Apr/24 10:14,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 10:14:20 UTC 2024,,,,,,,,,,"0|z1nwvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 10:14;Sergey Nuyanzin;Merged as [82116865b01f6e4009a64d0be8c381be616dd070|https://github.com/apache/flink/commit/82116865b01f6e4009a64d0be8c381be616dd070];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid RPC time when apply SchemaChangeEvent to external system,FLINK-34648,13571554,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,kunni,kunni,kunni,12/Mar/24 01:27,22/May/24 12:42,04/Jun/24 20:40,24/Apr/24 08:05,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"When SchemaOperator receive SchemaChangeEvent, it will send request to SchemaRegistry and wait for applying this SchemaChangeEvent to external system synchronously.
However, if this process take too long time, it will cause RPC TimeoutException, and During the process of task recovery, there may be other errors like `AddColumnEvent is already existed`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/24 07:01;kunni;截屏2024-03-12 14.55.09.png;https://issues.apache.org/jira/secure/attachment/13067391/%E6%88%AA%E5%B1%8F2024-03-12+14.55.09.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 08:05:19 UTC 2024,,,,,,,,,,"0|z1nwvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/24 02:10;loserwang1024;During the process of task recovery,  will the enumerator not restart? If enumerator restarts and restore from last checkpoint, the event will not exist.;;;","12/Mar/24 06:58;kunni;Enumerator will not restart but add splits back. See the picture in the attachment.;;;","24/Apr/24 08:05;leonard;Implemented in master via c1e1aa48d93933c11ce9629d6f98041338f4d7a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Path normalization is allocation intensive,FLINK-34647,13571551,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,schlosna,schlosna,schlosna,12/Mar/24 00:41,21/Mar/24 04:48,04/Jun/24 20:40,21/Mar/24 04:48,,,,,,,1.20.0,,,,API / Core,Runtime / Checkpointing,,,,0,pull-request-available,,,"While investigating allocation stalls and GC pressure of a Flink streaming pipeline, I noticed significant allocations in JFR from Flink path normalization:

 !image-2024-03-11-20-38-12-573.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/24 00:38;schlosna;image-2024-03-11-20-38-12-573.png;https://issues.apache.org/jira/secure/attachment/13067389/image-2024-03-11-20-38-12-573.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Thu Mar 21 04:48:46 UTC 2024,,,,,,,,,,"0|z1nwuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/24 04:48;masteryhx;Merged 32144689 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AggregateITCase.testDistinctWithRetract timed out,FLINK-34646,13571499,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,11/Mar/24 14:51,17/Apr/24 14:49,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Table SQL / Runtime,,,,,0,test-stability,,,"https://github.com/apache/flink/actions/runs/8211401561/job/22460442229#step:10:17161
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f70abeb7000 nid=0x4cff3 waiting on condition [0x00007f70ac3f6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000cd24c690> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2131)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2099)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2077)
	at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:876)
	at org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testDistinctWithRetract(AggregateITCase.scala:345)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 14:49:19 UTC 2024,,,,,,,,,,"0|z1nwjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/24 14:49;rskraba;1.18 AdaptiveScheduler: Test (module: table) https://github.com/apache/flink/actions/runs/8715237382/job/23907050959#step:10:12476

This is the exact same timeout but it's happening on {{AggregateITCase.testMinMaxWithBinaryString}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount fails,FLINK-34645,13571498,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,11/Mar/24 14:49,27/May/24 15:15,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Table SQL / Runtime,,,,,0,github-actions,test-stability,,"{code}
Error: 02:27:17 02:27:17.025 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.658 s <<< FAILURE! - in org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest
Error: 02:27:17 02:27:17.025 [ERROR] org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount  Time elapsed: 0.3 s  <<< FAILURE!
Mar 09 02:27:17 java.lang.AssertionError: 
Mar 09 02:27:17 
Mar 09 02:27:17 Expected size: 8 but was: 6 in:
Mar 09 02:27:17 [Record @ (undef) : +I(c1,0,1969-12-31T23:59:55,1970-01-01T00:00:05),
Mar 09 02:27:17     Record @ (undef) : +I(c2,3,1969-12-31T23:59:55,1970-01-01T00:00:05),
Mar 09 02:27:17     Record @ (undef) : +I(c2,3,1970-01-01T00:00,1970-01-01T00:00:10),
Mar 09 02:27:17     Record @ (undef) : +I(c1,0,1970-01-01T00:00,1970-01-01T00:00:10),
Mar 09 02:27:17     Watermark @ 10000,
Mar 09 02:27:17     Watermark @ 20000]
Mar 09 02:27:17 	at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:110)
Mar 09 02:27:17 	at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:70)
Mar 09 02:27:17 	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.assertOutputEquals(ArrowPythonAggregateFunctionOperatorTestBase.java:62)
Mar 09 02:27:17 	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount(StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java:326)
Mar 09 02:27:17 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 15:15:54 UTC 2024,,,,,,,,,,"0|z1nwj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/24 14:49;mapohl;https://github.com/XComp/flink/actions/runs/7610966824/job/20726603399#step:10:21897;;;","11/Mar/24 14:49;mapohl;https://github.com/apache/flink/actions/runs/8027474081/job/21931683454#step:10:21875;;;","02/Apr/24 16:00;rskraba;1.18, hadoop 3.1.3: [https://github.com/apache/flink/actions/runs/8502821617/job/23287755742#step:10:21899|https://github.com/apache/flink/actions/runs/8502821617/job/23287755742#step:10:21899] 

Note that this and some of the logs above are the same error but on a different test (included here for searchability) and probably the same as FLINK-24333
{code:java}
Error: 02:35:54 02:35:54.414 [ERROR] org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredOnCheckpoint  Time elapsed: 0.237 s  <<< FAILURE!
Apr 01 02:35:54 java.lang.AssertionError: 
Apr 01 02:35:54 
Apr 01 02:35:54 Expected size: 8 but was: 6 in:
Apr 01 02:35:54 [Record @ (undef) : +I(c1,0,1969-12-31T23:59:55,1970-01-01T00:00:05),
Apr 01 02:35:54     Record @ (undef) : +I(c2,3,1969-12-31T23:59:55,1970-01-01T00:00:05),
Apr 01 02:35:54     Record @ (undef) : +I(c2,3,1970-01-01T00:00,1970-01-01T00:00:10),
Apr 01 02:35:54     Record @ (undef) : +I(c1,0,1970-01-01T00:00,1970-01-01T00:00:10),
Apr 01 02:35:54     Watermark @ 10000,
Apr 01 02:35:54     Watermark @ 20000]
Apr 01 02:35:54     at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:110)
Apr 01 02:35:54     at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:70)
Apr 01 02:35:54     at org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.assertOutputEquals(ArrowPythonAggregateFunctionOperatorTestBase.java:62)
Apr 01 02:35:54     at org.apache.flink.table.runtime.operators.python.aggregate.arrow.stream.StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredOnCheckpoint(StreamArrowPythonGroupWindowAggregateFunctionOperatorTest.java:241)
Apr 01 02:35:54     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {code}
 ;;;","30/Apr/24 16:02;rskraba;1.18 Java 11 / Test (module: misc) https://github.com/apache/flink/actions/runs/8872328847/job/24356773170#step:10:21780

Again a slightly different output was received, but in the same test:
{code}
Apr 29 02:31:31 Expected size: 6 but was: 4 in:
Apr 29 02:31:31 [Record @ (undef) : +I(c1,0,1969-12-31T23:59:55,1970-01-01T00:00:05),
Apr 29 02:31:31     Record @ (undef) : +I(c1,0,1970-01-01T00:00,1970-01-01T00:00:10),
Apr 29 02:31:31     Record @ (undef) : +I(c1,1,1970-01-01T00:00:05,1970-01-01T00:00:15),
Apr 29 02:31:31     Record @ (undef) : +I(c1,2,1970-01-01T00:00:10,1970-01-01T00:00:20)]
Apr 29 02:31:31 	at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:110)
Apr 29 02:31:31 	at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:70)
Apr 29 02:31:31 	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.assertOutputEquals(ArrowPythonAggregateFunctionOperatorTestBase.java:62)
Apr 29 02:31:31 	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount(BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java:209)
{code};;;","03/May/24 15:54;rskraba;1.18 Java 11 / Test (module: misc) https://github.com/apache/flink/actions/runs/8825970611/job/24231267277#step:10:21751;;;","27/May/24 15:15;rskraba;* 1.18 Hadoop 3.1.3 / Test (module: misc) https://github.com/apache/flink/actions/runs/9232146944;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
RestServerEndpointITCase.testShouldWaitForHandlersWhenClosing failed with ConnectionClosedException,FLINK-34644,13571497,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,11/Mar/24 14:31,17/Apr/24 14:49,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Runtime / Coordination,,,,,0,test-stability,,,"https://github.com/apache/flink/actions/runs/8189958608/job/22396362238#step:10:9215

{code}
Error: 15:13:33 15:13:33.779 [ERROR] Tests run: 68, Failures: 0, Errors: 1, Skipped: 4, Time elapsed: 17.81 s <<< FAILURE! -- in org.apache.flink.runtime.rest.RestServerEndpointITCase
Error: 15:13:33 15:13:33.779 [ERROR] org.apache.flink.runtime.rest.RestServerEndpointITCase.testShouldWaitForHandlersWhenClosing -- Time elapsed: 0.329 s <<< ERROR!
Mar 07 15:13:33 java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.ConnectionClosedException: Channel became inactive.
Mar 07 15:13:33 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Mar 07 15:13:33 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
Mar 07 15:13:33 	at org.apache.flink.runtime.rest.RestServerEndpointITCase.testShouldWaitForHandlersWhenClosing(RestServerEndpointITCase.java:592)
Mar 07 15:13:33 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 07 15:13:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Mar 07 15:13:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Mar 07 15:13:33 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
Mar 07 15:13:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Mar 07 15:13:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Mar 07 15:13:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Mar 07 15:13:33 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Mar 07 15:13:33 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
Mar 07 15:13:33 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Mar 07 15:13:33 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Mar 07 15:13:33 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Mar 07 15:13:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Mar 07 15:13:33 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Mar 07 15:13:33 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Mar 07 15:13:33 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
Mar 07 15:13:33 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
Mar 07 15:13:33 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Mar 07 15:13:33 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Mar 07 15:13:33 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Mar 07 15:13:33 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Mar 07 15:13:33 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Mar 07 15:13:33 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Mar 07 15:13:33 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 07 15:13:33 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 07 15:13:33 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 07 15:13:33 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 07 15:13:33 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Mar 07 15:13:33 Caused by: org.apache.flink.runtime.rest.ConnectionClosedException: Channel became inactive.
Mar 07 15:13:33 	at org.apache.flink.runtime.rest.RestClient$ClientHandler.channelInactive(RestClient.java:749)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.stream.ChunkedWriteHandler.channelInactive(ChunkedWriteHandler.java:137)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageAggregator.channelInactive(MessageAggregator.java:441)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelInactive(CombinedChannelDuplexHandler.java:418)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:411)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:376)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.HttpClientCodec$Decoder.channelInactive(HttpClientCodec.java:329)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelInactive(CombinedChannelDuplexHandler.java:221)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:411)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:376)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1085)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
Mar 07 15:13:33 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
Mar 07 15:13:33 	at java.lang.Thread.run(Thread.java:750)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 14:49:38 UTC 2024,,,,,,,,,,"0|z1nwiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Mar/24 08:56;rskraba;https://github.com/apache/flink/actions/runs/8423217883/job/23064824741#step:10:9197;;;","17/Apr/24 14:49;rskraba;1.20 Java 21: Test (module: core) https://github.com/apache/flink/actions/runs/8715237422/job/23907067308#step:10:9495;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobIDLoggingITCase failed,FLINK-34643,13571496,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,11/Mar/24 14:12,03/Apr/24 08:15,04/Jun/24 20:40,27/Mar/24 09:54,1.20.0,,,,,,1.20.0,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58187&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7897

{code}
Mar 09 01:24:23 01:24:23.498 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.209 s <<< FAILURE! -- in org.apache.flink.test.misc.JobIDLoggingITCase
Mar 09 01:24:23 01:24:23.498 [ERROR] org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(ClusterClient) -- Time elapsed: 1.459 s <<< ERROR!
Mar 09 01:24:23 java.lang.IllegalStateException: Too few log events recorded for org.apache.flink.runtime.jobmaster.JobMaster (12) - this must be a bug in the test code
Mar 09 01:24:23 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
Mar 09 01:24:23 	at org.apache.flink.test.misc.JobIDLoggingITCase.assertJobIDPresent(JobIDLoggingITCase.java:148)
Mar 09 01:24:23 	at org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(JobIDLoggingITCase.java:132)
Mar 09 01:24:23 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 09 01:24:23 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 09 01:24:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 09 01:24:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 09 01:24:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 09 01:24:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Mar 09 01:24:23 
{code}

The other test failures of this build were also caused by the same test:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58187&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8349
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58187&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8209",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34417,,FLINK-34994,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 03 08:10:44 UTC 2024,,,,,,,,,,"0|z1nwio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/24 14:12;mapohl;FLINK-34417 introduced this test. [~roman] can you have a look?;;;","11/Mar/24 14:19;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58192&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8243
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58201&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7898
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58201&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8004;;;","11/Mar/24 14:33;roman;Sure [~mapohl] , will take a look.;;;","11/Mar/24 15:01;mapohl;Thanks.

* https://github.com/apache/flink/actions/runs/8211401122/job/22460460967#step:10:7643
* https://github.com/apache/flink/actions/runs/8211401122/job/22460457228#step:10:7501
* https://github.com/apache/flink/actions/runs/8211401122/job/22460447071#step:10:7609
* https://github.com/apache/flink/actions/runs/8218856722/job/22476316489#step:10:8668
* https://github.com/apache/flink/actions/runs/8218856722/job/22476309886#step:10:11629
* https://github.com/apache/flink/actions/runs/8218856722/job/22476303995#step:10:8188
* https://github.com/apache/flink/actions/runs/8226669379/job/22494228715#step:10:8272
* https://github.com/apache/flink/actions/runs/8226669379/job/22494127725#step:10:8204
* https://github.com/apache/flink/actions/runs/8226669379/job/22493696266#step:10:7536
* https://github.com/apache/flink/actions/runs/8226669379/job/22494197386#step:10:8089;;;","12/Mar/24 07:40;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58232&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8242
* https://github.com/apache/flink/actions/runs/8228497559/job/22498277501#step:10:11643
* https://github.com/apache/flink/actions/runs/8230355449/job/22504508857#step:10:11393
* https://github.com/apache/flink/actions/runs/8239288679/job/22532540457#step:10:11461
* https://github.com/apache/flink/actions/runs/8239791756/job/22534164458#step:10:11389
* https://github.com/apache/flink/actions/runs/8242516657/job/22541862449#step:10:8735
* https://github.com/apache/flink/actions/runs/8242516657/job/22541846624#step:10:8301;;;","13/Mar/24 17:12;rskraba;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58254&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8244;;;","14/Mar/24 09:41;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58272&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7898]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58272&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=8205]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58272&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8349] (1.20)

The latter has a different error message than yesterday but shares the same root at {{{}assertJobIDPresent{}}}:


{code:java}
 Mar 14 01:23:27 01:23:27.069 [INFO] Running org.apache.flink.test.misc.CustomSerializationITCase
Mar 14 01:23:28 01:23:28.480 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.331 s <<< FAILURE! -- in org.apache.flink.test.misc.JobIDLoggingITCase
Mar 14 01:23:28 01:23:28.480 [ERROR] org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(ClusterClient) -- Time elapsed: 1.450 s <<< FAILURE!
Mar 14 01:23:28 org.opentest4j.AssertionFailedError: too many events without Job ID recorded for org.apache.flink.runtime.taskexecutor.TaskExecutor: [Logger=org.apache.flink.runtime.taskexecutor.TaskExecutor Level=INFO Message=JobManager for job e1d10f96bb6854dddf9819faf1f78216 with leader id 8197712ec807a18af492ef44b8ad4093 lost leadership.] ==> expected: <true> but was: <false>
Mar 14 01:23:28 	at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
Mar 14 01:23:28 	at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
Mar 14 01:23:28 	at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
Mar 14 01:23:28 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
Mar 14 01:23:28 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214)
Mar 14 01:23:28 	at org.apache.flink.test.misc.JobIDLoggingITCase.assertJobIDPresent(JobIDLoggingITCase.java:180)
Mar 14 01:23:28 	at org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(JobIDLoggingITCase.java:119)
Mar 14 01:23:28 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Mar 14 01:23:28 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 14 01:23:28 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
Mar 14 01:23:28 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
Mar 14 01:23:28 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
Mar 14 01:23:28 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
Mar 14 01:23:28 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
{code};;;","14/Mar/24 14:21;rskraba;On GitHub Actions for master (1.20):
 * [https://github.com/apache/flink/actions/runs/8258416220/job/22590870181#step:10:8385]  (too few log events) 
 * [https://github.com/apache/flink/actions/runs/8266862056/job/22616450397#step:10:8615] (too many events without Job ID)
 * [https://github.com/apache/flink/actions/runs/8274402076/job/22639957300#step:10:7705] (too many events without Job ID)
 * [https://github.com/apache/flink/actions/runs/8274402076/job/22639930619#step:10:7630] (too few log events)

 ;;;","15/Mar/24 19:16;roman;Merged into master as 6b5ae445724b68db05a3f9687cff6dd68e2129d7.;;;","19/Mar/24 12:55;rskraba;Weird – I collected a lot of build logs yesterday from over the weekend that resemble this error, but apparently my comment didn't get added :/  I'll go back and find those links.

In the meantime, [~roman]: we are still seeing failures in the same test that seem very related to this issue.  Is it possible that this fix is incomplete and should be reopened, or would you prefer that I raise a new JIRA?
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58398&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8249]

{code:java}
Mar 19 01:23:06 [not all expected events logged by org.apache.flink.runtime.jobmaster.JobMaster, logged:
Mar 19 01:23:06 [Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=INFO Message=Initializing job 'Flink Streaming Job' (2ef7e557551a93ef716b6c3ba580bcd6)., Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=INFO Message=Using restart back off time strategy NoRestartBackoffTimeStrategy for Flink Streaming Job (2ef7e557551a93ef716b6c3ba580bcd6)., Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=INFO Message=Starting execution of job 'Flink Streaming Job' (2ef7e557551a93ef716b6c3ba580bcd6) under job master id 90514ce7689864236ebeb94380dc474d., Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=DEBUG Message=Trigger heartbeat request., Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=INFO Message=Connecting to ResourceManager pekko://flink/user/rpc/resourcemanager_1(8eee414f9dea640cb3668826c12e4976), Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=INFO Message=Resolved ResourceManager address, beginning registration, Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=DEBUG Message=Registration at ResourceManager attempt 1 (timeout=100ms), Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=DEBUG Message=Registration with ResourceManager at pekko://flink/user/rpc/resourcemanager_1 was successful., Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=INFO Message=JobManager successfully registered at ResourceManager, leader id: 8eee414f9dea640cb3668826c12e4976., Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=INFO Message=Stopping the JobMaster for job 'Flink Streaming Job' (2ef7e557551a93ef716b6c3ba580bcd6)., Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=INFO Message=Disconnect TaskExecutor 23ae1952-8d6f-476e-b23b-4fad48feec15 because: Stopping JobMaster for job 'Flink Streaming Job' (2ef7e557551a93ef716b6c3ba580bcd6)., Logger=org.apache.flink.runtime.jobmaster.JobMaster Level=DEBUG Message=Close ResourceManager connection 58e840ebb5c16d7fb17f233b9e93cb3c.]] 
Mar 19 01:23:06 Expecting empty but was: [Checkpoint storage is set to .*,
Mar 19 01:23:06     Running initialization on master for job .*,
Mar 19 01:23:06     Starting scheduling.*,
Mar 19 01:23:06     State backend is set to .*,
Mar 19 01:23:06     Successfully created execution graph from job graph .*,
Mar 19 01:23:06     Successfully ran initialization on master.*,
Mar 19 01:23:06     Triggering a manual checkpoint for job .*.,
Mar 19 01:23:06     Using failover strategy .*]
Mar 19 01:23:06 	at org.apache.flink.test.misc.JobIDLoggingITCase.assertJobIDPresent(JobIDLoggingITCase.java:241)
Mar 19 01:23:06 	at org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(JobIDLoggingITCase.java:170)
Mar 19 01:23:06 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 19 01:23:06 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 19 01:23:06 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 19 01:23:06 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 19 01:23:06 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 19 01:23:06 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code};;;","19/Mar/24 13:52;mapohl;I'm reopening the issue. [~roman] is this due to the fact that we haven't collected all the logs that should be considered (due to the ""empirical nature"" of this test)?;;;","20/Mar/24 11:55;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58429&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8243]

Here are the older build logs that should have been added over the weekend.
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58320&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8349]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58320&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8244]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8210]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58352&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7904]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8354]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58359&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8249]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8354]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58366&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7903]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58398&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8249];;;","21/Mar/24 10:10;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58455&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8349]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58455&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7898];;;","21/Mar/24 10:48;mapohl;* https://github.com/apache/flink/actions/runs/8290287716/job/22688325865#step:10:9328
* https://github.com/apache/flink/actions/runs/8304571223/job/22730531076#step:10:9194
* https://github.com/apache/flink/actions/runs/8312246651/job/22747312383#step:10:8539
* https://github.com/apache/flink/actions/runs/8320242443/job/22764925776#step:10:8913
* https://github.com/apache/flink/actions/runs/8320242443/job/22764920830#step:10:8727
* https://github.com/apache/flink/actions/runs/8320242443/job/22764903331#step:10:9336
* https://github.com/apache/flink/actions/runs/8336454518/job/22813901357#step:10:8952
* https://github.com/apache/flink/actions/runs/8336454518/job/22813876201#step:10:9327
* https://github.com/apache/flink/actions/runs/8352823788/job/22863786799#step:10:8952
* https://github.com/apache/flink/actions/runs/8352823788/job/22863772571#step:10:9337
* https://github.com/apache/flink/actions/runs/8368626493/job/22913270846#step:10:8418;;;","21/Mar/24 13:21;roman;Thanks for reporting

My first suspicion was that the assertion happens too early for some reason, but that's not the case, because later log messages are present.

The only reason I can think of is async logging or buffering in log4j - will try to verify that.

Btw, some runs are about old version (""Too few log events recorded"" was removed in master), but others are valid.

 ;;;","21/Mar/24 13:29;roman;Or maybe it's actually simple: [https://github.com/apache/flink/pull/24550] :);;;","22/Mar/24 11:13;rskraba;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58481&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8247;;;","22/Mar/24 13:26;rskraba;* [https://github.com/apache/flink/actions/runs/8375475096/job/22933386950#step:10:7849]
 * [https://github.com/apache/flink/actions/runs/8384698540/job/22962603273#step:10:8296]
 * https://github.com/apache/flink/actions/runs/8384423503/job/22961956846#step:10:7958;;;","22/Mar/24 19:01;roman;Merged into master as ed4d6f091f27ffc778cbb6de6a3fa19251277bdc..4edafcc8b0b96920036a1afaaa37ae87b77668ed.;;;","25/Mar/24 07:22;Weijie Guo;[~roman] Not sure if this is the same kind of problem?

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58523&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8243;;;","25/Mar/24 07:34;fanrui;JobIDLoggingITCase.testJobIDLogging fails in this CI: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58511&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8243;;;","25/Mar/24 08:10;roman;Thanks for reporting, reopening the issue.;;;","25/Mar/24 14:23;rskraba;* [https://github.com/apache/flink/actions/runs/8398672279/job/23003965347#step:10:8418]
 * [https://github.com/apache/flink/actions/runs/8406174862/job/23019901189#step:10:7838]
 * [https://github.com/apache/flink/actions/runs/8414062406/job/23037406079#step:10:8598];;;","26/Mar/24 05:48;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58548&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8245;;;","26/Mar/24 07:23;mapohl;Should we try to reproduce the test failure in a PR by modifying the CI scripts (i.e. executing the test in a loop)? That way we could disable the test in {{master}} for now.;;;","26/Mar/24 08:56;rskraba;https://github.com/apache/flink/actions/runs/8429762645/job/23084767868#step:10:7601;;;","27/Mar/24 03:30;fanrui;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58570&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8391;;;","27/Mar/24 09:54;roman;Fix merged into master as d4c1a0a1ba4a1f1919f5ecccd3baa3d2cd44cef6.;;;","27/Mar/24 10:05;rskraba;1.20 [https://github.com/apache/flink/actions/runs/8445595387/job/23133281867#step:10:8019] (before the fix)

Thanks for the fix!;;;","01/Apr/24 02:47;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58640&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8735

It looks like after the fix.;;;","02/Apr/24 16:00;rskraba;1.20, on JDK 11, not adaptive schedule: [https://github.com/apache/flink/actions/runs/8502821551/job/23287730632#step:10:8131]

(This happened on commit [https://github.com/apache/flink/commits/3841f0622552674fe0013b89605594a401cb6902] , which includes the last fix... should this be reopened?)

1.20, JDK 8: https://github.com/apache/flink/actions/runs/8507870399/job/23300810619#step:10:8086;;;","03/Apr/24 06:53;mapohl;I guess, reopening the issue would be fine. But for the sake of not putting too much into a single ticket, it wouldn't be wrong to create a new ticket and linking FLINK-34643 as the cause, either. I personally would go for the latter option.;;;","03/Apr/24 08:10;roman;I agree, I'd open a separate ticket since the message (and probably the root cause) is now different.

 

edit:

created https://issues.apache.org/jira/browse/FLINK-34994 ;;;"
Add a dedicated test for when the lower utilization boundary is zero,FLINK-34642,13571494,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mxm,mxm,mxm,11/Mar/24 14:05,11/Mar/24 20:06,04/Jun/24 20:40,11/Mar/24 20:06,kubernetes-operator-1.7.0,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,,0,pull-request-available,,,"Users recently reported that scale down as ""broken"". Turned out that {{target.utilization}} and {{target.utilization.boundary}} were set to the same value. This prevents scale down entirely, which is expected and desired.

We should add a test to verify this behavior to avoid breaking this in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-11 14:05:03.0,,,,,,,,,,"0|z1nwi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possibility to add Python Files from http/https sources in PythonDriver,FLINK-34641,13571477,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,alexey.perminov,alexey.perminov,11/Mar/24 11:43,11/Mar/24 15:21,04/Jun/24 20:40,,1.18.0,1.18.1,1.18.2,,,,,,,,API / Python,,,,,0,,,,"Problem: Now it is impossible to add python files from the external http sources, only fs sources are available. It is not very convenient to add Python packages to the Flink Docker image or external fs every time the new version of the Python job is built, especially when using the Session mode. 

Proposal: Add possibility to handle the http/https protocols in the PythonDriver so that it is possible to add packages from the external Python repositories (e.g. PyPi). 

Example of the solution in the PythonEnvUtils.java
{code:java}
private static void addToPythonPath(PythonEnvironment env, List<Path> pythonFiles) {
...
 if (isValidURL(pythonFile.toString()) && new URL(pythonFile.toString()).getProtocol().startsWith(""http"")) {
    getFileFromHTTP(pythonFile, targetPath);
} else if (!pythonFile.getFileSystem().isDistributedFS()) {
...
}{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2024-03-11 11:43:05.0,,,,,,,,,,"0|z1nweg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace DummyMetricGroup usage with UnregisteredMetricsGroup,FLINK-34640,13571467,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeyhunkarimov,chesnay,chesnay,11/Mar/24 10:52,14/Mar/24 12:34,04/Jun/24 20:40,14/Mar/24 12:34,,,,,,,1.20.0,,,,Runtime / Metrics,Tests,,,,0,pull-request-available,,,The {{DummyMetricGroup}} is terrible because it is decidedly unsafe to use. Use the {{UnregisteredMetricsGroup}} instead.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 12:33:58 UTC 2024,,,,,,,,,,"0|z1nwc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 12:33;jingge;master: a0c811ec03d51c3b184c28f6f58b79466061ae71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CDC: Support DebeziumDeserializationSchema in OceanBase source connector,FLINK-34639,13571453,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanghe,wanghe,wanghe,11/Mar/24 09:12,26/Apr/24 03:32,04/Jun/24 20:40,26/Apr/24 03:32,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"Support DebeziumDeserializationSchema in OceanBase source connector.

 

Related issue: [https://github.com/apache/flink-cdc/issues/2450]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 07:18:23 UTC 2024,,,,,,,,,,"0|z1nw94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 07:18;renqs;flink-cdc master: 05281e5d6f74d1b9a0d2f9845af25a61bdd49066

flink-cdc release-3.1: 23979a716dbe63076d799ff14e12c71279d2ebca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support default value of table column.,FLINK-34638,13571431,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kunni,kunni,11/Mar/24 05:33,17/May/24 03:21,04/Jun/24 20:40,,cdc-3.1.0,,,,,,cdc-3.2.0,,,,Flink CDC,,,,,0,,,,Support default value of table column of literal constant.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-11 05:33:21.0,,,,,,,,,,"0|z1nw48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate JoinConditionEqualityTransferRule,FLINK-34637,13571425,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,11/Mar/24 00:31,11/Mar/24 00:43,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-11 00:31:26.0,,,,,,,,,,"0|z1nw2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Requesting exclusive buffers timeout causes repeated restarts and cannot be automatically recovered,FLINK-34636,13571400,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vincentwoo,vincentwoo,10/Mar/24 10:20,10/Mar/24 10:24,04/Jun/24 20:40,,,,,,,,,,,,Runtime / Network,,,,,0,,,,"Based on the observation of logs and metrics, it was found that a subtask deployed on a same TM consistently reported an exception of requesting exclusive buffers timeout. It was discovered that during the restart process, 【{*}Network{*}】 metric remained unchanged (heap memory usage did change). I suspect that the network buffer memory was not properly released during the restart process, which caused the newly deployed task to fail to obtain the network buffer. This problem persisted despite repeated restarts, and the application failed to recover automatically.

（I'm not sure if there are other reasons for this issue）

Attached below are screenshots of the exception stack and relevant metrics:
{code:java}
2024-03-08 09:58:18,738 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - GroupWindowAggregate switched from DEPLOYING to FAILED with failure cause: java.io.IOException: Timeout triggered when requesting exclusive buffers: The total number of network buffers is currently set to 32768 of 32768 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max',  or you may increase the timeout which is 30000ms by setting the key 'taskmanager.network.memory.exclusive-buffers-request-timeout-ms'.
at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalRequestMemorySegments(NetworkBufferPool.java:246)
at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestPooledMemorySegmentsBlocking(NetworkBufferPool.java:169)
at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.reserveSegments(LocalBufferPool.java:247)
at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setupChannels(SingleInputGate.java:427)  
at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:257)  
at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:84)  
at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:952)  
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:655)  
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)  
at java.lang.Thread.run(Thread.java:748) {code}
!image-20240308101407396.png|width=866,height=171!

Network metric：Only this TM is always 100%, without any variation.

!image-20240308100308649.png|width=868,height=338!

The status of the task deployed to this TM cannot be RUNNING and the status change is slow

!image-20240308101008765.png|width=869,height=118!

Although the root exception thrown by the  application is PartitionNotFoundException, the actual underlying root cause exception log found is IOException: Timeout triggered when requesting exclusive buffers

!image-20240308101934756.png|width=869,height=394!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/24 10:04;vincentwoo;image-20240308100308649.png;https://issues.apache.org/jira/secure/attachment/13067371/image-20240308100308649.png","10/Mar/24 10:04;vincentwoo;image-20240308101008765.png;https://issues.apache.org/jira/secure/attachment/13067370/image-20240308101008765.png","10/Mar/24 10:04;vincentwoo;image-20240308101407396.png;https://issues.apache.org/jira/secure/attachment/13067369/image-20240308101407396.png","10/Mar/24 10:04;vincentwoo;image-20240308101934756.png;https://issues.apache.org/jira/secure/attachment/13067368/image-20240308101934756.png",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-10 10:20:21.0,,,,,,,,,,"0|z1nvxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clear successful records from the batch in JDBC connector,FLINK-34635,13571364,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dsaisharath,dsaisharath,09/Mar/24 09:03,09/Mar/24 09:05,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Connectors / JDBC,,,,,0,,,,"Currently, when batch execution fails in the JDBC connector, the whole batch is retried in the JDBC connector which is unnecessary. We should clear the records that were successful in the [SimpleBatchStatementExecutor|https://github.com/apache/flink-connector-jdbc/blob/main/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/SimpleBatchStatementExecutor.java]


{code:java}
 @Override
    public void executeBatch() throws SQLException {
        if (!batch.isEmpty()) {
            for (T r : batch) {
                parameterSetter.accept(st, r);
                st.addBatch();
            }
            st.executeBatch();
--> catch the exception and clear successful records from the batch here
            batch.clear();
        }
    }{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-09 09:03:32.0,,,,,,,,,,"0|z1nvpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restarting the job will not read the changelog anymore if it stops before the synchronization of meta information is complete and some table is removed,FLINK-34634,13571361,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,loserwang1024,loserwang1024,loserwang1024,09/Mar/24 07:30,08/May/24 03:10,04/Jun/24 20:40,12/Apr/24 02:16,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,"h3. What's the problem

Once, I removed a table from the option and then restarted the job from the savepoint, but the job couldn't read the binlog anymore. When I checked the logs, I found an Error level log stating:

' The enumerator received invalid request meta group id 6, the valid meta group id range is [0, 4].'

It appears that the Reader is requesting more splits than the Enumerator is aware of.

However, the code should indeed remove redundant split information from the Reader as seen in [https://github.com/ververica/flink-cdc-connectors/pull/2292]. So why does this issue occur?

 
h3. why occurs

!image-2024-03-09-15-25-26-187.png|width=751,height=329!

Upon examining the code, I discovered the cause. If the job stops before completing all the split meta information and then restarts, this issue occurs. Suppose that the totalFinishedSplitSize of binlogSplit in the Reader is 6, and no meta information has been synchronized, leaving the finishedSnapshotSplitInfos of binlogSplit in the Reader empty. After restarting, the totalFinishedSplitSize of binlogSplit in the Reader equals (6 - (0 - 0)) which is still 6, but in the Enumerator, it is only 4(the removed table have two split). This could lead to an out-of-range request.

!image-2024-03-09-15-27-46-073.png|width=755,height=305!
h3. How to reproduce
 * Add Thread.sleep(1000L) in com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader#handleSourceEvents to postpone split meta infos synchronization.

{code:java}
public void handleSourceEvents(SourceEvent sourceEvent) {
else if (sourceEvent instanceof BinlogSplitMetaEvent) {
    LOG.debug(
            ""Source reader {} receives binlog meta with group id {}."",
            subtaskId,
            ((BinlogSplitMetaEvent) sourceEvent).getMetaGroupId());
    try {
        Thread.sleep(1000L);
    } catch (InterruptedException e) {
        throw new RuntimeException(e);
    }
    fillMetadataForBinlogSplit((BinlogSplitMetaEvent) sourceEvent);
} {code}
 * Add Thread.sleep(500L) in com.ververica.cdc.connectors.mysql.source.NewlyAddedTableITCase#testRemoveTablesOneByOne to trigger savepoint before meta infos synchronization finishes.

 
{code:java}
// step 2: execute insert and trigger savepoint with all tables added
{
    // ..ingore 

    waitForSinkSize(""sink"", fetchedDataList.size());
    Thread.sleep(500L);
    assertEqualsInAnyOrder(fetchedDataList, TestValuesTableFactory.getRawResults(""sink""));
    finishedSavePointPath = triggerSavepointWithRetry(jobClient, savepointDirectory);
    jobClient.cancel().get();
}

// test removing table one by one, note that there should be at least one table remaining
for (int round = 0; round < captureAddressTables.length - 1; round++) {
...
}

{code}
 
 * Add chunk-meta.group.size  =2 in com.ververica.cdc.connectors.mysql.source.NewlyAddedTableITCase#getCreateTableStatement

Then, run test(com.ververica.cdc.connectors.mysql.source.NewlyAddedTableITCase#testJobManagerFailoverForRemoveTable), the error log will occur.

 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-34666,,,,,,,,,,,,,"09/Mar/24 07:25;loserwang1024;image-2024-03-09-15-25-26-187.png;https://issues.apache.org/jira/secure/attachment/13067363/image-2024-03-09-15-25-26-187.png","09/Mar/24 07:27;loserwang1024;image-2024-03-09-15-27-46-073.png;https://issues.apache.org/jira/secure/attachment/13067362/image-2024-03-09-15-27-46-073.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 12 02:16:32 UTC 2024,,,,,,,,,,"0|z1nvoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/24 02:27;loserwang1024;[~ruanhang1993] , [~renqs] , [~Leonard] , CC;;;","15/Mar/24 03:21;loserwang1024;Please assign it to me.;;;","12/Apr/24 02:16;renqs;flink-cdc master: 48ca8623bb8fa405adb56dbe505dbad10902db89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support unnesting array constants,FLINK-34633,13571354,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jeyhunkarimov,xccui,xccui,09/Mar/24 01:17,18/Apr/24 07:12,04/Jun/24 20:40,18/Apr/24 02:28,1.18.1,,,,,,1.19.1,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"It seems that the current planner doesn't support using UNNEST on array constants.(x)
{code:java}
SELECT * FROM UNNEST(ARRAY[1,2,3]);{code}
 
The following query can't be compiled.(x)
{code:java}
SELECT * FROM (VALUES('a')) CROSS JOIN UNNEST(ARRAY[1, 2, 3]){code}
 
The rewritten version works. (/)
{code:java}
SELECT * FROM (SELECT *, ARRAY[1,2,3] AS A FROM (VALUES('a'))) CROSS JOIN UNNEST(A){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 18 02:27:05 UTC 2024,,,,,,,,,,"0|z1nvn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/24 02:27;qingyue;Fixed in master 43a3d50ce3982b9abf04b81407fed46c5c25f819;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log checkpoint Id when logging checkpoint processing delay,FLINK-34632,13571353,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liuml07,liuml07,liuml07,09/Mar/24 01:10,11/Mar/24 09:18,04/Jun/24 20:40,11/Mar/24 09:18,1.18.1,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,Currently we log a warning message when the checkpoint barrier takes too long to start processing. It has the delay and would be easier for debugging respective checkpoint if the id is also logged.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 09:18:25 UTC 2024,,,,,,,,,,"0|z1nvmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/24 01:12;liuml07;CC [~markcho];;;","11/Mar/24 09:18;masteryhx;merged 22309fdc into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible memory leak in pyflink when using state with RocksDB ,FLINK-34631,13571325,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,marklidenberg,marklidenberg,08/Mar/24 15:11,08/Mar/24 15:52,04/Jun/24 20:40,08/Mar/24 15:52,1.18.1,,,,,,,,,,API / Python,,,,,0,,,,"I have had issues with memory constantly growing in our production pipelines on pyflink task managers until they crash, which should not really happen when we use RocksDB as our state backend. 

I've made a simple example to demonstrate the possible memory leak. In this example I update state with 1mb value for each key and then sleep for 1 second. Memory growth 1mb per second until the process crashes, as if the state value stays in memory. Same thing happens if I send 100 messages per second with 10kb each. I've also tested `MapState`, it's the same. 

Either there is a memory leak, or my setup with default RocksDB configuration just doesn't fit the example. 

 

```python 
import time

import psutil

from pyflink.common import Types
from pyflink.datastream import (
    EmbeddedRocksDBStateBackend,
    KeyedProcessFunction,
    RuntimeContext,
    StreamExecutionEnvironment,
)
from pyflink.datastream.state import ValueStateDescriptor

class Processor(KeyedProcessFunction):
    def open(self, runtime_context: RuntimeContext):
        self.state = runtime_context.get_state(
            ValueStateDescriptor(
                name=""my_state"",
                value_type_info=Types.STRING(),
            )
        )

    def process_element(self, value: int, ctx: KeyedProcessFunction.Context):
        print(""Processing"", value, ""Memory: "", round(psutil.Process().memory_info().rss / 1024 / 1024, 2), ""MB"")

        # Processing 1 Memory:  171.25 MB -> Processing 2 Memory:  172.12 MB -> ... grows 1mb per second, which should not happen because we use RocksDB as state backend
        self.state.update(""a"" * 1_000_000)  # 1 mb of data per second
        time.sleep(1.0)

if {_}{{_}}name{{_}}{_} == ""{_}{{_}}main{{_}}{_}"":
    # - Create flink environment

    environment = StreamExecutionEnvironment.get_execution_environment().set_parallelism(1)

    # - Make sure to use RocksDB as state backend

    environment.set_state_backend(EmbeddedRocksDBStateBackend())

    # - Create pipeline

    (
        environment.from_collection(
            collection=list(range(3600 * 12)),
        )
        .key_by(lambda value: value)
        .process(Processor())
    )

    # - Execute pipeline

    environment.execute(job_name=""memory_leak_test"")

```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 15:52:19 UTC 2024,,,,,,,,,,"0|z1nvgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 15:52;marklidenberg;After further investigation and testing, I've discovered that apparently the issue was due to a mistake in my test setup. The provided example works fine, on appr. 2gb of memory in my setup it starts dropping, so everything is functioning as expected.

I'll look deeper into our production memory problems. 

I am closing this ticket for now. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar source lost topic subscribe,FLINK-34630,13571312,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,wangminchao,wangminchao,08/Mar/24 12:21,08/Mar/24 12:49,04/Jun/24 20:40,08/Mar/24 12:49,pulsar-3.0.1,,,,,,,,,,Connectors / Pulsar,,,,,0,,,,"The non-partition pulsar topic partition id is `-1`, using multiples of the non-partition topics  
 in Pulsar source maybe lose topic subscribe.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-08 12:21:05.0,,,,,,,,,,"0|z1nvds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar source lost topic subscribe,FLINK-34629,13571311,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangminchao,wangminchao,wangminchao,08/Mar/24 12:19,03/May/24 06:59,04/Jun/24 20:40,15/Apr/24 05:49,pulsar-3.0.1,,,,,,pulsar-4.2.0,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,"The non-partition pulsar topic partition id is `-1`, using multiples of the non-partition topics  
 in Pulsar source maybe lose topic subscribe.

",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34627,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 05:49:40 UTC 2024,,,,,,,,,,"0|z1nvdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 12:33;wangminchao;Fix https://github.com/apache/flink-connector-pulsar/pull/84;;;","11/Mar/24 08:27;mapohl;I re-opened that one in favor of FLINK-34627 because it appears to be the one that is used to fix the issue.

[~wangminchao] Thanks for reporting issues. But please be mindful when doing this. It appears that you created the same issue 5 times;;;","15/Apr/24 05:49;tison;Master via https://github.com/apache/flink-connector-pulsar/commit/7a5eef268cb3f598589ad9cc32648ac92fbbee1d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar source lost topic subscribe,FLINK-34628,13571310,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,wangminchao,wangminchao,08/Mar/24 12:18,08/Mar/24 12:49,04/Jun/24 20:40,08/Mar/24 12:49,pulsar-3.0.1,,,,,,,,,,Connectors / Pulsar,,,,,0,,,,"The non-partition pulsar topic partition id is `-1`, using multiples of the non-partition topics  
 in Pulsar source maybe lose topic subscribe.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-08 12:18:38.0,,,,,,,,,,"0|z1nvdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar source lost topic subscribe,FLINK-34627,13571309,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,wangminchao,wangminchao,08/Mar/24 12:18,11/Mar/24 08:28,04/Jun/24 20:40,11/Mar/24 08:28,pulsar-3.0.1,,,,,,,,,,Connectors / Pulsar,,,,,0,,,,"The non-partition pulsar topic partition id is `-1`, using multiples of the non-partition topics  
 in Pulsar source maybe lose topic subscribe.

",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34629,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 08:28:08 UTC 2024,,,,,,,,,,"0|z1nvd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/24 08:28;mapohl;I'm closing this one as well in favor of the reopened duplicate FLINK-34629. It looks like the contributor decided to create the contributions under FLINK-34629.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar source lost topic subscribe,FLINK-34626,13571308,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,wangminchao,wangminchao,08/Mar/24 12:18,08/Mar/24 12:50,04/Jun/24 20:40,08/Mar/24 12:50,pulsar-3.0.1,,,,,,,,,,Connectors / Pulsar,,,,,0,,,,"The non-partition pulsar topic partition id is `-1`, using multiples of the non-partition topics  
 in Pulsar source maybe lose topic subscribe.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-08 12:18:11.0,,,,,,,,,,"0|z1nvcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TTL doesn't seem to work in pyflink ,FLINK-34625,13571303,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,marklidenberg,marklidenberg,08/Mar/24 11:54,20/Mar/24 06:59,04/Jun/24 20:40,,1.18.1,,,,,,,,,,API / Python,,,,,0,,,,"I've made a simple example to test the ttl and couldn't get the expected results. I went further and replicated this example in Java and it worked just fine. There is an inconsistency in behavior, so there is something wrong in pyflink or my pyflink setup. 

Here is a code to reproduce. In the example I create a state with ttl 1 second and then process events every 1.5 seconds and print current state.  I expect it to print `None, None, None, ...` (because ttl expires after 1.5 seconds), but instead it prints `None, ""state"", ""state, ...`. In Java it works as expected, prints `Null, Null, ...`

```python

import time

from pyflink.common import Time, Types
from pyflink.datastream import KeyedProcessFunction, RuntimeContext, StreamExecutionEnvironment
from pyflink.datastream.state import StateTtlConfig, ValueStateDescriptor


class Processor(KeyedProcessFunction):
    def open(self, runtime_context: RuntimeContext):
        state_descriptor = ValueStateDescriptor(
            name=""my_state"",
            value_type_info=Types.STRING(),
        )

        state_descriptor.enable_time_to_live(
            ttl_config=StateTtlConfig.new_builder(Time.seconds(1))
            .set_update_type(StateTtlConfig.UpdateType.OnCreateAndWrite)
            .set_state_visibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
            .build()
        )

        self.state = runtime_context.get_state(state_descriptor)

    def process_element(self, value: int, ctx: KeyedProcessFunction.Context):
        # Print current state
        print(self.state.value())
        # expect to print `None` all the time, but prints: `None, 'state', 'state', ...` instead

        # Update state
        self.state.update(""state"")

        # sleep to reset the state
        time.sleep(1.5)


if __name__ == ""__main__"":
    # Init environment
    environment = StreamExecutionEnvironment.get_execution_environment().set_parallelism(1)

    # Setup pipeline
    (
        environment.from_collection(
            collection=list(range(10)),
        )
        .key_by(lambda value: 0)
        .process(Processor())
    )

    # Execute pipeline
    environment.execute(""ttl_test"")

```

 

```java

import org.apache.flink.api.common.state.StateTtlConfig;
import org.apache.flink.api.common.state.ValueState;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.api.common.time.Time;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.metrics.Histogram;
import org.apache.flink.streaming.api.functions.KeyedProcessFunction;
import org.apache.flink.util.Collector;

import java.io.IOException;
import java.time.LocalDateTime;

public class Processor extends KeyedProcessFunction<Integer, String, String> {

    private transient ValueState<String> state;


    @Override
    public void open(Configuration parameters) {
        var stateTtlConfig = StateTtlConfig
                .newBuilder(Time.seconds(1))
                .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
                .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
                .build();

        var stateDescriptor = new ValueStateDescriptor<>(""state"", String.class);
        stateDescriptor.enableTimeToLive(stateTtlConfig);
        state = getRuntimeContext().getState(stateDescriptor);

    }

    @Override
    public void processElement(String event, Context context, Collector<String> collector) throws IOException, InterruptedException {
        // print state
        var state = state.value();
        System.out.println(state); # prints `Null, Null, ...` 

        // update state
        state.update(LocalDateTime.now().toString());

        // sleep to reset the state
        Thread.sleep(1500);
    }
}

```",Image used: flink:1.18.1-scala_2.12-java11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 06:59:05 UTC 2024,,,,,,,,,,"0|z1nvbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 12:08;martijnvisser;[~dianfu] [~hxbks2ks] Any thoughts?;;;","20/Mar/24 06:59;polorad;Hello, any updates on this task? The issue remains a priority for us.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable local recovery in ChangelogRescalingITCase,FLINK-34624,13571292,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,08/Mar/24 10:42,13/Mar/24 06:12,04/Jun/24 20:40,12/Mar/24 03:29,,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,"Enable local recovery in ChangelogRescalingITCase, since the local recovery of changelog state backend was supported by FLINK-27693 and FLINK-27692.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34660,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 12 03:29:38 UTC 2024,,,,,,,,,,"0|z1nv9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/24 03:29;Yanfei Lei;Merged via 94b55d1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink creating main.jar files in jobmanager jars upload folder,FLINK-34623,13571273,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nikhil_d,nikhil_d,08/Mar/24 08:20,19/Mar/24 06:20,04/Jun/24 20:40,,,,,,,,,,,,flink-docker,,,,,0,,,,"Flink is creating a main.jar and uploading it to jars upload folder. This would cause lot of problems like filling up jars folder, filling up the metaspace.

!image-2024-03-08-17-59-25-982.png|width=697,height=347!

Can we understand why this is happening and is there any way to resolve this through Flink config or set up.


Note: We're using bitnami flink helm chart. 
[https://github.com/bitnami/charts/tree/main/bitnami/flink]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/24 12:29;nikhil_d;image-2024-03-08-17-59-25-982.png;https://issues.apache.org/jira/secure/attachment/13067349/image-2024-03-08-17-59-25-982.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 06:20:49 UTC 2024,,,,,,,,,,"0|z1nv54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 06:20;nikhil_d;Hi,
Can someone check and update on this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo of execution_mode configuration name in Chinese document,FLINK-34622,13571260,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,Yu Chen,Yu Chen,08/Mar/24 06:41,08/Mar/24 09:22,04/Jun/24 20:40,08/Mar/24 09:22,,,,,,,1.18.2,1.19.1,1.20.0,,Documentation,,,,,0,pull-request-available,,,"!image-2024-03-08-14-46-34-859.png|width=794,height=380!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/24 06:46;Yu Chen;image-2024-03-08-14-46-34-859.png;https://issues.apache.org/jira/secure/attachment/13067338/image-2024-03-08-14-46-34-859.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 09:22:39 UTC 2024,,,,,,,,,,"0|z1nv28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 09:22;yunta;merged
master: 17487c0c944c3925b89b26eadf38169da35410f7
release-1.19: 0a85a08303ced5715437ead15ce60203c70aa58d
release-1.18: ff256ef85f4edf4e86ce2bd73e4bdef8b7e07fbb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump com.google.guava:guava from 31.1-jre to 32.0.0-jre in /flink-connector-hbase-base,FLINK-34621,13571205,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,07/Mar/24 20:19,07/Mar/24 23:27,04/Jun/24 20:40,07/Mar/24 23:27,,,,,,,hbase-4.0.0,,,,Connectors / HBase,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 07 23:27:23 UTC 2024,,,,,,,,,,"0|z1nuq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/24 23:27;martijnvisser;Fixed in apache/flink-connector-hbase:main 3e7bb17e2489f4ddbb93061d72f726b33e24f22c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Process recursive protobuf schemas in PbToRowTypeUtil,FLINK-34620,13571157,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wckdman,wckdman,07/Mar/24 15:02,01/Apr/24 16:25,04/Jun/24 20:40,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,"The current version of _PbToRowTypeUtil.generateRowType_ doesn't support recursive protobuf schemas like
{code:java}
message RecursiveMessage {
int32 id = 1;
RecursiveMessage message = 2;
} {code}
It throws _java.lang.StackOverflowError._

This could be implemented by trimming the recursion. Same approach is used in Apache Spark, for example. [GitHub|https://github.com/apache/spark/blob/master/connector/protobuf/src/main/scala/org/apache/spark/sql/protobuf/utils/SchemaConverters.scala#L178]

 

then output Table schemas for the above proto-schema will be like this:
{code:java}
recursiveFieldMaxDepth=0: message: ROW< id: INTEGER > 
recursiveFieldMaxDepth=1: message: ROW< id: INTEGER, message: ROW< id: INTEGER > > recursiveFieldMaxDepth=2: message: ROW< id: INTEGER, message: ROW< id: INTEGER, message: ROW< id: INTEGER > > > {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 16:25:26 UTC 2024,,,,,,,,,,"0|z1nufk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/24 15:07;wckdman;Opened a PR https://github.com/apache/flink/pull/24466;;;","01/Apr/24 16:25;wckdman;[~libenchao] Hello!

Could you take a look on this small feature and assign this ticket to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not wait for scaling completion in UPGRADE state with in-place scaling,FLINK-34619,13571144,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,07/Mar/24 13:04,14/Mar/24 14:29,04/Jun/24 20:40,14/Mar/24 14:29,kubernetes-operator-1.7.0,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"The operator currently puts the resource into upgrading state after triggering in-place scaling and keeps observing until the desired parallelism is reached before moving to deployed / stable. 

However this means that due to how the adaptive scheduler works this parallelism may never be reached and this is expected.

We should simplify the logic to consider scaling ""done"" once the resource requirements have been set correctly and then leave the rest to the adaptive scheduler",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 13:04:37.0,,,,,,,,,,"0|z1nuco:",9223372036854775807,Merged via 8938658ed245545e6436ff22cbb8b2fabd4047f1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate SplitPythonConditionFromJoinRule,FLINK-34618,13571143,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,damumu,damumu,07/Mar/24 12:57,13/Mar/24 02:14,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 12:57:04.0,,,,,,,,,,"0|z1nucg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the Javadoc of org.apache.flink.api.common.time.Time,FLINK-34617,13571139,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,07/Mar/24 11:47,08/Mar/24 09:23,04/Jun/24 20:40,08/Mar/24 09:23,1.19.0,,,,,,1.19.1,1.20.0,,,Documentation,,,,,0,pull-request-available,,,"The current Javadoc of {{org.apache.flink.api.common.time.Time}} said it will fully replace {{org.apache.flink.streaming.api.windowing.time.Time}} in Flink 2.0. However, the {{Time}} class has been deprecated, and we should remove the description.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 09:23:21 UTC 2024,,,,,,,,,,"0|z1nubk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 09:23;yunta;merged
master: 9617598de33b2b23b97ddb84887392659070c344
release-1.19: c6d96b7f7c07faad363779a5175d5772140891a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python dist doesn't clean when open method construct resource,FLINK-34616,13571127,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,07/Mar/24 09:57,08/Mar/24 01:43,04/Jun/24 20:40,08/Mar/24 01:43,1.18.1,1.19.0,,,,,1.18.2,1.19.1,1.20.0,,API / Python,,,,,0,pull-request-available,,,"our enviroment found lots of python-dist causing disk full.

the main resource is
constructEnvironmentVariables -> constructArchivesDirectory -> CompressionUtils.extractFile which has 
ClosedByInterruptException Exception and the root exception has lost. we found it by arthas.
and it will not run the clean dir logic
 
2024-03-07 18:19:34,265 ERROR [[vertex-1]MiniBatchAssigner(interval=[5000ms], mode=[ProcTime]) -> PythonCalc(select=[content, sourc (18/128)#31] org.apache.flink.python.env.AbstractPythonEnvironmentManager [] - Error when create resource.
java.nio.channels.ClosedByInterruptException: null
at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:199) ~[?:?]
at sun.nio.ch.FileChannelImpl.endBlocking(FileChannelImpl.java:162) ~[?:?]
at sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:816) ~[?:?]
at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:796) ~[?:?]
at org.apache.commons.compress.archivers.zip.ZipFile$BoundedFileChannelInputStream.read(ZipFile.java:1420) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.commons.compress.utils.BoundedArchiveInputStream.read(BoundedArchiveInputStream.java:82) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at java.io.BufferedInputStream.fill(BufferedInputStream.java:252) ~[?:?]
at java.io.BufferedInputStream.read1(BufferedInputStream.java:292) ~[?:?]
at java.io.BufferedInputStream.read(BufferedInputStream.java:351) ~[?:?]
at java.io.SequenceInputStream.read(SequenceInputStream.java:199) ~[?:?]
at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:243) ~[?:?]
at org.apache.commons.compress.archivers.zip.InflaterInputStreamWithStatistics.fill(InflaterInputStreamWithStatistics.java:52) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:159) ~[?:?]
at org.apache.commons.compress.archivers.zip.InflaterInputStreamWithStatistics.read(InflaterInputStreamWithStatistics.java:67) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at java.io.FilterInputStream.read(FilterInputStream.java:107) ~[?:?]
at org.apache.flink.util.IOUtils.copyBytes(IOUtils.java:61) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.util.IOUtils.copyBytes(IOUtils.java:86) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.python.util.CompressionUtils.extractZipFileWithPermissions(CompressionUtils.java:223) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.python.util.CompressionUtils.extractFile(CompressionUtils.java:61) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.python.env.AbstractPythonEnvironmentManager.constructArchivesDirectory(AbstractPythonEnvironmentManager.java:365) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.python.env.AbstractPythonEnvironmentManager.constructEnvironmentVariables(AbstractPythonEnvironmentManager.java:178) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.python.env.AbstractPythonEnvironmentManager.lambda$open$0(AbstractPythonEnvironmentManager.java:126) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.python.env.AbstractPythonEnvironmentManager$PythonEnvResources.createResource(AbstractPythonEnvironmentManager.java:468) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.python.env.AbstractPythonEnvironmentManager$PythonEnvResources.getOrAllocateSharedResource(AbstractPythonEnvironmentManager.java:435) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.python.env.AbstractPythonEnvironmentManager.open(AbstractPythonEnvironmentManager.java:114) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.streaming.api.operators.python.AbstractEmbeddedPythonFunctionOperator.open(AbstractEmbeddedPythonFunctionOperator.java:68) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.table.runtime.operators.python.scalar.EmbeddedPythonScalarFunctionOperator.open(EmbeddedPythonScalarFunctionOperator.java:127) ~[flink-python_2.12-1.15.2-SNAPSHOT-jar-with-dependencies.jar:1.15.2-SNAPSHOT]
at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:703) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:679) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:646) ~[flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:963) [flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932) [flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:756) [flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:578) [flink-dist_2.12-1.15.2-SNAPSHOT.jar:1.15.2-SNAPSHOT]
at java.lang.Thread.run(Thread.java:991) [?:?]
 

 

!image-2024-03-07-17-58-06-493.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/24 09:58;jackylau;image-2024-03-07-17-58-06-493.png;https://issues.apache.org/jira/secure/attachment/13067319/image-2024-03-07-17-58-06-493.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 01:43:04 UTC 2024,,,,,,,,,,"0|z1nu8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 01:43;hxb;Merged into master via 21306f4f5dbcc72a2cde2f15e6c072951aa03f49

Merged into release-1.19 via 75c88fa4f19d3f703e0cce3b917a9aa070eadffe

Merged into release-1.18 via 31f13614c5e1bccbcfc14f31561aac3892b86e85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move ExternalizedCheckpointCleanup to flink-core,FLINK-34615,13571115,13566461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,spoon-lz,spoon-lz,spoon-lz,07/Mar/24 07:52,17/May/24 07:25,04/Jun/24 20:40,17/May/24 07:25,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,  Split `ExternalizedCheckpointCleanup` out of `CheckpointConfig` and move it to flink-core module,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 17 07:25:40 UTC 2024,,,,,,,,,,"0|z1nu68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/24 07:25;zakelly;Merged into master via f5a8edbcb911c2247c83dae104bbe82adf4910e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add e2e test for Flink CDC pipeline job.,FLINK-34614,13571107,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kunni,kunni,kunni,07/Mar/24 07:01,17/May/24 03:19,04/Jun/24 20:40,17/May/24 03:19,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,,,,Add e2e test for Flink CDC pipeline job.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 17 03:16:31 UTC 2024,,,,,,,,,,"0|z1nu4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/24 03:16;kunni;done in pull-request https://github.com/apache/flink-cdc/pull/2997.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow specifing a savepoint file to restart using cli script.,FLINK-34613,13571105,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kunni,kunni,kunni,07/Mar/24 06:47,31/May/24 01:17,04/Jun/24 20:40,09/Apr/24 07:37,cdc-3.1.0,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,Allow user to specify a savepoint file with `-s` option to restart like Flink cli [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/cli/#starting-a-job-from-a-savepoint] does.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 07:37:24 UTC 2024,,,,,,,,,,"0|z1nu40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Apr/24 07:37;renqs;flink-cdc master: b10a666e3ebfc1a8fd0ac50167bee5cbf39bf84a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate StreamPhysicalValuesRule,FLINK-34612,13571101,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 05:45,07/Mar/24 06:09,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 05:45:46.0,,,,,,,,,,"0|z1nu34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate StreamPhysicalUnionRule,FLINK-34611,13571100,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 05:45,07/Mar/24 06:10,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 05:45:15.0,,,,,,,,,,"0|z1nu2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate StreamPhysicalLimitRule,FLINK-34610,13571099,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 05:44,07/Mar/24 06:12,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 05:44:46.0,,,,,,,,,,"0|z1nu2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate BatchPhysicalScriptTransformRule,FLINK-34609,13571098,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 05:44,07/Mar/24 06:31,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 05:44:01.0,,,,,,,,,,"0|z1nu2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate BatchPhysicalValuesRule,FLINK-34608,13571092,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 03:29,07/Mar/24 06:12,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 03:29:17.0,,,,,,,,,,"0|z1nu14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate BatchPhysicalUnionRule,FLINK-34607,13571091,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 03:28,07/Mar/24 06:13,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 03:28:48.0,,,,,,,,,,"0|z1nu0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate BatchPhysicalExpandRule,FLINK-34606,13571090,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 03:27,07/Mar/24 06:14,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 03:27:41.0,,,,,,,,,,"0|z1nu0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate BatchPhysicalDistributionRule,FLINK-34605,13571089,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 03:27,07/Mar/24 06:14,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 03:27:08.0,,,,,,,,,,"0|z1nu0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate BatchPhysicalCalcRule,FLINK-34604,13571088,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 03:26,07/Mar/24 06:16,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 03:26:43.0,,,,,,,,,,"0|z1nu08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate BatchPhysicalBoundedStreamScanRule,FLINK-34603,13571087,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,07/Mar/24 03:26,07/Mar/24 06:20,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 03:26:07.0,,,,,,,,,,"0|z1nu00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate StreamPhysicalWindowJoinRule,FLINK-34602,13571076,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,07/Mar/24 00:19,07/Mar/24 00:26,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-07 00:19:01.0,,,,,,,,,,"0|z1ntxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate StreamPhysicalConstantTableFunctionScanRule,FLINK-34601,13571074,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/24 23:55,10/Mar/24 19:10,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 23:55:20.0,,,,,,,,,,"0|z1ntx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate PushLimitIntoLegacyTableSourceScanRule,FLINK-34600,13571073,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/24 23:47,06/Mar/24 23:58,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 23:47:17.0,,,,,,,,,,"0|z1ntww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate BatchPhysicalConstantTableFunctionScanRule,FLINK-34599,13571071,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/24 23:24,06/Mar/24 23:37,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 23:24:30.0,,,,,,,,,,"0|z1ntwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate RemoveRedundantLocalRankRule,FLINK-34598,13571066,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/24 21:59,06/Mar/24 22:08,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 21:59:23.0,,,,,,,,,,"0|z1ntvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate SimplifyFilterConditionRule,FLINK-34597,13571062,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/24 21:36,06/Mar/24 21:47,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 21:36:41.0,,,,,,,,,,"0|z1ntug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate RemoveRedundantLocalHashAggRule,FLINK-34596,13571053,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/24 20:19,06/Mar/24 20:28,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 20:19:35.0,,,,,,,,,,"0|z1ntsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ClassNotFoundException: com.ververica.cdc.common.utils.StringUtils,FLINK-34595,13571028,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kunni,kunni,06/Mar/24 16:31,28/May/24 01:54,04/Jun/24 20:40,,,,,,,,cdc-3.2.0,,,,Flink CDC,,,,,0,,,,"In [this pr|[https://github.com/apache/flink-cdc/pull/2986/files#diff-cec13810c47e9465e4f2a72507f655b86f41579768b9924fe024aabc60b31d17R21|https://github.com/apache/flink-cdc/pull/2986/files#diff-cec13810c47e9465e4f2a72507f655b86f41579768b9924fe024aabc60b31d17R21[]], we introduced 
org.apache.flink.cdc.common.utils.StringUtils class of flink-cdc-common module in flink-connector-mysql-cdc module.
However, the sub module flink-sql-connector-mysql-cdc doesn't include 
flink-cdc-common module when packaging, so we can't find this class in sql jar.. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 01:43:52 UTC 2024,,,,,,,,,,"0|z1ntmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 01:43;kwafor;[~kunni] Looks like closed by [https://github.com/apache/flink-cdc/pull/3118].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Vote on the release candidate,FLINK-34594,13571009,13571005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,06/Mar/24 15:49,15/Mar/24 04:27,04/Jun/24 20:40,15/Mar/24 04:27,1.19.0,,,,,,1.19.0,,,,,,,,,0,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 15 04:27:08 UTC 2024,,,,,,,,,,"0|z1ntio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/24 06:01;lincoln.86xy;Voting thread: https://lists.apache.org/thread/x61673q3ro52jvwj137hct2bqowzcy2v;;;","15/Mar/24 04:27;lincoln.86xy;vote result https://lists.apache.org/thread/stps3d8m6hsny5mglbr4h09879ny5nog;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Propose a pull request for website updates,FLINK-34593,13571008,13571005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,06/Mar/24 15:49,19/Mar/24 04:30,04/Jun/24 20:40,19/Mar/24 04:30,1.19.0,,,,,,1.19.0,,,,,,,,,0,pull-request-available,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 # update [apache/flink-web:_config.yml|https://github.com/apache/flink-web/blob/asf-site/_config.yml]
 ## update {{FLINK_VERSION_STABLE}} and {{FLINK_VERSION_STABLE_SHORT}} as required
 ## update version references in quickstarts ({{{}q/{}}} directory) as required
 ## (major only) add a new entry to {{flink_releases}} for the release binaries and sources
 ## (minor only) update the entry for the previous release in the series in {{flink_releases}}
 ### Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number.
 ## add a new entry to {{release_archive.flink}}
 # add a blog post announcing the release in _posts
 # add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/]). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project.

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]
 * (major only) Check {{docs/config.toml}} to ensure that
 ** the version constants refer to the new version
 ** the {{baseurl}} does not point to {{flink-docs-master}}  but {{flink-docs-release-X.Y}} instead",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34533,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 04:30:10 UTC 2024,,,,,,,,,,"0|z1ntig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/24 04:30;lincoln.86xy;release note

master: 875a431950da066aa696f2c3a335f3267b85194a

1.19: 511814bbc44ba0cc2a56147a29240445d571106b

 

web pr

46b47589e5a77906febed94a17e2a2ba879e1015;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Stage source and binary releases on dist.apache.org,FLINK-34592,13571007,13571005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,06/Mar/24 15:49,12/Mar/24 06:02,04/Jun/24 20:40,12/Mar/24 06:02,,,,,,,,,,,,,,,,0,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 12 06:02:12 UTC 2024,,,,,,,,,,"0|z1nti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/24 06:02;lincoln.86xy;[https://dist.apache.org/repos/dist/dev/flink/flink-1.19.0-rc2]/  

[https://repository.apache.org/content/repositories/orgapacheflink-1709];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Build and stage Java and Python artifacts,FLINK-34591,13571006,13571005,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,06/Mar/24 15:49,12/Mar/24 06:02,04/Jun/24 20:40,12/Mar/24 06:02,,,,,,,,,,,,,,,,0,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 12 06:02:45 UTC 2024,,,,,,,,,,"0|z1nti0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Mar/24 06:02;lincoln.86xy;[https://dist.apache.org/repos/dist/dev/flink/flink-1.19.0-rc2];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Release Candidate: 1.19.0-rc2,FLINK-34590,13571005,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,06/Mar/24 15:49,19/Mar/24 04:30,04/Jun/24 20:40,19/Mar/24 04:30,1.19.0,,,,,,1.19.0,,,,,,,,,0,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.

h4. Prerequisites
Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code}
RC_NUM=""1""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,,,,,,,,,,,,,FLINK-34530,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 15:49:46.0,,,,,,,,,,"0|z1nths:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManager doesn't handle errors in the resource reconcilliation step,FLINK-34589,13571004,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,06/Mar/24 15:49,07/Mar/24 06:37,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,Runtime / Coordination,,,,,0,,,,"I noticed during my work on FLINK-34427 that the reconcilliation is scheduled periodically when starting the {{SlotManager}}. But it doesn't handle errors in this step. I see two options here:
1. Fail fatally because such an error might indicate a major issue with the RM backend.
2. Log the failure and continue the scheduled task even in case of an error.

My understanding is that we're just not able to recreate TaskManagers which should be a transient issue and could be resolved in the backend (YARN, k8s). That's why I would lean towards option 2.

[~xtsong] WDYT?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34427,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 07 06:37:30 UTC 2024,,,,,,,,,,"0|z1nthk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/24 04:14;xtsong;[~mapohl], thanks for reaching out.

Do you already see errors thrown from the reconciliation? Or you just noticed the absence of safe-net but don't observe any errors so far? Just trying to understand what possible errors would there be.

My understanding is that, ideally we expect no exceptions thrown from the reconciliation? If it does, then there might be some possibilities that we are not yet aware of. In such case, I'd be in favor of fail eagerly so that we don't ignore the problem. Thus, I'd be in favor of option 1.

But it's not a strong preference and I'd also be fine with option 2.;;;","07/Mar/24 06:37;mapohl;No, I didn't observe errors. I raised this issue just because I came across it when reading the code. I guess, if we're not expecting any failures, adding a {{FutureUtils#handleUncaughtException}} would be the best option. Thanks for the clarification. I'm gonna transformed the issue to {{Technical Debt}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManager checks whether resources need to reconcile but doesn't act on the result,FLINK-34588,13570997,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,06/Mar/24 15:32,11/Mar/24 11:16,04/Jun/24 20:40,,1.18.1,1.19.0,1.20.0,,,,,,,,Runtime / Coordination,,,,,0,,,,"There are a few locations in {{FineGrainedSlotManager}} where we check whether resources can/need to be reconciled but don't care about the result and just trigger the resource update (e.g. in [FineGrainedSlotManager:626|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/FineGrainedSlotManager.java#L626] and [FineGrainedSlotManager:682|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/FineGrainedSlotManager.java#L682]). Looks like we could reduce the calls to the backend here.

It's not having a major impact because this feature is only used in the {{ActiveResourceManager}} which triggers [checkResourceDeclarations|https://github.com/apache/flink/blob/c678244a3890273145a786b9e1bf1a4f96f6dcfd/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/active/ActiveResourceManager.java#L331] and reevaluates the {{resourceDeclarations}}. Not sure whether I missed something here and there's actually a bigger issue with it. But considering that nobody complained about it in the past, I'd assume that it's not a severe issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31443,,,,,,,FLINK-34427,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 11:16:38 UTC 2024,,,,,,,,,,"0|z1ntg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 15:36;mapohl;cc [~huwh];;;","06/Mar/24 16:20;gyfora;The links in the description don't seem to work :/ ;;;","07/Mar/24 06:42;mapohl;Sorry for that. I updated the links. They should work now. For the record: This was also just observed in a code review. I'm not aware of any actual issues that arise from this.;;;","08/Mar/24 05:24;huweihua;Thanks [~mapohl]  reporting this. 

At the first time. the function `checkResourcesNeedReconcile` is called `checkTaskManagerReleasable`, it is only responsible for release idle task managers. So we only care the result of `checkTaskManagerReleasable` in release path([Line 816|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/FineGrainedSlotManager.java#L816]). 

In [FLINK-32880|https://issues.apache.org/jira/browse/FLINK-32880], we change it to `checkResourcesNeedReconcile` and let it check whether need to allocate redundant task manager.


There are two functions to allocate/release task managers now. 

`checkResourcesNeedReconcile`: allocate redundant task manager and release idle task manager

`checkResourceRequirements`: allocate task manager for job requirement

So, in periodic check of `checkClusterReconciliation`, we take the result of `checkResourcesNeedReconcile` in account because we don't try to fulfill the job requirement here. In other place we ignore the result of `checkResourcesNeedReconcile` because `checkResourceRequirements` may also allocate/release taskmanagers.

 

 ;;;","11/Mar/24 11:16;mapohl;Ok, thanks for clarification. I might add this information as comments to my FLINK-34427 PR. (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support MODE aggregate function,FLINK-34587,13570967,13076759,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Mar/24 11:11,18/Mar/24 11:18,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,"There is {{AVG}} function however sometimes it is required to find the most frequent item and {{MODE}} could be helpful to solve this

it is already supported by a number of vendors like DuckDB[1], Snowflake[2], Spark [3], PostgreSQL[4]

It also seems that vendors ignore {{NULL}} values and if there is no values at all or no not {{NULL}}  values the result will be {{NULL}}

[1] https://duckdb.org/docs/sql/aggregates.html
[2] https://docs.snowflake.com/en/sql-reference/functions/mode
[3] https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html
[4] https://www.postgresql.org/docs/16/functions-aggregate.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 11:11:29.0,,,,,,,,,,"0|z1nt9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the README in Flink CDC,FLINK-34586,13570954,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ruanhang1993,ruanhang1993,06/Mar/24 09:19,06/Mar/24 09:19,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,,,,We should update the README file in Flink CDC.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-06 09:19:10.0,,,,,,,,,,"0|z1nt6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: Flink CDC,FLINK-34585,13570922,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,kunni,ruanhang1993,ruanhang1993,06/Mar/24 06:49,30/May/24 09:45,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,,,,Most tests in Flink CDC are still using Junit 4. We need to use Junit 5 instead.,,,,,,,,,,,,,,,,FLINK-35490,FLINK-35491,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 09:39:07 UTC 2024,,,,,,,,,,"0|z1nszc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/24 01:43;kunni;I am willing to take this.;;;","15/Mar/24 10:16;jiabaosun;Thanks [~kunni] for volunteering.
Assigned to you.;;;","30/May/24 09:39;m.orazow;Hey all,

Since this is lots of effort, I suggest we divide this ticket into two parts. With the following modules for each ticket:


First ticket to include the following module:
 * flink-cdc-connect/flink-cdc-source-connectors

And the second ticket to include the following modules:
 * flink-cdc-common
 * flink-cdc-composer
 * flink-cdc-runtime
 * flink-cdc-connect/flink-cdc-pipeline-connectors
 * flink-cdc-e2e-tests


Because the `flink-cdc-source-connectors` is big part that contains the Junit4 tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change package name to org.apache.flink.cdc,FLINK-34584,13570916,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,ruanhang1993,ruanhang1993,ruanhang1993,06/Mar/24 06:21,09/Mar/24 02:53,04/Jun/24 20:40,06/Mar/24 08:03,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,Flink CDC need to change its package name to org.apache.flink.cdc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 08:03:27 UTC 2024,,,,,,,,,,"0|z1nsy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 08:03;leonard;Resolved in flink-cdc(master) via: e10c8691add2cfedfe859a5e51115a08212d7cf0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug for dynamic table option hints with multiple CTEs,FLINK-34583,13570912,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xccui,xccui,06/Mar/24 04:51,17/Apr/24 20:58,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Table SQL / Planner,,,,,0,,,,"The table options hints don't work well with multiple WITH clauses referring to the same table. Please see the following example.

 

The following query with hints works well.
{code:java}
SELECT * FROM T1 /*+ OPTIONS('foo' = 'bar') */ WHERE...;{code}
The following query with multiple WITH clauses also works well.
{code:java}
WITH T2 AS (SELECT * FROM T1 /*+ OPTIONS('foo' = 'bar') */ WHERE...),
T3 AS (SELECT ... FROM T2 WHERE...)
SELECT * FROM T3;{code}
The following query with multiple WITH clauses referring to the same original table failed to recognize the hints.
{code:java}
WITH T2 AS (SELECT * FROM T1 /*+ OPTIONS('foo' = 'bar') */ WHERE...),
T3 AS (SELECT ... FROM T2 WHERE...),
T4 AS (SELECT ... FROM T2 WHERE...),
T5 AS (SELECT ... FROM T3 JOIN T4 ON...)
SELECT * FROM T5;{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/24 20:35;xccui;image-2024-04-17-16-35-06-153.png;https://issues.apache.org/jira/secure/attachment/13068260/image-2024-04-17-16-35-06-153.png","17/Apr/24 20:48;xccui;image-2024-04-17-16-48-49-073.png;https://issues.apache.org/jira/secure/attachment/13068261/image-2024-04-17-16-48-49-073.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 17 20:58:43 UTC 2024,,,,,,,,,,"0|z1nsx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 10:44;lincoln.86xy;cc [xuyang|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuyangzhong] can you take a look when you have time since you're familiar with this part？;;;","06/Mar/24 11:35;xuyangzhong;[~lincoln.86xy]  Sure.;;;","17/Apr/24 11:51;xuyangzhong;Hi, [~xccui] can you provide more details about this bug? I try to run this test in my local env with Flink 1.18-SNAPSHOT, but could not re-produce it.
{code:java}
// run it in org.apache.flink.table.planner.plan.stream.sql.CalcTest
@Test
def test(): Unit = {
  util.tableEnv.executeSql(s""""""
                              |create temporary table T1 (
                              |  a int,
                              |  b int,
                              |  c int)
                              |  with ( 'connector' = 'values' )
                              |"""""".stripMargin)
  util.verifyExecPlan(
    ""with q1 as (SELECT * FROM T1 /*+ OPTIONS('changelog-mode' = 'I,D') */ WHERE a > 10),"" +
      ""q2 as (SELECT a, b, c FROM q1 where b > 10),"" +
      ""q3 as (select a,b,c from q1 where c > 20),"" +
      ""q4 as (select * from q2 join q3 on q2.a = q3.a) SELECT * FROM q4"");
}

// result
Join(joinType=[InnerJoin], where=[(a = a0)], select=[a, b, c, a0, b0, c0], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
:- Exchange(distribution=[hash[a]])
:  +- Calc(select=[a, b, c], where=[((a > 10) AND (b > 10))])
:     +- TableSourceScan(table=[[default_catalog, default_database, T1, filter=[]]], fields=[a, b, c], hints=[[[OPTIONS options:{changelog-mode=I,D}]]])(reuse_id=[1])
+- Exchange(distribution=[hash[a]])
   +- Calc(select=[a, b, c], where=[((a > 10) AND (c > 20))])
      +- Reused(reference_id=[1]){code};;;","17/Apr/24 20:58;xccui;Hi [~xuyangzhong], thanks for looking into this. I hit the issue when using the Paimon table source. The execution plan looks good. However, the options don't work. It could be a runtime issue or Paimon source implementation bug. I can't remember clearly if Flink generates multiple table sources and then merges them at runtime. If it does, the options may not be merged properly.

!image-2024-04-17-16-48-49-073.png!

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
release build tools lost the newly added py3.11 packages for mac,FLINK-34582,13570907,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxb,lincoln.86xy,lincoln.86xy,06/Mar/24 02:50,24/May/24 09:26,04/Jun/24 20:40,06/Mar/24 03:39,1.19.0,1.20.0,,,,,1.19.0,1.20.0,,,,,,,,0,pull-request-available,,,"during 1.19.0-rc1 building binaries via tools/releasing/create_binary_release.sh

lost the newly added py3.11  2 packages for mac",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/24 02:39;lincoln.86xy;image-2024-03-07-10-39-49-341.png;https://issues.apache.org/jira/secure/attachment/13067314/image-2024-03-07-10-39-49-341.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 24 09:26:16 UTC 2024,,,,,,,,,,"0|z1nsw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 03:39;hxb;Merged into master via a9d9bab47a6b2a9520f7d2b6f3791690df50e214

Merged into release-1.19 via fa738bb09310a0012b5c8341e403c597855079b1;;;","07/Mar/24 02:40;lincoln.86xy;verified in 1.19.0 rc2 build: [https://dev.azure.com/lincoln86xy/lincoln86xy/_build/results?buildId=644&view=artifacts&pathAsName=false&type=publishedArtifacts]

exactly 12 whl files

!image-2024-03-07-10-39-49-341.png!;;;","23/May/24 13:31;m.orazow;Hello [~hxb] ,

 

Any idea why we don't also update and use same Python version in Linux build?

 

Thanks and best,

Muhammet;;;","23/May/24 15:08;m.orazow;At the moment the latest one for Linux is 3.10, [https://github.com/HuangXingBo/flink/blob/master/flink-python/dev/build-wheels.sh#L19-L26]

 

Should we also update the Python version on linux builds? Could you please advise [~hxb]?;;;","24/May/24 06:35;mapohl;You're checking [~hxb]'s fork where the {{master}} branch doesn't seem to be up-to-date. [apache/flink:flink-python/dev/build-wheels.sh|https://github.com/apache/flink/blob/master/flink-python/dev/build-wheels.sh#L19-L26] does, indeed, have 3.11 added to the python version list.;;;","24/May/24 09:26;m.orazow;Ohh no. Yes indeed you are right [~mapohl] , thanks for the update (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming code throws java.lang.reflect.InaccessibleObjectException,FLINK-34581,13570901,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,henning,henning,06/Mar/24 01:51,15/Mar/24 03:17,04/Jun/24 20:40,,1.18.1,,,,,,,,,,Runtime / Checkpointing,,,,,0,,,,"I have a pretty simple test pipeline (read a bunch of tables from Apache Kafka, join and project them, then write to Apache Iceberg) that I run locally with Junit5 and the MiniClusterExtension. This works ok with Java 11 and Flink 1.17.2 and Flink 1.18.1.

With Java 17, I see

{{[WARN ] IcebergStreamWriter (1/1)#1 (78d9251dbbab3aae84bf303dfc080d23_626c1e687bcaad0c13c507629a5894a8_0_1) switched from RUNNING to FAILED with failure cause:}}
{{java.io.IOException: Could not perform checkpoint 2 for operator IcebergStreamWriter (1/1)#1.}}
{{at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1275)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:488)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint(AbstractAlignedBarrierHandlerState.java:74)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived(AbstractAlignedBarrierHandlerState.java:66)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)}}
{{at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)}}
{{at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:118)}}
{{at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)}}
{{at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562)}}
{{at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)}}
{{at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)}}
{{at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)}}
{{at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)}}
{{at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)}}
{{at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)}}
{{at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)}}
{{at java.base/java.lang.Thread.run(Thread.java:840)}}
{{Caused by: java.lang.RuntimeException: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @6af93788}}
{{at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:69)}}
{{at org.apache.flink.api.java.typeutils.runtime.kryo.FlinkChillPackageRegistrar.registerSerializers(FlinkChillPackageRegistrar.java:67)}}
{{at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance(KryoSerializer.java:513)}}
{{at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized(KryoSerializer.java:522)}}
{{at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(KryoSerializer.java:348)}}
{{at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)}}
{{at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)}}
{{at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)}}
{{at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:141)}}
{{at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)}}
{{at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)}}
{{at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)}}
{{at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)}}
{{at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)}}
{{at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)}}
{{at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)}}
{{at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)}}
{{at org.apache.iceberg.flink.sink.IcebergStreamWriter.flush(IcebergStreamWriter.java:113)}}
{{at org.apache.iceberg.flink.sink.IcebergStreamWriter.prepareSnapshotPreBarrier(IcebergStreamWriter.java:66)}}
{{at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.prepareSnapshotPreBarrier(RegularOperatorChain.java:89)}}
{{at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:322)}}
{{at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$15(StreamTask.java:1318)}}
{{at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)}}
{{at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1306)}}
{{at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1263)}}
{{... 22 more}}
{{Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @6af93788}}
{{at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)}}
{{at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)}}
{{at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)}}
{{at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)}}
{{at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:67)}}
{{... 46 more}}

This is blocking us right now from upgrading to Java 17.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 15 03:17:46 UTC 2024,,,,,,,,,,"0|z1nsuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 08:19;lorenzo.affetti;Hello, could you please check if you are overriding `java.env.opts` in flink-conf.yaml?

 

If yes, then you need to add `--add-opens=java.base/java.util=ALL-UNNAMED`;;;","07/Mar/24 01:19;henning;I am using the MiniClusterExtension as part of a unit test. Does that load the flink-conf.yaml? Where would it need to be located? 

I don't use an actual cluster or deploy to a cluster (yet). Would I need the same open statement for running locally as well? ;;;","07/Mar/24 01:23;henning;The code basically looks like this:

 

{{public class FlinkTester {}}

{{    @RegisterExtension}}
{{    public static MiniClusterExtension flinkCluster = new MiniClusterExtension(}}
{{        new MiniClusterResourceConfiguration.Builder()}}
{{        .setNumberSlotsPerTaskManager(1)}}
{{        .setNumberTaskManagers(1)}}
{{        .build());}}

{{    @Test}}
{{    public void runLocalTest() throws Exception {}}

{{        String[] args = new String[] {}}

{{            ... some args here ...}}
{{        };}}

{{{}        FlinkJobMain job = new }}\{{{}FlinkJobMain{}}}{{{}(args);{}}}

{{        job.run();}}

    }

{{}}}

and I run this in the IDE with ""run unit test (which uses the JUnit 5 test runner).;;;","08/Mar/24 10:55;martijnvisser;I've downgraded this from Blocker to Major ticket, and I actually think this isn't necessarily a bug, but more a user questions. Also, keep in mind that you're using the Iceberg Sink, but that's not owned by the Flink project but by the Iceberg project. It could also be that there's actually a bug/missing feature in that implementation. ;;;","08/Mar/24 21:50;henning;It looks the same if I just print the lines. This is deeply within the flink code. I am comfortable with ""this is a documentation issue, you need to run your tests with this parameters"". You asked for ""This feature is still in beta mode. Issues should be reported in Flink’s bug tracker."" and that is what I did. :) ;;;","15/Mar/24 03:17;Yanfei Lei;Hi [~henning], it looks like it's caused by Java setting, have you tried to add `--add-opens java.base/java.util=ALL-UNNAMED`[1] to VM  parameters?

[1]https://stackoverflow.com/questions/77980473/error-module-java-base-does-not-opens-java-util;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Job run via REST erases ""pipeline.classpaths"" config",FLINK-34580,13570833,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,05/Mar/24 12:54,07/Mar/24 16:44,04/Jun/24 20:40,07/Mar/24 16:44,1.17.2,1.18.1,1.19.0,,,,1.20.0,,,,Runtime / REST,,,,,0,pull-request-available,,,"The [{{JarHandlerContext#applyToConfiguration}}|https://github.com/apache/flink/blob/e0b6c121eaf7aeb2974a45d199e452b022f07d29/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/handlers/utils/JarHandlerUtils.java#L134] creates a {{PackagedProgram}} and then overwrites the {{pipeline.jars}} and {{pipeline.classpaths}} values according to that newly created {{{}PackagedProgram{}}}.

Although that [{{PackagedProgram}} init|https://github.com/apache/flink/blob/e0b6c121eaf7aeb2974a45d199e452b022f07d29/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/handlers/utils/JarHandlerUtils.java#L185] does not set {{classpaths}} at all, so it will always overwrites the effective configuration with an empty value, even if it had something previously.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 07 16:44:20 UTC 2024,,,,,,,,,,"0|z1nsfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/24 16:44;gyfora;merged to main d0ce5349fdf1a611518eba20a169c475ee0b46c5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce metric for time since last completed checkpoint,FLINK-34579,13570831,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,srichter,srichter,srichter,05/Mar/24 12:34,05/Mar/24 16:10,04/Jun/24 20:40,05/Mar/24 16:10,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,This metric will help us to identify jobs with checkpointing problems without first requiring to complete or fail the checkpoint first before the problem surfaces.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-05 12:34:58.0,,,,,,,,,,"0|z1nsf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove issue template,FLINK-34578,13570789,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,gongzhongqiang,gongzhongqiang,gongzhongqiang,05/Mar/24 08:15,05/Mar/24 09:33,04/Jun/24 20:40,05/Mar/24 09:32,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,Flink cdc had done repo migration. So we should remove issue template.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 09:32:59 UTC 2024,,,,,,,,,,"0|z1ns5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/24 09:32;leonard;Implemented via flink-cdc (master): 99a86fbb92699659e3574df409fce52edd2bc31c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add IssueNavigationLink for IDEA git log,FLINK-34577,13570788,13565600,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,gongzhongqiang,gongzhongqiang,gongzhongqiang,05/Mar/24 08:06,05/Mar/24 09:56,04/Jun/24 20:40,05/Mar/24 09:56,,,,,,,cdc-3.1.0,,,,Flink CDC,,,,,0,pull-request-available,,,Add IssueNavigationLink for IDEA git log,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 09:56:20 UTC 2024,,,,,,,,,,"0|z1ns5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/24 09:56;jiabaosun;Implemented via flink-cdc (master): 96888b2ce0a7981ebe5917b6c27deb4015d845d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink deployment keep staying at RECONCILING/STABLE status,FLINK-34576,13570785,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stupid_pig,stupid_pig,05/Mar/24 07:23,07/Mar/24 09:05,04/Jun/24 20:40,,kubernetes-operator-1.6.1,,,,,,,,,,Kubernetes Operator,,,,,0,,,,"The HA mode of flink-kubernetes-operator is being used. When one of the pods of flink-kubernetes-operator restarts, flink-kubernetes-operator switches the leader. However, some flinkdeployments have been in the *JOB_STATUS=RECONCILING&LIFECYCLE_STATE=STABLE* state for a long time.

Through the cmd ""kubectl describe flinkdeployment xxx"", can see the following error, but there are no exceptions in the flink-kubernetes-operator log.

 
{code:java}
Status:
  Cluster Info:
    Flink - Revision:             b6d20ed @ 2023-12-20T10:01:39+01:00
    Flink - Version:              1.14.0-GDC1.6.0
    Total - Cpu:                  7.0
    Total - Memory:               30064771072
  Error:                          {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: Failed to load configuration"",""additionalMetadata"":{},""throwableList"":[{""type"":""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException"",""message"":""java.lang.RuntimeException: Failed to load configuration"",""additionalMetadata"":{}},{""type"":""java.lang.RuntimeException"",""message"":""Failed to load configuration"",""additionalMetadata"":{}}]}
  Job Manager Deployment Status:  READY
  Job Status:
    Job Id:    cf44b5e73a1f263dd7d9f2c82be5216d
    Job Name:  noah_stream_studio_1754211682_2218100380
    Savepoint Info:
      Last Periodic Savepoint Timestamp:  0
      Savepoint History:
    Start Time:     1705635107137
    State:          RECONCILING
    Update Time:    1709272530741
  Lifecycle State:  STABLE {code}
 
!image-2024-03-05-15-13-11-032.png!

 

version：

flink-kubernetes-operator: 1.6.1

flink: 1.14.0/1.15.2 (flinkdeployment 1200+)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/24 07:13;stupid_pig;image-2024-03-05-15-13-11-032.png;https://issues.apache.org/jira/secure/attachment/13067267/image-2024-03-05-15-13-11-032.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 07 09:05:13 UTC 2024,,,,,,,,,,"0|z1ns4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/24 11:36;stupid_pig;[~gyfora] 

 

Hello, I'm sorry to bother you. Could you help to have look at this operator problem when you have time? I'd be very grateful.;;;","05/Mar/24 13:18;gyfora;I am a bit busy at the moment so it will take some time until I get to this. In the meantime, I have 1-2 questions:

1.  Is there a way to somehow repro this on a smaller case?
2. Have you tried operator version 1.7.0? We may have fixed the issue there already
3. Does it also affect newer Flink versions as well?
4. Can you share some relevant operator logs?

Thanks;;;","05/Mar/24 15:07;stupid_pig;Thanks for the reply.

1.  Is there a way to somehow repro this on a smaller case?

I have tried to simulate leader switching by deleting pod in the test environment, but without repro. In the production environment, it is very likely to occur (maybe it is related to the load?).

 

Maybe there is some way to make the operator pod lost the leader to repro(not delete pod, but I haven't found any other way to make the pod lost the leader)


2. Have you tried operator version 1.7.0? We may have fixed the issue there already

It has not been upgraded to use 1.7.0 because this version no longer supports Flink1.14.0, but our production environment is still using it.

 
Are you pointing about this [JOSDK issue|https://github.com/operator-framework/java-operator-sdk/issues/2056]? We did encounter a split-brain problem similar to multiple leaders earlier, but As mentioned in the first question, this status exception will still occur after the master is successfully switched (by checking the log oldLeader exit, newLeader takeover).
 
3. Does it also affect newer Flink versions as well?
 

Our highest Flink version is 1.15.2, and the impact of higher versions is uncertain.
 

4. Can you share some relevant operator logs?

Sure.

 
operatorA log when leader switches (stopping leader appears), take it from log-file

 
{code:java}
2024-03-05 04:35:46,565 o.a.f.c.Configuration          [WARN ][gdc-qdata-bu/prod-s1-monitor-reward-sjuneizhandoushemenrel] Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
2024-03-05 04:35:46,567 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-panama-panama-h73na-serverlog-produ] Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
2024-03-05 04:35:46,569 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-erie-erie-gzailab-sym2-ns-imageveri] Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
2024-03-05 04:35:46,569 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/test-vk-log3] Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
2024-03-05 04:35:46,574 i.j.o.LeaderElectionManager    [INFO ] New leader with identity: 
2024-03-05 04:35:46,584 o.a.f.c.Configuration          [WARN ][gdc-cld-bu/logdistribution-grand-cld-dnode-contianer-ba3] Config uses deprecated configuration key 'kubernetes.jobmanager.cpu' instead of proper key 'kubernetes.jobmanager.cpu.amount'
2024-03-05 04:35:46,584 o.a.f.c.Configuration          [WARN ][gdc-cld-bu/logdistribution-grand-cld-dnode-contianer-ba3] Config uses deprecated configuration key 'kubernetes.taskmanager.cpu' instead of proper key 'kubernetes.taskmanager.cpu.amount'
2024-03-05 04:35:46,586 o.a.f.k.o.r.d.AbstractFlinkResourceReconciler [INFO ][gdc-cld-bu/logdistribution-grand-cld-dnode-contianer-ba3] Resource fully reconciled, nothing to do...
2024-03-05 04:35:46,586 i.j.o.LeaderElectionManager    [INFO ] Stopped leading for identity: flink-kubernetes-operator-85f6994468-cpsx9. Exiting.
2024-03-05 04:35:46,589 o.a.f.k.o.l.AuditUtils         [INFO ][gdc-gdc-bu/test-lag-202306-v2-copy-cpu] >>> Status | Error   | STABLE          | {""type"":""org.apache.flink.kubernetes.operator.exception.RecoveryFailureException"",""message"":""HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. Manual restore required."",""additionalMetadata"":{},""throwableList"":[]} 
2024-03-05 04:35:46,591 o.a.f.c.Configuration          [WARN ][gdc-a29-bu/logdistribution-xia-xia-a29-pc-vm-log-product] Config uses deprecated configuration key 'kubernetes.jobmanager.cpu' instead of proper key 'kubernetes.jobmanager.cpu.amount'
2024-03-05 04:35:46,591 o.a.f.c.Configuration          [WARN ][gdc-a29-bu/logdistribution-xia-xia-a29-pc-vm-log-product] Config uses deprecated configuration key 'kubernetes.taskmanager.cpu' instead of proper key 'kubernetes.taskmanager.cpu.amount'
2024-03-05 04:35:46,592 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-grand-grand-s8-serverlog-production] Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
2024-03-05 04:35:46,592 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-jinghang-jinghang-g106-seazyi-nginx] Config uses deprecated configuration key 'kubernetes.jobmanager.cpu' instead of proper key 'kubernetes.jobmanager.cpu.amount'
2024-03-05 04:35:46,592 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-jinghang-jinghang-g106-seazyi-nginx] Config uses deprecated configuration key 'kubernetes.taskmanager.cpu' instead of proper key 'kubernetes.taskmanager.cpu.amount'
2024-03-05 04:35:46,592 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-jinghang-jinghang-artct-outer-p4-se] Config uses deprecated configuration key 'kubernetes.jobmanager.cpu' instead of proper key 'kubernetes.jobmanager.cpu.amount'
2024-03-05 04:35:46,592 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-jinghang-jinghang-artct-outer-p4-se] Config uses deprecated configuration key 'kubernetes.taskmanager.cpu' instead of proper key 'kubernetes.taskmanager.cpu.amount'
2024-03-05 04:35:46,592 o.a.f.c.Configuration          [WARN ][gdc-qdata-bu/prod-s1-monitor-reward-sjuneizhandoushemenrel] Config uses deprecated configuration key 'kubernetes.jobmanager.cpu' instead of proper key 'kubernetes.jobmanager.cpu.amount'
2024-03-05 04:35:46,593 o.a.f.c.Configuration          [WARN ][gdc-qdata-bu/prod-s1-monitor-reward-sjuneizhandoushemenrel] Config uses deprecated configuration key 'kubernetes.taskmanager.cpu' instead of proper key 'kubernetes.taskmanager.cpu.amount'
2024-03-05 04:35:46,593 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-panama-panama-h73na-serverlog-produ] Config uses deprecated configuration key 'kubernetes.jobmanager.cpu' instead of proper key 'kubernetes.jobmanager.cpu.amount'
2024-03-05 04:35:46,593 o.a.f.c.Configuration          [WARN ][gdc-gdc-sa/logstream-panama-panama-h73na-serverlog-produ] Config uses deprecated configuration key 'kubernetes.taskmanager.cpu' instead of proper key 'kubernetes.taskmanager.cpu.amount' {code}
 
OperatorB log when switching, take it from es (the format is a little different from the above log file)
 
{code:java}
-- Meters ---------------------------------------------------------------------
flink-kubernetes-operator-85f6994468-92xsz.k8soperator.streamfly.flink-kubernetes-operator.system.KubeClient.HttpResponse.NumPerSecond: 0.35
flink-kubernetes-operator-85f6994468-92xsz.k8soperator.streamfly.flink-kubernetes-operator.system.KubeClient.HttpRequest.NumPerSecond: 0.35
flink-kubernetes-operator-85f6994468-92xsz.k8soperator.streamfly.flink-kubernetes-operator.system.KubeClient.HttpResponse.201.NumPerSecond: 0.0
flink-kubernetes-operator-85f6994468-92xsz.k8soperator.streamfly.flink-kubernetes-operator.system.KubeClient.HttpResponse.200.NumPerSecond: 0.3333333333333333
flink-kubernetes-operator-85f6994468-92xsz.k8soperator.streamfly.flink-kubernetes-operator.system.KubeClient.HttpResponse.101.NumPerSecond: 0.016666666666666666
flink-kubernetes-operator-85f6994468-92xsz.k8soperator.streamfly.flink-kubernetes-operator.system.KubeClient.HttpRequest.Failed.NumPerSecond: 0.0-- Histograms ---------------------------------------------------------------------
flink-kubernetes-operator-85f6994468-92xsz.k8soperator.streamfly.flink-kubernetes-operator.system.KubeClient.HttpResponse.TimeNanos: count=1000, min=939944, max=49957558, mean=1717875.3819999998, stddev=2272368.0974273267, p50=1293964.5, p75=1475059.25, p95=3561530.249999989, p98=6726813.320000002, p99=8400899.390000004, p999=4.9932472127003446E7=========================== Finished metrics report ===========================""
2024-03-04T20:35:48.416Z,""2024-03-05 04:35:48,027 INFO  io.javaoperatorsdk.operator.LeaderElectionManager             - New leader with identity: 
""
2024-03-04T20:35:48.416Z,""2024-03-05 04:35:48,121 INFO  io.javaoperatorsdk.operator.LeaderElectionManager             - New leader with identity: flink-kubernetes-operator-85f6994468-92xsz
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:48,126 INFO  io.javaoperatorsdk.operator.processing.Controller             - Started event processing for controller: flinksessionjobcontroller
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:49,150 WARN  org.apache.flink.configuration.Configuration                 [gdc-gdc-sa/logstream-wei-ma65-production] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:49,150 WARN  org.apache.flink.configuration.Configuration                 [gdc-nsh-bu/logdistribution-kiel-kiel-nsh-lhall-eos-produ] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:48,905 WARN  org.apache.flink.configuration.Configuration                 [gdc-nsh-bu/logdistribution-kiel-kiel-nsh-lhall-eos-produ] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:48,905 WARN  org.apache.flink.configuration.Configuration                 [gdc-a29-bu/logdistribution-tang-tang-a29-zycenter-hub-pr] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:49,150 WARN  org.apache.flink.configuration.Configuration                 [gdc-g117-bu/logdistribution-welland-welland-g117-serverlo] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:48,901 WARN  org.apache.flink.configuration.Configuration                 [gdc-gdc-sa/logstream-jinghang-jinghang-opd-java-fs-log-p] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:48,913 WARN  org.apache.flink.configuration.Configuration                 [gdc-g117-bu/logdistribution-welland-welland-g117-serverlo] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.420Z,""2024-03-05 04:35:48,921 WARN  org.apache.flink.configuration.Configuration                 [gdc-qdata-bu/prod-g17-reward-dynamic-huodongchangzhuhuodon] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.421Z,""2024-03-05 04:35:49,150 WARN  org.apache.flink.configuration.Configuration                 [gdc-qdata-bu/prod-g48-monitor-reward-xinzengdaojujiankong] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.421Z,""2024-03-05 04:35:48,920 WARN  org.apache.flink.configuration.Configuration                 [gdc-qdata-bu/prod-g17-reward-dynamic-reward-huodongchangzh] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.421Z,""2024-03-05 04:35:49,150 WARN  org.apache.flink.configuration.Configuration                 [gdc-gdc-sa/logstream-jinghang-jinghang-opd-java-fs-log-p] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.421Z,""2024-03-05 04:35:48,919 WARN  org.apache.flink.configuration.Configuration                 [gdc-gdc-sa/logstream-panama-panama-h72-hexfps-proxima-pr] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type'
""
2024-03-04T20:35:49.421Z,""2024-03-05 04:35:49,150 WARN  org.apache.flink.configuration.Configuration                 [gdc-qdata-bu/prod-g17-reward-dynamic-reward-huodongchangzh] - Config uses deprecated configuration key 'high-availability' instead of proper key 'high-availability.type' {code}
 

 ;;;","06/Mar/24 06:05;stupid_pig;I have check our operaotr's log， got some error log：
{code:java}
2024-03-05 04:35:54,520 INFO  org.apache.flink.kubernetes.operator.listener.AuditUtils     [gdc-gdc-sa/logstream-grand-grand-dep364-sym-stdlog-produ] - >>> Event  | Warning | CLUSTERDEPLOYMENTEXCEPTION | Status have been modified externally in version 3628984324 Previous: {""jobStatus"":{""jobName"":""logstream-dep364-grand-grand_dep364_sym_stdlog"",""jobId"":""4158bd9846dcb70aced5ecff2351a73e"",""state"":""RECONCILING"",""startTime"":""1709299557282"",""updateTime"":""1709299596007"",""savepointInfo"":{""lastSavepoint"":{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null},""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""savepointHistory"":[{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null}],""lastPeriodicSavepointTimestamp"":0}},""error"":""{\""type\"":\""org.apache.flink.kubernetes.operator.exception.ReconciliationException\"",\""message\"":\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: Failed to load configuration\"",\""additionalMetadata\"":{},\""throwableList\"":[{\""type\"":\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException\"",\""message\"":\""java.lang.RuntimeException: Failed to load configuration\"",\""additionalMetadata\"":{}},{\""type\"":\""java.lang.RuntimeException\"",\""message\"":\""Failed to load configuration\"",\""additionalMetadata\"":{}}]}"",""lifecycleState"":""STABLE"",""clusterInfo"":{""flink-revision"":""b6d20ed @ 2023-12-20T10:01:39+01:00"",""flink-version"":""1.14.0-GDC1.6.0"",""total-cpu"":""1.6"",""total-memory"":""4294967296""},""jobManagerDeploymentStatus"":""READY"",""reconciliationStatus"":{""reconciliationTimestamp"":1709299529048,""lastReconciledSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""lastStableSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""state"":""DEPLOYED""},""taskManager"":{""labelSelector"":""component=taskmanager,app=logstream-grand-grand-dep364-sym-stdlog-produ"",""replicas"":1}} Latest: {""jobStatus"":{""jobName"":""logstream-dep364-grand-grand_dep364_sym_stdlog"",""jobId"":""4158bd9846dcb70aced5ecff2351a73e"",""state"":""RECONCILING"",""startTime"":""1709299557282"",""updateTime"":""1709299596007"",""savepointInfo"":{""lastSavepoint"":{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null},""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""savepointHistory"":[{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null}],""lastPeriodicSavepointTimestamp"":0}},""error"":null,""lifecycleState"":""STABLE"",""clusterInfo"":{""flink-revision"":""b6d20ed @ 2023-12-20T10:01:39+01:00"",""flink-version"":""1.14.0-GDC1.6.0"",""total-cpu"":""1.6"",""total-memory"":""4294967296""},""jobManagerDeploymentStatus"":""READY"",""reconciliationStatus"":{""reconciliationTimestamp"":1709299529048,""lastReconciledSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""lastStableSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""state"":""DEPLOYED""},""taskManager"":{""labelSelector"":""component=taskmanager,app=logstream-grand-grand-dep364-sym-stdlog-produ"",""replicas"":1}}
2024-03-05 04:35:54,589 INFO  org.apache.flink.kubernetes.operator.listener.AuditUtils     [gdc-gdc-sa/logstream-grand-grand-dep364-sym-stdlog-produ] - >>> Status | Error   | STABLE          | {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.kubernetes.operator.exception.StatusConflictException: Status have been modified externally in version 3628984324 Previous: {\""jobStatus\"":{\""jobName\"":\""logstream-dep364-grand-grand_dep364_sym_stdlog\"",\""jobId\"":\""4158bd9846dcb70aced5ecff2351a73e\"",\""state\"":\""RECONCILING\"",\""startTime\"":\""1709299557282\"",\""updateTime\"":\""1709299596007\"",\""savepointInfo\"":{\""lastSavepoint\"":{\""timeStamp\"":1709299519672,\""location\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646\"",\""triggerType\"":\""UPGRADE\"",\""formatType\"":\""CANONICAL\"",\""triggerNonce\"":null},\""triggerId\"":null,\""triggerTimestamp\"":null,\""triggerType\"":null,\""formatType\"":null,\""savepointHistory\"":[{\""timeStamp\"":1709299519672,\""location\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646\"",\""triggerType\"":\""UPGRADE\"",\""formatType\"":\""CANONICAL\"",\""triggerNonce\"":null}],\""lastPeriodicSavepointTimestamp\"":0}},\""error\"":\""{\\\""type\\\"":\\\""org.apache.flink.kubernetes.operator.exception.ReconciliationException\\\"",\\\""message\\\"":\\\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: Failed to load configuration\\\"",\\\""additionalMetadata\\\"":{},\\\""throwableList\\\"":[{\\\""type\\\"":\\\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException\\\"",\\\""message\\\"":\\\""java.lang.RuntimeException: Failed to load configuration\\\"",\\\""additionalMetadata\\\"":{}},{\\\""type\\\"":\\\""java.lang.RuntimeException\\\"",\\\""message\\\"":\\\""Failed to load configuration\\\"",\\\""additionalMetadata\\\"":{}}]}\"",\""lifecycleState\"":\""STABLE\"",\""clusterInfo\"":{\""flink-revision\"":\""b6d20ed @ 2023-12-20T10:01:39+01:00\"",\""flink-version\"":\""1.14.0-GDC1.6.0\"",\""total-cpu\"":\""1.6\"",\""total-memory\"":\""4294967296\""},\""jobManagerDeploymentStatus\"":\""READY\"",\""reconciliationStatus\"":{\""reconciliationTimestamp\"":1709299529048,\""lastReconciledSpec\"":\""{\\\""spec\\\"":{\\\""job\\\"":{\\\""jarURI\\\"":\\\""local:///opt/flink/usrlib/usercode.jar\\\"",\\\""parallelism\\\"":1,\\\""entryClass\\\"":\\\""com.netease.gdc.streaming.serverdump.Main\\\"",\\\""args\\\"":[\\\""-id\\\"",\\\""grand-grand_dep364_sym_stdlog\\\"",\\\""-"",""additionalMetadata"":{},""throwableList"":[{""type"":""org.apache.flink.kubernetes.operator.exception.StatusConflictException"",""message"":""Status have been modified externally in version 3628984324 Previous: {\""jobStatus\"":{\""jobName\"":\""logstream-dep364-grand-grand_dep364_sym_stdlog\"",\""jobId\"":\""4158bd9846dcb70aced5ecff2351a73e\"",\""state\"":\""RECONCILING\"",\""startTime\"":\""1709299557282\"",\""updateTime\"":\""1709299596007\"",\""savepointInfo\"":{\""lastSavepoint\"":{\""timeStamp\"":1709299519672,\""location\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646\"",\""triggerType\"":\""UPGRADE\"",\""formatType\"":\""CANONICAL\"",\""triggerNonce\"":null},\""triggerId\"":null,\""triggerTimestamp\"":null,\""triggerType\"":null,\""formatType\"":null,\""savepointHistory\"":[{\""timeStamp\"":1709299519672,\""location\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646\"",\""triggerType\"":\""UPGRADE\"",\""formatType\"":\""CANONICAL\"",\""triggerNonce\"":null}],\""lastPeriodicSavepointTimestamp\"":0}},\""error\"":\""{\\\""type\\\"":\\\""org.apache.flink.kubernetes.operator.exception.ReconciliationException\\\"",\\\""message\\\"":\\\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: Failed to load configuration\\\"",\\\""additionalMetadata\\\"":{},\\\""throwableList\\\"":[{\\\""type\\\"":\\\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException\\\"",\\\""message\\\"":\\\""java.lang.RuntimeException: Failed to load configuration\\\"",\\\""additionalMetadata\\\"":{}},{\\\""type\\\"":\\\""java.lang.RuntimeException\\\"",\\\""message\\\"":\\\""Failed to load configuration\\\"",\\\""additionalMetadata\\\"":{}}]}\"",\""lifecycleState\"":\""STABLE\"",\""clusterInfo\"":{\""flink-revision\"":\""b6d20ed @ 2023-12-20T10:01:39+01:00\"",\""flink-version\"":\""1.14.0-GDC1.6.0\"",\""total-cpu\"":\""1.6\"",\""total-memory\"":\""4294967296\""},\""jobManagerDeploymentStatus\"":\""READY\"",\""reconciliationStatus\"":{\""reconciliationTimestamp\"":1709299529048,\""lastReconciledSpec\"":\""{\\\""spec\\\"":{\\\""job\\\"":{\\\""jarURI\\\"":\\\""local:///opt/flink/usrlib/usercode.jar\\\"",\\\""parallelism\\\"":1,\\\""entryClass\\\"":\\\""com.netease.gdc.streaming.serverdump.Main\\\"",\\\""args\\\"":[\\\""-id\\\"",\\\""grand-grand_dep364_sym_stdlog\\\"",\\\""-server\\\"",\\\""http://logstreamapi-in.nie.netease.com:9099/\\\"",\\\""-auth_key\\\"","",""additionalMetadata"":{}}]} 
2024-03-05 04:35:54,593 ERROR io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher [gdc-gdc-sa/logstream-grand-grand-dep364-sym-stdlog-produ] - Error during event processing ExecutionScope{ resource id: ResourceID{name='logstream-grand-grand-dep364-sym-stdlog-produ', namespace='gdc-gdc-sa'}, version: 3628981337} failed.
org.apache.flink.kubernetes.operator.exception.ReconciliationException: org.apache.flink.kubernetes.operator.exception.StatusConflictException: Status have been modified externally in version 3628984324 Previous: {""jobStatus"":{""jobName"":""logstream-dep364-grand-grand_dep364_sym_stdlog"",""jobId"":""4158bd9846dcb70aced5ecff2351a73e"",""state"":""RECONCILING"",""startTime"":""1709299557282"",""updateTime"":""1709299596007"",""savepointInfo"":{""lastSavepoint"":{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null},""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""savepointHistory"":[{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null}],""lastPeriodicSavepointTimestamp"":0}},""error"":""{\""type\"":\""org.apache.flink.kubernetes.operator.exception.ReconciliationException\"",\""message\"":\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: Failed to load configuration\"",\""additionalMetadata\"":{},\""throwableList\"":[{\""type\"":\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException\"",\""message\"":\""java.lang.RuntimeException: Failed to load configuration\"",\""additionalMetadata\"":{}},{\""type\"":\""java.lang.RuntimeException\"",\""message\"":\""Failed to load configuration\"",\""additionalMetadata\"":{}}]}"",""lifecycleState"":""STABLE"",""clusterInfo"":{""flink-revision"":""b6d20ed @ 2023-12-20T10:01:39+01:00"",""flink-version"":""1.14.0-GDC1.6.0"",""total-cpu"":""1.6"",""total-memory"":""4294967296""},""jobManagerDeploymentStatus"":""READY"",""reconciliationStatus"":{""reconciliationTimestamp"":1709299529048,""lastReconciledSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""lastStableSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""state"":""DEPLOYED""},""taskManager"":{""labelSelector"":""component=taskmanager,app=logstream-grand-grand-dep364-sym-stdlog-produ"",""replicas"":1}} Latest: {""jobStatus"":{""jobName"":""logstream-dep364-grand-grand_dep364_sym_stdlog"",""jobId"":""4158bd9846dcb70aced5ecff2351a73e"",""state"":""RECONCILING"",""startTime"":""1709299557282"",""updateTime"":""1709299596007"",""savepointInfo"":{""lastSavepoint"":{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null},""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""savepointHistory"":[{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null}],""lastPeriodicSavepointTimestamp"":0}},""error"":null,""lifecycleState"":""STABLE"",""clusterInfo"":{""flink-revision"":""b6d20ed @ 2023-12-20T10:01:39+01:00"",""flink-version"":""1.14.0-GDC1.6.0"",""total-cpu"":""1.6"",""total-memory"":""4294967296""},""jobManagerDeploymentStatus"":""READY"",""reconciliationStatus"":{""reconciliationTimestamp"":1709299529048,""lastReconciledSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""lastStableSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""state"":""DEPLOYED""},""taskManager"":{""labelSelector"":""component=taskmanager,app=logstream-grand-grand-dep364-sym-stdlog-produ"",""replicas"":1}}
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:148) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:138) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:96) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:95) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62) [flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:414) [flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
    at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: org.apache.flink.kubernetes.operator.exception.StatusConflictException: Status have been modified externally in version 3628984324 Previous: {""jobStatus"":{""jobName"":""logstream-dep364-grand-grand_dep364_sym_stdlog"",""jobId"":""4158bd9846dcb70aced5ecff2351a73e"",""state"":""RECONCILING"",""startTime"":""1709299557282"",""updateTime"":""1709299596007"",""savepointInfo"":{""lastSavepoint"":{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null},""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""savepointHistory"":[{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null}],""lastPeriodicSavepointTimestamp"":0}},""error"":""{\""type\"":\""org.apache.flink.kubernetes.operator.exception.ReconciliationException\"",\""message\"":\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: Failed to load configuration\"",\""additionalMetadata\"":{},\""throwableList\"":[{\""type\"":\""org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException\"",\""message\"":\""java.lang.RuntimeException: Failed to load configuration\"",\""additionalMetadata\"":{}},{\""type\"":\""java.lang.RuntimeException\"",\""message\"":\""Failed to load configuration\"",\""additionalMetadata\"":{}}]}"",""lifecycleState"":""STABLE"",""clusterInfo"":{""flink-revision"":""b6d20ed @ 2023-12-20T10:01:39+01:00"",""flink-version"":""1.14.0-GDC1.6.0"",""total-cpu"":""1.6"",""total-memory"":""4294967296""},""jobManagerDeploymentStatus"":""READY"",""reconciliationStatus"":{""reconciliationTimestamp"":1709299529048,""lastReconciledSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""lastStableSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""state"":""DEPLOYED""},""taskManager"":{""labelSelector"":""component=taskmanager,app=logstream-grand-grand-dep364-sym-stdlog-produ"",""replicas"":1}} Latest: {""jobStatus"":{""jobName"":""logstream-dep364-grand-grand_dep364_sym_stdlog"",""jobId"":""4158bd9846dcb70aced5ecff2351a73e"",""state"":""RECONCILING"",""startTime"":""1709299557282"",""updateTime"":""1709299596007"",""savepointInfo"":{""lastSavepoint"":{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null},""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""savepointHistory"":[{""timeStamp"":1709299519672,""location"":""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-4158bd-e211ea884646"",""triggerType"":""UPGRADE"",""formatType"":""CANONICAL"",""triggerNonce"":null}],""lastPeriodicSavepointTimestamp"":0}},""error"":null,""lifecycleState"":""STABLE"",""clusterInfo"":{""flink-revision"":""b6d20ed @ 2023-12-20T10:01:39+01:00"",""flink-version"":""1.14.0-GDC1.6.0"",""total-cpu"":""1.6"",""total-memory"":""4294967296""},""jobManagerDeploymentStatus"":""READY"",""reconciliationStatus"":{""reconciliationTimestamp"":1709299529048,""lastReconciledSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""lastStableSpec"":""{\""spec\"":{\""job\"":{\""jarURI\"":\""local:///opt/flink/usrlib/usercode.jar\"",\""parallelism\"":1,\""entryClass\"":\""com.netease.gdc.streaming.serverdump.Main\"",\""args\"":[\""-id\"",\""grand-grand_dep364_sym_stdlog\"",\""-server\"",\""http://logstreamapi-in.nie.netease.com:9099/\"",\""-auth_key\"",\""3e0e9294802f4b868dc89e8f32ae43ab\"",\""-auth_user\"",\""_loghub\"",\""-auth_project\"",\""dep364\"",\""-type\"",\""logstream\""],\""state\"":\""running\"",\""savepointTriggerNonce\"":1,\""initialSavepointPath\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c/savepoint-08f339-b28d20b0fc24\"",\""upgradeMode\"":\""savepoint\"",\""allowNonRestoredState\"":true},\""restartNonce\"":null,\""flinkConfiguration\"":{\""containerized.master.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.master.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.master.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.master.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.master.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.JAVA_LIBRARY_PATH\"":\""$JAVA_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_APP_NAME\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_DEPARTMENT\"":\""sa\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_LAMBDA_ID\"":\""6531f6605ce54184efd7092c\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG_PROJECT\"":\""gdc\"",\""containerized.taskmanager.env.LAMBDA_METRICS_TAG__share_project_\"":\""gdc\"",\""containerized.taskmanager.env.LD_LIBRARY_PATH\"":\""$LD_LIBRARY_PATH:/home/hadoop/hadoop/lib/native\"",\""env.hadoop.conf.dir\"":\""/home/hadoop/gdcconf/hadoop/mogra\"",\""env.java.opts\"":\""-Duser.timezone=GMT+08 -XX:+UseG1GC -Xloggc:log/gc.log -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=10M\"",\""execution.checkpointing.externalized-checkpoint-retention\"":\""RETAIN_ON_CANCELLATION\"",\""execution.checkpointing.interval\"":\""30s\"",\""execution.checkpointing.tolerable-failed-checkpoints\"":\""3\"",\""execution.shutdown-on-application-finish\"":\""false\"",\""high-availability\"":\""zookeeper\"",\""high-availability.kubernetes.leader-election.lease-duration\"":\""60 s\"",\""high-availability.kubernetes.leader-election.renew-deadline\"":\""60 s\"",\""high-availability.kubernetes.leader-election.retry-period\"":\""30 s\"",\""high-availability.storageDir\"":\""hdfs://mogra/flink/gdc_sa/ha/\"",\""high-availability.zookeeper.path.root\"":\""/flink\"",\""high-availability.zookeeper.quorum\"":\""mogra2.realtime.zk.gdc.x.netease.com:2181\"",\""io.tmp.dirs\"":\""/tmp/flink-io\"",\""jobmanager.archive.fs.dir\"":\""hdfs://mogra/flink/gdc_sa/completed-jobs\"",\""jobmanager.retrieve-taskmanager-hostname\"":\""false\"",\""metrics.latency.interval\"":\""10000\"",\""metrics.reporter.jmx.factory.class\"":\""org.apache.flink.metrics.jmx.JMXReporterFactory\"",\""metrics.reporter.jmx.port\"":\""8800-8880\"",\""metrics.reporter.prom.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporter\"",\""metrics.reporter.prom.factory.class\"":\""org.apache.flink.metrics.prometheus.PrometheusReporterFactory\"",\""metrics.reporter.prom.factory.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.factory.port\"":\""9900-10099\"",\""metrics.reporter.prom.filterLabelValueCharacters\"":\""false\"",\""metrics.reporter.prom.port\"":\""9900-10099\"",\""rest.flamegraph.enabled\"":\""true\"",\""restart-strategy\"":\""failure-rate\"",\""restart-strategy.failure-rate.delay\"":\""1 min\"",\""restart-strategy.failure-rate.failure-rate-interval\"":\""10 min\"",\""restart-strategy.failure-rate.max-failures-per-interval\"":\""3\"",\""restartNonce\"":\""1\"",\""security.kerberos.login.keytab\"":\""/opt/scheduler/keytab/gdc_sa.keytab\"",\""security.kerberos.login.principal\"":\""gdc_sa/scheduler\"",\""state.checkpoints.dir\"":\""hdfs://mogra/flink/gdc_sa/checkpoints/6531f6605ce54184efd7092c\"",\""state.savepoints.dir\"":\""hdfs://mogra/flink/gdc_sa/savepoints/6531f6605ce54184efd7092c\"",\""taskmanager.numberOfTaskSlots\"":\""1\"",\""taskmanager.slot.timeout\"":\""60 s\"",\""web.cancel.enable\"":\""true\""},\""image\"":\""ncr.nie.netease.com/v1-gdcstreaming/streamfly-job-6531f6605ce54184efd7092c:latest\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""streamfly\"",\""flinkVersion\"":\""v1_14\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""annotations\"":{\""clusterSelector\"":\""{\\\""affinity\\\"":{\\\""nodeAffinity\\\"":{\\\""preferredDuringSchedulingIgnoredDuringExecution\\\"": [{\\\""weight\\\"": 1, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""ssd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}},{\\\""weight\\\"": 2, \\\""preference\\\"": {\\\""matchExpressions\\\"":[{\\\""key\\\"":\\\""hdd\\\"",\\\""operator\\\"":\\\""Exists\\\""}]}}]}}}\"",\""gdc-logs\"":\""{\\\""server-log-gdc-flink\\\"":{\\\""type\\\"":\\\""gdc\\\"",\\\""size\\\"":\\\""10\\\"",\\\""logType\\\"":\\\""server\\\""}}\"",\""initializer.kubernetes.io/gdc-logservice\"":\""true\"",\""streamfly.nie.netease.com/lambda_biztype\"":\""logstream\"",\""streamfly.nie.netease.com/lambda_department\"":\""sa\"",\""streamfly.nie.netease.com/lambda_id\"":\""6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_jm_cores\"":\""\\\\\\\""0.8\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_job_url\"":\""https://streamfly.nie.netease.com/_gdc_sa/lambda/jobs/6531f6605ce54184efd7092c\"",\""streamfly.nie.netease.com/lambda_name\"":\""logstream-grand-grand_dep364_sym_stdlog-production\"",\""streamfly.nie.netease.com/lambda_owner\"":\""_loghub\"",\""streamfly.nie.netease.com/lambda_parallelism\"":\""\\\\\\\""1\\\\\\\""\"",\""streamfly.nie.netease.com/lambda_project\"":\""gdc\"",\""streamfly.nie.netease.com/lambda_tags\"":\""_cost_dep364_bu\"",\""streamfly.nie.netease.com/lambda_tm_cores\"":\""\\\\\\\""0.8\\\\\\\""\""},\""labels\"":{\""app.kubernetes.io/instance\"":\""6531f6605ce54184efd7092c\"",\""app.kubernetes.io/managed-by\"":\""streamfly\""}},\""spec\"":{\""affinity\"":{\""nodeAffinity\"":{\""requiredDuringSchedulingIgnoredDuringExecution\"":{\""nodeSelectorTerms\"":[{\""matchExpressions\"":[{\""key\"":\""node-role.kubernetes.io/vk_streamfly\"",\""operator\"":\""Exists\""}]}]}}},\""automountServiceAccountToken\"":false,\""containers\"":[{\""env\"":[{\""name\"":\""FLINK_POD_ID\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""metadata.uid\""}}},{\""name\"":\""FLINK_POD_IP\"",\""valueFrom\"":{\""fieldRef\"":{\""fieldPath\"":\""status.podIP\""}}},{\""name\"":\""FLINK_LOG_DIR\"",\""value\"":\""/flink-data/gdc-sa/logstream/6531f6605ce54184efd7092c/$(FLINK_POD_IP)/$(FLINK_POD_ID)\""},{\""name\"":\""AUTH_PROJECT\"",\""value\"":\""gdc\""},{\""name\"":\""AUTH_DEPT\"",\""value\"":\""sa\""},{\""name\"":\""LAMBDA_JOB_NAME\"",\""value\"":\""logstream-grand-grand_dep364_sym_stdlog-production\""},{\""name\"":\""KUBERNETES_SERVICE_PORT\"",\""value\"":\""6443\""},{\""name\"":\""KUBERNETES_SERVICE_HOST\"",\""value\"":\""k596.elk.x.netease.com\""}],\""imagePullPolicy\"":\""Always\"",\""name\"":\""flink-main-container\"",\""ports\"":[{\""containerPort\"":9900,\""name\"":\""flink-metric\"",\""protocol\"":\""TCP\""}],\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""20Gi\""},\""requests\"":{\""ephemeral-storage\"":\""10Gi\""}},\""securityContext\"":{\""capabilities\"":{\""add\"":[\""SYS_PTRACE\""]}},\""volumeMounts\"":[{\""mountPath\"":\""/flink-data\"",\""name\"":\""server-log-gdc-flink\""}]}],\""imagePullSecrets\"":[{\""name\"":\""ncr-pull-secret\""}],\""tolerations\"":[{\""effect\"":\""NoSchedule\"",\""key\"":\""virtual-kubelet.io/provider\"",\""value\"":\""whale-virtual-kubelet\""}],\""volumes\"":[{\""emptyDir\"":{},\""name\"":\""server-log-gdc-flink\""}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.8,\""memory\"":\""2048m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":null},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":3},\""firstDeployment\"":false}}"",""state"":""DEPLOYED""},""taskManager"":{""labelSelector"":""component=taskmanager,app=logstream-grand-grand-dep364-sym-stdlog-produ"",""replicas"":1}}
    at org.apache.flink.kubernetes.operator.utils.StatusRecorder.replaceStatus(StatusRecorder.java:164) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at org.apache.flink.kubernetes.operator.utils.StatusRecorder.patchAndCacheStatus(StatusRecorder.java:101) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:135) ~[flink-kubernetes-operator-1.6.1-shaded.jar:1.6.1]
    ... 13 more {code}
It was logged in the newLeader after leader switching.

 

 

 ;;;","06/Mar/24 14:25;stupid_pig;According to this JOSDK [issue|https://github.com/operator-framework/java-operator-sdk/issues/2009], is it possible that during the stopLeader process, oldLeader cannot exit within the leaseDuration-renewDuration time window, causing oldLeader and newLeader to update the status of flinkdeployment at the same time, resulting in a conflict. Because the JOSDK(the version before 4.5) can't tell running instance is actually leading or not

 

I think we can write a testCase to test this situation, but I am not very familiar with operator. It seems that there are relatively few unit tests for HA above. If I can get guidance, I would like to try it.
 

Thanks.

[~gyfora] ;;;","06/Mar/24 14:44;gyfora;Not sure how to repro this in a test easily, you could try upgrading the JOSDK version and testing with a custom build in your env to see if that solves the issue. ;;;","06/Mar/24 14:45;gyfora;I think what you found could definitely explain the problem, so there is a good chance that the JOSDK version upgrade will resolve it :) ;;;","06/Mar/24 14:46;gyfora;I am happy to review your PR if you try to bump the version;;;","07/Mar/24 07:48;stupid_pig;""Not sure how to repro this in a test easily, you could try upgrading the JOSDK version and testing with a custom build in your env to see if that solves the issue. ""

 
This is a bit confusing. Does it mean to only upgrade the JOSDK version to 4.5, or to upgrade the JOSDK version to 4.5  and use callback to set the leader fence (one of the solutions I can think of). In addition, what is custom build? Maybe I am not very familiar with the operator, but I am glad to try to solve this problem with the community.

 
[~gyfora] ;;;","07/Mar/24 08:06;gyfora;Ah I see, I thought the issue you linked was a fix for this specific problem in the JOSDK but It's not.

I think it would be weird if we should set a leader fence in the callback, why doesn't the JOSDK leader election mechanism already does this? If this is a problem with that, the issue should be fixed there right?;;;","07/Mar/24 09:05;stupid_pig;Analyzing from the log of the operator above, when the operator switches leader, although oldLeader finally exits, but oldLeader does not know that it is no longer the leader until the final process exits, so it updates flinkdeployment at the same time with newLeader, resulting in statusConflict.

Refer to [JOSDK 4.4.4 source code|https://github.com/operator-framework/java-operator-sdk/blob/6238ef21d6761fb99731fd4903c077ad10258b64/operator-framework-core/src/main/java/io/javaoperatorsdk/operator/LeaderElectionManager.java#L79] ，

 before version 4.5, JOSDK did not provide this notification mechanism to tell the upper-layer application instance that it is no longer the leader. Instead, it maintained the logic of startLeader/stopLeader internally.

 

After version 4.5, JOSDK provides a callback mechanism to notify the upper-layer application whether it is the leader, which is the scenario mentioned in the above  [issue|https://github.com/operator-framework/java-operator-sdk/issues/2009] . However, this requires certain adaptations on the application side, such as setting a leader flag variable. We can refer to flink using fabric8 Leader callback mechanism which  using configMap for HA, refer to [flink k8s HA source code|https://github.com/apache/flink/blob/9b1375520b6b351df7551d85fcecd920e553cc3a/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L116]

 

Of course, we currently plan to adjust the two parameters leaseDuration/renewDuration, such as making the difference between (leaseDuration - renewDuration) (the default value is 15s - 10 s = 5 s) larger to ensure that oldeLeader has more time to exit before newLeader is generated. , which can alleviate the current update conflict problem, but it cannot completely solve it.

 

 
h1.;;;",,,,,,,,,,,,,,,,,,,,,,
"Vulnerabilities in commons-compress 1.24.0; upgrade to 1.26.0 needed.",FLINK-34575,13570695,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,adrianalexvasiliu,adrianalexvasiliu,04/Mar/24 15:25,19/Mar/24 16:39,04/Jun/24 20:40,,1.18.1,,,,,,hbase-4.0.0,,,,,,,,,2,,,,"Since Feb. 19, medium/high CVEs have been found for commons-compress 1.24.0:
[https://nvd.nist.gov/vuln/detail/CVE-2024-25710]
https://nvd.nist.gov/vuln/detail/CVE-2024-26308

[https://github.com/apache/flink/pull/24352] has been opened automatically on Feb. 21 by dependabot for bumping commons-compress to v1.26.0 which fixes the CVEs, but two CI checks are red on the PR.

Flink's dependency on commons-compress has been upgraded to v1.24.0 in Oct 2023 (https://issues.apache.org/jira/browse/FLINK-33329).
v1.24.0 is the version currently in the master branch:[https://github.com/apache/flink/blob/master/pom.xml#L727-L729|https://github.com/apache/flink/blob/master/pom.xml#L727-L729).].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 07 09:14:46 UTC 2024,,,,,,,,,,"0|z1nrkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/24 19:58;martijnvisser;Thanks for filing the ticket, I've changed it from a bug to a tech debt. I also don't think that Flink is immediately affected by this, but it's still a good practice to upgrade Flink in its next minor version (meaning Flink 1.20, since 1.19 is in feature freeze). ;;;","05/Mar/24 20:18;adrianalexvasiliu;Thanks Martijn. I couldn't find an estimated date for 1.20, would you know?
Also, no bug / security fix release planned for 1.18.x / 1.19.x, before 1.20?
I ask because of the pressure from vulnerability scanners...;;;","06/Mar/24 21:25;martijnvisser;[~adrianalexvasiliu] I don't think a security fix is necessary, since Flink isn't affected directly by it. ;;;","06/Mar/24 21:26;martijnvisser;Fixed in apache/flink-connector-hbase:main dfe7646de6d4bbac0bdd5b51f90c8a18a02996bf;;;","07/Mar/24 09:14;adrianalexvasiliu;Thanks Marijn for getting [https://github.com/apache/flink-connector-hbase/pull/41] merged. I suppose at some point this will be complemented by
[https://github.com/apache/flink/pull/24352|https://github.com/apache/flink/pull/24352] .



> I don't think a security fix is necessary, since Flink isn't affected directly by it.

Good to know. Now, vulnerability scanners don't know it, and in enterprise context vulnerabilities create trouble / extra processes even when the vulnerability can't really be exploited.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add CPU and memory size autoscaler quota,FLINK-34574,13570674,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,04/Mar/24 13:14,19/Apr/24 15:37,04/Jun/24 20:40,19/Apr/24 15:37,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 19 15:37:05 UTC 2024,,,,,,,,,,"0|z1nrg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/24 15:37;gyfora;merged to main baad90088ea5b5b240186a530a79b64fb84cc77e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the task is stuck on the high presure,FLINK-34573,13570667,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,,liusz,liusz,04/Mar/24 12:53,08/Apr/24 01:44,04/Jun/24 20:40,08/Apr/24 01:44,1.14.3,,,,,,,,,,Runtime / Network,,,,,0,,,,"we havae a flink job , jst one taskmanger;

when use high presure as soure data,it will be stuck. sometimes it will be run 1d ,somtimes it will be run 30min.

!stuck.PNG!

like this: (13:30 the taskmanager reboot,then run 30min, result is stuck )

test 3 cases:

1: low presure (1200eps ),  it will run 30 min or 1d 。

2: close checkpoint , it will run 3d , high presure (1800eps) ，did not run stuck。

3：double the orignal  managermemory, it still stuck, jst The appearance time has been changed to 3 days from 30mins.

!rate.PNG!

 

the threads dump info ,when high presure , cpu 90%~100%:

[^tm-thread-dump-chk-0123[1].json]

this is the normal info, when the low presure :
[^tm-thread-dump-no-lock-0123[1].json]

 

 ",,,432000,432000,,0%,432000,432000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/24 12:56;liusz;rate.PNG;https://issues.apache.org/jira/secure/attachment/13067249/rate.PNG","04/Mar/24 12:43;liusz;stuck.PNG;https://issues.apache.org/jira/secure/attachment/13067248/stuck.PNG","04/Mar/24 12:47;liusz;tm-thread-dump-chk-0123[1].json;https://issues.apache.org/jira/secure/attachment/13067247/tm-thread-dump-chk-0123%5B1%5D.json","04/Mar/24 12:48;liusz;tm-thread-dump-no-lock-0123[1].json;https://issues.apache.org/jira/secure/attachment/13067246/tm-thread-dump-no-lock-0123%5B1%5D.json",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 03:29:41 UTC 2024,,,,,,,,,,"0|z1nreo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 03:29;Weijie Guo;1.14.3 has long been out of official support by the flink community. I will close this issue if it cannot be reproduced in recent releases.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC: Support OceanBase catalog,FLINK-34572,13570662,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wanghe,wanghe,04/Mar/24 11:48,03/Jun/24 07:20,04/Jun/24 20:40,,,,,,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,"As the OceanBase dialect has been introduced, we can support the OceanBase catalog in jdbc connector now.

- The catalog of OceanBase MySQL mode should be same with MySQL catalog except 'builtinDatabases'.

- In OceanBase Oracle mode, there is no 'database' but 'schema', which is corresponded to the user name. As there is no Oracle catalog for now, I'm not sure how should we use 'database' and 'schema' in the catalog. Personally, I prefer to use 'schema' as 'database'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-04 11:48:27.0,,,,,,,,,,"0|z1nrdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortMergeResultPartitionReadSchedulerTest.testOnReadBufferRequestError failed due an assertion,FLINK-34571,13570631,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,mapohl,mapohl,04/Mar/24 07:53,18/Mar/24 01:36,04/Jun/24 20:40,18/Mar/24 01:36,1.19.0,1.20.0,,,,,1.19.1,1.20.0,,,Runtime / Network,,,,,0,github-actions,pull-request-available,test-stability,"https://github.com/apache/flink/actions/runs/8134965216/job/22228875618#step:10:8586
{code}
Error: 02:39:36 02:39:36.688 [ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 13.68 s <<< FAILURE! -- in org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest
Error: 02:39:36 02:39:36.689 [ERROR] org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.testOnReadBufferRequestError -- Time elapsed: 0.174 s <<< FAILURE!
Mar 04 02:39:36 org.opentest4j.AssertionFailedError: 
Mar 04 02:39:36 
Mar 04 02:39:36 Expecting value to be true but was false
Mar 04 02:39:36 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Mar 04 02:39:36 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Mar 04 02:39:36 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Mar 04 02:39:36 	at org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionReadSchedulerTest.testOnReadBufferRequestError(SortMergeResultPartitionReadSchedulerTest.java:225)
Mar 04 02:39:36 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 04 02:39:36 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 04 02:39:36 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 04 02:39:36 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 04 02:39:36 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 04 02:39:36 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 08:17:03 UTC 2024,,,,,,,,,,"0|z1nr6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/24 08:20;lincoln.86xy;[~xtsong] Could you help to check this if it is a production issue or just test instabilities?;;;","05/Mar/24 09:33;Weijie Guo;Thanks [~mapohl] and [~lincoln.86xy] for reporting this. I did a bit of debugging and can confirm that this is an unstable test, not a production issue. I will fix it as soon as possible.;;;","05/Mar/24 10:08;lincoln.86xy;[~Weijie Guo] Thanks for your quick reply!:);;;","11/Mar/24 08:17;Weijie Guo;master(1.20) via fcf61b9108942a892aa93765178a81f2c0e787c5.
release-1.19: 4f7f6a97f799a4a30d1aea57fc0af5d07dc0eeed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinITCase.testLeftJoinWithEqualPk times out,FLINK-34570,13570630,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,04/Mar/24 07:49,11/Mar/24 13:53,04/Jun/24 20:40,11/Mar/24 13:53,1.18.1,,,,,,,,,,Table SQL / Planner,,,,,0,github-actions,test-stability,,"https://github.com/apache/flink/actions/runs/8127069912/job/22211928085#step:10:14479

{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007ff4ae2b7000 nid=0x2168b waiting on condition [0x00007ff4affdc000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000ab096950> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2131)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2099)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:2077)
	at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:876)
	at org.apache.flink.table.planner.runtime.stream.sql.JoinITCase.testLeftJoinWithEqualPk(JoinITCase.scala:705)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34227,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 13:52:51 UTC 2024,,,,,,,,,,"0|z1nr6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 10:22;mapohl;1.18: https://github.com/apache/flink/actions/runs/8165884599/job/22324024789#step:10:14479;;;","11/Mar/24 13:52;mapohl;Looks like it's the same issue as FLINK-34227 (example from the [2nd workflow failure|https://issues.apache.org/jira/browse/FLINK-34570?focusedCommentId=17823954&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17823954]):
{code}
[...]
02:38:49,475 [flink-pekko.actor.default-dispatcher-12] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:90, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=256.000gb (274877906944 bytes), taskOffHeapMemory=256.000gb (27
4877906944 bytes), managedMemory=20.000mb (20971520 bytes), networkMemory=16.000mb (16777216 bytes)}, allocationId: de31d274eac0b672e07d3c50c2afdab4, jobId: 8703cdbfc8a74bf746e5773f37d3923c).
02:38:49,475 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 84e717f317f7164a6ebda49dabad4377@pekko://flink/user/rpc/jobmanager_26 for job 8703cdbfc8a74bf746e5773f37d3923c from the resourc
e manager.
02:38:49,475 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 4a55449ca5d03fdf894e7f2a8b3013da: Stopping JobMaster for job 'Flink Streaming Job' (8703cdbfc8a74bf746e5773f37d3923c).
02:38:49,475 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager pekko://flink/user/rpc/resourcemanager_2(98cfda58ccf7da70416a83b8f4fb424a)
02:38:49,476 [flink-pekko.actor.default-dispatcher-12] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:88, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=256.000gb (274877906944 bytes), taskOffHeapMemory=256.000gb (27
4877906944 bytes), managedMemory=20.000mb (20971520 bytes), networkMemory=16.000mb (16777216 bytes)}, allocationId: 137f19ce765a4f8b6b6b596ba5ea3978, jobId: 8703cdbfc8a74bf746e5773f37d3923c).
02:38:49,476 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer [] - Freeing slot de31d274eac0b672e07d3c50c2afdab4.
02:38:49,476 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration
02:38:49,476 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 84e717f317f7164a6ebda49dabad4377@pekko://flink/user/rpc/jobmanager_26 for job 8703cdbfc8a74bf746e5773f37d3923c.
02:38:49,476 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 84e717f317f7164a6ebda49dabad4377@pekko://flink/user/rpc/jobmanager_26 for job 8703cdbfc8a74bf746e5773f37d3923c.
[...]
{code}

I'm gonna close this one in favor of FLINK-34227;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Streaming File Sink s3 end-to-end test' failed,FLINK-34569,13570629,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,robyoung,mapohl,mapohl,04/Mar/24 07:34,14/Mar/24 00:15,04/Jun/24 20:40,,1.19.0,,,,,,,,,,Tests,,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58026&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3957

{code}
Mar 02 04:12:57 Waiting until all values have been produced
Unable to find image 'stedolan/jq:latest' locally
Error: No such container: 
docker: Error response from daemon: Get ""https://registry-1.docker.io/v2/"": read tcp 10.1.0.97:42214->54.236.113.205:443: read: connection reset by peer.
See 'docker run --help'.
Mar 02 04:12:58 Number of produced values 0/60000
Error: No such container: 
Unable to find image 'stedolan/jq:latest' locally
latest: Pulling from stedolan/jq
[DEPRECATION NOTICE] Docker Image Format v1, and Docker Image manifest version 2, schema 1 support will be removed in an upcoming release. Suggest the author of docker.io/stedolan/jq:latest to upgrade the image to the OCI Format, or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/
237d5fcd25cf: Pulling fs layer
[...]
4dae4fd48813: Pull complete
Digest: sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e09
Status: Downloaded newer image for stedolan/jq:latest
parse error: Invalid numeric literal at line 1, column 6
Error: No such container: 
parse error: Invalid numeric literal at line 1, column 6
Error: No such container: 
parse error: Invalid numeric literal at line 1, column 6
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 13 15:09:53 UTC 2024,,,,,,,,,,"0|z1nr68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/24 02:28;robyoung;Hi, I've had a look into it and I think the repeated:
{code:java}
parse error: Invalid numeric literal at line 1, column 6
Error: No such container: {code}
is a symptom of a failure to start the aws-cli image, from earlier in the logs:
{code:java}
2024-03-02T04:10:55.5496990Z Unable to find image 'banst/awscli:latest' locally 2024-03-02T04:10:56.3857380Z docker: Error response from daemon: Head ""https://registry-1.docker.io/v2/banst/awscli/manifests/latest"": read tcp 10.1.0.97:33016->54.236.113.205:443: read: connection reset by peer. 2024-03-02T04:10:56.3857877Z See 'docker run --help'. 2024-03-02T04:10:56.4586492Z Error: No such object:{code}
this means that the `AWSCLI_CONTAINER_ID` was empty and hence the error `Error: No such container:` from the docker exec. The failed docker exec in the subshell does not cause a failure, for example:
{code:java}
export ABC=$(docker exec -it """" bash)
Error: No such container: 
[roby@roby flink-end-to-end-tests]$ echo $?
0 {code}
I tested locally by changing `banst/awscli` to a nonexistant name like `banstz/awscli`, which produces similar output.

I have a branch which fails fast if `AWSCLI_CONTAINER_ID` is unset/empty after the export and adds a single naive retry of `aws_cli_start`, since it looks like a transient network error. Keen to create a PR if that approach sounds okay to you.

 

 ;;;","13/Mar/24 15:09;mapohl;Sure, go ahead. Thanks for looking into it. I assigned the ticket to you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnFileStageTest.destroyHDFS timed out,FLINK-34568,13570628,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,04/Mar/24 07:24,04/Mar/24 07:24,04/Jun/24 20:40,,1.17.2,,,,,,,,,,Connectors / Hadoop Compatibility,,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58024&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=26698

{code}
Mar 02 07:28:56 ""Listener at localhost/33933"" #25 daemon prio=5 os_prio=0 tid=0x00007f08490be000 nid=0x12cae runnable [0x00007f082ebfc000]
Mar 02 07:28:56    java.lang.Thread.State: RUNNABLE
Mar 02 07:28:56 	at org.mortbay.io.nio.SelectorManager$SelectSet.stop(SelectorManager.java:879)
Mar 02 07:28:56 	- locked <0x00000000d7ae0030> (a org.mortbay.io.nio.SelectorManager$SelectSet)
[...]
Mar 02 07:28:56 	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2123)
Mar 02 07:28:56 	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2060)
Mar 02 07:28:56 	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2031)
Mar 02 07:28:56 	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2024)
Mar 02 07:28:56 	at org.apache.flink.yarn.YarnFileStageTest.destroyHDFS(YarnFileStageTest.java:90)
[...]
{code}

Looks like a HDFS issue during shutdown? This will most likely also affect newer versions because there was not much done in the Yarn space since 1.17 (hadoop was bumped in 1.17 itself; FLINK-29710).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-04 07:24:39.0,,,,,,,,,,"0|z1nr60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink task manager error occur, msg: Encountered error while consuming partitions",FLINK-34567,13570614,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yamanda,yamanda,04/Mar/24 03:49,04/Mar/24 06:20,04/Jun/24 20:40,,1.16.2,,,,,,,,,,,,,,,0,flink,,,"I deploy flink cluster (version: 1.16.2) and it run normally about 2 months, but recently i meet a problem. I see some sub tasks back pressure is high and the flink job is totally blocked(in pic1.jpg), these sub tasks are all in one task manager. so i stop the abnormal task manager and deploy flink job again, the problem is solved. I find some error log in the abnormal task manager:

2024-03-03 15:57:25,088 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestQueue [] - Encountered error while consuming partitions
org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection timed out

I check the abnormal task manager deployed machine. cpu, memory, network is as normal as other task manager deployed machine, so it doesn't look like a hardware problem.

What does it mean?

What should i do to solve this problem completely?",,,345600,345600,,0%,345600,345600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/24 03:38;yamanda;pic1.jpg;https://issues.apache.org/jira/secure/attachment/13067235/pic1.jpg",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,Mon Mar 04 06:13:25 UTC 2024,,,,,,,,,,"0|z1nr2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/24 06:13;yamanda;flink version: 1.16.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kubernetes Operator reconciliation parallelism setting not work,FLINK-34566,13570613,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Fei Feng,Fei Feng,Fei Feng,04/Mar/24 03:23,08/Mar/24 10:37,04/Jun/24 20:40,08/Mar/24 10:37,kubernetes-operator-1.7.0,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"After we upgrade JOSDK to version 4.4.2 from version 4.3.0 in FLINK-33005 , we can not enlarge reconciliation parallelism , and the maximum reconciliation parallelism was only 10. This results FlinkDeployment and SessionJob 's reconciliation delay about 10-30 seconds when we have a large scale flink session cluster and session jobs in k8s cluster。
 

After investigating and validating, I found the reason is the logic for reconciliation thread pool creation in JOSDK has changed significantly between this two version. 

v4.3.0: 
reconciliation thread pool was created as a FixedThreadPool ( maximumPoolSize was same as corePoolSize), so we pass the reconciliation thread and get a thread pool that matches our expectations.

!image-2024-03-04-10-58-37-679.png|width=497,height=91!

[https://github.com/operator-framework/java-operator-sdk/blob/v4.3.0/operator-framework-core/src/main/java/io/javaoperatorsdk/operator/api/config/ConfigurationServiceOverrider.java#L198]

 

but in v4.2.0:

the reconciliation thread pool was created as a customer executor which we can pass corePoolSize and maximumPoolSize to create this thread pool.The problem is that we only set the maximumPoolSize of the thread pool, while, the corePoolSize of the thread pool is defaulted to 10. This causes thread pool size was only 10 and majority of events would be placed in the workQueue for a while.  

!image-2024-03-04-11-17-22-877.png|width=569,height=112!

[https://github.com/operator-framework/java-operator-sdk/blob/v4.4.2/operator-framework-core/src/main/java/io/javaoperatorsdk/operator/api/config/ExecutorServiceManager.java#L37]

 

the solution is also simple, we can create and pass thread pool in flink kubernetes operator so that we can control the reconciliation thread pool directly, such as:

!image-2024-03-04-11-31-44-451.png|width=483,height=98!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/24 02:58;Fei Feng;image-2024-03-04-10-58-37-679.png;https://issues.apache.org/jira/secure/attachment/13067232/image-2024-03-04-10-58-37-679.png","04/Mar/24 03:17;Fei Feng;image-2024-03-04-11-17-22-877.png;https://issues.apache.org/jira/secure/attachment/13067231/image-2024-03-04-11-17-22-877.png","04/Mar/24 03:31;Fei Feng;image-2024-03-04-11-31-44-451.png;https://issues.apache.org/jira/secure/attachment/13067234/image-2024-03-04-11-31-44-451.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Fri Mar 08 10:37:35 UTC 2024,,,,,,,,,,"0|z1nr2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/24 07:17;gyfora;You are saying that this is a bug in the JOSDK ? 
[https://github.com/operator-framework/java-operator-sdk]

Can you please also report this there, maybe it's already fixed and we can updgrade to a newer version?;;;","04/Mar/24 12:42;gyfora;Looking at this in detail I think it should work as expected. The thread pool max size is set correctly and we can have at most the defined parallelism number of threads.;;;","04/Mar/24 12:43;gyfora;Even if the core pool size is 10, the maxpoolsize defines how many threads are created maximum. After 1 minute of idling some of these threads are released and the pool can shrink when not in use. 

This is how it was designed.;;;","04/Mar/24 12:45;gyfora;From the java docs:

""A ThreadPoolExecutor will automatically adjust the pool size (see getPoolSize) according to the bounds set by corePoolSize (see getCorePoolSize) and maximumPoolSize (see getMaximumPoolSize). When a new task is submitted in method execute(Runnable), if fewer than corePoolSize threads are running, a new thread is created to handle the request, even if other worker threads are idle. Else if fewer than maximumPoolSize threads are running, a new thread will be created to handle the request only if the queue is full. By setting corePoolSize and maximumPoolSize the same, you create a fixed-size thread pool. By setting maximumPoolSize to an essentially unbounded value such as Integer.MAX_VALUE, you allow the pool to accommodate an arbitrary number of concurrent tasks. Most typically, core and maximum pool sizes are set only upon construction, but they may also be changed dynamically using setCorePoolSize and setMaximumPoolSize."";;;","04/Mar/24 15:04;gyfora;I am closing this ticket for now, if you feel that this resolution is incorrect please re-open it.;;;","05/Mar/24 02:44;Fei Feng;Thanks [~gyfora]  for comment this issue. 
Firstly, I am not saying JOSDK has bug, I think JOSDK just change the creation logic about reconciliation thread pool.
And, Yes, ""The thread pool max size is set correctly"", but, actually, we can never have at most parallelism as we defined. 
As [Java doc|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/ThreadPoolExecutor.html] saying:
""If there are more than corePoolSize but less than maximumPoolSize threads running, {*}a new thread will be created only if the queue is full.{*}""

this means the we can only have a parallelism equals corePoolSize, other event was put in workqueue and wait for handle. And Let's see about queueing's mechanism for detail：

""Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. {*}Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.){*}""

The Java doc states very clearly that when we using an unbounded queue ( JOSDK use LinkedBlockingQueue exactly :( ), the setting of maximumPoolSize is essentially ineffective, so we can only hava a parallelism equals corePoolSize. So In flink kubernetes operator , the  reconciliation parallism was only 10   (too small for a large scale k8s cluster),  even we give a large maximumPoolSize. ;;;","05/Mar/24 05:51;gyfora;Thanks for the detailed explanation you are completely right, I missed this part. Sounds actually like a bug to me in JOSDK. Can you open a PR to fix it on our part by replacing it with the fixed thread pool? (Or overriding the min parallelism as well to the max value );;;","05/Mar/24 06:36;Fei Feng;Of course, yes. Please assign this ticket for me , and I will open a PR  as soon as possible.  As for JOSDK, I will also open a issue to them to see how they understand this problem.
 ;;;","05/Mar/24 07:06;gyfora;Thanks [~Fei Feng] !;;;","05/Mar/24 15:34;gyfora;[~Fei Feng] here is the JOSDK side fix, can you please help review it?
[https://github.com/operator-framework/java-operator-sdk/pull/2262/files]

 ;;;","06/Mar/24 03:39;Fei Feng;OK, I have reviewed their changes.
 ;;;","08/Mar/24 10:37;gyfora;merged to main 726e484c6a9b4121563829bc094b3eebeb8ddcf3;;;",,,,,,,,,,,,,,,,,,,,,
Enhance flink kubernetes configMap to accommodate additional configuration files,FLINK-34565,13570540,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,surendralilhore,surendralilhore,02/Mar/24 06:28,03/Apr/24 09:21,04/Jun/24 20:40,,,,,,,,,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,"Flink kubernetes client currently supports a fixed number of files (flink-conf.yaml, logback-console.xml, log4j-console.properties) in the JM and TM Pod ConfigMap. In certain scenarios, particularly in app mode, additional configuration files are required for jobs to run successfully. Presently, users must resort to workarounds to include dynamic configuration files in the JM and TM. This proposed improvement allows users to easily add extra files by configuring the '{*}kubernetes.flink.configmap.additional.resources{*}' property. Users can provide a semicolon-separated list of local files in the client Flink config directory that should be included in the Flink ConfigMap.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 03 08:08:55 UTC 2024,,,,,,,,,,"0|z1nqmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 09:57;surendralilhore;[~zhuzh] can you review this ?;;;","27/Mar/24 12:56;zhuzh;IIUC, the requirement is to ship more user files, which may be needed by user code, to the pod. Supporting configuration files is just a special case of it. Shipping them via ConfigMap sounds a bit tricky to me.
cc [~wangyang0918];;;","03/Apr/24 08:08;surendralilhore;[~zhuzh] If the user files are dynamic, then it is very useful to support them in ConfigMap, especially in an App Mode cluster.

 A similar use case was discussed on the user mailing list: [link to the mailing list thread.|https://lists.apache.org/thread/md2zq0dbvt2dxytdfxw16jbfh02yq0w9]

[~wangyang0918] , Any thought about this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test case TableSourceITCase#testTableHintWithLogicalTableScanReuse,FLINK-34564,13570536,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,RocMarshal,RocMarshal,02/Mar/24 03:01,06/Mar/24 07:54,04/Jun/24 20:40,06/Mar/24 07:54,1.19.0,2.0.0,,,,,,,,,Tests,,,,,0,,,,"* branch 1.19 & master
 * java version 1.8
 * how to re-produce
 ** Add '@RepeatedTest' for TableSourceITCase#testTableHintWithLogicalTableScanReuse
 ** then run it
 ** !image-2024-03-02-11-01-12-718.png!
 ** !image-2024-03-02-11-01-44-431.png!",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29114,,,,,,,,,,,"02/Mar/24 03:01;RocMarshal;image-2024-03-02-11-01-12-718.png;https://issues.apache.org/jira/secure/attachment/13067217/image-2024-03-02-11-01-12-718.png","02/Mar/24 03:01;RocMarshal;image-2024-03-02-11-01-44-431.png;https://issues.apache.org/jira/secure/attachment/13067216/image-2024-03-02-11-01-44-431.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 07:54:20 UTC 2024,,,,,,,,,,"0|z1nqlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/24 03:19;RocMarshal;I discovered this issue while assisting [~fanrui]  in fixing a bug in try https://github.com/apache/flink/pull/24407#issuecomment-1970101117.

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57951&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&s=ae4f8708-9994-57d3-c2d7-b892156e7812;;;","02/Mar/24 03:19;lincoln.86xy;[~RocMarshal] Thanks for reporting this! Looks like this is a duplicate one with https://issues.apache.org/jira/browse/FLINK-29114. You can check the details via the pr discussions [#24390|https://github.com/apache/flink/pull/24390].

 ;;;","02/Mar/24 04:48;RocMarshal;Thx a lot for [~lincoln.86xy] your comments, I'll check the details via the pr mentioned above~ 

:);;;","06/Mar/24 07:54;mapohl;Thanks for reporting this, [~RocMarshal]. This test is especially flaky in github actions. And [~lincoln.86xy] is right: It's already covered by FLINK-29114. I'm gonna close this one in favor of FLINK-29114.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaling decision improvement,FLINK-34563,13570512,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yang,yang,01/Mar/24 17:20,12/Mar/24 11:54,04/Jun/24 20:40,,kubernetes-operator-1.7.0,,,,,,,,,,Kubernetes Operator,,,,,1,pull-request-available,,,"Hi, I'd like to propose a minor improvement based on my autoscaling experiments. The concept revolves around identifying the vertex with the highest level of parallelism and matching it to the maximum parallelism supported by our task manager.

The primary goal of this enhancement is to prevent any task slots from remaining unused after the Flink autoscaler performs a rescaling operation. I've already tested this modification in a custom build of the operator, excluding the memory tuning feature. However, I believe it could be beneficial, especially in scenarios where the memory tuning feature is not enabled.

And I have prepared this small pr also :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 16:40:03 UTC 2024,,,,,,,,,,"0|z1nqg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/24 16:40;gyfora;Copying over my comment from GitHub for completeness:



I have some concerns about this change:
 # It doesn't work with custom slot sharing configuration which is very common
 # It provides almost no benefit with large taskmanager sizes / low number of task slots.
 # It goes against some basic design philosophy in the autoscaler such that we do not scale vertices beyond their target capacity. It ties to [@mxm|https://github.com/mxm] 's question why the logic wouldn't apply to all vertices?

Taking that one step further why don't we scale all vertices to the same parallelism at that point? That would naturally cause more resource usage and less throughput. By the same logic I don't think we should scale even the largest ones further.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port Debezium Avro Confluent changes (FLINK-34509) to Chinese,FLINK-34562,13570509,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,vincentwoo,lorenzo.affetti,lorenzo.affetti,01/Mar/24 16:34,29/Mar/24 08:59,04/Jun/24 20:40,29/Mar/24 08:59,,,,,,,1.20.0,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,Port the changes applied in [https://issues.apache.org/jira/projects/FLINK/issues/FLINK-34509] to the Chinese version of the documentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 29 08:58:43 UTC 2024,,,,,,,,,,"0|z1nqfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/24 05:49;vincentwoo;I'm willing to contribute, could someone assign this to me?;;;","29/Mar/24 08:58;jingge;master: 3841f0622552674fe0013b89605594a401cb6902;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Downgrading flink-kubernetes-operator causes failure,FLINK-34561,13570463,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,asanwal,asanwal,01/Mar/24 11:13,03/Mar/24 05:32,04/Jun/24 20:40,01/Mar/24 12:20,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,"Hi,

We are currently using flink-kubernetes-operator 1.4 version (with v1beta1 CRD) to manage flink deployments.

We tried an upgrade to version 1.6 version, but when we try to rollback to 1.4, noticed that flink-operator fails to come up with this execption:
{noformat}
Exception in thread ""main"" io.javaoperatorsdk.operator.OperatorException: Error starting operator
	at io.javaoperatorsdk.operator.Operator.start(Operator.java:125)
	at org.apache.flink.kubernetes.operator.FlinkOperator.run(FlinkOperator.java:215)
	at org.apache.flink.kubernetes.operator.FlinkOperator.main(FlinkOperator.java:229)
Caused by: io.javaoperatorsdk.operator.OperatorException: java.lang.IllegalStateException: com.fasterxml.jackson.databind.JsonMappingException: Could not deserialize spec, this indicates a bug... (through reference chain: org.apache.flink.kubernetes.operator.api.FlinkDeployment[""status""]->org.apache.flink.kubernetes.operator.api.status.FlinkDeploymentStatus[""lifecycleState""])
	at io.javaoperatorsdk.operator.api.config.ExecutorServiceManager.lambda$executeAndWaitForAllToComplete$2(ExecutorServiceManager.java:107)
	at java.base/java.util.ArrayList.forEach(Unknown Source)
	at io.javaoperatorsdk.operator.api.config.ExecutorServiceManager.executeAndWaitForAllToComplete(ExecutorServiceManager.java:102)
	at io.javaoperatorsdk.operator.api.config.ExecutorServiceManager.boundedExecuteAndWaitForAllToComplete(ExecutorServiceManager.java:82)
	at io.javaoperatorsdk.operator.ControllerManager.start(ControllerManager.java:36)
	at io.javaoperatorsdk.operator.Operator.start(Operator.java:120)
	... 2 more
Caused by: java.lang.IllegalStateException: com.fasterxml.jackson.databind.JsonMappingException: Could not deserialize spec, this indicates a bug... (through reference chain: org.apache.flink.kubernetes.operator.api.FlinkDeployment[""status""]->org.apache.flink.kubernetes.operator.api.status.FlinkDeploymentStatus[""lifecycleState""])
	at io.javaoperatorsdk.operator.api.config.ConfigurationService$1.clone(ConfigurationService.java:112)
	at java.base/java.util.Optional.map(Unknown Source)
	at io.javaoperatorsdk.operator.processing.event.source.informer.InformerManager.get(InformerManager.java:175)
	at io.javaoperatorsdk.operator.processing.event.source.informer.ManagedInformerEventSource.get(ManagedInformerEventSource.java:110)
	at io.javaoperatorsdk.operator.processing.event.EventProcessor.submitReconciliationExecution(EventProcessor.java:128)
	at io.javaoperatorsdk.operator.processing.event.EventProcessor.handleMarkedEventForResource(EventProcessor.java:120)
	at io.javaoperatorsdk.operator.processing.event.EventProcessor.handleAlreadyMarkedEvents(EventProcessor.java:376)
	at io.javaoperatorsdk.operator.processing.event.EventProcessor.start(EventProcessor.java:371)
	at io.javaoperatorsdk.operator.processing.Controller.start(Controller.java:336)
	at io.javaoperatorsdk.operator.ControllerManager.lambda$start$0(ControllerManager.java:37)
	at io.javaoperatorsdk.operator.api.config.ExecutorServiceManager.lambda$executeAndWaitForAllToComplete$0(ExecutorServiceManager.java:96)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: com.fasterxml.jackson.databind.JsonMappingException: Could not deserialize spec, this indicates a bug... (through reference chain: org.apache.flink.kubernetes.operator.api.FlinkDeployment[""status""]->org.apache.flink.kubernetes.operator.api.status.FlinkDeploymentStatus[""lifecycleState""])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:316)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:782)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:733)
	at io.fabric8.kubernetes.model.jackson.BeanPropertyWriterDelegate.serializeAsField(BeanPropertyWriterDelegate.java:66)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:774)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:480)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:319)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4624)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:3869)
	at io.javaoperatorsdk.operator.api.config.ConfigurationService$1.clone(ConfigurationService.java:110)
	... 14 more
Caused by: java.lang.RuntimeException: Could not deserialize spec, this indicates a bug...
	at org.apache.flink.kubernetes.operator.api.utils.SpecUtils.deserializeSpecWithMeta(SpecUtils.java:64)
	at org.apache.flink.kubernetes.operator.api.status.ReconciliationStatus.deserializeLastReconciledSpecWithMeta(ReconciliationStatus.java:71)
	at org.apache.flink.kubernetes.operator.api.status.ReconciliationStatus.deserializeLastReconciledSpec(ReconciliationStatus.java:59)
	at org.apache.flink.kubernetes.operator.api.status.CommonStatus.getLifecycleState(CommonStatus.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:689)
	at io.fabric8.kubernetes.model.jackson.BeanPropertyWriterDelegate.serializeAsField(BeanPropertyWriterDelegate.java:66)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:774)
	... 24 more
Caused by: com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field ""ephemeralStorage"" (class org.apache.flink.kubernetes.operator.api.spec.Resource), not marked as ignorable (2 known properties: ""cpu"", ""memory""])
 at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.apache.flink.kubernetes.operator.api.spec.FlinkDeploymentSpec[""jobManager""]->org.apache.flink.kubernetes.operator.api.spec.JobManagerSpec[""resource""]->org.apache.flink.kubernetes.operator.api.spec.Resource[""ephemeralStorage""])
	at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:61)
	at com.fasterxml.jackson.databind.DeserializationContext.handleUnknownProperty(DeserializationContext.java:1132)
	at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:2202)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1705)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownVanilla(BeanDeserializerBase.java:1683)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:320)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:177)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:314)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:177)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:314)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:177)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)
	at com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4706)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2879)
	at com.fasterxml.jackson.databind.ObjectMapper.treeToValue(ObjectMapper.java:3343)
	at org.apache.flink.kubernetes.operator.api.utils.SpecUtils.deserializeSpecWithMeta(SpecUtils.java:60)
	... 34 more
{noformat}
Looks like some FlinkDeployments were created by 1.6 version and later 1.4 was able to handle those.

But as per [docs|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/compatibility/], the FlinkDeployment created by 1.6 should have been backward compatible with 1.4.
{quote}Starting from v1beta1 (operator version 1.0.0) we aim to provide backward compatibility for the already deployed Flink custom resources (FlinkDeployment, FlinkSessionJob).
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 03 05:32:24 UTC 2024,,,,,,,,,,"0|z1nq5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 12:20;gyfora;This works as expected. CRD and the operator is backward compatible, not forward compatible. As you see in the docs:

```
This means that if you have Flink resources deployed (and Flink applications running), you can still safely upgrade to newer versions of the operator and CRD without any problems.
```

We do not currently provide any guarantee for downgrading the operator as it may populate new status fields that are not compatible with previous operator versions.

However newer operator versions are much more flexible on JSON deserialisation. So I would assume that going back to 1.6 from 1.7 would not be an issue however we do not have explicit guarantee there.

We recommend upgrading to the latest prod release: 1.7.0;;;","03/Mar/24 05:32;asanwal;[~gyfora], thanks for checking! Apologies, I misunderstood the intent behind backward compatibility. Now I am clear that the downgrade is not forward compatible currently.

Still, I'd like to request if this is possible to keep the forward compatibility in mind for future releases, considering that due to unexpected issues, if we need to rollback the operator to older version, currently it would cause significant downtime.

Glad to hear that deserialization is already made more liberal in 1.7.0, we would consider upgrading to it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinITCase seems to fail on a broader scale (MiniCluster issue?),FLINK-34560,13570455,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,01/Mar/24 10:19,01/Mar/24 11:36,04/Jun/24 20:40,01/Mar/24 11:36,1.18.1,,,,,,,,,,Tests,,,,,0,github-actions,test-stability,,"https://github.com/apache/flink/actions/runs/8105495458/job/22154140154#step:10:11906

The {{o.a.f.table.planner.runtime.stream.table.JoinITCase}} test methods share the same MiniCluster. Job 0d045f91b959c15f49ab18f0613aebd6 is submitted in test {{JoinITCase#testJoinWithMultipleKeys[StateBackend=ROCKSDB]}}. This is the last test that successfully ran. But the job gets reassigned to the cluster before the next test {{testInnerJoinWithExpressionPreds[stateBackend=ROCKSDB]}} submits its job e54a8aec0b89012d66a1cf7e4b3c283f. 0d045f91b959c15f49ab18f0613aebd6 occupies all the resources (4 slots) which won't become available for the subsequent test runs and make them fail.

{code}
Missing resources:
         Job e54a8aec0b89012d66a1cf7e4b3c283f
                ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}
Current resources:
        TaskManager 3be92028-41f6-4c4c-891b-57ec9d150bfd
                Available: ResourceProfile{cpuCores=0, taskHeapMemory=0 bytes, taskOffHeapMemory=0 bytes, managedMemory=0 bytes, networkMemory=0 bytes}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34227,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 01 11:36:07 UTC 2024,,,,,,,,,,"0|z1nq3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 11:36;mapohl;This is the same issue that's already reported with FLINK-34227. I'm closing this issue in favor of FLINK-34227.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TVF Window Aggregations might get stuck,FLINK-34559,13570453,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,roman,roman,roman,01/Mar/24 10:06,04/Jun/24 08:15,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,1.20.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,"RecordsWindowBuffer flushes buffered records in the following cases:
 * watermark
 * checkpoint barrier
 * buffer overflow

 

In two-phase aggregations, this creates the following problems:

1) Local aggregation: enters hard-backpressure because for flush, it outputs the data downstream and doesn't check network buffer availability

This already disrupts normal checkpointing and watermarks progression

 

2) Global aggregation: 

When the window is large enough and/or the watermark is lagging, lots of data is flushed to state backend (and the state is updated) in checkpoint SYNC phase.

 

All this eventually causes checkpoint timeouts (10 minutes in our env).

 

Example query
{code:java}
INSERT INTO `target_table` 

SELECT window_start, window_end, some, attributes, SUM(view_time) AS total_view_time, COUNT(*) AS num, LISTAGG(DISTINCT page_url) AS pages 

FROM TABLE(TUMBLE(TABLE source_table, DESCRIPTOR($rowtime), INTERVAL '1' HOUR)) 

GROUP BY window_start, window_end, some, attributes;{code}
In our setup, the issue can be reproduced deterministically.

 

As a quick fix, we might want to:
 # limit the amount of data buffered in Global Aggregation nodes
 # disable two-phase aggregations, i.e. Local Aggregations (we can try to limit buffing there two, but network buffer availability can not be easily checked from the operator)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 01:51:55 UTC 2024,,,,,,,,,,"0|z1nq34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/24 01:51;xuyangzhong;Hi, [~roman], I'm a little interested in how you would solve this problem. 

> limit the amount of data buffered in Global Aggregation nodes

IIUC, for non-session global window aggregation, it only stores the aggregated results in the state. I am a little curious about what buffering the data means here(maybe you mean input network buffer in operator level, right?).

> disable two-phase aggregations

Even if two-stage optimization is disabled, if the user's watermark interval is set very long, or the window is set very large, then in the global window aggregation node, the state will still be updated frequently.

As you mentioned, I believe it is now difficult to relate the availability of network buffers to the optimization of the plan, such as two-phase optimization. So, I'm somewhat looking forward to seeing your solution. :)

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RocksDB key/value size metrics,FLINK-34558,13570430,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hejufang001,hejufang001,01/Mar/24 08:10,08/Mar/24 01:46,04/Jun/24 20:40,,1.19.0,,,,,,,,,,Runtime / State Backends,,,,,0,,,,"In some scenarios, the poor performance of RocksDB may be caused by too large key/value size, but now there is a lack of metrics for key/value size. By adding these metrics, we can conveniently calculate the distribution of key/value size, such as the average size and p99 size.  To reduce the negative impact of adding metric on RocksDB performance, we could reduce the impact by supporting sampling.

The possible implementation is as follows: After the RocksDB key/value serialization, we could obtain the byte array and report the size of the array through histogram metrics.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 01:46:29 UTC 2024,,,,,,,,,,"0|z1npy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 01:46;masteryhx;I think this mertic maybe useful, but since it's under the critical path of every element, we should treat it very carefully.

So I'd suggest to start with:
 # try to implement this just like state-latency track (just sampling and be disabled by default).
 # micro benchmark and show result firstly (maybe three results: before this pr, disabed, enabled).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When the Flink task ends in application mode, there may be issues with the Znode and HDFS files not being deleted",FLINK-34557,13570429,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tltheshy,tltheshy,01/Mar/24 07:59,14/Mar/24 11:07,04/Jun/24 20:40,,1.16.2,1.17.0,,,,,,,,,Deployment / YARN,Runtime / Task,,,,0,,,,"In Flink 1.16.2, we all use application mode to submit tasks to Yarn. However, there are several situations during use that result in Znode not being deleted and some files on HDFS not being deleted. These should be deleted after the task is stopped, otherwise it may cause some resource occupancy problems. Below are the several situations I have encountered:
 # After the Flink task is submitted to the cluster, if there is a conflict or missing jar package, the task will be restarted multiple times by Yarn and ultimately fail to end. At this point, it will be found that the Znode persists, and there are files with corresponding appids in the '/.flink' directory and '/flink/recovery' directory in HDFS;
 # When using the yarn kill command to kill a task, the task ends directly and the final state is killed, with the final result being the same as the first one;
 # When the Flink task is disconnected from zk (we will not analyze the specific reason for the disconnection), if zk is disconnected from the jm container, the task will hang and be pulled back by yarn. When the last disconnection occurs, the task will eventually end and the same result as above will appear;

!image-2024-03-01-15-38-48-396.png|width=877,height=171!

!image-2024-03-01-15-39-13-953.png|width=882,height=174!

 

!image-2024-03-01-15-39-39-524.png|width=1001,height=67!

 
*Add：*
Through consulting with the community and other colleagues, we found that the community had previously raised the issue of Znode not being deleted. Later, by adding the closeAndCleanupAllData# method, it was uniformly deleted at the end of a highly available cluster. However, in the aforementioned situations, there are still issues with file and data residue. Among them, when using the yarn kill command, after successfully submitting a task to the cluster, Flink would indicate through console logs that there would be HDFS file residue after successfully submitting tasks to the cluster, however, I don't understand why the community did not improve this and instead retained the existence of this situation. At the same time, we believe that Znode residue should not exist, regardless of the task status, it must be cleaned up after stopping the task",,,,,,,,,,,FLINK-6522,FLINK-10694,FLINK-11336,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/24 07:38;tltheshy;image-2024-03-01-15-38-48-396.png;https://issues.apache.org/jira/secure/attachment/13067194/image-2024-03-01-15-38-48-396.png","01/Mar/24 07:39;tltheshy;image-2024-03-01-15-39-13-953.png;https://issues.apache.org/jira/secure/attachment/13067193/image-2024-03-01-15-39-13-953.png","01/Mar/24 07:39;tltheshy;image-2024-03-01-15-39-39-524.png;https://issues.apache.org/jira/secure/attachment/13067192/image-2024-03-01-15-39-39-524.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 11:07:47 UTC 2024,,,,,,,,,,"0|z1npxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 08:15;tltheshy;I hope someone can discuss it and raise relevant questions for communication。

And in our company, I have already fixed it through some means. The specific repair logic is to add a shutdownhook to YarnApplicationClusterEntryPoint, which will execute some actions before jm hangs up. We can add deletion logic to this hook, and operate the corresponding files by creating a new YarnClient and a CuratorClient. If  #getYarnApplicationState() results in killed, delete the file with the corresponding appid directly. If not, use #yarnClient.getApplication Attempts() to determine whether jm hung up last time and is in valid time. Within the interval, residual files will also be deleted if the conditions are met。;;;","05/Mar/24 07:34;mapohl;hi [~tltheshy], thanks for sharing your observations. can you share some JobManager logs as well to get a better understanding of what's going on?

{quote}
After the Flink task is submitted to the cluster, if there is a conflict or missing jar package, the task will be restarted multiple times by Yarn and ultimately fail to end. At this point, it will be found that the Znode persists, and there are files with corresponding appids in the '/.flink' directory and '/flink/recovery' directory in HDFS;
[...]
When the Flink task is disconnected from zk (we will not analyze the specific reason for the disconnection), if zk is disconnected from the jm container, the task will hang and be pulled back by yarn. When the last disconnection occurs, the task will eventually end and the same result as above will appear;
{quote}
This sounds like the job failed which should result in the job reaching the globally terminal state with the data being cleaned up. Here, JM logs would be helpful to understand what happens.

{quote}
When using the yarn kill command to kill a task, the task ends directly and the final state is killed, with the final result being the same as the first one;
{quote}
Isn't this a normal failover where you don't want to delete the tasks to be able to recover from it? Or am I misunderstanding you here?;;;","13/Mar/24 03:19;tltheshy;hi [~mapohl] ,thanks for your comment, I'm sorry for not replying to you for so long.

The jobmanager log, like other tasks, can be 100% replicated in some scenarios. For example, you can submit a WordCount task through application mode, and then use the  ‘ Yarn application - kill ’  command to kill the task. At this point, the remaining znode in zk will not be deleted, and files in two different folders on hdfs will also have residues. Similarly, you can also proactively create situations where jar packages are missing or dependency conflicts occur. For example, when Flink Histroy submits a simple SQL task like WordCount through application mode, several jar packages related to the table are intentionally missing. In this case, the task will not run continuously after being submitted to the Yarn cluster, and Yarn will repeatedly pull it up, but the last attempt will still fail to end due to package shortage. After the task ends, the situation where Znode and files are not cleaned up will also occur。

 

For the phenomenon of residual Znode and HDFS files not being cleaned up, if the task runs normally and ends, or if the task ends abnormally during the client submission phase, these data nodes or HDFS files will be directly deleted because the Flink framework takes these into consideration. But in the two scenarios I mentioned, residual situations may occur, so I think this aspect should also need improvement. There should not be any issues with the Znode and HDFS files generated by Flink remaining after the final task is completed;;;","13/Mar/24 03:29;tltheshy;My current temporary solution is to add a shutdownhook to the YarnApplicationClusterEntryPoint class, which will determine whether to clean up files and znode based on the condition when jm exits. If yarnFinalState is KILLED, then directly clean up the znode and hdfs files corresponding to the corresponding application ID. Similarly, if the task fails multiple times and Yarn pulls multiple times but still fails the last time, the cleaning action will also be executed.  How do you evaluate this approach？[~mapohl] ;;;","14/Mar/24 11:07;mapohl;It would be still helpful if you could provide the logs instead of describing how to reproduce it because it appears that you have your Yarn setup already running (ideally with debug logs enabled).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate EnumerableToLogicalTableScan,FLINK-34556,13570408,13565339,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,01/Mar/24 04:25,08/Apr/24 09:17,04/Jun/24 20:40,08/Apr/24 09:17,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 09:17:15 UTC 2024,,,,,,,,,,"0|z1npt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 09:17;Sergey Nuyanzin;Merged as [d9510893004c7cf86d45bed941041600beb20158|https://github.com/apache/flink/commit/d9510893004c7cf86d45bed941041600beb20158];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate JoinConditionTypeCoerceRule,FLINK-34555,13570404,13565339,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,01/Mar/24 03:28,15/May/24 05:28,04/Jun/24 20:40,,1.20.0,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-03-01 03:28:11.0,,,,,,,,,,"0|z1nps8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using EXACTLY_ONCE with KafkaSink cause broker's OOM due to newly created transactionalId per checkpoint,FLINK-34554,13570373,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hilmialf,hilmialf,29/Feb/24 21:37,16/Mar/24 06:20,04/Jun/24 20:40,,1.16.3,1.17.2,1.18.1,,,,,,,,Connectors / Kafka,,,,,0,,,,"Flink version: 1.17.1
Kafka Broker version: 2.7.1 * 4 GB heap memory for each

Hi, We recently had an outage in our production system after we perform a Flink kafka-connector API upgrade. To give a brief context, our application is a simple kafka-to-kafka pipeline with minimal processing. We run in EXACTLY_ONCE mode, thus kafka transaction is involved.

Our application runs with total around 350 sink subtask. Checkpoint period was set to 5 seconds to avoid blocking {{read_committed}} consumers too long. We recently performed an upgrade with the following details:

Previous state:
 * Flink version: 1.14.4
 * Broker version: 2.7.1
 * kafka connector API: FlinkKafkaProducer

Update to:
 * Flink version: 1.17.1
 * Broker version: 2.7.1
 * kafka connector API: KafkaSink

Around 10 hours after the deployment, our kafka broker started to failing with OOM error. Heap dump entries are dominated by the ProducerStateEntry records.
Our investigation leads to finding the total implementation change between FlinkKafkaProducer and KafkaSink.
 * KafkaSink generate different transactionalId for each checkpoint,
 * FlinkKafkaProducer uses constant set of transactionalId pool.

With this behavior, KafkaSink seemed to exhaust our broker heap very fast and the ProducerStateEntry will only expire after [transactional.id.expiration.ms|http://transactional.id.expiration.ms/] , which by default is set to 7 days.  ([ref1|https://github.com/apache/kafka/blob/61dbce85d0d41457d81a4096ecaea049f3a4b3ae/core/src/main/scala/kafka/log/Log.scala#L677], [ref2|https://github.com/apache/kafka/blob/61dbce85d0d41457d81a4096ecaea049f3a4b3ae/core/src/main/scala/kafka/log/LogManager.scala#L268], [ref3|https://github.com/apache/kafka/blob/61dbce85d0d41457d81a4096ecaea049f3a4b3ae/core/src/main/scala/kafka/log/LogManager.scala#L1207])

For our job, it means it creates roughly:
 * 10 hour running) 350 ids * 12 times/minute * 60 min/hour * 10 hour ~ 2,520,000
 * 7 days) ~ 42mil entries.

Attached below is the number of ProducerStateEntry entries of heap dump when it is OOM:
 * 505,000 (6.5%), in total it would be roughly ~ 7,000,000 entries.

There are several things that come up in our mind to mitigate the drawbacks such as:
 * reduce the number of subtasks, so it reduces the number of transactionalId
 * Enlarge the checkpoint period to reduce the newly generated transactionalId rate.
 * Shorten [transactional.id.expiration.ms|http://transactional.id.expiration.ms/] to expire the unused transactionalId soon.
 * Increase the broker heap

However, above mitigation might be too cumbersome and need careful tuning which harm our flexibility.In addition, due to the lack of maintaining lingering transaction state, TransactionAborter seems to abort old transaction naively. We might be accidentally (or purposefully) reuse the same transactionalIdPrefix and start the counter from 0. In that case, if the old transactionalId happens to have epoch >0, it will keep looping aborting the nonexistent transactions up to the latest checkpoint counter (which may be too big) and make the job stuck.

Btw, I am aware that in Flink 2.0, you guys are putting a lot of effort on creating better integration with Kafka transaction ([FLIP-319|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=255071710]). In FLIP-319, it mentions something about TID pooling. However, it is seem that there is no relevant page yet for it, so I wonder whether there are any concrete plan already that I can follow, or if there is something I can contribute to, I will be really happy to help.
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Feb/24 21:37;hilmialf;image (4).png;https://issues.apache.org/jira/secure/attachment/13067172/image+%284%29.png","29/Feb/24 21:37;hilmialf;image (5).png;https://issues.apache.org/jira/secure/attachment/13067171/image+%285%29.png","16/Mar/24 06:17;hilmialf;image-2024-03-16-17-17-16-152.png;https://issues.apache.org/jira/secure/attachment/13067478/image-2024-03-16-17-17-16-152.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 16 06:20:34 UTC 2024,,,,,,,,,,"0|z1nplc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 10:57;martijnvisser;How did you migrate from the FlinkKafkaProducer to the KafkaSink? Did you follow the instructions as outlined in the release notes, https://nightlies.apache.org/flink/flink-docs-release-1.14/release-notes/flink-1.14/#port-kafkasink-to-new-unified-sink-api-flip-143 ?;;;","08/Mar/24 11:09;hilmialf;Hi [~martijnvisser] , yes. We stop with a savepoint and use different transactionalId prefix so that new transactionalId does not conflict with the one generated by the old job;;;","08/Mar/24 11:16;martijnvisser;I think it will be hard to find something to mitigate this problem at this point. You're mentioning that you're using Kafka 2.7, but Flink uses the Kafka Client 3.2.3 in version 1.17. Since the Flink Kafka connector relies on reflection, I could very well imagine that this causes some issues along the way. The only thing that you could try is actually use the latest Flink Kafka connector version, which is 3.1.0 and is still compatible with Flink 1.17 as well. 

Until the 2PC on both the Kafka side and the Flink side are available, there is only so much that we can do from an improvement perspective. ;;;","08/Mar/24 11:29;hilmialf;[~martijnvisser] IIUC my situation correctly, the problem was more on the different transactional-id generated per checkpoint (unlike previous FlinkKafkaProducer where it uses pooling), so it creates a lot of transactional-id entry on broker. 

Also, IIUC, the reflection is only be used to restore the producer state in the committables so that later on we can commit the transaction to complete the 2PC. While this may cause some other issue, I believe this does not have something to do with the main problem I am trying to address here.;;;","08/Mar/24 11:39;hilmialf;> The only thing that you could try is actually use the latest Flink Kafka connector version, which is 3.1.0 and is still compatible with Flink 1.17 as well

Btw what do you mean by this part?;;;","08/Mar/24 11:40;martijnvisser;IIRC it was a deliberate choice to not reusing the pooling, but I can't quickly find anymore why that was the case. That the implementation differs isn't too surprising: FlinkKafkaProducer used the SinkFunction, while KafkaSink uses Flink's unified Sink API. ;;;","08/Mar/24 11:41;martijnvisser;{quote}> The only thing that you could try is actually use the latest Flink Kafka connector version, which is 3.1.0 and is still compatible with Flink 1.17 as well

Btw what do you mean by this part?{quote}

You're not using the latest version of the Flink Kafka connector. The one that's bundled with Flink 1.17 is outdated. Since Flink connectors have been externalized, you'll need to make sure that you're updating to the latest version yourself. For the Flink Kafka connector, that's currently 3.1.0-1.17;;;","08/Mar/24 11:48;hilmialf;{quote}You're not using the latest version of the Flink Kafka connector. The one that's bundled with Flink 1.17 is outdated. Since Flink connectors have been externalized, you'll need to make sure that you're updating to the latest version yourself. For the Flink Kafka connector, that's currently 3.1.0-1.17
{quote}
I see. I thought I can just use the connector bundled with flink 1.17.

Btw do you think it will be stable to keep using FlinkKafkaProducer (bundled with connector 3.1.0) with flink 1.17? If so I would like to proceed this way since I believe if I use KafkaSink (even with connector 3.1.0), the same issue will happen again since its underlying implementation is generally the same (recreate transactionId per checkpoint).
{quote}IIRC it was a deliberate choice to not reusing the pooling, but I can't quickly find anymore why that was the case. That the implementation differs isn't too surprising: FlinkKafkaProducer used the SinkFunction, while KafkaSink uses Flink's unified Sink API.
{quote}
If you could somehow find reference on why pooling was not chosen for the new KafkaSink, I would love to know!;;;","08/Mar/24 11:51;martijnvisser;You can definitely stick with FlinkKafkaProducer if that's what you prefer. I do think that we would like to remove it in one of the upcoming versions. ;;;","08/Mar/24 11:58;hilmialf;Honestly I am afraid if you will remove FlinkKafkaProducer before we have alternatives other than current KafkaSink behavior. Would you not consider this (the fact that different transactionalId can cause broker OOM) as an issue to be properly addressed though?;;;","08/Mar/24 12:06;martijnvisser;The problem is that I can't be sure that this is indeed a problem for everyone, or just for you. You're running an almost 3 year old Kafka version (that's also no longer supported in the Kafka community), and there have been many bug fixes since then. You are also not running the latest Flink Kafka connector. I do know some other rather large Flink-Kafka setups where the new KafkaSink runs without any problems, both for Flink and for Kafka. ;;;","08/Mar/24 12:18;hilmialf;I see. Let me get back to you by reproducing the behavior with latest Kafka version.

I think I can reproduce it by trying to overwhelm the broker with transactions that has different transactionalIds.
If I can see a heap increase that cannot be cleaned within the [transaction.max.timeout.ms|https://kafka.apache.org/documentation/#brokerconfigs_transaction.max.timeout.ms] interval, that should be an indication if the KafkaSink behavior can cause issue even with latest kafka.

 
{quote}I do know some other rather large Flink-Kafka setups where the new KafkaSink runs without any problems, both for Flink and for Kafka
{quote}
Excuse me, but I am just wondering the those large setup handles with similar condition, that is:
 * running EXACTLY_ONCE
 * have many sink subtasks
 * run with short checkpoint interval 

I think that problem won't appear if any of the conditions above is absent

 ;;;","16/Mar/24 06:20;hilmialf;Hi [~martijnvisser],
I just reproduced the behavior with one of the recent version of Kafka (3.6.1)

Here is the java code. It basically does: 1) initTransaction 2) send one record 3) commit.
{code:java}
public class KafkaSinkMimic {
    private static final Logger logger = LoggerFactory.getLogger(KafkaSinkMimic.class);
    public static String BOOTSTRAP_SERVERS = ""localhost:9092"";
    public static final String TOPIC = ""test-kafkasink-topic"";
    public static void main(String[] args) throws InterruptedException {
        if (args.length < 2){
            System.out.println(""Usage: java -cp <jarname> KafkaSinkMimic <num_subtasks> <checkpoint_interval_ms> <bootstrap_server>"");
        }
        int numSubtasks = Integer.parseInt(args[0]);
        int checkpointIntervalMs = Integer.parseInt(args[1]);
        if (args.length == 3){
            KafkaSinkMimic.BOOTSTRAP_SERVERS = args[2];
        }
        System.out.println(""Running with args "" + Arrays.asList(args).stream().collect(Collectors.joining("" "")));
        KafkaSinkMimic mimic = new KafkaSinkMimic(checkpointIntervalMs, numSubtasks);
        mimic.run();
    }

    private int checkpointInterval = 5000;
    private int numSubtasks = 1;
    private long currentTs;
    private Executor executor;
    public KafkaSinkMimic(int checkpointInterval, int numSubtasks){
        this.checkpointInterval = checkpointInterval;
        this.numSubtasks = numSubtasks;
        this.currentTs = Instant.now().getEpochSecond();
        this.executor = Executors.newFixedThreadPool(numSubtasks);
    }

    public void run() throws InterruptedException {
        int checkpointId = 0;
        String transactionalIdPrefix = System.getenv(""HOSTNAME"") + ""-"" + currentTs + ""-"";
        while(true){
            for(int i = 0; i<numSubtasks; i++){
                int subtaskId = i;
                int finalCheckpointId = checkpointId;
                executor.execute(() -> this.runSubtask(transactionalIdPrefix, subtaskId, finalCheckpointId));
            }
            Thread.sleep(checkpointInterval);
            checkpointId++;
        }
    }

    public void runSubtask(String transactionalIdPrefix, int subtaskId, int checkpointId) {
        Properties props = new Properties();
        props.put(""acks"", ""all"");
        props.put(""transaction.timeout.ms"", ""900000"");
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        String transactionalId = transactionalIdPrefix + subtaskId + ""-"" + checkpointId;
        props.put(""transactional.id"", transactionalId);
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        producer.initTransactions();
        try {
            logger.info(""Starting transaction with id {}"", transactionalId);
            producer.beginTransaction();
            producer.send(new ProducerRecord<>(TOPIC, Integer.toString(subtaskId), Instant.now().toString()));
            producer.commitTransaction();
        } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {
            producer.close();
        } catch (KafkaException e) {
            producer.abortTransaction();
        }
        producer.close();
    }
} {code}
As you can see in the figure below, it consumes broker heap significantly.

!image-2024-03-16-17-17-16-152.png|width=750,height=284!

Compared to old kafka broker version, the expiration settings for the ProducerStateEntry has changed from [transactional.id.expiration.ms|https://kafka.apache.org/36/documentation.html#brokerconfigs_transactional.id.expiration.ms] (default 7 days) to [producer.id.expiration.ms|https://kafka.apache.org/36/documentation.html#brokerconfigs_producer.id.expiration.ms] (default 1 day) ([KIP-854|https://cwiki.apache.org/confluence/display/KAFKA/KIP-854+Separate+configuration+for+producer+ID+expiry]).

Still, for our usecase, it may cause OOM due to its large number of entries.

In any case, producer Id entries are also one of important concerns by Kafka team. There are several KIPs that tries to mitigate that issue:
 * [KIP-847: Add ProducerIdCount metrics|https://cwiki.apache.org/confluence/display/KAFKA/KIP-847%3A+Add+ProducerIdCount+metrics]
 * [KIP-854 Separate configuration for producer ID expiry|https://cwiki.apache.org/confluence/display/KAFKA/KIP-854+Separate+configuration+for+producer+ID+expiry]
 * [KIP-936: Throttle number of active PIDs|https://cwiki.apache.org/confluence/display/KAFKA/KIP-936%3A+Throttle+number+of+active+PIDs]

Well, as I said in the ticket, there are many possible tuning to ease the situation (increase broker heap, reduce checkpoint interval, use less subtasks, reduce producer.id.expiration.ms, etc), but it harms our flexibility a lot compared to using pooling as implemented by FlinkKafkaProducer.

Speaking about kafka new feature (2PC participant), I believe this issue will not be solved even after introducing the new feature if we still uses this generate-new-transactionalid-per-checkpoint approach.

Please let me know your thoughts on this ;;;",,,,,,,,,,,,,,,,,,,,
Time travel support by Flink catalogs,FLINK-34553,13570362,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mfatihaktas,mfatihaktas,29/Feb/24 18:49,01/Mar/24 23:23,04/Jun/24 20:40,,,,,,,,,,,,,,,,,0,,,,"I am trying to add time travel support for the Flink backend in [Ibis|https://github.com/ibis-project/ibis].

I found that Flink requires the {{catalog}} to implement {{getTable(ObjectPath tablePath, long timestamp)}} for time travel support:
Attention: Currently, time travel requires the corresponding catalog that the table belongs to implementing the getTable(ObjectPath tablePath, long timestamp) method. See more details in Catalog.
[[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/time-travel/]]


The default {{GenericInMemoryCatalog}} does not seem to implement {{getTable()}} . I set up a {{hive metastore}} and created a {{{}hive catalog{}}}, but it turns out that, hive catalog also does not implement {{getTable()}} --  I wish Flink docs were more detailed about these ...
py4j.protocol.Py4JJavaError: An error occurred while calling o8.sqlQuery. : org.apache.flink.table.api.ValidationException: SQL validation failed. getTable(ObjectPath, long) is not implemented for class org.apache.flink.table.catalog.hive.HiveCatalog.
I have two options now to continue with this: * Try another catalog, like Iceberg catalog, to see if that one implements {{getTable()}}
 ** {{{}{}}}I am not able to find information on whether a given catalog implements this function. Should I dig into their source code to figure this out, or is there any other way to find out?
 * Implement a custom catalog with {{{}getTable(){}}}, following the *very-brief-instructions* given in the [Flink doc|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/catalogs/#interface-in-catalog-for-supporting-time-travel].

Is there a doc, article, mailing list or anything else that I can use to get more information on
 * Which catalogs implement time travel support?
 * If there is not readily available catalog supporting time travel, then how can we implement a custom catalog with time travel support?

 ** The instructions given [here|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/catalogs/#interface-in-catalog-for-supporting-time-travel] are unfortunately not sufficient for a Java-illiterate person like myself.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 01 23:23:40 UTC 2024,,,,,,,,,,"0|z1npiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 01:38;hackergin;From what I currently know，only Paimon supports Flink's timetravel. For more details, please refer to the documentation: 

https://paimon.apache.org/docs/master/how-to/querying-tables/#batch-time-travel


And the source code: 
https://github.com/apache/incubator-paimon/blob/master/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java;;;","01/Mar/24 23:23;mfatihaktas;Thanks [~hackergin].

> only Paimon supports Flink's timetravel

The Flink doc does not imply this, but thank you for pointing me to Paimon. Today I tried Paimon catalog and verified that it supports time travel.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support message deduplication for input data sources,FLINK-34552,13570337,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,anohovsky,anohovsky,29/Feb/24 14:28,29/Feb/24 15:53,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,,0,,,,"My main proposal is to have duplicate message suppression logic as a part of Flink Table API to be able to suppress duplicates from the input sources. It might be a parameter provided by the user if they want to suppress duplicates from the input source or not. Below I provided more details about my use case and available approaches.

 

I have a Flink job which reads from two keyed kafka topics and emits messages to the keyed kafka topic. The Flink job executes the join query:

SELECT a.id, adata, bdata

FROM a

JOIN b

ON a.id = b.id

 

One of the input kafka topics produces messages with duplicate payload within PK in addition to meaningful data. That causes duplicates in the output topic and creates extra load to the downstream services.

 

I was looking for a way to suppress duplicates and I found two strategies which doesn't seem to work for my use case:
 #  Based on the deduplication window as a kafka[ sink buffer|https://github.com/apache/flink-connector-kafka/blob/main/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertSink.java#L46] for example. The Deduplication window doesn't work well for my case because the interval between duplicates is one day and I don't want my data to be delayed if I use such a big window.

 
 #  Using [ROW_NUMBER|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/sql/queries/deduplication/] . Unfortunately, this approach doesn't suit my use case either. Kafka topics a and b are CDC data streams and contain DELETE and REFRESH messages. If DELETE and REFRESH messages are coming with the same payload the job will suppress the last message which will lead to the incorrect output result. If I add message_type to the PARTITION key then the job will not be able to process messages sequences like this: DELETE->REFRESH->DELETE (with the same payload and PK), because the last message will be suppressed which will lead to the incorrect output result.

 

Finally, I had to create a separate custom Flink service which reads the output topic of the initial job and suppresses duplicates keeping hashes of the last processed message for each PK in the Flink state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 29 15:53:00 UTC 2024,,,,,,,,,,"0|z1npdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Feb/24 14:48;akhatkar;Adding an example for this

consider two tables
Table A
Fields :

{A, B, C, D}

Table B
Fields :

{A, E, F, G}

Query : Select A, B, F from table A join table b on field A

consider a case where the join will contain 1B records for \{A, B, C, D, E, F, G} but number of unique records for fields we are interested in are 1M \{A, B, F}.

Now any change that happens in fields C, D, E, G is going to produce records -U\{A, B, F} +U\{A, B, F} (as the join will emit changelog stream) but effectively the records which we are interested in hasn’t changed.;;;","29/Feb/24 15:12;martijnvisser;A couple of points from my point of view:

{quote}produces messages with duplicate payload within PK{quote}

That basically conflicts with the entire premise of Flink where a primary key constraint is a hint for Flink to leverage for optimizations.

{quote}The Deduplication window doesn't work well for my case because the interval between duplicates is one day and I don't want my data to be delayed if I use such a big window.{quote}

But how would it then work at the source? Aren't you just moving the problem from somewhere else in the logic to the front of the logic, since then the source would have to keep this interval there? You will still encounter a large delay in that case. 

{quote}Kafka topics a and b are CDC data streams and contain DELETE and REFRESH messages. {quote}

Are you actually using the upsert-kafka source, and treat these input sources as a changelog stream? 

I would like to understand as well how you would propose to have this logic expressed? Is it still SQL? Is it connector parameters?;;;","29/Feb/24 15:53;anohovsky;Thank you for the comment [~martijnvisser] 
{quote}> produces messages with duplicate payload within PK

That basically conflicts with the entire premise of Flink where a primary key constraint is a hint for Flink to leverage for optimizations.
{quote}
I'm not sure I understood your message. Here is an example of the message sequence for the kafka topic with duplicates: (key1, Refresh, payload1), (key2, Refresh, payload2), (key1, Refresh, payload1), (key2, Refersh, payload3)

 
{quote}
But how would it then work at the source? Aren't you just moving the problem from somewhere else in the logic to the front of the logic, since then the source would have to keep this interval there? You will still encounter a large delay in that case.
{quote}
As mentioned in the description of the ticket there are different strategies to deduplicate data stream. For my use case, I'm particularly interested in the last one where the job keeps hashes of the last message for each PK and filter out messages if the payload & message_type are not changed.

At some point I agree with you that it's a question about the best place for this logic. I had to create a separate service, however I'd like to see it as parameterized feature in Table API

 
{quote}Kafka topics a and b are CDC data streams and contain DELETE and REFRESH messages.

Are you actually using the upsert-kafka source, and treat these input sources as a changelog stream?

 
{quote}
The main point was that ROW_NUMBER approach doesn't work for CDC streams due to inability to take into account message_type

 
{quote}I would like to understand as well how you would propose to have this logic expressed? Is it still SQL? Is it connector parameters?
{quote}
More as a connector parameter

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align retry mechanisms of FutureUtils,FLINK-34551,13570336,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,29/Feb/24 14:26,05/Apr/24 07:58,04/Jun/24 20:40,,1.20.0,,,,,,,,,,API / Core,,,,,0,pull-request-available,,,The retry mechanisms of FutureUtils include quite a bit of redundant code which makes it hard to understand and to extend. The logic should be aligned properly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34427,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 11:12:22 UTC 2024,,,,,,,,,,"0|z1npd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/24 08:26;mallikarjuna;Hi [~mapohl] , if I understand this correctly, we need to refactor the retry logic here, right? Can I take this up?;;;","27/Mar/24 08:57;mapohl;The intention of the ticket is to remove the code redundancy, yes. I'm gonna assign the issue to you.;;;","27/Mar/24 09:37;mallikarjuna;Thank you!;;;","27/Mar/24 16:38;mallikarjuna;Hi [~mapohl] , I've opened a PR ([#24578|https://github.com/apache/flink/pull/24578]) refactoring `retryOperation()` and `retryOperationWithDelay()`. I haven't refactored `retrySuccessfulWithDelay()`, since you had some changes on that already in your PR ([#24309|https://github.com/apache/flink/pull/24309]). Would you mind reviewing?

 ;;;","28/Mar/24 07:44;mapohl;The intention of this ticket came from FLINK-34227 where I wanted to add logic for retrying forever. I managed to split the {{retrySuccessfulOperationWithDelay}} in FLINK-34227 in a way now that I didn't generate too much additional redundant code. I created FLINK-34551 as a follow-up anyway because I noticed that {{retrySuccessfulOperationWithDelay}} and  {{retryOperation}} share some common logic and that we could improve the way how these methods decide on which executor to run the {{operation}} on (scheduledExecutor vs calling thread).

Your current proposal has still redundant code. We would need to iterate over the change a bit more and discuss the contract of these methods in more detail. But unfortunately, I am gone for quite a bit soon. So, I would not be able to help you. Additionally, it's not a high-priority task right. I'm wondering whether we should unassign the task again. I want to avoid that you spend time on it and then get stuck because of missing feedback from my side.

I should have considered it yesterday already. Sorry for that.;;;","28/Mar/24 11:12;mallikarjuna;I see, makes sense. Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
attempted task still report metric of currentEmitEventTimeLag,FLINK-34550,13570331,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,siwei.gao,siwei.gao,29/Feb/24 13:52,01/Mar/24 01:51,04/Jun/24 20:40,,1.17.1,,,,,,,,,,Connectors / Kafka,,,,,0,streamsource,,,"Attempted task still report metric of currentEmitEventTimeLag when use kafka-connector.Attempt_num for reporting indicators of multiple tasks with the same subtask_index but different task_attempt_num times within the same time period.  !image-2024-03-01-09-51-08-909.png|width=992,height=242!

Only the metric which  tash_attempt_num is 4 should be reported normally.

This condition shows in taskmanager with multiple slots and it's ok when taskmanager only has one slot.

!image-2024-02-29-21-50-55-160.png|width=973,height=730!

 ","flink version:1.17.1  

kafka-connector：1.17.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Feb/24 13:41;siwei.gao;image-2024-02-29-21-41-01-709.png;https://issues.apache.org/jira/secure/attachment/13067162/image-2024-02-29-21-41-01-709.png","29/Feb/24 13:43;siwei.gao;image-2024-02-29-21-43-18-340.png;https://issues.apache.org/jira/secure/attachment/13067161/image-2024-02-29-21-43-18-340.png","29/Feb/24 13:50;siwei.gao;image-2024-02-29-21-50-55-160.png;https://issues.apache.org/jira/secure/attachment/13067160/image-2024-02-29-21-50-55-160.png","01/Mar/24 01:51;siwei.gao;image-2024-03-01-09-51-08-909.png;https://issues.apache.org/jira/secure/attachment/13067175/image-2024-03-01-09-51-08-909.png",,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-29 13:52:14.0,,,,,,,,,,"0|z1npc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FLIP-410: Config, Context and Processing Timer Service of DataStream API V2",FLINK-34549,13570315,13570313,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,29/Feb/24 12:21,22/May/24 04:05,04/Jun/24 20:40,22/May/24 04:05,,,,,,,1.20.0,,,,API / DataStream,,,,,0,pull-request-available,Umbrella,,"This is the umbrella ticket for FLIP-410: Config, Context and Processing Timer Service of DataStream API V2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 22 04:05:10 UTC 2024,,,,,,,,,,"0|z1np8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/24 04:05;Weijie Guo;master(1.20) via e502d2371ffea52ebfa6b8a1b9d2b4a23bc9bbaa.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FLIP-409: DataStream V2 Building Blocks: DataStream, Partitioning and ProcessFunction",FLINK-34548,13570314,13570313,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,29/Feb/24 12:20,02/Apr/24 10:07,04/Jun/24 20:40,01/Apr/24 06:37,1.20.0,,,,,,1.20.0,,,,API / DataStream,,,,,0,pull-request-available,,,"This is the ticket for FLIP-409: DataStream V2 Building Blocks: DataStream, Partitioning and ProcessFunction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34988,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 06:37:02 UTC 2024,,,,,,,,,,"0|z1np88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/24 06:37;Weijie Guo;master(1.20) via 28762497bdf574855e4a9293f22ce80e6c7f9002.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] FLIP-408: Introduce DataStream API V2,FLINK-34547,13570313,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,Weijie Guo,Weijie Guo,29/Feb/24 12:15,29/Feb/24 12:23,04/Jun/24 20:40,,,,,,,,,,,,API / DataStream,,,,,0,Umbrella,,,This is the umbrella ticket for all efforts related to DataStream API V2.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-29 12:15:56.0,,,,,,,,,,"0|z1np80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Emit span with failure labels on failure,FLINK-34546,13570288,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,srichter,srichter,srichter,29/Feb/24 09:25,15/Mar/24 08:36,04/Jun/24 20:40,01/Mar/24 09:41,,,,,,,1.20.0,,,,Runtime / Metrics,,,,,0,pull-request-available,,,"To improve observability, we should emit a span for each failure that contains details about the failure classification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-29 09:25:10.0,,,,,,,,,,"0|z1np2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add OceanBase pipeline connector to Flink CDC,FLINK-34545,13570274,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wanghe,wanghe,29/Feb/24 08:10,31/May/24 06:52,04/Jun/24 20:40,,,,,,,,,,,,Flink CDC,,,,,0,pull-request-available,,,"Flink CDC supports end-to-end jobs starting from version 3.0.0, but now only supports sink to Doris and StarRocks. OceanBase has made a lot of improvements on AP recently, so supporting writing to OceanBase via Flink CDC would be helpful to users.

OceanBase maintains a sink connector that supports multi-table sink and ddl sink. We can use it to build the pipeline connector. [https://github.com/oceanbase/flink-connector-oceanbase]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-29 08:10:48.0,,,,,,,,,,"0|z1nozc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix tiered result partition check state issue when finishing partition,FLINK-34544,13570267,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,tanyuxin,tanyuxin,tanyuxin,29/Feb/24 06:35,02/Mar/24 15:27,04/Jun/24 20:40,02/Mar/24 15:27,1.20.0,,,,,,,,,,Runtime / Network,,,,,0,pull-request-available,,,"When finishing the tiered result partition, it may throw a check state issue because the downstream consumes it too quickly. The bug should be fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 02 15:25:42 UTC 2024,,,,,,,,,,"0|z1noxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/24 15:25;tanyuxin;I noticed that the issue has been fixed by changing the order of checkState and broadcasting the end of partition event in https://issues.apache.org/jira/browse/FLINK-33743. Close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Full Partition Processing On Non-keyed DataStream,FLINK-34543,13570253,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,Wencong Liu,Wencong Liu,29/Feb/24 02:15,14/Mar/24 03:07,04/Jun/24 20:40,14/Mar/24 03:07,1.20.0,,,,,,1.20.0,,,,API / DataStream,,,,,0,pull-request-available,,,"Introduce the PartitionWindowedStream and provide multiple full window operations in it.

The related motivation and design can be found in [FLIP-380|https://cwiki.apache.org/confluence/display/FLINK/FLIP-380%3A+Support+Full+Partition+Processing+On+Non-keyed+DataStream].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 03:07:19 UTC 2024,,,,,,,,,,"0|z1nouo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/24 03:07;Weijie Guo;master(1.20) via a021db68a8545b4183e935beccd9b0d62329ee67.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Gradle Quick Start build.gradle with Better Gradle API Alternatives,FLINK-34542,13570252,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,lassaegis,lassaegis,29/Feb/24 02:01,01/Mar/24 00:17,04/Jun/24 20:40,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,,,"This is a ticket of misc. improvements on the build.gradle script provided at {{Application Development}} >> {{Project Configuration}} >> {{Overview}} >> {{Getting Started:}}
 * Add {{mergeServiceFiles()}} call to the {{shadowJar}} configuration block
 ** Absence of this will cause class-not-found errors in SPI related class loading if the user has multiple connectors/formats in their implementation.
 * Move the top level {{mainClassName}} project property setting into application \{ mainClass = 'foo.Bar' }
 ** This is because the top-level mainClassName property will be deprecated in Gradle 9.0+
 * Replace the use of {{sourceCompatibility}} and {{targetCompatibility}} properties with java \{ toolChain \{ languageVersion = JavaLanguageVersion.of(11) \} \}
 ** This is the recommended way by Gradle to streamline langauge version configuration.
 ** Also the original configured Java version - Java 8 - is getting close to its terminal support phase, and it's better to move on to Java 11.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-29 02:01:32.0,,,,,,,,,,"0|z1noug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink uses insecure http confluent endpoint in its build,FLINK-34541,13570229,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,fanningpj,fanningpj,28/Feb/24 21:06,28/Feb/24 22:44,04/Jun/24 20:40,,,,,,,,,,,,Tests,,,,,0,,,,"See https://github.com/apache/flink/blob/641f4f4d0d0156b84bdb9ba528b1dd96f7ae9d9c/flink-end-to-end-tests/test-scripts/kafka-common.sh#L55

Please use https instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 22:44:43 UTC 2024,,,,,,,,,,"0|z1nopc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 22:44;martijnvisser;There's only one test that still uses this; I don't think it's worthy of a major priority. If anything, it's tech debt. Updating the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tune number of task slots,FLINK-34540,13570200,13570189,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mxm,mxm,28/Feb/24 17:08,14/Mar/24 14:33,04/Jun/24 20:40,,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,Kubernetes Operator,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-28 17:08:08.0,,,,,,,,,,"0|z1noiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tune JobManager memory,FLINK-34539,13570190,13570189,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mxm,mxm,mxm,28/Feb/24 16:18,14/Mar/24 14:33,04/Jun/24 20:40,,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,Kubernetes Operator,,,,0,,,,"Adjustments similar to FLINK-34152, but simpler because we only need to adjust heap memory and metaspace for the JobManager.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-28 16:18:55.0,,,,,,,,,,"0|z1nogo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tune Flink config of autoscaled jobs,FLINK-34538,13570189,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mxm,mxm,mxm,28/Feb/24 16:17,27/Mar/24 19:23,04/Jun/24 20:40,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,0,pull-request-available,,,Umbrella issue to tackle tuning the Flink configuration as part of Flink Autoscaling.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-28 16:17:25.0,,,,,,,,,,"0|z1nogg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler JDBC Support HikariPool,FLINK-34537,13570141,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ConradJam,ConradJam,ConradJam,28/Feb/24 10:15,16/May/24 07:17,04/Jun/24 20:40,16/May/24 07:17,1.8.0,,,,,,kubernetes-operator-1.9.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,Autoscaler Using HikariPool to replace native JDBC connections.  Helps reduce database pressure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 16 07:17:30 UTC 2024,,,,,,,,,,"0|z1no5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/24 07:17;fanrui;merged to 1.9.0 via eb191337251410d9d40300a0050a17d5c2300d8b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support reading long value as Timestamp column in JSON format,FLINK-34536,13570120,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,nilerzhou,nilerzhou,28/Feb/24 08:22,05/Mar/24 03:56,04/Jun/24 20:40,05/Mar/24 03:55,1.19.0,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,,,,"In many scenarios, timestamp data is stored as Long value and expected to be operated as Timestamp value. It's not user-friendly to use an UDF to convert the data before operating it.

Meanwhile, in Avro format, it seems it can receive several types of input and convert it into TimestampData. Hope the same ability can be introduced into JSON format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 05 03:55:31 UTC 2024,,,,,,,,,,"0|z1no14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 03:03;libenchao;It sounds like a useful feature from the user perspective.

Usually a numeric representation is a unixtime, and it would much like {{cast(numeric as timstamp[_ltz])}}, which has been discussed in [FLIP-162|https://cwiki.apache.org/confluence/display/FLINK/FLIP-162%3A+Consistent+Flink+SQL+time+function+behavior#FLIP162:ConsistentFlinkSQLtimefunctionbehavior-2.DisableCASTbetweenNUMERICandTIMESTAMP]. Hence it should be {{TIMSTAMP_LTZ}} instead of {{TIMESTAMP}} that is allowed to be converted from numeric.

Besides, the precision of unixtime has multiple choices, second/millisecond/microsecond, so how would you suppose to distinguish them when converting it to {{TIMSTAMP_LTZ}}.

For now, users can deal with the conversion in SQL expression via {{to_timestamp_ltz}} as an alternative, so I don't have a strong opinion whether to introduce a feature in format level.

Anyway, I think this deserves a FLIP since JSON format is a very fundamental format and this would be a public API change.;;;","04/Mar/24 12:13;nilerzhou;[~libenchao] Thanks for your insight, it helps a lot! After reading FLIP-162, I think explicit calling of `to_timestamp_ltz` maybe more reasonable and conforming to design.  Otherwise we should expose configs for users to declare the details about the conversion, e.g. precision. It seems to be too complex. ;;;","05/Mar/24 03:55;libenchao;I'm closing this due the discussions above, feel free to reopen it if anyone thinks that it deserves to do.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support JobPlanInfo for the explain result,FLINK-34535,13570089,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,heigebupahei,heigebupahei,28/Feb/24 02:29,01/Mar/24 09:45,04/Jun/24 20:40,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"In the Flink Sql Explain syntax, we can set ExplainDetails to plan JSON_EXECUTION_PLAN, but we cannot plan JobPlanInfo. If we can explain this part of the information, referring to JobPlanInfo, I can combine it with the parameter `pipeline.jobvertex-parallelism-overrides` to set up my task parallelism",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 01 09:41:21 UTC 2024,,,,,,,,,,"0|z1nnu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 02:29;heigebupahei;cc [~luoyuxia] ;;;","01/Mar/24 05:43;lincoln.86xy;[~heigebupahei] The current `explain` syntax is just for showing the planinfo, if we want to override something to the real plan which is for execution, I suggest to extend current compiled plan, as we have such ways to compile a query and execute the compiled plan:
{code:java}
COMPILE PLAN FOR '<your_path.json>' <insert_query>
EXECUTE PLAN FOR '<your_path.json>'{code}
(Also adding new mode to current `ExplainDetail` is a public api change, there should be a [FLIP|https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals?src=contextnavpagetreemode] discussion）

Regarding to the motivation 'combine it with the parameter `pipeline.jobvertex-parallelism-overrides` to set up my task parallelism', could you explain more about it？;;;","01/Mar/24 06:32;heigebupahei;[~lincoln.86xy] 
Thank you for your comment,

> Regarding to the motivation 'combine it with the parameter `pipeline.jobvertex-parallelism-overrides` to set up my task parallelism', could you explain more about it？

`pipeline.jobvertex-parallelism-overrides` Parameters can modify the parallelism of each flink task before flink runs the task. He needs to specify the parallelism value of vertex in jobgraph, like this：
{code:java}
//代码占位符
pipeline.jobvertex-parallelism-overrides = 0a448493b4782967b150582570326227:4,bc764cd8ddf7a0cff126f51c16239658:3 {code}
This way when flink runs, the parallelism of the corresponding vertexid: 0a448493b4782967b150582570326227 and bc764cd8ddf7a0cff126f51c16239658 will be set to 4,3.

So my motivation is that I want to set the parallelism of each of my tasks in the task generated by flinksql, but in flinksql parallelism is set globally, so I need to get each of my jobVertexId before the task is run. But the existing explain does not return this information, so I want to return this information in the explain

> (Also adding new mode to current `ExplainDetail` is a public api change, there should be a [FLIP|https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals?src=contextnavpagetreemode] discussion）

 
I noticed ExplainDetail have a @PublicEvolving, But in fact, my function is relatively simple, and it should be to build a separate FLIP? Maybe if we could put this information in What about JSON_EXECUTION_PLAN? 

 
 

>  COMPILE PLAN

COMPILE PLAN It seems to be a reasonable way also，Can you help me @ some friends who are more familiar with this area （flinksql）? 
Discuss their views on this issue
 
 
 

 

 ;;;","01/Mar/24 08:40;jeyhunkarimov;Hi [~heigebupahei] +1 for FLIP, since the issue is mainly touches/adds user-relevant changes.

I still dont fully get the motivation behind the setting parallelism, maybe check [this PR|https://github.com/apache/flink/pull/22818] and then rewrite your motivation accordingly?

 ;;;","01/Mar/24 09:41;heigebupahei;Hi [~jeyhunkarimov] 

Thank you for your comment. In fact, my idea is very simple. I hope that when using flink sql, sql-client or sql-gateway, I can have a way to independently set the parallelism of each vertex in the generated flink task.

For example, I now have a simple task and the jobgraph generated is

: 

 

                              -> window1  -> sink1

KAFKA SOURCE ->  window2  -> sink2

                              -> window3  -> sink3

 
In flinksql, I can set the parallelism of the entire job through set parallelism.default=x, but if I want to set the parallelism of each vertex, I cannot do it.

 
So if I get the jobgraph information of the entire task through explain syntax(with vertexId):

 

                                     -> window1(id2)  -> sink1(id5)

KAFKA SOURCE(id1) ->  window2(id3)  -> sink2(id6)

                                     -> window3(id4)  -> sink3(id7)

 
Fit parameters: pipeline.jobvertex-parallelism-overrides

like:  

set pipeline.jobvertex-parallelism-overrides=id1:1,id2:2,id3:2,id4:2,id5:1,id6:1,id7:1

 
I can change the graph to:

                                           -> window1(id2 p=2)  -> sink1(id5 p=1)

KAFKA SOURCE(id1 p=1) ->  window2(id3 p=2)  -> sink2(id6 p=1)

                                           -> window3(id4  p=2)  -> sink3(id7 p=1)

 
So I hope that this part of the information can be planned out when explaining. Maybe this is not necessarily the best method. Maybe it can be done through `COMPILE PLAN`?
 

 
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vote on the release candidate,FLINK-34534,13570030,13570026,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,lincoln.86xy,lincoln,lincoln.86xy,27/Feb/24 15:22,06/Mar/24 15:51,04/Jun/24 20:40,06/Mar/24 15:51,1.19.0,,,,,,1.19.0,,,,,,,,,0,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 01:16:34 UTC 2024,,,,,,,,,,"0|z1nnh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 01:16;lincoln.86xy;https://lists.apache.org/thread/10bxy1zhzy6hycjyohyl3pzx3xs3zh34;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propose a pull request for website updates,FLINK-34533,13570029,13570026,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,lincoln.86xy,lincoln,lincoln.86xy,27/Feb/24 15:22,15/Mar/24 13:18,04/Jun/24 20:40,15/Mar/24 13:18,1.19.0,,,,,,1.19.0,,,,,,,,,0,pull-request-available,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 # update [apache/flink-web:_config.yml|https://github.com/apache/flink-web/blob/asf-site/_config.yml]
 ## update {{FLINK_VERSION_STABLE}} and {{FLINK_VERSION_STABLE_SHORT}} as required
 ## update version references in quickstarts ({{{}q/{}}} directory) as required
 ## (major only) add a new entry to {{flink_releases}} for the release binaries and sources
 ## (minor only) update the entry for the previous release in the series in {{flink_releases}}
 ### Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number.
 ## add a new entry to {{release_archive.flink}}
 # add a blog post announcing the release in _posts
 # add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/]). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project.

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]
 * (major only) Check {{docs/config.toml}} to ensure that
 ** the version constants refer to the new version
 ** the {{baseurl}} does not point to {{flink-docs-master}}  but {{flink-docs-release-X.Y}} instead",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34593,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-27 15:22:01.0,,,,,,,,,,"0|z1nngw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stage source and binary releases on dist.apache.org,FLINK-34532,13570028,13570026,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,27/Feb/24 15:22,06/Mar/24 01:14,04/Jun/24 20:40,06/Mar/24 01:14,,,,,,,,,,,,,,,,0,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 01:14:57 UTC 2024,,,,,,,,,,"0|z1nngo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 01:14;lincoln.86xy;[https://dist.apache.org/repos/dist/dev/flink/flink-1.19.0-rc1/]

[https://github.com/apache/flink/releases/tag/release-1.19.0-rc1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build and stage Java and Python artifacts,FLINK-34531,13570027,13570026,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln,lincoln.86xy,27/Feb/24 15:22,06/Mar/24 01:15,04/Jun/24 20:40,06/Mar/24 01:15,,,,,,,,,,,,,,,,0,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-27 15:22:01.0,,,,,,,,,,"0|z1nngg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Release Candidate: 1.19.0-rc1,FLINK-34530,13570026,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,lincoln.86xy,lincoln,lincoln.86xy,27/Feb/24 15:22,06/Mar/24 15:50,04/Jun/24 20:40,06/Mar/24 15:50,1.19.0,,,,,,1.19.0,,,,,,,,,0,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.

h4. Prerequisites
Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code}
RC_NUM=""1""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,,,,,,,,,,,,FLINK-34590,FLINK-31154,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 06 15:50:31 UTC 2024,,,,,,,,,,"0|z1nng8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/24 03:51;lincoln.86xy;The release package check should be updated, record here(will open a pr)

{code}

diff --git a/tools/releasing/create_binary_release.sh b/tools/releasing/create_binary_release.sh
index 63dc6aa2377..9f378b8d235 100755
--- a/tools/releasing/create_binary_release.sh
+++ b/tools/releasing/create_binary_release.sh
@@ -123,7 +123,7 @@ make_python_release() {
 #    wheel_packages_num=0
   # py38,py39,py310,py311 for mac 10.9, 11.0 and linux (12 wheel packages)
-  EXPECTED_WHEEL_PACKAGES_NUM=12
+  EXPECTED_WHEEL_PACKAGES_NUM=10

{code};;;","06/Mar/24 01:48;lincoln.86xy;Rechecked the python wheel packages  [https://dev.azure.com/lincoln86xy/lincoln86xy/_build/results?buildId=640&view=artifacts&pathAsName=false&type=publishedArtifacts]

and also recent changes related to this https://issues.apache.org/jira/browse/FLINK-33030

Seems we lost the newly added py3.11 packages for mac(both 10.9 & 11.0), this explains the delta during creating binary release  [~dianfu] ;;;","06/Mar/24 15:50;lincoln.86xy;Close this as rc1 vote cancelled;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Projection cannot be pushed down through rank operator.,FLINK-34529,13569971,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,nilerzhou,nilerzhou,nilerzhou,27/Feb/24 09:29,07/Apr/24 05:17,04/Jun/24 20:40,,1.19.0,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,"When there is a rank/deduplicate operator, the projection based on output of this operator cannot be pushed down to the input of it.

The following code can help reproducing the issue:
{code:java}
val util = streamTestUtil() 
util.addTableSource[(String, Int, String)](""T1"", 'a, 'b, 'c)
util.addTableSource[(String, Int, String)](""T2"", 'd, 'e, 'f)
val sql =
  """"""
    |SELECT a FROM (
    |  SELECT a, f,
    |      ROW_NUMBER() OVER (PARTITION BY f ORDER BY c DESC) as rank_num
    |  FROM  T1, T2
    |  WHERE T1.a = T2.d
    |)
    |WHERE rank_num = 1
  """""".stripMargin

util.verifyPlan(sql){code}
The plan is expected to be:
{code:java}
Calc(select=[a])
+- Rank(strategy=[AppendFastStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=1], partitionBy=[f], orderBy=[c DESC], select=[a, c, f])
   +- Exchange(distribution=[hash[f]])
      +- Calc(select=[a, c, f])
         +- Join(joinType=[InnerJoin], where=[=(a, d)], select=[a, c, d, f], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
            :- Exchange(distribution=[hash[a]])
            :  +- Calc(select=[a, c])
            :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, T1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
            +- Exchange(distribution=[hash[d]])
               +- Calc(select=[d, f])
                  +- LegacyTableSourceScan(table=[[default_catalog, default_database, T2, source: [TestTableSource(d, e, f)]]], fields=[d, e, f]) {code}
Notice that the 'select' of Join operator is [a, c, d, f]. However the actual plan is:
{code:java}
Calc(select=[a])
+- Rank(strategy=[AppendFastStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=1], partitionBy=[f], orderBy=[c DESC], select=[a, c, f])
   +- Exchange(distribution=[hash[f]])
      +- Calc(select=[a, c, f])
         +- Join(joinType=[InnerJoin], where=[=(a, d)], select=[a, b, c, d, e, f], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
            :- Exchange(distribution=[hash[a]])
            :  +- LegacyTableSourceScan(table=[[default_catalog, default_database, T1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
            +- Exchange(distribution=[hash[d]])
               +- LegacyTableSourceScan(table=[[default_catalog, default_database, T2, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])
 {code}
the 'select' of Join operator is [a, b, c, d, e, f], which means the projection in the final Calc is not passed through the Rank.

And I think an easy way to fix this issue is to add org.apache.calcite.rel.rules.ProjectWindowTransposeRule into FlinkStreamRuleSets.LOGICAL_OPT_RULES.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 26 11:55:13 UTC 2024,,,,,,,,,,"0|z1nn40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 07:54;nilerzhou;hi [~lincoln.86xy] , could you please help reviewing the issue? If you agree with the solution, I'd like to help resolving it.;;;","29/Feb/24 06:32;libenchao;[~nilerzhou] Thanks for reporting this, it sounds good to me.

In the description, the expected plan seems already pushing the {{Calc}} through {{Rank}} (but not through {{Join}}), why is that?

Besides, I noted that there is a {{CalcRankTransposeRule}} in Flink already, why does it not work as expected?;;;","29/Feb/24 10:17;xuyangzhong;Hi, [~nilerzhou] . The plan to fix bug LGTM.

IIUC, the fix is that when doing program about `LOGICAL_REWRITE`, the calc transpose the rank node by `CalcRankTransposeRule`. However, we don't have a rule to let the calc transpose the join node. The rule that lets calc transpose join is only in `PROJECT_REWRITE` and `LOGICAL`. One way to fix this bug is what you said, to add the rule `ProjectWindowTransposeRule` in `LOGICAL`. Another way to fix this bug is to add a rule like `CalcJoinTransposeRule` in `LOGICAL_REWRITE`. cc [~libenchao] ;;;","29/Feb/24 11:42;nilerzhou;[~xuyangzhong] Thanks for the explanation. You're completely right. Another solution is to add a rule like `CalcJoinTransposeRule` in 'LOGICAL'. However reusing existing rules in Calcite sounds better.

[~libenchao] In addition,  in`PROJECT_REWRITE` we do have `FlinkProjectJoinTransposeRule` which is a copy of `
org.apache.calcite.rel.rules.ProjectJoinTransposeRule`, that's the reason why `ProjectWindowTransposeRule` works in this case.;;;","29/Feb/24 13:04;libenchao;[~xuyangzhong] [~nilerzhou] Thank you for the explanation, it helps.

I would prefer to putting these transposing rules all in ""LOGICAL"" stage, since in this stage we are using cost-based planner. I'm wondering if it's really necessary to have some transposing rules (now only {{{}CalcRankTransposeRule{}}}) in ""LOGICAL_REWRITE"" stage, could you check whether we still needs {{CalcRankTransposeRule}} in ""LOGICAL_REWRITE"" after introducing {{ProjectWindowTransposeRule}} in ""LOGICAL"" stage?

What's more, I'm even wondering that if we really needs {{{}CalcRankTransposeRule{}}}. {{Rank}} is a special form of {{{}Window{}}}, so {{ProjectWindowTransposeRule}} should supersede {{{}CalcRankTransposeRule{}}}.

(By saying ""transposing rules"", usually I would expect these rules are only generating more plan alternatives, cost-based planner chooses which is is better via cost. That's why you can see many counter pairs of rules like {{AggregateFilterTransposeRule}} and {{FilterAggregateTransposeRule}} in Calcite);;;","29/Feb/24 13:04;libenchao;[~nilerzhou] Assigned to you~;;;","01/Mar/24 03:50;lincoln.86xy;[~nilerzhou] Thanks for reporting this!
As you folks discussed above, there indeed need a projection pushdown, reuse the calcite's corerules always be the first choice for flink(except for the special things in streaming for now).
And for this case itself, can we just add `CoreRules.PROJECT_WINDOW_TRANSPOSE` into `FlinkStreamRuleSets#DEFAULT_REWRITE_RULES` (which will take effect during the `DEFAULT_REWRITE` rbo phase)？As in the current code, we tend to treat projection/predicate pushdown as a deterministic optimization.  WDYT?;;;","01/Mar/24 03:59;nilerzhou;[~libenchao]  Thanks for assigning the ticket to me. 

To the point 'I would prefer to putting these transposing rules all in ""LOGICAL"" stage', I agree with that. I'll add `ProjectWindowTransposeRule` in LOGICAL stage. With regard to other already-existing transpose rules in other stages, moving them into LOGICAL in this MR or in a independent MR, which do you think is better?

For the question 'I'm even wondering that if we really needs {{{}CalcRankTransposeRule{}}}', I've tried to remove it, and found that `ProjectWindowTransposeRule` can completely cover the functionality of `{{{}CalcRankTransposeRule{}}}`(from the results of tests introduced along with this rule) and even do  much better job in some cases. Therefore, I prepare to remove this rule in the this PR, WDT? 

 

 ;;;","01/Mar/24 07:07;nilerzhou;hi [~lincoln.86xy] , thanks for your advice.

After discussion with [~libenchao] , I agreed that putting these kind of rules to cost-based planner seems to be in line with future trend.

Meanwhile I found that most of ProjectXXTransposeRules are in `FlinkStreamRuleSets#PROJECT_RULES`and `PROJECT_RULES` seems to be used both in 'LOGICAL' (volcano)and 'PROJECT_REWRITE'(hep).  I prepare to add `CoreRules.PROJECT_WINDOW_TRANSPOSE` to `FlinkStreamRuleSets#PROJECT_RULES`too, so that both kind of planner can utilize the rule.  WDT? ;;;","01/Mar/24 12:35;lincoln.86xy;[~nilerzhou]  +1 for adding `CoreRules.PROJECT_WINDOW_TRANSPOSE` into both `DEFAULT_REWRITE` and `LOGICAL` phases.

This way we can have early pruning and at the same time get a potentially better plan in cbo.

 ;;;","06/Mar/24 08:55;libenchao;+1 for adding it to ""PROJECT_RULES"" so that we can utilize it in both cbo and rbo stages.

bq. For the question 'I'm even wondering that if we really needs CalcRankTransposeRule', I've tried to remove it, and found that `ProjectWindowTransposeRule` can completely cover the functionality of `CalcRankTransposeRule`(from the results of tests introduced along with this rule) and even do  much better job in some cases. Therefore, I prepare to remove this rule in the this PR, WDT? 

[~nilerzhou] I'm ok with it;;;","07/Mar/24 11:20;nilerzhou;Hi, [~libenchao] , [~lincoln.86xy] , I got some updates about the solution.  I prepared to :
 # add `ProjectWindowTransposeRule` into PROJECT_RULES(used in `PROJECT_REWRITE` and `LOGICAL`),
 # remove `CalcRankTransposeRule` from `LOGICAL_REWRITE` 

For {*}Action 1{*}, there is optimization regression in one test, I'm not sure if we can ignore it and optimize the case in other way. The test is `RankTest#testRankWithAnotherRankAsInput`.

The original plan is 
{code:java}
Calc(select=[CAST(w0$o0 AS INTEGER) AS rn1, CAST(w0$o0_0 AS INTEGER) AS rn2])
+- Rank(strategy=[UpdateFastStrategy[0,2,3]], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=200], partitionBy=[a], orderBy=[b DESC], select=[a, b, c, w0$o0, w0$o0_0])
   +- Exchange(distribution=[hash[a]])
      +- Rank(strategy=[AppendFastStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=100], partitionBy=[a, c], orderBy=[b DESC], select=[a, b, c, w0$o0])
         +- Exchange(distribution=[hash[a, c]])
            +- Calc(select=[a, b, c])
               +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime]) {code}
The changed plan is 
{code:java}
Calc(select=[CAST(w0$o0 AS INTEGER) AS rn1, CAST(w0$o0_0 AS INTEGER) AS rn2])
+- Rank(strategy=[RetractStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=200], partitionBy=[a], orderBy=[b DESC], select=[a, b, w0$o0, w0$o0_0])
   +- Exchange(distribution=[hash[a]])
      +- Calc(select=[a, b, w0$o0])
         +- Rank(strategy=[AppendFastStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=100], partitionBy=[a, c], orderBy=[b DESC], select=[a, b, c, w0$o0])
            +- Exchange(distribution=[hash[a, c]])
               +- Calc(select=[a, b, c])
                  +- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
{code}
You can find that the strategy of second Rank changed from `UpdateFastStrategy` to `RetractStrategy`. The reason is that the component of primary key of input, field `c ` is  pruned, so we cannot meet the requirements of `UpdateFastStrategy` anymore.

IMO, it's not a general query optimization but query execution optimization like different implementations in Join depending on upsert keys of input. Therefore, I think we can ignore this change here, WDT?

For {*}Action 2{*}, I found that `CorrelateSortToRankRule` can translate LogicalCorrelate into LogicalRank, no LogicalWindow used in the process. Therefore  `ProjectWindowTransposeRule` can not cover all cases of Rank and Project transpose.  We should keep it. And I tried to move this rule into `LOGICAL`  stage, it didn't work well.

The root cause is that cost of Exchange and Rank are not related to length of output row, therefore adding a Calc before Rank maybe cost more and is not chosen by the planner.  I'll keep the `CalcRankTransposeRule` in `LOGICAL_REWRITE` util the costs of Rank and Calc are optimized in future PR.

Look forward to your opinions about these two updates.;;;","14/Mar/24 02:29;libenchao;[~nilerzhou] Thanks for the update. 

For #1, is there a way to preserve 'primary key' fields for {{Rank}}? Usually an regression is not acceptable.

For #2, I agree. 

And putting all these together, I'm just thinking that can we just improve current case without introducing the general {{ProjectWindowTransposeRule}}, since it does not take 'primary key' into account and we'll end up with an regression. Putting more transposing rules into 'default rewrite' phase might be the minimal step to move forward without any regression. What do you think?;;;","14/Mar/24 14:53;lincoln.86xy;[~nilerzhou]  Sorry for my late response! I agree with Benchao’s suggestion to address this case by introducing minimal changes. 

As for how to seamlessly replace {{{}the `CalcRankTransposeRule{}}}`, further investigation may be worthwhile and we can track it in a separate issue. WDYT? 
 
 ;;;","26/Mar/24 11:55;nilerzhou;Hi [~libenchao] [~lincoln.86xy] , as your suggestions, I tried to introduce `FlinkProjectWindowTransposeRule` which is based on CoreRules.PROJECT_WINDOW_TRANSPOSE but keeps upsert keys of input not being pruned.  Then the regression mentioned before is gone.  The PR is ready, please feel free to review my code when you have some time.;;;",,,,,,,,,,,,,,,,,,
Disconnect TM in JM when TM was killed to further reduce the job restart time,FLINK-34528,13569964,13569949,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Won't Do,easonqin,easonqin,easonqin,27/Feb/24 08:53,12/Mar/24 09:06,04/Jun/24 20:40,12/Mar/24 09:06,,,,,,,,,,,,,,,,0,,,,"In https://issues.apache.org/jira/browse/FLINK-34526 we disconnect the killed TM in RM. But in the following scenario, we can further reduce the restart time.
h3. Phenomenon

In the test case, the pipeline looks like:

!image-2024-02-27-16-35-04-464.png!

The Source: Custom Source generates strings, and the job keyBy the strings to Sink: Unnamed.
 # parallelism = 100
 # taskmanager.numberOfTaskSlots = 2
 # disable checkpoint

The worker was killed at 
{code:java}
2024-02-27 16:41:49,982 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (6/100) (2f1c7b22098a273f5471e3e8f794e1d3_0a448493b4782967b150582570326227_5_0) switched from RUNNING to FAILED on container_e2472_1705993319725_62292_01_000046 @ xxx (dataPort=38827).org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Connection unexpectedly closed by remote task manager 'xxx/10.169.18.138:35983 [ container_e2472_1705993319725_62292_01_000010(xxx:5454) ] '. This might indicate that the remote task manager was lost.    at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelInactive(CreditBasedPartitionRequestClientHandler.java:134)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)    at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)    at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelInactive(NettyMessageClientDecoderDelegate.java:94)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403)    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_252]{code}
{code:java}
// The task was scheduled to a task manager that had already been killed
2024-02-27 16:41:53,506 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source (16/100) (attempt #3) with attempt id 2f1c7b22098a273f5471e3e8f794e1d3_bc764cd8ddf7a0cff126f51c16239658_15_3 and vertex id bc764cd8ddf7a0cff126f51c16239658_15 to container_e2472_1705993319725_62292_01_000010 @ xxx (dataPort=35983) with allocation id 975dded4548ad15b36d0e5e6aac8f5b6

// The last task switched from INITIALIZING to RUNNING
2024-02-27 16:42:05,176 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Sink: Unnamed (64/100) (2f1c7b22098a273f5471e3e8f794e1d3_0a448493b4782967b150582570326227_63_14) switched from INITIALIZING to RUNNING. {code}
In this scenario, we found the task was repeatedly scheduled to a task manager which has already been killed.
h3. Solution

We can disconnect the killed TM actively in JobMaster when updating the task execution state.
{code:java}
// class JobMaster
public CompletableFuture<Acknowledge> updateTaskExecutionState(final TaskExecutionState taskExecutionState) {
    // ...
    // Throwable error = taskExecutionState.getError(userCodeLoader);
    // disconnectTaskManager() if the TM was killed
    if (schedulerNG.updateTaskExecutionState(taskExecutionState)) {
        return CompletableFuture.completedFuture(Acknowledge.get());
    } else {
        // ...
    }
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/24 08:35;easonqin;image-2024-02-27-16-35-04-464.png;https://issues.apache.org/jira/secure/attachment/13067055/image-2024-02-27-16-35-04-464.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 02:21:57 UTC 2024,,,,,,,,,,"0|z1nn2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/24 09:50;wanglijie;Does this exception always mean that the TM is killed or unavailable? I'm a bit doubtful.;;;","27/Feb/24 10:22;easonqin;Hi [~wanglijie] good question, the exception is ""Connection unexpectedly closed by remote task manager *. This might indicate that the remote task manager was lost.""  And it can't always mean the TM is killed or unavailable. Maybe providing an option to enable or disable disconnecting the TM in JM when the task manager is lost (enabled redundant TM) is a choice. If you have any good ideas, feel free to share them as well.;;;","28/Feb/24 12:17;xtsong;I'm not sure about introducing an option for this.

A big concern of the Flink project is that it has too many configuration options, many of which are too detailed and require user to be extremely familiar with the Flink internals in order to use. The community is working hard to reduce such no-one-knows-how-to-use knobs, rather than introducing more of them.

Pursuing ultimate performance must not come at the price of throwing every single decision to the users. The engine itself should learn to smartly decide its behavior, or we should admit that there's no good way to achieve such improvements.

In this particular case, if the user wants to discover the TM lost faster, he/she can simply decrease the heartbeat timeout.;;;","04/Mar/24 02:21;easonqin;[~xtsong] Thanks for the detailed information. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate Time classes also in PyFlink,FLINK-34527,13569960,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,27/Feb/24 08:43,27/Feb/24 08:45,04/Jun/24 20:40,,1.20.0,,,,,,,,,,API / Python,,,,,0,,,,FLINK-32570 deprecated the Time classes. But we missed touched the PyFlink-related APIs. Also the Docs should be checked for code samples related to Time classes.,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32570,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-27 08:43:19.0,,,,,,,,,,"0|z1nn1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Actively disconnect the killed TM in RM to reduce restart time,FLINK-34526,13569952,13569949,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,easonqin,easonqin,easonqin,27/Feb/24 07:55,25/Mar/24 09:46,04/Jun/24 20:40,25/Mar/24 09:46,,,,,,,1.20.0,,,,,,,,,0,pull-request-available,,,"In our test case, the pipeline is:

!image-2024-02-27-15-50-25-071.png!
 # parallelism = 100
 # taskmanager.numberOfTaskSlots = 2
 # disable checkpoint

h3. Phenomenon

When the worker was killed at 2024-02-27 15:10:13,691
{code:java}
2024-02-27 15:10:13,691 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Worker container_e2472_1706081484717_60538_01_000050 is terminated. Diagnostics: Container container_e2472_1706081484717_60538_01_000050 marked as failed. Exit code:137. Diagnostics:[2024-02-27 15:10:12.720]Container killed on request. Exit code is 137[2024-02-27 15:10:12.763]Container exited with a non-zero exit code 137. [2024-02-27 15:10:12.839]Killed by external signal {code}
It took about 20 seconds to restart the job.
{code:java}
2024-02-27 15:10:30,749 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: datagen_source[1] -> Sink: print_sink[2] (70/100) (2a1d06e6610bc499475fa6e647f8cac9_d3f21cabc6fe0fdf76c8be915bdb22a2_69_0) switched from RUNNING to FAILED on container_e2472_1706081484717_60538_01_000050 @ xxx (dataPort=38597).org.apache.flink.runtime.jobmaster.JobMasterException: TaskManager with id container_e2472_1706081484717_60538_01_000050(xxx:5454) is no longer reachable.
    at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyTargetUnreachable(JobMaster.java:1515)
    at org.apache.flink.runtime.heartbeat.DefaultHeartbeatMonitor.reportHeartbeatRpcFailure(DefaultHeartbeatMonitor.java:126)
    at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.runIfHeartbeatMonitorExists(HeartbeatManagerImpl.java:275)
    at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.reportHeartbeatTargetUnreachable(HeartbeatManagerImpl.java:267)
    at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.handleHeartbeatRpcFailure(HeartbeatManagerImpl.java:262)
    at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.lambda$handleHeartbeatRpc$0(HeartbeatManagerImpl.java:248)........// Deploy and run task
2024-02-27 15:10:32,426 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: datagen_source[1] -> Sink: print_sink[2] (70/100) (2a1d06e6610bc499475fa6e647f8cac9_d3f21cabc6fe0fdf76c8be915bdb22a2_69_1) switched from DEPLOYING to INITIALIZING.2024-02-27 15:10:32,427 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: datagen_source[1] -> Sink: print_sink[2] (69/100) (2a1d06e6610bc499475fa6e647f8cac9_d3f21cabc6fe0fdf76c8be915bdb22a2_68_1) switched from DEPLOYING to INITIALIZING.2024-02-27 15:10:33,347 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: datagen_source[1] -> Sink: print_sink[2] (70/100) (2a1d06e6610bc499475fa6e647f8cac9_d3f21cabc6fe0fdf76c8be915bdb22a2_69_1) switched from INITIALIZING to RUNNING.2024-02-27 15:10:33,421 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: datagen_source[1] -> Sink: print_sink[2] (69/100) (2a1d06e6610bc499475fa6e647f8cac9_d3f21cabc6fe0fdf76c8be915bdb22a2_68_1) switched from INITIALIZING to RUNNING. {code}
 
h3. Reason

When the RM received the message the TM was killed, the JobMaster still kept the connection with the killed TM.  And the JobMaster found the TM is no longer reachable after about 17 seconds.
h3. Solution:

We can reduce the restart time by disConnectTaskManager actively in ResourceManager
{code:java}
// class ResourceManager
protected Optional<WorkerType> closeTaskManagerConnection(
        final ResourceID resourceID, final Exception cause) {
   // ....
   // using JobManagerGateway to actively disconnect TaskManager 
    workerRegistration.getTaskExecutorGateway().disconnectResourceManager(cause);
} {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/24 07:50;easonqin;image-2024-02-27-15-50-25-071.png;https://issues.apache.org/jira/secure/attachment/13067050/image-2024-02-27-15-50-25-071.png","27/Feb/24 07:50;easonqin;image-2024-02-27-15-50-39-337.png;https://issues.apache.org/jira/secure/attachment/13067049/image-2024-02-27-15-50-39-337.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 09:45:58 UTC 2024,,,,,,,,,,"0|z1nmzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 12:30;xtsong;This makes sense tome.

Just to provide some backgrounds on this. Flink's JM, RM and TM coordinates based on an assumption that the ground truth of how TMs' resources are allocated for JMs lies with the TMs. Therefore, to avoid inconsistency, RM does not tell JM which resource (slot) from which TM is (de)allocated for it, nor tell JM to connect to or disconnect from a TM.

However, this IMHO might be a bit over protecting. If a TM is known for sure to be terminated, I don't see any problem in notifying relevant JMs earlier about it.;;;","04/Mar/24 02:23;easonqin;[~xtsong] Thanks for the detailed information. I will prepare the PR.;;;","25/Mar/24 09:45;fanrui;Merged to master(1.20) via : a54ee77a64c2055bbdb187f0c7d12c8a0210f96c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When redundant task managers are enabled, it will still take a considerable amount of time to restart a job if a task manager is killed.",FLINK-34525,13569949,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,easonqin,easonqin,easonqin,27/Feb/24 07:38,25/Mar/24 09:46,04/Jun/24 20:40,25/Mar/24 09:46,,,,,,,1.20.0,,,,,,,,,0,,,,"In our scenario, with redundant task managers enabled, we observed that it will take a considerable amount of time to restart the job if a task manager is killed.
I will show the detailed information in the subtask and I am willing to contribute a PR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 08:00:30 UTC 2024,,,,,,,,,,"0|z1nmz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/24 07:56;martijnvisser;[~easonqin] This isn't really a complete ticket. What is your expectation with this ticket? To report it, to fix it? How have you concluded that this is the case. What is ""considerable"" ? It's basically incomplete at this moment, and I would be inclined to close it as not having sufficient information. ;;;","27/Feb/24 08:00;easonqin;[~martijnvisser] Sorry for the incomplete message, the detailed information is in the subtask, and I will and more information in the parent issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scale down JobManager deployment to 0 before deletion,FLINK-34524,13569942,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,27/Feb/24 06:43,11/Mar/24 20:07,04/Jun/24 20:40,11/Mar/24 20:07,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,"We recently improved the JM deployment deletion mechanism, however it seems like task manager pod deletion can get stuck sometimes for a couple of minutes in native mode if we simply try to delete everything at once.

It speeds up the process and leads to cleaner shutdown if we scale down the JM deployment to 0 (shutting down the JM pods first) and then perform the deletion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 20:07:36 UTC 2024,,,,,,,,,,"0|z1nmxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/24 06:44;gyfora;cc [~mateczagany] ;;;","11/Mar/24 20:07;gyfora;merged to main ede1a610b3375d31a2e82287eec67ace70c4c8df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.maven.surefire:surefire-shared-utils:jar:3.2.2 couldn't be downloaded,FLINK-34523,13569940,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,lincoln,lincoln.86xy,27/Feb/24 06:35,28/Feb/24 07:03,04/Jun/24 20:40,,1.19.0,,,,,,,,,,Build System / CI,BuildSystem / Shaded,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57862&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=670

{code}
Feb 26 12:12:08 12:12:08.393 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.2.2:test (default-test) on project flink-parent: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.2.2:test failed: Plugin org.apache.maven.plugins:maven-surefire-plugin:3.2.2 or one of its dependencies could not be resolved: Could not transfer artifact org.apache.maven.surefire:surefire-shared-utils:jar:3.2.2 from/to google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/): GET request of: org/apache/maven/surefire/surefire-shared-utils/3.2.2/surefire-shared-utils-3.2.2.jar from google-maven-central failed: Connection reset -> [Help 1]
Feb 26 12:12:08 12:12:08.393 [ERROR] 
Feb 26 12:12:08 12:12:08.393 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Feb 26 12:12:08 12:12:08.393 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Feb 26 12:12:08 12:12:08.393 [ERROR] 
Feb 26 12:12:08 12:12:08.393 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Feb 26 12:12:08 12:12:08.394 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException
Feb 26 12:12:08 Process exited with EXIT CODE: 1.
{code}",,,,,,,,,,,,,,,,,,,,,,FLINK-34207,,,,,,,FLINK-17274,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 07:02:17 UTC 2024,,,,,,,,,,"0|z1nmx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 07:01;mapohl;{code}
GET request of: net/hydromatic/foodmart-data-hsqldb/0.3/foodmart-data-hsqldb-0.3.jar from google-maven-central failed: Connection reset -> [Help 1]
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57908&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=4546;;;","28/Feb/24 07:02;mapohl;{code}
GET request of: org/freemarker/freemarker/2.3.28/freemarker-2.3.28.jar from google-maven-central failed: Connection reset -> [Help 1]
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57808&view=logs&j=d871f0ce-7328-5d00-023b-e7391f5801c8&t=77cbea27-feb9-5cf5-53f7-3267f9f9c6b6&l=4990;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateTtlConfig#cleanupInRocksdbCompactFilter still uses the deprecated Time class,FLINK-34522,13569936,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,fanrui,fanrui,27/Feb/24 05:16,11/Mar/24 06:18,04/Jun/24 20:40,11/Mar/24 06:18,1.19.0,,,,,,1.19.0,1.20.0,,,API / Core,,,,,0,pull-request-available,,,"FLINK-32570 deprecated the Time class and refactor all Public or PublicEvolving apis to use the Java's Duration.

StateTtlConfig.Builder#cleanupInRocksdbCompactFilter is still using the Time class. In general, we expect:
 * Mark {{cleanupInRocksdbCompactFilter(long, Time)}} as {{@Deprecated}}
 * Provide a new cleanupInRocksdbCompactFilter(long, Duration)

Note: This is exactly what FLINK-32570 does, so I guess FLINK-32570 missed cleanupInRocksdbCompactFilter.

But I found this method is introduced in 1.19(FLINK-30854), so a better solution may be: only provide cleanupInRocksdbCompactFilter(long, Duration) and don't use Time.

The deprecated Api should be keep for 2 minor version. IIUC, we cannot remove Time related class in Flink 2.0 if we don't deprecate it in 1.19. If so, I think it's better to merge this JIRA in 1.19.0 as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 06:17:56 UTC 2024,,,,,,,,,,"0|z1nmw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 11:17;mapohl;I increased the priority to Blocker because we agreed to get this change into 1.19 in yesterday's release sync;;;","28/Feb/24 12:33;mapohl;[~lincoln] can you help finding support for finalizing the PR? It looks like this issue is now the only problem that prevents us from preceding with the RC, isn't it?;;;","28/Feb/24 12:42;lincoln.86xy;[~mapohl] I contacted [~dianfu] offline today and he'll take a look as soon as he can.;;;","28/Feb/24 12:44;mapohl;great, thanks for the support :);;;","28/Feb/24 12:45;lincoln.86xy;[~mapohl]  Btw, I missed a new blocker on yesterday’s discussion(https://issues.apache.org/jira/browse/FLINK-34498)

As it stands now, we also need to fix it in 1.19.;;;","28/Feb/24 14:18;mapohl;I see - it's related to FLINK-34499 as far as I can see. I approved Chesnay's FLINK-34499 PR and created the corresponding backports.;;;","29/Feb/24 02:56;lincoln.86xy;[~mapohl]  Thanks for resolving it!

Back to this issue, [~dianfu] & [~hxb] have made some attempts, but there're no clear leads yet, looks like it will take some time.;;;","02/Mar/24 04:42;yunta;merged in release-1.19: 161defe0bb2dc8136133e07699b6ac433d52dc65 ... 7618bdeeab06c09219136a04a62262148c677134;;;","11/Mar/24 06:17;fanrui;merged in master(1.20): 
* eb9bcfa5893374093d3add272ca698dcb262c4fd
* 972dc0cfc79e0afbe7567520cc53b24b5727e11f
* 7bfa1d303d84c1489e372c150d01ef62450ba2fd
;;;",,,,,,,,,,,,,,,,,,,,,,,,
Using the Duration instead of the deprecated Time classes,FLINK-34521,13569924,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,fanrui,fanrui,fanrui,27/Feb/24 02:58,01/Mar/24 02:48,04/Jun/24 20:40,01/Mar/24 02:48,,,,,,,1.20.0,,,,,,,,,0,,,,"FLINK-32570 deprecated org.apache.flink.api.common.time.Time and org.apache.flink.streaming.api.windowing.time.Time.

We should refactor all internal callers from Time to Duration. (Public callers should be removed in 2.0).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 01 02:48:14 UTC 2024,,,,,,,,,,"0|z1nmtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/24 14:58;mapohl;I guess, that's already covered by FLINK-14068, isn't it?;;;","01/Mar/24 02:48;fanrui;Thanks for the reminder, i close this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Confusing MIT language in the NOTICE file,FLINK-34520,13569909,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bayard,bayard,26/Feb/24 22:55,26/Feb/24 22:55,04/Jun/24 20:40,,,,,,,,,,,,Connectors / AWS,,,,,0,,,,"This NOTICE file rather confusingly has an MIT license on the end, but no explanation of why:

[https://github.com/apache/flink-connector-aws/blob/main/NOTICE]

 

I think either it should explain why it's there, or it shouldn't be there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-02-26 22:55:52.0,,,,,,,,,,"0|z1nmq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refine checkpoint scheduling and canceling logic,FLINK-34519,13569836,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,26/Feb/24 12:11,08/Mar/24 02:43,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Runtime / Checkpointing,,,,,0,,,,"In the current implementation, CheckpointCoordinator#startCheckpointScheduler would stop the checkpoint scheduler before starting it, and CheckpointCoordinator#stopCheckpointScheduler would cancel all ongoing and pending checkpoints. When a stop-with-savepoint request is received, checkpoint coordinator would trigger stopCheckpointScheduler before creating the savepoint, and start the scheduler afterwards if the savepoint fails.

The problem with this behavior is that it mixed up behavior different checkpointing types. For example, stopCheckpointScheduler() only needs to cancel previous periodic checkpoints, while the current behavior cancels ongoing savepoints as well. This behavior is still acceptable for now, given that there have only been periodic checkpoints and manual savepoints, and savepoints are the only one to change checkpointing behavior once a Flink job starts. However, as the Batch-Streaming Unification optimizations need to change some of these assumptions, the checkpoint coordinator should fix this problem.

To be exact, checkpoint coordinator should at least distinguish between the following semantics.

- Periodic checkpoint is enabled to ensure that failover recovery time should be kept within a time limit.
- Periodic checkpoint is disabled to reduce corresponding performance overhead, but the ability to checkpoint still exists and users can trigger a savepoint anytime.
- Checkpoint or savepoint is not allowed due to job status or topological requirements. There might be multiple requirements applicable to a Flink job at the same time, and releasing one of them is not enough to enable checkpoints.

It should also be supported for a Flink job to change between the checkpointing semantics mentioned above dynamically during runtime.

Besides, checkpoints canceled in stopCheckpointScheduler() would fail with an error message saying ""Checkpoint Coordinator is suspending"", which is ambiguous for debugging. The detailed reason should be recorded as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 02:43:14 UTC 2024,,,,,,,,,,"0|z1nma0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/24 02:43;masteryhx;Thanks for reporting this.
{quote}stopCheckpointScheduler() only needs to cancel previous periodic checkpoints, while the current behavior cancels ongoing savepoints as well.
{quote}
I agree that it's not reasonble. Seems it only happens when there are more than one on-going checkpoints which has different checkpoint type, right ?
{quote}However, as the Batch-Streaming Unification optimizations need to change some of these assumptions, the checkpoint coordinator should fix this problem.
{quote}
So Could you share more about how the ""Batch-Streaming Unification optimizations"" suffered from it ? It may help me to better understand the affected scope. Thanks.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive Scheduler restores from empty state if JM fails during restarting state,FLINK-34518,13569823,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,gyfora,gyfora,26/Feb/24 10:15,27/Feb/24 14:13,04/Jun/24 20:40,27/Feb/24 14:13,1.17.2,1.18.1,1.19.0,1.20.0,,,1.17.3,1.18.2,1.19.0,,Runtime / Checkpointing,Runtime / Coordination,,,,0,pull-request-available,,,"If a JobManager failover occurs while the Job is in a Restarting state, the HA metadata is deleted (as if it was a globally terminal state) and the job restarts from an empty state after the JM comes back up:

Jobmanager killed after killing Taskmanager (restarting phase):
{noformat}
2024-02-26 10:10:12,147 DEBUG org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Ignore TaskManager pod that is already added: autoscaling-example-taskmanager-3-2
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.jobmaster.JobMaster                 [] - Received heartbeat request from 9b7e17b75812ab60ecf028e02368d0c2.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Received heartbeat from 251c25cf794e3c9396fc02306613507b.
2024-02-26 10:10:14,091 DEBUG org.apache.pekko.remote.transport.netty.NettyTransport       [] - Remote connection to [/10.244.0.120:55647] was disconnected because of [id: 0x4a61a791, /10.244.0.120:55647 :> /10.244.0.118:6123] DISCONNECTED
2024-02-26 10:10:14,091 DEBUG org.apache.pekko.remote.transport.ProtocolStateActor         [] - Association between local [tcp://flink@10.244.0.118:6123] and remote [tcp://flink@10.244.0.120:55647] was disassociated because the ProtocolStateActor failed: Unknown
2024-02-26 10:10:14,092 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2024-02-26 10:10:14,094 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesApplicationClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2024-02-26 10:10:14,095 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
2024-02-26 10:10:14,095 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
2024-02-26 10:10:14,315 DEBUG org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.Watcher [] - Watcher closed
2024-02-26 10:10:14,511 DEBUG org.apache.pekko.actor.CoordinatedShutdown                   [] - Performing task [terminate-system] in CoordinatedShutdown phase [actor-system-terminate]
2024-02-26 10:10:14,595 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.
2024-02-26 10:10:14,596 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.{noformat}
Then the new JM comes back it doesn't find any checkpoints as the HA metadata was deleted (we couldn't see this in the logs of the shutting down JM):


{noformat}
2024-02-26 10:10:30,294 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Recovering checkpoints from KubernetesStateHandleStore{configMapName='autoscaling-example-5ddd0b1ba346d3bfd5ef53a63772e43c-config-map'}.2024-02-26 10:10:30,394 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Found 0 checkpoints in KubernetesStateHandleStore{configMapName='autoscaling-example-5ddd0b1ba346d3bfd5ef53a63772e43c-config-map'}.{noformat}
Even the main method is re-run and the jobgraph is regenerated (which is expected given the HA metadata was removed incorrectly)",,,,,,,,,,,,,,FLINK-34451,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 09:06:34 UTC 2024,,,,,,,,,,"0|z1nm74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 10:15;gyfora;cc [~mapohl] [~chesnay] ;;;","26/Feb/24 11:12;mapohl;thanks for reporting this issue, [~gyfora]. Are you suppressing certain INFO/DEBUG logs from being generated? I would have expected a bit more content after the log line about ""Shutting KubernetesApplicationClusterEntrypoint down with [...]"", e.g. the info that the currently running job was suspended in [DefaultExecutionGraph:1060|https://github.com/apache/flink/blob/c3c836216eaaaf24c1add3b490c8f425fda01d7c/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/DefaultExecutionGraph.java#L1060].

Btw. [this sequence diagram|https://raw.githubusercontent.com/XComp/flink-diagrams/main/diagrams/dispatcher-shutdown-sequence.svg] of the JobManager shutdown process might help if somebody else wants to look into it as well.;;;","26/Feb/24 11:25;mapohl;{quote}
I would have expected a bit more content after the log line about ""Shutting KubernetesApplicationClusterEntrypoint down with [...]"", e.g. the info that the currently running job was suspended in DefaultExecutionGraph:1060.
{quote}

Ok, never mind. The cancellation of the {{ExecutionGraph}} in the {{Restarting}} state's constructor ([Restarting:65|https://github.com/apache/flink/blob/676e8e5528f99eb8ba5747f7489b0f02ee025dd6/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/Restarting.java#L65]) switches the {{ExecutionGraph}} eventually into a globally terminal state prior to suspending the graph as part of the JM failover which prevents the suspend logic to kick in [DefaultExecutionGraph:1029|https://github.com/apache/flink/blob/c3c836216eaaaf24c1add3b490c8f425fda01d7c/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/DefaultExecutionGraph.java#L1029].

Suspending the job will still be executed in the {{Restarting}} state (see [StateWithExecutionGraph:168|https://github.com/apache/flink/blob/16bac7802284563c95cfe18fcf153e91dc06216e/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/StateWithExecutionGraph.java#L168]) resulting in the {{AdaptiveScheduler}} reaching {{Finished}} state with an {{ArchivedExecutionGraph}} in {{CANCELED}} state (which is [globally terminal|https://github.com/apache/flink/blob/582941b0f13d1cc51077e0e69fd100afe080779f/flink-core/src/main/java/org/apache/flink/api/common/JobStatus.java#L48]).

The {{ExecutionGraph}}'s status is then translated to a successful {{JobManagerRunnerResult}} instance in [DefaultJobMasterServiceProcess:219|https://github.com/apache/flink/blob/4882fbd9744d456e09ca60b6c7cf7a5b60326c73/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/DefaultJobMasterServiceProcess.java#L219] which takes precendence over the completion of the {{JobManagerRunner}}'s result in [JobMasterServiceLeadershipRunner:143|https://github.com/apache/flink/blob/c9fcb0c74b1354f4f0f1b7c7f62191b8cc6b5725/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMasterServiceLeadershipRunner.java#L143] that happens when closing the runner as part of the JM failover.

But the actual race condition happens between the {{ExecutionGraph#cancel}} call in the constructor of the {{Restarting}} state and the {{ExecutionGraph#suspend}} call in {{StateWithExecutionGraph#suspend}}. The behavior we're seeing can only happen if the cancellation completes before the suspend call is triggered (because the {{ExecutionGraph}} will only then reach the globally-terminal JobStatus {{CANCELLED}}).;;;","27/Feb/24 09:06;mapohl;master: [3ba6f15043aac3e2d4744ad2431b4805ce5dec5a|https://github.com/apache/flink/commit/3ba6f15043aac3e2d4744ad2431b4805ce5dec5a]
1.19: [d743ee33067dad18817d158f91db821cb494a31e|https://github.com/apache/flink/commit/d743ee33067dad18817d158f91db821cb494a31e]
1.18: [384f6b24de9941d452ec84f9a7f617a65cb207a8|https://github.com/apache/flink/commit/384f6b24de9941d452ec84f9a7f617a65cb207a8]
1.17: 1.17: [539a6fefa4cd049139e3be04f1672437e12744d5|https://github.com/apache/flink/commit/539a6fefa4cd049139e3be04f1672437e12744d5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
environment configs ignored when calling procedure operation,FLINK-34517,13569818,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JustinLee,JustinLee,JustinLee,26/Feb/24 09:46,06/May/24 01:27,04/Jun/24 20:40,06/May/24 01:27,1.18.0,,,,,,1.18.2,1.19.1,1.20.0,,Table SQL / Planner,,,,,0,pull-request-available,,,"when calling procedure operation in Flink SQL, the ProcedureContext only contains the underlying application-specific config , not environment-specific config.
to be more specific, in a  Flink sql app of the same StreamExecutionEnvironment which has a config1. when executing a sql query, config1 works, while calling a sql procedure, config1 doesn't work, which apparently is not an expected behavior.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 08:17:24 UTC 2024,,,,,,,,,,"0|z1nm60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 09:56;JustinLee;I'd like to take this ticket. [~yuxia] [~hackergin]  Could you help to assign this to me?;;;","27/Feb/24 04:04;xuyangzhong;Hi, [~JustinLee] . Do you mean there are some configs lost when executing procedure in SqlGateway? IIUC, the table config used when calling procedure is from `ExecutableOperationContextImpl`, which is built in `OperationExecutor` with the logic following.
{code:java}
private TableConfig tableConfig() {
    Configuration operationConfig = sessionContext.getSessionConf().clone();
    operationConfig.addAll(executionConfig);

    TableConfig tableConfig = TableConfig.getDefault();
    tableConfig.setRootConfiguration(sessionContext.getDefaultContext().getFlinkConfig());
    tableConfig.addConfiguration(operationConfig);

    return tableConfig;
} {code};;;","28/Feb/24 03:21;JustinLee;Hi, [~xuyangzhong] . Not executing in SqlGateway , just in a normal sql app can remake the problem , i illustrate it in the test case in [pr24397|[https://github.com/apache/flink/pull/24397]]  you can have a look If it's convenient for you. Thanks

 ;;;","04/Mar/24 08:17;luoyuxia;1.18: 620c5a7aeba448d247107ad44a6ba6f1e759052e

1.19: fa426f104baa1343a07695dcf4c4984814f0fde4

master: 24b6f7cbf94fca154fc8680e8b3393abd68b8e77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move CheckpointingMode to flink-core,FLINK-34516,13569810,13566461,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,26/Feb/24 08:57,09/Apr/24 07:02,04/Jun/24 20:40,22/Mar/24 03:35,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35060,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 09 07:01:33 UTC 2024,,,,,,,,,,"0|z1nm48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/24 03:35;masteryhx;merged 8fac8046...77945916 into master;;;","08/Apr/24 07:59;martijnvisser;[~Zakelly] I've been trying to get the Flink Kafka connector to compile with Flink 1.20-SNAPSHOT, but that currently throws the following error

{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-connector-kafka: Compilation failure
[ERROR] /Users/mvisser/Developer/flink-connector-kafka/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/testutils/KafkaSinkExternalContext.java:[136,92] incompatible types: org.apache.flink.core.execution.CheckpointingMode cannot be converted to org.apache.flink.streaming.api.CheckpointingMode
[ERROR] 
{code}

Was this change a breaking change for connectors?;;;","08/Apr/24 12:54;zakelly;[~martijnvisser] It shouldn't be a breaking change in 1.20. I missed the point that the {{TestingSinkSettings}} is used by downstream projects. I will restore the former signature. Thank you for pointing this out!;;;","09/Apr/24 07:01;zakelly;Created FLINK-35060 to address this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document new FLIP process,FLINK-34515,13569807,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,26/Feb/24 08:22,26/Feb/24 08:39,04/Jun/24 20:40,26/Feb/24 08:39,,,,,,,,,,,Documentation,,,,,0,,,,"Per https://lists.apache.org/thread/rkpvlnwj9gv1hvx1dyklx6k88qpnvk2t

Contributors create a Google Doc and make that view-only, and post that Google Doc to the mailing list for a discussion thread. When the discussions have been resolved, the contributor ask on the Dev mailing list to a committer/PMC to copy the contents from the Google Doc, and create a FLIP number for them. The contributor can then use that FLIP
to actually have a VOTE thread.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 08:39:08 UTC 2024,,,,,,,,,,"0|z1nm3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 08:39;martijnvisser;FLIP page updated, see https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=293047020;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e (1) times out because of an error that's most likely caused by a networking issue,FLINK-34514,13569801,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,26/Feb/24 07:29,10/Apr/24 10:07,04/Jun/24 20:40,,1.20.0,,,,,,,,,,Test Infrastructure,,,,,0,github-actions,test-stability,,"https://github.com/apache/flink/actions/runs/8027473891/job/21931649433

{code}
Sat, 24 Feb 2024 03:35:54 GMT
ERROR: failed to solve: process ""/bin/sh -c set -ex;   wget -nv -O /usr/local/bin/gosu \""https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)\"";   wget -nv -O /usr/local/bin/gosu.asc \""https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc\"";   export GNUPGHOME=\""$(mktemp -d)\"";   for server in ha.pool.sks-keyservers.net $(shuf -e                           hkp://p80.pool.sks-keyservers.net:80                           keyserver.ubuntu.com                           hkp://keyserver.ubuntu.com:80                           pgp.mit.edu) ; do       gpg --batch --keyserver \""$server\"" --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 && break || : ;   done &&   gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu;   gpgconf --kill all;   rm -rf \""$GNUPGHOME\"" /usr/local/bin/gosu.asc;   chmod +x /usr/local/bin/gosu;   gosu nobody true"" did not complete successfully: exit code: 4
Sat, 24 Feb 2024 07:10:28 GMT
==========================================================================================
Sat, 24 Feb 2024 07:10:28 GMT
=== WARNING: This task took already 95% of the available time budget of 299 minutes ===
Sat, 24 Feb 2024 07:10:28 GMT
==========================================================================================
{code}

There are also warning messages in this build that state that the timeout of the Http client was reached:
{code}
Java 21 / Test (module: misc)
Failed to download action 'https://api.github.com/repos/actions/download-artifact/tarball/eaceaf801fd36c7dee90939fad912460b18a1ffe'. Error: The request was canceled due to the configured HttpClient.Timeout of 100 seconds elapsing.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 10 10:07:14 UTC 2024,,,,,,,,,,"0|z1nm28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/24 07:31;mapohl;Similar warning observed (w/o failure): https://github.com/apache/flink/actions/runs/8027473900;;;","26/Feb/24 07:33;mapohl;e2e (1) timed out: https://github.com/apache/flink/actions/runs/8027474081/job/21931640837#step:3:18653

Again, the Http client warnings appeared.;;;","10/Apr/24 10:07;rskraba;1.19 Java 11: E2E (group 1): https://github.com/apache/flink/actions/runs/8624933787/job/23640970021

This looks suspiciously related, but occurs much earlier in the {{actions/download-artifacts}} execution.  A similar URL and HttpClient.Timeout can be seen: 

{code}
2024-04-10T02:18:45.2233496Z Requested labels: ubuntu-22.04
2024-04-10T02:18:45.2233822Z Job defined at: apache/flink/.github/workflows/template.flink-ci.yml@refs/heads/release-1.19
2024-04-10T02:18:45.2234105Z Reusable workflow chain:
2024-04-10T02:18:45.2234257Z apache/flink/.github/workflows/nightly.yml@refs/heads/release-1.19 (b1165a89edb9857754e283c6afd7983a34acd465)
2024-04-10T02:18:45.2234423Z -> apache/flink/.github/workflows/template.flink-ci.yml@refs/heads/release-1.19 (b1165a89edb9857754e283c6afd7983a34acd465)
2024-04-10T02:18:45.2234587Z Waiting for a runner to pick up this job...
2024-04-10T02:18:45.6381893Z Job is waiting for a hosted runner to come online.
2024-04-10T02:18:48.8294299Z Job is about to start running on the hosted runner: GitHub Actions 641 (hosted)
2024-04-10T02:18:50.8589349Z Current runner version: '2.315.0'
2024-04-10T02:18:50.8612977Z ##[group]Operating System
2024-04-10T02:18:50.8613697Z Ubuntu
2024-04-10T02:18:50.8614044Z 22.04.4
2024-04-10T02:18:50.8614407Z LTS
2024-04-10T02:18:50.8614797Z ##[endgroup]
2024-04-10T02:18:50.8615193Z ##[group]Runner Image
2024-04-10T02:18:50.8615619Z Image: ubuntu-22.04
2024-04-10T02:18:50.8616045Z Version: 20240407.1.0
2024-04-10T02:18:50.8617050Z Included Software: https://github.com/actions/runner-images/blob/ubuntu22/20240407.1/images/ubuntu/Ubuntu2204-Readme.md
2024-04-10T02:18:50.8618509Z Image Release: https://github.com/actions/runner-images/releases/tag/ubuntu22%2F20240407.1
2024-04-10T02:18:50.8619424Z ##[endgroup]
2024-04-10T02:18:50.8619846Z ##[group]Runner Image Provisioner
2024-04-10T02:18:50.8620333Z 2.0.361.1
2024-04-10T02:18:50.8620699Z ##[endgroup]
2024-04-10T02:18:50.8622780Z ##[group]GITHUB_TOKEN Permissions
2024-04-10T02:18:50.8624494Z Actions: read
2024-04-10T02:18:50.8624896Z Checks: read
2024-04-10T02:18:50.8625427Z Contents: read
2024-04-10T02:18:50.8625962Z Deployments: read
2024-04-10T02:18:50.8626353Z Discussions: read
2024-04-10T02:18:50.8626782Z Issues: read
2024-04-10T02:18:50.8627146Z Metadata: read
2024-04-10T02:18:50.8627531Z Packages: read
2024-04-10T02:18:50.8627960Z Pages: read
2024-04-10T02:18:50.8628340Z PullRequests: read
2024-04-10T02:18:50.8628746Z RepositoryProjects: read
2024-04-10T02:18:50.8629244Z SecurityEvents: read
2024-04-10T02:18:50.8629649Z Statuses: read
2024-04-10T02:18:50.8630013Z ##[endgroup]
2024-04-10T02:18:50.8633173Z Secret source: Actions
2024-04-10T02:18:50.8633767Z Prepare workflow directory
2024-04-10T02:18:50.9605558Z Prepare all required actions
2024-04-10T02:18:50.9764099Z Getting action download info
2024-04-10T02:18:51.1165514Z Download action repository 'actions/checkout@v4' (SHA:b4ffde65f46336ab88eb53be808477a3936bae11)
2024-04-10T02:18:52.7120185Z Download action repository 'actions/download-artifact@v4' (SHA:c850b930e6ba138125429b7e5c93fc707a7f8427)
2024-04-10T02:20:32.7293860Z ##[warning]Failed to download action 'https://api.github.com/repos/actions/download-artifact/tarball/c850b930e6ba138125429b7e5c93fc707a7f8427'. Error: The request was canceled due to the configured HttpClient.Timeout of 100 seconds elapsing.
2024-04-10T02:20:32.7298050Z ##[warning]Back off 10.8 seconds before retry.
2024-04-10T02:22:23.5329717Z ##[warning]Failed to download action 'https://api.github.com/repos/actions/download-artifact/tarball/c850b930e6ba138125429b7e5c93fc707a7f8427'. Error: The request was canceled due to the configured HttpClient.Timeout of 100 seconds elapsing.
2024-04-10T02:22:23.5332253Z ##[warning]Back off 19.002 seconds before retry.
2024-04-10T02:23:25.2159238Z ##[error]Connection reset by peer (api.github.com:443)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
